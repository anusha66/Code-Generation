iso_gdf.plot();
store_items['combi'] = store_items['pants'] + store_items['shirts'] $ store_items
cursor = con.execute('SELECT * FROM samples') $ cursor.fetchall()
the_data = tmp_df.applymap(lambda x: 1 if x > 3 else 0).as_matrix() $ print(the_data.shape)
meanMonthlyReturns
df_new[['CA','US']] = pd.get_dummies(df_new['country'])[["CA","US"]]
loan_stats = loan_stats[meets_credit_policy] $ loan_stats['loan_status'].table()
( $     autos["registration_year"].value_counts(normalize=True) $     .head(10) $ )
wed11.shape
questions['years_attend'].value_counts()
frame.loc[['a', 'b', 'c', 'd'], ['Texas', 'Utah', 'California']]
datatest.loc[datatest.place_name == "Solar del Bosque",'lat'] = -34.905555 $ datatest.loc[datatest.place_name == "Solar del Bosque",'lon'] = -58.507635
final_data = pd.read_pickle('data/pickled/new_ratings_data.pkl') $ final_data.info() $ final_data.head()
for topic in topic_words: $     print(topic[:20])
stg(df['y'])
df['MeanFlow_cms'].describe()
df.tail()
user_m = user_mentions['user_mention'][user_mentions['created_at'].dt.week == 3].value_counts()[0:20] $ print(user_m)
boro = pc_order.index.values[1] $ bb = bo.loc[bo['BORO']==boro,'BORONAME'].values[0]
archive_copy = archive_copy.drop(['text', 'source', 'in_reply_to_status_id', 'in_reply_to_user_id'], axis=1)
top_supports.head(5).to_csv("top_supporters.csv")
c_numer = [0, 420, 666, 1776] $ archive_df[archive_df.rating_numerator.isin(c_numer)][['text', 'rating_numerator', 'rating_denominator']]
now = pd.Timestamp('now') $ now, now.tz is None
temp_df = Ralston.TMAX $ for i in range(10): $     temp_df= temp_df + 5 $     print ("Iteration",i+1,":", temp_df.mean(), temp_df.std())
plt.scatter(df_elect['Polarity'], df_elect['Subjectivity'], alpha=0.5, color='darkred') $ plt.title('Tweet #Election2018, Subjectivity on Polarity') $ plt.ylabel('Subjectivity') $ plt.xlabel('Polarity') $ plt.show()
user_tweet_freq(tweet_df, 'Abigail Lis')
for unigram_sentence in it.islice(unigram_sentences, 230, 240): $     print u' '.join(unigram_sentence) $     print u''
etsamples.columns
hitDetector=HitDetector() $ hitDetector.init(path='../data/')
df_new.head(3)
result1 = (df1 < df2) & (df2 <= df3) & (df3 != df4) $ result2 = pd.eval('df1 < df2 <= df3 != df4') $ np.allclose(result1, result2)
error = train_ALS(training_RDD, test_RDD, best_rank, seed, iterations, lambda_=0.01) $ print('For testing data the RMSE is %s' + str(error))
y_proba = safe_norm(caps2_output, axis=-2, name="y_proba")
points[points>100] $
s = pd.Series(['a1', 'b2', 'c3']) $ s
from sklearn.cross_validation import train_test_split $ train, test = train_test_split(final_titles_list, train_size = 0.8, random_state = 44 ) $ train10, test10 = train_test_split(new_titles_list10, train_size = 0.8, random_state = 44 )
t = pd.DataFrame(t.groupby('location').size())
people['a'].loc['Joe']
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace = True) $ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace = True)
training_labels[:5]
if '311_2015_flood_reports.csv' in os.listdir(INPUT_PATH): $     print("Using existing 311_2015_flood_reports.csv") $ else: $     load_data() $     filter_311(pd.read_csv(INPUT_PATH + '311_2015_short.csv')) $
kickstarter[kickstarter["usd pledged"].isnull()][0:10]
iw53Result=iw53Reader("IW53.txt")
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 2) & (jobs.GPU == 1)][['Memory','Wait']].groupby(['Memory']).agg(['mean', 'median','count']).reset_index()
file = 'https://assets.datacamp.com/production/course_1975/datasets/titanic_all_numeric.csv' $ titanic = pd.read_csv(file) $ titanic.head()
lowest_temp_recorded = session.query(func.min(Measurement.tobs)).filter(Measurement.station=='USC00519281').group_by(Measurement.station).all() $ lowest_temp_recorded $
controldf=df2[df2.group=="control"] $ controldf[controldf.converted==1].count()[0]/controldf.count()[0]
df.num_comments.min()
users_orders = users_orders.assign(Cost_14 = lambda x: x.Costs * (x.Reg_date + pd.DateOffset(14) >= x['Order Date'])) $ users_orders = users_orders.assign(Cost_30 = lambda x: x.Costs * (x.Reg_date + pd.DateOffset(30) >= x['Order Date'])) $ users_orders = users_orders.assign(Cost_60 = lambda x: x.Costs * (x.Reg_date + pd.DateOffset(60) >= x['Order Date'])) $ users_orders.head()
df0 = pd.read_csv('2000.csv')
from datetime import timedelta $ a_day = timedelta(days=1) $ now + a_day
actor = pd.read_sql_query('select actor_id, first_name, last_name from actor where first_name="Joe"', engine) $ actor.head()
stats.ttest_ind(clean_users[clean_users['active']==1]['account_life'].dt.days,clean_users[clean_users['active']==0].dropna(how = 'any')['account_life'].dt.days,equal_var = False)
df_z= df_cb.groupby(["landing_page","group"]).count() $ df_z $
 sum(edad.isnull())
result.summary()
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05/2)))
df = df.replace('tomato', 'pea') $ df
print(autos["price"].unique().shape) $ print(autos["price"].describe()) $ autos["price"].value_counts().head(20)
building_pa_prc_shrink.dtypes
config = Configure("nhtsa_demo")
start = datetime.datetime.strptime("2012-09-09", "%Y-%m-%d") $ date_list = [start + relativedelta(weeks=x) for x in range(0,len(df.sales))] $ df['index'] =date_list $ df.set_index(['index'], inplace=True) $ df.index.name=None
reply_or_retweet  = tweet_archive_df[(tweet_archive_df.in_reply_to_status_id.notnull()) | $                                      (tweet_archive_df.retweeted_status_id.notnull())] $ any(tweet_id for tweet_id in list(reply_or_retweet.tweet_id) \ $     if tweet_id in list(tweet_archive_clean.tweet_id))
fulldf = pd.concat([Akrondf, Albuquerquedf, Anaheimdf, Anchoragedf, Arlingtondf, Atlantadf, Auroradf, Austindf, Bakersfielddf, Baltimoredf, Baton_Rougedf, Birminghamdf, Bostondf, Buffalodf, Chandlerdf, Charlottedf, Chesapeakedf, Chicagodf, Chula_Vistadf, Cincinnatidf, Clevelanddf, Colorado_Springsdf, Columbusdf, Corpus_Christidf, Dallasdf, Denverdf, Detroitdf, Durhamdf, El_Pasodf, Fort_Waynedf, Fort_Worthdf, Fremontdf, Fresnodf, Garlanddf, Glendaledf, Greensborodf, Hendersondf, Hialeahdf, Honoluludf, Houstondf, Indianapolisdf, Jacksonvilledf, Jersey_Citydf, Kansas_Citydf, Laredodf, Las_Vegasdf, Lexington_Fayettedf, Lincolndf, Long_Beachdf, Los_Angelesdf, Louisvilledf, Lubbockdf, Madisondf, Memphisdf, Mesadf, Miamidf, Milwaukeedf, Minneapolisdf, Modestodf, Montgomerydf, Nashvilledf, New_Orleansdf, New_Yorkdf, Newarkdf, Norfolkdf, Oaklanddf, Oklahoma_Citydf, Omahadf, Orlandodf, Philadelphiadf, Phoenixdf, Pittsburghdf, Planodf, Portlanddf, Raleighdf, Renodf, Riversidedf, Rochesterdf, Sacramentodf, San_Antoniodf, San_Diegodf, San_Franciscodf, San_Josedf, Santa_Anadf, Scottsdaledf, Seattledf, Shreveportdf, St_Louisdf, St_Pauldf, St_Petersburgdf, Stocktondf, Tampadf, Toledodf, Tucsondf, Tulsadf, Virginia_Beachdf, Washingtondf, Wichitadf])
(temp_df.emailHash == '44d0dc437936b13f7cea2f77053806bd').sum()
df.loc['Equatorial Diameter:']
sns.distplot(train_df.Log_price.values, bins=100, kde=True)
act_diff = df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean() $ (act_diff < p_diffs).mean()
df_combined = countries.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_combined.head()
reqs.set_index(['Unnamed: 0', 'Agency', 'Complaint Type', 'Descriptor', 'Location Type', 'Bridge Highway Direction'], inplace=True) $ reqs.head()
bacteria_data[['phylum','value','patient']]
%time resultvalue_records = odm2rest_request('resultvalues', rv_params)
people.query("age > 30 and pets == 0")
committee_df[['name','state', 'party_full', 'party_type_full', 'designation_full', 'committee_type_full', 'website']]
plt.figure(figsize=(8, 5)) $ plt.hist(train_df.favs_lognorm); $ plt.title('The distribution of the property favs_lognorm');
new_page = df2.query('landing_page == "new_page"') $ new_page.landing_page.count()
raw_data
print 'If we create a Series from the DataFrame, then passing a single value does work.  CONFUSING!' $ msftC = msft['Close'] $ msftC['2012-01-03']
apps[apps.bad_rate > 0.8]
mainstream_facts_metrics.shape
print(soup.prettify())
a= '10000040'
to_be_predicted_Day4 = 22.34273663 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
rf.score(clim_test, size_test)
image_predictions = pd.read_csv(filename, sep='\t') $ image_predictions.head()
coming_next_reason.to_csv('../data/coming_next_reason.csv')
retention_10 = unstacked[[i for i in range(11)]]
countries_df = pd.read_csv('countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
df = pandas.DataFrame(clean_words, columns=['Words']) $ df['Words'].value_counts()
df1['Adj. Close'].plot() $ plt.xlabel('Date') $ plt.ylabel('Price') $
population / area
filtered = vwap.between_time('10:00', '16:00') $ filtered.head(20)
trip_data_q5["date"] = trip_data_q5["date"].astype('category') $ trip_data_q5.groupby("date").agg(["mean"])
poly17= PolynomialFeatures(degree=17) $ X_17 = poly17.fit_transform(X) $ linear = linear_model.LinearRegression() $ linear.fit(X_17, y2) $ (linear.coef_, linear.intercept_)
algo = make_pipeline(preprocessing.MinMaxScaler(), svm.LinearSVC(class_weight='balanced')) $ scores = cross_val_score(algo, X, y, cv=5, scoring='f1') $ scores.mean() $ print(np.round(scores.mean(),3))
geo_enable = dataset['User_Geo_Enabled'] $ chart_title = 'Proportion of user has enabled the possibility of geotagging of their tweets' $ geo_enable.value_counts().plot.pie(label='', fontsize=11, autopct='%.2f', figsize=(5, 5), title=chart_title, legend=True);
df['production_companies'].head(15)
%time headdtm2 = stfvect.fit_transform(revs.head(20000))
quarterly_revenue = pd.Series([300, 320, 290, 390, 320, 360, 310, 410], index = quarters) $ quarterly_revenue
data['Log_Ret'] = np.log(data['Close'] / data['Close'].shift(1)) $ data['Volatility'] = pd.rolling_std(data['Log_Ret'], window=252) * np.sqrt(252) $ data['ATR'] = abs (data['High'] - data['Low']) $ data.tail()
new_page_converted = np.random.choice([0, 1], size = n_new, p = [1 - p_new, p_new]) $ p_new_sim = new_page_converted.mean()
new_page = df2[((df2['landing_page'] == 'new_page') )].count()[0] $ all_page = df2['landing_page'].count() $ prob_new_page = round(new_page/ (all_page),2) $ print('the probability that an individual received the new page {}'.format(prob_new_page)) $
p_old = df2['converted'].mean() $ print(p_old)
pyLDAvis.save_html(vis, f"output/lda_{topics}topics_score{round(coherence_lda,4)}.html")
flight2.groupby('stop_info', $                 'stop_info1', 'stop_loc1', $                 'stop_duration1', 'stop_duration_h1', 'stop_duration_m1', $                 'stop_info2', 'stop_loc2', $                 'stop_duration2',  'stop_duration_h2', 'stop_duration_m2').count().show(500, truncate=False) $
df_protest.dtypes=='object'
import re $ pattern = re.compile('AA') $ the_str = 'AAbc'
mean_squared_error(y_true, y_pred)
finalSymbolsList = strippedSymbolsList.apply(getFinalSymbol)
df_r.head()
submission_full[['proba']].mean()
df_transactions['membership_duration']  = df_transactions['membership_duration']  / np.timedelta64(1, 'D')
df_total.head()
ds_temp_casts_CTD_1988.Project.dtype
roc_auc_score(y_test, y_pred_clf)
unique_brands = autos['brand'].value_counts(normalize=True) $ unique_brands
import os $ import sys $ import pickle $ import pandas as pd
%time train = pd.read_csv("../assets/trainaa") $ train.head(1)
station_availability_df['avail_docks'].plot(kind='hist', rot=70, logy=True)
errors = pd.read_csv('errors.csv', encoding='utf-8') $ errors.head()
df.count(), len(df.columns)
logit = sm.Logit(df2['converted'],df2[['intercept' ,'treatment']]) $ results = logit.fit()
nitrogen['HydrologicEvent'].unique()
df.groupby('episode_id')['id'].nunique().min()
solar_wind_df = pd.read_excel(weather_folder+'/imputed_solar_wind.xlsx')
regr2 = linear_model.LinearRegression() $ X2 = cc[['open','high','low']]
gc.collect()
autos.registration_year.value_counts(normalize = True).sort_index()
print(f"Fit3 shape: {fit3.shape}, Fit3 non-nulls: {fit3.nnz}") $ print(f"Non-null fraction of total: {'{:.10f}'.format(fit3.nnz/(fit3.shape[0] * fit3.shape[1]))}")
fig = plt.figure(figsize=(15,8)) $ ax = fig.add_subplot(111) $ ax = resid_713.plot(ax=ax);
result['lastUpdatedUTC'] = pd.to_datetime(result['lastUpdated'], unit='ms')
def get_num(x): $     return int(''.join(ele for ele in x if ele.isdigit())) $ print(get_num("2,000")) $ print(get_num("100+")) $ print(get_num("63[a]"))
df2.shape[0]
a = df.user_id.unique() $ print(' The number of unique users in the dataset is {}'.format(len(a))) $
status_df.info()
day = pd.tseries.offsets.Day()
print(len(df)) $ print(df.head()) $ print(topics.head())
n_old_page = len(df2.query("group == 'control'")) $ print(n_old_page)
df.to_csv('Tweets_sports.csv')
question_3_dataframe = data_2017_subset.copy() $ question_3_dataframe.head(2)
stadium_arr.groupby('home_team')['arrests'].sum().sort_values(ascending=False)[:10].plot(kind='bar', title='Overall Number of Arrests at Home Stadium (top 10)') $ plt.savefig('overallArrestsAtHome.png', bbox_inches='tight')
sfs1.k_feature_names_
cserie(exploration_titanic.narows_full) # no rows of only missing values
df_complete_b.shape[0]
data.columns
sensor_clean
plt.rcParams['axes.unicode_minus'] = False $ dta_50.plot(figsize=(15,5)) $ plt.show()
proc.document_length_stats
prcp_1_df = pd.DataFrame(prcp_data, columns=['Precipitation Date', 'Precipitation']) $ prcp_1_df.set_index('Precipitation Date', inplace=True) # Set the index by date $ prcp_1_df.count()
all_years_by_DRG = all_years_by_DRG.reset_index() $ all_years_by_DRG.head()
l2=len(histories_b5) $ print(l2)
old_page_converted = np.random.choice([1, 0], size = n_old, p = [p_mean, (1-p_mean)], replace = True)
pr_item = (2017, 1) $ url = 'https://www.sec.gov/news/press-release/' + str(pr_item[0]) + '-' + str(pr_item[1]) $ page = urlopen(url)
import pandas as pd $ import numpy as np
df.product_type.value_counts()
df_new = df2.set_index('user_id').join(df_countries.set_index('user_id'),how='inner') $ df_new.head()
train = pd.read_csv("train.csv", header=0, sep=",", encoding="latin_1") $ test =  pd.read_csv("test.csv", header=0, sep=",", encoding="latin_1")
dfb_train.isnull().sum()
bands.at['2018-04-29 13:24:49', '68'] = 1 $ bands = bands.drop(['drop', 'drop2', '68_2', 'drop3', 'drop4'], axis=1)
from sklearn.decomposition import LatentDirichletAllocation $ from sklearn.feature_extraction.text import CountVectorizer
outcols = ['site_name', 'user_location_country', 'user_location_region', 'user_location_city', $            'is_mobile', 'is_package', 'channel', 'srch_adults_cnt', 'srch_children_cnt', $            'srch_destination_type_id', 'hotel_continent', 'hotel_country', 'hotel_market', 'hotel_cluster'] $ dfXy = dfXy[outcols].copy() $ dfXy.to_csv(dataurl+'train200thdrop.csv', sep=',', encoding='utf-8', index=False)
frames=[NameEvents,ValidNameEvents] $ TotalNameEvents = pd.concat(frames)
print metrics.confusion_matrix(y_test, predicted) $ print metrics.classification_report(y_test, predicted)
data_df['clean_desc'] = data_df.apply(text_clean, axis=1) $ data_df['clean_desc'] = data_df['clean_desc'].astype(unicode) $ data_df['nwords'] = data_df['clean_desc'].str.count(' ').add(1)
len(X)
dta.loc[(dta.risk == "Risk 1 (High)") | (dta.risk == "Risk 1 (Medium)")].head()
df.reset_index(inplace=True)
feedback
df_campaigns = pd.read_csv(master_folder + campaigns)
date = pd.date_range('1975',periods = 47, freq = 'A') $ print(pd.DataFrame(date))
df_goog
dataset.info()
jobPostDFSample['JobDescription'].fillna("TBD", inplace=True)
prcp_df.describe()
tweets_df.head()
tset.to_csv('training_data/utah_training_set.csv')
r = requests.get('http://api.open-notify.org/iss-now.json') $ r.status_code
del datatest['covered/total'] $ datatest['covered/total'] = datatest['surface_covered_in_m2']/datatest['surface_total_in_m2']
s_filled = s.fillna(0) $ s_filled
fb_train = fb_features[:'2016'] $ fb_test = fb_features['2017':]
df_all.age.value_counts()
readme = processing_test.README() $ readme = open(readme, "r") $ print(readme.read())
indices = [ i for i, item in autos.iterrows() if item.str.contains(r'(?i)tiguan', na=False).values.any() ] $ tiguan2 = autos.loc[indices] # 931 rows x 20 columns $ tiguan2 = tiguan2.query("powerPS == 140 and gearbox == 'automatik'") $ tiguan2
data
page = BeautifulSoup(logged_out.content) $ with open('logged_out'+str(2)+'.html', 'w') as html_file: $     html_file.write(page.prettify('utf-8'))
accepted = train[train['project_is_approved'] == 1] $ for i in check_cols: $     print(accepted[i].value_counts()) $     print('')
remove_list = archive_clean[(archive_clean['rating_numerator'] == 0) | (archive_clean['rating_numerator'] == 1)].tweet_id $ for i in remove_list: $     print(i) $
sample_size_old_page = df2.query('landing_page == "old_page"').shape[0] $ print('Sample size old_page: {}'.format(sample_size_old_page)) 
!ls ../data
transactions[~transactions['TransactionID'].isin(dfTemp['TransactionID'].values)]
df_json.info()
age_gender_bkts = pd.read_csv('./airbnb firstdestinations/age_gender_bkts.csv')
no_specialty.shape
t = time.perf_counter() $ cv_score(lasso, cv=5) $ elapsed_time = (time.perf_counter() - t)/60 $ print("This cell took {:0.2f} minutes to run".format(elapsed_time))
isWeekend = 0 if  datetime.datetime.now().weekday()<5 else 1 $ isWeekend $
csv_paths = sorted([os.path.join(data_dir, fname) for fname in os.listdir(data_dir) $                     if fname.endswith('.csv')])
old_page_converted = np.random.choice([1,0],n_old,p=[p_old,(1-p_old)]) $ old_page_converted
SEA["Team"] = "SEA"
a.size
df = pd.DataFrame(bmp_series, columns = ['mean_price']) $ df
df_temp = df.query('landing_page == "new_page" & group == "treatment"') $ df2 = df.query('landing_page == "old_page" & group == "control"') $ df2 = df2.append(df_temp)
daily_transaction=file4.groupby('date').agg({'uuid':'count','trans_amt':'avg'}).sort(desc("date")) $ daily_transaction.show()
math.sqrt(((lmscore.predict(X_test)-y_test)**2).mean())
tweet_archive_clean = pd.melt(tweet_archive_clean, id_vars=columns_to_keep, $                               var_name='stages', value_name='dog_stage')
intake.shape
p_new = df2[df2['converted']==1]['user_id'].count() / df2.shape[0] $ p_new
twitter_archive_clean[twitter_archive_clean['retweeted_status_user_id'].isnull()==False].shape[0]
df = pd.DataFrame({'one' : pd.Series(np.random.randn(3), index=['a', 'b', 'c']), $     'two' : pd.Series(np.random.randn(4), index=['a', 'b', 'c', 'd']), $     'three' : pd.Series(np.random.randn(3), index=['b', 'c', 'd'])}) $ df
file_path = "2017 CAM data from iPads.xlsx"
X['y'] = np.where(X["userid"].isin(nextWeekUserList),0 , 1) $ X.head(5)
twitter_df_clean.head(2)
most_activity = temp_data_query[0][0:2] $ most_activity
visual_recognition = VisualRecognitionV3( $     '2018-03-19', $     iam_api_key=os.environ['BLUEMIX_API_KEY'])
soft_outliers_fare = outlier_detection.outlier_detection_serie_1d('fare',cutoff_params=outlier_detection.basic_cutoff) $ strong_outliers_fare = outlier_detection.outlier_detection_serie_1d('fare',cutoff_params=outlier_detection.strong_cutoff)
d7 = d6.stack() $ d7
dump["country"].value_counts() # Returns the number of times a single country appears in the column
tfa = np.vstack(df_all.timestamp_first_active.astype(str).apply(lambda x: list(map(int, [x[:4],x[4:6],x[6:8],x[8:10],x[10:12],x[12:14]]))).values) $ df_all['tfa_year'] = tfa[:,0] $ df_all['tfa_month'] = tfa[:,1] $ df_all['tfa_day'] = tfa[:,2] $ df_all = df_all.drop(['timestamp_first_active'], axis=1)
m.save('val0')
df_concensus_uaa = df_concensus[(df_concensus['ticker']=='UAA') & (df_concensus['fiscal_year'] == 2017)] $ df_concensus_uaa
daily_cases.unstack().head()
cohort['users'].sum()
for i in range(0, 10000): $     dfs_quantile[i] = dfs_morning.ENTRIES_MORNING.quantile(i/10000) $ plt.plot(range(0,10000), dfs_quantile) $ plt.axis([9900, 10000, None, None])
logreg = LogisticRegression() $ logreg.fit(X_train, Y_train) $ Y_pred = logreg.predict(X_test) $ acc_log = round(logreg.score(X_test, Y_test) * 100, 2) $ acc_log
for topic in topic_words: $     print(topic[:20])
keys.shape
X_train = X_train.to_frame() $ X_test = X_test.to_frame()
orgs['founded_on'] = pd.to_datetime(orgs['founded_on'], errors = 'coerce') $ orgs['first_funding_on'] = pd.to_datetime(orgs['first_funding_on'], errors = 'coerce') $ orgs['last_funding_on'] = pd.to_datetime(orgs['last_funding_on'], errors = 'coerce') $ orgs['closed_on'] = pd.to_datetime(orgs['closed_on'], errors = 'coerce') $ orgs['founded_year'] = orgs.founded_on.dt.year
dfs_resample.reset_index(inplace = True) $ dfs_resample['TIME'] = pd.to_datetime(dfs_resample['DATE_TIME']).dt.hour $ dfs_morning = dfs_resample[(dfs_resample['TIME'] == 6) | (dfs_resample['TIME'] == 12)]
sites_on_net = sites[sites['On Zayo Network Status'] != 'Not on Zayo Network'].groupby(['Account ID'])['Building ID'].count().reset_index().sort_values(by='Building ID', ascending=False)
df.groupby('group').mean()['converted']
autos['num_photos'].value_counts()
hi_conf_gain
from sklearn.datasets import fetch_california_housing
yxe_tweets['userHandle'].value_counts().head(10)
QUIDS_wide.drop(labels=["qstot_12","qstot_14"], axis = 1, inplace=True)
test_df["labels"] = probs_test.detach().data.cpu().numpy()[:, 1];
from sklearn.cross_validation import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
mlp_fp = 'data/richmond_median_list_prices.csv' $ mlp_df = pd.read_csv(mlp_fp) $ mlp_df.head()
tips.groupby(["sex","day"]).tip.mean().plot(kind="bar")
mydata[mydata.isnull().any(axis=1)]
def clean_dataset(x): $     if isinstance(x, str): $         return str.lower(x.replace(" ", "").replace("&", ",").replace("/",",")) $     else: $         return ''
df.age.describe()
a.split(',')
dog_stages = ['doggo', 'floofer', 'pupper', 'puppo'] $ for column in dog_stages: $     tweet_archive_clean.eval(column).loc[tweet_archive_clean.tweet_id.isin(manual_stage_fix_ids)] = 'None' $ tweet_archive_clean.update(manual_stage_fixes.rename('doggo'))
driver_sum = merged_df_nodup.groupby("type").sum().driver_count $ plt.pie(driver_sum, explode = explode, labels = labels, colors = colors, autopct = "%.1f%%", shadow = True, startangle= 150) $ plt.title("% of Total Drivers by City Type") $ plt.savefig("totaldrivers.png")
Base.prepare(engine, reflect=True) $
df = pd.read_sql('SELECT * FROM booking_contacts', con=conn_a) $ df
dfs['Stock First Difference'].plot()
requests.saveAsTextFiles("/dir/requests")
elos, probs, elo_dict = elo_applier(afl_data, 30)
FREEVIEW.plot_fixation_durations(raw_freeview_df, option='facet_subjects')
dateparse = lambda x: pd.datetime.strptime(x, '%Y%m') $ ff = pd.read_csv('F-F_Research_Data_Factors.csv',index_col=0, parse_dates=True, date_parser=dateparse)['2011':'2013'] $ ff = ff / 100 $ ff.head()
gdax_trans_btc.plot(kind='line',x='Timestamp',y='BTC_balance_GBP', grid=True);
players = pd.read_sql_query('select * from Player', conn)  # don't forget to specify the connection $ print(players.shape) $ players.head()
import json $ import requests $ import numpy as np
latest_timelog.head()
nba_df.shape
from tpot import TPOTRegressor $ from sklearn.model_selection import train_test_split
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq,df)
notus.isna().sum()
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=32000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
predictions = model_rf.transform(test_data) $ evaluatorRF = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy") $ accuracy = evaluatorRF.evaluate(predictions) $ print("Accuracy = %g" % accuracy)
df2[df2.user_id.duplicated(keep = False)]
p_diffs = np.array(p_diffs)
print(kmeans.cluster_centers_)  $
df.shape
IMDB_LABEL = data.Field(sequential=False) $ splits = torchtext.datasets.IMDB.splits(TEXT, IMDB_LABEL, 'data/')
autos['last_seen'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
df_clean3 = df_clean2.drop('timestamp', axis=1)
import pandas as pd
df - subframe
proportion_and_count(active_psc_statements,'statement',len(active_psc_statements))
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=1&api_key='+API_KEY $ r = requests.get(url) $
mx = confusion_matrix(y_test, y_hat) $ print (mx)
def join_df(left, right, left_on, right_on=None, suffix="_y"): $   if right_on is None: right_on=left_on $   return left.merge(right, left_on=left_on, right_on=right_on, suffixes=("",suffix))
df.tail() $
abe =reddit.submission(id=data['id'][1]) $ abe.comments.replace_more(limit=0) $ for top_level_comment in abe.comments: $     print(top_level_comment.body)
if True==0: $     price_mat['Ethereum'].plot() $     plt.title('Ethereum price') $     plt.show()
data_df.index = data_df['date'] $ consumption_df = data_df.loc[:,['consumption']]
os.system('zcat ' + file_path + ' | grep "#CHROM" | head -n 1 > /tmp/samples')
pred_probas_rfc_over
from CHECLabPy.core.io import DL1Reader $ reader = DL1Reader("refdata/Run17473_dl1.h5") $ reader.monitor.load_entire_table()
csvData[csvData['street'].str.match('.*South.*')]['street']
import codecs $ def save_file(text, file_name): $     with codecs.open(file_name, 'wb', 'utf8') as outfile: $         outfile.write(text) $
sorted(autos["registration_year"].unique())
to_be_predicted_Day2 = 22.39137095 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
print df.groupby('state')['amount',].sum().sort_values(by='amount', ascending=False)[0:10] $ print df.groupby('state')['donor_id',].count().sort_values(by='donor_id', ascending=False)[0:10]
grid = GridSearchCV(logreg, param_grid, cv=5, scoring='roc_auc') $ grid.fit(X, y) $ grid.grid_scores_
x = df.query('group != "treatment" and landing_page == "new_page"').count()[0] $ y = df.query('group == "treatment" and landing_page != "new_page"').count()[0] $ x + y
df2[df2.duplicated('user_id')].user_id
rdg = Ridge(alpha=5) $ rdg.fit(train_data, train_labels)
search_request_url
df_predictions_clean.info()
print (all_preds.shape) $ print (all_preds.shape[0]) $ print (all_preds.shape[1]) $ print (all_preds.shape[2])
time_window["screen_name"].value_counts().head(25)
sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger')
absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus), $                              name="absent_error_raw") $ absent_error = tf.reshape(absent_error_raw, shape=(-1, 10), $                           name="absent_error")
df["ARRIVIAL_DELAYED"].value_counts()
print '%d tweets after quake out of %d by users active before quake (%.2f%%)' % \ $ (beforeUsersAfter.shape[0],fullDf[fullDf.index>pd.datetime(2015,4,25)].shape[0],float(beforeUsersAfter.shape[0])/fullDf[fullDf.index>pd.datetime(2015,4,25)].shape[0])
loans_df.loan_status.value_counts()
df_users_4=pd.merge(df_users_3,pt_unit_completed,left_on='uid',right_on='uid',how='left') $ df_users_4.loc[df_users_4['no_units_completed'].isnull(),'no_units_completed']=0 $ df_users_4.loc[df_users_4['units_completed_flag'].isnull(),'units_completed_flag']='No unit completed'
dfUD_unique = dfUD.groupby(dfUD.columns[0],as_index=False,sort=False).min() $ dfUD_unique.columns = [['url','popdate']] $ dfUD_unique['urlid'] = dfUD_unique['url'].map(lambda x : x.split('-')[-1]) $ dfUD_unique.set_index('urlid',inplace=True) $ dfUD_unique.head()
prop.head()
rl = "https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_test.csv" $ u = urllib.request.urlopen(url) $ rawdata = u.read()
to_be_predicted_Day2 = 52.30367575 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
pd.merge(df1,df3,how='outer')
ip.head()
plt.hist(length)
test_df.labels.mean()
db = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False) $ cnx = db.connect() $ DF.to_sql(name = 'data_base_table_here', con= cnx, if_exists='append', index=True) $ cnx.close() $ db.dispose()
datetime.datetime.strptime("Mar 03, 2010", "%b %d, %Y")
sns.barplot(data=df.groupby('education').agg({'applicant_id':lambda x:len(set(x))}).reset_index(), $             x='education',y='applicant_id')
df[(df.state == 'YY') & (df.amount >= 45000)]\ $     .sort_values(by='amount', ascending=False)\ $     .head(6)[source_columns]\ $     .to_csv('out/0/outlier_data.csv')
knn.fit(train[['property_type', 'lat', 'lon','surface_total_in_m2']], train['covered/total'])
df['date'].min()
f_ip_os_clicks.show(1)
edad = data["Age"]
sensors_num_df.data.head()
print(log_loss(test_y, prediction_proba))
'Ohio' in frame3.columns
active_station_df= station_whighfreq() $ active_station_df
btc_forum_df = pd.read_csv('btc_forum_cleaned_new_pos_neg_sub.csv') $ btc_forum_df.set_index('timestamp', inplace=True) $ btc_forum_df.index = pd.to_datetime(btc_forum_df.index)
sns.pairplot(iris, hue='Species')
f_ip_app_hour_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_app_hour_clicks"), header=True) $ print('Found %d observations.' %f_ip_app_hour_clicks.count())
sensor.type
df_not4new = df.query('landing_page == "new_page"') $ df_43 = df_not4new.query('group != "treatment"') $ df_43.nunique()
df_meta[['domain_userid', 'domain_sessionid', 'user_id']].apply( $     pd.Series.nunique) # unique number of values for each column
y.info()
k1.head()
some_df = sqlContext.createDataFrame(some_rdd) $ some_df.printSchema()
y_pred = pipe_nb.predict(pulledTweets_df.emoji_enc_text) $ y_proba = pipe_nb.predict_proba(pulledTweets_df.emoji_enc_text) $ pulledTweets_df['sentiment_predicted_nb']=[classes[y_pred[i]] for i in range(len(y_pred))]
merged[['US', 'UK', 'CA']] = pd.get_dummies(merged['country'])
df.max()
def proportion(col): $     return col / col.sum() $ clinton_pivoted['aides_proportion'] = proportion(clinton_pivoted['False']) $ clinton_pivoted['clinton_proportion'] = proportion(clinton_pivoted['True']) $ clinton_pivoted[['aides_proportion', 'clinton_proportion']].plot.barh()
mu1, sd1 = np.mean(feature1), np.std(feature1) $ mu2, sd2 = np.mean(feature2), np.std(feature2) $ mu3, sd3 = np.mean(feature3), np.std(feature3) $ print sd1, sd2, sd3
tokens.sort_values('five_star_ratio', ascending=False).head(10)
asset1 = 'COPUSD' $ asset2 = 'AUDUSD' $ window = 30 $ bt_start_date = '2014-01-02' $ bt_end_date = '2017-12-31'
df_new[['US', 'UK']] = pd.get_dummies(df_new['country'])[['US', 'UK']]
Trump_week = Trump.groupby(by=["year","week","source"]).count().reset_index()[["year","week","text","source"]].merge(Trump.groupby(by=["year","week","source"]).sum().reset_index()[["year","week","negative","source"]],on=["year","week","source"]) $ Trump_week.head()
daily = data.set_index('created_at').resample('D').size() $ monthly_mean = daily.resample('M').mean() $ monthly_mean.index = monthly_mean.index.strftime('%Y/%m')
df2.head(3)
dfFull.GarageCars = dfFull.GarageCars.fillna(dfFull.GarageCars.mean().round())
data=(X*t1) + t0 $ data
all_sets["setSize"] = all_sets.apply(lambda x: x.cards.shape[0], axis = 1)
portal_pth = "../data"
twitter_archive_master = twitter_archive_master.drop(twitter_archive_master[twitter_archive_master['likes'] == 0].index)
session = Session(engine)
tizibika[tizibika['retweets']==tizibika['retweets'].max()]
hist = model.fit(Xtr_scale, Ytr_scale, epochs=20, $                  validation_data=(Xts_scale,Yts_scale), $                  verbose=0)
country = pd.DataFrame(g_all.loc[0:259,'Life expectancy']) $ country.columns = ['Life expectancy'] $ print(country.shape) $ print(country.head())
auth = tp.OAuthHandler(consumer_key=key, consumer_secret=secret) $ api = tp.API(auth)
df=pd.read_csv('LibCon03-06-2018.csv') $ df.shape
users_van = van_final.dropna(axis=0) $ users_van.apply(lambda x: sum(x.isna()))
df_grp = df.groupby('group') $ df_grp.describe()
workspace_uuid
pd.Series(data=predicted5).value_counts()
p1_table = profits_table.groupby(['Product']).Profit.sum().reset_index() $ p1_result = p1_table.sort_values('Profit', ascending=False) $ p1_result.head()
dft = buildDFbis(home_dir, tweet_dir)
msftAC[:5]
tweet_archive_enhanced_clean['rating_numerator'] = tweet_archive_enhanced_clean['text'].str.extract('([0-9]+[.]*[0-9]*)\/(10)',expand = True)[0].astype('float')
autos["price_dollars"].value_counts().sort_index(ascending=False)
df1 = pd.DataFrame(list(range(20)))
X.drop(["userid","recentTime","createdtm"], axis=1, inplace=True)
conn.setsessopt(caslib='otherlib') $
data[['team','captain','points']]
import pandas as pd $ %matplotlib inline
engagement[0]
df_inventory_santaclara['Year']=(df_inventory_santaclara['Date'].str.split('-').str[0]) $ df_inventory_santaclara['Month']=(df_inventory_santaclara['Date'].str.split('-').str[1]) $ df_inventory_santaclara
display(raw_data[(raw_data['name'].isna())]) $ raw_data[(raw_data['desc'].isna())]
X_mice = deepcopy(X_age_notnull)
df_vow.plot() $ df_vow[['Open','Close','High','Low']].plot()
(new_page_converted.mean()) - (old_page_converted.mean())
new_page_converted=np.random.binomial(n=1,p=pnew,size = n_new) $ new_page_converted
Image("/Users/jamespearce/repos/dl/data/dogscats/train/dog.7223.jpg")
model = pd.get_dummies(auto_new.CarModel) $ model = model.ix[:, ["ToyotaCamry", "ToyotaCorolla","ToyotaRav4", "ToyotaLandCruiserPrado", "ToyotaIpsum", "ToyotaSienna", "Toyota4-Runner"]] $ model.head()
week15 = week14.rename(columns={105:'105'}) $ stocks = stocks.rename(columns={'Week 14':'Week 15','98':'105'}) $ week15 = pd.merge(stocks,week15,on=['105','Tickers']) $ week15.drop_duplicates(subset='Link',inplace=True)
holidays=['state_hol', 'city_hol', 'nat_hol', 'nat_event']
tb_melt = pd.melt(tb, id_vars=['country', 'year']) $ tb_melt['gender'] = tb_melt.variable.str[0] $ tb_melt['age_group'] = tb_melt.variable.str[1:] $ print(tb_melt.head()) $
myarray=[['one','one','two','two','three','three'],['Red','Blue','Green','Cyan','Magenta','Yellow']] $ print(myarray)
autos['price'].head()
type(now)
auto_new.Country.unique()
linkNYC.head()
learn.save("dnn20")
logmod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results = logmod.fit() $
with open('data/chefkoch_12.json') as data_file:    $     chef12 = json.load(data_file) $ clean_new(chef12) $ chef12df = convert(chef12) $ chef12df.info()
ts = pd.Series(np.random.randn(len(rng)), index=rng)
(apple.tail(1).index[0] - apple.head(1).index[0]).days
path = os.path.join('Monthly EPA emissions.csv') $ epa = pd.read_csv(path)
def discounts(row): $     return 1 - (row['Gross Sales'] - row['Discounts']) / (row['Gross Sales']) $ discounts_table['Discount %'] = discounts_table.apply(discounts, axis=1) $ discounts_table.head()
knn_reg.score(x_test,y_test)
fraud_data_updated.shape
regr.score(experiment_X, experiment_y)
assert df_total.isna().any().any() == False
len(companies)
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment']
test_data = pd.read_csv("test.csv") $ print test_data.head() $ print test_data.count()
image_array = to_array_variabel(image, (153,1)) $ format_array = to_array_variabel(review_format, (153,1)) $ fake_array = to_array_variabel(fake, (153,1)) $ X_test = np.hstack((image_array, format_array, body.toarray(), title.toarray(), fake_array))
df2[df2.group=='control']['converted'].mean()
twitter_archive.source.value_counts()
low_odometer = (autos["odometer_km"] < odometer_LF) == True $ odometer_outliers = autos.loc[low_odometer, :].index
df1 = pd.read_csv('captures/botnet-capture-20110810-neris.csv')
np.histogram(noaa_data.loc["2018-05-29":"2018-05-29","AIR_TEMPERATURE"])
df4.query('four < 3')
cpq['CreatedDate'] = cpq['CreatedDate'].apply(lambda x : parse(x)) $ cpq.sort_values(by='CreatedDate', inplace=True) $ cpq.drop_duplicates(['Account ID', 'Product Group', 'Building ID'], inplace=True) $ opportunities.drop_duplicates(['Account ID', 'Product Group', 'Building ID'], inplace=True)
content_wed11 = pd.merge(wed11, mainstream, how='inner', $                          left_on='dimensions_item_id', $                          right_on='id')
1/np.exp(results.params)
def percentile(n): $     def percentile_(x): $         return np.percentile(x, n) $     percentile_.__name__ = 'percentile_%s' % n $     return percentile_
prices = pd.DataFrame(index=df.index)
output= "SELECT tweet_id,retweets, favourate, retweets+favourate as SUM from tweet_details limit 15  " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['tweet_id','Retweets','favourate','SUM'])
review = pd.read_excel('Product Reviews.xlsx') $ review
autos['last_seen'] = autos['last_seen'].str[:10] $ autos['last_seen'].value_counts(normalize=True, dropna=False).sort_index()
lr_params = {'penalty':['l1', 'l2'], $              'C': [2.3, 2.5, 2.6, 2.8,]} $ gs = GridSearchCV(LogisticRegression(), param_grid = lr_params) $ gs.fit(X_train_total, y_train) $ gs.best_score_, gs.best_params_
len(df2)
df_archive_clean = df_archive.copy()
conn_str = ( $     r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};' $     r'DBQ=C:\Users\Daniel.Aragon\Desktop\DJA_TEMP\FortCollins\COFC Bridge Mgmnt DB_MASTER_1.0_2018.accdb;' $     r'UID=FCADMIN;' $     r'PWD=password;')
projects
token_sendReceiveAvg_month = token_sendReceiveAvg_month.apply(lambda x:round(x,2))
pnew=df2['converted'].mean() $ print(pnew)
lst_all_date = [pd.to_datetime(x).date() for x in pd.date_range(start, end, freq=pd.tseries.offsets.BDay())] $ lst_all_date $
index_ts = index_ts[['price_date', 'ticker', 'adj_close_price']]
train_set.topic.value_counts()
datatest.loc[datatest.expenses >50000,'expenses'] = np.NaN
refl = hdf5_file[sitename]['Reflectance'] $ refl
z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new], [n_old,n_new],value=None, alternative='smaller', prop_var=False) $ print(z_score,p_value) $
fin_p = pd.read_excel('../../datasets/crypto-index-fund/data-financial/price_tradfinance.xlsx', $                       index_col=0, parse_dates=True)
len(used_color)
workspace_data.columns
ypred = lr.predict(X_test)
df_subset['Borough'] = df_subset['Borough'].astype(str) 
events_enriched_df = pd.merge(events_df[['event_id','group_id','yes_rsvp_count','waitlist_count','event_time']] $                               ,groups_topics_unique_df[['group_id','topic_name','topic_id']] $                               ,on ='group_id' , how='left' )
pd.DataFrame({('A', 'a1'): {('Y', 'y1'): 1, ('Y', 'y2'): 2}, $               ('A', 'a2'): {('Y', 'y1'): 3, ('Y', 'y2'): 4}, $               ('A', 'a3'): {('Z', 'z1'): 5, ('Z', 'z2'): 6}, $               ('B', 'b1'): {('Z', 'z1'): 7, ('Z', 'z2'): 8}, $               ('B', 'b2'): {('Z', 'z1'): 9, ('Z', 'z2'): 10}})
dtrain = DMatrix(X_tr[0:len(X_train)-40-1], label=y_ls) $ param = {'gamma':2.0,'nthread':8, 'max_depth':15, $          'eta':0.000000003, 'silent':1, 'objective':'multi:softprob', $          'eval_metric':'auc' ,'num_class':105}
model = LogisticRegression()
grouped = mean_df.groupby('Indicator').mean() $ grouped $
autos['offer_type'] = autos['offer_type'].replace('Angebot', 'offer')
years = df.groupby(pd.TimeGrouper(freq="AS")).count() $ years
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') #Merging 2 data frames with index as user_id $ df_new.head(10)
google_stock.head(8)
df2 = df2.drop_duplicates()
git_log['timestamp'].describe()
_delta=(np.abs(volt_prof_before[' pu1']-volt_prof_after[' pu1'])+ $         np.abs(volt_prof_before[' pu2']-volt_prof_after[' pu2'])+ $         np.abs(volt_prof_before[' pu3']-volt_prof_after[' pu3']))
a.shape
df.corr()
local.get_dataset(3)
1 / _
getFullScoreGraph(sentiment_by_month_df, sentiment_by_week_df, 'LiverpoolFC', ['2017-10-22'])
ticker = 'IBM'
num_missing = df.isnull().values.ravel().sum() $ print("Number of row with missing values - {}".format(num_missing))
from sklearn.metrics import jaccard_similarity_score $ jaccard_similarity_score(y_test, yhat)
df['Trip_duration']=df['Trip_duration'].astype('timedelta64[s]')
mw.to_csv('mw.csv',sep='~',index=False)
ab_page_index = df2[df2['group']=='treatment'].index
exog.info()
noise_data_2 = noise_data.join(hour) $ noise_data_2.head()
d=exp_convert - ctr_convert $ d
len(df)
convert_me = "2016bbb12---15" $ datetime.datetime.strptime(convert_me, "%Ybbb%m---%d")
Ralston.loc[:,"TMAXc"] = (5/9)*(Ralston.loc[:,"TMAX"] - 32) $ Ralston.loc[:,"TMINc"] = (5/9)*(Ralston.loc[:,"TMIN"] - 32)
perf_test = pd.read_csv('performance_test.csv') $ print('Testing data shape: ', perf_test.shape) $ perf_test.head(50)
train[ ['CompetitionOpenSinceYear','CompetitionDaysOpen','CompetitionMonthsOpen','CompetitionOpenSinceMonth']].sort_values('CompetitionOpenSinceYear')
Historical_Raw_Data.set_index("Date_Monday")["2017"].groupby([pd.TimeGrouper('M'),"Product_Motor"]).sum().unstack(level = 0)
x = df[ (df['group'] == 'treatment') & (df['landing_page'] != 'new_page')].shape[0] $ y= df[ (df['group'] != 'treatment') & (df['landing_page'] == 'new_page')].shape[0] $ x+y
user_info_df = pd.read_csv('source_data/user_info_lookup.csv', names=['user_id', 'name', 'organization', 'position', $                                             'gender', 'followers_count', 'following_count', 'tweet_count', $                                             'user_created_at', 'verified', 'protected'], $                           dtype={'user_id': str}).set_index(['user_id']) $ user_info_df.count()
manager = ImageManager('../tree_photos/', '../data/image_log_20180205.csv', '../data/image_syncs_20180205.csv')
df_B4
grid.best_params_
df.loc['EUR-CHF','Value']
'Best Params: ', rsskb_gbm.best_params_, 'Best Score: ', rsskb_gbm.best_score_
pipe.get_params().keys()
tweet_data_clean.head()
(df2['converted'] == 1).mean()
pickle_it(naive_bayes_classifier, 'naive_bayes_classifier') $ naive_bayes_classifier = open_jar('naive_bayes_classifier')
autos.unrepaired_damage.unique()
accuracy_valid = pipeline.predict(X_valid) $ accuracy = np.mean(np.array(accuracy_valid['PredictedLabel'].astype(int)) == y_valid) $ print('validation accuracy: ' + str(accuracy))
!ptdump -av 'data/my_pytables_file.h5'
df_clean2['Time'] = temp.time $ df_clean2['Hour'] = temp.hour
plt.figure(figsize=(16,8)) $ fb['2012':].resample('W').count()['message'].plot()
csvData.head(5)
import calendar $ month_names = [calendar.month_name[i] for i in range(1, 13)] $ events.date.dt.strftime("%B").value_counts().reindex(month_names).plot.bar(color = "steelblue") $ plt.title("Number of records in swimming set in each month");
df2[df2.group=='treatment']['converted'].mean()
sampled_authors_grouped_by_author_id_flattened.show()
print targetUserItemInt.shape $ targetUserItemInt.head()
def rmse(x,y): return math.sqrt(((x-y)**2).mean()) $ def print_score(m): $     res = [m.score(X_train, y_train), m.score(X_valid, y_valid)] $     if hasattr(m, 'oob_score_'): res.append(m.oob_score_) $     print(res)
trip_data = pd.read_csv("green_tripdata_2015-09.csv", header=0, delimiter=",")
df.to_csv("msft_temp2.txt", sep="|") $ ! head -5 msft_temp2.txt
(hdf['Age'].mean(level=1) $  .reset_index(name='AvgAge') $  .sort_values('AvgAge', ascending=False) $  .nsmallest(10, 'AvgAge'))
datetime_df = pd.DataFrame([now], columns=['Time'])
run txt2pdf.py -o "SOUTHCOAST HOSPITAL GROUP, INC  Sepsis.pdf"   "SOUTHCOAST HOSPITAL GROUP, INC  Sepsis.txt"
df.groupby(['product_type', 'state']).get_group(('Investment',1.0))
features = [col for col in train.columns if not col == 'Close'] $ X = train[features] $ y = train['Close'] $ etr = RandomForestRegressor() $ cross_val_score(etr, X, y).mean()
tweets['created_at'] = pd.to_datetime(tweets['created_at']) $ tweets.dtypes
stocks.index
t2.tail(10)
secclintondf = secclintondf[secclintondf.datetime>'2016-07-25 00:00:00']
df3 = df2[['andrew_id_hash', 'WT', 'HT', 'topic_id']] $ print(df3.info()) $ print(df3.head()) $ print(topics.head())
length_of_new = (df2[df2['landing_page'] == "new_page"].user_id.count()) $ length_of_new 
org_old_mean = df.query('group =="control"').converted.mean() $ org_new_mean = df.query('group =="treatment"').converted.mean() $ org_diff = org_new_mean - org_old_mean $ p_diffs = np.array(p_diffs) $ (p_diffs > org_diff).mean()
epoch3_df.shape
df[df.location_id>0].raw_location_text.value_counts().head()
from datetime import date $ today = str(date.today())
import re $ re.sub("Hi", "Hello", "Hi world")
len(pres_df['hour_aired'].value_counts()) # sanity check - should be 24
twitter_archive_with_json[twitter_archive_with_json.tweet_id == 835246439529840640][['tweet_id','rating_denominator']]
query_geographic = feature_layer.query(where='POP2010 > 1000000', out_sr='4326') $ query_geographic.features[0].geometry
df_1 = df.query('group == "treatment" and landing_page == "old_page"') $ df_2 = df.query('group == "control" and landing_page == "new_page"') $ df_3 = pd.concat([df_1, df_2]) $ df_3.shape[0] $
bad_content_size_df = base_df.filter(~ base_df['value'].rlike(r'\d+$')) $ bad_content_size_df.count()
len(apple.resample('M').mean())
grouper
((loans.originated_since_date<datetime.date(2015,2,28))).sum()
bnbAx.head()
cityID = '813a485b26b8dae2' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Albuquerque.append(tweet)  
f_ip_device_minute_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_device_minute_clicks"), header=True) $ print('Found %d observations.' %f_ip_device_minute_clicks.count())
from operator import add $ clients_with_crashes =\ $     crashes_misbehaving_clients.map(lambda p: (p.get('clientId'), 1)).reduceByKey(add).map(lambda p: p[0]).collect()
import pandas as pd
print len(data[data['ComplaintID'].notnull()])
import google.datalab.bigquery as bq $ def create_query(): $   return base_query $ query = create_query()
autos["odometer"].astype(int)
full['Age'].hist(bins=26,figsize=(12,4)) $ plt.title('Distribution of Age',size=15) $ plt.xlabel('yrs old') $ plt.ylabel('count')
first_result.contents[2]
preds, model = runXGB(train_X, train_y, test_X, num_rounds=400)
eval_RF_CT_tts = clf_RF_CT.score(X_testcvCT, y_testcvCT) $ print(eval_RF_CT_tts)
users = pd.read_sql_table('users', con=engine) $ customers = users[users.role == 0] $ cleaners = users[users.role == 1] $ leads = users[users.role == 3] $ applicants = users[users.role == 4]
df_archive["retweeted_status_timestamp"].unique()[0:10]
from matplotlib.pyplot import figure $ figure(num=None, figsize=(17, 4), dpi=80, facecolor='w', edgecolor='k') $ genders = list(gender_freq_hist.index) $ frequencies = list(gender_freq_hist.gender_freq) $ plt.bar(genders, frequencies)
try: $     'my string'.index('x') $ except Exception as error_message: $     print(error_message)
import pandas as pd $ import numpy as np $ import seaborn as sns $ import matplotlib.pyplot as plt $ %matplotlib inline
print(type(items))
pd.DataFrame(np.array([[10, 11], [20, 21]]))
g1 = df.groupby('end_station_name').count() #sorting by end station names $ g_sorted1 = g1.tripduration.sort_values(ascending=False) #arranging in descending order
candidates = pd.read_csv("https://calaccess.download/latest/Candidates.csv")
more_100_df = en_tweets_df[en_tweets_df['username'].isin(more_100)].copy()
park_statistic.to_csv('phillydata/Parking_Violation_Details.csv',index=False)
dframe_team['Draft_year'] = (dframe_team['start_year']+1)[(dframe_team['Start'] >= dframe_team['start_cut'])] # This gives the GM's first year of the draft as the year after he started if he started after July 1 of the previous year $ dframe_team['Draft_year'] = dframe_team['Draft_year'].fillna(dframe_team['start_year']) $ dframe_team.head()
count_non_null(geocoded_df, 'Judgment.Against')
tweets_df.info()
classifier = svm.SVC(kernel='linear') $ classifier.fit(X_train, y_train) $ y_prediction_SCM = classifier.predict(X_test) $
nx.info(tweetnet)
new_df = pd.DataFrame(df_final[['created_time', 'total_likes', 'total_comments']]) $ new_df.columns
resultJson = result.to_json(orient='records')
conditions_clean = conditions_m.str.lower().str.strip() $ conditions_clean
data[data['candidate_id'] == 2315074]
sample = pd.read_csv('../data/ebola/sl_data/2014-08-12-v77.csv')
conditions_m.value_counts(dropna=False)
import pickle $ pickle.dump(rf_reg,open('random_forest.sav','wb'))
quotes = yahoo_finance.download_quotes("GILD") $ print quotes
bb.plot(y='volume')
%matplotlib inline $ import seaborn as sns
deletes["YBP sub-account"].replace(195099, 590099, inplace= True) $ deletes
merged_NNN.head()
weeks = pd.date_range(start=datetime.date(2016,1,1), $                       end=datetime.datetime(2016,12,31), $                       freq='W') $ weeks
gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_
data = pd.merge( $     pd.merge(setup[['SetId', 'Street', 'station']], tubes[['SetId', 'mean']], on='SetId'), $     pd.DataFrame(dict(VMM=VMMmeans)), left_on='station', right_index=True) $ data
sites_no_net.head()
prob_treat_converted - prob_conrol_converted
for v in data.values(): $     if v['answers']['Q4'] == 'No': $         v['answers']['Q4A'] = 'n/a'
pickleobj('specs_eval_api', specs_eval_api)
subway4 = subway3_df.groupby(['STATION','LINENAME','datetime'])[['Hourly_Entries','Hourly_Exits']].sum()
contractor_clean[contractor_clean.city.isnull()]
tweet_df.iloc[:,15:].sample(5)
df['is_rank_1'] = df['Rank'] == 1
import matplotlib.pyplot as plt $ plt.plot(df['price'],'.') $ plt.show()
print(results_baseline.summary()) $ print(results_clusters.summary())
model.summary()
p_stats = pd.DataFrame() $ for i in range(1986, 2017): $     tmp = pd.read_csv("main_players_{}.csv".format(i)) $     tmp['season'] = i $     p_stats = p_stats.append(tmp)
MATTHEWKW = pd.DataFrame(MATTHEWKEYWORD) $ MATTHEWKW['date'] = MATTHEWKW.created_at.apply(lambda x: x.date()) $ print(len(df))
results.head()
cercanasA1_11_14Entre125Y150mts = cercanasA1_11_14.loc[(cercanasA1_11_14['surface_total_in_m2'] >= 125) & (cercanasA1_11_14['surface_total_in_m2'] < 150)] $ cercanasA1_11_14Entre125Y150mts.loc[:, 'Distancia a 1-11-14'] = cercanasA1_11_14Entre125Y150mts.apply(descripcionDistancia, axis = 1) $ cercanasA1_11_14Entre125Y150mts.loc[:, ['price', 'Distancia a 1-11-14']].groupby('Distancia a 1-11-14').agg(np.mean)
merged_df.reset_index('state', inplace=True)
students.sort_index(axis=1)
df_copy['date'] = df_copy['timestamp'].apply(lambda time: time.strftime('%m-%d-%Y')) $ df_copy['time'] = df_copy['timestamp'].apply(lambda time: time.strftime('%H:%M')) $ df_copy=df_copy.drop(columns=['timestamp'])
df = pd.read_csv('./ab_data.csv') $ df.head() $
df_pol_t = pd.concat([df_pol_matrix_df, df_pol_t.drop('title', axis=1)], axis=1) $
top20.to_csv("ratio.csv")
print(all_complaints['Descriptor'].nunique())
df3[['CA','UK','US']] = pd.get_dummies(df3['country']) $ df3 = df3.drop(df3['CA'])
final_xgb = xgb.train(best_params, dtrain, num_boost_round = num_boost)
df2 = df2.set_index('user_id') $ df2.head()
print(df['score'].mean())
weather_all.head(3)
betas_mask = np.zeros(shape=(mcmc_iters, n_bandits)) $ betas_mask[np.arange(mcmc_iters), betas_argmax] = 1
happiness_df=pd.DataFrame(columns=['dates','happiness']) $ for j in range(0,len(happiness)): $             happiness_df.loc[j]=[str(time[j])+"-12-31"+"T00:00:00Z",happiness[j]*33.33] $
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, func, distinct
df['gathering'].fillna(0, inplace = True)
dist = np.sum(train_data_features, axis=0) $ print (dist) $ for tag, count in zip(vocab, dist): $     print (count, tag) $
trainFIC = train $ y = trainFIC.Purchase $ x = trainFIC.drop(['User_ID', 'Product_ID', 'City_Category', 'Purchase'], axis=1)
plt.hist(p_diffs) $ plt.xlabel('p_diffs') $ plt.title('Histogram for p_diffs')
building_pa_prc_shrink[building_pa_prc_shrink.columns[0:5]].head(10)
lr.fit(X_train, y_train) $ lr.score(X_test, y_test)
model = sm.OLS.from_formula('arrests ~ gal_eth_21', stadium) $ results = model.fit() $ print(results.summary())
unique_top_tracks = df_track.merge(df_artist, on = ['artist_id','artist_name']).sort_values( $     by = 'artist_followers', $     ascending = False)[['track_name','artist_name','artist_followers']].drop_duplicates(subset = 'artist_name', $                                                                                        keep = 'first').head(5) $ unique_top_tracks
finaldf.to_csv('irl_votes.csv')
%bash $ gsutil ls -l gs://$BUCKET/taxifare/ch4/taxi_preproc/
data=pd.read_sql_query("select date,prcp from measurement where date >'2016-08-22';",engine) $ data
d = 30 $ df['date_after_30d'] = df['datetime'].apply(lambda x: x + pd.Timedelta(d, unit='d')) $ print(df.to_string())
df_sched2.insert(loc=0, column='ProjectId', value=df_sched['ProjectId'])
engine.execute('SELECT *FROM measurement LIMIT 20').fetchall()
full_p_old = df2.converted[df2.group == 'control'].mean() $ full_p_old
typesub2017['MTU2'] = typesub2017.MTU.str[:16] $ typesub2017['DateTime'] = pd.to_datetime(typesub2017['MTU2'])
len(finnal_data["event"].unique())
df.describe()
props.info()
df2[df2['group']=='treatment']['converted'].mean()  #Probability of the users in control group who coonverted
new_dems.newDate.head()
s = pd.Series([89.2, 76.4, 98.2, 75.9], index=list('abcd')) $ 'b' in s
df2['tripduration'].describe()
autos['datecrawled'].str[:10].value_counts(dropna=False).sort_index()
z_value,p_value=sm.stats.proportions_ztest([convert_new,convert_old],[n_new,n_old],alternative='larger') $ print(z_value,p_value)
contribs = pd.read_csv('http://www.firstpythonnotebook.org/_static/contributions.csv')
scn_genesis[0]
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True) $ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True) $ df.info()
QUIDS = QUIDS.loc[(QUIDS['week'] == 0) |(QUIDS['week'] == 12)  |(QUIDS['week'] == 14)] $ QUIDS = QUIDS[["subjectkey", "qstot", "week"]].sort_values(['subjectkey', 'week'])
df_twitter_archive.name.isnull().sum()
len(speakers)
df_train = df_train.drop_duplicates(subset='features') $ display(df_train.describe())
df.info()
kick_projects_ip_copy= kick_projects_ip.copy()
hpd['Complaint Type'].value_counts().head(5)
xmlData['statezip'] = xmlData[['state', 'zipcode']].apply(lambda x: ''.join(x), axis = 1) $ xmlData.drop('state', axis = 1, inplace = True) $ xmlData.drop('zipcode', axis = 1, inplace = True)
print(measures * 2)
!g++ test_cpp_cuda_codes/hello-world.c $ !./a.out
dforders = ml4t.build_orders(dfprediction, abs_threshold=0.01, startin=False, symbol='USD-BTC') $ dforders
nold = len(df2.query("group == 'control'")) $ print(nold)
df_users = df_users.dropna(axis=0) $ print(df_users.shape) $ df_users.head(10)
dr = pd.to_datetime(pd.to_datetime(merged_df.index.date)) $ cal = calendar() $ holidays = cal.holidays(start=dr.min(), end=dr.max()) $ merged_df['holiday'] = dr.isin(holidays)
tmp_cov = cov_df.loc[i].copy() $ tmp_cov
res = sts.query(qry)
from datetime import datetime $ from datetime import timedelta
df['sp_close_chg'] = sp.close_chg $ df['sp_open_chg'] = sp.open_chg $ df['sp_high_chg'] = sp.high_chg $ df['sp_low_chg'] = sp.low_chg $ df['sp_vol_chg'] = sp.vol_chg
weather_x = weather_norm.drop('tavg', axis=1) $ weather_y = weather_norm['tavg'].shift(-1)
pd.Period('1/2016')
crime_tweek_tweek_window, stops_two_week_window = stops_vs_crime(zip_1_df,zip_1_sns,'14D','14D') $ crime_week_tweek_window, stops_two_week_window = stops_vs_crime(zip_1_df,zip_1_sns,'7D','14D') $ crime_day_tweek_window, stops_two_week_window = stops_vs_crime(zip_1_df,zip_1_sns,'D','14D')
probab_converted = df2['converted'].mean() $ print('Probability of an individual converted:{}'.format(probab_converted))
df.columns
eval_vot_tf_tts = clf_vot_tf.score(X_testcv_tf, y_testcv_tf) $ print(eval_vot_tf_tts)
reddit_df.columns
>ljan1=pd.read_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_1/observed_field.cat',names=['name']) $ matched_id=[np.where((final0['name']==i)==True)[0][0] for i in jan1['name']] $ final1=final0.drop(matched_id) $ final1.head() $ print final0.shape, final1.shape
import urllib3, requests, json $ headers = urllib3.util.make_headers(basic_auth='{username}:{password}'.format(username=wml_credentials['username'], password=wml_credentials['password'])) $ url = '{}/v3/identity/token'.format(wml_credentials['url']) $ response = requests.get(url, headers=headers) $ mltoken = json.loads(response.text).get('token')
df.date[0]
print("%s" % dataset.description) $ print("%d documents" % len(dataset.data)) $ print("%d categories" % len(dataset.target_names))
pvt['customerId'] = pvt['ga:dimension2'].str.rpartition('-')[0].str.strip() $ pvt['customerName'] = pvt['ga:dimension2'].str.rpartition('-')[2].str.strip() $ pvt
scores = cross_validate(lr2, X_new, y_tfidf, cv=10, $                         scoring=['accuracy'], $                         return_train_score=True $                        ) $ scores # the results are pretty much the same as before
shows.head()
first_result.find('strong').text
tokendata.sendReceiveAvg = tokendata.sendReceiveCntAvg_mon.map(lambda x: round(x,2))
clean_appt_df.groupby('Neighbourhood')['No-show']\ $              .value_counts(normalize=True)\ $              .loc[:,'No']\ $              .plot.hist() $
logit_pageCountry = sm.Logit(merged['converted'],merged[['ab_page', 'intercept', 'UK', 'US']]) $ result_pageCountry = logit_pageCountry.fit()
pd.NaT
kick_projects = pd.merge(kick_projects, ks_particpants, on = ['category', 'launched_year', 'launched_quarter','goal_cat_perc'], how = 'left')
a = np.array([1, 2, 3]); b = np.array([4, 5, 6])
people.eval("body_mass_index = weight / (height/100) ** 2", inplace=True) $ people
df['num_payouts'].nunique()
experiment_df = pd.read_csv('multi_arm_bandit_example_distrib.csv') $ experiment_df.drop(['Unnamed: 0'], axis=1, inplace=True) $ experiment_df.head(2)
arparams = np.r_[1, -arparams] $ maparams = np.r_[1, maparams] $ nobs = 250 $ y = arma_generate_sample(arparams, maparams, nobs)
train.first_affiliate_tracked.isnull().sum()
autos['no_of_pictures'].value_counts()
to_be_predicted_Day4 = 38.62500475 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
sub_df = pd.DataFrame({"click_id":test.click_id ,"is_attributed":preds})
top_brands = autos["brand"].value_counts(normalize=True).head(20).index $ top_brands
result = json_normalize(engagement) $ result.head()
df_small = df_small[df_small['dog_value'] != 'None']
metrics, predictions = pipeline.test(X_eval, y_eval, output_scores=True) $ print("Performance metrics on test set: ") $ display(metrics)
Lab7 = Lab7.drop(Lab7.index[[0]])
to_be_predicted_Day4 = 22.24313147 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
print(tweet1.text)
pop.unstack(level=1)
val sorted_requests = requests $     .map(pair => pair.swap) $     .transform(rdd => rdd.sortByKey(false))
twitter_archive_df_clean[~pd.isnull(twitter_archive_df_clean['retweeted_status_id'])].head()
result = pd.merge(result2,result3, how='inner', on=['id_ndaj1']) $ result.shape
new_reps.shape[0]
from bs4 import BeautifulSoup $ import urllib.request
df2 = df.drop(index_of_wrong_mapped_cols, axis=0)
train.sample(5)
pred_probas_under_fm = gs_from_model_under.predict_proba(X_test) $ fm_bet_under = [x[1] > .62 for x in pred_probas_under_fm]
import pandas $ import numpy as np $ %matplotlib inline
pd.Series({'square of 1':1, 'square of 2':4, 'square of 3':9, 'square of 4':16, 'square of 5':25})
rng_utc = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz=dateutil.tz.tzutc())
words_hash_sk = [term for term in words_sk if term.startswith('#')] $ corpus_tweets_streamed_keyword.append(('hashtags', len(words_hash_sk))) # update corpus comparison $ print('List and total number of hashtags: ', len(words_hash_sk)) #, set(terms_hash_stream))
ax1 = sns.scatterplot(subs, comments, hue=senti, ); $ ax1.set_ylabel('Comment Count'); $ ax1.set_xlabel('Subreddit Sub Count');
prices = dict_prices()
T[T.notnull()].head(10)
bp = USvideos.boxplot(column='like_ratio', by='category_name', vert = False) $
plt.plot(df[base_col].rolling(window=12).std(),color='green',lw=2.0,alpha=0.4) $ plt.plot(df[base_col].rolling(window=3).std(),color='purple',lw=0.75) $ plt.show;
a.find(':')
accuracies.mean()
from config import * $ auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(app_key, app_secret) $ api = tweepy.API(auth)
pos_bow = [pos_dic.doc2bow(review) for review in pos_review_tokens] $ neg_bow = [neg_dic.doc2bow(review) for review in neg_review_tokens] $ pos_tokens = pd.DataFrame(list(pos_dic.token2id.items()), columns=['token','id']) $ pos_dt = pd.DataFrame(pos_bow[0], columns=['id','freq']) $ pos_dt.merge(pos_tokens)
from datetime import datetime as dt $ datAll['Date'] = pd.to_datetime(datAll['Date']) $ datAll = datAll[(datAll['Date']>='2010-01-01') & $                 (datAll['Date']<='2018-01-01')]
df[df['converted']==1]['converted'].count()/df['user_id'].count()
from sklearn.ensemble import RandomForestClassifier $ rf = RandomForestClassifier(n_estimators = 10, random_state = 42) $ rf.fit(X_train,y_train)
rfe.fit(x_train,y_train) $ rfe.score(x_test,y_test)
merged[merged.contributor_state=="CA"].amount.sum() / merged.amount.sum()
look_back_dt = dt.datetime.strptime(valid_start_dt, '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=T-1) $ valid = energy.copy()[(energy.index >=look_back_dt) & (energy.index < test_start_dt)][['load']] $ valid.head()
odometer.unique().shape
total = df2.nunique() $ pct_conv = df2.query('converted == 1').user_id.nunique()/total $ print(pct_conv)
autos['last_seen'].str[:10].value_counts(normalize=True, dropna=False).sort_index() * 100
feature_cols = ['TV', 'radio'] $ X = data[feature_cols] $ print(np.sqrt(-cross_val_score(lm, X, y, cv=10, scoring='mean_squared_error')).mean())
data_year=data_nonan_temp.groupby(['Year']) $ data_year['Temperature(C)'].mean().plot()
twitter_archive_df_clean.head(3)
df_main.p2.head(3)
n_old = df2.query('landing_page == "old_page" and group == "control"').user_id.nunique()
df32.count().mean() / df1.count().mean()
poiModel.summary()
gatecount_station_line_gameless = gatecount_station_line[gatecount_station_line.service_day.isin(all_game_dates) == False]
trans.groupby('msno').count().sort_values('is_cancel').tail()
n_old = df2['group'].value_counts()[1] $ n_old $
df2.drop_duplicates('user_id', inplace=True)
df_daily = df.groupby(["C/A", "UNIT", "SCP", "STATION", "DATE"]).ENTRIES.first().reset_index() $ df_daily.head(5)
%sql \ $ SELECT twitter.tweet_text FROM twitter \ $ WHERE twitter.tweet_text LIKE "%Roger Federer%";
nvidia = cached.filter(lambda p: p["adapter"]["vendorID"] == '0x10de') $ nvidia.map(lambda p: "mobile" if devices[int(p['adapter']['deviceID'], 16)].endswith("M") else "desktop").countByValue()
print (pd.concat([df1,df2]))
tfav_k2.plot(figsize=(16,4), label="Likes", legend=True) $ tret_k2.plot(figsize=(16,4), label="Retweets", legend=True);
retweet_df = base_retweet_df.join(user_summary_df['gender'], on='user_id') $ retweet_df.count()
pd.options.display.max_colwidth = 280 $ for _, row in cleanedData.iterrows(): $     row.text = ftfy.fix_text(row.text) $ irows = cleanedData['text'].str.contains("&amp;") $ cleanedData.loc[irows, 'text'] = cleanedData.loc[irows, 'text'].str.replace('&amp;', '&')
biased_train_15.to_csv('biased_train_15.csv',sep=',') $ biased_train_33.to_csv('biased_train_33.csv',sep=',') $ biased_train_60.to_csv('biased_train_60.csv',sep=',') $ biased_train_05.to_csv('biased_train_05.csv',sep=',')
january_jail_census.head()
cat_start = 1 $ cat_end = 2 $ category_filter = cats.iloc[cat_start:cat_end + 1, 0].values.tolist() $ category_filter
df_data.isnull().sum()
df_words
type(rng.asi8)
from sklearn.linear_model import LogisticRegression $ model = LogisticRegression() $ model = model.fit(X, y) $ model.score(X, y)
df['text_no_urls'][55]
import json $
actual_diff = df2[df2['group'] == "treatment"]['converted'].mean() - (df[df['group'] == "control"]['converted'].mean()) $ p_diffs= np.array(p_diffs) $ (p_diffs > actual_diff).mean()
plt.scatter(df['ticket_price'], df['percentage_seats_sold']) $ plt.xlabel('price') $ plt.ylabel('percentage of seats sold') $ plt.title('2016') $ plt.show()
from numpy import pi $ f = np.linspace(0,pi,100) $ sinf=np.sin(f) $ print("f: ", f)
%load "solutions/sol_2_9.py"
class Example: $         self.instance_var = 'this is an instance var' $     def class_method(): $
vals2.sum(), vals2.min(), vals2.max()  # runtime warnings, not exceptions
QUIDS = QUIDS[QUIDS["level"]=="Level 1"]
td_amb = st_streams['/streams/amb'].index.values[-1] - st_streams['/streams/amb'].index.values[0] $ td_door = st_streams['/streams/door'].index.values[-1] - st_streams['/streams/door'].index.values[0] $ td_mcu = st_streams['/streams/mcu'].index.values[-1] - st_streams['/streams/mcu'].index.values[0] $ td_relay = st_streams['/streams/relay'].index.values[-1] - st_streams['/streams/relay'].index.values[0] $ td_temp = st_streams['/streams/temp'].index.values[-1] - st_streams['/streams/temp'].index.values[0]
model = load_model('wikigrader/data/nn_model.hdf5', custom_objects={'r2': utils.r2})
csv_file = "ab_data.csv" $ df2 = pd.read_csv(csv_file) $ df2.head()
df.index = df['created_date'] $ df.head(2)
basicmodel = LogisticRegression() $ basicmodel = basicmodel.fit(basictrain, train["rise_in_next_week"])
n_new = df2[df2['landing_page'] == 'new_page'].shape[0] $ n_new
classification_data = classification_data[cols].copy()
p_old = round(df2['converted'].mean(),4) $ print(p_old)
n_old = ab_df2.query('landing_page == "old_page"').shape[0] $ n_old
%ls
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(classifier.score(X_test,y_test )))  ### using logistic.score method
df_input = pd.read_csv('pydata_vw_tweets.csv') $ display(df_input.head()) $ display(df_input.tail()) $ display(df_input.describe())
df_sched[df_sched.Initiation < 0 ]
(df2.converted == 1).sum()/len(df2)
young.join(logs, logs.userId == users.userId, "left_outer")
claims_std_dev = utility_patents_df.number_of_claims.std() $ claims_median = utility_patents_df.number_of_claims.median() $ utility_patents_subset_df = utility_patents_df[utility_patents_df.number_of_claims <= (claims_median + 3*claims_std_dev)].copy()
cityID = '018929347840059e' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Indianapolis.append(tweet) 
df2=pd.read_csv("https://s3.amazonaws.com/tripdata/201707-citibike-tripdata.csv.zip")  #July 2017
standardize_column_names(df)
X_mice.head()
ts = pd.DataFrame(np.random.randn(758, 4), columns=list('ABCD'), index=dates)
dfSummary[4:9].plot(kind='bar', $                     figsize=(20,6), $                     title="Data Summaries: Quantiles");
accuracy(y_test,lr_pred)
s.str.isnumeric()
df2.shape[0] $ print ("Total Number of row : {}".format(df2.shape[0]))
Obama = pd.read_csv(r"C:\users\kevin\Obama.csv") $ Obama.head()
extract_deduped_with_elms.shape
fig, ax = plt.subplots(figsize = (20,15)) $ plt.hist(np.log(df.num_comments),bins = 100,range = (0,25));
c.insert_one(bowie)
assert np.mean(mean_rw_history[-10:]) > 10. $ print("That's good enough for tutorial.")
for i in sorted(d, reverse=True): $     print(i)
Project = Project[['Date', 'Contract Value (Daily)']] $ Project = Project.groupby(['Date'])[['Contract Value (Daily)']].sum().reset_index()
shows['fixed_runtime'] = shows['runtime'].dropna().apply(fix_runtime)
user_summary_df = user_summary_df[user_summary_df.tweets_in_dataset != 0] $ user_summary_df.count()
df1[df1['dev_state']=='.'][['io_state','dev_state']]
import os $ import sys $ module_path = os.path.abspath(os.path.join('..')) $ if module_path not in sys.path: $     sys.path.append(module_path)
df3.fillna(0, inplace=True) $ df3.head() $ df3.isnull().sum()
joined_store_stuff = counted_store_events.join(stores, counted_store_events.store_id == stores.id)
data = check_y(df, delta_change=-3.0, start = 560, end = 600) 
plt.scatter(df_joy['Polarity'], df_joy['Subjectivity'], alpha=0.1, color='purple') $ plt.title('Tweet #joy, Subjectivity on Polarity') $ plt.ylabel('Subjectivity') $ plt.xlabel('Polarity') $ plt.show()
week40 = week39.rename(columns={280:'280'}) $ stocks = stocks.rename(columns={'Week 39':'Week 40','273':'280'}) $ week40 = pd.merge(stocks,week40,on=['280','Tickers']) $ week40.drop_duplicates(subset='Link',inplace=True)
df_clean.info()
knn.fit(train[['property_type', 'lat', 'lon','surface_covered_in_m2','surface_total_in_m2','floor','rooms']], train['expenses'])
city_names = pd.Series(['San Francisco', 'San Jose', 'Sacramento']) $ population = pd.Series([852469, 1015785, 485199]) $ pd.DataFrame({ 'City name': city_names, 'Population': population })
Image(url='https://tf-curricula-prod.s3.amazonaws.com/curricula/b04b6f653b9d364d5612ac767527458a/DATA-001/v2/assets2/1.1.3_Get_To_Know_Pythons_Data_Types/Data_types_2.png')
reddit.Upvotes.value_counts(ascending=False).head(25) #just seeing the number of upvotes for each Reddit post $
posts.sample(100)
pd.date_range('2017-01', '2017-12', freq='M')  # This gives us 12 dates, one for each month, on the last day of each month
closingPrices.max()
linkNYC['days'] = (datetime.datetime.now().date()-linkNYC.days).apply(lambda x: x.days)
obs_diff = convereted_rate_new - convereted_rate_old $ print(obs_diff)
cityID = '0c2e6999105f8070' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Anaheim.append(tweet)  
n_old = len(df2.loc[~new_page]) $ n_old
from sklearn.metrics import classification_report $ print(classification_report(y_test,y_pred))
x = search.cv_results_["param_n_neighbors"].data $ score = search.cv_results_["mean_test_score"] $ yerr = search.cv_results_["std_test_score"] $ fig, ax = plt.subplots(figsize=(12,8)) $ ax.errorbar(x, score, yerr = yerr);
df_onc_no_metac[ls_other_columns] = df_onc_no_metac[ls_other_columns].applymap(clean_string)
fin_df[fin_df['symbol']=='AAPL']
%load_ext version_information $ %version_information
resampled1_groups = resampled_groups.reset_index()
s.str.cat(sep='_')
for name, group in grouped: $     print(name) $     print(group) $     print()
trump.head()
data = data[data['Borough'] != 'Unspecified']
random.randint(0,9)
df_new['intercept'] = 1 $ logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'uk', 'us']]) $ results = logit_mod.fit() $ results.summary()
pold=df2['converted'].mean() $ pold
twitter_archive_master['rating_denominator'].value_counts()
country_dummies = pd.get_dummies(df_countries['country']) $ df_new = df_countries.join(country_dummies)
import matplotlib.pyplot as plt $ import pylab as plt $ import seaborn as sb $ from pylab import rcParams
print('Total number of manually extracted contradicted tweet pairs is:  ',cotradicted_pairs.shape[0], 'pairs')
df_clean.rating_denominator.value_counts()
twitter_master2.head(2)
cust.iloc[:8, 6:]
df = pd.read_csv('Water_Quality_complaints.csv') $
os.chdir(Base_Directory) $ print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $ a,summary_of_DRG_that_year = show_everything(303,2015,True) $ a.head()
import pandas as pd $ import matplotlib.pyplot as plt $ import numpy as np $ %pylab inline $ df = pd.read_csv('/Users/doyelm/Documents/Project/listings.csv') $
df_genres = df.groupby("genre") $ print(df_genres) $ print(df_genres['score'].mean().sort_values(ascending=False))
columnsTitles=["UserID","User","Gender","Registered","Cancelled","TransactionDate","TransactionID","ProductID","Quantity"] $ hist=hist.reindex(columns=columnsTitles)
display_code('models.py',[246,261])
df_clean['name'].value_counts()
prop_conv = df['converted'].mean() $ output = round(prop_conv, 4) $ print("The proportion of Converted users is {}%".format(output*100))
data.groupby(['Year'])['Salary'].mean()
y = df['loan_status'].values $ y[0:5]
people.sort_index(ascending=False)
df_new['day'] = np.where(pd.to_datetime(df_new['timestamp']).dt.dayofweek >= 5, 'weekend', 'weekday')
autos["price"].describe()
N = 500 $ df = pd.DataFrame() $ df['W'] = norm.rvs(size=N)
extract_all.loc[(extract_all.APP_SSN==149640712)]
hp.save('new_houseprint.pkl')
import pandas as pd
rows_with_missing_response = loan_stats['loan_status'].isna() $ no_missing_values_response = rows_with_missing_response.logical_negation()
tmp_df[tmp_df.columns[tmp_df.dtypes == 'category']] = tmp_df[tmp_df.columns[tmp_df.dtypes == 'category']].astype(str) $ tmp_df['Judgment.In.Favor.Of.Plaintiff'] = tmp_df['Judgment.In.Favor.Of.Plaintiff'].astype(int) $ tmp_df['Case.Duration'] = tmp_df['Case.Duration'].dt.days
%%capture $ weather_mean['Rel Hum (%)', 'Temp (deg C)']
df_clean.loc[:, 'rating_numerator'] = df_clean['rating_numerator'].astype(float)
tweets_original.head()
np.histogram(noaa_data.loc["2018-05-29":"2018-05-29","AIR_TEMPERATURE"])
clf = LinearRegression() $ clf.fit(X_train, y_train) $ confidence  = clf.score(X_test, y_test) $ print("Confidence our Linear Regression classifier is: ", confidence)
learn.unfreeze()
df4
len(lst), len(df)
from scripts.processing import *
from collections import Counter $ c = Counter([int(stats.coleman_liau(x['cdescr'])) for x in df.to_dict(orient='records')]) $
blink= condition_df.get_condition_df(data=(etsamples,etmsgs,etevents),condition="BLINK")
print(soup.prettify())
training.StateHoliday.replace(to_replace='0',value=0,inplace=True) $ testing.StateHoliday.replace(to_replace='0',value=0,inplace=True)
dfData.describe()
df.replace({'appeal': {'0': ''}}, inplace=True) $ df.appeal.fillna('', inplace=True) $ df.fund.fillna('', inplace=True)
selected_beats = ['10H40','10H60','10H70','10H10' ]
lr2 = LogisticRegression() $ %time lr2 = lr2.fit(train_4_reduced, y_train) $ %time y_hat_lr2 = lr2.predict(train_4_reduced) $ print(roc_auc_score(y_train, y_hat_lr2)) $ print(log_loss(y_train, y_hat_lr2))
gearbox_list = list(set(train_data.gearbox))
import nltk $ nltk.download()  # Download text data sets, including stop words
volt_prof_before['Bus']=volt_prof_before['Bus'].apply(lambda x:x.lower()) $ volt_prof_after['Bus'] =volt_prof_after['Bus'].apply(lambda x:x.lower())
events.groupBy("event_type").count().toPandas().head(50).plot(kind="bar")
crimes.drop(['Ward', 'Community Area'], axis=1, inplace=True) $ crimes.dropna(inplace=True)
from splinter import Browser $ from bs4 import BeautifulSoup
import os $ from dotenv import load_dotenv
cm_knn = confusion_matrix(y_test, y_knn_predicted)
run txt2pdf.py -o"2018-06-19 2015 UNIVERSITY OF MICHIGAN HEALTH SYSTEM Sorted by discharges.pdf"  "2018-06-19 2015 UNIVERSITY OF MICHIGAN HEALTH SYSTEM Sorted by discharges.txt"
from src.image_manager import ImageManager $
import os.path $ import pandas as pd $ import numpy as np $ from scipy import stats
df.head()
offset1 = timedelta(hours=1) $ offset2 = timedelta(days=3) $ offset3 = timedelta(days= ((365*12)+3), hours=1, seconds=43) $ df_goog.index + offset3
df2[(df2['landing_page']=='old_page')].count()[0]
df.plot.scatter(x='B', y='C', title = 'Scatterplot', color='r')
df_transactions.head()
temp1 = np.array([ $         ['', '', ''], $         ['', '', ''], $         ['1', '2', '3'] $     ], str)
df2.shape[0] $ print("Number of rows in df2 : {}".format(df2.shape[0]))
commits = EQCC(git_index) $ commits.since(start=start_date).until(end=end_date) $ commits.get_cardinality("hash").by_period() $ print(pd.DataFrame(commits.get_ts()))
targetUserItemInt=targetUserItemInt.join(userData.set_index(['id'])['premium'],on='user_id',how='left') $ print targetUserItemInt.shape $ targetUserItemInt.head()
df[2]
overlapping_trip_passenger_ids = trips_sorted_passenger[trips_sorted_passenger["overlap"] == 1]["id"].values $ df_trips = df_trips.drop(overlapping_trip_passenger_ids)
list = [1,2,3,4,5] $ print("Original data type: %s" % (type(list)))
test_sentence = test_sentence.replace(re.compile(r"http.?://[^\s]+[\s]?")) $ test_sentence[0]
referred_users = clean_users[clean_users['invited_by_user_id'] > 0] $ unreferred_users = clean_users[clean_users['invited_by_user_id'].isnull()]
df_group.sort_values(['Keyword'], ascending=False)
df2['intercept'] = 1 $ df2[['old_page', 'new_page']] = pd.get_dummies(df2['landing_page']) $ df2.head() # I needed two passages to create the ab_page, although I see it's just what I labeled old_page
get_items_purchased('alpa.poddar@gmail.com', product_train, customers_arr, products_arr, item_lookup)
retweets.head(1)
pd.date_range('2015-07-03', periods=9)
df_new['country_US'] = df_new['country'].replace(('US', 'UK', 'CA'), (1, 0, 0)) $ lm_us = sm.OLS(df_new['converted'], df_new[['intercept', 'country_US']]) $ results_us = lm_us.fit() $ results_us.summary()
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new.country)
bus['text'].iloc[0]
df_new['CA_ab_page'] = df_new['CA']*df_new['ab_page'] $ df_new['UK_ab_page'] = df_new['UK']*df_new['ab_page'] $ df_new['US_ab_page'] = df_new['US']*df_new['ab_page'] $ df_new.head(1)
api_df.head()
crimes['2011-06-15']['TYPE'].value_counts().head(5)
raw_data = '/s3/three-word-weather/hack/3ww-all-raw.csv'
site_vals = odm2rest_request('datasetvalues', {'datasetUUID': siteds_dct['DataSetUUID']})
df[0].plot()
proj_df.shape
print "Fit booths are: ", set(bthlst) ^ set(df_bug[u'Service Location'].unique())
ddp = dfd.dropna(axis=0, subset=['in_reply_to_screen_name'], how='any')
appleinbounds = appleNegs[appleNegs.inbound == True]
twitter_dataset.info()
measure.tail()
df = pd.read_csv('https://query.data.world/s/kw3bkr2haxalgzcit3snux4yrtqmtf')
y_pred = nb_pred[:,0] $ print('precision: {:.2f}\nrecall: {:.2f}\naccuracy: {:.2f}'.format(precision_score(y_test,y_pred), $                                                        recall_score(y_test,y_pred), $                                                        accuracy(y_test,y_pred)))
unique_animals = set(["Dog", "Cat", "Hippo", "Dog", "Cat", "Dog", "Dog", "Cat"]) $ print(unique_animals)
new_p = df2['converted'].mean() $ new_p
engine.execute('SELECT * FROM Station LIMIT 5').fetchall()
response.status_code, response.url
old_page_converted = np.random.binomial(n_old,p_old) $ print('The old_page_converted is: {}.'.format(old_page_converted))
data["Improvements_raw"] = data["Improvements"]
df
def to_array_variabel(df, shape = (100,100)): $     to_array = pd.factorize(df)[0] $     var_array = np.reshape(to_array, shape) $     return var_array
measurements_df=precipitation_data()
df_gnis = df_gnis.replace(np.nan, '', regex=True) $ geometry = [Point(xy) for xy in zip(df_gnis.PRIM_LAT_DEC, df_gnis.PRIM_LONG_DEC)] $ gdf_gnis = gpd.GeoDataFrame(df_gnis, crs={'init':'epsg:4269'}, geometry=geometry)
df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
confusion_matrix(data['zika_bool'], data['zika_pred'])
%timeit StockData = pd.read_csv(StockDataFile, index_col=['Date'], parse_dates=['Date'])
InfinityWars_Predictions.head()
X1 = pd.DataFrame(X[['Loan_Amount','Property_Value']])
ACCESS_TOKEN        = 'Nothing' $ ACCESS_TOKEN_SECRET = 'to' $ CONSUMER_KEY        = 'see' $ CONSUMER_SECRET     = 'here.'
before.head()
twitter_archive = pd.read_csv('twitter-archive-enhanced.csv')
df['name']
for key,value in trends_per_year_avg_ord.items(): $     print( "Trend avg in " + str(key) + " = " + str(value))
cur.execute("SELECT name FROM sqlite_master WHERE type='table';") $ print(cur.fetchall())
text = 'My cat is a great cat' $ tokens = text.lower().split() $ print('Words in the our text:', tokens)
!ls
rate["answer"] = pd.to_numeric(rate["answer"]) # Converting the answer Series to numeric
gb = GradientBoostingClassifier(n_estimators=10) $ gb.fit(train_data_features, y_train) $ y_fit_proba = gb.predict_proba(test_data_features)
y_pred=knn3.predict(X_test)
print(df_users.shape) $ df_users.isnull().sum()
X = pd.merge(X, menu_about_latent_features, left_on='master_menu_id', right_on='menu_id', how='left') $ del X['menu_id']
train_sample = pd.read_csv("train_sample.csv") $ train_sample.head()
other = [1375, 10225] $ mask = df['other'].isin(other) $ df.loc[(~mask) & (df.company == True) & (df.other == True), 'other'] = False
FAFRPOSRequest = requests.get(FAFRPOS_pdf, stream=True) $ print(FAFRPOSRequest.text[:1000])
X = aux[['intercept', 'ab_page', 'number_of_days']] $ y = aux['converted'] $ lm = sm.Logit(y,X) $ results = lm.fit() $ results.summary()
Test.FlowVals
url = form_url(f'actionTypes/{baseball_swing_action_type_id}/metricTypes', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_status(response)
data["type"].unique()
data['month'] = ['Jan']*len(data) $ data
qty_recorded_in_1c_and_store
util.get_rebalance_date(start_idx, end_idx, 'month')
n_new = df2.query('group == "treatment"')['user_id'].count() $ n_new
times = pd.DatetimeIndex(data.datetime_col) $ grouped = df.groupby([times.hour, times.minute])
df.nlargest(1,'rating')
unique_tweets = find_unique_tweets_crawled() $ print(unique_tweets.head()) $ config.dump_tweets_dataframe(unique_tweets) $ unique_tweets = config.load_tweets_dataframe()
train2 = dfs['Close'] 
df2['ab_page_and_UK'] = df2['ab_page']*df2['UK'] $ df2['ab_page_and_CA'] = df2['ab_page']*df2['CA'] $ df2.head(10)
df2.to_csv('clean_data_df2.csv', index=False)
np.unique(val_small_sample.click_timeDay), np.unique(val_small_sample.click_timeHour)
dire = os.getcwd() $ tr = pd.read_csv(dire+'/train.csv',header = 0,encoding="utf-8") $ re = pd.read_csv(dire+'/resources.csv',header = 0,encoding="utf-8")
shows.isnull().sum()
full_orig = full.copy()
s[criteria].head()
%%time $ located_data['country'][:50] = located_data['coordinates'][:50].apply(Tag_country)  #1m38s $
bnbAx[bnbAx['language_english']==0].head()
data = {'Integers' : [1,2,3], $         'Floats' : [4.5, 8.2, 9.6]} $ df = pd.DataFrame(data) $ df
Base.prepare(engine, reflect=True)
def get_integer1(s): $     return abtest_list.index(s)
lm = sm.OLS(df3['converted'], df3[['intercept', 'UK', 'US']]) $ results = lm.fit() $ results.summary()
last_id = trump[0]._json['id'] $ print('tweet url: https://twitter.com/i/web/status/{}'.format(last_id)) $ retweets = [x._json for x in api.retweets(id=last_id,count=100)] $ print('got {} retweets!'.format(len(retweets))) # ... unreliable, won't get exactly 100. oh well!
mom.set_index('Date', inplace = True)
from sklearn.neighbors import KNeighborsClassifier $ k = 3 $ kNN_model = KNeighborsClassifier(n_neighbors=k).fit(X_train,y_train) $ kNN_model
yhat1 = neigh.predict(X_test) $ yhat2 = tree.predict(X_test) $ yhat3 = clf.predict(X_test) $ yhat4 = lr.predict(X_test)
df_combined['country'].unique()
df.pct_chg_opencls.hist(bins= 100)
nzi = pd.notnull(train_data["totals.transactionRevenue"]).sum() $ nzr = (revenue["totals.transactionRevenue"]>0).sum() $ print("Number of instances in train set with non-zero revenue : ", nzi, " and ratio is : ", nzi / train_data.shape[0]) $ print("Number of unique customers with non-zero revenue : ", nzr, "and the ratio is : ", nzr / revenue.shape[0])
import statsmodels.api as sm $ convert_old = np.random.binomial(size_treatment, converted_prob, 10000).mean() $ convert_new = np.random.binomial(size_control, converted_prob, 10000).mean() $ n_old = size_control $ n_new = size_treatment
(df.set_index('STNAME').groupby(level=0)['POPESTIMATE2010','POPESTIMATE2011'] $     .agg({'avg': np.average, 'sum': np.sum}))
val.encode('latin1') $ val.encode('utf-16') $ val.encode('utf-16le')
old_page_converted = np.random.choice(a=[1,0], size=n_old, replace=True, p=[p_old, 1-p_old])
df2 = df.drop((df[(df.group == 'treatment') & (df.landing_page == 'old_page')].index)|(df[(df.group == 'control') & (df.landing_page == 'new_page')].index))
df.loc['1998-09-10':'1998-09-15']
BID_PLANS_df.loc['506b9ecc'].to_frame().transpose()
df_stations = df_all_weeks.groupby('STATION').sum().reset_index()[['STATION','ENTRIES']].sort_values('ENTRIES', ascending = False) $ df_stations
zipincome.head()
scoresdf = {} $ for key in scores.keys(): $     scoresdf[key] = pd.DataFrame(scores[key]) $     scoresdf[key].columns = ['min_df', 'time_taken', 'num_cols', 'score']
image_predictions_df[(image_predictions_df.p1_dog == False) & $                      (image_predictions_df.p2_dog == False) & $                      (image_predictions_df.p3_dog == False)]
definition_1_details = client.repository.store_definition(filename_mnist, model_definition_1_metadata) $ definition_1_url = client.repository.get_definition_url(definition_1_details) $ definition_1_uid = client.repository.get_definition_uid(definition_1_details) $ print(definition_1_url)
inputFile = os.path.join(edfDir, "pt1sz2_eeg.csv") $ outputFile = os.path.join(edfDir, "chanheadersOut.json") $ outputFile = os.path.join(edfDir, "pt1sz2_eeg.json") $ outputFile = os.path.join(edfDir, "pt1sz2_eeg.npy")
twitter_df_clean.stage_name.replace('None',np.nan, inplace=True)
parking_planes=get_parkingplanes(planevisits, start_t, end_t, end_t1)
properati.loc[((properati['state_name'] == "Bs.As. G.B.A. Zona Norte") | (properati['state_name'] == "Bs.As. G.B.A. Zona Oeste") | \ $               (properati['state_name'] == "Bs.As. G.B.A. Zona Sur")),\ $               'state_name'] = "G.B.A"
tobs_info = (session.query(Measurement.date, Measurement.tobs).\ $     filter(Measurement.station == 'USC00519281').\ $     filter(Measurement.date >= '2016-08-18').all()) $ tobs_info
tweets_clean.columns
predictions
x = df.day_of_year.value_counts(sort=False) $ vc = pd.DataFrame(x)
df_joined = df3.set_index('user_id').join(df2.set_index('user_id')) $ df_joined.sample(5)
fda_drugs = pd.read_table('../../static/Products.txt', usecols = ['Form', 'Strength','DrugName', 'ActiveIngredient'])
trainDF.drop(trainDF.columns[0], axis = 1, inplace = True)
cols_to_drop = ['date_account_created', 'timestamp_first_active', 'date_first_booking', 'splitseed'] $ X_train.drop(cols_to_drop, axis=1, inplace=True) $ X_age_notnull.drop(cols_to_drop, axis=1, inplace=True)
print (mars_weather)
df_new.shape
bob_shopping_cart = pd.DataFrame(items, columns=['Bob']) $ bob_shopping_cart
from pandas_datareader.data import Options $ aapl = Options('AAPL','yahoo') $ data = aapl.get_all_data() $ data.iloc[0:6,0:4]
plt.hist(p_diffs) $ plt.xlabel('Conversion rate difference') $ plt.ylabel('Total number') $ plt.title('Sampling Distribution from the Null vs Observed Difference') $ plt.axvline(obs_diff, c='red');
results
df['tweet_ats'] = df.text.str.extract('(\s@\w*)')
import seaborn as sns
compound_final.set_index(['Date'], inplace=True) $ compound_final.head()
df_treatment_group = df.query('group == "treatment"') $ df_treatment_not_new = df_treatment_group.query('landing_page != "new_page"') $ df_new_page = df.query('landing_page == "new_page"') $ df_new_not_treatment = df_new_page.query('group != "treatment"') $ df_treatment_not_new.shape[0] + df_new_not_treatment.shape[0]
plt.hist(p_diffs) $ plt.axvline(differ, color = 'red');
predictions.show()
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results = logit_mod.fit()
pd.DataFrame(data_for_model.groupby(['final_status','country'])['goal'].count())
rng.to_pydatetime()
dta.risk == "Risk 1 (High)"
missing_info = list(data.columns[data.isnull().any()]) $ missing_info
test = import_all(data_repo + 'intervention_test.csv')
import pandas as pd $ import numpy as np $ from matplotlib import pyplot as plt $ %matplotlib inline
df2['user_id'].nunique()
BID_PLANS_df.loc['5ec976dd'].to_frame().transpose()
df = pd.read_csv("prepped_data.csv"); print(len(df)) $ vs = 52234 # manually set this to match above $ import ast $ df["numerized_tokens"] = df["numerized_tokens"].apply(lambda x: ast.literal_eval(x))
df_substrate = df_mysql[df_mysql.endDate==max(df_mysql.endDate)] $ df_substrate = df_mysql[['barrelID', 'substrate']] $ def getSubstrate(barrelID): $     return df_substrate.substrate[df_substrate.barrelID==barrelID].values[0]
guineaFileList = glob.glob("Data/ebola/guinea_data/*.csv") $ frameList = [pd.read_csv(file,usecols=['Description','Date','Totals'],index_col=['Description','Date']) for file in guineaFileList] $ len(guineaFileList)
(autos['registration_year'].between(1900,2016)).sum() / autos.shape[0]
df_new=df2.merge(countries_df,on='user_id',how='left') $ df_new.head()
gps__interestlevel_df = train_df[['latitude','longitude','interest_level']].copy() $ gps__interestlevel_df.head(5)
def num_missing(x): $   return sum(x.isnull()) $ print (filtered_df.apply(num_missing, axis=0))
equipment.head()
df[abs(df.dollar_change_open).round(2) != abs((df.open_price-df.offer_price)).round(2)]
datetime.strptime('09/Aug/1995:09:22:01 -0400',DATETIME_PARSE_PATTERN).weekday()
reg_mod_us = sm.OLS(df_all['converted'], df_all[['US_int', 'ab_page']]) $ analysis_us = reg_mod_us.fit() $ analysis_us.summary()
condos = condos[condos.STATUS != 'RETIRE']
dates = pd.date_range('10-01-2016', periods=9, freq='2W-SUN') $ dates
print sqlContext.sql(query).toPandas()
results.summary()
notnaindex=tweets["retweet_status_user_name"].notna()
from sklearn.linear_model import LogisticRegression $ from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score $ from sklearn.model_selection import train_test_split
SANDAG_ethnicity_df = pd.read_csv(open(SANDAG_ethnicity_filepath), dtype={'TRACT': str}) $ SANDAG_ethnicity_df.set_index(['YEAR', 'TRACT'], inplace=True) $ SANDAG_ethnicity_df.sort_index(inplace=True) $ SANDAG_ethnicity_df.head()
df2=df2.drop('control',axis=1)
tweet_th.describe()
a = np.arange(1, 5); b = np.arange(5, 9); a, b, a+b, a-b, a/b.astype(float)
index_df = index_df[start:end]
df = pd.read_sql('SELECT a.address_id, a.city_id, c.city FROM address a RIGHT JOIN city c ON a.city_id = c.city_id ORDER BY a.address_id DESC;', con=conn) $ df
df['duration'].quantile(0.995) $ df = df[df.duration <= 360]
First of all, get to know your working directory:
pd.__version__
valles_dict = {} $ valles_dict["title"] = valles_title $ valles_dict["img_url"] = valles_full_img $ print(valles_dict) $ hemisphere_image_urls.append(dict(valles_dict)) $
events[['date', 'dateend']]
dfRegMet2016.info()
p_new = new_page_converted.sum()/len(new_page_converted) $ p_old = old_page_converted.sum()/len(old_page_converted) $ print("p_new - p_old = ",p_new - p_old) $ answer_of_g = p_new - p_old # used in j part
df.show()
compound_final.head()
stores = stores.withColumnRenamed("type", "store_type") $ train_holiday_oil_store = train_holiday_oil.join(stores, 'store_nbr', 'left_outer') $ train_holiday_oil_store.show()
test.datetime.min(), test.datetime.max() # there is one day of overlapping between data and test
bus['zip_code'] = bus['zip_code'].str.replace("94602", "94102") $
jobs.loc[(jobs.GPU == 1) & (jobs.FAIRSHARE == 10)].groupby('ReqCPUS').JobID.count().sort_values(ascending= False)
cutoff = 0.75 $ base_filename = '04-15-2018_SubsetFeatures-TestTrainVal-SMOTE-XGBoostRecallError-ParamOptCV'
test = all_sets.cards["XLN"] $ test.loc["Search for Azcanta", ["manaCost", "types", "printings"]]
df['State Bottle Cost'] = df['State Bottle Cost'].str.extract('([^$][0-9.]*)').astype(float) $ df['State Bottle Retail'] = df['State Bottle Retail'].str.extract('([^$][0-9.]*)').astype(float) $ df['Sale (Dollars)'] = df['Sale (Dollars)'].str.extract('([^$][0-9.]*)').astype(float) $ df.head()
twitter_data = pd.read_csv("twitter-archive-enhanced.csv")
values = []
auc_modeling_next = round(bcEval.evaluate(prediction_modeling2),6) $ print('Estimated AUC modeling next:', round(auc_modeling_next,6))
mins = np.array([str(datetime.now() + timedelta(seconds=el)) for el in range(100)], $                dtype=np.datetime64) $ mins[:3]
dfEtiquetas
df3 = pd.DataFrame(df, index = ['b', 'c', 'd', 'a']) $ df3
lm=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','UK','CA']])
verify_response.json().keys()
train_size = 0.7 $ unique_days = np.unique( df['Date'].values ) $ split_date = unique_days[ math.floor(len(unique_days)*train_size)] $ split_date
blacklist = vacancies[(vacancies.hour >= 20) | (vacancies.hour < 10) | (vacancies.weekday > 5)] $ blacklist.sort_values('company')[['company', 'weekday', 'hour']]
fb.info()
svm_tunned.best_params_
print(f'There are {len(cutoff_times)} 1st of the month cutoff times in partition {PARTITION}.')
engine.execute('SELECT * FROM measurements limit 10').fetchall()
advancedmodel = LogisticRegression() $ advancedmodel = advancedmodel.fit(advancedtrain, train["rise_in_next_week"])
empInfo["sender"] = empInfo.ID $ empInfo["receiver"] = empInfo.ID
tweet1.lang
df.head()
reddit.head()
fpr_a = (grid_pr_fires.sort_values(['glat', 'glon'], ascending=[False,True])['pr_fire'] $          .values.reshape(26,59))
import sklearn $ from sklearn.svm import LinearSVC $ classifier = LinearSVC(verbose=1) $ classifier.fit(X_train, y_train)
from IPython.display import YouTubeVideo $ YouTubeVideo('1O1S0RGfJ2E')
start = get_start(start_coord_list, 1) $ end = get_end(end_coord_list, 1)
tweets['created_at'] = tweets['created_at'].dt.tz_localize('GMT').dt.tz_convert('US/Eastern')
s = pd.Series([1, 2], index=["1", "2"]) $ print(s["1"])  # matches index type; use explicit $ print(s[1])  # integer doesn't match index type; use implicit positional
df_all_columns['datetime'] = pd.to_datetime(df_all_columns.Timestamp, unit='s') $ df = df_all_columns[['datetime','Volume_(BTC)','Weighted_Price']] $ df = df.rename(index=str, columns={"datetime": 'timestamp', 'Volume_(BTC)':'value', 'Weighted_Price':'price'}) $ print("row count: " + str(len(df.index))) $ df.head() $
print(len(a.intersection(b)))
channels = df['channels']
subte = pd.read_csv('../Datos Capital/estaciones-de-subte.csv',low_memory=False)
sentiments_groupby = sentiments_pd.groupby(['Products']) $ sentiments_mean = sentiments_groupby['Tweet Polarity'].mean() $ sentiments_final = pd.DataFrame(sentiments_mean).reset_index() $ sentiments_final.head()
Z = np.array([1,2,3,4,5]) $ nz = 3 $ Z0 = np.zeros(len(Z) + (len(Z)-1)*(nz)) $ Z0[::nz+1] = Z $ print(Z0)
p_diffs = [] $ for _ in range(10000): $     samp = df2.sample(df2.shape[0], replace=True) $     p_diffs.append(samp.query('landing_page == "new_page"').converted.mean() -\ $                    samp.query('landing_page == "old_page"').converted.mean()) $
new_DUL_file = folder + "\Duluth-all.txt" $ new_DUL_file
topC['date_simple'] = pd.DataFrame(topC.date.dt.date) #not ideal python syntax, but for now it's ok
dt_features['state_changed_at'] = pd.to_datetime(dt_features['state_changed_at'],unit='s')
top_songs['Artist'].isnull().sum()
lr1 = LogisticRegression() $ params1 = {'penalty': ['l1', 'l2'], 'C':np.logspace(-5,0,100)} $ gs_1 = GridSearchCV(lr1, param_grid=params1, cv=10, verbose=1) $ gs_1.fit(X1, y1)
crimes.columns
tg_u= df[df['group']=='treatment'] $ tg_u_prob_of_conv = tg_u['converted'].mean() $ print('The probability of a treatment group user converting is:  ' + str(tg_u_prob_of_conv)) $
df2 = df.drop(df.query('(group == "treatment" and landing_page != "new_page") or (group != "treatment" and landing_page =="new_page") or (group == "control" and landing_page !="old_page") or (group !="control" and landing_page == "old_page")').index)
tweet_text = pd.DataFrame.from_dict(text)
odm = pd.read_csv('data/odm2.csv')
pd.read_sql('SELECT * FROM experiments WHERE irradiance = 700 ORDER BY temperature', conn, index_col='experiment_id')
new_df['index'] = pd.to_datetime(new_df['index'])
locationing.corr()
apple_tweets2 = ioDF.loc[ioDF.author_id_y == 'AppleSupport'] \ $     .loc[ioDF.date_x > datetime(2017, 11, 8)] \ $         .loc[ioDF.date_x < datetime(2017, 11, 10)]
df.head()
x2[0]
pd.DataFrame(population, columns=['population'])
readRDS = robjects.r['readRDS'] $ df = readRDS('loan_data.rds') $ df = pandas2ri.ri2py(df)
IBMpandas_df = pd.read_csv("IBM.csv") $ IBMpandas_df.head()
reload(pt)
np.exp(reg_lm2.params)
tokens['one_star'] = tokens.one_star / nb.class_count_[0] $ tokens['five_star'] = tokens.five_star / nb.class_count_[1]
sessions_summary = sessions_summary.reset_index() $ sessions_summary["len"] = sessions_summary.apply(lambda l: len(l["action"]), 1) $ sessions_summary["len"].value_counts()
df_ad_airings_filter_3 = df_ad_airings_filter_3[df_ad_airings_filter_3['start_time'] <=  datetime(2016, 11, 8, 23, 59, 59)]
del df['index'] $ print 'Number of nulls in the data set: \n', df.isnull().sum(),'\n' $ df.head()
df.interpolate()
wrd_full.query('favorite > 9447')['hour'].value_counts().rename_axis('hour').reset_index(name='counts').sort_values(by=['hour']).plot(x="hour",y="counts");
merged.amount.sum()
sfpd_clean = sfpd.withColumn("date", expr("parse_date(date)")) $ sfpd_clean.limit(10).toPandas()
rounds = 10 $ steps = [('mapper', mapper),('XGBClassifier', XGBClassifier)] $ pipeline = sklearn.pipeline.Pipeline(steps) $ model = (pipeline.fit(X_train, y_train, XGBClassifier__eval_metric='error', XGBClassifier__early_stopping_rounds=rounds, $         XGBClassifier__eval_set=[((mapper.fit_transform(X_train), y_train)),(mapper.fit_transform(X_test), y_test)]))
trainDF.head()
writer.save() $ writer.close()
xml_in_sample.head()
df2['intercept'] = 1 $ df2[['ab_page_reverse', 'ab_page']] = pd.get_dummies(df2['group']) $ df2.drop('ab_page_reverse', axis =1, inplace=True);
from langdetect import detect
DataSet = toDataFrame(results)
future_forecast = pd.DataFrame(future_forecast,index = test.index,columns=['Prediction'])
client.repository.list_experiments()
print('Total records {}'.format(len(non_na_df))) $ print('Start / End : {}, {}'.format(non_na_df.index.min(), non_na_df.index.max()))
mw = pd.read_csv('mw.csv', sep='~')
df2['intercept'] = 1 $ logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit_mod.fit() $ results.summary()
df2['ab_page']=pd.get_dummies(df2['group'])['treatment'] $ df2.head()
len(df2.query('group == "treatment" and converted == 1')) / len(df2.query('group == "treatment"'))
loc_name='Skagit Watershed' $ streamflow_watershed_drainage_area=8010833000 # square meters
len(targets)
train[train.date_first_booking.isnull()]['country_destination'].value_counts()
len(stopset) #417
priors_reordered = priors_product[priors_product['reordered'] == 1] $ priors_reordered.head()
def getvalueofnode(node): $
city_eco["economy"].cat.categories
allqueryDF.shape
train['question_dt']=pd.to_datetime(train['question_utc'], unit='s') $ test['question_dt']=pd.to_datetime(test['question_utc'], unit='s') $ train['answer_dt']=pd.to_datetime(train['answer_utc'], unit='s') $ test['answer_dt']=pd.to_datetime(test['answer_utc'], unit='s')
open('test_data//open_close_test.txt', encoding='utf8')
df = pd.DataFrame({ $     'A': s, $     'B': ['a', 'b', 'c', 'd', 'e', 'f'] $ }) $ df
freeways.layers 
joined = join_df(train, store, "Store") $ joined_test = join_df(test, store, "Store") $ len(joined[joined.StoreType.isnull()]),len(joined_test[joined_test.StoreType.isnull()])
df3 = pd.read_csv('countries.csv') $ df3.head(5)
ec550 = adm.where(in_domain(adm), drop=True).ec550 $ ec550
analyzer = SentimentIntensityAnalyzer() $ ltc[['compound','neg','neu','pos']] = ltc['body'].apply(lambda body: pd.Series(analyzer.polarity_scores(body))) $ xrp[['compound','neg','neu','pos']] = xrp['body'].apply(lambda body: pd.Series(analyzer.polarity_scores(body))) $ eth[['compound','neg','neu','pos']] = eth['body'].apply(lambda body: pd.Series(analyzer.polarity_scores(body))) $
val_small_data.info("deep")
len(x), y.size
import pandas as pd
score = pd.DataFrame(y_test) $ score['pred_prob_rf'] = y_pred_rf $ score['pred_prob_lr'] = y_pred_lr $ score['pred_rf'] = np.where(score['pred_prob_rf']>=.25, 1, 0) $ score['pred_lr'] = np.where(score['pred_prob_lr']>=.25, 1, 0)
pl.hist(yc_new3['tipPC'], bins=30) $ pl.ylabel('N') $ pl.xlabel('tipPC') $ pl.title('Distribution of Tips as a percentage of Fare, under 100%') $ print('The first moment is 4.67 and the second moment is 9.61')
sns.boxplot(x=df.author, y=df.created_at.dt.weekday); $ plt.xticks(np.arange(10), ('collins', 'hclinton', 'hirono', 'hoeven', 'mccain', 'obama', 'ryan', 'sanders', 'schwarzenegger', 'trump'), rotation = 50); $ plt.yticks(np.arange(7), ('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'));
c0vm = df5['HT'].where(df5['new_id'] == 'c0vm').dropna() $ plt.hist(c0vm, bins=50)
nypd_df['create_date'] = nypd_df['create_date_time'].dt.date $ nypd_df['day_of_week'] = nypd_df.create_date.apply(lambda x: x.weekday()) $ nypd_df['weekend'] = 0 $ nypd_df['weekend'][(nypd_df['day_of_week'] == 4) | (nypd_df['day_of_week'] == 5)] = 1 $ nypd_df['hour_of_day'] = nypd_df.create_date_time.apply(lambda x: x.hour())
df_new[['CA', 'UK']] = pd.get_dummies(df_new['country'])[['CA','UK']] $ log_m2 = sm.Logit(df_new['converted'], df_new[['intercept','CA', 'UK']]) $ results2 = log_m2.fit() $ results2.summary()
plot_chernoff_data(df01, 1e-6, 1, "Nth = 0.1") $ plt.savefig('../output/g_perr_vs_M_01.pdf', bbox_inches='tight')
y3 = df3['converted'] $ X3 = df3[['intercept', 'ab_page']] $ X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.20, random_state=0)
autos[['date_crawled','ad_created','last_seen']].head(5)
tNormal = len(df[ (df['bmi'] > 18.5) & (df['bmi'] < 25.0) ]) $ tNormal
theta = theta_sgd(X_train_1,y_train, 0.001, 100)
y=np.ravel(y)
pickle.dump(lsa_tfidf_data, open('iteration1_files/epoch3/lsa_tfidf_data.pkl', 'wb'))
import test_package
data_helper = GoogleDataHelper() $ df = data_helper.get_data(channel='MachineLearning', querystring='Trend')#get_data(date_range=['2017-07-24', '2017-07-25', '2017-07-26'])
trainer = NaiveBayesClassifier.train $ classifier = sentim_analyzer.train(trainer, training_set)
acc = ha.accounts.ing_diba_giro(filename, path=path) $ print(acc)
stars.info()
print('Before removing reactivations:',df.shape) $ df = df[df.Injury != 0] $ print('With only placements onto the Disabled List:',df.shape)
F_hc = a_hc[score_variable].var() / m_hc[score_variable].var() $ degrees1hc = len(m_hc[score_variable]) -1 $ degrees2hc = len(a_hc[score_variable]) -1
popular_programs = challange_1["program_code"].value_counts() $ popular_programs
talks['text'] = talks['text'].apply(tlss)
type(station_tobs)
list(df_json_tweets.columns.values)
sentiment_df["Date"] = pd.to_datetime(sentiment_df["Date"]) $ sentiment_df["Date"]
imsi_with_1T = event_num[event_num == 1].index.values
print(gen2)
in_cp_25_1 = suspects_with_25_1['in_cp']
df_MC_least_Convs = pd.concat([year_month.transpose(), df_MC_leastConvs], axis=1) $ print 'DataFrame df_MC_least_Convs: ', df_MC_least_Convs.shape $ df_MC_least_Convs
rhum_nc = Dataset("../data/nc/rhum.mon.mean.nc")
twitter.head(5)
df.to_csv('ab_cleaned.csv', index=False) $ df2 = pd.read_csv('ab_cleaned.csv')
s1[3:8]
ekos.load_workspace(user_id, alias = workspace_alias) $
sns.regplot(x=data['avg_c_gap'], y=data['ltv'])
left = pd.DataFrame({'key': ['yes', 'no'], 'lval': [7, 2]}) $ left
from bmtk.simulator import popnet $ configure = popnet.config.from_json('config.json') $ pprint.pprint(configure)
selection = spice_df.query('parsley & tarragon') $ len(selection)
students.iloc[0:3,[0,2]]
content_wed11.tail(20)
to_be_predicted_Day2 = 48.54345518 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
top_20 = data['Company'].value_counts()[0:20] $ top_20[::-1].plot(kind='barh');
df_new.groupby('country').mean()['converted']
df2[['afternoon', 'evening', 'morning']] = pd.get_dummies(df2['time_of_day']) $ df2 = df2.drop('morning', axis=1)
Test.SetFlowValues(9.25, 12.00, 9.25)
df_merged.hist(column='retweet_count',bins=100);
%load_ext autoreload $ %autoreload 2
score_l50 = score[(score["score"] < 50) & (score["score"] > 0)]  $ score_l50.shape[0]
scalable_variables = ['unit_sales','dcoilwtico'] $ for var in scalable_variables: $     mini, maxi = pd_train_filtered[var].min(), pd_train_filtered[var].max() $     pd_train_filtered.loc[:,var] = (pd_train_filtered[var] - mini) / (maxi - mini)
minimum = df['time_open'].min() $ print(minimum) $
print(opp["Germany"] * 100)
recommendation_df.shape
print('Pad sequences (samples x time)') $ x_train = sequence.pad_sequences(x_train, maxlen=maxlen) $ x_test = sequence.pad_sequences(x_test, maxlen=maxlen) $ print('x_train shape:', x_train.shape) $ print('x_test shape:', x_test.shape)
ins.head()
frames = [df26, df27, df28, df29, df30, df1, df2, df3] $ df = pd.concat(frames) $ df.drop('Unnamed: 0', axis=1, inplace=True) $ df.drop('user_id', axis=1, inplace=True) $ df.info()
import mysql.connector $ from sqlalchemy import create_engine $ engine = create_engine('mysql+mysqlconnector://admin:geotwitter@geotwitter.uncg.edu:3306/geotwitter', echo=False) $ myPD.to_sql(name='precipitation', con=engine, if_exists = 'append')
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=1?api_key="+API_KEY
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost','ooIdleCost','corrCost','floor_date'], axis=1) $ dfY = data['corrCost']
del frame2['eastern']
STUDENT_EMAIL = 'skooch@gmail.com' $ STUDENT_TOKEN = 'zAdUrexg9Er5TbMc' $ grader.status()
temp_ser = pd.Series(temp) $ temp_cat = pd.Series(temp).astype('category') $ print (type(temp_cat))       # Series object $ print (type(temp_cat.cat))   # Categorical Accessor
minval = data[["TMIN"]].min().values[0] $ maxval = data[["TMAX"]].max().values[0] $ minval, maxval
df2[df2['GrossOut'] > df2['GrossIn']].index
country=pd.get_dummies(df3['country']) $ country.tail()
actual_diff = df2[df2['group'] == 'treatment']['converted'].mean() -  df2[df2['group'] == 'control']['converted'].mean() $ (p_diffs > actual_diff).mean()
mis_match = [] $ for i in range(len(image_predictions)): $     if (image_predictions['tweet_id'][i]) not in list(twitter_archive['tweet_id']): $         mis_match.append(image_predictions['tweet_id'][i]) $ print(len(mis_match))
df=pd.read_csv("../UserData/1000ShareAllColumns.csv") $ df.dtypes
tweet_data.retweeted.value_counts()
vals = billtargs.bill_type.get_values() $ np.unique(vals)
for c in ccc: $     vhd[c] /= vhd[c].max()
df = df[ ['Department', 'Lottery', 'Literacy', 'Wealth', 'Region'] ] $ df.tail(5)
my_df_free1.iloc[100:110]
type(AAPL)  # returns a dataframe
import os $ import subprocess $ from subprocess import check_output, run $
jobPostDF['date'] = jobPostDF.date.astype(datetime)
df.query('converted == 1')['converted'].count() / df.shape[0]
daily_df = pd.concat([twitter_delimited_hourly, stock_delimited_hourly], axis=1, join='inner') $ daily_df.head()
def flight_type(x): $     if 'light search:' in x: $         return re.search(r'(^.*)( search:)', x).group(1).lower()
cleaned_df = pd.read_csv('data/cleaned_data.csv')
now_start = datetime.datetime.now() $ time_start = now_start.strftime("%Y-%m-%d (yyyy-mm-dd); %H:%M hrs.") $ print "# Starting time of computing: %s"%time_start
len(df.user_id.unique())
tweets_df = tweets_df[['id_str', 'retweet_count', 'favorite_count']].set_index('id_str')
def businessyear(x, y): $     Month = x.dt.month
data['Gender'] = data['Gender'].replace(['M'], ['Male']) $ data['Gender'] = data['Gender'].replace(['Male/Female'], ['Unknown']) $ data['Gender'] = data['Gender'].replace(['M/F'], ['Unknown']) $ data['Gender'] = data['Gender'].replace(['Unkown'], ['Unknown']) $ data['Gender'].value_counts()
has_text.sort_index(by='grade_levels', axis=0).tail(20)
nOld = counts['old_page'] $ nOld
metadata_df = pd.read_excel(DATA_FOLDER+'/microbiome'+'/metadata.xls') $ metadata_df
timeFilter = 8 $ sorted_stays = removeNonStays(timeFilter) $ removeNonStays(timeFilter).head()
from autosklearn.regression import AutoSklearnRegressor
ben_dummy = ben_final.loc[:,['diffs','userid','pagetitle']]
train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \ $     train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)
df_DRGs.head()
autos.rename({'odometer':'odometer_km'},axis = 1, inplace=True)
qW = ps.queen_from_shapefile(os.getenv('PUIDATA')+'/pumashplc.shp')
google_request.content[0:500]
type2017.isnull().sum() 
print(df['one'].isnull(),df['one'].isnull())
results[results['type']=='Other']['tone'].value_counts()
weights['0.encoder.weight'] = T(new_w) $ weights['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w)) $ weights['1.decoder.weight'] = T(np.copy(new_w))
act_diff = df2.query('group == "treatment"')['converted'].mean() - df2.query('group == "control"')['converted'].mean() $ act_diff
followup.columns
dataset = pd.read_csv("inner_join_2018-06-04.csv", $                       dtype=str, $                       keep_default_na=False) \ $             .drop_duplicates()
for e in _td: $     print e.text
p_value
excelDF = pd.read_excel("C://SuperStore.xls", sheet_name = "Orders")
df_tsv.duplicated()
enroute_4x_tabledata = enroute_4x_count_prop_byloc.reset_index() $ create_study_table(enroute_4x_tabledata, 'locationType', 'remappedResponses', $                    location_remapping, atloc_response_list)
tweets.plot(x='created_at', y='chars')
tweet_info.info()
dti2 = pd.to_datetime(['Aug 1, 2014', 'foo']) $ type(dti2)
len(b'mat\xc3\xa9')
users.created_at = pd.to_datetime(users.created_at)
lm = sm.OLS(df_new['converted'],df_new[['a/b_page','uk_intercept']]) $ result = lm.fit() $ result.summary()
viajes_sin_destino = trips[trips['start_station_name'] == trips['end_station_name']].shape[0] $ print str(viajes_sin_destino) + " \"viajes\""
def quadratic(x, **kwargs): $     return np.hstack([np.ones((x.shape[0], 1)), x, x**2])
reddit['title'] = reddit['title'].map(lambda x: x.lower())
df.head()
n_new = df2[df2.group == 'treatment'].shape[0] $ n_new
plt.style.use('seaborn-notebook') $ bb.plot(y='close')
df_samples.to_pickle(train_data_dir+'/LSTM_train_data.pkl')
churned_ordered['end_date'] = pd.to_datetime(churned_ordered_end_date).strftime('%Y-%m')
%matplotlib inline
! wget -c "https://s3.amazonaws.com/fair-data/starspace/wikipedia_devtst.tgz"
max = session.query(func.max(Measurement.tobs)).\ $     filter(Measurement.station == "USC00519281").all() $ print(f"Highest Temp: {max}")
 print(r.text)
from sklearn.svm import LinearSVR $ lin_svr = LinearSVR(random_state=42) $ lin_svr.fit(X_train_scaled, y_train)
missing_sample.dropna(axis=1)
tipsDF.head()
get_req = requests.get(get_url(job=json_out['job']), headers=headers)
df.to_sql('places', conn_aws)
autos["unrepaired_damage"].value_counts()
len(preg)
twelve_months_prcp.head()
ac['Description'].describe()
longest_date_each_costumer = longest_date_each_costumer.apply(func) $ longest_date_each_costumer.head(10)
timestamp = pd.to_datetime('now')-pd.Timedelta(hours=5) $ path='C:\\Users\\Christopher\\Google Drive\\TailDemography\\outputFiles\\' $ filename = path + 'cleaned CC data 2000-2017_' + '.csv' $ df.to_csv(filename,index = False) $ filename
lons, lats = np.meshgrid(lon_us, lat_us) $ plt.plot(lons, lats, marker='.', color='k', linestyle='none') $ plt.show()
mismatch1 = (ab_df['landing_page'] == "new_page")&(ab_df['group'] == "control") $ mismatch2 = (ab_df['landing_page'] == "old_page")&(ab_df['group'] == "treatment") $ print(ab_df[mismatch1].shape[0]+ab_df[mismatch2].shape[0])
from sklearn.linear_model import LogisticRegression $ from sklearn.metrics import confusion_matrix $ LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train) $ LR
fprice_df['feature'] = list1 $ fprice_df['price_no'] = list2 $ fprice_df['price_yes'] = list3 $ print(fprice_df)
sigma_est = sim_closes.iloc[-1].std() $ (call.iloc[-1].Prima-1.96*sigma_est*np.exp(-r*ndays)/np.sqrt(nscen),call.iloc[-1].Prima+1.96*sigma_est*np.exp(-r*ndays)/np.sqrt(nscen))
with open("Valid_Politician_and_Events","wb") as f: $     pickle.dump(ValidNameEvents,f)
data.keys()
digits.data.shape
acs_df.head()
twitter_archive_clean.head()
abc = abc.reset_index(level=[0,1]) $ abc.columns
data = pd.read_csv("Data.csv")
tweet_archive_enhanced_clean.info()
noaa_data.head()
df_TempIrregular['timeStamp'] = pd.to_datetime(df_TempIrregular.pubTimeStamp) $
idx
new_reps.newDate[new_reps.Trump.isnull()]
tmp = df.corr(method = 'pearson')[['meantempm']] $
df2.groupby(['group'])['converted'].mean()
df.dtypes
cur.execute(insert_coindesk, res[0])
baseball_newind.sort_index().head()
twitter_archive_master.shape
y_pred = model.predict(X_test) $ utils.metrics(y_test, y_pred)
mismatch_grp1 = ab_file.query("group == 'treatment' and landing_page == 'old_page'") $ print("Times treatment group user lands incorrectly on old_page is {}".format(len(mismatch_grp1))) $ mismatch_grp2 = ab_file.query("group == 'control' and landing_page == 'new_page'") $ print("Times control group user incorrectly lands on new_page is {}".format(len(mismatch_grp2))) $ print("Times new_page and treatment don't line up is {}".format(len(mismatch_grp1) + len(mismatch_grp2)))
plt.figure(figsize=(15,5)) $ sns.countplot(auto_new.Body_Type)
from pandas.tseries.offsets import BDay $ pd.date_range('2015-07-01', periods=5, freq=BDay())
resultsList = responsesJsonList.apply(getResults)
print 'There are far less non smokers and the mean cholesterol of the population is different' $ print 'Mean chol. smoker: {}'.format(yes.chol.mean()) $ print 'Mean chol. non smoker: {}'.format(no.chol.mean())
sl = df.loc[df.country_name=='Sierra Leone'] $ print(sl.shape)
liberiaDf = pd.concat([liberiaCases, liberiaDeaths],axis=1) $ liberiaDf.index.name = 'Date' $ liberiaDf.head()
index_change = df2_new[df2_new['group']=='treatment'].index $ df2_new.set_value(index=index_change, col='ab_page', value=1) $ df2_new.set_value(index=df2_new.index, col='intercept', value=1) $ df2_new[['intercept', 'ab_page']] = df2_new[['intercept', 'ab_page']].astype(int) $ df2_new = df2_new[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
(~autos['registration_year'].between(1900,2016)).sum()/autos.shape[0]
df.describe(include=[np.number])
data.values
df0 = df[ (df['group'] == 'control') & (df['landing_page'] == 'old_page')  ] $ df1=  df[(df['group'] == 'treatment') & (df['landing_page'] == 'new_page')] $ df2=pd.concat([df0,df1]) $ unique_users_2=df2['user_id'].unique().shape[0] $ df2.head() $
%matplotlib inline
trunc_df.loc[list(top_10)].to_csv('Civatech_Oncology.csv')
! unzip -l {path_to_zips}On_Time_On_Time_Performance_2015_1.zip
sort_b_desc = noNulls['b'].desc()
new = zc.merge(data3, on='zipcode') $ new.head()
pd.bdate_range(start=s,end=e,weekmask=weekmask,holidays=holidays)
data= pd.concat([btypedums, data], axis=1)
trips=trips3 #get historical data $ trips['day']=pd.to_datetime(trips['day']) # create datetime dates $ trips=trips.set_index('day') #index it on time $ del trips.index.name $ fcst_trips = pd.concat([fcst_trips, trips], axis=1) #join data to commbine history+forecasts
df2.query('group == "control"').converted.mean()
max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(history.history['val_acc'])) $ print('Maximum accuracy at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(max_val_acc))
display(Markdown(q3a_answer))
%%R $ summary(flightsDB)
from gensim.models import Doc2Vec $ model = Doc2Vec.load('/tmp/movie_model.doc2vec')
np.shape(prec_fine)
!spark-submit --properties-file dir/myspark.conf script.py data/*
total.last_valid_index()
index = df[(df.donor_id == '-28K0T47RF') & (df.donation_date == '2007-11-30') & (df.city == 'Cupertino')].index $ df.ix[index,'state'] = 'CA' $ index = df[(df.donor_id == '9F4812A118') & (df.donation_date == '2012-06-30') & (df.city == 'San Juan')].index $ df.ix[index,'state'] = 'WA' $ df.ix[index,'zipcode'] = 98250
import language_recognition as lr
df['HOUR'].fillna(99, inplace = True) $ df['NEIGHBOURHOOD'].fillna('N/A', inplace = True) $ df['HUNDRED_BLOCK'].fillna('N/A', inplace = True)
print(q5c_answer)
Vy = Valid_events["diff"]
tia1 = tia[['company','location','skills','title','salary_clean','category','duration_int','summary_clean']] $ tia1.shape
df_old = df2[df2['landing_page'] == 'old_page'] $ n_old = df_old.user_id.count() $ n_old
df.empty
from sklearn.model_selection import KFold $ cv = KFold(n_splits=10, random_state=None, shuffle=True) $ estimator = Ridge(alpha=2500) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
btypedums = pd.get_dummies(data.bill_type) $ data.drop(['bill_id', 'bill_type'], axis=1, inplace=True) $ data.billtext = map(str.lower, data.billtext) $
merged_data['tax'].fillna(0, inplace=True)
ldamodel_Tesla= models.ldamodel.LdaModel(corpus_Tesla, num_topics=3, id2word = dictionary, passes=20)
pd.concat([test, train]).plot(y=['PJME', 'ElasticNet_Prediction'], figsize=(15,5))
description_predict_clean = clean_corpus(description_predict) $ predicted_2018 = text_clf.predict(description_predict_clean)
station_count = measure_df['station'].nunique() $ station_names = measure_df['station'].unique() $ print(station_count) $ print(station_names)
targets = ['1D', '1W', '1M', '3M'] $ df_symbols = df.loc[df_symbols.index] $ df_symbols.drop(targets, axis=1, inplace=True) $ df_symbols.dropna(axis=1, inplace=True) $ df_symbols.head()
TrainData_ForLogistic.to_csv('training_logistic.csv')
s_n_s_epb_two.Date = s_n_s_epb_two.Date.str.replace('-',"") $ s_n_s_epb_two.Date = pd.to_datetime(s_n_s_epb_two.Date,format="%Y%m%d")
feature_names = list(df_wb) $ feature_names
df_course_association.head()
file = 'boulder_weather.csv' $ weather = pd.read_csv(file)
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 1), 'type'] = 'defenders'
df2 = df.copy() $ df2 = df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) != False]
popt_opb_brace_saddle, pcov_opb_brace_saddle = fit(d_opb_brace_saddle)
api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True) $ retweet_fol = [] $ for re_tweet_id in retweet_lst_ids_flat: $     user = api.get_user(re_tweet_id) $     retweet_fol.append(user.followers_count) $
X_train, X_test, y_train, y_test = train_test_split(df['full_text'], y_retweet, test_size=0.3, random_state=0) $ vectorizer = TfidfVectorizer(stop_words='english',token_pattern = r'\b[a-zA-Z]{3,}\b',lowercase = True) $ X_train_tfidf = vectorizer.fit_transform(X_train) 
type(api)
fulldata_copy = h5['data']
((val,trn), (y_val,y_trn)) = split_by_idx(val_idx, df.values, yl)
len(ids)
shows['plot'].head()
plt.hist(shows['first_year'].dropna()) $ plt.title('Distribution of Release Years') $ plt.ylabel('Frequency') $ plt.xlabel('Year Released')
import pandas_datareader as pdr $ import datetime $ aapl = pdr.get_data_yahoo('AAPL', $                           start=datetime.datetime(2014, 1, 1), $                           end=datetime.datetime(2017, 3, 1))
num_id = df.nunique()['user_id'] $ print("{} unique users in the dataset.".format(num_id))
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2[['control','treatment']] = pd.get_dummies(df2['group']) $ df2.head()
cust.iloc[:, 6:]
date_series['2014-08-05':'2014-08-07']
model.summary()
d_ny = d_utc.tz_convert('US/Eastern') $ d_ny
df['A'] > 2 $ df[df['A'] > 2]
merged_portfolio_sp_latest = pd.merge(merged_portfolio_sp, sp_500_adj_close, left_on='Latest Date', right_on='Date') $ merged_portfolio_sp_latest.head()
bruins = pd.read_csv('../../../data/bruins/home.csv')
print('The proportion of users converted: ' ,df['converted'].sum()/len(df))
len(youthUser1)
sen = np.array(sen) $ neg = np.array(neg) $ neu = np.array(neu) $ pos = np.array(pos) $ com = np.array(com) $
log_with_day = access_logs_df.withColumn('dayOfWeek',weekday('dateTime')) $
seed = 2210 $ (train_sample, validation) = modeling2.randomSplit([0.9,0.1], seed=seed)
store_items.fillna(0)
data_set.head(5)
df_test = pd.read_csv("C:/Users/ajayc/Desktop/ACN/2_Spring2018/ML/Project/WSDM/DATA/sample_submission_v2.csv")
acc.find(bdate='20.08.2016')
new_dems.newDate.isnull().sum()
df['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','CA', 'US']]) $ results = log_mod.fit() $ results.summary()
df['imm_hold'] = 0 $ df.loc[hold_mask, 'imm_hold'] = 1
for k,d in dataset_dict.items()[:10]: $     print 'id: ',k $     print 'name: ',d $     print '---'
autos.brand.value_counts(normalize=True)
noNulls.orderBy(sort_a_asc).show(5)
closes.ix['MSFT'][:3]
pwd
countries_df = pd.read_csv('data/countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
km.labels_
print(["1" "2" "abc" "Two words."]) $ print(["1", "2", "abc", "Two words."])
df_prod.to_csv('production.csv')
transit_df_rsmpld = transit_df_byday.reset_index().groupby('FUZZY_STATION').apply(lambda x: x.set_index('DATETIME').resample('1M').sum()).swaplevel(1,0) $ transit_df_rsmpld.info() $ transit_df_rsmpld.head()
proj_df['Project Subject Category Tree'].value_counts(dropna=False)
with tb.open_file(filename='data/NYC-yellow-taxis-100k.h5', mode='a') as f: $     table = f.get_node(where='/yellow_taxis_2017_12') $     table.cols.trip_distance.remove_index() $     table.cols.passenger_count.remove_index()
def getHour(date): $     time = date.split(" ")[1] $     hour = time.split(":")[0] $     return int(hour)
%matplotlib inline $ sns.violinplot(data=november, inner="box", orient="h", bw=.03)
df2_control = df2.query('group == "control"').converted.mean() $ df2_control
%%time $ df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv')
url = "http://www.fifeweather.co.uk/cowdenbeath/200606.csv"
top_score = df.rating_score.max() $ print('The highest rating is a {} out of 5.'.format(top_score))
val_small_data.head(1)
df = post_discover_sales[~(post_discover_sales['discover_first_date'].isnull()) & $                     (post_discover_sales['used_discover_already'] == 'Used Discover') & $                         (post_discover_sales['Buy Count'] == 1)].groupby(['Email', 'discover_sales_lead_time', 'Created at', 'Buy Count'])['Bought Recommended'].sum()
y.T-30
pd.Series(data=y5_train).value_counts()
from tqdm import tqdm
predictions = dt.predict(test[['expenses', 'floor', 'lat', 'lon', 'property_type',\ $                                 'rooms', 'surface_covered_in_m2', 'surface_total_in_m2']])
m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), $                    0.04, 1, [1000,500], [0.001,0.01], y_range=y_range) $ m.summary()
dfJobs.ix[4009]
short_window = 40 $ long_window = 100 $ signals = pd.DataFrame(index=aapl.index) $ signals['signal'] = 0.0
one_hot_domains_questionable = df_questionable_3.groupby('user.id')[media_classes].sum().fillna(0) $ one_hot_domains_questionable = one_hot_domains_questionable.apply(normalize, axis=1).fillna(0) $ tsne = TSNE(n_components=2, learning_rate=150, verbose=2).fit_transform(one_hot_domains_questionable)
cursor = db.tweets.find({}, {'text':1, 'id':1,'user':1, 'hashtags':1,'_id': 0}) $ df =  pd.DataFrame(list(cursor)) $ df.head(3)
df.groupby('converted')['user_id'].count()
merge[merge.columns[13]].value_counts()
df.groupby('group').mean()['converted']
kickstarter_failed_successful.sort_values(by='difference', ascending=False)
from conceptnet5.vectors.formats import load_word2vec_bin $ w2v = load_word2vec_bin('data/word2vec-googlenews-300.bin.gz', nrows=2000000) $ w2v.index = [label.casefold() for label in w2v.index] $ w2v = w2v.reset_index().drop_duplicates(subset='index', keep='first').set_index('index') $ retrain_model(w2v)
%run -i 'label_image/label_image.py' --graph='/tmp/output_graph.pb' --labels='/tmp/output_labels.txt' --input_layer='Mul' --output_layer='final_result' --input_mean=128 --input_std=128 --image='test/Colin_Powell.jpg'
Z = np.zeros(10) $ Z[4] = 1 $ print(Z)
tweet_df_clean = tweet_df_clean[tweet_df_clean['retweeted_status'].isnull() == True]
test.head()
oppose.amount.sum()
yrEnd = qrt['1982-10-01':'2017-09-30']
support.amount.sum() / merged.amount.sum() $
pd.crosstab(test_df_01.label, test_df_01.predict)
crimes.shape
columns = inspector.get_columns('measurement') $ for c in columns: $     print(c['name'], c["type"]) $
curves = ['GR','ILD'] $ windows = [5,7,11,21]
labels = pd.read_csv('response5.csv',header =None )
us.loc[us['country'] != 'USA', 'cityOrState'] = us.loc[us['country'] != 'USA', 'country']
y = x.loc[:,"A"] $ y
dates=pd.date_range('1-Sep-2017',periods=15,freq='D') $ print(dates)
df.drop('date_account_created',axis=1,inplace=True)
with open('celebrities.json', 'w') as file: $     file.write(df_celebrities.to_json(orient='records'))
look_back_dt = dt.datetime.strptime(test_start_dt, '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=T-1) $ test = energy.copy()[test_start_dt:][['load']] $ test.head()
fulldata_copy[['AUDUSD','RSI']][:500].plot(figsize=(10,6),secondary_y = 'RSI');
reddit_comments_data.select('author').distinct().count()
prcp_analysis_df = pd.read_sql("SELECT date, prcp FROM measurements", con=engine, columns=[["date"],["prcp"]])
m.fit(lr, 1, metrics=[exp_rmspe])
run txt2pdf.py -o '2018-06-22  2013 872 discharges.pdf'  '2018-06-22  2013 872 discharges.txt'
help(requests.post)
data = drive.CreateFile({'id': '1iviqKhwsy_q3-SZ4WUlGDSOIRpzluf1I'}) $ data.GetContentFile('training_car_x_y_train.csv') $ test_data = drive.CreateFile({'id': '1Lxt5lvzbYFWSHrdtwNFoH_5n_-OEYcoK'}) $ test_data.GetContentFile('test_car_x_test.csv') $
red_4.isna().sum()
from datetime import datetime $ x = [datetime.strptime(d, '%Y-%m-%d %H:%M:%S.%f') for d in df['time']] $ y = df['moisture']
results = lrm.fit() $ results.summary()
reddit['Evening/Night Hours'] = reddit['Hours'].apply(lambda x: 1 if x<10 and x>5 else 0)
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=2500) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
b = np.array([[1,2,3], $               [4,5,6], $               [7,8,9]]) $ b
data.loc[data['affairs']<=1]
gender_top_school_tab = pd.crosstab(gender_top_school.candidate_gender, gender_top_school.is_top, margins=False) $ gender_top_school_tab
breed_predict_df.sample(20)
mumbai_data = df.loc[df.location.str.contains('umb'),:] $ print(mumbai_data.head(2))
ac['Monitoring End'].groupby([ac['Monitoring End'].dt.year]).agg('count')
pd.pivot_table(train, index=['srch_destination_id'], columns=['hotel_cluster'], values='cnt', aggfunc='count', fill_value=0) $
np.random.seed(123456) $ ps = pd.Series(np.random.randn(12),mp2013) $ ps
print(trump.axes) $ print(trump.shape)
ffr.index.year
dep.add_account(yah) $ dep.add_account(stx) $ dep.account_infos
pickle.dump(skills, open('./data/skills.pickle.dat', 'wb'))
feature_layer.properties.drawingInfo.renderer.type
events.category.iloc[0]
afl_data.tail(3)
base_hems_url= "https://astrogeology.usgs.gov" $ links[0].attrs.get('href') $ links_visit = ["https://astrogeology.usgs.gov" + x.attrs.get('href') for x in links] $ links_visit $
for x in links: $     print(base_hems_url + x.attrs.get('href')) $
review = !cat {TRN}{trn_files[6]} $ review[0]
tweets['text'][0]
df_data.head()
autos['unrepaired_damage'].value_counts()
newdf = newdf.join(ff3, how='inner')
data["Tweets"].duplicated().sum() $ duplicates = data.loc[data["Tweets"].duplicated(keep = 'first'), :] $ data = data.drop_duplicates(['Tweets'], keep = 'first') $ data = data.reset_index(drop=True)
df_congress = ux.datasets.load_congress_twitter_links() $ print(f'The dataset has {len(df_congress)} rows') $ df_congress.tail(2)
pd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])
df['Complaint Type'].resample('M').count().plot()
tweet_data['rating'] = tweet_data['rating_numerator'] / tweet_data['rating_denominator']
j = tweets["date"].iloc[2] $ terror["type"] = terror["type"].fillna("Unkown")
print len(ind['AFFGEOID'].unique()) $ print len(noise_graf['AFFGEOID'].unique()) $ print len(noise_graf)
vow[['Open', 'High', 'Low', 'Close']].plot()
lr_model_saga = LogisticRegression(C=0.1, class_weight=None, max_iter=125, solver='saga')
ether = df[df['tweet'].str.contains("Ethereum") | df['tweet'].str.contains("eth") | df['tweet'].str.contains("ETH")] $ ether = ether.reset_index(drop=True) $ ether.info()
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=32000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
january17_jail_census.head()
N_new = df2.query('landing_page == "new_page"')['user_id'].nunique() $ N_new 
dfWords.to_pickle(data + "dfWordsRegMet.p") $
(autos["ad_crawled"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_values() $         )
import pandas as pd
Raw_Forecast.set_index("Date_Monday").groupby( $     [pd.TimeGrouper('M'), "Product_Motor", "Part_Number"]).sum().fillna(0)[["Qty"]]
props.prop_name.value_counts().reset_index()
crimeData.to_csv('/home/hoona/Python/mpug/checkIndexIsWritten.csv')
len(Data.groupby('to_account')['deposited', 'withdrawn'].sum())
session.query(func.count(weather.id)).scalar()
day_of_year14["avg_day_of_month"] = day_of_year14.index.map(lambda x: int(x.split("/")[1])) $ day_of_year14.head()
df_sale_price =df_sale_price.transpose()          ##Transposing the dataframe in order to better analyze the data $ df_sale_price =df_sale_price.iloc[5:,]            ##removing unnecessary columns $ df_sale_price.reset_index(level=0,inplace=True) $ df_sale_price.columns=['Date','Sale_Price']    ##renaming the columns to more meaningful names $ df_sale_price.head(5)
from pandas.tseries.offsets import Hour, Minute $ from datetime import timedelta
weather_yvr.plot?
twitter_archive.rating_numerator.value_counts()
if len(df_payments['amountpaid']) >0: $     total_earned=df_payments['amountpaid'].values.sum() $     'US$ {:,.2f}'.format(float(total_earned)) $ else: $     print("Unfortunately, there is no data available!")
import pandas $ import pandas_datareader $ import datetime as dt
fb_vec = cv.fit_transform([ftfy.ftfy(fb_cleaned)])
meal_inferred_types_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/meal_inferred_types.csv')['Body'].read().decode('utf-8') $ meal_inferred_types = pd.read_csv(StringIO(meal_inferred_types_csv_string), header=0, delimiter='|')
new_page_converted = np.random.choice([0, 1], size = n_new, p = [(1 - p_new), p_new])
model = keras.models.load_model("/home/kmisiunas/Documents/Quipu/models/binding_metric971_2018-04-03_no1.h5")
plt.grid() $ plt.bar(x,Y) $ plt.plot(x,Y) $
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=58000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
print('RF score: {:.4f}'.format(scores))
goog.describe()
df_goog.head()
sns.regplot(x=final["ETH Price"], y=final['Crypto Compound'], fit_reg=False) $ sns.regplot(x=final["ETH Price"], y=final['Crypto Negative'], fit_reg=False, color = 'r') $ sns.regplot(x=final["ETH Price"], y=final['Crypto Positive'], fit_reg=False, color = 'g').invert_yaxis()
X_train.shape, X_valid.shape, X_test.shape
plan_repaid.columns
cohort_churned_df = pd.DataFrame(index=daterange,columns=daterange).fillna(0)
df_5_3 = pd.concat([df_5_apply, df_5_2], axis=1).apply(lambda x: x/x['applicant_id'], axis=1).drop('applicant_id', axis=1) $ df_5_3.columns = ['% submit in {}th week'.format(int(i)) for i in df_5_3.columns] $ df_5_3 = pd.concat([df_5_apply.rename(columns={'applicant_id': '# apply'}), df_5_3], axis=1) $ df_5_3.head(10)
cancel_count_dfs = perspective_time_dfs.copy()
MAX_NB_WORDS = 200000 $ MAX_SEQUENCE_LENGTH = 25 $ EMBEDDING_DIM = 300
dupl_row = df2_duplicated_row.index.values # retrieving index of duplicated row $ df2.drop(dupl_row,inplace=True) # dropping duplicated row from df2
columns_to_merge = tweet_json[['id','retweet_count','favorite_count']].copy() $ twitter_archive_clean = twitter_archive_clean.merge(right=columns_to_merge,how='left',left_on='tweet_id',right_on='id') $ twitter_archive_clean = twitter_archive_clean.drop(columns='id')
fte_approvals = pd.read_csv('https://projects.fivethirtyeight.com/trump-approval-data/approval_topline.csv',parse_dates=['modeldate']) $ fte_voters = fte_approvals[fte_approvals['subgroup'] == 'Voters'] $ fte_approvals.head() $
from keras.utils import np_utils $ y_label_train_OneHot = np_utils.to_categorical(y_resampled) $ y_label_test_OneHot = np_utils.to_categorical(Vycnn,4) $ y_label_test_OneHot.shape
data['Processing Time'] = data['Closed Date'].subtract(data['Created Date']) $ data['Processing Time'].describe()
jobPostDF_B.head(5)
kimanalysis.getfile(model, 'kimspec.edn', contentformat='edn')
(~autos["price"].between(1,350000)).sum() / autos.shape[0]
msftAC.tail(5), shifted_forward.tail(5)
df2_treat = df2.query('group == "treatment"') $ df2_treat_conv = df2_treat[df2_treat['converted'] == 1].count() $ total = df2_treat['converted'].count() $ prop_treat = (df2_treat_conv['converted'] / total) $ print(prop_treat)                   $
dfList.head()
tweets['retweeted'].value_counts()
ttTimeEntry['DT'] = ttTimeEntry['DATE']+' '+ttTimeEntry['TIME']
df_twitter = pd.read_csv('data/twitter-archive-enhanced.csv') $ df_twitter_copy = df_twitter.copy() $ df_twitter_copy.head()
temps_df.Missouri
style_bw.head(5)
df = item.to_pandas() $ df.tail()
import pandas as pd $ import numpy as np $
def drop_constraint(cur, table_name, constraint_name): $     cur.execute(sql)
segments.st_time.dtype $ datetime.strptime(segments.st_time.loc[0],'%m/%d/%y %H:%M')
_ = ok.grade('q05b') $ _ = ok.backup()
autos = autos[autos["price_dollars"].between(1,4000000)]
df.head()
from skmultilearn.problem_transform import BinaryRelevance $ from sklearn.naive_bayes import GaussianNB $ classifier = BinaryRelevance(GaussianNB()) $ classifier.fit(X_tr[0:len(X_train)-40-1], y_ls) $ classifier.score(X_tr[0:len(X_train)-40-1], y_ls)
X = Feature $ X[0:5]
f
ttarc_clean['floofer'] = ttarc_clean['floofer'].map({'floofer': 4, 'None': 0}) $ ttarc_clean['doggo'] = ttarc_clean['doggo'].map({'doggo': 3, 'None': 0}) $ ttarc_clean['pupper'] = ttarc_clean['pupper'].map({'pupper': 2, 'None': 0}) $ ttarc_clean['puppo'] = ttarc_clean['puppo'].map({'puppo': 1, 'None': 0})
print(samples_query)
subway3_df[['datetime','Hourly_Entries', 'Hourly_Exits']].sample(50) #this is what clued me in to the error in how the function was parsing ridership.
weather.to_csv('data/crime/weather.csv')
label_and_pred = rfModel.transform(testData).select('label', 'prediction') $ label_and_pred.rdd.zipWithIndex().countByKey() $
df_2 = pd.read_csv('./scraping_results.csv')
for id_never_moved in Del_list: $     df_new = df[df['id'] == id_never_moved] $     frames = [df_never_moved, df_new] $     df_never_moved = pd.concat(frames)
Probas = pd.DataFrame(estimator.predict_proba(X1), columns=["Proba_Una", "Proba_Amanda"])
df.select('longitude').distinct().sort('longitude', ascending=True).show(10)
weather_mean.values
explode = [0.1,0,0] $ colors = ["gold", "lightblue", "lightcoral"] $ labels = ["Urban", "Suburban","Rural"]
clustered_hashtags.filter(clustered_hashtags.hashtag == 'brexit').show()
top_5_percent = autos_pr['brand'].value_counts()[autos_pr['brand'].value_counts(normalize = True) > 0.05]
lr2 = LogisticRegression(random_state=20, max_iter=10000, C= 1, multi_class='ovr', solver='saga') $ lr2.fit(X_tfidf, y_tfidf)
tweet_archive_enhanced_clean['expanded_urls'] = "https://twitter.com/dog_rates/status/" + tweet_archive_enhanced_clean['tweet_id'].astype(str)
f = netCDF4.Dataset('../OISST/OISST_1982-2010.nc', 'r') $ print(f) 
(final_rf_predictions['predict']==test['Cover_Type']).as_data_frame(use_pandas=True).mean()
probs_test[:, 1].mean()
c[c['days_to_next_churn'] < 30]
plt.hist(p_diffs); $ plt.xlabel('difference'); $ plt.ylabel('frequency'); $ plt.title('Simulated differences in conversion rate under the null');
autos['registration_year'].describe()
my_data = np.array([['','Column1','Column2','Column2'], $                 ['Row1',1,2,3], $                 ['Row2',4,5,6]]) $ my_data
model_df['target'] = model_df.close.shift(-1) $ model_df['next_day_open'] = model_df.open.shift(-1) $ model_df['true_grow'] = model_df[['target', 'next_day_open']].apply(lambda x: 1 if x[0] - x[1] >= 0 else 0, axis=1)
from sklearn.neighbors import KNeighborsRegressor $ neigh = KNeighborsRegressor(n_neighbors=30) $ x = np.float32(x) $ x = x.reshape((len(x),1)) $ neigh.fit(x,y) 
dfEtiquetas.dropna(subset=["created_time"], inplace=True)
print('UPDATED DATAFRAME, QUESTION 3 FINDINGS:') $ print('- The number of unique user_ids in df2 is {}.'.format(df2_unique)) $ print('- The repeated user_id was {}'.format(user_repeated))
train.pivot_table(values = 'Fare', index = 'Gender', aggfunc=np.mean)
injuries_hour.rename(columns={'NUMBER OF PERSONS INJURED':'injuries'},inplace=True)
b_cal_q1.loc[:,'price'] = new_price
cov_df = factor_ts.rolling(250).cov().dropna()
rfc_features = sorted(list(zip(test_features, rfc.feature_importances_)), key=lambda x: x[1], reverse=True) $ rfc_features
import requests $ resp = requests.get('http://www.elpais.com/') $ resp.content[:500]
to_be_predicted_Day2 = 50.82048625 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
(p_diffs > (p_treatment_convert - p_control_convert)).mean()
(np.asarray(p_diffs) > obs_diff).mean()
avg_day_of_month14 = day_of_year14.groupby("avg_day_of_month").mean() $ avg_day_of_month14.head()
preds, model = runXGB(train_X, train_y, test_X, num_rounds=400) $ out_df = pd.DataFrame(preds) $ out_df.columns = ["high", "medium", "low"] $ out_df["listing_id"] = test_df.listing_id.values $ out_df.to_csv("data/prediction.csv", index=False)
twitter_final['date'] = pd.to_datetime(twitter_final['date'])
dfM['t+1'] = dfM['COUNT']  #X $ dfM['t'] = dfM['COUNT_s1']  #y
trainheadlines = train["text"].values $ advancedvectorizer = CountVectorizer(ngram_range=(2,2)) $ advancedtrain = advancedvectorizer.fit_transform(trainheadlines)
df['fruits'][0]
owns = repos[['id', 'owner_id', 'forked_from', 'created_at']] $ owns = owns.rename(columns={'id': 'repo_id', 'owner_id': 'user_id', 'forked_from': 'forked_from_repo_id'}) $ owns['owned'] = 4
temp = ['low','high','medium','high','high','low','medium','medium','high'] $ temp_cat = pd.Series(temp, dtype='category') $ print (type(temp_cat))       # Series object $ print (type(temp_cat.cat))   # Categorical Accessor
print(largest_change)
df.dropna(axis = 0, inplace = True) $ df.reset_index(inplace=True, drop=True) $ df.shape
df_users_6_after=df_users_6[df_users_6['created']>='2017-07-01']
set_themes = legos['sets'].merge(legos['themes'], left_on = 'theme_id', right_on = 'id') \ $     .merge(legos['inventories'], on = 'set_num') \ $     .merge(legos['inventory_parts'], left_on = 'id_y', right_on = 'inventory_id')
len([x for x in eligible_posts if x['previous.posts']==0]) / len(eligible_posts)
api_copy.rename(columns = {'id': 'tweet_id'} ,  inplace= True) $ api_copy['tweet_id'] = api_copy['tweet_id']. astype('str') $ type(archive_copy['tweet_id'].iloc[0]) $
obs_diff = new_conv_rate - old_conv_rate $ (p_diffs > obs_diff).mean()
df[1:1]
twitter_archive_master.info()
run txt2pdf.py -o"2018-06-19 2012 FLORIDA HOSPITAL Sorted by payments.pdf"  "2018-06-19 2012 FLORIDA HOSPITAL Sorted by payments.txt"
df3[['wk1', 'wk2', 'wk3']] = pd.get_dummies(df3['period']) $ df3.head() $
ratings = (spark.read.format("csv") $ .options(header = True, inferSchema = True) $ .load(home_dir + "ratings.csv") $ .persist())
%matplotlib inline
pd.set_option('display.max_colwidth', -1) $ df.loc[df.Sentiment==1, ['description','Sentiment']].head(10)
combined_city_df = pd.merge(city_data_df, ride_data_df, $                                  how='outer', on='city') $ combined_city_df.head(5)
new_page_converted = np.random.choice([0, 1], size=n_new, p=[1 - p_new, p_new])
def Describe_Data(dataframe): $     return dataframe.describe()
df["ARRIVIAL_DELAYED"] =  df["ARR_DELAY"].apply(lambda x: "YES" if x > 8 else "NO")
import numpy as np $ a = np.array([1, 2, 3, 4]) $ a.dtype
users_visits.sort_values('visits', ascending=False).head()
def calculate_cumulative_returns(returns): $     cumulative_returns = (returns.sum(axis=1)+1).cumprod() $     return cumulative_returns $ project_tests.test_calculate_cumulative_returns(calculate_cumulative_returns)
s = pd.Series([1,3,5,np.nan,6,8]) $ print(s)
y_preds = knn_grid.best_estimator_.predict(X_test) $ knn_scores = show_model_metrics('K-NN', knn_grid, y_test, y_preds)
lq.head(1)
control_converting = df2[df2['group'] == 'control']['converted'].mean() $ print('Probability of an individual in the control group converting:') $ print(control_converting)
%cd ../../ $ %load_ext autoreload $ %autoreload 2
cal_map = { $     1:1, # Keep 1 as meaning "calibrated" $     2:0 # Set 2 to 0 to mean "not calibrated" $ } $ pax_raw['paxcal'] = pax_raw.paxcal.map(cal_map)
list_users = df2['user_id'].value_counts() $ list_users.head()
N_old = df2.query('landing_page == "old_page"')['user_id'].nunique() $ N_old 
len(reddit_df)
user_ips = log_data \ $     .map(lambda line: line.split()) \ $     .map(lambda words: (words[IP_ADDRESS],words[USER_ID])) \ $     .groupByKey()
den = temp_long_df[(temp_long_df['grid_id'] == '965')]
%matplotlib inline
df.columns
df4 = df3.reindex(index=d[0:4], columns=list(df.columns) + ['E']) $ df4.loc[d[0]:d[1],'E'] = 1 $ df4
for topic_idx, topic in enumerate(nmf.components_): $     print "Topic %d:" % (topic_idx) $     print " ".join([tfidf_feats[i] $                     for i in topic.argsort()[:-10 - 1:-1]])
posts_by_sampled_authors_saved = non_blocking_df_save_or_load( $     posts_by_sampled_authors, "{0}/posts_by_sampled_authors_5".format(fs_prefix)).alias("posts_by_sampled_authors")
print(df2.query('converted==1').converted.count()) $ print(df2.converted.count()) $ print(df2.query('converted==1').converted.count()/df2.converted.count())
new_df['movieId']=new_df['movieId'].apply(int)
df_a.join(df_b, how = "right") # right join (see above for definition)
import dask $ import dask.dataframe as dd $ from dask import delayed, compute $ df_csv = dd.read_csv('../datasets/CSVs/1500000 Sales Records.csv') $ df_csv.head()
len(new_cols)
USvideos.info()
GA_profit['Estimated Build Cost'] = build_cost_GA[' Estimated Build Cost '] $ GA_profit['Profit Including Build Cost'] = GA_profit['X36 NPV'] - GA_profit['Estimated Build Cost'] $ GA_profit['Number of Accounts'] = GA_accounts['Account ID']
train.head()
df2['intercept'] = 1 $ df2[['control','treatment']] =  pd.get_dummies(df2['group'])
cv_title_model(cvec_4, lr)
dates=pd.date_range('1/Oct/2020', '5/Oct/2020') $ print(dates)
df_archive_clean["doggo"].value_counts()
pd.DataFrame(data={'goles_a_favor': goles_favor, $               'goles_en_contra': goles_contra} $             )
import pandas as pd $ import matplotlib.pyplot as plt $ import matplotlib.ticker as ticker $ import os $ %matplotlib notebook
df = df.join(d, how='outer')
TestData[['DOB_clean', 'Lead_Creation_Date_clean']].describe()
n_old = df2.query('landing_page == "old_page"').shape[0] $ print('Number of occurrences for landing_page == "old_page": {}'.format(n_old))
(list_of_sites)
tweet = 'I have lived in China all my life, but I was born in Lisbon, Portugal' $ print("{} :  {}".format(color_formatting(ner.is_tweet_about_country(tweet, 'PT')), tweet)) $ print("\t\t| \n\t\t|-> {}".format(ner.get_countries_from_content(tweet)) )
prob_new_page = df2.query('landing_page== "new_page"')['user_id'].count()/df2.shape[0] $ print("Probability of individual receiving new page is :", prob_new_page)
tscv = model_selection.TimeSeriesSplit(n_splits=4) $ tscv_cv = tscv.split(train)
print (repeated_user) $ print (repeated_user.index)
actual_diff = p_treatment - p_control $ (p_diffs > actual_diff).mean()
closed_pr = PullRequests(github_index).get_average("time_to_close_days")\ $                                       .is_closed()\ $                                       .since(start=start_date).until(end=end_date)\ $                                       .by_period() $ print(get_timeseries(closed_pr, dataframe=True).tail())
orders.write.mode("overwrite").saveAsTable("orders")
df.to_csv('data/dibeBot_ff.csv',index=False)
TEXT.vocab.itos[2]
datAll['year'] = datAll['Date'].map(lambda x: x.year) $ datAll['month'] = datAll['Date'].map(lambda x: x.month) $
testing = df.text[:100]
filt_dt=pd_aux2.issued_date.isna() $ pd_aux2['new_date']=pd_aux2['issued_date'] $ pd_aux2.loc[filt_dt,['new_date']]=pd_aux2.permit_creation_date[filt_dt] $ pd_aux2.head(10)
extended_tweets.isnull().sum()
vocab = set(tokens) $ vocab = pd.Series(range(len(vocab)), index=vocab) $ vocab
print 'After dropping the anonymous donor, total amounts from the unknown state as a percentage of all amounts is: '\ $     , thousands_sep(100*df[(df.state == 'YY')].amount.sum()/df.amount.sum()), '%'
df[df['Promo2SinceYear'] == 1900].head()
def get_sparse_matrix(trainIndices, valIndices): $     train_word_features = char_vectorizer.transform(train.loc[trainIndices, 'comment']) $     val_word_features = char_vectorizer.transform(train.loc[valIndices, 'comment']) $     return (train_word_features, val_word_features)
master_file['FILENAME'].value_counts()
ride_percity=original_merged_table["type"].value_counts() $
session.query(Measurements.station, func.avg(Measurements.prcp) ).\ $         filter(Measurements.date >= Pre_start_date).\ $            filter(Measurements.date <= Pre_end_date).\ $         group_by(Measurements.station).all() $
df.select(a).show(5)
cand_date_df = cand_date_df[cand_date_df['sponsor_class'] != 'Other'] $ cand_date_df.head(2)
fine_xs = xs_library[fuel_cell.id]['transport'] $ condensed_xs = fine_xs.get_condensed_xs(coarse_groups)
time2 = time_utc - start_time $ time2
z4
keyPop = 'B00001_001E' $ url = "https://api.census.gov/data/2016/acs/acs1?get=" + keyPop +\ $ ",NAME&for=public%20use%20microdata%20area:*&in=state:36&key=" + myAPI $ resp = requests.request('GET', url).content $ pumaPop = pd.read_csv(io.StringIO(resp.decode('utf-8').replace('[','').replace(']','')))
df = df.sort_values(by="text") $ vader_df = vader_df.sort_values(by="text") $ df = df.reset_index(drop=True) $ vader_df = vader_df.reset_index(drop=True) $ df.head()
df2_tr=df2[df2['group']=="treatment"] $ prob2 = df2_tr[df2_tr['converted']==1].shape[0]/df2_tr.shape[0] $ prob2
act_diff = df2[df2['group']=='treatment']['converted'].mean()- df2[df2['group']=='control']['converted'].mean() $ act_diff
zipcodesdetail = pd.read_csv('in/zip_code_database.csv')
df['time_detained'].head()
control_df=df2.query('group=="control"') $ control_df.head()
tweet_df_clean.info()
test_data.loc[(test_data.gearbox == 'manuell') & (test_data.vehicleType.isnull()), 'vehicleType'] = 'kleinwagen'
api.get_data(a=1)
wo_rt_df = en_tweets_df[~en_tweets_df['full_text'].str.contains('RT @')]
trump = pd.read_json("TrumpTweets_061615-010518.json")
crimes.groupby(['PRIMARY_DESCRIPTION', 'SECONDARY_DESCRIPTION']).size()
LARGE_GRID.plot_accuracy_be(raw_large_grid_df)+ggtitle("Median-Block of Mean-Element Accuracy")
bnb.shape
toy = df.sample(frac=0.00005)
to_be_predicted_Day4 = 26.71780383 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
cntry = dmatrix("country_destination", age_gender_bkts, return_type = 'dataframe')
git_timed = git_log.set_index('timestamp').author $ git_timed.head()
set(clean_rates.name.sample(20).tolist())
joined_sampled_and_infered = sampled_contirbutors_human_agg_by_gender_and_proj_df.join( $     relevant_info_agg_by_gender_and_proj_df, $     on="project")
plt.hist(review_length, bins =100) $ plt.show() # most reviews are short, only few reviews are very long.
df2['zeros']=0  # created a column of zeros with zero values $ df2.head()
def aggregated_prediction(name,adr,st): $     return (4.3*name + 1.2*adr) $ aggregated_prediction_udf = F.udf(aggregated_prediction,types.DoubleType()) $ join_c = join_b.withColumn("aggregated_prediction", aggregated_prediction_udf(join_b['name_similairity'],join_b['address_similairity'],join_b['st_name_similairity']))#.select(['endcustomerlinefixed_data','predicted'])
engine.make_classifier("nhtsa_classifier", $                        "SingleTag", $                        "SimpleFreqDist", $                        "NaiveBayesClassifier")
iplot({'data': [{'type': 'scatter', $                  'mode': 'lines+markers', $                  'x' : [0,2,4,6], $                  'y' : [2,5,5,2]}]})
print(autodf.groupby('offerType').size()) $ autodf = autodf[autodf['offerType'] != 'Gesuch'] $ autodf = autodf.drop('offerType',axis = 1) $ print( "\nSize of the dataset - " + str(len(autodf)))
X_train, X_test, y_train, y_test = train_test_split(X_d, y, test_size=0.5, random_state=42) $
images_df.describe(include="all")
house_db = [] $ for i in range(125): $     db = build_house_database(house_data.iloc[i],i) $     house_db.append(db)
print(classification_report(yy_test, dtmodel_predictionsX))
alg5 = GaussianNB() $ alg5.fit(X_train, y_train) $ probs = alg5.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
sm.qqplot(price_mm, loc=price_mm.mean(), scale=price_mm.std())
plt.hist(p_diffs, alpha = 0.5)
df_birth.info()
data.info()
df = pd.read_sql_query("select count(*) as cantidad, application.site_id from tracks where ds >= '2018-05-13' and ds < '2018-05-14' and path='/checkout/congrats' and application.site_id in ('MLA', 'MLC', 'MLU', 'MCO', 'MLM') group by application.site_id", con)
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4) $ print ('Train set:', X_train.shape,  y_train.shape) $ print ('Test set:', X_test.shape,  y_test.shape)
training_df.tail()
mailing_list_posts_mbox_df_saved = non_blocking_df_save_or_load( $     mailing_list_posts_mbox_df, $     "{0}/processed_mbox_data_4".format(fs_prefix))
survey = pd.read_csv("./survey.csv")
import multiprocessing $ cores = multiprocessing.cpu_count() $ print(cores) $ assert gensim.models.word2vec.FAST_VERSION > -1, "This will be painfully slow otherwise"
df_clean.info()
p_old = df2[(df2.group == 'control')&(df2.converted == 1)].shape[0]/df2[df2.group == 'control'].shape[0] $ p_old
result['timestampCorrected'].isnull().sum()
s2.value_counts()
merged1.index
features = pd.merge(features, features_rolling_averages, on=['game', 'team'])
vals2 = np.array([1, np.nan, 3, 4]) $ vals2.dtype
hp
df.groupby('episode_id').location_id.shift().fillna(0).astype('int64').head()
high_rev_acc_opps_net.rename(columns={'Building ID': 'Total Buildings'}, inplace=True)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(classifier.score(X_test, y_test)))
df.shape
pd.io.json.json_normalize(list(c.find({}, {'_id': 0})))
autos["date_crawled"].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True)
rule_basesd_matching_url = 'https://spacy.io/usage/linguistic-features#rule-based-matching' $ iframe = '<iframe src={} width=1000 height=700></iframe>'.format(rule_basesd_matching_url) $ HTML(iframe)
sns.barplot(x="alone", y="survived", hue="sex", data=titanic)
asf_people_human_df = non_blocking_df_save_or_load( $     rewrite_human_data(asf_people_human_raw_df), $     "{0}/human_data_cleaned/asf_people".format(fs_prefix)) 
pop_df = pd.DataFrame({'total': pop, $                       'under18': [2346345,234245234, $                                  36341435,23452453, $                                  234234235,34563453]}) $ pop_df
weather_all.index
articles
pyLDAvis.enable_notebook()
to_be_predicted_Day2 = 22.24028033 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
val_utf8 = val.encode('utf-8') $ val_utf8 $ type(val_utf8)
df_new['intercept_country'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['intercept_country','US', 'UK']]) $ results = log_mod.fit() $ results.summary()
new_page_converted = np.random.choice([1, 0], size=nnew, p=[np.mean([pnew, pold]), (1-np.mean([pnew, pold]))]).mean() $ print(new_page_converted)
tweets_df.in_reply_to_status_id.describe()
stock.head()
df_bill_data= df_bill_data.drop('bill_id', axis=1) $ df_bill_data = df_bill_data.groupby(['patient_id','date_of_admission']).sum().reset_index() $ df_bill_data.to_csv('df_bill_data.csv')
print df.shape[0] + noloc_df.shape[0]
siteInfo = siteInfo.query('Range >= 10 & EndYear >= 2017') $ siteInfo
In this case, the p value obtained from the ztest and hypothesis testing comes to .90. whereas in the Logistic regression we can see $ the p value of 0.19. Still the p value is more and is not statistically significant to predict the value of convertion
sl[sl.status_binary==0].groupby('wpdx_id')[['status_binary','today_preds','one_year_preds','five_year_preds']].sum().sum()
data['Gender'].value_counts()
df_ad_airings_5['location'][0].split(",")[0]
names2 = ['Date','South West Pacific','South East Pacific','PNG/Solomon Islands','Vanuatu','Vanuatu - Gaua','Vanuatu - Ambrym','Vanuatu - Ambae','Vanuatu - Tanna','New Caledonia','Samoa','Raoul/Curtis','Society Islands','New Zealand']
df = df[["message","message_likes_dummy"]]
diff = new_page_converted.mean() - old_page_converted.mean() $ diff
from bokeh.io import show, output_notebook $ from bokeh.models import ColumnDataSource $ from bokeh.plotting import figure $ from bokeh.transform import jitter $ output_notebook()
np.*load*?
job_a_requirements = pd.DataFrame(requirements, columns = ['Job_A']) $ job_a_requirements
size_t = df2.query("group=='treatment'").count()[0] $ size_t
plt.plot(data['time_step'][0:800],data['valuea'][0:800])
doctors_trend = fit_linear_trend(doc_duration) $ doctors_detrended = doc_duration - doctors_trend
morning_rush.iloc[:5]
df = df.loc[(df['Direction'] == 1)]
g = g[['Unique Key', 'Created Date', 'X Coordinate (State Plane)', 'Y Coordinate (State Plane)']]
df.head(2)
Show_Row(election_data, election_data['st'], "DC", list1) $
df.info()
ridgereg = Ridge(alpha=0, normalize=True) $ ridgereg.fit(X_train, y_train) $ y_pred = ridgereg.predict(X_test) $ print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
overdue = data_full.query('timeliness_overdue == 1 & paid_status_PAID == 1')
df_twitter_copy.loc[707].text
Describe_Data(election_data)
import numpy as np $ da['Sentiment'] = np.array(labels)
northern_sea_level = pd.read_table("http://sealevel.colorado.edu/files/current/sl_nh.txt", $                                    sep="\s+") $ northern_sea_level
len(label_index)
for tweet in DataSet['tweetText']: $     if "egg"in tweet: $         print(tweet)
pd.date_range(start, end, freq='W')
df_new.country.unique() $ df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new['intercept'] = 1 $ df_new.head()
actual=test_y[0:203] $ actuals_dataframe=pd.DataFrame(actual,columns=['actual']) $ actuals_dataframe.shape
tot_stations = session.query(station.station).filter(station.station == 'USC00519397').count() $ tot_stations $
suspects_with_25_2['timestamp'] = pd.to_datetime(suspects_with_25_2['timestamp'])
df_tweet.info()
from patsy import dmatrices $ from statsmodels.stats.outliers_influence import variance_inflation_factor
first_row = session.query(Measurement).first() $ first_row.__dict__
r = q_mine.results() $ r
Train.DOB_clean.value_counts()[1]
tweet_counts_by_month.tweet_count.plot()
import pandas as pd $ import numpy as np
appointments['Specialty'].unique()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) $ model.fit(X, y, epochs=500, verbose=2)
import statsmodels.api as sms
a.strip('"<>/a').split('=')[-1].split('">')
data = ['peter', 'Paul', None, 'MARY', 'gUDIO'] $ [s.capitalize() for s in data]
tweet_archive_clean['rating_denominator'].value_counts()
test_kyo1 = tweets_kyoto_filter[:31]
new_page_converted = np.random.binomial(1, p_new,size = n_new)
for num, sentence in enumerate(parsed_review.sents): $     print 'Sentence {}:'.format(num + 1) $     print sentence $     print ''
(np.array(null_vals) > p_diff_act).mean()
cityID = '300bcc6e23a88361' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Seattle.append(tweet) 
df2['tripDay'] = df2['tripduration'][(df2['DN'] == "D")] $ df2['tripNight'] = df2['tripduration'][(df2['DN'] == "N")] $ df2.head()
len(df)
df2[df2.duplicated(['user_id'], keep=False)]
full_globe_temp.plot()
pizza_poor_corpus, pizza_poor_train_model, pizza_poor_dictionary = create_LDA_model(pizza_poor_train, $                                                                                     'bad_reviews_token', 10, 40)
people_person['date_joined'] = pd.to_datetime(people_person['date_joined'])
df = pd.read_json('realDonaldTrump.jsonl', lines=True)
import json $ import requests
import seaborn as sns $ sns.lmplot(x='TV',y='sales',data=data,fit_reg=True)
def compute_cook_days_on_platform(row): $     if (row['meal_date'] - row['cook_joined_date']).days < 0: $         return None $     else: $         return (row['meal_date'] - row['cook_joined_date']).days
loans_df.emp_length.value_counts()
station_cluster = (turnstile_cluster $                      .groupby(['STATION','DATE'])['DAILY_ENTRY'] $                      .sum() $                      .reset_index() $                      )
orgs.dtypes
reddit_df.head()
rf = RandomForestClassifier(n_estimators=50, random_state=42) $ scores = cross_val_score(rf, X, y, cv=skf) $ print "Cross-validated RF scores based on subreddit:", scores $ print "Mean score:", scores.mean()
session.query(measurement.date).order_by(measurement.date.desc()).first()
chk = joined.loc[joined['city_hol']==1]
tsvData=autoData.map(lambda x : x.replace(",","\t")) $ print (autoData.take(5)) $ print ("\n") $ print (tsvData.take(5)) $
res = joinresult.set_index(['Name', 'BARCODE']) $ print(res)
df.isnull().sum() #there are no missing values
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
pd.set_option('max_seq_items', 200)
p + np.timedelta64(7200, 's')
dflunchbreak = pd.merge(dfstationmeanlunch[['STATION','totals']], dfbrexits, on='STATION') $ dflunchbreak.columns = ['STATION','lunch_totals', 'breakfast_exits'] $ dflunchbreak.head(2)
extract_answer_only_stractural_features(posts_df)
historicFollowers.head()
daily.rolling(50, center=True, $              win_type='gaussian').sum(std=10).plot(style=[':', '--', '-'])
tol_num_stations = session.query(measurement).group_by(measurement.station).count() $ tol_num_stations $
data.groupby(['Name'])['Salary'].sum()
git_log.head(10)
X_train_matrix = cvec.fit_transform(X_train)
churned_unordered_end_date = [churned_unordered.loc[bid,'canceled_at'][np.argmax(churned_unordered.loc[bid,'scns_created'])] if churned_unordered.loc[bid,'cancel_at_period_end'][np.argmax(churned_unordered.loc[bid,'scns_created'])] == False else churned_unordered.loc[bid,'current_period_end'][np.argmax(churned_unordered.loc[bid,'scns_created'])] if churned_unordered.loc[bid,'cancel_at_period_end'][np.argmax(churned_unordered.loc[bid,'scns_created'])]==True else None for bid in churned_unordered.index]
prcp.describe()
bnbB.age.plot.hist()
port_val = predict_allocate_equities(allocs=[0.25,0.25,0.25,0.25]) $ port_val.plot() $ plt.show() $ r= average_daily_return(port_val,portfolio=True) $ print(r)
conv_mean = df2.converted.mean() $ conv_mean
__encode_categorical(df_list=df_list, cat_cols=cat_cols)
df.head()
import pickle $ output = open('votes_by_party.pkl', 'wb') $ pickle.dump(multiple_party_votes_all, output) $ output.close()
properati['zone'] = properati.place_with_parent_names.apply(lambda x : x.split('|')[3])
a = 4 $ b = 5 $ c = 4 $ print(a == b) $ print(a == c)
prng
dfWordsEn = package.run('WordsEn.dprep', dataflow_idx=0) $ dfFirstNames = package.run('FirstNames.dprep', dataflow_idx=0) $ dfBlackListWords = package.run('WordsBlacklist.dprep', dataflow_idx=0)
score = cross_val_score(mnnb, Xs, y, cv = cv, verbose = 1)
arr - arr[0]
total_cost = df.groupby(['uid'])[['total_cost']].agg(['sum','count']) $ total_cost.columns = ['total','number'] $ total_cost = total_cost.dropna() $ total_cost = total_cost[total_cost['total']>0] $ total_cost = total_cost.sort_values(by='total') $
data = [trace] $ py.iplot(data, filename='line')
autos["registration_year"].between(1900,2016).sum()
df_homeless = df[df['Complaint Type'] == 'Homeless Encampment'] $ df_homeless.groupby(df_homeless.index.month)['Created Date'].count().plot(kind="bar") $
autos['price'].value_counts().sort_index( ascending = False).head(20)
lmdict.head()
hist = sns.distplot(senti_vader, bins=20)
X_test = ["this is an innocent comment", "stfu baddie", "shit fuck piss"] # create list of strings to test $ comment_df = pd.DataFrame(data=X_test,columns=["comment_text"]) # create dataframe from this list for later $ X_test=tok.texts_to_sequences(X_test) $ x_test=sequence.pad_sequences(X_test,maxlen=maxlen) $ predictions = model.predict(x_test, batch_size=1024, verbose=1)[0:len(X_test)]
tweet_df.info()
idx + pd.tseries.offsets.Hour(2)
check_all('2017-01-17').head()
type(t1.index)
plot_confusion_matrix(cm,title='Confusion matrix', cmap=plt.cm.Blues)
temp['c'] = temp['contents'].str.split()
all_zeros = 1 - val_y.mean() $ all_zeros
df['Market'].value_counts()
tablename='settings' $ pd.read_csv(read_inserted_table(dumpfile, tablename),delimiter=",",error_bad_lines=False).head(10)
inv=repaid_loans_cash[(repaid_loans_cash.fk_loan==36)].fk_user_investor.unique()
raw.age.head()
df.rename(columns={'85235_00060_00003':'MeanFlow_cfs','85235_00060_00003_cd':'Confidence'},inplace=True) $ df.head()
latest_date
merged_df_flightdelays = pd.concat([merged_df_prec, flight_delays], axis=1)
cityID = 'ab2f2fac83aa388d' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Oakland.append(tweet) 
data = {'Integers': [1, 2, 3], $         'Floats': [4.5, 8.2, 9.6]} $ df = pd.DataFrame(data, index=['label 1', 'label 2', 'label 3']) $ df
multiG = nx.MultiDiGraph() $ loadInNodes(multiG) $ loadInEdges(multiG)
df_enhanced['rating_10_scaled'].dtypes
bad_dul_isbns = pd.read_table("C:/Users/kjthomps/Documents/GOBI holdings reports/IDs to exclude/Duluth invalid ISBNs.txt") $ bad_dul_isbns = bad_dul_isbns['ISBN'] $ bad_dul_isbns.size
closingPrices.tail()
def parse(path): $   g = gzip.open(path, 'r') $   for l in g: $     yield eval(l)
price_series = pd.Series(price_dict) $ odom_series = pd.Series(odom_dict)
df['EVtoEBITDA']= [float(x) for x in df['EVtoEBITDA']] $ df['PricetoCF']= [float(x) for x in df['PricetoCF']] $
autos.tail(3)
etr.fit(X_train, y_train)
tweets.favorite_count
q1d_answer = "There are missing values for the phone numbers, and in violations table, the descriptions are not equally indented" $ display(Markdown(q1d_answer))
plt.figure(figsize=(8, 5)) $ train_df.polling.value_counts().plot.bar(); $ plt.title('Number of #hab by polling') $ plt.xticks(rotation='horizontal'); $
noaa_data.loc[:,'AIR_TEMPERATURE'].groupby(pd.Grouper(freq='W')).mean()
pickle_it(SVC, 'svc_classifier') $ SVC = open_jar('svc_classifier')
n_new = df2.query('landing_page == "new_page"').count()[0] $ print("Number of users with new page :",n_new)
df5.country.value_counts()
df_l2.loc[df_l2["CustID"].isin([customer])]
ssc.start()
df.reset_index(inplace=True) $ df.groupby('dog_type')['timestamp'].describe()
autos["price"].unique()
new_page_converted = np.random.binomial(1, p_new,n_new) $ new_page_converted
rate_change['rating'] = rate_change['rating_numerator']/rate_change['rating_denominator']
df_agg_abuse_rand.head()
_ = df_pilots["created"].plot(kind="density")
gs.best_params_
new_page_converted.converted.mean() - old_page_converted.converted.mean()
h2o.ls()
Duration_Null_Values_Replacement(jobPostDFSample)
12+7-9+1
print(cbow_m1.wv.similarity('men', 'women')) $ print(cbow_m3.wv.similarity('men', 'women')) $ print(cbow_m4.wv.similarity('men', 'women')) $ print(cbow_m5.wv.similarity('men', 'women'))
%%time $ df["sentences"] = df["tweet"].str.lower()
import statsmodels.api as sm  $ logit_model=sm.Logit(Y_train,X_train) $ result=logit_model.fit() $ print(result.summary2());
daily_hashtag = english_df.select( $     functions.month(english_df.date).alias('month'), $     functions.weekofyear(english_df.date).alias('week'), $     functions.dayofyear(english_df.date).alias('day'), $     english_df.hashtag)
import pandas as pd $ df = pd.DataFrame(data,columns=["views","paid_amount","transaction","name",'date',"city","state"]) $ df['date'] = pd.to_datetime(df['date']) $ df.to_csv("I_Paid_Bribe.csv",index=False,encoding='utf-8')
multi_bulk = sess.get_data(['ibm us equity','aa us equity'],['best analyst recs bulk','dvd hist all','px last','px open']) $ multi_bulk
df2[df2['converted'] == 1].user_id.count() / df2.user_id.count()
df_release.isnull().sum()
test['srch_ci'].fillna(test['date_time'],inplace=True) $ test['srch_co'].fillna(test['date_time'],inplace=True)
lr=np.array([1e-4,1e-3,1e-2])
sp.info()
poverty_2011_2015.columns.name=None
model_CSCO.aic
temp_df=data
transactions = pd.DataFrame([2, 4, 5], $                            index=['Mon', 'Wed', 'Thu']) $ customers = pd.DataFrame([2, 2, 3, 2], $                         index=['Sat', 'Mon', 'Tue', 'Thu']) $ transactions / customers
p_diffs = [] $ for i in range(10000): $     new_page_converted = np.random.choice(a, size=n_new, p=ps_new) $     old_page_converted = np.random.choice(a, size=n_old, p=ps_old) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean()) 
df.instagrams
X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid $                                                     , y_train_valid $                                                     , test_size=0.20)
userMerged['createdtm'] = pd.to_datetime(userMerged['createdtm'], format = '%Y-%m-%d %H:%M:%S.%f') $ userMerged = userMerged[userMerged["createdtm"] < startDate] $ print("the latest account was created at:",max(userMerged.createdtm))
number_of_commits = len(git_log) $ number_of_authors = len(git_log["author"].dropna().unique()) $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
dtypes={'date':np.str,'type':np.str,'locale':np.str,'locale_name':np.str,'description':np.str,'transferred':np.bool} $ parse_dates=['date'] $ holidays_events = pd.read_csv('holidays_events.csv', dtype=dtypes, parse_dates=parse_dates) # opens the csv file $ print("Rows and columns:",holidays_events.shape) $ pd.DataFrame.head(holidays_events)
autos['date_created'].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True)
comps_df.drop_duplicates(['entity_uuid','competitor_uuid'],inplace = True)
plt.figure(figsize=(8,6)) $ plt.hist(hm_data.weight, bins=25) $ plt.title('Weight distribution');
dates = pd.read_sql_query('select * from "dimensions_dates"',con=engine)
total_users_control = df2.query('group == "control"')['user_id'].nunique() $ user_control_conv = df2.query('group == "control" & converted == 1')['user_id'].nunique() / total_users_control $ user_control_conv
def day_of_week(date): $     days_of_week = {0: 'monday', 1: 'tuesday', 2: 'wednesday', 3: 'thursday', 4: 'friday', 5: 'saturday', 6: 'sunday'} $     return days_of_week[date.weekday()] $ day_of_week(lesson_date)
hold_mask = df['booking_charge_desc'] == 'HOLD IMMIGRAT'
1/np.exp(-0.0149), np.exp(0.0506), np.exp(0.0408)
temps2  = pd.Series([70,75,83,79,77, 69], index = dates) $ temps_diffs = temps1 - temps2 $ temps_diffs
met.T.plot(kind='barh', figsize=(5,3), xlim=(0,1)) $ met
d.hour
merged.head()
plot_df['WEEK_OF_YEAR'] = plot_df['DATE'].apply(lambda x: x.isocalendar()[1])
y = sales_2015[['store_number']].rename(columns={'sum':'store_number'}) $ y['sale_dollars'] = sales_2015['sale_dollars'] $ y.head()
prob_control = x_control/y_control $ prob_control $
m21 = m[:,1].reshape(5,1) $ m22 = m[:,2].reshape(5,1) $ m2 = np.hstack((m21, m22)) $ m2 = np.array(m2) $ print("m2: ", m2)
user_total = df.nunique()['user_id'] $ print("all the Unique users are : {}".format(user_total))
df.tail(3)
import os $ print(os.listdir('.'))
n_old= df2[df2["landing_page"] == "old_page"].count() $ n_old = n_old[0] $ n_old
g = df_usa['GDP'].values $ p= df_usa['Population'].values $ df_usa['GDP/capita']= g/(p*1000) $ df_usa.round(decimals=2)
pd.Series(np.random.rand(5),index=['a','b','c','d','e'])
engine.execute('Select * from measurements LIMIT 5').fetchall()
hour_of_day14 = uber_14["hour_of_day"].value_counts().sort_index().to_frame()
df[df.index.month.isin([6,7,8])].head()
group_time = sales_agg[['Store Number','month','Sale (Dollars)','Date']] $ group_time_2015 = group_time[group_time['Date'].dt.year == 2015] $ over_time_2015 = group_time_2015.groupby(['Store Number','month']).sum() $ over_time_2015.head(1)
data.loc[data['hired']==1].groupby('category').hourly_rate.std()
result['ownerId'].nunique()
model_avg
ab_df2.isna().sum()
sess.get_historical_data('ibm us equity','g:ohlc', start_date='2014-01-01', end_date='2015-01-01').head()
jobs.loc[(jobs.FAIRSHARE == 64) & (jobs.ReqCPUS == 4) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
data['text_sentiment'] = data['text'].apply(lambda x: sid.polarity_scores(x)['compound'])
parser.HHParser.regexp.match(aSingleLine)
print "Queen", sentiDf['2015-04-16' == sentiDf.index].mean().mean() # queen $ print "Space", sentiDf['2015-09-02' == sentiDf.index].mean().mean() # dane in space $ print "Gold OL", sentiDf['2016-08-15' == sentiDf.index].mean().mean() # Swimmer wins gold
del data["AnimalID"] $ del data["DateTime"] $ del data["OutcomeSubtype"]
df.loc['1930-01-01':'1979-12-31','status'] = "Before FL" $ df.loc['1984-01-01':'2017-12-31','status'] = "After FL" $ df.sample(10)
print(df['one'][1]); df.iloc[1,0]
news_df.sample(5)
import pandas as pd $ def one_hot_encode(x): $     print(len(x)) $     return np.eye(10)[x] $ tests.test_one_hot_encode(one_hot_encode)
grid.best_params_
tlen = pd.Series(data=df['len'].values, index=df['Date']) $ tfav = pd.Series(data=df['Likes'].values, index=df['Date']) $ tret = pd.Series(data=df['RTs'].values, index=df['Date'])
zipcode = "nyc_zip.geojson" $ zc = gpd.read_file(zipcode) $ zc.shape
df = panel_data.to_frame() $ df.head()
data = spark.read.csv('../datasets/ATM_CleanData.csv', header='true',inferSchema='true') $ data.toPandas().head()
bars.open.at_time('9:30')
pca_data = pca.transform(scaled_data)
last_year = today.year + 1 $ years = list(range(join_date.year, last_year)) $ years
print(data.columns)
sub_gene_df.sample(10)
import pickle $ output = open('speeches_metadata_evidence.pkl', 'wb') $ pickle.dump(speeches_metadata_evidence2, output) $ output.close()
MergeWeek['Date'] = MergeWeek['Date'].dt.to_period("W").dt.start_time
fish[fish['size'] > 100]['weight']
npath = out_file3 $ resource_id = hs.addResourceFile('1df83d07805042ce91d806db9fed1eeb', npath)
output_file = root_folder + 'temperature_spectrum.csv' $ new_df.to_csv(output_file, header=False)
stringlike_instance.content
df_cont.apply(lambda c: sum(c.isnull()),axis=0) 
print("Since the conversion rate is assumed to be equal under the null, the convert rate for p(old) equals",Conversion_Rate)
top_tracks.head()
df_all.tail()
condos.info()
pd.Timestamp(2015,11,11)
tweets_df.tweet_text.head(10)
pd.to_datetime(data['created'], infer_datetime_format=True)
census_pd_complete.shape
len(recSets)
new_page_converted = np.random.choice([1,0], size=NewPage, p=[0.12, 0.88]) $ new_page_converted
free_data.iloc[:5]
prob_treat = (df2.query('group=="treatment"')['converted']==1).mean() $ print('Probability converted given that individual in treatment group: ' + str(prob_treat))
regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]' $ def compress(word): $     pieces = re.findall(regexp, word) $     return ''.join(pieces) $ print(nltk.tokenwrap(compress(w) for w in tokens[10:85]))
df_freq_users.user_location_longitude.isnull().sum()
round(df.loc[df.converted == 1].converted.sum()/df.converted.value_counts().sum(),4)
liquor['Year'] = liquor['Date'].dt.year $ liquor['Month']=liquor['Date'].dt.month
notus.loc[notus['country'].isin(canada), 'country'].value_counts(dropna=False)
report_score(y_test, y_fit_proba, 0.2)
df2['landing_page'].value_counts()
x  = [ analyze_sentiment(tweet) for tweet in data['Tweets'] ]
tweets_df.in_reply_to_user_id.describe()
arraylist = [['classA']*5 + ['classB']*5, ['s1', 's2', 's3','s4', 's5']*2] $ print(arraylist)
databreach_2017.head()
investors_df = investors[investors.investor_type.isin(accepted_types)].copy()
fg = sns.FacetGrid(data=motion_at_home_df[motion_at_home_df['is_weekday']==0], hue='time_category', aspect=5)  # $ fg.map(plt.scatter, 'time', 'state').add_legend()
import enchant $ import pandas as pd $ import re $ from autocorrect import spell
trip_data.ix[trip_data.Trip_distance == 0].shape[0]
data2['len'] = np.array([len(tweet.text) for tweet in lista]) $ data2['Likes'] = np.array([tweet.favorite_count for tweet in lista]) $ data2['RTs'] = np.array([tweet.retweet_count for tweet in lista]) $ data2.head()
daily.asfreq('H',method='ffill')
stock.tail()
date + pd.to_timedelta(np.arange(12), 'D')
del(joined)
len(data_df[data_df['failed'] == False])
ratings.loc[ratings.rating_denominator > 10].head(3)
systemuseData.to_sql(con=engine, name='systemuse', if_exists='replace', flavor='mysql',chunksize=10000)
clinton_df = make_dataframe(clinton_tweets)
pos_dic = corpora.Dictionary(pos_review_tokens) $ pos_dic.filter_n_most_frequent(4) $ neg_dic = corpora.Dictionary(neg_review_tokens) $ neg_dic.filter_n_most_frequent(4)
clf=clf.fit(X_train, y_train) # perform training $ y_predict = clf.predict(X_test) $ print(accuracy_score(y_test, y_predict)*100)
results = session.query(Measurements.date,Measurements.prcp).order_by(Measurements.date.asc()).all()
autos["offer_type"].value_counts()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ alpha = norm.ppf(0.95) $ print('z-score: ' + str(z_score)) $ print('p_value: ' + str(p_value)) $ print('alpha: ' + str(alpha))
print("Mean squared error: %.2f" % mean_squared_error(y_test, y_pred))
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ plt.style.use(['seaborn-whitegrid', '14pt']) $ %matplotlib inline
tweet_archive_clean.loc[tweet_archive_clean['tweet_id'] == '681340665377193984',['tweet_id', 'text', 'rating_numerator', 'rating_denominator']]
import shutil
print type(topic_word) $ print topic_word.shape $ print topic_word[0][:20] $ print topic_word[0].sum()
print(DatePrcp)
len(tt1_ok_indices)
building_pa = pd.read_csv('Building_Permits.csv') $ building_pa.head(5)
url = 'http://cs.carleton.edu/cs_comps/0910/netflixprize/final_results/knn/img/knn/cos.png' $ Image(url,width=300, height=500)
day = datetime.now() $
gs.best_estimator_.steps
All_tweet_data_v2.rating_denominator=10
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/adult.data.TAB.txt" $ mydata = pd.read_csv(path, sep= '\t') $ mydata.head(5)
reddit['Time']=(pd.to_datetime(reddit['Time'],unit='ms')) 
abstract = df.at[3,'abstract'] $ abstract
%%timeit -n1 -r2 $ T3 = rentals['description'].progress_apply(get_rental_concession_2)  # replace `progress_apply` with `apply` if you don't use tqdm
unique_domains.sort_values('num_authors', ascending=False)[['domain', 'num_authors']][0:50]
df_step1 = w.get_filtered_data(step = 1, subset = subset_uuid) $ df_step1.columns
accuracy_score(test['class'],logpred)
nba_df.loc["2017-01-01", "Referee1":]
false_positive(predicted_table,test)
user1 = user1[(user1.CallTime < "06:00:00") | (user1.CallTime > "22:00:00") ] $
transformed.shape
BDAY_PAIR_df = query_df(BID_BDAY_URL)
dta.query(("facility_type == 'Restaurant' and " $            "inspection_type == 'Complaint Re-Inspection'")) $
twitter_df_clean.describe()
data.sample(5)
%matplotlib inline $ import matplotlib.pyplot as plt $ import numpy as np
msft.loc['2012-01-01':'2012-01-05'] #slicing
df_course_association=df_course[['uid','course_name','course_started_date']]
df2[df2['landing_page'] == 'new_page'].user_id.count()/df2.user_id.count()
a = np.array([[1., 2.], [3., 4.]]) $ a[0, 0] #Element [row, column]. Equivalent to a[0][0].
total_events_number = len(duplicate_check_df) $ unique_events_number = len(duplicate_check_df['event_id'].unique()) $ duplicate_events_number = total_events_number - unique_events_number $ print("The total events number is %d, and the unique events number is %d, so in dataset %d events appeard more than once." $       % (total_events_number, unique_events_number, duplicate_events_number))
groceries
avg_day_of_month14.to_excel(writer, index=True, sheet_name="2014")
autos_p['last_seen'].str[:10].value_counts(normalize = True, dropna = False).sort_index(ascending = True).head(100)
weights = {"alice": 68, "bob": 83, "colin": 86, "darwin": 68} $ s3 = pd.Series(weights) $ s3
get_freq(series_obj=raw.educ_flag)
print(' '+"Total Precipitation Analysis of 1 year span") $ rain_df.describe()
gdf_gnis.shape
%matplotlib inline $ import pandas as pd $ import numpy as np
dummy_IntServ = pd.get_dummies(df['InternetService'], prefix='IntServ') $ print(dummy_IntServ.head())
cr_old_null = len(df2.query('converted == 1')) / len(df2['converted']) $ print('The convert rate for P_old under the null is {}'.format(cr_old_null))
c['landmark'].value_counts().head(10)
tips.sex = tips.sex.astype('category') $ tips.smoker = tips.smoker.astype('category') $ print(tips.info()) $
pd.Series(['a','b','c','d','e'])           # from Python list
df_date[katespadeseries].groupby("date").sum().sort_values(by="postcount",ascending=False)['postcount'][:10]
df_raw = pd.read_csv("./datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv") $ df = df_raw.copy() $ print(df_raw.head())
data2 = data.set_index('Date')
errors['datetime'] = pd.to_datetime(errors['datetime'], format="%Y-%m-%d %H:%M:%S") $ errors.count()
type(requirements)
DATA in sets_node['date']
repeated_user = df_counts[df_counts.counts > 1].index.values[0] $ df2.query('user_id == @repeated_user')
mySql = 'SELECT dateTime, endpoint FROM AccessLog WHERE responseCode = 403' $ access_logs_df.createOrReplaceTempView('AccessLog') $ spark.sql(mySql).show(10, False)
df2[df2['user_id'].duplicated() == True]
df_page = pd.get_dummies(df2['group']) $ df_page.drop('control', axis=1, inplace=True) $ df2['ab_page'] = df_page $ df2.head()
churned_ix = USER_PLANS_df[churned_bool].index
import numpy as np $ cs = np.log(df['Size']) $ cs = cs.reset_index()
df2.converted.sum()/df2.shape[0]
df['k'] = km.labels_
print("Number of Groups in PRE-ATT&CK") $ groups = lift.get_all_pre_groups() $ print(len(groups)) $ df = json_normalize(groups) $ df.reindex(['matrix', 'group', 'group_aliases', 'group_id', 'group_description'], axis=1)[0:5]
def tolowercase(text): $     try: $         return text.lower() $     except AttributeError: $         return text 
senti_vader = df.groupby('subreddit')['Vader'].mean()
sfctmp = gfs.variables['Temperature_surface'] $ print(sfctmp) $ for dname in sfctmp.dimensions:   $     print(gfs.variables[dname])
relationships = lift.get_relationships_by_object('software') $ relationships[0]
best_epoch = np.argmin(np.array(history.history['val_loss']))+1 $ model.load_weights("model_{}.h5".format(best_epoch))
users.loc[users.age < 18,'age'] = np.nan $ users.loc[users.age > 150, 'age'] = 2014 - users.age $ users.loc[users.age > 122, 'age'] = np.nan
print('Number of unique users in the dataset :: ',df['user_id'].nunique())
import json $ import pandas as pd $ import math $ with open('./data/go-original.cyjs', 'r') as f: $     gotree = json.load(f)
df_new[['US', 'UK', 'CA']] = pd.get_dummies(df_new['country']) $ df_new.drop('CA', axis=1, inplace=True)
r.xs('UA', level=1)[['weight']].plot()
surveys_df = pd.read_csv("surveys.csv", keep_default_na=False, na_values=[""]) $ species_df = pd.read_csv("species.csv", keep_default_na=False, na_values=[""])
train.columns
df
temperature = session.query(measurement.date, measurement.tobs).\ $     filter(measurement.date >= '2016-08-18').\ $     filter(measurement.station == 'USC00519281').\ $     order_by(measurement.date).all() $ temperature
from matplotlib_venn import venn3 $ plt.figure(figsize=(8,8)) $ plt.rcParams.update({'font.size': 9}) $ venn3(subsets={'100':n_just_jobroles,'010':n_just_titles,'001':n_just_tags,'110':n_just_jobroles_titles,'111':n_all,\ $                '011':n_just_titles_tags,'101':n_just_jobroles_tags},set_labels=('Job Roles','Titles','Tags')) $
autos['odometer_km'].value_counts().sort_index()
a = pd.DataFrame(DataAPI.schema.get_schema("indicator")).T $
apple = apple.loc[apple.index.sort_values(),:] $ apple.head()
time_window = df[(df["stamp"] >= "2018-02-21 21:00:00") & (df["stamp"] <= "2018-02-21 23:00:00")]
import numpy as np $ import statsmodels.api as sm $ import statsmodels.formula.api as smf $ results = sm.OLS(gdp_cons_df.PCECC96, gdp_cons_df.GDPC1).fit() $ print(results.summary())
import statsmodels.api as sm $ logit = sm.Logit(df['converted'],df[['intercept','treatment']])
appleInitialNegs = neg_tweets[neg_tweets.author_id_y == 'AppleSupport']
import bs4 as bs #importing beautiful script $ import urllib.request # to request a url $ import time $ import requests
Y_train.iloc[0:5]
from utils import fetch_and_cache $ data_url = 'http://www.ds100.org/sp18/assets/datasets/old_trump_tweets.json.zip' $ file_name = 'old_trump_tweets.json.zip' $ dest_path = fetch_and_cache(data_url=data_url, file=file_name) $ print(f'Located at {dest_path}')
k1.head()
category = pb.Category(commons_site, 'Category:Files by User:Rodelar') $ category.categoryinfo
session=Session(engine)
taxiData.Trip_distance.describe()
archive_df_clean.head(10)
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" & converted == 1').shape[0] $ convert_new = df2.query('landing_page == "new_page" & converted == 1').shape[0] $ n_old = df2.query("landing_page == 'old_page'").shape[0] $ n_new = df2.query("landing_page == 'new_page'").shape[0]
nf = 1e-1 $ loss = tf.reduce_mean(tf.squared_difference(Ypred*nf,Y*nf)) $
sst.dimensions
daily_averages['Weekday'] = daily_averages.index.weekday
df=pd.concat([df,gender_dummies],axis=1) $ df.head(2)
nold_sim = df2.loc[(df2['landing_page'] == 'old_page')].sample(nold,replace = True) $ old_page_converted = nold_sim.converted $ old_page_converted.mean()
articles = pd.read_json('small_data.json')
extract_deduped_with_elms_v2.loc[(~extract_deduped_with_elms_v2.ACCOUNT_ID.isnull()) $                              &(extract_deduped_with_elms_v2.LOAN_AMOUNT.isnull())].shape
model = pipeline.fit(train)
titanic.fare
loans_act_origpd_xirr=cashflows_act_origpd_investor.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
h = experiment.historian
cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']] $ cdf.head(9)
cars= cars.drop(['yearOfRegistration','dateCreated','lastSeen','dateCreatedMod','lastSeenMod'],axis=1) $
MostHourlyExits.to_csv('Largest_hourly_exits.csv')
new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating)) $ new_user_recommendations_rating_title_and_count_RDD = new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD) $ new_user_recommendations_rating_title_and_count_RDD.take(3)
tq1.remove('question1') $ tq2.remove('question2')
fish
Google_stock.isnull().any()
INT['Create_Date'].min()
df_archive.info()
df_tests=[z[z.fk_loan==loan_test].groupby(['dcf']).payment.sum().reset_index() for z in [loan_principals,loan_payments,residuals,recoveries, repaid_loans_cash]]
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='two-sided') $ z_score, p_value
train = pd.concat([train, percent_change_features, df['Close'].rolling(30).mean()], axis=1) $ train.dropna(inplace=True) $ train.head()
filteredTweets.groupby(['DateTime']).mean()['Has_IRMA']
data_set.tail(5)
from gensim.models import LdaModel $ loading = LdaModel.load('topic.model')
data.head(3)
csvDF.head()
df_trump_device_non_retweets['before_or_after_election'].value_counts()
conn = pymysql.connect(host='localhost', port=3306, user='root', password='1234')
sns.set(style="whitegrid") $ ax = sns.barplot(x=df['main_category'].value_counts(), y=df['main_category'].value_counts().index)
dcrime = pd.read_csv(dic_inp["detroit-crime.csv"]) $ dcrime.rename(columns = {"LON":"long","LAT":"lat"},inplace=True)
df3.dtypes
y = np.ravel(y) $ y
print(len(df_proj)) $ df_proj.head()
new = df2['landing_page'] $ new_converted_null = df2.query('converted == 1') $ p_new = new_converted_null.count()[0]/new.count() $ p_new
fs_df
tweets_prediction = pd.merge(archive_clean, images_clean, how='left', on='tweet_id')
df_ab_raw[(df_ab_raw['line_up'] ==1)];
predictions_clean['p1'] = predictions_clean['p1'].str.title() $ predictions_clean['p2'] = predictions_clean['p2'].str.title() $ predictions_clean['p3'] = predictions_clean['p3'].str.title()
generation_index(eia_gen_quarterly, quarterly_index, 'year_quarter')
import numpy as np $ import pandas as pd $ date_time_index = pd.date_range(start='2015-01-01', end='2015-12-31', freq='B') #Creating the date time index for year 2015 i.e. from 01 January, 2015 to 31 December, 2015 $ s = pd.Series(np.random.rand(len(date_time_index)), index=date_time_index) #Creating a series 's' of random numbers with index as date_time_index $ print(s) #Printing the created series 's'
df_wm['cleaned_text'] = df_wm['text'].apply(lambda x : text_cleaners(x))
x=df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].index.values
ab_data = pd.read_csv(r"D:\courses\Nanodegree\term1\statistics\AnalyzeABTestResults\ab_data.csv")
pd.isnull(df)                     $ df.drop(df.tail(8).index,inplace=True)    #droping the last 8 rows $ df = df.drop(columns = ['Md'])    #removing Md column
df.index # pandas Index object
sdf = sdf.assign(Norm_summary = normalize_corpus(sdf.summary))
usage_ct = pd.crosstab(combined_df5['llpg_usage'],combined_df5['bin_label']) $ usage_ct['neg_pctg']=usage_ct[1]/usage_ct[0]*100 $ usage_ct.sort_values('neg_pctg',ascending=False)
melted[(melted.Date=='2012-01-03') & (melted.Symbol=='MSFT')]
autos = autos[autos['price'].between(1,150000)]
repos.to_pickle('data/pickled/new_subset_repos.pkl')
import pandas as pd $ from datetime import timedelta $ %pylab inline $ df_goog = pd.read_csv('../../assets/datasets/goog.csv')
s1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])
df_joined = df_country.set_index('user_id').join(df2.set_index('user_id'), how = 'inner') $ df_joined.head()
sn.distplot(train_binary_dummy['obs_count'])
m2T=m2.T $ m3= m2T.dot(m) $ print("m3: ", m3)
pres_df['metro_area'].tail()
commits = Query(git_index).get_cardinality("hash")\ $                           .since(start=start_date)\ $                           .until(end=end_date)\ $                           .by_period() $ print(get_timeseries(commits, dataframe=True).tail())
import brunel $ %brunel data('df') treemap x(DAY_OF_WEEK,TIME_OF_DAY_BAND) color(DAY_OF_WEEK) size(FRAUD) sum(FRAUD) tooltip(#all)
import statsmodels.api as sm $ df2.head(5)
df_train['trafficSource.isTrueDirect'].unique()
serc_pixel_df = pd.DataFrame() $ serc_pixel_df['reflectance'] = sercRefl[500,500,:] $ serc_pixel_df['wavelengths'] = sercRefl_md['wavelength']
s2.index = s2.index.values.astype(int) $ s1 + s2
a_set.intersection(another_set)
bloomfield_pothole_data.info()
tfidf.get_feature_names()[75]
df.columns
contractor_clean = contractor.copy()
yhat_prob = lr.predict_proba(X_test) $ yhat_prob
twitter_ar.head(2)
df.shape
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old]) $ print "z-score:",z_score, "\np-value:",p_value
del squares['square of 6'] $ squares
gram_collection.find({"account": "deluxetattoochicago"}).count()
print('RF: {}'.format(rf.score(X_test, y_test))) $ print('KNN: {}'.format(knn.score(X_test, y_test)))
import tensorflow as tf $ tf.test.gpu_device_name()
wsj = sorted(set(nltk.corpus.treebank.words())) $ fd = nltk.FreqDist([vs for word in wsj $            for vs in re.findall(r'[aeiou]{2,}', word.lower())]) $ fd.most_common(12)
prcp_analysis_df["date"] = pd.to_datetime(prcp_analysis_df["date"],format="%Y-%m-%d", errors="coerce")
d_salary=detroit_census2.drop(detroit_census2.index[:46]) $ d_salary=d_salary.drop(d_salary.index[50:]) $ d_salary.head(4)
model_df = model_df.drop(['score', 'timestamp', 'flair_text'], axis=1)
data = [10, 20, 30, 40, 50] $ distData = sc.parallelize(data) $ reducedRDD = distData.reduce(lambda a, b: a + b) $ print(reducedRDD)
X = joined.drop('CHURN', axis = 1) $ y = joined['CHURN'] $ X.sample(n=5, random_state=2)
players_df.drop(columns={'player_id', 'Unnamed: 0', 'Name'}, inplace=True) $ players_df.to_csv('~/dotaMediaTermPaper/data/players_df.csv')
gen.c.unique()
import gmplot $ gmap = gmplot.GoogleMapPlotter.from_geocode("New York",10) $
df2.query('user_id == 773192')
df_pivot = df_cryptdex.pivot(index='date',columns='symbol',values='close').reset_index()
top_songs['Country'].isnull().sum()
ticks.head()
table2.head(3)
new_df.head()
df.info() # studying the dataframe 
stocks_pca_m3= np.zeros(shape=(len(stocks_pca_SP500),3)) $ for i in range(0,len(stocks_pca_happiness)): $     stocks_pca_m3[i][0]=float(stocks_pca_SP500[i]) $     stocks_pca_m3[i][1]=float(stocks_pca_DJIA[i]) $     stocks_pca_m3[i][2]=float(stocks_pca_happiness[i])
plt.title('Stella Mccartney ngram', fontsize=18) $ stellamccartney_ng.plot(kind='barh', figsize=(20,16)); $ plt.savefig('../visuals/Stella_Mccartney_ngram.jpg')
In the logistic reegression the null hypothesis is that there is no relation between the variables and convertion rate. $ In the Alternate hyothesis is that there is a relationship between them
newdf = df[df.tournament != "Friendly"] $ print (newdf.shape)
tweets = pd.read_csv('trumpTwitterArchive.csv')
csv_path = 'fire_log_cape_town.csv' $ df = pd.read_csv(join(path, csv_path), parse_dates=['Datetime']) $ df.head()
df_all['US_int'] = df_all['US'] $ df_all['UK_int'] = df_all['UK'] $ df_all['CA_int'] = df_all['CA']
go_no_go_date = go_no_go.groupby('subject_id').Date.unique() $ simp_rxn_time_date = simp_rxn_time.groupby('subject_id').Date.unique() $ proc_rxn_times_date = proc_rxn_time.groupby('subject_id').Date.unique()
df.head() $ df.plot.scatter('Bottles Sold','Sale (Dollars)') $ df.plot.scatter('Volume Sold (Liters)','Sale (Dollars)')
tweet_th['text'].apply(lambda x: "I'm at" in x).value_counts()
X_age_test_dummy, _ = custom_dummify(X_age_test, 0.01, train_cols) $ y_pred_test = grid.predict(X_age_test_dummy) $ rmse = np.sqrt(np.mean((y_pred_test[:,np.newaxis]-y_age_test)**2)) $ print ('RMSE of age imputation via random forest is {}'.format(rmse))
df_main.p1.head(3)
f['converted'].mean()
dfOther.head(80)
sum(autos["registration_year"].between(2019,99999))
df.groupby('hireable')['login'].nunique()
dinw_filter_set = w.get_step_object(step = 2, subset = subset_uuid).get_indicator_data_filter_settings('din_winter') $ dinw_filter_set.settings.df
df.head(n=2)
stock.news_sources = stock.news_sources.fillna('') $ mlb = MultiLabelBinarizer() $ stock = stock.join(pd.DataFrame(mlb.fit_transform(stock.news_sources), columns=mlb.classes_, index=stock.index)) $ stock = stock.drop('news_sources', 1) $ stock = stock.fillna(0)
full_data = pd.concat([jobs_data1, jobs_data2, jobs_data3, jobs_data4], ignore_index=True)
vegas_train, vegas_tests = get_train_and_tests('data_Las_Vegas')
explains = combine(wk_prob, "l") $ find_top_k_ngrams(explains, 3, 10, stop_words = license_stop)
import os $ import pandas as pd $ import glob
StockData.reset_index(inplace=True) $ StockData.index.values
df.to_csv("0016_clean.csv")
train_df1 = train_df.copy() $ train_df1["created"] = pd.to_datetime(train_df1["created"]) $ train_df1['month'] = train_df1['created'].dt.strftime('%b')
df2 = df.dropna(how = 'any') $ df2
saved_model_details = client.repository.store_model(best_model_uid, {'name': 'k_flw-test2'})
df_main.text = new_line_sub(df_main.text)
print(today.strftime("Today is %Y-%m-%d"))
inspector = inspect(engine) $ columns = inspector.get_columns('measurements') $ for c in columns: $     print(c['name'], c["type"]) $
end = pd.datetime(2012, 1, 1)
rfr.fit(data[['expenses', 'floor', 'lat', 'lon', 'property_type', \ $               'rooms', 'surface_covered_in_m2', 'surface_total_in_m2']], \ $         data[['price_aprox_usd']].values.ravel())
final_topbikes['Timestamp +2'] = final_topbikes['Timestamp'].apply(addtwo)
n_old = df2.query('landing_page == "old_page"').shape[0] $ n_old
autos['gearbox'].value_counts()
sessions_path = '../../Data/sessions.csv' $ sessions = pd.read_csv(sessions_path) $ print(len(sessions)) $ sessions.head()
def query_by_name(url, params, name): $     params["query"] = "artist:" + name $     return query_site(url, params)
db.collection_names(include_system_collections=False)
print "Fraction of data containing delays: %.3f" % float(sum(training_DF['IncidentInfo'])/len(training_DF))
pipe = pc.PipelineControl(data_path='examples/simple/data/varying_data.csv', $                           prediction_path='examples/simple/data/predictions.csv', $                           retraining_flag=True, $                           sliding_window_size=500) $ pipe.runPipeline()
dfs_morning[["PREV_DATE", "PREV_ENTRIES"]] = (dfs_morning $                                                     .groupby(["STATION", "DATE"])['DATE','ENTRIES'] $                                                     .transform(lambda grp: grp.shift(1))) $ dfs_morning.dropna(inplace=True) $ dfs_morning['ENTRIES_MORNING'] = dfs_morning['ENTRIES'] - dfs_morning['PREV_ENTRIES']
df.info() # looks like all rows have values
g = sns.plt.title('Top Complaint Types on the 360 Days with The Most Complaints') $ g = sns.barplot(x = 'days_top_complaint', y='complaint_type', data = dftop2, color = "b" ) $ g.set(xlabel='Days of Top Complaint', ylabel='complaint type') $ plt.tight_layout() $ plt.savefig('Top_complaints_days_most_volume.png')
result2.summary()
df.describe().transpose()
close = {} $ for lis in answer1: $     close[lis[0]] = lis[4]
subway5.reset_index(inplace=True)
df2.loc['2018-01-18', 'high_tempf']
tweets = pd.read_csv("dataframe_terror.csv", parse_dates=[0]) $ terror = pd.read_csv('attacks.csv', parse_dates=[0])
df0901.head()
len(public_tweets)
from sklearn.linear_model import LinearRegression $ lr = LinearRegression() $ lr.fit(new_array, y_train) $ print(lr.score(new_array, y_train))
hot_df.nunique()
pd.to_datetime('2010/11/12', format='%Y/%m/%d')
print test.shape $ print candSplit.shape $ candSplit.head()
engine = Engine(config)
marijuanamatches = props[props.prop_name == 'PROPOSITION 064- MARIJUANA LEGALIZATION. INITIATIVE STATUTE.'] $ marijuanamatches.head()
csvData['street'] = csvData['street'].str.replace(' Northeast', ' NE') $ csvData[csvData['street'].str.match('.*North.*')]['street']
file_path = "/mnt/idms/temporalNodeRanking/data/filtered_timeline_data/tsv/ausopen/ausopen_mentions.csv" $ au.recode_and_export_mentions(file_path,mentions_df,user_names)
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True) $ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
df2 = df2.drop_duplicates(['user_id'], keep='first') $ df2[df2['user_id'] == 773192] 
full['<=30Days'] = (full['DaysSinceAdmission'] <= pd.Timedelta('30 days')).astype(np.int)
iris_df = h2o.import_file(path=iris_data_path)
archive_clean['name'].replace('None',np.nan,inplace= True)
df2.converted[df2.group == "control"].mean()
df = pd.read_csv('loan_test.csv') $ df.head()
run txt2pdf.py -o"2018-06-19 2014 FLORIDA HOSPITAL Sorted by payments.pdf"  "2018-06-19 2014 FLORIDA HOSPITAL Sorted by payments.txt"
def calc_sub(i): $     tweet = TextBlob(dataset['Text'][i]) $     return tweet.subjectivity $ dataset['Sent_Subjectivity'] = [calc_sub(i) for i in range(len(dataset))]
close = df.reset_index().pivot(index='ticker', columns='date', values='adj_close') $ volume = df.reset_index().pivot(index='ticker', columns='date', values='adj_volume') $ ex_dividend = df.reset_index().pivot(index='ticker', columns='date', values='ex-dividend')
df2[['control','treatment']]= pd.get_dummies(df2['group']) $ df2 = df2.drop('control',axis = 1) $ df2.head()
y_test_under[fm_bet_under].sum()
df.isnull().sum()
data['Session'] = pd.DatetimeIndex(data['Session']) $ data.head()
pold = df2[df2['landing_page']=='old_page']['converted'].mean() $ print("the convert rate for  pold = ",pold)
x_train_numerical = x_train[:, 0:2].toarray() # We only want the first two features which are the numerical ones. $ x_test_numerical = x_test[:, 0:2].toarray()
import h2o $
df.dropna(axis=1,inplace=True) $ df.head(3)
ridge1 = linear_model.Ridge(alpha = 0) $ ridge1.fit(x3, y) $ (ridge1.coef_, ridge1.intercept_)
df_samples = pd.concat([df_failed_samples, df_ok_samples]) $ df_samples = df_samples.reset_index(drop=True)
to_be_predicted_Day4 = 21.39121267 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
s=datetime(2018,1,1) $ e=datetime(2018,12,30)
vader_df = pd.DataFrame(vader_scores)[['text', 'compound', 'created_at']] $ vader_df = vader_df.sort_values('compound', ascending=True) $ vader_df.head(7)
df['created_at'] = pd.to_datetime(df['Created Date'], format="%m/%d/%Y %I:%M:%S %p")
chefdf = pd.merge(chefdf, chef12df,  how='left', left_on=['name','user'], $                   right_on = ['name','user'], suffixes=('','_12'))
X_train, X_test, y_train, y_test = train_test_split(X, y, $                                                     train_size=0.75, test_size=0.25, random_state=1) $
l = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']]) $ result=l.fit()
df[df.sentiment == 0].index
df_h1b_nyc_ft.groupby(['lca_case_employer_address'])['lca_case_employer_name'].unique()
(StockData['Dell'] / StockData['Facebook']).tail()
df[(df.state == 'YY') & (df.amount >= 45000)]
soup.title.text
tw.sample(5)
type(git_log.timestamp[0])
kickstarter['main_category'].unique()
from scipy.stats import norm $ norm.ppf(0.95) $
len(df.user_id.unique()) $
nold=df2.query('landing_page=="old_page"').count()[0] $ nold
df3 = df2
print(fdist["species"]) $ print(fdist["sexual"]) $ print(fdist["origin"])
sns_plot = sns.barplot(x=stages,y=retweets,color='lightblue',order=['puppo','doggo','floofer','pupper'],ci=None) $ plt.ylabel('Median Retweet Count') $ plt.xlabel('Dog Stages') $ sns_plot.figure.savefig('dog_stages_vs_retweet.jpg')
logm = sm.Logit(df3['converted'], df3[['intercept', 'ab_page']]) $ logres = logm.fit()
from nltk.sentiment.vader import SentimentIntensityAnalyzer $ sid = SentimentIntensityAnalyzer() $ data_sentiment = data[["id_str", "created_at", "text"]] $ data_sentiment.head()
troll_users.drop(columns=diff_user_cols, axis=1, inplace=True)
access_logs_df.limit(10).show()
high = out_df.copy() $ high['low']=0 $ high['medium']=0 $ high['high']=1 $ high.to_csv('all_high.csv',index=False)
feature_imp
plt.bar([1,2,3,4,5,6,7,8,9,10,11,12],monthly_percent_off) $ plt.xlabel('Month of 2017') $ plt.ylabel('Monthly % Error')
df['Agency Name'].value_counts()
store_items.dropna(axis = 1)
by_hour = sample.resample("H") $ by_hour
print("Created the following features:\n Number of purchases per user \n Number of purchases per product \n Average purchase amount per user \n Average purchase amount per product \n Proportion of purchases above average product purchase per user")
precipitation_df = pd.read_sql(session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >= "2016-08-23").statement,session.bind) $ precipitation_df.head(15)
df2[df2.duplicated("user_id")]
width, height = 18, 6 $ plt.figure(figsize=(width, height)) $ jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 4) & (jobs.GPU == 0) & (jobs.Memory % 4000 >= 0)].groupby('Memory').Wait.median().plot(kind = 'bar', logy = False) $
df = pd.read_csv('ZILLOW-Z49445_ZRISFRR.csv') $ df.head()
len(twitter_archive_df[pd.isnull(twitter_archive_df['expanded_urls'])])
MergeMonth.head()
dft['2013-1-15':'2013-1-15 12:30:00']
url = 'https://www.dropbox.com/s/yz5biypudzjpc12/PSI_tweets.txt?dl=1' $ location = './' #relative location for Linux, saves it in the same folder as this script $ download_file(url, location)
control = df2.query("group == 'control'")
mlb = MultiLabelBinarizer() $ stock.news_sources = stock.news_sources.fillna('') $ stock = stock.join(pd.DataFrame(mlb.fit_transform(stock.news_sources), columns=mlb.classes_, index=stock.index)) $ stock = stock.drop('news_sources', 1)
tl_2030 = pd.read_csv('input/data/trans_2030_ls.csv', encoding='utf8', index_col=0)
clf = decomposition.NMF(n_components=6, random_state=1)
results["units"].head()
predict_acct_id(2640238,'2015-12-09 15:44:08.0','CITY OF CEDAR PARK',78613,'450 CYPRESS CREEK RD - 120','CYPRESS CREEK')
movies_df['year'] = movies_df.title.str.extract('(\(\d\d\d\d\))',expand=False) $ movies_df['year'] = movies_df.year.str.extract('(\d\d\d\d)',expand=False) $ movies_df['title'] = movies_df.title.str.replace('(\(\d\d\d\d\))', '') $ movies_df['title'] = movies_df['title'].apply(lambda x: x.strip()) $ movies_df.head()
model_gb_20 = GradientBoostingClassifier(min_samples_leaf=20, random_state=42) $ model_gb_20.fit(x_train,y_train) $ print("Train: ", model_gb_20.score(x_train,y_train)*100) $ print("Test: ", model_gb_20.score(x_test,y_test)*100) $ print("Difference between train and test: ", model_gb_20.score(x_train,y_train)*100-model_gb_20.score(x_test,y_test)*100)
twitter_df.head(2)
import json $ from pandas.io.json import json_normalize $ import urllib.request
!du -sh input
csv_df[csv_df['uploader_registration'].isin([np.nan])]['uploader'].unique()
weather_mean.shape
df2['intercept'] = 1 $ df2[['treatment', 'ab_page']] = pd.get_dummies(df2['group']) $ print(df2.info()) $ df2.drop(['treatment'], axis=1, inplace=True) $ df2.head()
users= pd.merge(avg_reorder_days,avg_usercart_size) $ users_fin=pd.merge(users,total_order_per_user) $ users_fin.head()
print(len(train_df['os'].unique()))
fix_space_0 = lambda x: pd.Series([i for i in reversed(x.split(' '))])
n_new = df2['landing_page'].value_counts()[0] $ n_new
predictions_count = model_count_NB.predict(X_test_count) $ predictions_tfidf = model_tfidf_NB.predict(X_test_tfidf)
x = "dort" $ if x in stopword_list: $     print("Yes!" ,x, "is in the list." )    #Prints "Yes!", if x in list. $ else: $     print("No!",x, "is not in the list.")   #Prints "No!" if not.
commits = Query(git_index).get_cardinality("hash").by_period() $ print("Trend for month: ", get_trend(get_timeseries(commits))) $ commits = Query(git_index).get_cardinality("hash").by_period(period="quarter") $ print("Trend for quarter: ", get_trend(get_timeseries(commits)))
driver = selenium.webdriver.Safari() # This command opens a window in Safari $ driver.get("https://xkcd.com/")
Correct the rating of denominators that are < 10
sum_of_values_for_every_wednesday = s[date_time_index.weekday == 2].sum() #Calculating the sum of values in s for every Wednesday $ print(sum_of_values_for_every_wednesday) #Printing the sum
df_R = df[df['Region_Name']=='Reading']
dfd.describe()
cov=np.cov(x,y) $ print(cov[0][1])
countdf = pd.DataFrame(chef03) $ countdf = countdf.drop_duplicates(subset=['name', 'user']) $ countdf.info()
df_twitter_archive_copy['stage'] = df_twitter_archive_copy.apply(get_dog_stage, axis=1) $ df_twitter_archive_copy['stage'] = df_twitter_archive_copy.stage.astype('category')
duration_train_df.index
eug_cg_counts.unstack(fill_value=0).plot(kind='bar', stacked=True);
import numpy as np $ import pandas as pd
actual_diff=actual_new_page_converted-actual_old_page_converted
glove = glove_word_vecs('/Users/mbk/desktop/glove.6B/glove.6B.100d.txt') $ tweet_embedding = [] $ for tweet in train['text']: $     tweet_embedding.append(get_centroid(tweet, glove))
twitter_archive_df.info()
venues.to_csv('./cleaned/venues.csv', header=False, index=False)
results = log_mod.fit() $ results.summary()
print(a) $ print('cat' in a)  $ print('dog' in a)
train.shape
dd_df.head()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY)
ticket = df_titanic['ticket'] $ print(ticket.describe()) $
actual_payments_combined["payout_date"]=np.array(actual_payments_combined["payout_date_x"],'datetime64[D]')
df.loc[df['edition']=='NBC', 'edition'] = 'National'
df_mes = df_mes[df_mes['total_amount']>=0] $ df_mes.shape[0]
import stackexchange
df_predictions.Q1.value_counts()
df['created_at'].tail()
by_year.set_index("election_year").plot.bar(figsize=(20, 10))
df.plot();
df_new[['CA', 'US', 'UK']] = pd.get_dummies(df_new['country'])[['CA', 'US', 'UK']] $ df_new['country'].value_counts()
BID_PLANS_df.loc[aliases]
Ext_input.build()
np.random.random((3, 3, 3))
SampleIndex = StockData.loc[StockData.Set == 'train'].index.values.copy()  # Make a copy to avoid modifying StockData.index $ NumRows = len(SampleIndex) $ NumTest = int(TestRatio * NumRows) $ NumTrain = NumRows - NumTest $ print("We have {:,} rows, we will use {:,} of them for training, {:,} for testing.".format(NumRows, NumTrain, NumTest))
import pandas as pd $ import numpy as np $ np.random.seed(0)
n_new=df2.query("landing_page=='new_page'").user_id.count() $ n_new
!hdfs dfs -mkdir hw3 $ os.getcwd() 
grades.plot('ATAR', 'Mark', kind='scatter')
import numpy as np $ cities.reindex(np.random.permutation(cities.index))
mnb_best = MultinomialNB(alpha = 0.2, fit_prior = False) $ mnb_best = mnb_best.fit( X_train, y_train )
exploration_titanic.structure()
pop_flat = pop.reset_index(name='population') $ pop_flat
autos.columns = ['date_crawled', 'name', 'seller', 'offer_type', 'price', 'abtest', $        'vehicle_type', 'registration_year', 'gearbox', 'power_pS', 'model', $        'odometer', 'registration_month', 'fuel_type', 'brand', $        'unepaired_damage', 'date_created', 'nr_of_pictures', 'postal_code', $        'last_seen']
df['SOCIAL_NETWORKS'] = df.text.apply(lambda text: pd.Series([x in text for x in SOCIAL_NETWORKS]).any()) $ df['DECISION_MAKING']  = df.text.apply(lambda text: pd.Series([x in text for x in DECISION_MAKING]).any()) $ df['ADAPTIVE_CAPACITY']  = df.text.apply(lambda text: pd.Series([x in text for x in ADAPTIVE_CAPACITY]).any())
verify_response.headers
round_count['count'].sum()
blame.timestamp.describe()
df3['new_page_UK'] = df3['UK']*df3['ab_page'] $ df3['new_page_US'] = df3['US']*df3['ab_page'] $ df3.head()
print(daily_returns['SPY'].kurtosis()) $ print(daily_returns['XOM'].kurtosis())
from sklearn import metrics $ print("Train set Accuracy: ", metrics.accuracy_score(y_train, neigh.predict(X_train))) $ print("Test set Accuracy: ", metrics.accuracy_score(y_test, yhat))
run txt2pdf.py -o"2018-06-19 2011 FLORIDA HOSPITAL Sorted by payments.pdf"  "2018-06-19 2011 FLORIDA HOSPITAL Sorted by payments.txt"
pos_threshold=0.1 $ neg_threshold=-0.1
Data = Data[(Data['ratio'] < 0.99)] $ Data = Data[(Data['deposited'] > 0)]
tweets_p1_month = tweets_p1_month.drop(['Month', 'Year'], axis=1) $ tweets_p1_month.head()
autos["ad_created"].str[:10].value_counts(normalize=True, $                                    dropna = False $                                   ).sort_index()
p_new = df2[df2['landing_page'] == 'new_page']['converted'].mean() $ print("Probability of conversion for new page is {}".format(p_new))
results["workers"].head()
df = gpd.GeoDataFrame(df)
df_B3
weeklyTotals = tweets['text'].groupby(tweets['weekNumber']).size()
pickle_full = "urinal-data-28-nov_clean.p"
%load "solutions/sol_2_24.py"
%run TextStats.py $ wf = WordFreq(bagmaker.masterbag) $ %timeit word_frequencies = wf.compute_frequency_of_words_in_bag()
headers = { $     'Authorization': APIToken, $     'Content-Type': 'application/json' $ }
!ls ./anaconda3_410 $
active_list_pending_ratio = x_train[['Active Listing Count ', 'Pending Ratio']].values $ ss_scaler.fit(active_list_pending_ratio) $ active_list_pending_ratio_transform = ss_scaler.transform(active_list_pending_ratio) $ active_list_pending_ratio_transform[0:5,:]
learn.save('clas_2')
merge[merge.columns[38]].value_counts()
zone_train.corr()
train_dum.shape
with pd.option_context('display.max_rows', 150): $     print(news_period_df[news_period_df['news_entities'] == ''].groupby(['news_collected_time']).size())
topCandidateInt=topCandidateInt.reset_index(drop=True) $ topCandidateInt.head()
<img src="/images/MongoDB1.png" alt="[img: MongoDB view]" title="MongoDB View" />
s2 = pd.Series(range(3)) $ s1[1:] + s2[:4]
lithlog.to_sql(con=engine, name='lithlog', if_exists='replace', flavor='mysql',index=False, chunksize=1000)
print(dfd.hspf.describe()) $ dfd.hspf.hist()
y_pred = classifier.predict(X_test) $ report = sklearn.metrics.classification_report(y_test, y_pred) $ print(report)
[column.name for column in get_child_descendant_column_data(observations_node)]
data.head()
gs.best_score_
subway4.to_csv('station_weekly.csv')
support.amount.sum()
config_path = os.path.join(config_root, model_name) $ config = Config.from_config_file(config_path) $ model = seed_model_from_config(config)
date_df = pd.DataFrame(normals, columns=('tmin','tavg','tmax')) $ date_df['date'] = trip_dates $ date_df.set_index('date')
print(coming_next_reason.sum(axis=0).sum()) $ print() $ print(coming_next_reason.sum(axis=0))
df[:5]
plt.figure(figsize=(16,8)) $ fb['2012':].resample('W')['share_count'].sum().plot()
df = pd.read_csv('data/dataset_sentiment.csv') $ df.head()
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn as sns
images.set_index('tweet_id', inplace = True) $ df2 = pd.merge(left=archive, right=images, left_index=True, right_index=True, how='left') $ df2 = pd.merge(left=df2, right=df, left_index=True, right_index=True, how='left') $ df2.to_csv('df2copy.csv', encoding = 'utf-8')
roc_auc_score(y_test, y_pred_mdl)
plt.hist(p_diffs) $ plt.axvline(x=a_diff,color='red')
fig, ax = plt.subplots(figsize=(10,8)) $ g = sns.distplot(condo_6.PRICE, rug=True, kde=True, ax=ax) $ t = g.set_title("Distribution of Sale Prices")
df2017['Week Ending Date'] = df2017.index $ melted_df = pd.melt(df2017, id_vars='Week Ending Date', var_name='age', value_name='Sales') $ melted_df['Dummy'] = 0 $ sns.tsplot(melted_df, time='Week Ending Date', unit='Dummy', condition='age', value='Sales', ax=ax)
p_old = df2[df2['landing_page'] == 'old_page']['converted'].mean() $ print("Probability of conversion for old page is {}".format(p_old))
MICROSACC.plot_mainsequence(microsaccades)+coord_cartesian(xlim=(-3,1))
liquor = liquor.dropna()
df['intercept']=1 $ df[['control', 'treatment']] = pd.get_dummies(df['group']) $
df_playlist_videos['channel_id'].unique()
reddit.head()
Results_rf10.to_csv('soln_rf10.csv', index=False)
jobs.loc[(jobs.MAXCPUS == 600) & (jobs.GPU == 0)].groupby('Group').JobID.count().sort_values(ascending= False)
dataframe.groupby('quarter').daily_worker_count.agg(['count','min','max','sum','mean'])
sorted(zipped, key=lambda value: value[1], reverse=True)
pivoted.T[labels == 0].T.plot(legend=False, alpha=0.1)
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','OriginationPhase']).closed_at.count() $ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True, grid=False) $
train_df[train_df['parcelid'].duplicated(keep=False)] $
to_be_predicted_Day3 = 17.68195036 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
ActiveStations = session.query(Measurement.station,func.count(Measurement.station)). \ $                  group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all() $ ActiveStations
pold=df2.converted.mean() $ pold
baseball_newind.iloc[:5, 5:8]
reviewsDF = reviewsDF.drop(reviewsDF.columns[0], axis = 1)
purls = urls[urls["screen_name"]=="lensjockey"] $ purls
park.named_drug.notnull().sum()
import numpy as np $ import pandas as pd $ import re $ from IPython.display import display $ import matplotlib.pyplot as plt
breed_predict_df_clean.drop(['jpg_url', 'img_num'], axis=1, inplace=True)
pickle_off = open("df_meta.pickle","rb") $ dfMeta = pickle.load(pickle_off)
image_predictions_clean.p1=image_predictions_clean.p1.astype('category') $ image_predictions_clean.p2=image_predictions_clean.p2.astype('category') $ image_predictions_clean.p3=image_predictions_clean.p3.astype('category')
regression_models = ws.models(tag = TICKER) $ for m in regression_models: $     print("Name:", m.name,"\tVersion:", m.version, "\tDescription:", m.description, m.tags)
ratings = tweet_archive_clean.text.str.extractall(r"(?P<numerator>\d+\.?\d*)\/(?P<denominator>\d+0)") $ ratings.numerator = ratings.numerator.astype(float) $ ratings.denominator = ratings.denominator.astype(float)
autos['price'].value_counts().sort_index(ascending=True).head(20)
logreg_words = LogisticRegression(random_state=42) $ scores = cross_val_score(logreg_words, X_words, y, cv=skf) $ print 'Cross-validated LogReg scores based on certain words:', scores $ print 'Mean of scores:', np.mean(scores)
import time $ def send_text(the_message): $     return
df2.drop(2893,inplace=True) $
p_diffs = np.array(p_diffs) $ p_diffs
shops.head()
coolCars[5:10]
def normalizer(mat): $     a = np.array(mat) $     b = np.zeros_like(a) $     b[np.arange(len(a)), a.argmax(1)] = True $     return b
ftfy.ftfy(u'facebook\u2019s')
print(s2.keys()) $ print(s3.keys()) $ s2 + s3
print(df_users['bio2'].value_counts())
tumblrAPItarget = 'http://{}.tumblr.com/api/read/json' $ r = requests.get(tumblrAPItarget.format('lolcats-lol-cat')) $ print(r.text[:1000])
duplicate_check_df = pd.DataFrame(duplicate_check_events, columns=duplicate_check_columns) $ duplicate_check_df.head()
df['created_at'] = pd.to_datetime(df['Created Date'], format='%m/%d/%Y %I:%M:%S %p')
autos["abtest"].value_counts()
qW.transform = 'r'
customer_visitors_new = customer_visitors.groupby([customer_visitors.DateCol.dt.year, customer_visitors.DateCol.dt.dayofweek]).mean() $ customer_visitors_new
merged2 = merged1.copy()
1-stats.norm.cdf((active_mean - inactive_mean)/diffSD)
tstamp = Timestamp('2014-08-01 12:00:00', tz='US/Mountain') $ tstamp
url = form_url(f'organizations/{org_id}/actions') $ response = requests.get(url, headers=headers) $ print_body(response, max_array_components=3)
import sys $ !conda config --add channels conda-forge $ !conda install --yes --prefix {sys.prefix} seaborn $ !conda install --yes --prefix {sys.prefix} xlrd
topics = topics.drop(['out_date', 'due_date', 'semester'], axis=1) $ print(topics.info())
features.shape
status = client.experiments.get_status(experiment_run_uid) $ best_model_uid = status['best_results']['experiment_best_model']['training_guid'] $ best_model_name = status['best_results']['experiment_best_model']['training_reference_name'] $ print(best_model_uid + ' (' +  best_model_name  + ')')
review_title = review_df.loc[:, 'review_title'] $ review_body = review_df.loc[:, 'review_body']
train_small_data.click_timeHour.unique()
ts.head()
bacteria_data.values
df.loc[df['Match']=='No_Match'].to_csv('geocoding_failures.csv', encoding='utf-8')
df2=df2.drop('group_receive_1',axis=1) $ df2.head()
volume = ticks.Volume.resample('1min', how='sum') $ value = ticks.prod(axis=1).resample('1min', how='sum') $ vwap = value / volume
df4.sort_values(by='BG')
cutoff_times.groupby('msno')['churn'].sum().sort_values().tail()
df_Tesla['tokens'] =stemmed $ df_Tesla.head(2)
station_count= session.query(Measurement.station ).group_by(Measurement.station ).count() $ print(f"There are {station_count} available in dataset")
final_A.loc[0,'best_time']
indeed['location'].value_counts()
df_tweets[~df_tweets.parent_tid.isnull()].shape[0] / df_tweets.shape[0] * 100
train_size = 12000 $ val_size = 120 $ x_tr, y_tr = gen_dataset(train_size, N_CLASSES, invoices, targets, tokenizer) $ x_val, y_val = gen_dataset(val_size, N_CLASSES, invoices, targets, tokenizer)
print('web service hosted in AKS:', aks_service.scoring_uri)
df_ad_airings_5['location'][0].split(",")
url = "http://www.fdic.gov/bank/individual/failed/banklist.html" $ banks = pd.read_html(url) $ banks[0][0:5].ix[:,0:4]
pcpData_df.prcp.describe()
cig_data.index
times
PIT = pd.read_excel(url_PIT, $                     skiprows = 8)
df_3.to_csv('LBDLduplicitepermanente_2016_day_media.csv') $
tweet_counts_by_month = tweet_archive_master[['timestamp', 'favorite_count', 'retweet_count']].groupby(pd.Grouper(key='timestamp', freq='M')).sum()
4719/7492
postsTypes = posts[(posts['type']=='status')].groupby(['year','type']).mean() $ postsTypes[['likesCount','commentsCount','sharesCount']].round(2)
plt.figure(figsize=(10,10)) $ g = sns.boxplot(x='dog_stages',y='retweet_count',data= df_master,palette='rainbow') $ g.axes.set_title('Boxplot between dog stages and retweet', fontsize=14);
inst_order
import calendar $ df['Month'] = df['Date'].apply(lambda x: calendar.month_abbr[x.month]) $ df['Year']  = df['Date'].apply(lambda x: x.year)
bigdf = pd.concat(dfs)
reddit.drop_duplicates(subset='title', keep='first', inplace=True)
plt.hist(p_diffs); $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('Simulated Conversion Rate Difference between New Page and Old Page under Hypothesis Null')
pipe_knn_2 = make_pipeline(tvec, knn) $ pipe_knn_2.fit(X_train, y_train) $ pipe_knn_2.score(X_test, y_test)
model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)
eggs = (1,3,0,3,2) $ [catch(lambda : 1/egg) for egg in eggs] $ [1, 0, ('integer division or modulo by zero'), 0, 0]
print ('training_active_listing_dummy - Shape: ', training_active_listing_dummy.shape)
'a' in data
cust_demo.columns
ben_fin['EditTime>15'] = ben_fin['diffs']>15
walking_df = df_summary[df_summary['Activity']=='walking'].groupby('Date').sum( $ ).reset_index().sort_values(by='Date', ascending=1) $ walking_df = walking_df.filter(items=['Date', 'Distance', 'Steps']) $ walking_df.rename(columns={'Distance': 'ttl_wal_distance', 'Steps': 'ttl_steps'}, inplace=True)
suspects_with_27_1 = suspects_with_1T_27[suspects_with_1T_27['imsi'] == '5250163679F3C42553B0DA718A87B785F44AD0']
from sklearn.metrics import roc_auc_score $ print(roc_auc_score(y_test, y_pred))
JournalStories
import numpy as np $ import matplotlib.pyplot as plt $ import pandas as pd $ df=pd.read_csv('inf.csv')
plot_BIC_AR_model(data=doc_duration.diff()[1:], max_order_plus_one=10)
date_var = ['visitStartTime', 'date']
def load_json_df(filename, num_bytes = -1): $
from test_package import package_within_package
print(len(node_names)) $ print(len(edges))
plt.hist(p_diffs); $ plt.title('Simulated Difference of New Page and Old Page Converted Under the Null'); $ plt.xlabel('Page difference'); $ plt.ylabel('Frequency');
print ('Difference of means:', Ralston["TMAX"].mean() - Ralston["TMIN"].mean())
geo_TempJams.info()
sub1 = sub1.sort(['hacker_id', 'time'], ascending = [False, False])
h5
!scrapy runspider src/ds_web_data_hello_scrapy.py
full_data_doc = [(token, token.ent_type_, token.pos_, token.lemma_) for token in doc] $ print(full_data_doc)
chart = top_supporters.head(5).amount.plot.barh() $ chart.set_yticklabels(top_supporters.contributor_fullname)
lbl = LabelEncoder() $ train['air_store_id2'] = lbl.fit_transform(train['air_store_id']) $ test['air_store_id2'] = lbl.transform(test['air_store_id'])
count_non_null(geocoded_df, 'Judgment.In.Favor.Of')
valid = pd.read_csv(valid_file, header=0, sep='\t', na_values=[''], keep_default_na=False) $ X_valid = valid.iloc[:,features_index] $ y_valid = valid['Label'] $ np.all(valid.columns == train.columns)
df_ratings.shape
df99 = pd.read_csv('1999.csv')
pd.merge(left,right, on='key')
df["booking_user_agent"] = df["booking_user_agent"].str.split("/", expand=True)[0]
import urllib.request $ post_url = 'https://api.instagram.com/oembed?url=' + df.iloc[0]['link_to_post'] + '/' $ response = urllib.request.urlopen(post_url) $ html = json.load(response)   $ print(html['html'])
plot = song_tracker["rockstar"][1:].astype(float).plot(figsize=(20,10),title="Rockstar by Post Malone") $ plot.set_xlabel("Date") $ plot.set_ylabel("Chart Position")
print('Most dril:', tweets_pp[tweets_pp.handle == 'wint'].sort_values( $     'dril_pp', ascending=False).text.values[0]) $ print('Least dril:', tweets_pp[tweets_pp.handle == 'wint'].sort_values( $     'dril_pp', ascending=True).text.values[0])
grouped = s4g.groupby('Symbol') $ type(grouped.groups) $ grouped.groups
df.info()
%timeit df_protest.rangecode.map(lambda x: our_function(x))
prcp_1_df.head() # Display the top 5 records of the dataframe
df.head(500).to_csv('comma_delim_trunc.csv', sep=',')
X_train[:3]
import numpy as np $ import pandas as pd $ from sklearn.ensemble import RandomForestClassifier $ from sklearn.model_selection import train_test_split $ from sklearn.metrics import log_loss
mention_pairs["FromType"] = "Person" $ mention_pairs["ToType"] = "Person" $ mention_pairs["Edge"] = "Mentioned" $ mention_pairs.rename(columns={"screen_name":"FromName","mention":"ToName"},inplace=True) $ mention_pairs.head()
liberiaCasesSuspected = liberiaFullDf.loc['New Case/s (Suspected)'] $ liberiaCasesProbable = liberiaFullDf.loc['New Case/s (Probable)'] $ liberiaCasesConfirmed = liberiaFullDf.loc['New case/s (confirmed)'] $ liberiaCasesSuspected.info()
df_city_dummy = pd.get_dummies(df_more['City']) $ df_city_dummy.head()
linear_svc = LinearSVC() $ linear_svc.fit(X_train, Y_train) $ Y_pred = linear_svc.predict(X_test) $ acc_linear_svc = round(linear_svc.score(X_test, Y_test) * 100, 2) $ acc_linear_svc
df_totalConvs_day = df_conv.groupby(['datestamp'], as_index=False).sum() $ print '\n DataFrame df_totalConvs_day', df_totalConvs_day.shape $ df_totalConvs_day.head()
learn.save('clas_0')
S.decision_obj.stomResist.value = 'simpleResistance' $ S.decision_obj.stomResist.value
!gcloud ml-engine models create {model_name} --regions {storage_region}
driver = webdriver.Firefox(executable_path = "C:/libs/web_driver_for_crawl/geckodriver") $ driver.wait = WebDriverWait(driver, 10)
data['Closed Date'] = data['Closed Date'].apply(lambda x:datetime.datetime.strptime(x,'%m/%d/%y %H:%M'))
mb.swaplevel('Patient', 'Taxon').head()
lda.print_topics()
fare = df_titanic['fare'] $ print(fare.describe()) $
p_diffs = [] $ for _ in range(10000): $     new_page_converted=np.random.binomial(n_new,p_new) $     old_page_converted=np.random.binomial(n_old,p_old) $     p_diffs.append(new_page_converted/n_new-old_page_converted/n_old)
df_weather_origin.groupby("ORIGIN").count()
image_predictions[image_predictions.duplicated('tweet_id')]
df_users.iloc[:,4] = pd.to_datetime(df_users.iloc[:,4], format='%Y-%m-%d %H:%M:%S') $ df_events.iloc[:,5] = pd.to_datetime(df_events.iloc[:,5], format='%Y-%m-%d %H:%M:%S') $ df_events.iloc[:,7] = pd.to_datetime(df_events.iloc[:,7], format='%Y-%m-%d %H:%M:%S') $ df_log.iloc[:,2] =  pd.to_datetime(df_log.iloc[:,2], format='%Y-%m-%d %H:%M:%S') $ unknown_users.iloc[:,2] = pd.to_datetime(unknown_users.iloc[:,2], format='%Y-%m-%d %H:%M:%S')
autos["odometer_km"].describe()
import datetime; print(datetime.datetime.now())
os.getcwd()
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(new_df2.set_index('user_id'), how='inner')
print response.content $
lgb_pred_best
pop_dog = timedog_df.groupby('userLocation')[['tweetRetweetCt', 'tweetFavoriteCt']].mean() $ pop_dog.head()
temp_df = proportion_and_count(active_psc_records,'nationality',len(active_psc_records[active_psc_records.kind == 'individual-person-with-significant-control'])) $ temp_df['count'].head(5).to_csv('data/viz/nationalities.csv',index=False) $ temp_df.head(5)
JSON_SERVICE_KEY = 'JSON SERVICE KEY PATH AND FILENAME' $ JSON_SERVICE_KEY = '~/BigDataSystems-Spring2018.json'
xml_in = pd.read_csv('/Users/aj186039/projects/PMI_UseCase/data/DblpParser/01titleAuthorVenue.txt', sep='|', $                      header=None, skiprows=1, names=['publicationTitle','authorId','authorName','venueName','publicationDate','publicationKey'], $                      encoding='utf-8', low_memory=False) $ xml_in.shape $
query = spark_hive.sql('create table reddit_comments_data_hive as select * from tmp_reddit_df') $ query.show()
testy.head()
print gs_2.best_params_ $ print gs_2.best_score_
x_train1, x_test1, y_train1, y_test1 = train_test_split(x_train, y_train, random_state = 0)
autos["odometer"].unique()
telecom2 = telecom1.drop(['count_rech_2g_6', 'count_rech_2g_7', 'count_rech_2g_8', 'aug_vbc_3g', 'jul_vbc_3g', 'jun_vbc_3g' ], axis=1) $ telecom2 = telecom2.drop(['count_rech_3g_6', 'count_rech_3g_7', 'count_rech_3g_8', 'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8'], axis=1) $ telecom2 = telecom2.drop(['arpu_3g_6', 'arpu_3g_7', 'arpu_3g_8', 'arpu_2g_6', 'arpu_2g_7', 'arpu_2g_8'], axis=1) $ telecom2 = telecom2.drop(['arpu_6', 'arpu_7', 'arpu_8'], axis=1) $ telecom2 = telecom2.drop(['total_rech_num_6', 'total_rech_num_7', 'total_rech_num_8'], axis=1)
testDataVecs[0][0:10]
pr = [predictions[i][0] for i in range(len(predictions))]
print(cc['name'].describe())
genreTable = moviesWithGenres_df.set_index(moviesWithGenres_df['movieId']) $ genreTable = genreTable.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1) $ genreTable.head()
after.head()
candy = ['chocolate', 'caramel', 'peppermint'] $ df['candy'] = candy $ df
ts.resample('D').mean()
(p_diffs>(df2[df2['group']=='treatment']['converted'].mean()-df2[df2['group']=='control']['converted'].mean())).mean()
data.loc[:, ['comments', 'review_comments', 'merge_duration']].describe()
print("The number of rows in the dataset is " + str(df.shape[0]))
df = pd.read_sql("SELECT * from blog order by blog_id desc limit 10", conn) $ df.head()
df.sort_values(by=['Year'],ascending=True, inplace = True) $ df.head(5)
train.StateHoliday.head()
df2.query('landing_page == "new_page"').count()[0]/df2.count()[0]
df_new[['US', 'UK', 'CA']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop(['CA'], axis=1)
logit_mod3 = sm.Logit(df2['converted'], df2[['intercept','new_page_uk','new_page_us']]) $ results3 = logit_mod3.fit() $ results3.summary()
exploration_titanic.nearzerovar()
dc.head(2)
df_json_tweets['date'] = df_json_tweets['date_timestamp'].apply(lambda time: time.strftime('%m-%d-%Y')) $ df_json_tweets['time'] = df_json_tweets['date_timestamp'].apply(lambda time: time.strftime('%H:%M'))
all_df.onpromotion.fillna(False, inplace=True)
avg_order_intervals = np.fromiter(result.values(), dtype=float) $ avg_order_intervals.size
prob_for_new_page = (df2.landing_page == 'new_page').mean() $ print('Probability that an individual received the new page: ') $ print(prob_for_new_page)
returns = (mydata / mydata.shift(1)) - 1 $ returns.head()
df.columns
Z = np.arange(10, dtype=np.float32) $ Z = Z.astype(np.int32, copy=False) $ print(Z)
df3['country'].value_counts()
pd.set_option('display.expand_frame_repr', False) $ df = pd.read_csv("twitter-archive-enhanced.csv") $ df
X_train_all = pd.concat([X_train_df, X_train.drop('title', axis=1)], axis=1) $ X_test_all = pd.concat([X_test_df, X_test.drop('title', axis=1)], axis=1)
grouped_dpt.ngroups # number of groups 
print('Number of rows with invalid values = {}'.format(len(joined[joined.isnull().any(axis=1)]))) $ joined[joined.isnull().any(axis=1)]
data_archie.isnull().sum()
b.c
from pandas import scatter_matrix $ scatter_matrix(train_data.get(["nb_words_content", "pp_uniq_words"]), alpha=0.2, $                figsize=(6, 6), diagonal='kde')
bgf = BetaGeoFitter(penalizer_coef=0.0) $ bgf.fit(m['frequency'], m['recency'], m['T']) $ print(bgf) $
plot_ngram(sorted_unigram,'Unigram')
f1_A.exclude_list_filter
dcrime["crime_inc"] = 1 $ dcrime_gb = dcrime.loc[dcrime_in,["long","lat","crime_inc"]].groupby(["long","lat"],as_index=False).sum()
df2.drop([2893], inplace=True)
print('The number of unique users in the dataset is {}.'.format(df['user_id'].nunique()))
snow.select("select * from st_rvo_me_ref").to_excel("out/ref.xlsx", index=False)
grouped_df = xml_in.groupby(['authorId', 'authorName'], as_index=False)['publicationKey'].agg({'countPublications': 'count'})
per_tweet_archive_by_month = tweet_counts_by_month.copy() $ per_tweet_archive_by_month['favorite_count'] /= per_tweet_archive_by_month['tweet_count'] $ per_tweet_archive_by_month['retweet_count'] /= per_tweet_archive_by_month['tweet_count'] $ per_tweet_archive_by_month.drop('tweet_count', axis=1, inplace=True) $ per_tweet_archive_by_month.columns = ['favorite_count_per_tweet', 'retweet_count_per_tweet']
soup = BeautifulSoup(html, 'lxml')
import statsmodels.api as sm $ convert_old = sum(old_df.converted) # count of 1's from new and old $ convert_new = sum(new_df.converted) $ n_old = len(old_df) $ n_new = len(new_df)
pd.Series([30, 35, 40], index=['2015 Sales', '2016 Sales', '2017 Sales'], name='Product A')
output_location = 's3://{}/{}/output'.format(bucket, prefix) $ print('training artifacts will be uploaded to: {}'.format(output_location))
psy_prepro = pd.read_csv("psy_prepro.csv") $ psy_prepro = psy_prepro.set_index('subjectkey', verify_integrity=True) $
merged1.drop('id_y', axis=1, inplace=True)
twitter_archive_enhanced_clean = twitter_archive_enhanced_clean[twitter_archive_enhanced_clean.retweeted_status_id.isna()] $ twitter_archive_enhanced_clean = twitter_archive_enhanced_clean.drop(['retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'], axis = 1)
df_draft = pd.merge(draft_dframe, gm_df, on = ['Team', 'Draft_Yr'], how = 'outer') $ df_draft = df_draft[df_draft.Player.notnull()] $ df_draft.drop(df_draft.columns[[0]],inplace=True,axis=1) $ df_draft.head()
df = pd.DataFrame({"id":[1,2,3,4,5,6], "raw_grade":['a', 'b', 'b', 'a', 'a', 'e']}) $ df
All_tweet_data_v2.rating_numerator.value_counts().sort_index()
closed_daily = data.set_index('closed_at').resample('D').size() $ closed_monthly_mean = closed_daily.resample('M').mean() $ closed_monthly_mean.index = closed_monthly_mean.index.strftime('%Y/%m') $ iplot(closed_monthly_mean.iplot(asFigure=True, dimensions=(750, 500), vline=['2017/09', '2017/03', '2016/09', '2016/03', '2015/09', '2014/12', '2014/04', '2013/10']))
df[df.pct_chg_opencls < -0.98]
frankfurt.info()
ttarc.info()
print(np.exp(results.params)) $ print('\n') $ print(1/np.exp(results.params))
old_page_converted = np.random.binomial(1, p_old, n_old) $ sum(old_page_converted)
pd.to_datetime(pd.Series(['Jul 31, 2009', 'Nov 22 1985', '2005/11/22']))
df2.tail()
df1 = df.set_index('time_created')
model.doesnt_match("france england germany berlin".split())
reviews.points.median()
df_weather['HOURLYVISIBILITY'] = df_weather['HOURLYVISIBILITY'].apply(lambda x: convert(x)) $ df_weather['HOURLYDRYBULBTEMPC'] = df_weather['HOURLYDRYBULBTEMPC'].apply(lambda x: convert(x)) $ df_weather['HOURLYWindSpeed'] = df_weather['HOURLYWindSpeed'].apply(lambda x: convert(x)) $ df_weather['HOURLYPrecip'] = df_weather['HOURLYPrecip'].apply(lambda x: convert(x))
twitter_archive_full[(twitter_archive_full.text.str.contains('doggos'))][['tweet_id','stage','text']]
data.area
for category in unique_categories: $     print (category, '\n') $     print (all_df[category].unique()) $     print ('\n')
print(len(df_new))
p_old = df2[df2['landing_page']=='old_page']['converted'].mean() $ print("The probability of conversion for the old page is: " + str(p_old))
raw_data = raw_data.loc[raw_data['stock_type'] != 'none'] $ edukits = raw_data.loc[raw_data['stock_type'] == 'education_kit'] $ raw_data = raw_data.loc[raw_data['stock_type'] != 'education_kit'] $ raw_data = raw_data[~raw_data['stock_num'].str.contains('SERVICE')] $ raw_data = raw_data.loc[raw_data['order_num'] != 109485]
chinadata.shape
%%R $ head(flightsDB)
access_logs_df = access_logs_parsed.toDF()
classes = df["sentiment"].values
df.sample(10)
df_everything_about_DRGs.insert(0, 'drg3_str', '') $ df_everything_about_DRGs['drg3_str'] =df_everything_about_DRGs['drg'].apply(lambda x: x[:3]) $ df_everything_about_DRGs.head()
np.mean(p_diffs)
df2 = df2.drop_duplicates(['user_id']) $ df2.shape
df2.head()
print total_sales.q1_pct_change.describe() $ print np.sum(total_sales.q1_pct_change > 3)
print(linreg.coef_) $ print("Function: " + " + ".join(["%.8f" % linreg.coef_[i] + "x^" + str(i) for i in range(len(linreg.coef_))])) $ print(linreg.score(quadratic, y)) # Figure out the R^2 "score" of our prediction (out of 1.0)
df = df.drop('Unnamed: 0', axis = 1)
df1['2017-06-21']
%store page.text > barclay_banks.txt
organisation = pd.read_csv(data_repo + 'organisation.csv', **import_params)
autos['odometer'].value_counts()
df_usa=df_usa.pivot(columns='Variable Name',values='Value')  $ df_usa.head()
nnew = 145311
for i in range(2): $     _ = save_tweet_loc(geocode[i], name[i], conv, since, until)
df2[df2['user_id'].duplicated()]
binary_cols=['slot', 'adNetworkType']
df = pd.read_csv('ab_data.csv') $ df.head()
print(spmat.shape)
api_df.info()
ol.data
df.loc[0, 'fruits']
first_movie.h3
station_distance['Start Station Latitude'] = station_distance['Start Station Latitude'].astype(str) $ station_distance['Start Station Longitude'] = station_distance['Start Station Longitude'].astype(str) $ station_distance['End Station Latitude'] = station_distance['End Station Latitude'].astype(str) $ station_distance['End Station Longitude'] = station_distance['End Station Longitude'].astype(str) $ station_distance.info()
all_311_requests.to_csv("MyLA311_All_Requests.csv", encoding = 'utf-8', index = False)
ffrM = ffrM_resample.first() $ df_info(ffrM)
df_mes2 = df_mes2.drop(['tpep_pickup_datetime','tpep_dropoff_datetime'], axis=1) $ df_mes2 = pd.get_dummies(df_mes2, drop_first=True)
df_complete['dog_stage'].value_counts()
brand_info
print("accuracy for train data is",best_model_lr.score(train_ind[features],train_dep[response])) $ print("accuracy for test data is",best_model_lr.score(test_ind[features],test_dep[response])) $ print("accuracy for complete data is",best_model_lr.score(kick_projects_ip_scaled_ftrs,kick_projects_ip[response])) $ print (" Confusion matrix on complete data is", confusion_matrix(kick_projects_ip[response], kick_projects_ip["Pred_state_LR"]))
dfd.query("zones == 'Multi'").hspf.hist()
for p in mp2013: $   print("{0} {1} {2} {3}".format(p, p.freq,  p.start_time, p.end_time))
autos['registration_month'].describe()
re.sub("[^a-zA-Z]", " ", df.text[175])
def Check_Data(dataframe, number): $     return dataframe.head(number)
education_data.head(10)
df.info()
new_page_converted = np.random.choice([0, 1], n_new, p = [p_new, 1-p_new])
frame.index.names = ['key1', 'key2'] $ frame.columns.names = ['state', 'color'] $ frame
logit_mod = sm.Logit(df2['converted'], df2[['intercept','new_page']]) $ results = logit_mod.fit()
df_train.to_csv("train_transformV2.csv")
locale_name_regional_hols=regional_holidays['locale_name'].unique() $ print(locale_name_regional_hols)
staff = staff.reset_index() $ staff
grouped.describe()
p_diffs= np.random.binomial(n_new, p_new, 10000)/n_new\ $         -np.random.binomial(n_old, p_old, 10000)/n_old
df_clean2['tweet_id'] = df_clean2['tweet_id'].astype(str)
print(autos['odometer_km'].unique().shape) $ print(autos['odometer_km'].describe()) $ print(autos['odometer_km'].value_counts().sort_index(ascending=False))
df.plot(x='favorite_count', y='retweet_count', title='Favorite Count by Retweet Count', kind='scatter'); $ plt.savefig('retweet_vs_favorite.png')
df['index1'] = df.index
submission_full['proba'].head()
from pymongo import MongoClient
 df2["intercept"] = 1
a_result = df1.append(df2) $ a_result
data_summary = data.describe() $ data_summary
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],alternative='smaller') $ z_score, p_value
for index in All_tweet_data_v2.index: $     if (All_tweet_data_v2.loc[index,'pupper']=='None' and All_tweet_data_v2.loc[index,'puppo']=='None' and All_tweet_data_v2.loc[index,'floofer']=='None' and All_tweet_data_v2.loc[index,'doggo']=='None'): $         All_tweet_data_v2.loc[index, 'unknown']='Unknown' $     else: $         All_tweet_data_v2.loc[index, 'unknown']='None'
collection.append('AAPL', aapl[-1:]) $ df = collection.item('AAPL').to_pandas() $ df.tail()
len(df[df['SalesOffice']=='STD'])
subset_uuid = ekos.get_unique_id_for_alias(user_id, workspace_alias = 'lena_newdata', subset_alias = 'A') $ w.get_subset_list()
featured_img_url = "https://www.jpl.nasa.gov" + current_img_url $ featured_img_url
BUCKET = 'cloudonair-ml-demo' $ PROJECT = 'cloudonair-ml-demo' $ REGION = 'us-central1'
del frame2['eastern'] $ frame2
giss_temp["Jan"].astype("float32")
pred1 = nba_pred_modelv1.predict(g1) $ prob1 = nba_pred_modelv1.predict_proba(g1) $ print(pred1) $ print(prob1)
calls_nocontact.zip_code.value_counts()
a[2]
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])
df = pd.DataFrame({'date': (list(pd.date_range('2000-01-03', '2000-01-05')) * 4), $           'item': (list('ABCD'*3)), $           'status': (np.random.randn(12))}) $ print df
lm = sm.Logit(df3['converted'], df3[['intercept', 'US', 'UK']]) $ results = lm.fit() $ results.summary()
convert_date = (lambda row: datetime.strptime(str(row['DTYYYYMMDD'])+str(row['TIME']), '%Y.%m.%d%H:%M')) $ pair['DATETIME'] = pair.apply(convert_date, axis=1)
stat_info[1]
df.floofer.value_counts()
linkNYC.crs = from_epsg(4326) $ linkNYC = linkNYC.to_crs(epsg=2263) $ linkNYC.crs
lostintranslation_imdb = response_imdb[0] $ ia.update(lostintranslation_imdb) $ print "Title:", lostintranslation_imdb["title"] $ print "Director:", lostintranslation_imdb["director"][0] $ print "Genre:", lostintranslation_imdb["genre"][0]
store_items[['bikes', 'pants']]
X = stock.drop(['target', 'true_grow', 'predict_grow'], 1) $ y = stock.true_grow
print(groceries['eggs']) $ print('\n', groceries[['milk', 'bread']]) $ print('\n', groceries[0]) $ print(groceries[-1]) $ print(groceries[[0, 1]])
min(df2.timestamp), max(df2.timestamp)
df = pd.DataFrame(tweets_data)
test['visitors']=model_avg $ test['visitors'] =(test['visitors']).clip(lower=0.)
clean_df = pd.DataFrame() $ clean_df["id"] = df["id"] $ clean_df["summary_ids"] = df["summary_ids"].apply(lambda x : x[:-1]) $ clean_df["summary_count"] = df["summary_ids"].apply(lambda x : len(x[:-1].split(","))) $ clean_df.head()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ z_score, p_value
small_train = small_train.dropna() $ small_train.shape
from repository.mlrepositoryclient import MLRepositoryClient $ from repository.mlrepositoryartifact import MLRepositoryArtifact
df.dtypes
1-0.090999999999999998
import pyspark.sql.functions as func $ hashed_test.groupBy().agg(func.max(col('id'))).show()
df2[['control', 'ab_page']] = pd.get_dummies(df['group']) $ df2.drop('control', axis = 1, inplace = True)
train_visitor_map = train_data[['date','fullVisitorId']] $ test_visitor_map = test_data[['date','fullVisitorId']]
data = {'empID':  [100,      101,    102,      103,     104], $         'year':   [2017,     2017,   2017,      2018,    2018], $         'salary': [40000,    24000,  31000,     20000,   30000], $         'name':   ['Alice', 'Bob',  'Charles', 'David', 'Eric']} $ pd.DataFrame(data)
top_songs['Track Name'].isnull().sum()
popCon.sort_values(by='counts', ascending=False).head(9).plot(kind='bar')
users_visits = users_visits.groupby('chanel', as_index=False).sum() $ users_visits = users_visits.assign(retention=lambda x:x.visits/x.regs).sort_values('retention') $ users_visits.head()
snow.select("select * from st_rvo_index order by index_date desc limit 5")
ti_jd.rename(columns={'review_image':'image_url','#comments':'review_image','harvest_product_description':'product_description','retailer_product_code':'rpc','user_id':'username'}, inplace=True) $ ti_jd['store'] = 'JD' $ ti_jd = ti_jd[ti_clm] $ ti_jd.shape
list_2d = [('Jack', 1, 'NY'), ('Sally', 2, 'CA'), ('Chris', 3, 'FL')] $ pandas_list_2d = pd.DataFrame(list_2d) $ print(pandas_list_2d)
titanic = get_dataset('titanic')
import random $ sample = all_data_merge.sample(n = 1000, replace=True) $ sample.to_csv('Nestle_sample_March.csv', index = False, sep = ',', encoding = 'utf-8')
neg = np.sum(neg_tf,axis=0) $ nut = np.sum(nut_tf,axis=0) $ pos = np.sum(pos_tf,axis=0) $ term_freq_df = pd.DataFrame([neg,nut,pos],columns=cvec.get_feature_names()).transpose() $ term_freq_df.head()
import pickle $ with open('Hospital_List', 'wb') as fp: $     pickle.dump(list_of_hospitals, fp) $
imagelist $ for k in range(len(imagelist)): $     print('run txt2pdf.py -o' + '"' + imagelist[k][:-4] + '.pdf' + '"' + ' '  + '"' +  imagelist[k] + '"' )
TEXT.vocab.stoi['the']
data=users.merge(transactions,how='left',on='UserID').groupby(['UserID']).min() $ data
combined_df['intercept'] = 1 $ logistic_model = sm.Logit(combined_df['converted'], combined_df[['intercept','ab_page', 'CA', 'US']]) $ result = logistic_model.fit()
Plot
if not os.path.exists(ml_1m_path): $     os.makedirs(ml_1m_path)
df_img_algo_clean.tweet_id = df_img_algo_clean.tweet_id.astype(str)
solar_wind_df = pd.read_excel(weather_folder + '/clean_solar_wind_df.xlsx', index = False)
y_range
df[~df.index.isin([5,12,23,56])].head(13)
cabs_df_rsmpld = cabs_df_byday.resample('1M')['passenger_count'].count() $ cabs_df_rsmpld.head()
anomaly_df.apply(lambda row: store_news_1day(row.Ticker, row.Date, stored_list=news_list, verbose=True), axis=1 )
merged1['AppointmentCreated'] = pd.to_datetime(merged1['AppointmentCreated'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d') $ merged1['AppointmentDate'] = pd.to_datetime(merged1['AppointmentDate'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d')
stream_measures = pd.DataFrame(stream_dict).T.rename_axis('stream').add_prefix('measures').reset_index()
tweets_clean.drop(columns = ['doggo','floofer','pupper','puppo'], inplace = True) $ tweets_clean.head()
print("Shape: " + str(cleanedDataNoRetweets.shape)) $ print(cleanedDataNoRetweets.head())
df1.index.values
autos[['date_crawled','last_seen', $       'ad_created']][0:5]
data = pd.concat(datasets, keys=['18', '17', '16', '15', '14', '13', $                                  '12', '11', '10', '09', '08', '07', $                                  '06', '05', '04', '03', '02', '01'], sort=False) $ data.head()
autos['num_photos'].value_counts()
path_leaderboard = '../metadata/leaderboards.json' $ leaderboard = pd.read_json(path_leaderboard) $ leaderboard['best'] = pd.to_numeric(leaderboard['best'], errors='coerce') $ leaderboard.head()
w = 'hitler' $ model.wv.most_similar (positive = w)
start = pd.Timestamp('2015-07-11 14:45:00') $ finish = pd.Timestamp('2015-09-24 19:15:00') $ time_index = pd.date_range(start,finish,freq='10t')
ad_groups_zero_impr.groupby( $     ['CampaignName', 'Date'] $ )['CampaignId'].count()
plt.style.use('ggplot') $ bb['close'].apply(rank_performance).value_counts().plot(kind='pie', legend=True)
import pandas as pd $ DATA_FILE = "jallikattu.json" $ data = "[{0}]".format(",".join([l for l in open(DATA_FILE).readlines()])) $ df = pd.read_json(data, orient='records') $ print "Successfully imported", len(df), "tweets"
jobs.loc[jobs.FAIRSHARE == 132].groupby('ReqCPUS').JobID.count().sort_values(ascending= False)
tweet_full_df['tweet_id']=tweet_full_df['tweet_id'].astype(str)
bacteria_data.index[0] = 15
df_new = df2.merge(df3, on='user_id', how='left') $ df_new.head()
results.coordinates
c = bnbAx[bnbAx['language']=='es'].first_browser.value_counts()/len(bnbAx[bnbAx['language']=='es']) $ c.head()
classifier = MLPClassifier(hidden_layer_sizes=(25,25)) $ classifier.fit(X_train,y_train) $ preds = classifier.predict(X_test) $ print(preds)
X = df[['word_count','sentiment','subjectivity','domain_d','post_duration']] $ y_cm = LabelEncoder().fit_transform(df['com_label'])#Comments $ y_sc = LabelEncoder().fit_transform(df['score_label'])# Score $
dtr = DecisionTreeRegressor(min_samples_leaf = 3, $                             max_depth = 8, $                             random_state = 2) $ scoring(dtr)
churned_ordered.index
gcv.fit(fb_train.message, fb_train.popular)
ndvi_us = ndvi_nc.variables['NDVI'][0, lat_li:lat_ui, lon_li:lon_ui] $ np.shape(ndvi_us)
text_classifier.get_step_params_by_name("text1_char_ngrams")
new_page_user_count = df2[df2['landing_page'] == 'new_page']['user_id'].count() $ new_page_user_count / total_users $
df_t_worst['Shipped At'] = df_t_worst['Shipped At'].apply(lambda x: x.date()) $ df_worst_chart = pd.DataFrame(df_t_worst[df_t_worst['Place Name'].isin(['Diemen','Stadscentrum','Eindhoven'])] $                     .groupby(['Place Name','State','Latitude','Longitude','Shipped At'])['Updated Shipped diff_normalized'] $              .mean()).sort_values(by='Place Name',ascending=True)
rate.dtypes # Check the dtypes of the rate dataframe
archive_clean['doggo'].replace(np.nan,'', inplace=True) $ archive_clean['floofer'].replace(np.nan,'', inplace=True) $ archive_clean['pupper'].replace(np.nan,'', inplace=True) $ archive_clean['puppo'].replace(np.nan, '', inplace=True) $ archive_clean['stage'] = archive_clean[['doggo', 'floofer','pupper','puppo']].apply(lambda x: ''.join(set(x.astype(str))), axis=1) $
df['Agency'].value_counts()
df.shape
aSL.loc[(aSL.BRANCH_STATE.isnull())].shape
af.sort_values(by=['length'], ascending=False).head()
datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
df_new = pd.merge(df2, countries_df, how='inner', left_index=True, right_index=True) $ df_new[['UK','US']] = pd.get_dummies(df_new['country'])[['US','UK']] $ df_new.sample(5)
reddit['Work Day Hours'] = reddit['Hours'].apply(lambda x: 1 if x >9 and x <17 else 0)
import numpy as np $ import pandas as pd $ import geocoder
pd.scatter_matrix(df);
bm1 = list(map(int, benchmark11)) $ print('benchmark1 score is: {}'.format(np.around(f1_score(actual1, bm1, average='weighted'), decimals=5))) $
len(train[train['price_per_bedroom']==np.inf]), len(train[train['price_per_bathroom']==np.inf]), len(train)
df_never_moved = df.head(0)
baseball.hr.sort_values()
restaurants = pd.read_csv("/home/ubuntu/data/restaurant.csv", dtype=unicode, index_col=["CAMIS"], encoding="utf-8") $ restaurants
rounds.dtypes
os.listdir()
for i in range(-5, 0, 1) : $     data[f'Close {i}d'] = data['Close'].shift(-i) $ data = data.dropna() $ data.head()
s.resample('Q', 'last').head()
raw_data = pd.read_csv('train.csv')#, encoding='1252',low_memory=False, parse_dates=True) $ col_names = ['deadline', 'state_changed_at', 'created_at', 'launched_at'] $ for i in col_names: $     raw_data[i] = pd.to_datetime(raw_data[i], unit='s') $ raw_data.head(3).T
intake.head()
df.groupby(by = ['County', 'Zip Code', 'City']).mean()[[5]].sort_values(by = ['State Bottle Retail'], ascending = False)
data_chunks = pd.read_csv("Data/microbiome.csv", chunksize=15) $ mean_tissue = pd.Series({chunk.Taxon[0]: chunk.Tissue.mean() for chunk in data_chunks}) $ mean_tissue
df
working_data = [df_train, df_test] $ working_data = pd.concat(working_data) $ working_data = working_data.reset_index() $ working_data['date'] = pd.to_datetime(working_data['date']) $ working_data = working_data.set_index('date')
data.last_trip_date = pd.to_datetime(data.last_trip_date) $ data.signup_date = pd.to_datetime(data.signup_date)
portfolio.df_returns()
Path=f'data/' $ train_path=Path+"train.csv" $
!curl -X GET 'http://localhost:8080/ping'
data['SA'] = np.array([analize_sentiment(tweet) for tweet in data['Tweets']])
print("The probability of individual in the treatment group converting is: {}".format(df2[df2['group'] == 'treatment']['converted'].mean())) $
df.drop_duplicates(subset=['last_name'], keep='last') $
idxs = get_cv_idxs(n, val_pct=150000/n) $ joined_samp = joined.iloc[idxs].set_index("Date") $ samp_size = len(joined_samp); samp_size
predictions.show(5)
def save_tweets(tweets, path): $     with open(path, "w") as f:        $         json.dump(tweets, f) $
i_rowcount = df.count() $ i_rowcount
to_be_predicted_Day1 = 14.48 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
df_train = pd.concat((df_train, pd.read_csv('C:/Users/ajayc/Desktop/ACN/2_Spring2018/ML/Project/WSDM/DATA/train_v2.csv',dtype={'is_churn' : np.int8} )), axis=0, ignore_index=True).reset_index(drop=True)
df_tweet_clean.duplicated('id').value_counts()
cityID = '960993b9cfdffda9' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Bakersfield.append(tweet) 
df.to_csv('ab_cleandata.csv', index=False)
wrd_api
htmldmh = dmh.HTML('https://www.fdic.gov/bank/' +         $                    'individual/failed/banklist.html', index=0)
sq83= "CREATE TEMPORARY TABLE  newtable_22222 ( SELECT * FROM Facebook_NBA order by 0.2*likes+0.4*Comments+0.4*shares DESC limit 150)" $ sq84="SELECT word, COUNT(*) total FROM ( SELECT DISTINCT Id, SUBSTRING_INDEX(SUBSTRING_INDEX(message,' ',i+1),' ',-1) word FROM newtable_22222, ints) x where word like'%#%'GROUP BY word HAVING COUNT(*) > 0 ORDER BY total DESC, word;"
pd.get_dummies(df2['landing_page']).head()
plt.plot(mydf6.index[:10000],mydf6.fuelVoltage[:10000]);
Google_stock.describe()
pd.Timestamp("now")
proj_df[proj_df['Project Cost'] > 1000]
df["grade"].cat.categories = ["excellent", "good", "okay", "bad", "miserable", "fail"] $ df
from scipy.stats import norm $ norm.cdf(z_score) $ norm.ppf(1-(0.05)) $
df2_control = df2.query('group == "control"') $ df2_control.converted.mean()
x = my_df.text $ y = my_df.target
control_df = df2.query('group == "control"') $ p_control = control_df.query('converted == 1').user_id.nunique()/control_df.user_id.nunique() $ print('Probability of conversion for the control group is {}.'.format(round(p_control,4)))
authors = db.get_sql(sql) 
with_countries['CA_ab_page'] = with_countries['ab_page']* with_countries['CA'] $ ca_new_page = sm.Logit(with_countries['converted'], with_countries[['intercept', 'ab_page', 'CA_ab_page', 'CA']]).fit() $ print(ca_new_page.summary())
nba_df.columns
json.dumps(obj)
series2 = df2['FlightDate'].head() + ' ' + df2['DepTimeStr'].head() $ pd.to_datetime(series2)
inputs, targets = zip(*dataset) $ inputs = np.array([string_to_int(i, 20, human_vocab) for i in inputs]) $ targets = [string_to_int(t, 20, machine_vocab) for t in targets] $ targets = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), targets)))
type(None)
dfSavours
X_train = np.concatenate((train[simple_features].values, X_train_feature_counts.toarray()), axis=1)
from pyaerocom.io.read_aeolus_l2b_data import ReadAeolusL2bData $ ADM = ReadAeolusL2bData(verbose=True)
google['high'].apply(custome_roune).nunique()
fdist.plot(100, cumulative=False)
res = dfs.join(dfpf, dfpf.rf == dfs.rf).select(dfs.rf, dfpf.qty, dfs.date, dfs.neutral, dfs.scenarios)
scaled = scaled.dropna() $ scaled.head()
from datetime import datetime $ def get_time_since_last_publication(input_): $     now = datetime.now() $     return (now - max(input_)).days $ grouped_publications_by_author['time_since_last_publication'] = grouped_publications_by_author['publicationDates'].apply(get_time_since_last_publication) $
trn_df.head(1)
p_old = (df2.converted).mean() $ p_old
b_cal_q1.head(2)
author_commits = git_log.groupby('author').count().sort_values(by='timestamp', ascending=False) $ top_10_authors = author_commits.head(10) $ top_10_authors
print(who_purchased.sum(axis=0).sum()) $ print() $ print(who_purchased.sum(axis=0))
Queens_gdf = newYork_gdf[newYork_gdf.boro_name == 'Queens'] $ Queens_gdf.crs = {'init': 'epsg:4326'}
print (test.shape)
df_protest.columns
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], 0, 'larger') $ print(z_score) $ print(p_value)
cityID = 'bced47a0c99c71d0' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Durham.append(tweet) 
datacamp['publishyymm'] = datacamp['publishdate'].dt.strftime("%Y-%b") $ datacamp["posts"] = 1
sub_df.drop('created', axis=1, inplace=True)
y = df3['converted'] $ X = df3[['intercept', 'new_page', 'UK_new_page', 'US_new_page', 'UK', 'US']] $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=0)
cur.execute('SELECT * FROM id LIMIT 5;') $ cur.fetchall()
X = data[features] $ y = data['new_claps']
house_data.head(20)
num_days_online = len(well_data['oil_bbl']) $ num_days_online
keys.loc[(keys.duplicated('APP_APPLICATION_ID'))].shape
ts + pd.Timedelta('5 days')
df_ror_1 = benchmark1['High'].pct_change() $ df_ror_2 = benchmark2['High'].pct_change()
vol
n_old = df2[df2['group'] == 'control'].shape[0] $ n_old
S_1dRichards.meta_basinvar.filename
plt.hist(p_diffs) #same histogram as above $ plt.axvline(x=obs_diff, color='red'); #adding in the line for the actual/observed difference since we are about to calculate p-value
X2 = PCA(2, svd_solver='full').fit_transform(X)
%matplotlib notebook $ pnls.plot()
events_pd = events.toPandas() $ events_pd['event_type'].value_counts().plot.bar()
df, y, nas, mapper = proc_df(joined_samp, 'Sales', do_scale=True) $ yl = np.log(y)
len(results)
JournalStories
image.sample(20)
for i in ["List_Of_Users_ID", "List_Of_Comments", "List_Of_Hashtags_Caption", "List_0f_Hashtags_Comments"]: $     df.loc[df[df[i].apply(lambda x: len(x)) == 0][i].index,i] = np.NaN $ df.loc[df[df["Main_Caption"] == 'No caption']['Main_Caption'].index,\ $    'Main_Caption'] = np.NaN
train.shape
modal_model.compile(optimizer=optimizers.Nadam(lr=0.0001), loss='cosine_proximity', metrics=['accuracy']) $ batch_size = 1200 $ epochs = 20 $ history = modal_model.fit([train_source_emb], train_target_emb, $                           batch_size=batch_size, epochs=epochs, validation_split=0.1)
thecmd = 'ogr2ogr -f "SQLite" -dsco SPATIALITE=YES ' + dataDir + 'input/new-york_new-york.db ' + dataDir + 'input/new-york_new-york.osm.pbf' $ print thecmd
all_vars = tf.trainable_variables() $ model_vars = [v for v in all_vars if v not in elmo_vars] $ print(model_vars) $ vars_dict = {v.name:v for v in tf.trainable_variables()}
def print_full(x): $     pd.set_option('display.max_rows', len(x)) $     print(x) $     pd.reset_option('display.max_rows') $
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY $ r = requests.get(url)
learn.lr_find()
new_page_converted = df2.sample(n_new, replace=True).converted $ p_new = sum(new_page_converted)/new_page_converted.shape[0] $ p_new
write_tweets(users_target[['username', 'tweet_id']], path='../data/twitter_personality/tweets_full_parsed.csv')
%matplotlib inline $ import numpy as np $ import matplotlib.pyplot as plt $ import sys
first_date = dt.date(2017,8, 23) - dt.timedelta(days=365) $ one_year_precipitation = session.query(Measurement.date, Measurement.prcp).\ $                         filter(Measurement.date > first_date).\ $                         order_by(Measurement.date).all() $ print(one_year_precipitation)
giss_temp.fillna(value=0).tail()
lift = attack_client()
namesnow = girls.loc[(girls.Fraction<1e-3)& (girls.Fraction>1e-4)&((girls.Year==2016) |(girls.Year==2013)) ,:] $ name_piv = pd.pivot_table(namesnow,values='Fraction',index=['Name'],columns=['Year']) $ name_piv['Delta']=(name_piv[2016]-name_piv[2013])/name_piv[2016] $ promising = name_piv.Delta.nlargest(30) $ print(promising.loc[promising.index.str.len()<7]) $
stock_data = pd.DataFrame(list(iex_coll_reference.find())) $ stock_data.head()
pandas_df = day_access.sort('dayOfWeek').select('count').toPandas() $ pandas_df
df['time open'] = df['closed_date'] - df['created_date'] $
has_null_value = blame.isnull().any(axis=1) $ dropped_entries = blame[has_null_value] $ blame = blame[~has_null_value] $ dropped_entries
merged = price2017.merge(typesub2017,how='inner',left_on='DateTime',right_on='DateTime')
%matplotlib inline $ import matplotlib.pyplot as plt $ import datetime as datetime $ df_tweet = pd.read_csv('twitter_archive_master.csv', encoding = 'utf-8')
train.head(5)
logit_new_countries = sm.Logit(df_countries['converted'], df_countries[['intercept','new_page','CA_new','UK_new','CA','UK']]) $ result_countries_new = logit_new_countries.fit() $ result_countries_new.summary()
twitter_archive_clean.head(3)
twitter_archive_enhanced.describe()
final_valid_pred_nbsvm1 = valid_probs.idxmax(axis=1).apply(lambda x: x.split('_')[1])
plt.hist(p_diffs); $ plt.axvline(-0.00127,color='red') $
bnb.groupby('affiliate_provider').count()
dfdaycounts['created_date'] = pd.to_datetime(dfdaycounts['created_date'], errors = 'coerce')
top_supporters.head(5).to_csv("top_supporters.csv")
users_converted=float(df.query('converted == 1')['user_id'].nunique()) $ p1 = (users_converted/total_users) $ print("The proportion of users converted is {0:.2%}".format(p1))
len(stock_name_ls)
pd.datetime.now()
df2.user_id.count()
Obama["sentiment"] = [sentiment_of_tweet(text) for text in Obama["text"]] $ Obama["negative"] = Obama["sentiment"]<0 $ Obama.groupby(by="negative").count()["id"]
wrd.query('name == "an"')['text']
df2['date'] = df.date
n_new = gb_page.loc['new_page'][0] $ n_new
data["hours"] = data["time_up_clean"].astype(str).str[0:2] $ data["hours"] = data["hours"].astype(int) $ data.loc[data["hours"] == 0, "hours"] = 1
len(fda_drugs.ActiveIngredient.unique())
count_non_null(geocoded_df, 'Disposition.Desc')
df2['landing_page'].replace({'old_page':0,'new_page':1},inplace=True) $
sentencelist = map(lambda text: list(jieba.cut(text,cut_all=True)),michael_df.text) $ reducedwordlist = reduce(lambda x,y:x+y,sentencelist)
movielen=len(movies['movieId'].unique()) $ userlen=len(ratings['userId'].unique()) $ sparsity=(len(ratings)*100)/(movielen*userlen) $ print(sparsity)
to_be_predicted_Day3 = 17.78535457 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
display(data.head(10))
staff.columns
df = pd.read_csv('ab_data.csv') # To Read CSV file $ df.head()
text1.concordance("monstrous")
import matplotlib.pyplot as plt $ df[['Date','AveCoalPrice']].set_index('Date').plot()
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','ca','uk']]) $ results = logit_mod.fit()
!wget https://s3.amazonaws.com/arrival/embeddings/wiki.multi.fr.vec -O /tmp/wiki.multi.fr.vec $ !wget https://s3.amazonaws.com/arrival/embeddings/wiki.multi.en.vec -O /tmp/wiki.multi.en.vec
X_train[['temp_c', 'prec_kgm2', 'rhum_perc']].plot(kind='density', subplots = True, $                                              layout = (1, 3), sharex = False)
autos["price"] = autos["price"].str.replace('$','').str.replace(',','').astype(float) $ autos["odometer"] = autos["odometer"].str.replace('km','').str.replace(',','').astype(float) $ autos.rename(columns={"odometer":"odometer_km"}, inplace=True) $ print(autos[["price","odometer_km"]])
autos["last_seen"].str[:10].value_counts(normalize=True, $                                    dropna = False $                                   ).sort_index(ascending = True)
lsi_corpus = lsi[tfidf_corpus] $ doc_vecs = [doc for doc in lsi_corpus] $
Nold = len(df2[df2['landing_page']=='old_page']) $ Nold
Z = np.random.random((10, 3)) $ zmin, zmax = Z.min(), Z.max() $ print((Z - zmin) / (zmax - Z)) $ print(Z.ptp)
tweets.shape
iris = pd.read_csv('data/iris.csv') $ print(iris.shape)
timelog['seconds'] = timelog.apply(dur2sec, axis=1)
rets = solar_df.pct_change() $ print(rets)
print("Score:", metrics.r2_score(stock.iloc[:-1].target, stock.iloc[:-1].forecast)) $ print("MSE:", metrics.mean_squared_error(stock.iloc[:-1].target, stock.iloc[:-1].forecast))
twitter_archive_full.rating_denominator.unique() $ len(twitter_archive_full[twitter_archive_full.rating_denominator == 0])
to_be_predicted_Day1 = 30.96 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
df2 = df2.drop_duplicates('user_id')
df_new = df2[(df2['landing_page'] == 'new_page')] $ converted = df_new['converted'] $ new_page_converted = np.random.choice(converted, n_new).mean() $ new_page_converted $
BID_PLANS_df.loc['e08b8a17']
cd NBA_Data
df['days_diff_period'] = df['datetime'].apply(lambda x: x.to_period('D') - df['datetime'].iloc[0].to_period('D')) $ df['month_diff_period'] = df['datetime'].apply(lambda x: x.to_period('M') - df['datetime'].iloc[0].to_period('M')) $ print(df.to_string())
%%bash $ cat 1st_flask_app_1/templates/first_app.html
run txt2pdf.py -o"2018-06-14 2148 FLORIDA HOSPITAL Sorted by Payments.pdf"  "2018-06-14 2148 FLORIDA HOSPITAL Sorted by Payments.txt"
df2.quarter_opened.value_counts()
tf.unique()
%matplotlib inline
df['hour'] = df['tweet_created'].dt.hour $ df['date'] = df['tweet_created'].dt.date $ df['dow'] = df['tweet_created'].dt.dayofweek
!pip install patsy $ from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt $ import matplotlib.pyplot as plt $ import pandas as pd
all_years_by_DRG =Grouping_Year_DRG_discharges_payments.groupby(level=['year','drg3']).sum() $ all_years_by_DRG.head() 
autos = autos[autos['price'].between(1,351000)] $ autos['price'].describe()
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country']) $ df_new.head()
store_items = store_items.set_index('pants') $ store_items
len(train),len(test)
df = pd.read_csv("data/311_Service_Requests_from_2010_to_Present.csv", $                  usecols= ['Borough', 'Agency', 'Closed Date', 'Created Date','Complaint Type', 'Descriptor'])
userArtistDF.describe().show()
current_img_url = current_img[21+len("('"):75] $ current_img_url
data.dropna()
vocab = vectorizer.get_feature_names() $ print(vocab)
url_NYG = "https://nygiants.strmarketplace.com/Images/Teams/NewYorkGiants/SalesData/New-York-Giants-Sales-Data.xls"
highest_temp_station_no = active_stations[0][0] $ highest_temp_obs = active_stations[0][1] $ print(f"The station with the highest number of temperature observations is {highest_temp_station_no} with total observations of {highest_temp_obs}.")
grpConfidence.count()
path = '/Users/Lucy/Google Drive/MSDS/2016Fall/DSGA1006/Data/csv_export' $ orgs = pd.read_csv(path + '/organizations.csv')
print(df2.query('landing_page == "new_page"').shape[0])
from scipy.stats import norm $ z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ print(str(z_score) + ", "+ str(p_value)) $ print("The significance of our z-score is " + str(norm.cdf(z_score))) $ print("Critical Z-score value at 95% confidence is " + str(norm.ppf(1-(0.05/2))))
print 'Use loc like any other Dataframe' $ print 'A single index value:' $ msft.loc['2012-01-03']
n_new = df2.loc[(df2.landing_page == "new_page")].user_id.nunique() $ n_new
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller' , prop_var=False ) $ print(z_score, p_value)
sandwich_ratings, sandwich_counts = topic_ratings_all(sandwich_dictionary, sandwich_train_model, cat_sandwich, 'reviews_without_rare_words', 10)
dfFull['BsmtFinSF2Norm'] = dfFull['BsmtFinSF2']/dfFull['BsmtFinSF2'].max()
pd.concat([df1,df2],axis=0,join='inner')
elms_all_0611.loc[(elms_all_0611.duplicated('ACCOUNT_ID'))].shape
result.summary2() # result.summary() wasn't working for some reason, but this one does
has_stage_archive = twitter_archive_master[twitter_archive_master['has_stage'] == 1] $ for column in has_stage_archive.iloc[:,8:12].columns: $     avg_rating = has_stage_archive[has_stage_archive[column] == 1]['rating'].mean() $     print('The average rating for {}s is {}'.format(column, avg_rating))
trans_counts=calc_arrears_pd(actual_payments,1,23)
df2 = df2.merge(dfc, on='user_id') $ df2.head()
df_vow.describe()
sum((df2.group == 'control')&(df2.converted == 1)) / sum(df2.group == 'control')
import requests $ from collections import defaultdict $ from datetime import datetime $
autos["registration_year"].describe()
r.text
tablename='email_templates' $ pd.read_csv(read_inserted_table(dumpfile, tablename), $             delimiter=",", $             error_bad_lines=False).head(10)
cohort_retention_sum = pd.Series([sum(cohort_retention_df[col]) for col in cohort_retention_df.columns],index=cohort_retention_df.columns,name='Total')
train = train.drop(['timestamp_first_active','date_first_booking', 'timestamp_first_active', 'date_first_booking'], axis=1)
len(treatment_df2)
sp = pd.read_csv('SPY.csv')
data['subreddit'].nunique()
model = model.fit(X,y)
Sort3 = stores.sort_values(by = ["Location","TotalSales"], ascending = [True,False])
print('Unique number of users notified: {}'.format(len(atloc_opp_loc['vendorId'].unique())))
%matplotlib inline
sets_node['date'][DATA].head(12)
logreg = LogisticRegression() $ model=logreg.fit(X_train, y_train)     # fitting the training data
ls_metac_colnames = ['MET_DATE2', 'METAC_SITE_NM2', 'MET_DATE3', 'METAC_SITE_NM3', 'MET_DATE4', 'METAC_SITE_NM4', 'MET_DATE5', $                      'METAC_SITE_NM5', 'MET_DATE6', 'METAC_SITE_NM6', 'MET_DATE7', 'METAC_SITE_NM7']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
dummy_var_df = ce.fit_transform(data['subreddit_other'].values.reshape(-1, 1))
import pandas as pd $ import sklearn $ print('The pandas version is {}.'.format(pd.__version__)) $ print('The scikit-learn version is {}.'.format(sklearn.__version__))
train_set.polarity_value.value_counts()
import pandas as pd $ df = pd.read_csv('movie_data.csv', encoding='utf-8') $ df.head(3)
SVPOL(data/'realdata'/'DD.Linux.dat').to_dataframe().head()
donors[donors['Donor Zip'] == 606 ].head()
tempXtf = pd.concat([X_trainfinaltf, X_testfinaltf]) $ print tempXtf.shape
len(all.user_id.unique())
log_mod=sm.Logit(result_df['converted'],result_df[['intercept','ab_page','UK','US']]) $ results=log_mod.fit()
tweets_df["retweet_mean"] = tweets_df.retweet_count.mean()
cnf_matrix = confusion_matrix(y_test, yhat_lr, labels=['PAIDOFF','COLLECTION']) $ np.set_printoptions(precision=2) $ print (classification_report(y_test, yhat_lr)) $ plt.figure() $ plot_confusion_matrix(cnf_matrix, classes=['PAIDOFF','COLLECTION'],normalize= False,  title='Confusion matrix')
train_session.head()
df_sb['blobw'] = df_sb['cleaned_text'].apply(TextBlob) $ df_sb['sentiment'] = df_sb['cleaned_text'].apply(sentiment_calc) $ df_sb['Polarity'] = df_sb['cleaned_text'].apply(polarity_calc) $ df_sb['Subjectivity'] = df_sb['cleaned_text'].apply(subjectivity_calc)
details["Popularity"] = pd.to_numeric(details.Popularity)
X_train.isnull().sum()
rng.tz is None
to_be_predicted_Day3 = 52.49437626 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
LSTM_test_inputs[0]
twitter_archive_master.sort_values(by=['favorite_count'], ascending = False).head()[['created_at','favorite_count','retweet_count','name','url','rating_num','dog_stage','breed' ]]
store = pd.HDFStore('../../sra.h5')
aldf[['salary_clean','salary_predict']]
print('band 58 center wavelength (nm): ',sercRefl_md['wavelength'][57]) $ print('band 90 center wavelength (nm) : ', sercRefl_md['wavelength'][89])
plt.figure(figsize=(10,10)) $ plt.plot(building_pa_prc_shrink['zipcode'],building_pa_prc_shrink['supervisor_district'],'o') $ plt.grid(True) $ plt.ylabel('supervisor_district') $ plt.xlabel('zipcode')
np.exp(rmse_CBoE)
predictions = mlb.inverse_transform(y_pred)
s = df_usa.idxmax() $ r = df_usa[df_usa['NRI']>0.1].idxmin()   #finding the min and max values $ print(s['NRI']) $ print(r['NRI'])
df_concat_2.boxplot(column="message_likes_rel", by="page")
lines = sc.textFile(csv_filename) $ parts = lines.map(lambda l: l.split(",")) $ rows = parts.map(parse)
df.drop('Ehail_fee', axis=1, inplace=True) $ df.columns
print(df_cat.shape) $ print(df_crea.shape) $ print(df_loc.shape) $ print(df_us_.shape)
df = pd.read_hdf('data/games.hdf','df') $ df
df = pd.read_csv('caderma_clean.csv')
dsdf = pd.DataFrame.from_records(ds)
df.columns = df.columns.str.lower() $ df.columns
df_tweet_clean.head()
df_new[['UK','US']] = pd.get_dummies(df_new['country'])[['UK','US']]
def cleaned_reviews(x): $     return (x.replace(':',' ').lower().replace('\r',' ').replace('!',' ').replace('\n',' '))
import pandas as pd $ from datetime import timedelta $ %matplotlib inline $ df_vow = pd.read_csv('../datasets/vow.csv')
for ix, file in enumerate(s3_files): $     os.environ["gs_key"] = "gs://resource-watch-public/" + s3_key_origs[ix] $     os.environ["asset_id"] = "users/resourcewatch/" + file[:-4] $     !earthengine upload image --asset_id=$asset_id $gs_key
for dtype in ['object', 'int']: $     print("dtype =", dtype) $     %timeit np.arange(1E6, dtype=dtype).sum() $     print()
final_topbikes['Distance'].count() / len(list(set(list(final_topbikes['id'])))) / 14
df_joined[['CA','UK','US']] = pd.get_dummies(df_joined['country'])
data[['Sales']].resample('D').mean().rolling(window=10, center=True).mean().plot()
orig_ct = len(dfd) $ dfd = dfd.query('hspf >= 10.0 and hspf <= 18') $ print(len(dfd) - orig_ct, 'eliminated')
cluster_names
ab_df2[((ab_df2['group'] == 'treatment') == (ab_df2['landing_page'] == 'new_page')) == False].shape[0]
evaluate_models(X, y)
km_res
dataframe.groupby('day_of_week').daily_worker_count.agg(['count','min','max','sum','mean'])
df_questionable_3[df_questionable_3['state_TX'] == 1]['link.domain_resolved'].value_counts()
len(df2.query('landing_page == "new_page"')) / len(df2)
r.json()
summaries = "".join(df.title) $ ngrams_summaries = cvec_1.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(10)
df.iloc[0,0]   # returns element of first row and first column as per zero indexing
df_clean['body_length'].hist(range = (0,100))
def removeHttp(myString): $     if 'http' in myString: $         k = myString.index('http') $         myString = myString[0:k] $     return myString
n = len(dataBPL.Total_Demand_KW) $ train = dataBPL.Total_Demand_KW[:int(.75*n)] $ test = dataBPL.Total_Demand_KW[int(.75*n):]
non_na_df.groupby('dayofweek').count()['sender_qq'].plot(kind='bar') $ plt.tight_layout() $ plt.title('Activity by Day of Week') $ plt.ylabel('Total Chat Count') $ plt.xlabel('Day of Week')
pd.read_sql('SELECT * FROM experiments', conn, index_col='experiment_id')
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
pd.Timestamp('2018-01-01')
print(np.array(0) / np.array(0)) $ print(np.array(0) // np.array(0)) $ print(np.array([np.nan]).astype(int).astype(float))
df.info()
y_valid.shape
aqmdata['location'].value_counts()
model_w = sm.formula.ols('y ~ C(w)',data=df).fit() $ anova_w_table = sm.stats.anova_lm(model_w, typ=1) $ anova_w_table.round(3)
df2[df2.duplicated(['user_id'], keep = False)]
f= pd.read_csv('./ab_data.csv') $ f.head()
list_of_df = [df, df2]
autos["price"].unique().shape $ autos["price"].head(100).value_counts() $ autos["price"].sort_values(ascending=True).head(100) $ autos=autos[autos["price"].between(100,100000)] $ autos["price"].describe()
daily_cases.unstack().T.fillna(0).cumsum().plot()
plt.scatter(litters['lsize'], litters['bodywt']) $ plt.xlabel('Litter Size') $ plt.ylabel('Body Weight (g)') $ plt.title('Body Weight of Mice compared to Litter Size') $ plt.show()
df.drop(["urlname"], axis = 1, inplace=True) $ top_rsvp.drop(["urlname"], axis = 1, inplace=True) $ top_rsvp.head(5)
oil_interpolation.index.names=[None] $ oil_interpolation.set_index('date',inplace=True) $ pd.DataFrame.head(oil_interpolation)
median = np.percentile(df['num_comments'], 50) $ per_75 = np.percentile(df['num_comments'], 75) $ print("The median of the num of comments: ", median) $ print("The 75th percentile of the num of comments: ", per_75)
fig = plt.figure(figsize=(12,8)) $ ax = fig.add_subplot(111) $ fig = qqplot(resid_713, line='q', ax=ax, fit=True)
Xs = pd.get_dummies(df.subreddit, drop_first = True)
val_prediction[0,7] + 100
fig, ax = plt.subplots() $ ffr.rolling(window=7).max().plot(ax=ax) $ ffr.rolling(window=7).min().plot(ax=ax) $ ax.legend(["max", "min"])
df = pd.read_sql_query("select path, type, ds from tracks where ds>='2018-01-01' and ds<'2018-01-02' limit 10", conn)
df2.query('landing_page == "new_page"').user_id.size / df2.user_id.size
from datetime import datetime, timedelta $ date = datetime.now() + timedelta(days=3) $ print(date) $ ntime = date2index(date,times,select='nearest') $ print('index = %s, date = %s' % (ntime, dates[ntime]))
user.where(user["friends_count"] > 200).head(8)
df2.head()
type(results)
print("Number of Groups in PRE-ATT&CK") $ print(len(all_pre['groups'])) $ df = all_pre['groups'] $ df = json_normalize(df) $ df.reindex(['matrix', 'group', 'group_aliases', 'group_id', 'group_description'], axis=1)[0:5]
optimizer = tf.train.AdagradOptimizer(0.01 ) $ train = optimizer.minimize(loss) $
pd.unique(dump["country"]) #Again, making sure I'm not crazy
df_twitter[df_twitter.p1 == "Saluki"]
month.to_csv('../data/month.csv')
df_movies.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/movies_v1.csv', sep=',', encoding='utf-8', header=True)
_,ax=plt.subplots(1,2,figsize=(12,7)) $ sns.countplot(calls_df["call_day"],ax=ax[0]) $ sns.countplot(calls_df["call_time"],ax=ax[1])
df.T.to_csv('../outputs/CSFV2_outlook_weekly_90th_per_summary_table_from_{:%Y%m%d}.csv'.format(dates_range[0]))
df2.query('user_id == 773192')
v_invoice_hub.shape
import pandas as pd $ dataset = pd.read_csv("judgements_metadata.tsv", sep="\t") $
top_comments_id=top_comments['id'] $ top_comments_id $ for i,x in top_comments_id.iteritems(): $     print ('https://www.facebook.com/'+x )
joined = pd.read_feather(f'{PATH}joined') $ joined_test = pd.read_feather(f'{PATH}joined_test')
df_new.iloc[1,]
(df2.query("converted >= 0")['landing_page'] == 'new_page').mean()
plt.plot(model_output.history['loss'],c='k',linestyle='--') $ plt.show;
red_inter_recr.to_pickle(folderPartial + 'red_inter_recr.pkl')
pnew-pold $
data_df["approx_size"] = human_size $ data_df["size_in_bytes"] = size
df_2007['bank_name'] = df_2007.bank_name.str.split(",").str[0] $
words = lines.flatMap(lambda line: line[0].split(" ")) $ hashtags = words.filter(lambda w: '#' in w).map(lambda x: (x, 1)) $ hashtag1 = hashtags.reduceByKey( lambda a, b: a + b ) $ df2 = hashtag1.map( lambda rec: Tweet(rec[0], rec[1])).toDF()
HTML(open('/home/ola/.ipython/profile_newprofile_34/static/custom/custom.css', 'r').read())
vectorizer.get_feature_names()
dt.datetime(yr, mo, dd, hr, mm, ss, ms)
tst_lat_lon_df.describe()
pd.set_option('display.max_colwidth', -1) $ data_2017_12_14_iberia["text_2"].head()
base_folder = os.path.abspath(os.getcwd()) $ os.chdir(".") $ db_folder = os.getcwd() + "/new_data" $ os.chdir(db_folder)
df_new_test = df_new[['UWI']]
gbm_v1 = H2OGradientBoostingEstimator( $     model_id="gbm_covType_v1", $     seed=2000000 $ ) $ gbm_v1.train(covtype_X, covtype_y, training_frame=train, validation_frame=valid)
oecd = pd.read_html(oecd_site, header=0)[1][['Country', 'Date']] $ oecd.head()
transactions.join(users.set_index('UserID'), on='UserID', how = 'outer')
df2.drop([1899], inplace = True) $ df2.duplicated(['user_id']).sum()
print(y_test.mean(), y_train.mean())
data['comments'] = data['comments'].astype(int)
active_psc_statements = pd.merge(active_psc_statements,active_companies[['CompanyNumber','first_and_postcode']],left_on='company_number',right_on='CompanyNumber') $ most_common_registered_addresses_for_no_psc_companies = active_psc_statements[active_psc_statements.statement.str.contains('no-individual-or-entity-with-signficant-control')].groupby(['first_and_postcode'])['company_number'].agg(lambda x: len(x.unique())) $ temp_s = most_common_registered_addresses[most_common_registered_addresses.index.isin(most_common_registered_addresses_for_no_psc_companies.index)]
df = pd.read_csv("Datasets/CDR.csv") $ df['CallDate'] = pd.to_datetime(df['CallDate']) $ df['CallTime'] = pd.to_datetime(df['CallTime']) $ df.dtypes
closep=np.array(data_dict['Close'],dtype='float64')
sp['day_ago_vol'] = sp.Volume.shift(periods = 1) $ sp['week_ago_vol'] = sp.Volume.shift(periods = 7)
df_times = pd.DataFrame(times, columns=['Time of daily highs'])
p_newpage=df2.query('landing_page=="new_page"').shape[0]/df2.shape[0] $ p_newpage
df_master.to_csv('twitter_archive_master.csv', index_label=False)
nth = 1 $ s = sss[0::nth] $
ts = type(merged_data.iloc[0, :][4])
set(train_data['abtest'])
plt.axvline(obs_diff) $ plt.axvline(np.mean(p_diffs)-obs_diff); $ plt.hist(p_diffs, alpha=0.3); $
k1.head()
print(df_van_asn.shape, $ df_tor_asn.shape, $ df_mtl_asn.shape)
loglikA = np.reshape(np.array([fitA.llnull, fitA_Cli1.llf, fitA_Cli2.llf, fitA.llf]), (4,1)) $ loglik = np.reshape(np.array([-152.763, -139.747, -149.521, -134.178]), (4,1)) $ lldf = pd.DataFrame(loglik, index=['Const','Const+li1','Const+li2','All'], columns=['Loglikelihood'])
print(model.summary())
new_page_converted=np.random.choice([1,0],size=n_new,p=[pnew,(1-pnew)]) $ new_page_converted.mean()
breed_predict_df.info()
print(training_target.value_counts()) $ print(test_target.value_counts())
train.loc[10000,'feature_list']
autos = autos[autos['registration_year'].between(1910,2016)] $ (autos['registration_year'] $ .value_counts(normalize=True) $ .sort_index())
print (mostRecentTweets.shape) $ print (tweetsOverall.shape) $ print (tidy.shape) $ tidy.to_csv('update/unfiltered-calc-agg-users.csv', index=False)
len(uname)
result_Sl[['last_sl_T']].sub(result_Sl['First_sl_T'], axis=0) $ result_Sl['Mean deaths'] = result_Sl['last_sl_T']/result_Sl['count_sl_T'] $ result_Sl
data = data.sort_values(['order','lastAttacked'],ascending=[0,1]) $ data.head()
df1.io_state.apply(lambda x: x.zfill(8))[70:800]
metadata = {} $ metadata
recommend["answer"] = pd.to_numeric(recommend["answer"]) # Converts the Series to `as_numeric`
autos.columns = final_new_columns_names
df4[["CA","UK","US"]] = pd.get_dummies(df4["country"]) $ df4.head()
autos["price_euro"].value_counts(normalize = True).sort_index(ascending = False).head(20)
tweet_json_df.info()
experiment_details = client.repository.store_experiment(meta_props=experiment_metadata) $ experiment_uid = client.repository.get_experiment_uid(experiment_details) $ print(experiment_uid)
grid_search.fit(X_train,y_train)
g.ngroups
house = elec['fridge'] #only one meter so any selection will do $ df = house.load().next() #load the first chunk of data into a dataframe $ df.info() #check that the data is what we want (optional) $
fm_confident_under = fm_confident_under.rename('bet_won_under_pred')
autos['brand'].value_counts(normalize=True) > .05
exploration_titanic.nacolcount()
BID_PLANS_df.set_index('baby_uid',inplace=True)
df2[df2['group'] == 'treatment'].mean()['converted']
n_old
doc_duration.tail()
fig = plt.figure() $ ay = fig.add_subplot(111, projection='3d') $ ay.scatter(lat_25_2.values, lng_25_2.values, minute_25_2.values, c=in_cp_25_2.values) $ plt.show()
df.loc['20180103', ['A','B']]
vectorizer.vocabulary_
air_reserve.head() $
snap_df = collection.item('AAPL', snapshot='snapshot_name') $ snap_df.to_pandas().tail()
lgr_grid.fit(X_train, y_train)
pd.read_csv("../data/microbiome/microbiome_missing.csv").head(20)
Raw_Forecast.set_index("Date_Monday").groupby([pd.TimeGrouper('M'),"Product_Motor","Part_Number"]).sum().fillna(0)[["Qty"]]
local.get_dataset(post_process_info["DatasetId"])
old_page_converted=np.random.choice([0,1],size=145274, p=[mean2, 1-mean2])
df['series_or_movie_name'].nunique()
item_hub.drop(item_hub_dropper, axis =1 , inplace = True)
test.info()
Grouping_Year_DRG_discharges_payments.groupby('drg3').head()
pd.merge(df1,df2)
df['Consumer complaint narrative'] = df['Consumer complaint narrative'].str.lower() $ df.head()[['Product', 'Consumer complaint narrative']]
from sklearn import linear_model $ lin_reg = linear_model.LinearRegression() $ lin_reg.fit(x_train,y_train)
tweets_df_clean.head()
knowledge_per_component.sort_values(ascending=False).head(10).plot.bar();
tweets_df_clean.drop_duplicates('id', inplace=True)
autos[['date_crawled','ad_created','last_seen']][0:5]
plt.hist(p_diffs) $ plt.xlabel('p_new - p_old') $ plt.ylabel('Frequency') $ plt.axvline(act_diff, color='red') #Red Line showing actual difference $ plt.show();
mean_mileage = pd.Series(brand_mean_mileage).sort_values(ascending=False) $ mean_prices = pd.Series(brand_mean_prices).sort_values(ascending=False)
print(site.text)
google_stock.isnull().any()
probs
import pandas.io.sql as psql
np.array(df[['Visitors','Bounce_Rate']]).tolist()
pd.concat([df1,df2,df3]) $
station_df =pd.read_sql('station',engine) $ station_unique_df = station_df['station'] $ station_unique_df.nunique() $
import numpy as np $ np.random.seed(12345) $ np.set_printoptions(precision=4, suppress=True)
transactions.merge(users,how="inner",on="UserID")
session.query(Actor.first_name, Actor.last_name).frame().head()
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)
df_new['country'].value_counts()
class_counts = sub_df.groupby('Rating').size() $ class_counts
tweet_df_clean = tweet_df_clean[['created_at', 'tweet_id', 'favorite_count', 'favorited','retweet_count']]
import pandas as pd $ import math $ %matplotlib inline $ %config InlineBackend.figure_format = 'svg'
import numpy as np $ import pandas as pd $ import seaborn as sns $ import matplotlib.pyplot as plt $ % matplotlib inline
plot = song_tracker["Bad and Boujee (feat. Lil Uzi Vert)"][1:].astype(float).plot(figsize=(20,10),title="Bad and Boujee by Migos") $ plot.set_xlabel("Date") $ plot.set_ylabel("Chart Position")
data["is_video"] = data["is_video"].astype(str)
df_new['intercept'] = 1 $ mod = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']]) $ results = mod.fit() $ results.summary()
df.source.value_counts().plot(kind='bar');
groceries.drop('apples', inplace=True) $ groceries
df_train.columns.values[np.where(df_train.columns.values != 'date_first_booking')]
autos['odometer_km'].hist()
df.head()
DummyDataframe2 = DummyDataframe2.apply(lambda x: calPercentage(x, "Token_Count", ["Positiv", "Negativ"]), axis=1) $ DummyDataframe2
investors_df.shape[0]
date = '2017-10-11' $ url = urllib.request.urlopen('https://www.epexspot.com/en/market-data/dayaheadauction/auction-table/%s/de'%date) $ url = url.read() $ soup = BeautifulSoup(url,"lxml")
sample = msft_cum_ret[1:3] $ sample
(test_intercept, test_slope) =  simple_linear_regression(graphlab.SArray(range(5)), $                                                          graphlab.SArray(1 + 1*graphlab.SArray(range(5)))) $ print('Intercept: {:.2f}'.format(test_intercept)) $ print('Slope: {:.2f}'.format(test_slope))
learn.load('clas_1')
dfg = df[df.source=='google'] $ dfg["clusterlabel"] = first_cluster.labels
def get_wordnet_pos(treebank_tag): $
p_old = len(df2.query('converted == 1'))/len(df2) $ p_old
g['created_year'].unique()
from gensim.corpora import Dictionary, MmCorpus $ from gensim.models.ldamulticore import LdaMulticore $ import cPickle as pickle
failures.head()
import tensorflow as tf $ x1 = tf.constant(5) $ x2 = tf.constant(6)
(~autos["registration_year"].between(1900,2016)).sum()/autos.shape[0]
hashed_test.count()
np.array(df1.index)
df_master.info()
rdd = sc.parallelize(range(10))
plt.rcParams['axes.unicode_minus'] = False $ dta_701.plot(figsize=(15,5)) $ plt.show()
df[pd.unique(['Country'] + df.columns.values.tolist()).tolist()].head()
unique_user_conversions = df.user_id[(df.converted == 1)].nunique() $ prop_conversions = unique_user_conversions / unique_users $ prop_conversions
nobel[nobel['Sex'] == 'Female'].groupby('Category').size().sort_values(ascending=False)
cityID = '161d2f18e3a0445a' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Raleigh.append(tweet) 
top_20_breed_stats = top_20_breed_archive.groupby('dog_breed').mean()
import statsmodels.api as sm $ convert_old = df2.query('group == "control" and converted == 1').count()[0] $ convert_new = df2.query('group == "treatment" and converted == 1').count()[0] $ convert_old, convert_new, n_old, n_new  
crimes.isnull().sum()
train_corpus = create_corpus(movieLines)
notus.loc[notus['country'].isin(canada), 'country'] = 'Canada' $ notus.loc[notus['country'].str.contains('Canada', case=False) == True, 'country'] = 'Canada' $ notus.loc[(notus['cityOrState'] == 'Canada') & (notus['country'] == 'Canada'), 'cityOrState'] = np.nan
varx=np.var(x) $ vary=np.var(y) $ print(varx) $ print(vary)
real_diff = real_new - real_old
data['month'] = ['Jan']*len(data) $ data
import statsmodels.formula.api as smf $ logit_model=smf.Logit(y,X) $ result=logit_model.fit()
tcat = pd.read_csv(filepath + 'cat_tweets.csv') $ tdog = pd.read_csv(filepath + 'dog_tweets.csv') $ tcat.head()
df_raw_tweet = pd.read_csv('./Datasets/Twitter_Training_Data2.csv', encoding='latin1') $ print (df_raw_tweet.head())
df3 = df2.fillna(30) $ df3.shape
data_df = data_df[cols] $ data_df
r.encoding
loan_test=921
ax = sns.barplot(x= "countCollaborators", y = "countCollaborators", data = data_final, palette='rainbow', $                  estimator = lambda countCollaborators: len(countCollaborators) / len(data_final) * 100) $ ax.set(ylabel="% of authors with a given number of collab.") $ ax.set(xlabel="Number of collab. by author") $ ax.set(xlim=(0, 20))
df_goog.plot()
df_columns[df_columns['Agency']=='NYPD']['Complaint Type'].value_counts().head() $
tokenizer_porter('runners like running and thus they run')
jobs.loc[jobs.FAIRSHARE == 64].groupby('ReqCPUS').JobID.count().sort_values(ascending= False)
df2.drop(1899,inplace=True)
len(set(b_list['id'].unique()) - set(b_cal_q1['listing_id'].unique()))
train['date'] = train['date_x'] $ train.drop(['date_y','date_x'],1, inplace= True)
from sklearn.feature_extraction.text import CountVectorizer $ vectorizer = CountVectorizer(token_pattern=r'\b\w+\b') # This token pattern keeps single-letter words $ train_matrix = vectorizer.fit_transform(train['text_clean']) $ val_matrix = vectorizer.transform(val['text_clean'])
from sklearn.feature_extraction.text import CountVectorizer $ count_vect = CountVectorizer(analyzer=text_process) $ bow_transformer = count_vect.fit(X) # Bag of Words Transformer
df = all_tables_df.tail()
authors = EQCC( git_index) $ previous_month_date = end_date - timedelta(days=31) $ authors.since(start=previous_month_date).until(end=end_date) $ authors.get_terms(field="author_name") $ print(buckets_to_df(authors.fetch_aggregation_results()['aggregations'][str(authors.parent_id-1)]['buckets']))
import os $ sc.addPyFile(os.path.expanduser('~/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar')) $ from graphframes import *
txHouse.info()
dicttagger_beverages = DictionaryTagger(['beverages.yml'])
donors_c.iloc[2097169, :] $
support.amount.sum()
df_master.loc[rate_change[rate_change['rating']==177.6].index]
r.xs('UA', level=1)[['share']].plot()
! head -1 ./data/raw-news_tweets-original/dataset1/news/2014-11-18.txt
    modules = Series(index=Index(Path(deathbeds.__file__).parent.glob('*.ipynb')));
plot_correlation_map(labeled) $ labeled.corr()
trace.analysis.cpus.plotCPU()
numberFilter = tweets["retweet_count"] > 10000 $ tweets[numberFilter].head()
total_sales['weighted_margin'] = total_sales['profit_total']/total_sales['bottles_total'] $ total_sales.head()
df.head(20)
archive_df = pd.read_csv('twitter-archive-enhanced.csv') $ image_df = pd.read_csv('image-predictions.tsv', sep='\t') $ status_df = tweet_df.copy() $ status_df.rename(columns = {"id": "tweet_id"}, inplace = True)
model_tree = DecisionTreeRegressor(random_state=42) $ model_linear = LinearRegression() $ model_rf = RandomForestRegressor(random_state=42) $ model_gb = GradientBoostingRegressor(random_state=42)
import keras $ from keras import regularizers, optimizers $ from keras.models import Sequential $ from keras.layers import Dense,Dropout $ from keras.callbacks import EarlyStopping, Callback
gender = {'male':1, 'female':2}
user_extract[user_extract['friends_count'] == 76707]
s4 = pd.Series([10, 0, 1, 1, 2, 3, 4, 5, 6, np.nan]) $ len(s4)
def generate(): $     for x in range(10): $         yield x $ Z = np.fromiter(generate(),dtype=float,count=-1) $ print(Z)
cars.info()
huffman = df5['HT'].where(df5['new_id'] == 'huffman').dropna() $ plt.hist(huffman, bins=50)
print(3 // 2)  # Integer division with truncation $ print(3 / 2)  # Float division
test_y = test_df['loan_status'].values $ test_y[0:5]
convert_old = df2.query('group == "control" and converted == 1').user_id.nunique() $ convert_new = df2.query('group == "treatment" and converted == 1').user_id.nunique()
set(orgs.primary_role)
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin' $ S.executable = excutable +'/summa.exe'
print(csgo_profiles.info()) $ print('Null object :',csgo_profiles.isnull().any().sum()) $ print('Shape'+str(csgo_profiles.shape))
df_new.head(5)
pumashplc['linkNYCppcBB'] = pumashplc['date_link_']/pumashplc['pcBB']*100
knn_reg = KNeighborsRegressor(n_neighbors = 101) $ knn_reg.fit(x_train,y_train)
df2 = pd.read_csv('ab_data_mod.csv')
df_signup = train.loc[:, ['gender', 'age', 'language', 'signup_method', 'signup_flow', 'signup_app', 'country_destination']] $ df_signup.tail()
ys_df = valid_scores[['point_diff']] $ ys_df.head()
for dim in d.dimensions: $     print('%s:\t%s' % (dim, d.dimensions[dim]))
sub_set = c_df[['DEVICE_MODEL','CUSTOMER_ID_CAT']] $ sub_set = sub_set.drop_duplicates(keep='first') $ sub_set.groupby('DEVICE_MODEL').size().reset_index(name='counts')
iris.head().iloc[:,0].values
so_head[s]
df_usage = pd.read_csv(path + "train_usage_data.csv")
df2.plot.bar()
data['clean_text'] = data.text.apply(lambda x: preprocessing(x))
df_columns['Agency'].value_counts() $
y_train = pd.DataFrame(y_train, columns=['country'], index=X_train.index)
frac_conv = df[df['converted'] == 1].shape[0] / rows $ frac_conv
url_domains = grouped['domain'].agg({'domain': lambda x: np.unique(x)}) $ unique_urls = pd.merge(unique_urls, url_domains) $ unique_urls.head()
src_map = dict(zip(mentions_df["src"],mentions_df["src_str"])) $ trg_map = dict(zip(mentions_df["trg"],mentions_df["trg_str"])) $ src_map.update(trg_map) $ user_names = src_map
type(giss_temp)
diffs_evening =np.array(diffs_evening)
print(hn.created_at.min()) $ print(hn.created_at.max())
html = browser.html $ mars_facts = bs(html, 'html.parser')
run txt2pdf.py -o"2018-06-18  2015 470 disc_times_pay.pdf"  "2018-06-18  2015 470 disc_times_pay.txt"
sql("show tables").show()
df_dummies = pd.get_dummies(df_onc_no_metac['METAC_SITE_NM1'], prefix = 'METAC_SITE') $ df_dummies.head()
poly17= PolynomialFeatures(degree=17) $ X_17 = poly17.fit_transform(X) $ linear = linear_model.LinearRegression() $ linear.fit(X_17, y2) $ (linear.coef_, linear.intercept_)
df.iloc[1]
log_mod = sm.Logit(df_new['converted'],df_new[['US','UK','intercept']]) $ result = log_mod.fit() $ result.summary()
pubs = db.get_publications() $ feeds = db.get_feeds()
df = pd.merge(df,rsvp_df,how="left",on="urlname") $ df.head()
df = pd.read_csv('Sales-2017-12.csv')
sdof_resp()  # arguments aren't necessary to use the defaults.
d = {'one': pd.Series([100, 200, 300], ['apple', 'orange', 'banana'], 'float32'), $       'two': pd.Series([111, 222, 333], ['apple', 'orange', 'clock'], 'float32')}
test.iloc[40:100]
in_cp_25_2 = suspects_with_25_2['in_cp']
df.Size.describe()
from sklearn.preprocessing import StandardScaler $ ss = StandardScaler() $ Xs = ss.fit_transform(X) $ type(Xs) $ Xs
subs_and_comments['thanks'].fillna(False, inplace=True)
table = Table.read("../datasets/catalogs/fermi/gll_psc_v16.fit.gz") $
df2.dtypes
machines = pd.read_csv('machines.csv', encoding='utf-8') $ machines.head()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY)
print len(ids&set(ids_posts)),"ids in posts and analyzable list" $ print  len(ids - set(ids_posts)),"ids not in posts:",ids - set(ids_posts) $ print len(set(ids_posts)-ids),"ids not analyzable ids (not worrisome - lots of testing or unusable students)"
result1 = -df1 * df2 / (df3 + df4) - df5 $ result2 = pd.eval('-df1 * df2 / (df3 + df4) - df5') $ np.allclose(result1, result2)
commits_per_day_cumulative = commits_per_day.cumsum() $ commits_per_day_cumulative.head()
atl_data = pd.read_csv('../data/external/atlanta_demographics.csv', index_col=0, header=None).transpose() $ atl_data.head()
dropoff_demand.head()
pd.read_csv("ign.csv",skiprows=[0]).head()
1/np.exp(-0.0175), 1/np.exp(-0.0469), np.exp(-0.0057), np.exp(0.0314)
df_complete_a[df_complete_a['dog_stage']=='None'].shape[0]
results_df.describe()
make_pickle(catboost_pred_pickle_file_name, y_pred)
z_score , p_value = sm.stats.proportions_ztest([convert_new,convert_old], [n_new,n_old],alternative='larger') $ print(z_score,p_value)
tweets3=pd.read_json(r'C:\Users\shampy\Desktop\project\RedCarpetUp\socialmediadata-tweets-of-congress-november\2017-11-03.json') $ tweets3.shape
us.loc[us['country'].isna() == True, 'country'] = us.loc[us['country'].isna() == True, 'cityOrState'] $ us['country'].value_counts(dropna=False)
plt.figure(figsize=(10,5)) $ plt.plot(df.index, df['distance']) $ plt.xlabel('time') $ plt.ylabel('distance') $ plt.show()
total_features.columns
repl_dir = {'petite sirah': 'syrah', $             'shiraz': 'syrah', $             'champagne' :'sparkling', $            }
tweet_archive_enhanced_clean.loc[1202,'text']
df = pd.read_sql_query('SELECT DISTINCT City FROM data', disk_engine) $ df.head()
df.shape    
assert 'daca'     in top_20.index $ assert 'nfl'     in top_20.index $ assert 'anthem' in top_20.index $ assert 'fbi'    in top_20.index $ assert 'russia'    in top_20.index $
df1a = df1.set_index('employee') $ df2a = df2.set_index('employee') $ display('df1a', 'df2a')
df["DateTime"] = pd.to_datetime(df["DATE"]+" "+df["TIME"],format="%m/%d/%Y %H:%M:%S")
%time results = odm2rest_request('results', res_params)
df_archive_clean.text[334]
from sklearn.neighbors import KNeighborsClassifier $ knn = KNeighborsClassifier(n_neighbors = 101) $ knn.fit(x_train,y_train > 0)
df = pd.read_sql('SELECT * from booked_room', con=conn_b) $ df.head(10) # show 10 rows only
fit.summary()
consumer_key = 'XWPLe64D8Jn8xjZrSLIGbkMYS' $ consumer_secret = 'GddzeZ3JtAWFesEIW0oIOYszXLM3DZvmJD2BbeUCF50PrkHfR6' $ access_token = '62859544-M3yQ1sbVPkrKFNRQs7JvO6merixEyGlIJqf0GMSEt' $ access_secret = 'zSaRFYstkZwU3y1WO4jPLabtayOMQSDMw7juvRABiZJt1'
h4 = qb.History(spy.Symbol, 360, Resolution.Daily) $
df.head()
countries_df = pd.read_csv('./countries.csv') $ countries_df.head()
ins2016
loans_df = loans[imp_cols] $ del (loans)
%matplotlib notebook $ import datetime
df2['timestamp'] = pd.date_range('8/8/2018', periods=len(df2['MATCHKEY']), freq='D')
train_tokens = trainset['tokens'].tolist() $ train_corpus = createCorpus(train_tokens)
import pandas as pd
test_orders_prodfill_final2=test_orders_prodfill_final[['order_id','product_id']] $ test_orders_prodfill_final2['products']=test_orders_prodfill_final2['product_id'] $ test_orders_prodfill_final2.head()
sp.head(10)
cityID = '00ab941b685334e3' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Nashville.append(tweet) 
print(pd.DataFrame(data = my_data[1:,1:], $                   index = my_data[1:,0], $                   columns = my_data [0,1:]))
conn_a.commit()
final_annotations_df.tail(5)
liberiaDf.index = ['-'.join([date.split("/")[2], date.split("/")[0].zfill(2), date.split("/")[1]]) $                      for date in liberiaDfOld.index] $ liberiaDf.index.name = 'Date' $ liberiaDf.head()
df_questionable_3[df_questionable_3['state_DC'] == 1]['link.domain_resolved'].value_counts(25)
!cat ../data/microbiome/microbiome.csv
BDAY_PAIR_df.head()
df.to_csv("final_prediction.txt", header=False, index=False, sep="\t", encoding="utf-8-sig")
cs = cosine_similarity(my_q, vec)
raw_df.shape
ea = pd.read_csv('twitter-archive-enhanced.csv') # Read in enhanced twitter archive
num_of_tags.loc[users['OneDayUser'] == False].agg([np.mean, np.median, np.std, max])
merged.sort_values("amount", ascending=False)
grouped_dpt_city = department_df.groupby(["Department", "City"])
for i in range(0,len(logrecnos)): $     if(logrecnos[i]=='0000617'): $         print i
users_pd = users.toPandas() $ pd.value_counts(users_pd['active'].values, sort=True).plot(kind="bar")
injuries_hour['wet']=injuries_hour['Heavy Rain']+injuries_hour['Light Freezing Fog']+injuries_hour['Light Freezing Rain']+injuries_hour['Light Rain']+injuries_hour['Heavy Rain']+injuries_hour['Light Snow']+injuries_hour['Heavy Snow']+injuries_hour['Rain'] $ injuries_hour['low_vis']=injuries_hour['Fog']+injuries_hour['Haze']+injuries_hour['Light Freezing Fog']+injuries_hour['Mist']
greater_p_diffs = (p_diffs > ab_data_diff).mean() $ greater_p_diffs
cfs_df = pd.read_csv(path+"crime_beat.csv")
pd.Period('2011-01')
squares.keys()
right = pd.DataFrame({'key' : ['bar', 'foo'], 'rval': [4,5]}) $ right
education_data.columns=edu_columns
temp = data.merge(right = data[['user_id','device_id']].groupby('device_id').count().reset_index().rename(columns={'user_id':'usage_count'}),how='left',on='device_id') $ data['usage_count'] = temp['usage_count']
link = soup.find('li').find('a')['href'] $ link
for df in (joined, joined_test): $     for c in df.columns: $         if c.endswith('_y'): $             if c in df.columns: df.drop(c, inplace=True, axis=1)
rodelar.groups()
y_pred, new_hidden = model(x_test, hidden)
pd.concat([d1, d2], axis=1)
cust_data.columns
data.head(5)
seen_click_read = pd.merge(new_seen_and_click, new_read, how='left', left_on=['article_id','project_id','user_id','user_type'], right_on = ['article_id','project_id','user_id','user_type']) $ seen_click_read[0:10]
converted.head()
ts = soup.find_all('div', attrs={'class':'art_title'}) $ for i in ts: $     print i.contents[0] $
example1_df = spark.read.json("./world_bank.json.gz")
store.replace({"PromoInterval": {np.nan: 0}}, inplace=True)
w2 = Window.partitionBy('user_id') $ df_selected = df_city_reviews\ $ .withColumn('numOfReviews', F.count("seq").over(w2))\
df_test.head()
null_values = np.random.normal(0, p_diffs.std(), p_diffs.size) $ plt.hist( null_values) $ plt.axvline(obs_diff, color = 'red');
precip_df.describe()
print pd.concat([s1, s4], axis=1, join='inner')
data.dropna(axis = 0).shape
df_sp_500.show()
sel_df.to_csv('crime_data_clean/beats_alternate_10_17.csv')
mom = pd.read_csv('FF_Momentum_Factor_Monthly.CSV')
reddit['Subreddits'].value_counts().sum() #how many total subreddits do we have? 
learner.load('lm_last_ft')
y_test = test.loan_status
df.head()
for stats_type in ('misc',): #('advanced', 'fourfactors','scoring'): $     for season in list(reversed(['2016-17','2015-16','2014-15'])): #,'2013-14','2012-13' $         curr_stats = download_all_game_stats(statstype=stats_type, season=season) $         curr_stats.to_csv('{}_{}.csv'.format(stats_type.title(), season), index=False)
result_merged.summary2()
import pandas as pd $ import numpy as np $ import seaborn as sns $ import matplotlib.pyplot as plt
new_count = df2.query('landing_page == "new_page"').count() $ old_count = df2.query('landing_page == "old_page"').count() $ print(new_count/(old_count + new_count))
!spark-shell --master local[2]
merged.loc[merged['state'].isnull(), 'state/region'].unique()
np.exp(logit_mod_joined_result.params)
comments.describe()
userItems = model3.transform(userItems) $ for row in userItems.take(5): $     print row.stockCode, row.custId, row.prediction
print(kmeans.labels_)  $
df_loan2 = df_loan1.merge(loans[['fk_loan','payback_state']], $                                           on='fk_loan')
print (classification_report(y_test, yhat)) $
metrics.accuracy_score(actual_value_second_measure, predicted_outcome_first_measure) $
%matplotlib inline $ import seaborn as sns $ from matplotlib import pyplot
print(dfh['Centrally Ducted or Ductless'].unique()) $ dfh['Centrally Ducted or Ductless'].value_counts()
class newHandler(handler.SVPOLHandler): $     _parser = newParser
!find {VAL} -name '*.txt' | xargs cat | wc -w
tw_clean.head()
classifier_comparison_df.to_pickle('classifier_comparison_table.pkl')
mol = molecules.loc[0, 'molecule'] $ m2 = Chem.AddHs(mol) $ AllChem.EmbedMolecule(m2) $ Chem.MolToMolFile(m2, 'mol.mol') $ conf = m2.GetConformer()
cities = cities * 0.5 $ print(cities)
autos.info()
normalized_df = normalize(df)
us[us['country'] != 'USA']
result['timestampCorrected'].describe()
foursquare_data_dict = dict(data)
removing_features_train = X_train[:, :5] $ removing_features_validate = X_validate[:, :5] $ X_train = np.delete(X_train, [0, 1, 2, 3, 4], axis=1) $ X_validate = np.delete(X_validate, [0, 1, 2, 3, 4], axis=1)
pgh_311_data = pd.read_csv("https://data.wprdc.org/datastore/dump/40776043-ad00-40f5-9dc8-1fde865ff571") $ pgh_311_data.head()
ds = df.loc[df['item_id'] == 11] $ ai = df.loc[df['item_id'] == 23]
for x in range(0,9): $     avg_values.append(25)
1/np.exp(-0.0150)
agency_borough.size().unstack().plot(kind='bar')
df_max
p_new=df2[df2['landing_page']=='new_page']['converted'].mean() $ p_new
pd.Period('2017-01')
exiftool -csv -createdate -modifydate cisuabe5/cisuabe5_cycle1.MP4 cisuabe5/cisuabe5_cycle2.MP4 cisuabe5/cisuabe5_cycle3.MP4 cisuabe5/cisuabe5_cycle4.MP4 > cisuabe5.csv
import pandas as pd
df_uro_dd_dummies_no_sparse.shape $ df_uro_dd_dummies_no_sparse.head()
bmp_series = pd.Series(branding_mileage)
order_pay_num = pd_data.loc[lambda pd_data: pd_data.pay > 0,:].groupby(['appCode','orderType','year','month'])['pay'].count()
comps.drop_duplicates(['entity_uuid','competitor_uuid'],inplace = True)
extract_nondeduped_cmp.loc[(extract_nondeduped_cmp.APP_SOURCE=='SFL') $                           &(extract_nondeduped_cmp.APPLICATION_DATE_short>=datetime.date(2018,6,1)) $                           &(extract_nondeduped_cmp.app_branch_state=='CA')].groupby('APP_PROMO_CD').size()
df1 = df[df.isnull().any(axis=1)] # axis=1 specifies rows instead of columns $ df1.shape
model.fit(np.asarray(train_x), np.asarray(new_train_y), batch_size = 1000, nb_epoch = 10, validation_data = (np.asarray(test_x), np.asarray(new_test_y)))
df.sort_index(inplace=True) # sort the dataframe by its indices $ df.loc[idx[['lossyrob', 'non'], datetime(2015, 3, 31):datetime(2015,4,28)], 'text']
sql = "SELECT * FROM paudm.photoz_bcnz as bcnz limit 5 " $ df1 = pd.read_sql(sql,engine)
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs); $ plt.axvline(x=p_new-p_old,c='r',label="real diff") $ plt.axvline(x=(p_diffs.mean()),c='y',label="simulated diff")
print "N_old:", n_old
question_1_dataframe = data_2017_subset[data_2017_subset['complaint_type'].isin(top_10_complaint_types.index)] $ len(question_1_dataframe)
sensor.unit
cust, c = generate_labels('jdYSK7I9TjjADXtMsv8vhN362hfIfC0Qyrn4fSZNHKY=', trans, $                           label_type = 'SMS', churn_period = 30, return_cust = True) $ c[c['days_to_next_churn'] < 15] $ cust.iloc[:, 6:]
rfc = RandomForestClassifier(class_weight='balanced_subsample') $ s = cross_val_score(rfc,subrcvec1, y_test, cv=cv, n_jobs=-1) $
merged.isnull().sum()
database='FSE' $ dataset='AFX_X' $ start_date='2017-05-01' $ end_date = '2017-05-01' $ url='https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key='+API_KEY+'&start_date='+start_date+'&end_date='+end_date $
h5py.File?
df.sort_values(['Country','Year','WHO Region', 'Publication Status']).sort_index().head(3)
tweets.head()
xnames = xd.columns $ xarr = np.array(xd)
s1 = "SELECT * FROM paudm.production as prod where prod.pipeline='memba' order by - prod.id  limit 1" $ d= pd.read_sql(s1,engine)
df_selection = df_selection.dropna(how='any') 
help(plt.scatter)
total_num_stations = session.query(func.count(Station.station)).first() $ print(f"Total number of stations: {str(total_num_stations[0])}")
data.columns
calc_temp()
tz_dateutil = dateutil.tz.gettz('Europe/London')
Del_list[0:5]
wd=1e-7 $ bptt= 70 $ bs= 52 $ opt_fn = partial(optim.Adam, betas=(0.8, 0.99))
z_score
print ('Loading and merging csv') $ df_log = pd.read_table(path_log, header=0, parse_dates=True, na_values='n/a') $ df_events = pd.read_table(path_events, header=0, parse_dates=True, na_values='n/a') $ df_users = pd.read_table(path_users, header=0, parse_dates=True, na_values='n/a') $
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import seaborn as sns $ import datetime
p_diffs = [] $ for _ in range(10000): $     old_page_converted = np.random.binomial(1, control_cnv, control_num) $     new_page_converted = np.random.binomial(1, treatment_cnv, treatment_num) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean()) $
train.groupby('popular').num_comments.describe().unstack()
df.groupby("cancelled")["ignore_availability_requirements"].mean()
df['complaint_'] = df['complaint'].apply(lambda x: (x[0]))
train_data_features = vectorizer.fit_transform(X)
print("Number of unique visitors in train set : ",train_data.fullVisitorId.nunique(), " out of rows : ",train_data.shape[0]) $ print("Number of unique visitors in test set : ",test_data.fullVisitorId.nunique(), " out of rows : ",test_data.shape[0]) $ print("Number of common visitors in train and test set : ",len(set(train_data.fullVisitorId.unique()).intersection(set(test_data.fullVisitorId.unique())) ))
p6_result = p3_table.sort_values('Profit', ascending=True) $ p6_result.head()
data.describe()
merged_df_cut.shape
iris_dataset['feature_names']
df.info()
slicer.apply(np.mean, axis=1)
lasso.fit(X_train,Y_train)
f = np.sin(np.linspace(0,np.pi,100)) $ print("f: ", f)
yc_trimmed = pd.read_csv('http://cosmo.nyu.edu/~fb55/data/yc200902_tripdata_trimmed.csv') $ yc_trimmed.head()
train_df['parcelid'].nunique(dropna=False)
calls_df["dial_type"].value_counts()
collection = store.collection('NASDAQ.EOD') $ collection
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ pd.options.mode.chained_assignment = None  # default='warn'
print (soup.prettify())
tm_2020 = pd.read_csv('input/data/trans_2020_m.csv', encoding='utf8', index_col=0)
test.info()
print ("Row Count before remove duplicate user_id : {} ".format(df2.shape[0])) $ df2.drop_duplicates('user_id', keep='first' ,inplace=True) $ print ("Row Count after remove duplicate user_id: {} ".format(df2.shape[0]))
df_countries.info()
print 'total number of observations:', len(electricity)
f.query("group =='treatment' and landing_page == 'old_page' or group == 'control' and landing_page=='new_page'").shape[0]
data = data.dropna(axis = 0)
tweets = pd.DataFrame(ndarray)
tweet_archive_clean = tweet_archive.copy() $ info_clean = info.copy() $ images_clean = images.copy()
suspects_with_1T = suspects_data[suspects_data['imsi'].isin(imsi_with_1T)]
(CLAS_PATH/'tmp').mkdir(exist_ok=True) $ np.save(CLAS_PATH/'tmp'/'tok_trn.npy', tok_trn) $ np.save(CLAS_PATH/'tmp'/'tok_val.npy', tok_val) $ np.save(CLAS_PATH/'tmp'/'trn_labels.npy', trn_labels) $ np.save(CLAS_PATH/'tmp'/'val_labels.npy', val_labels)
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_axial_brace_crown[0][0])) $ perr = np.sqrt(np.diag(pcov_axial_brace_crown[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
department_df.groupby(["Department", "City"], as_index = False).sum()
old_n = df2[df2['group'] == 'control'].shape[0] $ old_n
import numpy as np
names = open("../data/other_data/first_names.txt").read().split("\n") $ names_pattern = re.compile(r'\b(?:{})\b'.format('|'.join(names)))
import sqlite3 $
df2.rename(columns={'activity_date_time_c': 'median_activities_per_gallery'}, inplace=True) $ df2.groupby(df2.index).median()
path = '../DATA/yard_snapshots_saint_nazaire.csv' $ snapshots2.to_csv(path, index=False) $ snapshots2.shape
import tweepy $ import numpy as np $ import matplotlib.pyplot as plt $ import pandas as pd
tt_final[(tt_final.p1_dog == True)]['p1'].value_counts().head(10)
requests.get(wikipedia_marvel_comics)
bacteria2.isnull()
pd.Series(['a', 'a', 'b', 'c'] * 4).describe()
df2['user_id'].nunique()
x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)
df1 = newdf.drop(['Area Id','Variable Id','Symbol'],axis=1) $ df1
rng.is_month_end
groups_df.dropna(subset=[ 'MSA_CODE' ], inplace=True) $ metro_groups_df = groups_df[ groups_df['MSA_NAME'].str.contains('Metro') ] $ groups_by_metro = metro_groups_df.groupby([ 'MSA_CODE', 'MSA_NAME' ], as_index=False)
with open("./minisom.save", "w") as f: $     f.write("SOM {} {} {} {} {} {}\n".format(20, 30, 8292, 100, 0.5, 0.3)) $     for i in som.get_weights(): $         for j in i: $             f.write(" ".join(map(str, j)) + "\n")
hdf.sort_index(inplace=True, $               level=[0, 1]) $ hdf.index.lexsort_depth # both levels are sorted against
rf = RandomForestClassifier(n_estimators=50, max_depth=50, n_jobs=4) $ rf.fit(X = clim_train, y = size_train)
df.shape
top_10_authors = git_log.groupby("author").count().sort_values(by="timestamp", ascending=False).head(10) $ top_10_authors
train[train.age>1000].age.value_counts()
data[['a', 'e']]
locationIDsToExclude = [213385402,594107096,230734647 ] $ londonLocationsFiltered = [x for x in londonLocations if x['id'] not in locationIDsToExclude]
random_integers.max(axis=1)
users = pd.read_csv('data/users.csv', index_col = 0, parse_dates = [1,2]) $ users.head()
p_new = df2.query('converted == "1"').count()[0]/df2.shape[0] $ new_page_convert = np.random.choice([0,1],n_new, p=(1-p_new, p_new))
train_small_data = train[train_msk] $ val_small_data = train[val_msk]
%%time $ dfHashtags = pd.DataFrame(sentences)
data1Scaled['Label'] = np.array(data1['adj close'].shift(-(daysToForecast)).values)
df1_clean[df1_clean.name == 'None']
print(training_active_listing_dummy[0:5],training_active_listing_dummy.mean())
from sqlalchemy import create_engine $ import sqlalchemy.types as types
df['bikeid']= df['bikeid'].astype(str) $ df['bikeid'].describe()
scaler = MinMaxScaler(feature_range=(0,1)) $ cols_to_scale = [col for col in df_total.columns if 'weekday' not in col and 'month' not in col] $ scaled_cols = scaler.fit_transform(df_total[cols_to_scale]) $ df_total[cols_to_scale] = scaled_cols
with open('op_attr.pkl', 'rb') as picle: $     op_attr = pickle.load(picle) $ sub_df = pd.concat([sub_df, op_attr], axis=1)
print('There are {} missing values.'.format(df.isnull().any().sum()))
gs.score(X_test, y_test)
df_wm.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
from ch_api import util
monte.str.split()
p_diff = p_new-p_old $ print("Difference In conversion:", p_diff)
b = a
sample.asfreq('H',method='bfill')
(df2.query('group == "control"')['converted'].mean() - df2.query('group == "treatment"')['converted'].mean())
for c, v in zip(logreg_sentiment.fit(X, y).coef_[0].round(3), ['neg','pos','neu','compound','len']): $     print v, c
goog.index = goog.index.to_datetime()
cur = conn.cursor() $ cur.execute('ALTER TABLE actor ADD COLUMN middle_name varchar(45);') $ df = pd.read_sql('SELECT * FROM actor', con=conn) $ df.head()
%load "solutions/sol_2_20.py"
ts_utc = ts.tz_localize('UTC') $ ts_utc
df.groupby("cancelled")["local_rental"].mean()
news_df = guardian_data.copy()
obs_diff = df2.query('group=="treatment"')['converted'].mean() - df2.query('group=="control"')['converted'].mean() $ obs_diff
tlen_k2 = pd.Series(data=kelsey['len'].values, index=kelsey['Date']) $ tfav_k2 = pd.Series(data=kelsey['Likes'].values, index=kelsey['Date']) $ tret_k2 = pd.Series(data=kelsey['RTs'].values, index=kelsey['Date'])
weather_norm = weather_features.apply(lambda c: 0.5 * (c - c.mean()) / c.std())
df_new['country'].unique() # what countries are in the dataset now? $ df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new.drop(['US'], axis =1)
soup = BeautifulSoup(html, 'lxml')
pd.date_range('2017-12-30', '2017-12-31')
players_df.head()
plt.hist(y_res) $ plt.hist(y) $ plt.show()
! dir $ ! type sumxy.py $ ! python sumxy.py 1 2
cameraid_model_dict = { $     "128": model_128_id, $     "129": model_129_id, $     "130": model_130_id $ }
new_page_converted = np.random.binomial(N_new, P_new) $ new_page_converted
stories['created_dow'] = stories.created_at.map(lambda x: x.weekday())
trump_fb = trump.source == "Facebook"
v_item_hub.columns.tolist()
(p_diffs > p_diff_real).mean() $
p_mean = np.mean([p_new, p_old]) $
topics_by_time(period=topics_data.timestamp.dt.date, period_str='Date')
rain_df.describe()
max_vocab = 60000 $ min_freq = 2
house_data.head(20)
df_pol_d=df_pol_d.drop('text',axis=1)
processed_tweets.processed_tweet_features[150]
autos["price"].describe()
is_single = dfd.zones.str.lower().str.startswith('single') $ dfd.loc[is_single, 'zones'] = 'Single' $ dfd.loc[~is_single, 'zones'] = 'Multi' $ dfd.zones.value_counts()
condos.MAR_WARD.value_counts()
knn5=KNeighborsClassifier(5)
df2['country'].value_counts()
movies_df = pd.read_csv('movies.csv') $ ratings_df = pd.read_csv('ratings.csv') $ movies_df.head()
y = df_train['is_churn'] $ X = df_train.loc[:, df_train.columns != 'is_churn']
sorted.select('count').toPandas().hist(bins=15)
parking_planes_df.to_excel('parking_planes_df1.xlsx', index=False)
converted_first_week
model = denseModel2() $ model.summary()
sqlContext.sql(query).toPandas()
tm_2020 /= 1000 $ tm_2020_norm = tm_2020 ** (10/11) $ tm_2020_norm = tm_2020_norm.round(1) $ tm_2020_alpha = tm_2020 ** (1/3) $ tm_2020_alpha = tm_2020_alpha / tm_2020_alpha.max().max()
a=net_loans_exclude_US_outstanding[net_loans_exclude_US_outstanding.Ratenart=='J'] $ a.to_clipboard()
tsne_input.head()
m1.test_assumptions('1%')
All_tweet_data_v2.name[All_tweet_data_v2.name.str.contains('^[(0-9)]')]
test_sentence = test_sentence.replace(re.compile(r"@[^\s]+[\s]?")) $ test_sentence[0]
p_new = round(float(df2.query('converted == 1')['user_id'].nunique())/float(df2['user_id'].nunique()),4) $ p_new
df_int = df.copy() $ df_int[1:] = (df[1:] - df[:-1].values) / df[:-1].values $ df_int.iloc[0] = 0 $ df_int.head()
print("\nThere are {} toe marks with spaces surrounding hypens.".format(df.loc[df.toes.str.match(pattern1)==True].shape[0])) $ print("\nThere are {} toe marks with spaces surrounding numbers.".format(df.loc[df.toes.str.match(pattern2)==True].shape[0])) $ print("\nThere are {} toe marks with an ' preceeding the entry.".format(df.loc[df.toes.str.match(pattern3)==True].shape[0]))
jn = urllib.request.urlopen("https://query.yahooapis.com/v1/public/yql?q=select%20*%20from%20yahoo.finance.xchange%20where%20pair%20in%20(%22GBPHKD%22%2C%22HKDGBP%22)&format=json&diagnostics=true&env=store%3A%2F%2Fdatatables.org%2Falltableswithkeys&callback=").read() $ df = pd.read_json(jn)
np.sqrt(np.mean((df_y_pred.values - df_y_actual.values)**2))
np.random.seed(1)
reg.fit(X_.astype(float), y_.astype(float))
new_page_converted = np.random.binomial(n_new, p_new, 10000)/n_new $ old_page_converted = np.random.binomial(n_old, p_old, 10000)/n_old $ p_diffs = new_page_converted - old_page_converted $
nnew = len(df2.query("group == 'treatment'")) $ print(nnew)
bnb.first_affiliate_tracked.value_counts()
import pandas as pd $ from pandas import Series, DataFrame $ from datetime import timedelta
print 'Total number of integrated records: ' + str(len(integratedData)) $ print 'Total number of records existing in both XML and CSV: ' + str(len(integratedData[integratedData['True'] == 'both'])) $ print 'Total number of unique records from XML: ' + str(len(integratedData[integratedData['True'] == 'left_only'])) $ print 'Total number of unique records from CSV: ' + str(len(integratedData[integratedData['True'] == 'right_only']))
lm = sm.Logit(df["converted"],df[["intercept","control"]]) $ results = lm.fit() $ results.summary() $
results = logit_mod.fit() $ results.summary()
df_byzone.drop('time_stamp', axis = 1, inplace = True)
vectorizer2 = TfidfVectorizer(use_idf=True, ngram_range=(3,4))  $ transformed2 = vectorizer2.fit_transform(cleaned_texts.values) $ features2 = vectorizer2.get_feature_names()
tweet_model.wv.similar_by_word('Sheeran')
testing.replace(u"\ufffd", "?")
ts.index
overall_conv = df2.query('converted == 1').user_id.nunique() / df2.user_id.nunique() $ overall_conv
M7_RMSE
c
tweet_data.sort_values('rating', ascending= False).head() $
volume = [] $ for ele in r.json()['dataset']['data']: $     volume.append(ele[6]) $ print('The average daily trading volume during this year: {} '.format(sum(volume)/float(len(volume))))
index = similarities.MatrixSimilarity(lsi[corpus])
vals[5] = 0 $ vals
k1 = data[['cust_id','month','total_spend']].groupby(['cust_id','month']).agg('count').reset_index() $ k1.columns = ['cust_id','month','cust_id_month_count'] $ train = train.merge(k1,on=['cust_id','month'],how='left') $ test = test.merge(k1,on=['cust_id','month'],how='left')
no_specialty = merged1[appointments['Specialty'].isnull()] 
sel = [Measurement.date, Measurement.tobs] $ highest_tobs_station_yearly = session.query(*sel).\ $                 filter(Measurement.date > '2016-08-23').\ $                 filter(Measurement.station == station_max).all() $ highest_tobs_station_yearly $
active_station_df = pd.DataFrame(active_station[:], columns=['station','tobs',]) $ active_station_df.sort_values("tobs", ascending=[False], inplace=True) $ active_station_df.set_index('station', inplace=True) $ active_station_df.head()
words_sp, corpus_tweets_streamed_profile = count_freq(tweets_l_stream_profiles) $ words_sp_freq = FreqDist(words_sp) $ print('The 100 most frequent terms, including special terms: ', words_sp_freq.most_common(100))
clean_appt_df[['Age','No-show']].boxplot(by='No-show')
client = MongoClient('localhost', 27017)
smart_beta_tracking_error = helper.tracking_error(index_weighted_cumulative_returns, etf_weighted_cumulative_returns) $ helper.plot_tracking_error(smart_beta_tracking_error, 'Smart Beta Tracking Error')
providers_schedules['ProviderId'].unique(), len(providers_schedules['ProviderId'].unique())
for dataset in combine: $     dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int) $ train_df.head()
ideas['Time'].head(1)  # Inspecting progress
df1 = pd.read_csv('2001.csv')
df_en['polarity_vader'].plot.hist(figsize=(10,5), bins=70, title="Vader Histogram")
df_bthlst.head()
df1.head()
df_new[['US', 'UK', 'CA']] = pd.get_dummies(df_new['country']) $ df_new.head()
plt.hist(p_diffs, alpha=0.5);
cp311.head(2)
Project['Date'] = pd.to_datetime(Project['Date'])
import pandas as pd $ data=pd.read_table("https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2015-09.csv",sep=',')
score_rate = sudptable[["author", "score"]] $ score_rate = pd.merge(score_rate, posts, how="left", on="author") $ score_rate["rate"] = score_rate["score"] / score_rate["links"] $ score_rate.sort_values(inplace=True, by="links", ascending=False) $ score_rate[0:9].sort_values(by="rate", ascending=False)
new_columns[0:2]=['Community Name', 'Community Area']
user_predict_df
start_point_2 = [41.884260, -87.630344] # Traffic Court in Richard J. Daley center $ url_2 = ("{}isochrone?fromPlace={},{}&mode={}&date=2016-06-01&cutoffSec={}").format( $     base_url,start_point_2[0],start_point[1],mode,travel_time) $ iso_json_2 = json.loads(requests.get(url_2).text) $ iso_gdf_2 = gpd.GeoDataFrame.from_features(iso_json_2['features'])
a = df2['group']=='control' $ actual_p_old =df2[a].converted.mean() $ print('Probability of individual converted if he is in control group:{}'.format(actual_p_old))
tweet_archive_clean.loc[tweet_archive_clean.name == 'O', 'name']="O'Malley" $ tweet_archive_clean.loc[tweet_archive_clean.name == 'Gin', 'name']="Gin & Tonic"
df_twitter_archive_master[df_twitter_archive_master['tweet_id']=='749981277374128128'].jpg_url
bigdf_read.loc[bigdf_read['submission_title'].isnull()]
cars.isnull().sum()/cars.shape[0] * 100
df.count()
A = pd.DataFrame({'team':['cle','cle','bos'],'pts':[1,2,3]}) $ A
df3=df2 $ df3['intercept']=pd.Series(np.zeros(len(df3)),index=df3.index) $ df3['ab_page'] = pd.Series(np.zeros(len(df3)), index=df3.index)
msftVR2_4 = msftVR[2:4] $ msftVR2_4
train = full_data.loc[full_data.order_date <= pd.to_datetime('2016-03-27',format ='%Y-%m-%d' )] $ test = full_data.loc[full_data.order_date > pd.to_datetime('2016-03-27',format ='%Y-%m-%d' )]
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.binomial(1, p_new, n_new) $     old_page_converted = np.random.binomial(1, p_old, n_old) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean())
shelter_df_only_idx = shelter_df_idx $ shelter_df_only_idx = shelter_df_only_idx.drop('OutcomeType', 'AnimalType', 'SexuponOutcome', 'Breed', 'Color') $
twitter_archive_master.p1.value_counts()
print("cost is:", cost, "revenue is:", revenue, sep=", ")
DATA_DIR = 'data/' $ train = pd.read_csv(path_join(DATA_DIR, 'train.csv'), parse_dates=['date_created', 'user_date_created']) $ test = pd.read_csv(path_join(DATA_DIR,'test.csv'), parse_dates=['date_created', 'user_date_created'])
dup_labels2 = pd.Index(['bar', 'a'])
output= "SELECT count(*) from tweet where tweet_content like '%a%' " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['Count'])
new_dems.Clinton.describe() $
plt.plot([math.log(i) for i in range(1, 1001)])
df_3.keys()
html
stations = session.query(Measurement).group_by(Measurement.station).count() $ print(stations)
unique_users = len(df['user_id'].unique()) $ print('Number of unique users in the dataset:{}'.format(unique_users))
!pip install -r requirements.txt
autos["ad_created_10"].value_counts(normalize = True, dropna = False).sort_index(ascending = False)
ax = hits_df.plot(title="GitHub search hits for {} days".format(len(hits_df)), figsize=figsize) $ ax.set_xlabel('Date') $ ax.set_ylabel('# of ipynb files');
trump.drop('id_str', axis=1, inplace=True) $ trump.drop('in_reply_to_user_id_str', axis=1, inplace=True)
df_namb = df_namb[df_namb.index != 'mdb68'] $ df_ndoor = df_ndoor[df_ndoor.index != 'mdb68'] $ df_nmcu = df_nmcu[df_nmcu.index != 'mdb68'] $ df_nrelay = df_nrelay[df_nrelay.index != 'mdb68'] $ df_ntemp = df_ntemp[df_ntemp.index != 'mdb68']
grouped.groups
pd.crosstab(calls_df['call_time'], calls_df['call_type'], margins=True)
do_something()
new_cust_no_discover = post_launch_emails[post_launch_emails['Time To Buy'] < pd.Timedelta('00:00:00')]['Post Launch Emails'] $ post_discover_sales[~post_discover_sales['Email'].isin(new_cust_no_discover.unique())] $ first_sales_no_discover = post_discover_sales[~post_launch_emails.isin(pre_discover_sales['Email'].unique())] $ first_sales_no_discover = first_sales_no_discover.dropna() $ first_sales_no_discover
year_labeled= 2017 $ year_predict= 2018 $ description_labeled = df[df.year==year_labeled]['description'] $ description_predict = df[df.year==year_predict]['description']
print d.variables['time'][0:10]
change_int_datatype_cols(df_transactions) $ change_float_datatype_cols(df_transactions)
df.loc[df.bottle_vol_ml == 750, 'consolidated_bottle_ml'] = '750ml' $ df.loc[df.bottle_vol_ml == 1000, 'consolidated_bottle_ml'] = '1000ml' $ df.loc[df.bottle_vol_ml == 1750, 'consolidated_bottle_ml'] = '1750ml' $ df.head()
autos[["price", "odometer_km"]].describe()
RMSE_list = [] $ W =[i for i in range(n_training)[3::3]] $ for w in W: $     RMSE_w = ts_kfold(w,w,regr_M7) $     RMSE_list.append(RMSE_w)
click_condition_meta.head(3)
n_new = df[df.landing_page == 'new_page'].count()[0] $ n_new
data[(data['author_flair'] == 'Broncos') & (data['win_differential'] >= 0.9)].comment_body.head(15) $
dir(list)[:10]
prop57.committee_position.value_counts()
knowledge_per_component = git_blame.groupby('component').knowing.mean() $ knowledge_per_component.head()
gs_rfc_over = GridSearchCV(pipe_rfc, param_grid=params_rfc, cv=ts_split_over, scoring='roc_auc') $ gs_rfc_over.fit(X_train, y_train_over)
remove_dog = np.intersect1d(dog1, dog2)
df2.query("user_id == 773192")
batters = df.batter $ names = playerid_reverse_lookup(batters) $ names['batter_name'] = names['name_first'] + ' ' + names['name_last']
df2_curr=pd.get_dummies(df['TRANSACTION_CURRENCY'], prefix_sep='currency') $ df2_curr.head() $ df2=df2.drop(['TRANSACTION_CURRENCY','VOL'], axis=1)
msft_cum_ret.resample("M").ohlc()
logit_mod_1 = sm.Logit(df_new['converted'], df_new[['intercept','ab_page', 'CA', 'UK']]) $ results = logit_mod_1.fit() $ results.summary()
data.head()
predictDT_on_test = model_dt.predict(X_test) # model.predict_proba(X_test) # for predicting in probabilities $ predictDT_on_test $ predictRF_on_test = model_rf.predict(X_test) # model.predict_proba(X_test) # for predicting in probabilities $ predictRF_on_test
tlen_p.plot(figsize=(16,4), color='r');
taxa_count=grouped_by_plot['taxa'].nunique() $ taxa_count.head()
old_page_converted= np.random.binomial(1, p=p_old, size=n_old) $ old_page_converted
pytz.timezone('America/Chicago')
user_repeated = df2[df2.duplicated(['user_id'])]['user_id'].unique() $ print('The repeated user_id is {}'.format(user_repeated))
caps2_output_round_1
pumaBB['public use microdata area'] = pumaBB['public use microdata area'].astype('str')
new_page_converted = np.random.binomial(n_new , p_new) $ new_page_converted
reddit['Subreddits'].sort_values(ascending=False).head(25) #just seeing the top 25 subreddits
df.iloc[]
from experimental.portfolio_rl.clone.src.callbacks.keras_rl_callbacks import TrainIntervalLoggerTQDMNotebook
df.shape
startcut = 900
churn_df.columns
kimanalysis.listfiles(model)
test_join.filter('p_repo_id is not null').count()/test_forks.count()  
extractor = twitter_setup() $ tweets_neta = extractor.user_timeline(screen_name="netanyahu", count=200) $ print("Number of tweets extracted: {}.\n".format(len(tweets_neta))) $ tweets_erdog = extractor.user_timeline(screen_name="RT_Erdogan", count=200) $ print("Number of tweets extracted: {}.\n".format(len(tweets_erdog)))
p(locale.getpreferredencoding)
cnf_matrix = confusion_matrix(y_test, yhat, labels=[2,4]) $ np.set_printoptions(precision=2) $ print (classification_report(y_test, yhat)) $ plt.figure() $ plot_confusion_matrix(cnf_matrix, classes=['Benign(2)','Malignant(4)'],normalize= False,  title='Confusion matrix')
corn.expanding().sum()
counts1 = np.cumsum(np.random.randint(low = 1,high = 40,size= 9)) $ counts2 = np.cumsum(np.random.randint(low = 10,high = 40,size= 9))
games_to_bet['bet_either'] = list((games_to_bet['bet_over']==1) | (games_to_bet['bet_under']==1))
xirrs_all['rating_base'].to_clipboard()
len(seen_and_click.groupby(['article_id','user_type','user_id','project_id']).groups)
lims_query = "SELECT specimens.cell_depth, donors.weight, specimens.id, donors.id \ $               FROM specimens INNER JOIN donors ON specimens.id = donors.id" $ lims_df = get_lims_dataframe(lims_query) $ lims_df.tail()
df[df['abuse_number'].str.contains('ES147799')]
full_globe_temp = pd.read_table(filename, sep="\s+") $ full_globe_temp
@app.route("/api/v1.0/stations") $ def stations(): $     station_results = session.query(Station.station).all() $     all_stations = list(np.ravel(station_results)) $     return jsonify(all_stations)
dc['YearWeek'] = dc['created_at'].apply(lambda x: "%d/%d" % (x.year, x.week)) $ tm['YearWeek'] = tm['created_at'].apply(lambda x: "%d/%d" % (x.year, x.week))
clf.score(X, y)
import numpy as np $ np.random.randint(1,10,len(rng)) $
data_to_clean_body[:2]
financial_crisis.iloc[-2]
result['timestampUTC'] = pd.to_datetime(result['timestampCorrected'], unit='ms')
companies = pd.read_csv("companies.csv") $ companies[companies['trc_industry_group'] == "Investment Banking & Investment Services"]
df.drop(holidays, 1, inplace=True)
tensor_2d = np.arange(16).reshape(4, 4) $ print(tensor_2d) $ tf_tensor = tf.placeholder(tf.float32, shape=(4, 4)) $ with tf.Session() as sess: $     print(sess.run(tf_tensor, feed_dict={tf_tensor: tensor_2d}))
df2 = pd.DataFrame() $ df2['X'] = norm.rvs(size=N)
le.fit(df_members['bd_c'])
evals_mean = evals.mean(axis=0)
df_twitter_archive_master[df_twitter_archive_master['rating_numerator']==1] $
html = browser.html $ soup = bs(html, 'html.parser') $ url_hemispheres_link = soup.find_all('a', class_="itemLink product-item") $ url_hemispheres_link
data.dropna().describe()
import sys $ !{sys.executable} -m pip install pandas-profiling
df.loc[df.toes.str.match(pattern3)==True]
df_control = df2.query('group == "control"') $ cont_convert = df_control.query('converted == 1').shape[0] / df_control.shape[0] $ cont_convert
model = Sequential() $ model.add(LSTM(20, input_shape=(train_X.shape[1], train_X.shape[2]))) $ model.add(Dense(1)) $ model.compile(loss='mae', optimizer='adam') $ model.summary() $
autos['ad_created'] = autos['ad_created'].str[:10].str.replace('-', '').astype(int) $ autos['ad_created'].head()
people.plot(kind = "line", x = "body_mass_index", y = ["height", "weight"]) $ plt.show()
df_h1b_ft_US = df_h1b_ft_US.fillna(0).drop([47419, 80529])
returns.corrwith(volumn)
hyper_parameters(slidingwindow) $ model = setupNetwork(slidingwindow) $ print(model.summary())
labels = list(crf.classes_) $
df2['DepTime'].count()
tcp_files = [f for f in os.listdir(folder) if re.match('.*TC.*.phys.*.csv', f)] $ tcp_files
d = {'Flavor': ['Strawberry', 'Vanilla', 'Chocolate'], 'Price' : [3.50, 3.00, 4.25]} $ icecream = pd.DataFrame(data=d) $ icecream
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05))) $
plt = sns.boxplot(data=df, x="race_desc", y="charge_count", hue="race_desc", dodge=False) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) $ plt.set_xticklabels(plt.get_xticklabels(), rotation=45)
N_samples = 25 $ x=np.linspace(-2,2,N_samples)
right = pd.DataFrame({'key' : ['foo', 'foo'], 'rval': [4,5]}) $ right
path = '/home/gabriel/share/Prestashop/' $ data = pd.read_csv(path+"Prestashop_Ready_dataset.csv")
plt.scatter(tbl['mr_rf'],tbl['pr_rf']) $ plt.plot(tbl['mr_rf'], result.fittedvalues,'g')
la_inspections_new.describe()
df['converted'].sum() / df.shape[0]
tw_clean.to_csv("twitter_archive_master.csv")
full_diff = full_p_new - full_p_old $ full_diff
search_response
import pandas as pd $ dataset = pd.ExcelFile("basedados.xlsx") $ data = dataset.parse(0)
grinter_day0.head()
doi_pid.shape[0], doi_pid.drop_duplicates().shape[0]
twitter_archive.retweeted_status_id.count()
new_page_converted = np.random.choice([0,1], n_new, [1-p_new, p_new]) $
missing_info = list(df_.columns[df_.isnull().any()]) $ missing_info
testdata = raw_data.copy() $ testdata["order_created_at"] = pd.to_datetime(testdata["order_created_at"], utc=True, errors='coerce') $ testdata.set_index("order_created_at",inplace=True) $ test = testdata.index[0]
df = pd.read_csv("../../data/msft2.csv",skiprows=[0,2,3]) $ df
df.loc[:,'G'] = np.array([5] * len(df)) $ df
char_vectorizer =  TfidfVectorizer(max_features=2500,  $                                   stop_words=stop_words, $                                   analyzer='char', $                                  ) $ char_vectorizer.fit(all_text)
pitches_unique_labels = list(pitches.columns.difference(responses.columns)) $ pitches_unique_labels
tmp_cov_4
namb_instance = td_amb / np.timedelta64(30, 's') $ ndoor_instance = td_door / np.timedelta64(30, 's') $ nmcu_instance = td_mcu / np.timedelta64(30, 's') $ nrelay_instance = td_relay / np.timedelta64(30, 's') $ ntemp_instance = td_temp / np.timedelta64(30, 's')
df.iloc[[1, 3, 5], [1, 3]]
control_con = df2[(df2['group'] == 'control') & (df2['converted'] == 1)].shape $ control_con
p_new = df2.query('converted == 1').user_id.count()/df2.user_id.count() $ print('Conversion of new Page is: ', p_new)
df_master[df_master.retweet_count == [df_master['retweet_count'].max()]]
.5 / 1.6
author_data['reputation_score'] = author_data.apply(lambda row: reputation_score(row['reputation']), axis=1)
np.info(np.diag)
popt_axial_chord_crown, pcov_axial_chord_crown = fit(d_axial_chord_crown)
df_image_clean[df_image_clean['img_num'] == 2].sample().image_url
import pandas as pd $ groceries = pd.Series(data = [30, 6, 'Yes', 'No'], index = ['eggs', 'apples', 'milk', 'bread']) $ groceries
print(diff.days)
plt.plot(xs)
train_df[["Sex", "Survived"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)
p_diffs = [] $ for _ in range(10000): $     new_page_convert = np.random.binomial(n_new, p_new)/n_new $     old_page_convert = np.random.binomial(n_old, p_old)/n_old $     p_diffs.append(new_page_convert - old_page_convert)
import os $ import re $ folder = "2018-06-26" $ tce_files = [f for f in os.listdir(folder) if re.match('.*TC.*.elec.*.csv', f)] $ tce_files
profitMulti[150]
def filter_func(x): $     return x['data2'].std() > 4 $ display('df', "df.groupby('key').std()", "df.groupby('key').filter(filter_func)")
len(youthUser2015)
df = df.drop_duplicates().dropna()
new_page_converted = np.random.binomial(1, size = n_new, p=p_new) $ new_page_converted
df_cleaned_train = df['text_cleaned'].tolist()
df.query("landing_page=='new_page' and group=='treatment'").shape[0]
yr, mo, dd = 2012, 12, 21 $ dt.date(yr, mo, dd) $ dt.date(2012, 12, 21)
fullDf.level.value_counts() $
df2_control.query('converted == 1')['user_id'].count()/df2_control['user_id'].count()
giss_temp = pd.read_table("data/temperatures/GLB.Ts+dSST.txt", sep="\s+", skiprows=7, $                           skip_footer=11, engine="python") $ giss_temp
num_lines = access_logs_raw.count() $ num_lines
ideas.duplicated().sum()  # Checking for duplicated data 
!du -h train.zip
reddit['Subreddits'].nunique() #how many unique subreddits do we have? 
liquor2016_q1.Whisky = liquor2016_q1.CategoryName.str.contains('WHISK') * liquor2016_q1.SaleDollars $ liquor2016_q1_whisky = liquor2016_q1.Whisky.groupby(liquor2016_q1.StoreNumber).agg(['sum']) $ liquor2016_q1_whisky.columns = ['Whisky'] $ liquor2016_q1_whisky.tail()
with open('miya_kyoto_tweet','rb') as f: $     tweets_kyoto = pickle.load(f)
df_with_metac_with_onc = pd.concat([df_with_metac1, df_oncstage_dummies], axis=1)
center_attendance_pandas.head(10)
data_splitted = data.split("\n") $ print(data_splitted[0])
ogdata=pd.read_csv('datafinal.csv') $ ogxprep = ogdata.copy() $ ogxprep.drop(['irlco', 'Unnamed: 0', 'Unnamed: 0.1'], axis=1, inplace=True) $ ogxprep.columns.values[2] = '1hr'
LOG_DIR
df = final_annotations_df $ df[df.choice.str.contains('WOLF') & (df.how_many > 1)]
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ df = pd.read_table(path, sep ='\s+', header=None) $ df.head(5)
inspector = inspect(engine) $ mcolumns = inspector.get_columns('measurement') $ for c in mcolumns: $     print(c['name'], c["type"])
t2 = pd.Series(list('def'), [pd.Period('2016-09'), pd.Period('2016-10'), pd.Period('2016-11')]) $ t2
import pandas as pd $ import numpy as np $ from sklearn.ensemble import RandomForestRegressor $ from sklearn.model_selection import train_test_split
df_clean.rating_denominator.value_counts()
catalog_df = week1_df.append(week2_df) $ catalog_df
normalized_df = pd.DataFrame(normalized_df) $ normalized_df.columns = df.columns $ normalized_df.head()
mrtrumpdf['tags'] = mrtrumpdf.text.apply(lambda s: TextBlob(s).tags)
df_TempIrrs.info()
im.tail()
tweet_hour.count()
df2.drop_duplicates('user_id', inplace=True) $ df2.shape
customer_id = '/7/KMLZlMBnmWtb9NNkm3bYMQHWrt0C1BChb62EiQLM=' $ cust = trans.loc[trans['msno'] == customer_id].copy() $ cust.iloc[:, 1:].tail(6)
reviews.rename(columns={'region_1':'region','region_2':'locale'})
import time $ start = time.time() $ trips = pd.read_csv('trip.csv', sep=',', parse_dates=['start_date','end_date'], $                       infer_datetime_format=True,low_memory=False) $ print 'Time elapsed: ' + str(time.time() - start)
model.clusterCenters()
df = pd.read_sql('SELECT * FROM actor WHERE actor_id = 172', con=conn) $ df
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4) $ knn = KNeighborsClassifier(n_neighbors=5) $ knn.fit(X_train, y_train) $ y_pred = knn.predict(X_test) $ print(metrics.accuracy_score(y_test, y_pred))
image_predictions_df.sample(3)
%%timeit $ newton_raphson_variable(myfunc, x0=2)
label_index=np.repeat(np.array(labels), 3) $ community_index=np.repeat(np.array(communities), 3)
import sklearn $ sklearn.__version__
treatment_conversion = df2[df2['group'] == 'treatment']['converted'].mean() $ treatment_conversion
address_df['nndr_prop_ref']=address_df['nndr_prop_ref'].apply(str) $ address_df['nndr_prop_ref'].head(5)
df_enhanced = pd.read_csv('twitter-archive-enhanced.csv') $ df_enhanced.head()
m_ms = df_ms[df_ms['hour'] < df_ms['hour'].median()] $ a_ms = df_ms[df_ms['hour'] >= df_ms['hour'].median()] $ m_hc = df_hc[df_hc['hour'] < df_hc['hour'].median()] $ a_hc = df_hc[df_hc['hour'] >= df_hc['hour'].median()]
path_to_my_database = 'C:\\Users\Andrew\\Desktop\\School\\Introudction to Data Sciecne & Analytics\\database.sqlite' $ conn = sqlite3.connect(path_to_my_database)  # connect to database $ cur = conn.cursor()  # create cursor
X2.shape
plt.plot(df_y_actual, df_y_pred, 'go') $ plt.xlabel('Actual age') $ plt.ylabel('Predicted age')
All_tweet_data_v2['unknown']=False
const_cols = [c for c in train_data.columns if train_data[c].nunique(dropna=False)==1 ] $ const_cols
ggplot(raw_large_grid_df.query("subject=='VP4'"),aes(x="duration"))+geom_histogram(binwidth=0.1)+coord_cartesian(xlim=(0,3))+facet_grid("~eyetracker")
kimanalysis.getitem('LammpsExample__TD_567444853524_004')
dogs = 103*23/2 $ cats = 10*12*4 $ print str(dogs > cats) + ": Dogs are greater than cats" $ print "Dogs = " + str(dogs) $ print "Cats = " + str(cats)
predicted = model2.predict(X_test) $ print predicted
offseason16["InorOff"] = "Offseason"
df_ml_6204.tail(5)
file_name = 'Pairs of names and ID numbers.csv' $ all_sites_with_unique_id_nums_and_names.to_csv(file_name, index=False) $
y = list(train_1m_ag.is_attributed) $ X = train_1m_ag.drop(['is_attributed'],axis=1)
!du -h sample_submission.csv
with open("../tweets.json") as f: $     tweets = json.load(f) $ tweets[:2]
get_open_price = lambda key: get_float_value(r_dict, key, 'Open')
ex_index=df.index.isin([5,12,23,56]) $ df[~ex_index].head(10)
hr, mm, ss, ms= 12, 21, 12, 21 $ dt.time(hr, mm, ss, ms)
le=LabelEncoder() $ X = df[['word_count','sentiment','domain_d','title','post_duration','text']] $ y = le.fit_transform(df['subreddit']) $
iterator.df.head()
ls ../../outputs-git_ignored/gpu_mod_run_1
import numpy as np $ np.random.randint(1,10,len(rng)) $
autos = autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].value_counts(normalize=True).head(10)
df_gateways = pd.read_csv(read_inserted_table(dumpfile, tablename),delimiter=",",error_bad_lines=False) $ df_gateways.head(10)
dfb_train=train.date_first_booking $ dfb_test=test.date_first_booking
au.find_some_docs(ao18_qual_coll,limit=3)
depthdf.set_index('max_depth')['score'].plot(figsize=(16,8))
c_y = 
for x in range(5): $     print amanda[x]['text'] $     print('--')
treat_prob = df2[df2['group'] == 'treatment']['converted'].mean() $ treat_prob
lol_dummies.head()
num_unique_users = len(people_person['id'].unique()) $ print("The number of users signed up is: ", num_unique_users)
autos['nr_of_pictures'].head(10)
print("\nClassification Report:\n",classification_report(y_test, y_hat)) $
autos['price'].value_counts().sort_index(ascending=False).head(20)
processed_tweets.sample(4)
h + pd.offsets.Hour(1) # Will give us the same result as the h + 1 sum above
frame = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), $                     index=['Utah', 'Ohio', 'Texas', 'Oregon'])
p_old=df2.query('converted==1').count()[0]/df2.count()[0] $ p_old
bitinfocoins = ['eth', 'ltc', 'xrp', 'bch', 'dash', 'xmr', 'etc', 'rdd', 'zec', 'doge'] $ for coin in bitinfocoins: $     all_coins_df = all_coins_df.merge(my_cryptory.extract_bitinfocharts(coin), on="date", how="left") $ all_coins_df
frame
df_users_2=pd.merge(df_users_1,pt_download[['uid','no_of_downloads','downloaded_or_not']],left_on='uid',right_on='uid',how='left') $ df_users_2.loc[df_users_2['downloaded_or_not'].isnull(),'downloaded_or_not']='Not downloaded'
means = calculate_ensemble_score(forest, est, Xcols, 'Popular', train)
for c in ccc: $     ved[c] /= ved[c].max()
df['userid'] = range(101,107)
from nltk.corpus import stopwords $ stop = stopwords.words('english') $ df['body'] = df['body'].apply(lambda x: " ".join(x for x in x.split() if x not in stop)) $ df.body.head()
p_new = df2.query('landing_page == "new_page"').converted.mean() $ p_new
pd.melt(df, id_vars=['A'], value_vars=['B'])
train_view.sort_values(by=2, ascending=False)[0:10]
test.head()
dictionary=pd.read_csv("dictionary.csv") $ with open("IMDB_dftouse_dict.json", "r") as fd: $     IMDB = json.load(fd) $ IMDB_df = pd.DataFrame(IMDB)
print(len(forecast)) $ forecast.tail(3)
tweets.set_index('Serial',inplace=True)
Meter1.QurreyProgress(debug=True)
reflArray = reflClean / metadata['reflectance_scale_factor'] $ reflArray
bonus_points
trading = pt.PairsTrading(asset1 = 'COPUSD', $                           asset2 = 'AUDUSD', $                          df=df)
names.groupby('name')['retweets'].sum().sort_values(ascending=False).head(10)
quotes = yahoo_finance.download_quotes(["GILD", "IBM"]) $ print quotes
learner = md.get_model( $     opt_fn, em_sz, num_hidden, num_layers, dropouti=drops[0], dropout=drops[1], $     wdrop=drops[2], dropoute=drops[3])
dfs[25].describe()
model_w_int = Logit(df2['converted'], $                            df2[['intercept', 'ab_page', 'CA', 'UK', $                                'ab_page_and_CA', 'ab_page_and_UK']]) $ results_w_int = model_w_int.fit() $ results_w_int.summary()
logodds.drop_duplicates().sort_values(by=['count']).plot(kind='barh')
gs_k150.score(X_train, y_train_over)
model = SelectFromModel(lr2, prefit=True) $ index_smote = model.get_support()
ControlGroup = df2.query('group == "control"')['user_id'].count() $ ControlGroupConverted = df2.query('converted == 1 and group=="control"')['user_id'].count() $ print("Probability of Control Group Converted: ",(ControlGroupConverted/ControlGroup)) $ ctrl_conv_rt = (ControlGroupConverted/ControlGroup)
hn['popular'] = (hn.num_points > 5).astype(int)
df.loc[:, ["last_name"]]
cumulative_returns.sum()
len(news_df)
logisticRegr.fit(train_ind[features], train_dep[response]) $ zip(features,logisticRegr.coef_[0]) $ train_features_df= train_ind[features] $ train_features_arr= train_features_df.values $ zip(features,(np.std(train_features_arr)*logisticRegr.coef_[0]))
df_final = df_complete_b.drop(bad_replies.index) $ df_final.reset_index(drop=True, inplace=True) $ df_final.head()
dr.columns
! unzip ./data/train.zip
%matplotlib inline $ import pandas_profiling $ import seaborn as sns; $ titanic = sns.load_dataset('titanic') $ pandas_profiling.ProfileReport(titanic)  # You may need to run cell twice
network = popnet.PopNetwork.from_config(configure) $ sim = popnet.PopSimulator.from_config(configure, network) $ sim.run()
scores = model.evaluate(X_train, y_train, verbose=0) $ print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100)) $ test_sample=np.array([145,183,1902,1795,178]).reshape(1,5) $ sample_output=model.predict_classes(test_sample) $ print(sample_output) $
clfgtb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(x_train, y_train) $ clfgtb.score(x_test, y_test)
input_nodes_DF = nodes_table('network/source_input/nodes.h5', 'inputNetwork') $ input_nodes_DF[1:5]
df.rename( columns={'year.1':'year1', 'year.2':'year2'}, inplace=True) $ df.head(2)
author_name_id = xml_in_sample.drop_duplicates(subset=['authorId','authorName'], keep='first')[['authorId','authorName']] $ df_movies = pd.merge(df, author_name_id, left_on=['movieId'], right_on=['authorId'],  how='left')[['movieId', 'authorName']] $ df_movies.head() $ df_movies.drop_duplicates(subset=None, keep='first', inplace=True)
localized.tz_convert('UTC')
log_mod3 = sm.Logit(df3['converted'], df3[['intercept','ab_page', $                                             'evening']]) $ results3 = log_mod3.fit()
pd.merge(df1, df3, left_on='employee', right_on='name' )
%%HTML $ <iFrame src="https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior" width=900 height400></iFrame>
import os $ import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import seaborn as sns $
twitter_df_clean['ratio']=twitter_df_clean.rating_numerator/twitter_df_clean.rating_denominator
with_countries['UK_ab_page'] = with_countries['ab_page']* with_countries['UK'] $ uk_new_page = sm.Logit(with_countries['converted'], with_countries[['intercept', 'ab_page', 'UK_ab_page', 'UK']]).fit() $ print(uk_new_page.summary())
trigram_bow_corpus = get_trigram_bow_corpus(trigram_dictionary, from_scratch=False)
data.loc[data['hired']==1].groupby('category').hourly_rate.mean()
df_countries.isnull().sum()
stop = stopwords.words('english') $ locationing['Text'] = locationing['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) $ pd.Series(' '.join(locationing['Text']).lower().split()).value_counts()[:50]
pd.Series([1, 2, 3, 'a', 'b', 'c']).dtypes
n = -100 $ similar = LSI_model.get_dists(corpus[n]) $ print(df.iloc[n]['desc']) $ print(corpus[n]) $ df.loc[list(similar.keys())]['desc'].values.tolist()
tweets['created_at'] = tweets.apply( $     convert_timezone, $     axis=1, $     args=(ConvertTZArgs('created_at', 'US/Central'),) $ )
%%time $ like_plaintiff = geocoded_df.apply(lambda x: pratio(x['Plaintiff.Name'], x['Judgment.In.Favor.Of']), axis=1) $ like_defendant = geocoded_df.apply(lambda x: pratio(x['Defendant.Name'], x['Judgment.In.Favor.Of']), axis=1)
wgts['0.encoder.weight'] = T(new_w) $ wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w)) $ wgts['1.decoder.weight'] = T(np.copy(new_w))
caps2_n_caps = 10 $ caps2_n_dims = 16
lr = LogisticRegression() $ lr.fit(X_train,y_train)
ps
df = pd.read_csv('df')
Y_train_df = pd.DataFrame(Y_train, columns=['Tag']) $ Y_train_df = pd.get_dummies(Y_train_df)
df_test['due_date']=pd.to_datetime(df_test['due_date']) $ df_test['effective_date']=pd.to_datetime(df_test['effective_date']) $ df_test.head()
all_states = pd.merge(states, pd.DataFrame(df.state.unique(), columns=['state']), on='state', how='right') $ invalid_states = all_states[pd.isnull(all_states.id)].state
df_2011['bank_name'] = df_2011.bank_name.str.split(",").str[0] $
n_new = treatment.shape[0] $ n_new
localized[0]
finalmodel = setupNetwork(slidingwindow) $ finalmodel = trainModel(model, trX.reshape(-1,24,1), trY.reshape(-1,1), 10, 48,2)
df = pd.DataFrame(data=members) $ df.set_index(['id']); $ df.visited = (df.visited.values/1000).astype('datetime64[s]') $ df.joined  = (df.joined.values/1000).astype('datetime64[s]') $ df.columns
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ logit_mod = sm.Logit(df2.converted, df2[['intercept', 'ab_page']]) $ results = logit_mod.fit() $ results.summary()
df.columns
dbcon = sqlite3.connect("mobiledata.db") $
A - A[0]
popCon[popCon.content == 'post'].sort_values(by='counts', ascending=False).head(10).plot(kind='bar')
advanced_describe(train_data)
trip_df = pd.DataFrame(normals, columns=['tmin', 'tavg', 'tmax']) $ trip_df.head()
del res_agg $ del res
merged_data = pd.merge(trip_data, weather_data, left_on = ["start_date","zip"], right_on = ["Date","zip"])
to_be_predicted_Day1 = 17.56 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
print (df.iloc[ [1,3], [1,3]],'\n' )   # by individual rows/columns $ print (df.iloc[  1:3 ,  1:3], '\n')    # by range
df_new = pd.concat([df_2001, df_2002, df_2003, df_2004, df_2007, df_2008, df_2009, df_2010, df_2011, df_2012, df_2013, df_2014, df_2015, df_2016, df_2017], sort=True)
a = 'some string'
cp311[['status']].groupby([cp311.borough]).count()
order_data.set_index('userId', inplace=True)
df = system_df
conn.fetch(table=dict(name='data.iris', caslib='casuser'), to=5, $            sortby=['sepal_length', 'sepal_width'])
merkmale.columns
plt.plot(range(df_concensus_uaa_eps.shape[0]), df_concensus_uaa_eps['esimates_count'])
counts_df[counts_df.tweet_id.duplicated()]
df['Sentiment'] = np.array([get_sentiment(tweet) for tweet in df['Clean Tweets']])
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)
sets.head(12)
twitter_archive.name.value_counts()
print(autos['date_crawled'].str[:10].value_counts(normalize = True, dropna = False).sort_index()) $ print(autos['date_crawled'].str[:10].value_counts(normalize = True, dropna = False).shape)
themes = ' '.join(themes)
1/np.exp(.0783), 1/np.exp(.0469), 1/np.exp(.0118), 1/np.exp(.0175), np.exp(results.params)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative = 'larger') $ print('z-Score:',z_score,'\np-Value:', p_value)
  hp.dtypes
msftAC['2012-01-03']
users['Month'] = users['CreationDate'].apply(lambda x: x.month)
sox.head()
df4.fillna(value = 6)
r=rq.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')
in_latlon = lambda x: np.logical_and(in_range(x.lat, 'lat'), in_range(x.lon, 'lon')) $ in_domain = lambda x: np.logical_and(in_latlon(x), in_range(x.alt, 'alt')) $ print('in_latlon: %6d'%in_latlon(adm).sum()) $ print('in_domain: %6d'%in_domain(adm).sum())
stack_with_kfold_cv['gb_preds']=gb_best.predict(train[col]) $ stack_with_kfold_cv['knn_preds']=knn_best.predict(train[col]) $ stack_with_kfold_cv['xgb_preds']=xgb_best.predict(train[col]) $ stack_with_kfold_cv['rf_preds']=rf_best.predict(train[col]) $ stack_with_kfold_cv['lgb_preds']=lgb_best.predict(train[col])
(tech/tech.ix[0]-1).plot()
dt_year = toDate('07 15, 2014') $ new_df = review_df[review_df.reviewDate > dt_year] $ new_total = len(new_df) $ print (new_total)
sep2014 = aug2014 + 1 $ sep2014
solar_wind_col = ['solar_value', 'solar_maxvalue', 'solar_minvalue', 'wind_value', 'wind_maxvalue', 'wind_minvalue'] $ solar_wind_df = pd.read_excel(weather_folder + '/complete_solar_wind.xlsx') $ solar_wind_df[ solar_wind_col ]/=4 $ solar_wind_df.to_excel(weather_folder + '/averaged_solar_wind.xlsx', index = False)
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK','CA']]) $ results = mod.fit()
from sklearn.ensemble import RandomForestClassifier , ExtraTreesClassifier $ from sklearn.model_selection import cross_val_predict
joined_sample_saved = non_blocking_df_save_or_load( $     joined_sample, $     "{0}/joined_sample_3".format(fs_prefix)).alias("joined_sample")
states['area']
pprint("Month with the largest number of job ads: 9.0(September)-1652")
archive_copy[['tweet_id', 'id_str']].head()
flight_hex = h2o.H2OFrame(flight_pd)
a = raw.values.reshape(1,-1)
def infer_one_doc(doc): $     rep = doc2vec_model.infer_vector(doc[1]) $     return (doc[0], rep)
df_pol['text'] = [process_lem(text) for text in df_pol['text']]
predicted_logpdp95 = 4.63 + 0.53 * 7.07 $ predicted_logpdp95
data.pivot(columns="center_name", values="attendance_count").head()
from matplotlib_venn import venn2 $ venn2(subsets={'10': n_titles-n_titles_tags, '01': n_tags-n_titles_tags, '11': n_titles_tags}, set_labels = ('Titles', 'Tags')) $ print "Total of",len(bag_tags.intersection(bag_title)),"common keywords btw Tags and Title from a total of",\ $       len(bag_tags.union(bag_title)),"different keywords"
X_new.shape $
np.random.seed(123456) $ ts = Series([1,2,2.5,1.5,0.5],pd.date_range('2014-08-01',periods=5)) $ ts
df3[['ab_page', 'not_ab_page']] = pd.get_dummies(df3['landing_page']) $ df3 = df3.drop('not_ab_page', axis=1) $ df3['intercept'] = 1
data[data['lat-lon'] == '-34.3402525,-58.7849434']
df = df[df.first_message_month < 10] $ df = df[df.first_message_month > 2] $ print(min(df.first_message_timestamp)) $ print(max(df.first_message_timestamp)) $ print("... cropped date to Jan through Sept 2017")
import requests $ import pandas as pd
print('Reply tweets: ' + str(len(tweets_clean.query('in_reply_to_status_id == in_reply_to_status_id')))) $ print('Total tweets: ' + str(len(tweets_clean)))
kd915_filtered.head()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ z_score, p_value
np.vstack((a, a))[(0,1,2),:]
metadata_df[1]['Variable'].fillna(method='ffill', inplace=True) $ metadata_df[1].head()
brand_counts = autos["brand"].value_counts(normalize = True) $ popular_brands = brand_counts[brand_counts>0.05].index
df.query("converted == 1").user_id.unique().size/df.user_id.unique().size
import pickle $ pickle.dump((fraud_data_updated),open('preprocess.p', 'wb')) $
mismatch_g1 = df.query('group == "treatment" and landing_page == "old_page"') $ len(mismatch_g1)
df2_curr.head()
df.drop('Date', inplace=True, axis=1)
Station.__table__
s = pd.Series([80,2,50], index=['HPI','Int_rate','US_GDP_Thousands']) $ print s
from PIL import Image $ import requests $ url = user['profile']['image_192'] $ im = Image.open(requests.get(url, stream=True).raw) $ plt.imshow(im)
autos[["ad_created", $        "date_crawled", $        "registration_year", $        "registration_month", "last_seen"]].describe(include = "all")
flights_by_month = flights.groupby('MONTH') $ num_fli_by_month = flights_by_month.size() $ num_fli_by_month
n_new = df2['landing_page'].value_counts()[0] $ print('Number of tests with the new page:', n_new)
list_users = dict(df_ta_caps_user["user name"].value_counts()).keys() $ list_capsule = dict(df_ta_caps_user["capsule name"].value_counts()).keys()
geocoded_df[list(filter(lambda x: x.endswith('Date'), geocoded_df.columns))] = \ $     geocoded_df[list(filter(lambda x: x.endswith('Date'), geocoded_df.columns))].apply(pd.to_datetime)
nbar_clean.red
df_goog.dtypes
df.boxplot(figsize=(20, 10)) $ plt.show()
num1=df.query('landing_page=="new_page" and group!="treatment"').count()[0]
round(len(df_twitter[df_twitter.dog_label == 'puppo']) / len(df_twitter.dog_label), 2)
run txt2pdf.py -o "VIDANT MEDICAL CENTER  Sepsis.pdf"   "VIDANT MEDICAL CENTER  Sepsis.txt"
df1=final[['RA0','DEC0','name','exposure_time']] $ df2=sdss_final[['RA0','DEC0','name','exposure_time']]
!wget download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz $ !ls
df2.tail()
y_retweet = df['retweet_count'] #1 Output $ y_fav = df['favorite_count']    #2 Output $ x.head()
session_v2.head()
df.tail(2)
df = pd.read_csv('data/bigmac.csv') $ df.head()
df = pd.read_csv('./meal_seats.csv', delimiter='|', names=['meal_id', 'meal_created_date', 'meal_date', 'ticket_price',  'seats_available', 'seats_sold',  'percentage_seats_sold']) $ df['meal_year'] = pd.to_datetime(df.meal_date).apply(lambda x: x.year) $ df = df[(df.ticket_price < 90) & (df.ticket_price > 20) & (df.meal_year == 2016)]
conn_b.commit()
fig, ax = plt.subplots() $ load2017.iloc[:96].plot(ax=ax, title="Actual load 2017 (DE-AT-LUX)", x='timestamp', y='actual', lw=0.7) $ ax.set_ylabel("Actual Load") $ ax.set_xlabel("Time") $
import pandas as pd $ test = pd.read_csv("datatestSinNan.csv")
X_trainfinaltemp= X_trainfinal.copy() $ X_trainfinaltemp = X_trainfinaltemp.rename(columns={'fit': 'fit_feat'})
tweets_clean = tweets.copy() $ tweet_data_clean = tweet_data.copy() $ images_clean = images.copy() 
merged.sort_values("amount", ascending=False)
to_pickle('Data files/clean_dataframe.p',adopted_cats) $ adopted_cats=from_pickle('Data files/clean_dataframe.p')
h.steady_states $
df_goog.dtypes
df_signs = pd.DataFrame(signs, columns = ['sign'], index= newdf.index) $ newdf = newdf.join(df_signs, how = 'inner')
df = pd.read_csv("../data/sales_report.csv", sep=',', encoding='utf-8') $ df.head()
polarity_count
print('Control group : ', conv_control_prob) $ print('Treatment group: ', conv_treat_prob)
notus.loc[notus['country'] == 'Canada', 'cityOrState'].value_counts()
df_link_yt['video_title'].value_counts().head(10)
import pandas as pd $ df = pd.read_csv('./data/collapsed_go.no_IGI.propagated.term_sizes', sep='\t', names=['id', 'genes']) $ df.head(10)
np.save('p_diffs', p_diffs)
lite = df[df['tweet'].str.contains("Litecoin") | df['tweet'].str.contains("ltc") | df['tweet'].str.contains("LTC")] $ lite = lite.reset_index(drop=True) $ lite.info()
plt.figure(figsize=(8, 5)) $ plt.scatter(train_df.favs_lognorm, train_df.views); $ plt.title('The distribution of the favs_lognorm and number of the views'); $
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 2) & (jobs.GPU == 1) & (jobs.Group == 'cms')][['Memory','Wait']].groupby(['Memory']).agg(['mean', 'median','count']).reset_index()
len(us) + len(notus) == len(geo)
Test.head()
plt.savefig(str(output_folder)+'NB01_2_landscape_image01_'+str(cyclone_name)+'_'+str(location_name)+'_'+time_slice_str)
barcelona.index = barcelona['GMT']
df3.at[d[3],'A'] = 0 $ df3
import quandl $ import json $ import requests $
df2= df2.join(pd.get_dummies(df2['landing_page']))
false_churn_bool = [np.any([USER_PLANS_df.loc[unchurn]['status'][i] !='canceled' for i,j in enumerate(USER_PLANS_df.loc[unchurn]['scns_created']) if j==max(USER_PLANS_df.loc[unchurn]['scns_created'])]) for unchurn in churned_ix] 
submit.tail()
import pandas as pd $ results = pd.read_csv('results.csv', index_col=0) $ results
logs = pd.read_csv(os.path.join('', 'keylogger_20171005.testlog'), names=['timestamp', 'type', 'key'], header=None, $                   parse_dates=['timestamp'], index_col='timestamp', comment='#') $ print("Total of {} events".format(len(logs))) $ logs.head()
print('Demo date repeat ids', df_demo[df_demo.duplicated(['patient_id'])].shape) $ print('Clinic data repeat ids', df_clinic[df_clinic.duplicated(['patient_id'])].shape) $ print('Example of repeat ids in clinical data:') $ sample_repeat = df_clinic[df_clinic.duplicated(['patient_id'])]['patient_id'].iloc[0] $ df_clinic[df_clinic['patient_id'] == sample_repeat]
multi_app = picker[picker>1] $ print len(multi_app) $ df_nona[df_nona.district_id.isin(multi_app.index)].install_rate.hist() $
%matplotlib inline $ import matplotlib.pyplot as plt $ import pandas as pd $ print("pandas version {}".format(pd.__version__))
new_page_converted = np.random.choice([0,1],N_new, p=(p_new,1-p_new)) $ new_page_converted 
data = pd.read_csv('data/FremontBridge.csv', index_col='Date', parse_dates=True) $ data.head()
result_df = pd.DataFrame(test['comment_id']) $ result_df['is_fake'] = prediction[:, 1]
df1.head(500)
frequent_authors = authors[(authors['count'] >= 100) & (authors['mean'] >= 0.3)] $ frequent_authors.shape[0]
pos_tweets = [ tweet for index, tweet in enumerate(df['cleanText']) if df['label'][index] > 0] $ neu_tweets = [ tweet for index, tweet in enumerate(df['cleanText']) if df['label'][index] == 0] $ neg_tweets = [ tweet for index, tweet in enumerate(df['cleanText']) if df['label'][index] < 0]
df.drop(df.query("group != 'treatment' and landing_page == 'new_page'").index, axis = 0,inplace = True) $ df.drop(df.query("group == 'control' and landing_page != 'old_page'").index, axis= 0,inplace = True) $ df.drop(df.query("group != 'control' and landing_page == 'old_page'").index,axis = 0,inplace = True)
last_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first() $ first_date = dt.date(2018, 7, 4) - dt.timedelta(days=365) $ one_yr_prcp = session.query(Measurement.date, Measurement.prcp).\ $                         filter(Measurement.date > first_date).\ $                         order_by(Measurement.date).all()
for row in soup.select('h3 > a.fade'): $     print(row['href'])
new_converted = np.random.choice([1, 0], size=len(df_new), p=[P_mean, (1-P_mean)]) $ new_converted.mean()
df.track_popularity.hist()
pgh_311_data['REQUEST_TYPE'].value_counts()
users_orders = users_orders.assign(Order_14 = lambda x: 1 * (x.Reg_date + pd.DateOffset(14) >= x['Order Date'])) $ users_orders = users_orders.assign(Order_30 = lambda x: 1 * (x.Reg_date + pd.DateOffset(30) >= x['Order Date'])) $ users_orders = users_orders.assign(Order_60 = lambda x: 1 * (x.Reg_date + pd.DateOffset(60) >= x['Order Date'])) $ users_orders.head()
data.columns
twitter_data = addmorefeatures(tweets) $ display(twitter_data[-10:])
autos = autos.drop(["nr_of_pictures", "seller", "offer_type"], axis = 1)
sq81= "CREATE TEMPORARY TABLE  newtable_111 ( SELECT * FROM NBA_homepage order by 0.4*Fav+0.6*retweets DESC limit 50)" $ sq82="SELECT word, COUNT(*) total FROM ( SELECT DISTINCT Num, SUBSTRING_INDEX(SUBSTRING_INDEX(text,' ',i+1),' ',-1) word FROM newtable_111, ints) x where word like'%#%'GROUP BY word HAVING COUNT(*) > 0 ORDER BY total DESC, word;"
big_df_count=big_df_count[['winter1_Polarity','spring1_Polarity','summer1_Polarity','fall1_Polarity', $                   'winter2_Polarity','spring2_Polarity','summer2_Polarity','fall2_Polarity', $                   'winter3_Polarity','spring3_Polarity','summer3_Polarity','fall3_Polarity', $                   'winter4_Polarity','spring4_Polarity','summer4_Polarity','fall4_Polarity', $                   'winter5_Polarity','spring5_Polarity','summer5_Polarity','fall5_Polarity',]]
to_be_predicted_Day4 = 48.31143151 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
xgb = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, $        gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=5, $        min_child_weight=3, missing=None, n_estimators=100, nthread=-1, $        objective='binary:logistic', reg_alpha=0, reg_lambda=1, $        scale_pos_weight=1, seed=0, silent=True, subsample=1) 
sc.uiWebUrl
monthly_index.tail()
df_new[['CA', 'UK']] = df_new[['CA', 'UK']].mul(df_new['new_page'], axis=0)
THRESHOLD = 0.70 # 70% positives and higher, only $ DURATION = 60*60*24*7*3 # 3 weeks
results_sim_rootDistExp, output_sim_rootDistExp = S.execute(run_suffix="sim_rootDistExp", run_option = 'local')
week46 = week45.rename(columns={322:'322'}) $ stocks = stocks.rename(columns={'Week 45':'Week 46','315':'322'}) $ week46 = pd.merge(stocks,week46,on=['322','Tickers']) $ week46.drop_duplicates(subset='Link',inplace=True)
df.info()
df2[df2.duplicated('user_id',keep=False)] $
toRG = rGraphData.groupby(['to', 'from']) $ toRG.sum().head()
windfield = gdal.Open(input_folder+windfield_name, gdal.GA_ReadOnly) $ windfield
df.describe()
top_supporters["contributor_cleanname"]=top_supporters.apply(combine_names, axis=1)
len(df.query('converted==1'))/len(df.index)
cvec =CountVectorizer(max_features = 2000)
pd.crosstab(index = sample["polarity"], columns = "count").plot.barh(rot=0)
import pandas_datareader.data as web $ h = web.get_quote_google(['AMZN', 'GOOG']) $ h
pivoted_data.resample("M").sum().plot(figsize=(10,10))
tweet_scores_clean[tweet_scores_clean.retweet_count=='failed']
xgb_learner.num_rounds
witf = open("latlong_test3.txt","w", encoding="utf-8") $ for i in range(len(test_kyo3)): $     witf.write("{location: new google.maps.LatLng(%f, %f)},\n" % (test_kyo3['ex_lat'][i],test_kyo3['ex_long'][i])) $ witf.close()
senti = df.groupby('subreddit')['SA'].mean()
not_lineup = len(df[((df['group'] == 'treatment') == (df['landing_page'] != 'new_page')) ]) $ print('The new_page and treatment dont line up {} times'.format(not_lineup))
print(np.min(rhum), np.mean(rhum), np.max(rhum))
df2_dup['group'].iloc[0],df2_dup['landing_page'].iloc[0],df2_dup['converted'].iloc[0]
pred.head(10) $
names = [n.replace('open.', '') for n in price_mat.columns]
p_converted_user2 = df2.query('converted==1').user_id.nunique()/df2.user_id.nunique() $ p_converted_user2
bsf = before_sherpa["Date"].value_counts() $ bsf.to_csv("GG.csv") $ cols = ['Day', 'Count'] $ bsfr = pd.read_csv("GG.csv", header=None, names=cols) $
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','US','UK','ab_page']]) $ results=logit_mod.fit() $ results.summary()
import os $ import matplotlib.pyplot as plt
df = pd.concat(frames, axis=1) $
plt.title('GC_MARK_MS_parent') $ aggregated_parent.plot(kind='bar', figsize=(15, 7))
df[df.pre_clean_len > 140].head(10)
filter_df['start_time'].min(), filter_df['start_time'].max()
model.add(Dropout(0.25))
autos[autos.price=='$0']
df2.query('landing_page == "new_page"').user_id.nunique() / df2.user_id.nunique()
newdf['date'] = newdf.index
df['body'][0]
%time nb.fit(train_4, y_train)
product = df[df.asin=='B00L86ZKAK'].copy() $ product=product.sort_values('reviewTime') $ product.head()
cvecdata =cvec.fit_transform(X_train)
squares.values
df.info()
weather.info()
%%bash $ ls taxi_trained/export/exporter/
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group']).drop(columns=['control']) $ df2.head()
X = pj.convert_to_df(scaling=False, filtered=True)
data
tar_counts = cig_data['tar'].value_counts()
null_vals=np.random.normal(0, p_diffs.std(), p_diffs.size)
df.life_sq.isnull().sum()
day_of_month15.to_excel(writer, index=True, sheet_name="2015")
lon_us = lon[lon_li:lon_ui]-360 $ lat_us = lat[lat_li:lat_ui] $ print(np.min(lon_us), np.max(lon_us), np.min(lat_us), np.max(lat_us))
X.isnull().sum()
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df3.set_index('user_id'), how='inner') $ df_new.head(10)
! gcloud ml-engine versions list --model $MODEL_NAME
df.groupby('episode_id')['raw_character_text'].nunique().agg(['min', 'mean', 'max'])
model.summary()
%matplotlib notebook
issues_df = DataFrame(list_of_issues_dict_data)
results2.summary()
df2.query("group=='treatment'").converted.mean()
c=pd.Series([630,25,26,255]) $ print(c)
pd.Series(['a', 'b', 'c']).dtypes
all_sites_with_unique_id_nums_and_names.loc[indices_duplicated_sites[:5],:]
sb.pairplot(df_new[['converted','group']])
train_session_v2 = pd.merge(user_actions,train_session, on='user_id', how='inner')
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline $
jobs.loc[(jobs.FAIRSHARE == 132) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0) & (jobs.Group == 'h_vuiis')][['Memory','Wait']].groupby(['Memory']).agg(['mean', 'median','count']).reset_index()
print(len([f for f in frequency if frequency[f]>1]))
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt
display(df_btc.tail())
df2['converted'].mean()
df_tweets["oembed"] = oembeds $ df_tweets.sample()
twitter_archive_master.to_csv('./WeRateDogs_data/twitter_archive_master.csv')
inactive_creation_ratio = [x/len(clean_users[clean_users['active']==0]) for x in list(clean_users[clean_users['active']==0]['creation_source'].value_counts())] $ inactive_creation_ratio
input_data.head(10)
print("null size",sum(np.sum(np.array(tokendata.isnull()),axis=1)))
import numpy as np
sns.set(style="darkgrid") $ f, ax = plt.subplots(figsize=(10, 10)) $ sns.set_color_codes("pastel") $ sns.barplot(x="Average_Num_Comments", y="Subreddit", data=subred_num_avg[:10], color="b")
locations
autos.head()
activeWikis = dfClean[(dfClean['stats.activeUsers']>=1)&(dfClean['users_1']>0)] $ inactiveWikis = dfClean[(dfClean['stats.activeUsers']<1)|(dfClean['users_1']==0)]
salary_df2.columns = salary_df2.columns.str.lower() $ salary_df2.head(10)
retention
sqlContext.sql("select id, borrower from world_bank limit 2").toPandas()
table_names = ['train', 'store', 'store_states', 'state_names', 'googletrend', 'weather', 'test'] $ table_list = [pd.read_csv(f'{fname}.csv', low_memory=False) for fname in table_names] $ for table in table_list: display(table.head())
gs = GridSearchCV(estimator=Ridge(),param_grid=params,cv=5,scoring='neg_mean_squared_error')
X.fillna(0, inplace=True)
from pandas import * $ ghana = read_csv('Ghana_2014.csv', skipinitialspace=True)
all_data_merge = all_data_merge[clm] $ all_data_merge.rename(columns={'store_x':'store','Sub_store':'substore','Brand':'brand', 'url_x':'url'}, inplace=True) $ all_data_merge.columns
accuracy = metrics.r2_score(y_test, predictions) $ print("Cross-Predicted Accuracy:", accuracy)
df['three'] = df['one'] * df['two'] $ del df['two'] $ df.insert(1, 'bar', df['one'][:2]) $ df
gm_df.head()
c = pd.concat(days_since_the_last_transactions, ignore_index = True) $ c.info()
episcopalian = df_final[df_final['tweet_id']==666287406224695296].index $ df_final.drop(episcopalian, inplace=True) $ df_final.reset_index(drop=True,inplace=True)
autos['odometer_km'].describe()
!cat barclay_banks.txt
gas_df = gas_df.rename(columns={'Unnamed: 0':'MONTH'}) $ gas_df.columns
path = os.path.join('Data for plots', 'Quarterly generation.csv') $ eia_gen_quarterly.to_csv(path, index=False)
params = {'figure.figsize': [8, 8],'axes.grid.axis': 'both', 'axes.grid': True,'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_decomposition(RN_PA_duration, params=params, freq=31, title='RN/PA Decomposition')
df.loc[:,"message"] = df['message'].str.findall('\w{3,}').str.join(' ') $ df.head()
engine =create_engine('sqlite:///:memory:', echo=True)
dft.head()
df.user_id.unique().shape[0]
pr("Droping rows with NA values (location and creation date).") $ tw1 = twh.dropna(axis=0, how='any', subset=['createdAt']) $ tw1.dropna(subset=['longitude'], inplace=True) $ tw1.dropna(subset=['latitude'], inplace=True) $ pr('The data have been reduced from {} tweets to {} tweets.'.format(strNb(len(twh)), strNb(len(tw1))))
df.apply(lambda x: x.mean() - x.max())
p_conv_control = df2.query('group == "control"')['converted'].mean() $ p_conv_control
holidays_events.dtypes
def get_correct_result_type(row): $     if row['Date'] == "2017-01-01": $         return row['Result Types for Jan  1, 2017'] $     else: $         return row['Result Types for Sep  1, 2017']
df_symbols.head()
graffiti['created_year'] = graffiti['created_year'].replace(2015, 1) $ graffiti['created_year'] = graffiti['created_year'].replace(2014, 1) $ graffiti['created_year'] = graffiti['created_year'].replace(2013, 1) $ graffiti['graffiti_count'] = graffiti['created_year']
alpha = 0.0005 $ loss = tf.add(margin_loss, alpha * reconstruction_loss, name="loss")
conn = sqlite3.connect(os.path.join(outputs,'example.db')) $ df = pd.read_sql_query("SELECT * from stocks ORDER BY price", conn) $ print(df.head()) $ conn.close()
chefdf = pd.merge(chefdf, chef11df,  how='left', left_on=['name', 'user'], $                   right_on = ['name', 'user'], suffixes=('','_11'))
obs_diff = p_new - p_old
plt.style.use('seaborn-darkgrid') $ bb.plot(y='close')
df = df[(df.price < 150000) & (df.price > 100) ]
print("cost is: {}" .format(cost), "revenue is: {}" .format(revenue), sep=", ")
target0.head()
import datetime $ options_data['TTM'] = (pd.to_datetime(options_data['MATURITY']) - pd.to_datetime(options_data['DATE']))/datetime.timedelta(days=365)
Geocoder.geocode(festivals["Location"][9]).valid_address
df_group = df.groupby('group') $ df_group.describe()
df_ll.head(2) $ df_ll.isDuplicated.value_counts() $ df_ll.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
df['logViewsPercentChange'].hist()
X_train.shape, y_train.shape
df = pd.read_pickle("all_news.pkl") $ len(df)
logout_page = BeautifulSoup(logged_out.content) $ bool(logout_page.find_all(string=re.compile('All cookies cleared!')))
incidents_yes_no = [] $ for i in range(len(incidents_time_index)): $     teststamp = incidents_time_index[i] $     incidents_yes_no.append(len(incidents[(incidents['delay_start'] <= teststamp) & (incidents['delay_end'] >= teststamp)]))
df3 = df2.assign(intercept=np.ones(len(df2), dtype=int)) $ df3['ab_page'] = pd.get_dummies(df['group'])['treatment'] $ df3.head()
f.dimensions
props.prop_name.value_counts()
archive_clean.timestamp = pd.to_datetime(archive_clean.timestamp)
usd_gbp_rate = pd.read_csv('usd_gbp_rate.csv', header=0) $ usd_gbp_rate['Date'] = pd.to_datetime(usd_gbp_rate['Date'], format="%d/%m/%Y") $ usd_gbp_rate['Year'] = usd_gbp_rate['Date'].dt.year $ usd_gbp_rate['Month'] = usd_gbp_rate['Date'].dt.month $ usd_gbp_rate['Day'] = usd_gbp_rate['Date'].dt.day
df_new = df_new.join(pd.get_dummies(df_new['country']))
word = 'convolution' $ fig, ax = w2v.create_3d_tsne_plot(word, number_closest_words=25)
from scipy import stats
weather = weather.merge(locs[['latitude','Location']]) $ weather.head()
s = requests.session() $ response = s.get('http://prisontalk.com') $ print response.status_code, response.url $
first_datetime = df1['DATETIME'].min() $ df_day1 = df1[(df1['DATETIME'] > first_datetime)&(df1['DATETIME'] <= first_datetime + timedelta(days=1))] $ df_day1.head(5)
comments
from sklearn.svm import LinearSVC
autos['registration_year'].describe()
weather_df = clean_weather("../clean_data/weather_NY.csv")
import warnings $ warnings.filterwarnings('ignore') $ import pandas_datareader.data as web $ amzn = web.get_quote_yahoo('AMZN') $ amzn
from biopandas.mol2 import PandasMol2 $ pmol = PandasMol2() $ pmol.read_mol2('./data/1b5e_1.mol2') $ pmol.df.tail(10)
autos['price'].describe(percentiles=[.10, .90])
calc_tempsLast(startdate,enddate)
print "Total number of rows of table 'photoz_bcnz': ", len(df3) $ print df3.columns.values $ df3
df_train['totals.newVisits_bool'] = [ bool(c==1) for c in df_train['totals.newVisits']] $ df_test['totals.newVisits_bool'] = [ bool(c==1) for c in df_test['totals.newVisits']]
indeed.columns
station_count = session.query(Measurement.station).distinct().count() $ print('There are',station_count,'stations')
re.findall('title="(.*?)"', slices[7])[0]
yc_new2['Tip_Amt'] = yc_new['Tip_Amt'] / yc_new['Fare_Amt'] * 100 $ yc_new2.head()
print("MSE:", metrics.mean_squared_error(stock.target, xgb.predict(stock.drop(['target', 'close', 'open'], 1))))
['state'] = df_ad_airings_5['location'].map(lambda x: x.split(","))
donald_tweets = get_all_tweets( 'realDonaldTrump' )
df_new[(df_new['group'] == 'treatment')]['converted'].mean()
df2 = df2.drop(1899, axis = 0) $
import finance as fat
sns.distplot(error_ph.query("provider == 'HotelBedsDirect'")['smooth_error'], hist=False, rug=True) $ sns.distplot(error_ph.query("provider == 'HotelBedsB2C'")['smooth_error'], hist=False, rug=True) $ sns.distplot(error_ph.query("provider == 'GTA'")['smooth_error'], hist=False, rug=True) $ sns.distplot(error_ph.query("provider == 'DOTW'")['smooth_error'], hist=False, rug=True)
def strip_singlequote_and_dateformat2datetime(text): $     try: $         return  pd.to_datetime(text.strip('\''),format='%d-%m-%Y %H:%M') $     except AttributeError: $         return text 
tt_json.describe()
print(df.shape)
launch_df = pd.DataFrame(launch_events, columns=launch_columns) $ launch_df.head()
station_sum = session.query(Measurement.station).distinct().count() $ print (station_sum)
df_users_products_Qty = df_users_products.merge(transactions, how='left', \ $                         on=['UserID','ProductID']).groupby(['UserID', 'ProductID'] \ $                         ).apply(lambda x: pd.Series(dict(Quantity=x.Quantity.sum()))) $ df_users_products_Qty.reset_index().fillna(0)
df_columns.index.month.value_counts().sort_index().plot() $
ax = sns.countplot(x="num_comments", data=reddit)
data.columns
autos["brand"].value_counts()
ct_df.head()
etsamples_100hz.iloc[1000]
for element in x: $     print element['id'] $     print(element['full_text']) $     print('--')
state_party_df.shape
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score
import json $ with open('tweet_json.txt', 'w') as file:  $     json.dump(tweet_data, file, $               sort_keys = True, $               ensure_ascii = False)    
import pandas as pd
y = df['target'] $ X = df['subreddits'] $ cvec = CountVectorizer(stop_words = 'english') $ X  = pd.DataFrame(cvec.fit_transform(X).todense(), $              columns=cvec.get_feature_names())
dataset_filtered = dataset.drop(dataset.index[indices]).copy() $ dataset_filtered = dataset_filtered.reset_index() $ print("New Total reviews: %s"%(len(dataset_filtered)))
df.columns.values # underlying values are numpy.ndarray
drace_df = drace_df.merge(temp.reset_index(),how='left', left_on='manager_id', right_on='manager_id') $ new_manager_ixes = drace_df['high_frac'].isnull() $ drace_df.loc[new_manager_ixes,['high_frac','low_frac', 'medium_frac','manager_skill']] = mean_values.values $ drace_df.head()
y_train = train.click $ train = train.drop(columns=['click', 'id']) $
den.plot(x='date', y='temp_c')
rows - df.shape[0]
from dateutil import parser $ columns = ['timestamp', 'name', 'location'] $ formatted_retweets = [] $ for rt in retweets: $     formatted_retweets.append([parser.parse(rt['created_at']), rt['user']['name'], rt['user']['location']]) $
constructor.sort_values(by='price',ascending=True)
n_rows = df.shape[0] $ print ("Number of rows in the dataset: {}".format(n_rows))
active_with_return.dropna(inplace = True)
raw_scores = df.rating_score[df.rating_score.notnull()]
from sklearn.ensemble import RandomForestClassifier
from nltk.corpus import stopwords
learner.fit(3e-3, 1, wds=1e-6, cycle_len=10)
time_range = pd.DatetimeIndex( $     start=modifications_per_authors_over_time.index.min(), $     end=modifications_per_authors_over_time.index.max(), $     freq=TIME_FREQUENCY) $ time_range
X2 = PCA(2, svd_solver='full').fit_transform(X) $ X2.shape
df2_copy['intercept'] = 1 $ df2_copy[['landing_page_new', 'landing_page_old']] = pd.get_dummies(df2_copy['landing_page']) $ df2_copy[['ab_page_control','ab_page_treatment']] = pd.get_dummies(df2_copy['group'])
tweet_archive_clean['dog_type'] = tweet_archive_clean['text'].str.extract('(doggo|floofer|pupper|puppo)', expand=True)
np.exp(-1.7227), np.exp(-1.3968), np.exp(-1.4422) $
df.plot() $ plt.show()
stars = dataset.groupby('rating').mean() $ stars.corr()
prcp_df.plot() $ plt.show()
oppose.amount.sum()
new_df.shape
list(tweet_archive_clean.columns.values)
actual_diff = conversion_rate_treatment - conversion_rate_control $ actual_diff
autos['registration_year'].min()
data = fat.add_ema_columns(data, 'Close', [3,6,12,26,50]) $ data.tail()
result['Rev/sales']= result['Price']/5
temp_cat.categories = ['sejuk','sederhana','panas'] $ temp_cat   # original category object categories is changed
ranking = pd.read_csv('datasets/fifa_rankings.csv') # Obtained from https://us.soccerway.com/teams/rankings/fifa/?ICID=TN_03_05_01 $ fixtures = pd.read_csv('datasets/fixtures.csv') # Obtained from https://fixturedownload.com/results/fifa-world-cup-2018 $ pred_set = [] $
import pickle $ with open("processed_data.pkl", 'rb') as f: $     tweets, bible_data = pickle.load(f)
mismatch = df
DataSet.head(10) $ DataSet.tail(5)
df_students.head()
threeoneone_geo = threeoneone_geo.to_crs({'init': 'epsg:4326'}) $ census_withdata = census_withdata.to_crs({'init': 'epsg:4326'})
def create_soup(x): $     return ''.join(x['categoryname']) + ', ' + ''.join(x['eventname']) + ', ' + ''.join(x['location'])
df['retweets'].describe()
with pd.option_context("display.max_rows",10): $    print(pd.get_option("display.max_rows")) $    print(pd.get_option("display.max_rows"))
doneMult = slicer.pipe(multi,2) $ doneMult
df2['intercept'] = 1 $ df2[['ab_page','ab_page2']] = pd.get_dummies(df2['landing_page']) $
crime_data = crime_data[crime_data["DATE_TIME"].dt.year == 2017] $ crime_data.shape
typesub2017 = type2017_2[['MTU','Solar  - Actual Aggregated [MW]', 'Wind Offshore  - Actual Aggregated [MW]', 'Wind Onshore  - Actual Aggregated [MW]']]
train.isnull().sum()[train.isnull().sum()>0]
df_new = df2[df2['landing_page'] == 'new_page'] $ n_new = df_new.user_id.count() $ n_new
test_float['cochera'] = 0 $ test_float.loc[test_float.description.str.contains('garaje|cochera|garage|garagge|estacionamiento', na=False), 'cochera'] = 1 $ test_float.cochera.value_counts()
df.index = range(len(df)) $ timedifference = [] $ for i in range(len(df) - 1): # range is df - 1 because the last tweet can't add 1 $     timedifference.append(df.created_at[i] - df.created_at[i+1])
pumashp.head()
appt_mat.shape
df2.query('user_id == 773192')
newdf.head()
import pandas as pd
raw_train.keys()
print(list(tweets_df['location']))
print 'A data structure containing month-level frequencies for the year 2013' $ mp2013 = pd.period_range('1/1/2013', '12/31/2013', freq='M') $ mp2013
print(df2.loc['2016-06-19':'2016-10-20'])
vectorizer = TfidfVectorizer(max_df=1.0, min_df=0.0025, sublinear_tf=True, $                              ngram_range=(1,3), stop_words=stoplist) $ X = vectorizer.fit_transform(train) $ X.shape
p_new = df2['converted'].mean() $ p_old = p_new $ print('p_new:', p_new, 'p_old:', p_old)
!curl -X POST -F image=@$(pwd)/images/dog.jpg 'http://localhost:8080/predict'
pd.read_sql(sql, engine).head()
df_clean.loc[mask, 'name'] = 'None'
import requests $ import quandl $ quandl.ApiConfig.api_key = 'AnxQsp4CdfgzKqwfNbg8'
df_new['country'].unique()
varianceMonthlyReturns
df.text[df.text.str.isupper()].unique()
df.select('CRIME_CATEGORY_DESCRIPTION').distinct().show(truncate=False)
set(train_data.notRepairedDamage)
titanic_clean = preprocessor.basic_cleaning()
pprint.pprint(test_collection.find_one())
_ = us[us['cityOrState'].isin(states) == False]['cityOrState'].value_counts(dropna=False).index.tolist()[1:40] $ _
engine = create_engine('postgres://%s@localhost/%s'%(username,dbname)) $ print engine.url
print(date(2015, 4, 5), 'and', time(4, 2, 1))
np.random.seed(42)
classified = pd.read_csv("compared_sentiments_classified.csv", index_col=0) $ classified.head()
for topic in range(lsi.num_topics): $     best_words = lsi.show_topic(topic, 7) $     inwords = [(dictionary[int(x[0])], x[1]) for x in best_words] $     wordstring = ', '.join('{}: {:0.2f}'.format(*x) for x in inwords) $     print('Topic {}: {}'.format(topic, wordstring))
contract = import_contract_history('../data/contract_history.csv')
additional_limit_outstanding_user=calc_additional_limit(merkmale)
cryptos[ $     (cryptos.percent_change_7d > 25) & $     (cryptos.market_cap_usd > 200000000) $ ]
df.query('converted == 1').user_id.nunique( ) / df.user_id.nunique()
import time $ import datetime
df = pd.DataFrame() $ for (name, sex) in a_list: $     get_all_tweets(name, sex) $     df = df.append(pd.read_csv('%s_tweets.csv' % name))
json = {'a': 1, 'b': 2, 'c': [{'x': [{'d1': 3.0}, {'d2': 3.1}, {'d3': 3.2}, {'d4': 3.3}]}, {'e': 4}, {'f': 5}, {'g': 6}, {'h': 7}]} $ print_json(json, max_array_components=3)
df.lead_mgr.unique()[:5]
df.mean(axis=1, skipna=False)
DT_clf = DecisionTreeClassifier(min_samples_split=20, random_state=88, criterion='gini', class_weight = 'balanced') $ RF_clf = RandomForestClassifier(n_estimators = 200, random_state = 88, criterion='gini', class_weight = 'balanced', oob_score=True)
params =  {'reg_alpha': 0.1, 'colsample_bytree': 0.9, 'learning_rate': 0.1, 'min_child_weight': 1 $           , 'n_estimators': 300, 'reg_lambda': 1.0, 'random_state': 1, 'max_depth': 4} $ reg_final = xgb.XGBRegressor(**params).fit(X, y)#**params $
autos = autos[autos["registration_year"].between(1913,2018)] $ autos['registration_year'].describe()
oppose.sort_values("amount" , ascending=False).head(2)
type(df_vow['Date'].loc[0])
fundret_annualized.to_csv('output/fundret_annualized.csv')
all_39s_from_2011 = Grouping_Year_DRG_discharges_payments.loc[(slice(2011), slice(39)),:] $ all_39s_from_2011.loc[:,'discharges'][:5]
df.product_type.value_counts(normalize=True)
mlp_df = pd.read_csv(mlp_fp, index_col='Date', parse_dates=True) $ mlp_df.head()
condition = (us['country'] != 'USA') & (us['country'].str.len() != 2) & (us['country'].str.contains(r'[A-Z]{2}')) $ us.loc[condition, ['country']] = us.loc[condition, 'country'].str.extract(r'([A-Z]{2})') $ us['country'].value_counts(dropna=False)
dtrain = xgb.DMatrix(X_resampled, label=y_resampled, feature_names=feature_names) $ dval = xgb.DMatrix(X_val, label=y_val, feature_names=feature_names) $ dtest = xgb.DMatrix(test_features, label=test_target, feature_names=feature_names)
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4) $ print ('Train set:', X_train.shape,  y_train.shape) $ print ('Test set:', X_test.shape,  y_test.shape)
signal_lookback = 60 * 24 * 60 # days * hours * minutes $ no_of_std = 2 $
outcome.info()
df_ta_caps_user.to_csv('BaseTravailPSE')
ab_data['intercept'] = 1 $ ab_data[['control', 'treatment']] = pd.get_dummies(ab_data['group'])
sampling_rules = { $     "w_ConstructionType == 'ConstructionType_appt'": 10, $     "e_sqm < 70": 10, $     "w_Income > 13650": 10, $ }
wine_reviews = pd.read_csv("D:\kagglelearn\kaggledatasets\winemag-data-130k-v2.csv", index_col=0) $ wine_reviews.head()
with open('data/PN_and_QTY.csv', 'wb') as f: $     w = csv.DictWriter(f, ['PN', 'QTY']) $     w.writeheader() $     w.writerows([{'PN': key, 'QTY': val} $                  for key, val in pn_and_qty_no_duplicates.items()])
ts.shift(3)
conn = engine.connect() $ measure_df = pd.read_sql("SELECT * FROM Measurement", conn)
maint['datetime'] = pd.to_datetime(maint['datetime'], format="%Y-%m-%d %H:%M:%S") $ maint.count()
del df['usd_pledged'] $ del df['ID'] $ del df['goal'] $ del df['pledged'] $ del df['currency']
PMCIDs_in_demographics = [d['PutRequest']['Item']['pmcid']['S']  for d in output_dict['demographics']] $ jsondata = json.dumps(pmcid_in_demographics) $ f = open("PMCIDs_in_demographics.json","w") $ f.write(jsondata) $ f.close()
tabs = Tabs(tabs=[Panel(child=layout, title=layout_name) for layout_name, layout in table_layouts.items()])
xml_in_merged = pd.merge(xml_in_sample, grouped_authors_by_publication, on=['publicationKey'], how='left')
n_old = df2.query('landing_page == "old_page"').shape[0] $ n_old
df[df['converted'] == 1].count()[1]/df.shape[0]
cohorts.reset_index(inplace=True) $ cohorts.set_index(['CohortGroup', 'CohortPeriod'], inplace=True) $ cohort_group_size = cohorts['TotalUsers'].groupby(level=0).first() $ cohort_group_size.drop("", inplace=True, axis=0) $ cohort_group_size
from IPython.display import display $ pd.options.display.max_columns = None
corpus = st.CorpusFromParsedDocuments(df_trump_device_non_retweets, $                                       category_col='before_or_after_election', $                                       parsed_col='parsed').build()
StockData.set_index(['Date'], inplace=True)
df_dates_new = df_dates_final[df_dates_final[1]>'2015-01-01'] # keeps 5254 records
negGroups2 = list(neg_tweets.group_id_x) $ num_convos = len(set(negGroups)) $ print(f'Working with {num_convos} conversations') $ companyNeg2 = filtered[filtered.group_id.isin(negGroups2)]
Maindf = Maindf.sort_index()
df = pd.DataFrame(data, columns=["phenomenonTime", "name", "result"]) $ df.index = pd.to_datetime(df["phenomenonTime"]) $ df = df.drop("phenomenonTime", axis=1) $ print(df.shape) $ df.head()
evaluator.plot_confusion_matrix(normalize=True, $                                 title='Confusion matrix, with normalization', $                                 print_confusion_matrix=False, $                                 figsize=(8,8), $                                 colors=None)
activeDF.take(10)
temp_cat.cat.categories
df.user_id.nunique()
no_lined_up = df.query('(group == "treatment" and landing_page != "new_page") or (group == "control" and landing_page != "old_page")') $ no_lined_up.shape[0]
shelter_df_only_category = shelter_df_idx $ shelter_df_only_category = shelter_df_only_category.drop('DateTime', 'OutcomeType_idx', 'AnimalType_idx', 'SexuponOutcome_idx', 'Breed_idx', 'Color_idx','AgeuponOutcome_binned', 'Ageuponoutcome') $ shelter_cleaned_df = shelter_df_only_category.toPandas() $ shelter_cleaned_df
n_old = len(df2.query("group == 'control'")) $ n_old
b_list.shape
pca = PCA() $ pca.fit(x_train_scaled)
np.shape(glons)
final_rf_predictions
week30 = week29.rename(columns={210:'210'}) $ stocks = stocks.rename(columns={'Week 29':'Week 30','203':'210'}) $ week30 = pd.merge(stocks,week30,on=['210','Tickers']) $ week30.drop_duplicates(subset='Link',inplace=True)
contribs = pd.read_csv("http://www.firstpythonnotebook.org/_static/contributions.csv") $ contribs.info()
new_dems.head()
submission_full.describe(percentiles=[.1, .2,.3, .4,.5])
df2.any()
df = df[(df.age >= 18) & (df.age <= 105)] $ df.head(2)
%%bash $ cd /data/LNG/Hirotaka/ASYN $ awk '$6 < 5e-8 {print}' PPMI1_all > sign.txt
preds_test.head()
df.to_csv('comma_delim_clean.csv', sep=',')
doglist
ts3.index = pd.to_datetime(ts3.index) $ ts3
df_twitter_copy['jpg_url'] = df_twitter_copy['tweet_id'].map(image_predictions_copy.set_index('tweet_id')['jpg_url']) $ df_twitter_copy['p1'] = df_twitter_copy['tweet_id'].map(image_predictions_copy.set_index('tweet_id')['p1']) $ df_twitter_copy['p1_conf'] = df_twitter_copy['tweet_id'].map(image_predictions_copy.set_index('tweet_id')['p1_conf'])
airbnb_df['day_of_week'] = airbnb_df['date_listed'].dt.dayofweek  # Number, Monday == 0 $ airbnb_df.loc[0:5, ['date_listed', 'listed_year_month', 'day_of_week']]
%timeit MyList3 = qsort(MyList) $ MyList3 = qsort(MyList) $ MyList3[:5]
print("Training took {:.2f}s".format(t1 - t0))
sentence1 = ['first', 'sentence'] $ sentence2 = ['second', 'sentence'] $ sentences = [sentence1, sentence2]
bnbAx.country_destination.value_counts() $
tweet1.id
tt_final.groupby(['all_p'])['favorite_count'].mean().sort_values(ascending=False).head(10) $
toma = pd.concat([df1.iloc[:,0], $                   df8.iloc[:,0], $                   df32.iloc[:,0]], axis=1) $ toma.columns = ['Nodes 1', 'Nodes 8', 'Nodes 32'] $ toma.head()
m.load('val0')
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline $ from sklearn.neighbors import KNeighborsRegressor
bininfo = json.loads(bres.read())
graph_stats = [] $ for split in date_splits[1:]: $     partial_df = mentions_df[mentions_df["date"] < split] $     graph_stats += [au.get_graph_stats(partial_df)] $ graph_stats_df = pd.DataFrame(graph_stats, columns=["nodes","edges","weak_components","strong_components"])
scores.iloc[0]
autos['price'].value_counts().sort_index(ascending=False).head(20) $
plt.title('Gucci ngram', fontsize=18) $ gucci_ng.plot(kind='barh', figsize=(20,16)); $ plt.savefig('../visuals/gucci_ngram.jpg')
archive_copy['source'] = archive_copy['source'].str.replace('<a href="http://twitter.com/download/iphone" rel="nofollow">Twitter for iPhone</a>', 'Twitter for iPhone') $ archive_copy['source'] = archive_copy['source'].str.replace('<a href="http://twitter.com" rel="nofollow">Twitter Web Client</a>', 'Twitter Web Client') $ archive_copy['source'] = archive_copy['source'].str.replace('<a href="https://about.twitter.com/products/tweetdeck" rel="nofollow">TweetDeck</a>', 'TweetDeck')
text_area = driver.find_element_by_id('textarea') $ text_area.send_keys("print('Robots controlling robots.')")
campaign_cost = Cost.iloc[:, 0:3].groupby(['id_partner', 'campaign'], as_index=False).sum() $ display(campaign_cost.sort_values('Costs', ascending=False))
appleinbounds = appleNeg[appleNeg.inbound == True]
pd.DataFrame ([[101,'Alice',40000,2017], $                [102,'Bob',  24000, 2017], $                [103,'Charles',31000,2017]], columns = ['empID','name','salary','year'])
plt.rcParams['axes.unicode_minus'] = False $ dta_51.plot(figsize=(15,5)) $ plt.show()
conn.fetch(table=dict(name='banklist', caslib='casuser'), $            sastypes=False, to=3)
conn = sqlite3.connect("D:\kagglelearn\kaggledatasets\FPA_FOD_20170508.sqlite")
senti = ...
df_members['gender'] = df_members['gender'].fillna(0) #Filling the null values in the column
df = pd.read_csv('data/goog.csv', parse_dates=['Date'], index_col='Date') $ df
text_classifier.set_step_params_by_name("text_char_ngrams", ngram_range =(3,4)) $ text_classifier.get_step_params_by_name("text_char_ngrams")
autodf = autodf.drop('abtest',1)
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_axial_chord_saddle[1][0])) $ perr = np.sqrt(np.diag(pcov_axial_chord_saddle[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
result = pd.DataFrame(alg7.predict_proba(test[features]), index=test.index, columns=alg7.classes_) $ result.insert(0, 'ID', mid) $ result.head()
countries_df = pd.read_csv('countries.csv') $ countries_df.head()
df.query('converted == 1')['user_id'].nunique()/df['user_id'].nunique()
comvandal = dv1 $ comvandal = comvandal.append([dv2,dv3,dv4,dv5,dv6,dv7,dv8,dv9,dv10,dv11,dv12,dv13,dv14,dv15,dv16,dv17,dv18,dv19]) $ comvandal=pd.DataFrame(comvandal) $ comvandal.head()
for t in tables: display(t.name, t.head(), DataFrameSummary(t).summary())
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
obj.values  # array representation of the Series
df.groupby(['Agency Name'])['Complaint Type'].value_counts()
fake_today = parse("2017-08-14") $ dt = relativedelta(years=1) $ year_ago = fake_today - dt $ year_ago_str = year_ago.strftime("%Y-%m-%d") $ print(year_ago_str)
test_data_scores(16, "Mod_9+players", $                  model=reg_logit, model_specs = model_9_players, $                  x=x_train_advanced, y=y_train_advanced, $                  x_=x_test_advanced, y_=y_test_advanced)
df_from_json = pd.read_json("msft.json") $ df_from_json.head(5)
positive_examples.to_csv('training_data/utah_positive_examples.csv')
df_video_meta = pd.DataFrame(video_meta) $ df_video_meta.head(2)
time.gmtime().tm_year
get_real_types(ip)
for col in ['date_crawled', 'ad_created', 'last_seen']: $     autos[col] = autos[col].str[:10] $     autos[col] = autos[col].str.replace('-','').astype(int) $ autos.head()
p_old = (df2['converted']==1).mean() $ p_old
df_apps = pd.read_csv('apps.csv', index_col=None)
plot_data(1796)  # the last one -- not given
def getStockPrice(tickr, startYear, startMonth, startDay, endYear, endMonth, endDay): $     start = datetime.datetime(startYear, startMonth, startDay) $     end = datetime.datetime(endYear, endMonth, endDay) $     readData = dr.DataReader(tickr, "yahoo",start,end) $     return pd.DataFrame(readData) $
plt.scatter(x=score_pair["first"].values, y=score_pair["last"].values) $ plt.plot(np.arange(50, 100), np.arange(50, 100))
vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', lowercase=False, $                              tokenizer=lambda text: text) $ spmat = vectorizer.fit_transform(x_tokens) $
new_system = am.load('poscar', fcc_poscar) $ print(new_system)
df.sample(5)
from sklearn.grid_search import GridSearchCV
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 1) & $        (finals["reb_l"] == 1) & (finals["stl_l"] == 0), 'type'] = 'inside_gamers'
def our_function(input): $     return np.sqrt(input)
!cat "json_example.json"
app_pivot['Percent with Application'] = app_pivot['Application'] / app_pivot['Total'] $ app_pivot
df_new = df_new[['startDate', 'endDate','barrelID','trapType','lure','uvled','funnel','substrate','males','females','notes']] $ df_new.sort(['endDate','barrelID'],inplace=True) $ df_new.reset_index(drop=True, inplace=True) $ df_new
users.merge(sessions, left_on=['Registered','UserID'], right_on=['SessionDate','UserID'], how='inner')
type(airline_tw_collec)
lr = LogisticRegression()
cats_merge.shape
df_country.info()
invalid_names=['a','the','an','very','quite','his','life','infuriating', $                'all','unacceptable','my','old','officially','this']
model.most_similar('man')
df_clean['in_reply_to_status_id'].notnull().sum()
cb.organization('matter-io').description
df2[df2['group']=='treatment'].converted.mean()
AFX_url_str = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?' $ AFX_date_str = 'start_date=2017-01-01&end_date=2017-12-31' $ AFX_APIkey_str = '&api_key='+API_KEY $ AFX_X_2017_r = requests.get(AFX_url_str+AFX_date_str+AFX_APIkey_str) $
clf = OneVsRestClassifier(estimator=linear_model.SGDClassifier(loss='hinge', penalty='l2', $                                                          alpha=0.001, fit_intercept=True, n_iter=10, $                                                          shuffle=True, verbose=1, epsilon=0.1, n_jobs=-1, $                                                          random_state=SVM_SEED, learning_rate='optimal', eta0=0.0, $                                                          class_weight=None, warm_start=False), n_jobs=1) $
cluster = Cluster(['localhost']) $ session = cluster.connect('lz') $ rows = session.execute('select * from lz.tweets;') $ df = pd.DataFrame(list(rows))
import pandas as pd $ %matplotlib inline $ df = pd.io.json.read_json("./data.json") $ df.shape
rob = df.loc['lossyrob'].resample('D').size() # we can drop the users level with access by .loc and a username $
treat_old = len(df.query("group == 'treatment' & landing_page == 'old_page'")) $ control_new = len(df.query("group == 'control' & landing_page == 'new_page'")) $ print('The number of times the new_page and treatment don"t line up is {}'.format(treat_old + control_new))
plt.figure(figsize=(15,5)) $ sns.countplot(auto_new.CarYear)
dummies = pd.get_dummies(plate_appearances['batter']).rename(columns=lambda x: 'batterid_' + str(x)) $ plate_appearances = pd.concat([plate_appearances, dummies], axis=1) $ dummies = pd.get_dummies(plate_appearances['pitcher']).rename(columns=lambda x: 'pitcherid_' + str(x)) $ plate_appearances = pd.concat([plate_appearances, dummies], axis=1)
y = df['comments'] $ X = df['title']
BPL_electric = pd.rolling_mean(dataBPL[['Total_Demand_KW']], 3).resample('D').mean() $ CH_electric = pd.rolling_mean(dataCH[['Total_Demand_KW']], 3).resample('D').mean() $ BPL_electric.plot(figsize=(15,3)) $ CH_electric.plot(figsize=(15,3),color='g') $
ip.img_num.describe()
data.head()
students
n_old = df2[df2['landing_page'] == 'old_page']['user_id'].count() $ print('n_old: ', n_old)
treino = pd.read_csv("./data/treino.csv", encoding="utf-8", sep='\t') $ treino = treino.replace(np.NAN,"") $ treino.head()
hot_df.head()
def is_date(x): return np.issubdtype(x.dtype, np.datetime64)
joined=join_df(joined,googletrend,["State","Year","Week"]) $ joined_test=join_df(joined_test,googletrend,["State","Year","Week"]) $ sum(joined['trend'].isnull()),sum(joined_test['trend'].isnull())
results = results.fillna(0)
traces = df['Trace'].values $ for e in traces: $     print(e)
(token <- readRDS("data_sci_8001_token.rds"))
from imblearn.over_sampling import SMOTE
df.query('group == "treatment" and landing_page == "old_page"').count()
weekday = 2 $ transit_df_byday = transit_df_merged.loc[transit_df_merged.index.weekday==weekday] $ transit_df_byday.info() $ transit_df_byday.head()
tipsDF.hist(figsize=(10, 10)) $ plt.savefig('text_preparation/likes_target_feature.png')
p_diff = new_page_converted.mean() - old_page_converted.mean() $ p_diff
tweets.sort_values(by="frequency", ascending=True).head()
df = df[df["category"]=="Tech"].reset_index(drop=True) $ df.drop(["category"], axis = 1, inplace = True)
tlen_n.plot(figsize=(16,4), color='r');
os.chdir("..")
twitter_ar.info()
p_value
clean_fb_tokens
full['LOS'].plot(kind='box',figsize=(12,4),vert=False) $ plt.xlabel('days') $ plt.title('Distribution of length of stay',size=15)
df_link_meta = pd.DataFrame(unshortened_link_meta) $ df_link_meta.to_csv('data/resolved_links.csv', index=False)
hourly_df['Open_Price_Change'].value_counts()
calls_df.pivot_table(["length_in_sec"],["user"],aggfunc="mean").sort_values("length_in_sec",ascending=False)
df2_dup = df2[df2.duplicated(['user_id'], keep = False)] $ df2_dup
player_id = baseball.player + baseball.year.astype(str) $ baseball_newind = baseball.copy() $ baseball_newind.index = player_id $ baseball_newind.head()
rounds['announced_on'] = pd.to_datetime(rounds['announced_on'], errors = 'coerce')
import dask.dataframe as dd $ from time import sleep $ dask_df = dd.from_pandas(data=df, chunksize=10000) $ dask_df = dask_df.repartition(npartitions=8) $ dask_df.head()
heights_A=pd.Series([176.2, 158.4, 167.6, 156.2,161.4],index=['s1','s2','s3','s4','s5'])
tweet_json_clean.head()
percent_unique_convert
t.microsecond
len(class_data)
autos.describe()
train_data.isnull().sum()
!apt-get install -y -qq software-properties-common python-software-properties module-init-tools $ !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null $ !apt-get update -qq 2>&1 > /dev/null $ !apt-get -y install -qq google-drive-ocamlfuse fuse
data['age']=(data['Week Ending Date']-data['temp']).dt.days $ data.head(10) $
df_comment['likes_count']=df_comment.liked_by.apply(lambda x:len(x))
subs = subs[subs.veredict == 'AC'] $ subs.describe()
telecom3.shape
for i in range(len(unique)): $     print unique[i], counts[i]
y_clf = np.where(y_tr >= 1800, 1, 0) $ print(np.where(y_clf==0)[0].shape) $ print(np.where(y_clf==1)[0].shape)
Temperature_year = session.query(Measurements.date, Measurements.tobs) \ $     .filter(Measurements.date >= '2016-05-01').filter(Measurements.date <= '2017-06-10') \ $     .filter(Measurements.station == 'USC00519281').all() $ Temperature_year
html = browser.html $ soup = BeautifulSoup(html, 'html.parser')
url_BAL = "https://ravens.seasonticketrights.com/Images/Teams/BaltimoreRavens/SalesData/Baltimore-Ravens-Sales-Data-PSLs.xls"
df2.drop(2893, inplace = True)
type(ds[4]) $ ds[4].head()
ns_raw =  root.tag.split('{')[1].split('}')[0] $ ns = '{'+ ns_raw + '}' $ namespaces['af'] = ns_raw $ namespaces
st_bw.close()
series['a' : 'f'] # slicing works as well
cabin = df_titanic['cabin'] $ print(cabin.describe()) $
df2.drop([1899], inplace= True) $ df2.info()
sns.regplot('hours_left', 'prediction_min', event, color='red') $ sns.tsplot(event['real_min'], color='black')
for doc in corpus_lsi: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly $     print(doc)
plt.hist(p_diffs)
scenario = model.model2db() $ scenario.solve(model='MESSAGE', case='MESSAGE_GHD')
lr_best = gs.best_estimator_
df.show()
misaligned_row_count = ab_df[((ab_df['group'] == 'treatment') & (ab_df['landing_page'] == 'old_page')) | ((ab_df['group'] == 'control') & (ab_df['landing_page'] == 'new_page'))]['user_id'].count() $
obj.rank(ascending=False, method='max')
print("Feature types", tipsDF.dtypes)
p_new = df2[df2['landing_page']=='new_page']['converted'].mean() $ print("Probability of conversion for new page (p_new) is", p_new)
oil_interpolation=oil_interpolation.interpolate() $ pd.DataFrame.head(oil_interpolation)
master_df.name.value_counts().nlargest(10)
trends_per_year_avg_ord = OrderedDict(sorted(trends_per_year_avg.items()))
print metrics.confusion_matrix(y_test, predicted) $ print metrics.classification_report(y_test, predicted)
DataSet_sorted[['tweetText', 'prediction']].tail(10)
df_image_clean.duplicated('tweet_id').value_counts()
df_twitter_archive.source.value_counts()
df[['Open', 'Close']].plot(figsize=(15,5));
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new.head()
dicttagger_sentiment = DictionaryTagger(['positive.yml','negative.yml'])
dupli
data2=pd.read_csv('data/intraday_60min_CTL (1).csv') $ data2['timestamp']=pd.to_datetime(data2['timestamp'])
aru_df.head()
df2[df2['group'] == 'control']['converted'].mean()
run txt2pdf.py -o '2018-06-22 2013 FLORIDA HOSPITAL Sorted by discharges.pdf'  '2018-06-22 2013 FLORIDA HOSPITAL Sorted by discharges.txt'
joined = joined[joined.Sales!=0]
fundmean = fundret.mean()*12 $ fundvol = fundret.std()*np.sqrt(12) $ fundmean / fundvol
faa_data_pandas.shape
grouped['score'].mean().sort_values(ascending=False)
merged = pd.merge(zipShp, dfList, left_index=True, right_index=True, on='ZIPCODE', how = 'inner')
tweet = 'I know a very good Sino - Portuguese restaurant that makes food from Macau' $ print("{} :  {}".format(color_formatting(ner.is_tweet_about_country(tweet, 'PT')), tweet)) $ print("\t\t| \n\t\t|-> {}".format(ner.get_countries_from_content(tweet)) )
top_ten_teams=teams.groupby('Team').agg({'win_numb':np.sum,'Pts':np.sum}).nlargest(10,'win_numb') $ sns.regplot(x='Pts',y='win_numb',data=top_ten_teams);
!ls -lh losses.csv
twitter_final.head(2)
w.get_step_object(step = 3, subset = subset_uuid).calculate_indicator_status(subset_unique_id = subset_uuid, indicator_list = ['din_winter'])
plt.plot(ds_cnsm['time'],ds_cnsm['met_salsurf'],'r.') $ plt.title('CP01CNSM, Sea Surface Salinity') $ plt.ylabel('Salinity') $ plt.xlabel('Time') $ plt.show() $
tweet_archive_enhanced_clean.head()
emails = users.email.apply(email_clean).value_counts() $ good_emails = set(emails[emails > 2].index) $ good_emails
from PutModelData import get_game_results, get_boxscores, get_gamesheets, get_season_game_stats $ GameResults = get_game_results() $ GameResults.head()
dframe_team = pd.io.html.read_html(url) $ dframe_team = dframe_team[0] $ dframe_team.head()
dd2=cfs.diff_abundance('Subject','Control','Patient', alpha=0.25)
len(stockdf)
counts_df_clean = counts_df.copy()
red_4.to_csv(filename)
(autos['odometer_km'] $  .value_counts() $  .head() $  .sort_index(ascending=False))
plt.plot(x, x ** 2) $ plt.plot(x, -1 * (x ** 2))
grid.best_params_
options_data.info()
y.mean()
import requests $ import json $ from json import loads $
df.head()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ df = pd.read_table(path, sep =' ') $ df.head(5)
goog.head()
df_birth['Continent'].value_counts(dropna=False)
blahblah = 'This string is stored in the variable on the left.' $ blahblah
sns.regplot(x=dfz.rating_numeric, y=dfz.favorite_count)
from xgboost import XGBRegressor
fig, ax = plt.subplots(figsize=(5, 5)) $ plt.boxplot(df.pre_clean_len) $ plt.show()
plot_aop_refl(ndvi,sercRefl_md['spatial extent'], $               colorlimit = (np.min(ndvi),np.max(ndvi)), $               title='SERC Subset NDVI \n (VIS = Band 58, NIR = Band 90)', $               cmap_title='NDVI', $               colormap='seismic')
df.to_json("json_data_format_values.json", orient="values") $ !cat json_data_format_values.json
lda_corpus = model[corpus] $ results = [] $ for i in lda_corpus: $     results.append(i) $ results[:1]
df_ad_state_metro_1['sponsors'].value_counts()
twitter_final1 = twitter_final[(twitter_final.dogType != 'None') & (twitter_final.rating_num>7)].groupby(['dogType','rating_num'])['tweet_id'].size().unstack().plot(kind='bar', stacked=True)
conv_prob_control = df2[df2.group == 'control'].converted.mean() $ print('Given that an individual was in the control group, the probability they converted is {}.'.format(conv_prob_control))
airbnb_df['room_type'] = airbnb_df['room_type'].cat.set_categories(['Shared room', 'Private room', 'Entire home/apt'], $                                                                    ordered=True)
autos['odometer_km'].value_counts()
crimes_by_yr_month.reset_index(inplace=True) $ crimes_by_yr_month.columns = ["year","month","crime_count"]
autos = pd.read_csv("autos.csv", encoding="Latin-1") $ autos.head()
d={'c1':['A','B','C','D'],'c2':np.random.randint(0,4,4)} $ pd.DataFrame(d)
cust_demo.columns
df.head()
users.toPandas().head()
with open('nmf_mod_5_29.pkl', 'wb') as piccle2: $     pickle.dump(nmf_new, piccle2)
print(df2.loc[2862]);
def getResults(JsonReponse): $     return JsonReponse.get('ResultSet').get('Result')
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country']) $ df_new.reset_index(inplace=True) $ df_new.head()
def get_sec(time_str): $     h, m, s = time_str.split(':') $     return int(h) * 3600 + int(m) * 60 + int(s) $ def dur2sec(row): $     return get_sec(row['Duration']) $
if not os.path.exists('new_data_files'): $     os.mkdir('new_data_files') $ records.to_csv('new_data_files/Q2.csv')
k_fold = KFold(n_splits=10, shuffle=True, random_state=0) $ score = cross_val_score(classifier, xy, z, cv=k_fold, n_jobs=1, scoring='accuracy') $ print (score) $ round(np.mean(score)*100, 2)
by_area['AQI Category'].value_counts()
g = g[g['created_year']!=2016]
from lifetimes.plotting import plot_probability_alive_matrix $ plot_probability_alive_matrix(bgf)
transactions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/transactions.csv') $ transactions.head()
popStationData = session.query(Measurement.date, Measurement.tobs).filter_by(station="USC00519281").filter(Measurement.date > pastYear).all() $ popStationData
staff['Fulltime'] = True $ staff.head()
np.count_nonzero(np.any(na_df.isnull(), axis=0)) # total number of columns with missing values 
result[result['timestamp']> 4e+12]
import pandas as pd $ import numpy as np $ from dateutil import parser $ from datetime import datetime $ from datetime import timedelta
df2[df2['user_id'].duplicated()==True]
df.drop(df[((df.state == '') & (df.city != ''))].index, inplace=True)
sns.set(color_codes=True) $ sns.distplot(utility_patents_subset_df.number_of_claims, bins=40, kde=False) $ plt.show()
results3.summary()
engine.execute('SELECT * FROM measurement LIMIT 5').fetchall()
powerConsumptionsPerDay = powerConsumptions.groupby([pd.Grouper(freq='D', level=0), 'meter_id']).sum() $ powerConsumptionsPerDay
df_dummies = pd.get_dummies(df_onc_no_metac[ls_other_columns]) $ dummy_colnames = df_dummies.columns $ dummy_colnames = [clean_string(colname) for colname in dummy_colnames] $ df_dummies.columns = dummy_colnames
df2.drop(df2.index[2862], inplace = True);
data[['value']]
relevant_data['Event Type Name'].value_counts().plot(kind='barh')
(autos['date_crawled'] $         .str[:10] $         .value_counts(normalize = True, dropna = False) $         .sort_index() $         )
autos = pd.read_csv('autos.csv', encoding='Latin-1') $ autos.info() $ autos.head()
doctype_by_day = doctype_grouped.unstack('document_type', fill_value=0)
gr2_success = pd.merge(user_group2,success_order, how='inner', left_on=['CUSTOMER_ID'], right_on =['CUSTOMER_ID']) $ len(gr2_success)
print('Out of {} organisations, {} were matched as companies.'.format( $         len(df_rand), $         len(df_rand[df_rand.is_ch_company == True])))
print(autos['price'].describe()) $ autos['price'].value_counts().head()
df1=pd.DataFrame.from_records(raw,index='id_loan_request')
from sklearn.cluster import KMeans $ kmeans_model = KMeans(n_clusters=7) $ kmeans_model.fit(df)
a = news_df[news_df['Source Acc.'] == 'BBC'] $ a.head() $ print(a['Compound Score'].sum())
ca = ['California', 'Los Angeles', 'San Francisco', 'LA', 'SF', 'Southern California', 'SF Peninsula'] $ us.loc[us['country'].isin(ca)|us['cityOrState'].isin(ca), 'cityOrState'] = 'CA' $ us.loc[us['country'].isin(ca)|us['cityOrState'].isin(ca), 'country'] = 'USA' $ us['country'].value_counts(dropna=False)
buckets_to_df(contributors.fetch_aggregation_results()['aggregations']['0']['buckets'])
plot_results['f1_max_thresh_pred'] = np.where(plot_results.cv_probabilities>.10526, 1,0) $ print(metrics.classification_report(plot_results.status_binary, plot_results.f1_max_thresh_pred)) $ print(metrics.confusion_matrix(plot_results.status_binary, plot_results.f1_max_thresh_pred))
bikes.head()
valence_df.head()
len(df.select_dtypes(include=[np.number]).columns.tolist())
check_max_loan(pipeline, X_valid[30:60], y_valid[30:60])
def collect_sents(matcher, doc, i, matches): $     match_id, start, end = matches[i]  # indices of matched term $     span = doc[start:end]              # extract matched term $     print('span: {} | start_ind:{:5} | end_ind:{:5} | id:{}'.format( $         span, start, end, match_id))
plt.style.available
U_B_df.cameras[0]
logit = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','UK','US']]) $ result=logit.fit()
closePrice.nlargest()[0]
df2[df2['group'] != "treatment"].converted.mean()
exploration_titanic = DataExploration(titanic)
@app.route("/api/v1.0/stations") $ def stations(): $     stations = list(df.measurement.unique()) $     return jsonify(stations)
text = "The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ?" $ text = re.sub(r'[^a-zA-Z0-9]', ' ', text.lower()) $ words = text.split() $ print(words)
df = pd.DataFrame(rng.rand(1000, 3), columns=['A', 'B', 'C']) $ df.head() $
nnew=df2.query('landing_page=="new_page"')['converted'].shape $ print(nnew[0])
%%bash $ grep -A 20 "INPUT_COLUMNS =" taxifare/trainer/model.py
import matplotlib.pyplot as plt $ import matplotlib.dates as mdates $ from matplotlib.ticker import ScalarFormatter, FormatStrFormatter $ monthly = df.set_index('Created') $ monthly.info()
applications['application_created_at'] = pd.to_datetime(applications.application_created_at) $ applications['submitted_at'] = pd.to_datetime(applications.submitted_at) $ applications['submitted'] = np.where(applications['submitted_at'].isnull(), 0, 1) $ applications.groupby('submitted').agg({'applicant_id': lambda x: len(set(x))}).reset_index()
weather.head()
new = ins2016.groupby("business_id").count() $ new["inspections"] = new["score"].values $ numIns2numIDs = new.groupby("inspections").count()["score"].to_dict() $ numIns2numIDs
X_clf = training_data.values
lr = 4e-4 $ learn.fit(lr, 10, cycle_len=1, use_clr=(10,10))
f_u18 = pop_df['under18'] / pop_df['total'] $ f_u18.unstack()
SANDAG_age_filepath = 'Datasets/SANDAG Forecasts/Age.csv' $ SANDAG_ethnicity_filepath = 'Datasets/SANDAG Forecasts/Ethnicity.csv' $ SANDAG_housing_filepath = 'Datasets/SANDAG Forecasts/Housing.csv' $ SANDAG_population_filepath = 'Datasets/SANDAG Forecasts/Population.csv' $ SANDAG_jobs_filepath = 'Datasets/SANDAG Forecasts/Civilian Jobs.csv'
df.fillna(0)
P.plot_1d('scalarCanopyTranspiration')
engine.execute("SELECT * FROM stations").fetchall()
xmlData.drop('address', axis = 1, inplace = True)
production_df = pd.merge(future_predictions, features, on=['Date', 'HomeTeam', 'AwayTeam', 'season'])
details.dtypes
bb[['low','high']].plot()
metrics.roc_auc_score(actual_value_second_measure.values, predicted_probs_first_measure)
siteFeatures = read.getSamplingFeatures(type='Site') $ df = pd.DataFrame.from_records([vars(sf) for sf in siteFeatures if sf.Latitude])
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
f_close_clicks_os_train = spark.read.csv(os.path.join(mungepath, "f_close_clicks_os_train"), header=True) $ f_close_clicks_os_test = spark.read.csv(os.path.join(mungepath, "f_close_clicks_os_test"), header=True) $ print('Found %d observations in train.' %f_close_clicks_os_train.count()) $ print('Found %d observations in test.' %f_close_clicks_os_test.count())
import statsmodels.api as sm
df_raw[df_raw.song_title.isnull()] # look at rows without title
val_size = 0.05 $ state = 20 $ experiment_X, validation_X, experiment_y, validation_y = train_test_split(X, y, test_size=val_size, random_state = state)
pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True) $ tweets = api.search(q='Deloitte', rpp=1) $ pp.pprint([att for att in dir(tweets) if '__' not in att])
topics_data = pd.DataFrame(topics_dict)
e['lastAttacked']
merge.head(1)
del dfRegMet['tweet']
Quantile_95_disc_times_pay = df.groupby(['drg3','year']).agg([np.sum, np.mean, np.std]) $ Quantile_95_disc_times_pay.head(8) $
twitter_ar.shape
liberiaDeaths = liberiaFullDf.loc['Newly reported deaths'] $ liberiaDeaths.head()
outlier_indices = detect_outliers(cats_df, 'hair length', 1.5) $ cats_df['remove'].iloc[outlier_indices] = True
counted_store_events = no_null_events.groupBy("store_id").count()
Z = np.arange(9).reshape(3,3) $ for index, value in np.ndenumerate(Z): $     print(index, value) $ for index in np.ndindex(Z.shape): $     print(index, Z[index])
autos["brand"].value_counts(normalize=True)
preds_train= aml.leader.predict(htrain) $ h2o_train=df_train[['air_store_id','visit_date']]
df3['intercept'] = 1
len(AAPL_pred)
S.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
pageviews.info()
pd.read_sql("select * from related_content order by content_id desc limit 10", conn)
data_scrapped['datePublished'] = data_scrapped['datePublished'].apply(lambda x:datetime.strptime(x,'%Y-%m-%d'))
systemuseData.groupby('Year')[['Total','Domestic','Industrial','Commercial']].sum().plot() $ plt.xlim(1980,2020) $ plt.ylabel('Use (ac-ft)')
y_hat = model.predict(X)
autos[date_cols].head(3)
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, func $ from sqlalchemy import desc
extract_all['app_id_short'] = [np.nan if str(x)=='nan' else str(x)[0:16] for x in extract_all['system.record_id'].values]
contractor_clean.loc[contractor_clean['contractor_id'] == 139,'city'] = 'Fargo' $ contractor_clean.loc[contractor_clean['contractor_id'] == 139,'address1'] = '900 42nd Street S' $ contractor_clean.loc[contractor_clean['contractor_id'] == 139,'address2'] = 'P.O. Box 6740'
cnx = sqlite3.connect('database.sqlite') $ df = pd.read_sql_query("SELECT * FROM Player_Attributes", cnx) $ type(df)
autos['registration_year'].max()
stocks_pca_m2[:]
top_supporters = merged.groupby(["contributor_firstname", "contributor_lastname"]).amount.sum().reset_index().sort_values("amount", ascending=False).head(11)
with open("IMDB_dftouse_dict.json", "r") as fd: $     IMDB_dftouse_dict = json.load(fd)
tvec = TfidfVectorizer(stop_words='english') $ nb = MultinomialNB() $ nb_pipe = Pipeline([('tvec', tvec), ('nb', nb)])
df['created_at'].head()
data.dropna(how='all')
(data.isnull().sum()/len(data)).sort_values() #percentage of missing values $
json_df = get_json_df()
party_type_crosstab = by_party_type.set_index( $     ["election_year", "party_type"] $ ).unstack(1).reset_index().fillna(0)
all_tables_df.OBJECT_TYPE.count()
np.all(southern_sea_level.year == northern_sea_level.year)
len(bb_df)
y_train = train['any_spot'].values $ X_train = train.drop(['Temperature_Departure','Temperature_Avg','year','parkings_by_hour','index','date','any_spot','Real.Spots','datetime','month','DOW','Street','From','To','Block','Street.Length'], axis=1) $ new_val = val.dropna() $ y_valid = new_val['any_spot'].values $ X_valid = new_val.drop(['Temperature_Departure','Temperature_Avg','year','parkings_by_hour','index','date','any_spot','Real.Spots','datetime','month','DOW','Street','From','To','Block','Street.Length'], axis=1)
domain = 'twitter.com'
ts_filter = ts_mean[ticker][ts_mean[euphoria].shift(1)<0.05]
list(pgn2value.keys())[:3]
rtitle = [x.text for x in soup.find_all('a', {'data-event-action':'title'})] $ rtitle.pop(0) $ subreddit = [x.text for x in soup.find_all('a', {'class':'subreddit hover may-blank'})] $ rtime = [x.text for x in soup.find_all('time', {'class':'live-timestamp'})] $ comments = [x.text for x in soup.find_all('a', {'class':'bylink comments may-blank'})]
import numpy as np $ disaster_tables = pd.read_html("https://en.wikipedia.org/wiki/List_of_accidents_and_disasters_by_death_toll", header = 0) $ explotions = disaster_tables[4] # reading from html
y_age = (pd.DataFrame(X_age_notnull['age'], columns=['age'])) $ X_age = deepcopy(X_age_notnull) $ X_age.drop('age', axis=1, inplace=True)
p_new = df2.converted.mean() $ p_new # displaying the convart rate for the treatment
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key="+API_KEY $ data = requests.get (url) $ data
    i ='MMM' $     stock_ts = pd.read_sql("select * from daily_price where ticker='%s' and instrument_type='stock'" % i, engine) $     stock_df = stock_ts[['price_date',  'adj_close_price']].copy().set_index('price_date') $     stock_df.rename(columns={'adj_close_price': i}, inplace=True)
grouper = visits.groupby((visits.address, visits.dba_name))
soup = bs(response.text, 'html.parser')
predictions = knn.predict(test[['property_type', 'lat', 'lon']])
brand_top_10 = autos['brand'].value_counts().sort_values(ascending=False) $ brand_top_10 = brand_top_10.index[0:10] $ brand_top_10
airports_df[airports_df['city'].str.lower() =='new york']
modern_combos = modern_pings.flatMap(get_problem_combos)
cats_df = pd.read_csv('./data/cats.csv')
agg_stats.tail(3)
df_from_json = pd.read_json("../../data/stocks.json") $ df_from_json.head(5)
pd.DataFrame(not_missing_values_pd['SexuponOutcome'].value_counts()).plot(kind='bar')
X = hstack([main_cat_id_sparse, duration_sparse, goal_sparse, blurbs_to_vect], format="csr")
df.columns
t1 = pd.Series(list('abc'), [pd.Timestamp('2016-09-01'), pd.Timestamp('2016-09-02'), pd.Timestamp('2016-09-03')]) $ t1
gbc = ensemble.GradientBoostingClassifier(n_estimators=300) $ gbc.fit(X, y) $ cm = confusion_matrix(y, gbc.predict(X)) $ sns.heatmap(cm, annot=True)
selected_train_df = train_df[['Q0_RELEVANT', 'Q1_mood_of_speaker']].reset_index(drop=True)
df1.rdiv(1)
import numpy as np $ np.random.seed(0) $ df = df.reindex(np.random.permutation(df.index))
gdp = web.DataReader('GDP', 'fred', $                     datetime.date(2012, 1, 1), $                     datetime.date(2014, 1, 27)) $ gdp
cfs.plot(sample_field='Subject',gui='jupyter')
for category in page.categories() : $     print (category.title())
df_resolved_links = pd.DataFrame(resolved_links) $ df_resolved_links.tail(3)
wed11.head()
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $
len(text2)
counter_trump = Counter() $ mrtrumpdf.tags.apply(lambda s: counter_trump.update(s))
tweets_df = pd.read_csv("tweets_sample.csv", index_col=False, compression='gzip', encoding='utf-8') $ tweets_df
ac['Filing Date'].groupby([ac['Filing Date'].dt.year]).agg('count')
df.head(1)
columns = inspector.get_columns('measurement') $ for column in columns: $     print(column["name"], column["type"])
example['hour'] = example.index.hour $ example.head(5)
df_c_merge['intercept'] = 1 $ lm = sm.Logit(df_c_merge['converted'], df_c_merge[['intercept', 'US', 'CA']]) $ results_countries = lm.fit() $ results_countries.summary()
%matplotlib inline $ import numpy as np $ import seaborn as sns $ import scipy.stats as stats
oil_interpolation.columns=['dcoilwtico'] $ pd.DataFrame.head(oil_interpolation)
most_active_df = history_df.loc[history_df['station'] == 'USC00519281'] $ max_val = most_active_df['tobs'].max() $ min_val = most_active_df['tobs'].min() $ avg_val = most_active_df['tobs'].mean() $ print(f'Max:{max_val} | Min:{min_val} | Average:{avg_val}')
n_old = (df2[df2['landing_page'] == 'old_page']).shape[0] $ n_old
df_ml_54 = df.copy() $ df_ml_54.index.rename('date', inplace=True) $ df_ml_54_01=df_ml_54.copy()
merged1 = merged1.set_index('AppointmentDate')
print(donations['Donation Included Optional Donation'].value_counts()) $ print('percentage') $ print(donations['Donation Included Optional Donation'].value_counts(normalize=True))
ts = df15.groupby('day_of_week').agg({'sale':['sum']}) $ ts.plot()
import matplotlib.pyplot as plt $ %matplotlib inline $ plt.hist(results["units"]["uqs"]) $ plt.xlabel("Video Fragment Quality Score") $ plt.ylabel("Video Fragment")
print(df_users['bio3'].value_counts())
users = pd.read_csv('./data_airbnb firstdestinations/train_users.csv')
air_store = join_df(air_store, air_rsrv_by_date, "air_store_id") $ air_store.head() $
tweet_archive_df[tweet_archive_df.rating_numerator > 20].sort_values(by=['rating_numerator'],  ascending=False)[['tweet_id','rating_numerator', 'rating_denominator', 'text']][0:10]
print("All Tweets: {0} | Users: {1}".format(len(matthew), matthew.user.nunique())) $ print("Tweets in the Matthew 160 Collection: ", len(matthew.query('matthew160'))) $ print("Users in the Matthew 160 Collection: ", matthew.query('matthew160').user.nunique())
gearbox_dict = {'manuell':'manual', $                 'automatik':'automatic'    $ } $ autos['gearbox'].replace(gearbox_dict,inplace=True) $ autos['gearbox'].fillna('unspecified',inplace=True)
the_posts['BfHpoUZnhTY']["the_fname"]
len(genre_vectors.columns)
autos["brand"].value_counts().head(20)
YS1517['Adj Close'].corr(YS1315['Adj Close'],method = 'pearson')
df = pd.read_sql('SELECT * FROM genre_revenue', con=conn) $ df
first_words = my_data.map(lambda line: line.split()[0])
state_party_df['National_R']['2016-08-01':'2016-08-07'].sum() / 7
fluxes = table['nuFnu1000_3000'].quantity $ print(fluxes) $ coord = SkyCoord(table['GLON'],table['GLAT'],frame='galactic') $ print(coord.fk5)
local
stock_df = stock_df.pct_change()[1:] $ stock_df.head()
tt = (ttTimeEntry $       .groupby(['C/A','UNIT','SCP','STATION','DT']) $       .ENTRIES.count() $       .reset_index()) $ tt.head()
df = pd.DataFrame({'foo': [1,2,3], 'bar':[0.4, -1.0, 4.5]}) $ df.values, df.values.dtype
import glob
def calculate_cumulative_returns(returns): $     return None $ project_tests.test_calculate_cumulative_returns(calculate_cumulative_returns)
dfWords = pd.read_pickle("dfWords.p")
list(set(df17).intersection(list(df16)))
contribs.head()
small_train.isnull().sum(axis=0) # show columns with counts of null values
w.get_available_indicators(subset= 'A', step=2)
df_train.describe()
df_pilots.head(3)
All_tweet_data_v2.name[(All_tweet_data_v2.name.str.contains('^[(a-z)]'))].value_counts()
outcomes = [0,1] ### 0 is not converted and 1 is converted $ probs = [1-c_rate_null,c_rate_null] ### probability of 0 is one minus conversion rate at null, and probability of 1 is c_rate_null $ new_page_converted = np.random.choice(outcomes, size= n_new,replace = True, p = probs) $ new_page_converted
cnn_g , cnn_op= cnn_graph() $ runtime(name = "2", op_list = cnn_op, datalist = [ctrain_x ,ctrain_y, cvalid_x, cvalid_y], g = cnn_g  ) $ cnn_g.get_operations()
merge_left = pd.merge(url_authors, url_votes) $ merge_left = pd.merge(merge_left, url_payouts) $ unique_urls = pd.merge(merge_left, url_reputation)
out = conn.loadtable('data/iris.csv', caslib='casuser', $                      casout=dict(name='mydata', caslib='casuser')) $ out
model = gensim.models.Word2Vec(sentences, size=200)
data = {'Integers' : [1,2,3], $         'Floats' : [4.5, 8.2, 9.6]} $ df = pd.DataFrame(data) $ df
def timestamps_between(start, end, size): $     ts_start = int((start - datetime.datetime(1970, 1, 1)).total_seconds()) $     duration = int((end - start).total_seconds()) $     return np.array(random.sample(range(int(duration)), size)) + ts_start
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page','US','UK','US_ab','UK_ab']]) $ results = logit_mod.fit() $ results.summary()
df_other_dummies = pd.get_dummies(df_with_metac_with_onc[ls_other_columns]) $ dummy_colnames = df_other_dummies.columns $ dummy_colnames = [clean_string(colname) for colname in dummy_colnames] $ df_other_dummies.columns = dummy_colnames
import pandas as pd $ pd.options.display.max_columns = 99
noaa_data.head()
news_sentiment_analysis.head() $
res=geo.geoLocate('Pokhara') $ assert res[0][3]=='NP' $ assert hitDetector.isInCountry((res[0][2],res[0][1])) $ assert hitDetector.getRegion((res[0][2],res[0][1]))==(5,'West')
countries_df = pd.read_csv('countries.csv') $ df_dummy = pd.get_dummies(data=countries_df, columns=['country']) $ df_new = df_dummy.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
date_agency = data.groupby(['yyyymm','Agency']) $ date_agency.size().unstack().plot(kind='bar',figsize=(15,15))
sum_row=df[["Dividend Yield","Closing Price"]].sum() $ sum_row
sn.barplot(x ='funding_rounds', y="count", data=round_count)
from sklearn.neighbors import KNeighborsClassifier $ knn = KNeighborsClassifier(n_neighbors=1)
null_valls = np.random.normal(0,p_diffs.std(),p_diffs.size) $ plt.hist(null_valls); $ plt.axvline(x=actual_obs_diff, color = 'g', linewidth = 1);
i = 0 $ sample_sent = valid_labels[i] $ print(' '.join(sent2tokens(sample_sent)), end='\n')
fin_df['totalAssets'].corr(fin_df['totalRevenue'])
(loan_requests_indebtedness.postcheck_data!='null').sum()
df.plot('DATE_TIME','ENTRIES', figsize=(15,7)).set(ylabel='Entries')
my_gempro = GEMPRO(gem_name=PROJECT, root_dir=ROOT_DIR, genes_list=LIST_OF_GENES, pdb_file_type='pdb')
video_meta = [] $ for chunk in chunks(video_id, n=40): $     vm_ = yt.get_video_metadata(chunk, key, P.parse_video_metadata) $     video_meta.extend(vm_) $ len(video_id)
df2 = df[((df.landing_page == 'new_page') & (df.group == 'treatment')) | ((df.landing_page == 'old_page') & (df.group == 'control'))] $ df2
df['overworked'] = df['emails'] >= 120 $ df
accuracy(y_test,rf_pred)
tweets.dtypes
merged['onpromotion'].value_counts(normalize=True,dropna=False)
states
print(store['prealn/alignment_bad'].shape[0]) $ remove_chunk(store, 'prealn/alignment_bad', problems.srr) $ print(store['prealn/alignment_bad'].shape[0])
local_1 = pd.read_csv('mar_quarterly_raw_data/local_Nestle.reviews.adhoc.F180301T180318.xlsx.csv', sep=',', encoding='utf-8') $ local_1.columns
free_data[free_data['educ']>5].groupby('age_cat').describe()
all_colnames = [clean_string(colname) for colname in df_total.columns] $ df_total.columns = all_colnames $ df_total.head()
dset = dset.sel(lon=slice(100, 300), lat=slice(50, -50))
grouped
values=['2020Q1', '2020Q2', '2020Q3', '2020Q4'] $ index=pd.PeriodIndex(values, freq='Q') $ index
df_ad_airings_4.head(3)
import numpy as np $ import pandas as pd $ import requests $ import json
p
data.resample('3T').mean().plot()
age.sort_index()
df_prep17 = df_prep(df17) $ df_prep17_ = pd.DataFrame({'date':df_prep17.index, 'values':df_prep17.values}, index=pd.to_datetime(df_prep17.index))
nold = (df2['landing_page'] == 'old_page').sum() $ print(nold)
for row in cursor.columns(table='TBL_FCInspevnt'): $     print(row.column_name)
converted_proportion = (ab_data["converted"]==1).mean() $ converted_proportion
doctopic = clf.fit_transform(dtm)
co2_concentration = pd.read_table("data/greenhouse_gaz/co2_mm_global.txt", sep="\s+", $                                   parse_dates = [[0, 1]]) $ co2_concentration
sm.qqplot(price_log, loc=price_log.mean(), scale=price_log.std())
df.set_index(['srx', 'srr'], inplace=True)
ids = df2["user_id"] $ df2[ids.isin(ids[ids.duplicated()])]
restaurants[["GRADE DATE","VIOLATION CODE", "DBA"]]
df.printSchema()
number_of_commits = len(git_log['timestamp']) $ number_of_authors = len(pd.unique(git_log['author'].dropna())) $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
ebay["2017-01"].plot()
p_old = df2[df2['converted']==1]['user_id'].count() / df2.shape[0] $ p_old
building_pa_prc_shrink.to_csv("buildding_01.csv",index=False) $
pi = 22/7 $ f = np.linspace(0.0, pi, num=100, endpoint = True)    #creating array of 100 equally spaced numbers between o and pi $ f = np.sin(f)      #printing sin of the values $ print("f: ", f)
token_sendavg = token_sendcnt.groupby("sender").agg({"sendcount":mean_except_outlier}).reset_index()
ins.head(3)
import pandas as pd $ import numpy as np $ from fbprophet import Prophet
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=850) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
df.columns
reconstruction_mask_reshaped = tf.reshape( $     reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1], $     name="reconstruction_mask_reshaped")
api = API(auth) $ user = api.get_user('NVPatriot1') $ print (user.screen_name) $ print (user.followers_count)
df.groupby("cancelled")["pickup_week"].mean()
1-0.190/2
dfRegMet2013 = dfRegMet[dfRegMet.index.year == 2013]
lm = sm.Logit(df2['converted'],df2[['intercept','treatment']]) $ result = lm.fit() $ result.summary()
pickup_demand.head()
dfRegMet.shape
births.index = pd.to_datetime(10000 * births.year + $                              100 * births.month + $                              births.day, format='%Y%m%d') $ births['dayofweek'] = births.index.dayofweek
import pandas as pd $ file = open('datasets/git_log_excerpt.csv','r') $ read_data = file.read() $ print(read_data)
df2.head(5)
c.find_one({'name.last': 'Bowie'})
p_new = df2.converted.mean() $ print("Convert rate for p_new:", p_new)
df.reset_index(drop=False, inplace=True) $ zipcodes.reset_index(drop=False, inplace=True)
sns.countplot(calls_df["dial_type"])
type(filter1)
n_old = df2.query('landing_page=="old_page"').count()[0] $ n_old $
train_col.train_model(num_epochs=2,optimizer=optimizer_col)
dictionary = corpora.Dictionary(texts) $ dictionary.save(os.path.join(TEMP_FOLDER, 'deerwester.dict'))  # store the dictionary, for future reference $ print(dictionary)
CumulativeProfit = [] $ Money = 1 $ for i in range(0,len(Profit)): $     Money = (Profit[i])/100*Money + Money $     CumulativeProfit.append(Money)
mars_fact_url = "https://space-facts.com/mars/" $ mars_facts = pd.read_html(mars_fact_url) $ mars_facts[0]
ax = commits_per_hour.plot.bar() $ ax.set_title("Commits per Hour") $ ax.set_xlabel("Hour of Day") $ ax.set_ylabel("Number of Commits")
grouped_authors_by_publication.rename(columns = {'authorName':'authorNames_in_given_publication'}, inplace = True) $ grouped_authors_by_publication.rename(columns = {'authorId':'authorIds_in_given_publication'}, inplace = True)
consumer_key, consumer_secret, access_token, access_token_secret = (open("../../credentials.txt") $                                                                     .read().split("\n"))
soup = BeautifulSoup(html_dict['html'], "html5lib") $ soup
unsorted_df.sort_index() #aort by index
sibsp = df_titanic['sibsp'] $ print(sibsp.describe()) $ sibsp.value_counts()
rng_dateutil = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz=tz_dateutil)
GPU_CORE = [0] $ BATCH_SIZE = 512 $ BEGINING_LR = 0.01 $ model_name = 'supervisord_model' $ data_dir = '../data/imsa-cbf/'
for v in contin_vars: $     joined[v] = joined[v].fillna(0).astype('float32') $
hashtags = df.text.str.extractall(r"#(\w+)") $ mask = hashtags.loc[:, 0].value_counts() > 4 $ hashtags.loc[:,0].value_counts()[mask]
df_cs['Sentiment_class'] = df_cs.apply(conditions, axis=1) $ df_cs.to_csv("costco_senti_score.csv", encoding='utf-8', index=False)
return_list
is_duplicate
newdf = pd.DataFrame(taxiData.Trip_distance)
a = object()
ctc = ctc.round(1)
X = data_final[['EditTime>15', 'stiki_percent', 'FirstMeta', 'reverted_mode']] $ y = data_final['type']
import glob $ import pandas as pd $ l = [pd.read_csv(filename) for filename in glob.glob("data/ebola/guinea_data/*.csv")] $ guinea_data = pd.concat(l, join='outer')
import matplotlib.pyplot as plt $ import seaborn as sns $ % matplotlib inline $ df_input_pd = df_input_clean.toPandas()
user_corrs = df.groupby('user_id')[['user_answer', 'question_answer']].corr() $ user_corrs = user_corrs.iloc[0::2, -1].reset_index(level=[1])['question_answer'] $
cust_demo.size
twitter_data_v2=twitter_data.copy() $ Imagenes_data_v2=Imagenes_data.copy() $ tweet_data_v2=tweet_data.copy()
df_countries = pd.read_csv('./countries.csv') $ df_countries = df_countries.set_index('user_id') $ df_countries.head()
grouped_dpt["Revenue"].filter(lambda x: len(x) < 5)
df_country_join = df_country.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ print (df_country_join.head())
sequences = pd.DataFrame(index=range(seqsize), columns=users)
df2[df2['landing_page'] == "new_page"]['converted'].sum()
df_vow.dtypes
def validation_score(score_series): $     return score_series.mean()
sns_plot = sns.lmplot(x='favorite_count',y='retweet_count',data=twitter_archive_clean,fit_reg=False,scatter_kws={'alpha':0.1}) $ sns_plot.savefig('favorite_vs_retweet.jpg')
df_enhanced.head(5)
model = sm.OLS.from_formula('arrests ~ OT', stadium_arr) $ results = model.fit() $ print(results.summary())
stations_info_dict = dict(zip(stations_info.name, stations_info.station)) $ stations_info_dict
print ("Propotion of users converted:",df.converted.mean())
max(open_list)
machin = open_day('2017-01-17')
df_protest.ward.head()
reddit['title'].value_counts()/reddit.shape[0]
X = pd.get_dummies(X, drop_first=True)
(np.array(p_diffs).mean()) - ab_data_diff
samples = 10000 $ K_star = np.random.binomial(n, k/n, size=samples) $ e = 0.0633 $ np.count_nonzero(np.logical_and(k/n >= K_star/n-e, k/n <= K_star/n+e)) / samples
print('Columns:\n\t'+'\n\t'.join(map(str,[col for col in full.columns])))
df_control_group = df2.query('group == "control"') $ conversion_rate_control = df_control_group.query('converted == 1').shape[0] / df_control_group.shape[0] $ print('The probability of an individual from the control group converting is {}'.format(conversion_rate_control))
df.loc[df.followers.idxmax()]
print "Last version of the production as from the first row: ", df2.id[0] $ df2
yc_merged = yc_trimmed.merge(yc_sd, left_on='Unnamed: 0', right_on='Unnamed: 0', how='inner')
output.coalesce(2).write.parquet("/home/ubuntu/parquet/output.parquet/output.parquet")
y_train_pred_mdl
stopword_list.extend(["dass", "wer", "wieso", "weshalb", "warum", "gerade"]) #Add the words ["dass"] to the list. $ print(len(stopword_list))
rdf.to_csv('training_data/road_features.csv') $ df.to_csv('training_data/collisions_new.csv')
Describe_Data(california)
top_songs['Position'].isnull().sum()
<br> $ **Getting a Database** $     - A single instance of MongoDB can support multiple independent databases. $     - When working with PyMongo you access databases using attribute style access on MongoClient instances. $     - Database name can not use attribute style access (ie. test-data), but "test_data" is okay
nocachedf = mrgdf[mrgdf.fname_ifc.isnull()] $ nocachedf
try: $
logit_stats = sm.Logit(df['converted'],df[['intercept','treatment']])
useful_indeed = indeed[~indeed['salary_clean'].isnull()] $ useful_indeed.shape $
autos.columns
class_codes.head()
from sqlalchemy.orm import Session $ session = Session(bind=engine)
for o in ['Before', 'After']: $     for p in columns: $         a = o + p $         df[a] = df[a].fillna(0).astype(int)
autos['ad_created'].str[:10].head()
geo_db.describe()
df_new[['UK', 'US']] = pd.get_dummies(df_new['country'])[['UK','US']] $ df_new.head()
from datetime import date $ df_new['last_active'] = df_new['current_date'] - df_new['updated_at'] $ df_new['last_active'].astype(str) $
%%time $ df_from_csv = pd.read_csv('losses.csv', index_col=['epoch', 'batch', 'datapoint'], float_precision='high')
data_final.shape
com_grp.groups  # return Dictionary
from collections import Counter $ for user1, count in Counter(mentions).most_common(10): $       print(user1 + "\t" + str(count))
print ("Probability that individual was in the control group,and they converted: %0.4f" % (df2.query('converted == 1 and group == "control"').shape[0]/df2.shape[0]))
v = db.versions(GENOTYPE_ARRAY)[:] $ v
airquality_pivot = airquality_melt.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading') $ print(airquality_pivot.head())
model.wv['tree']  # raw NumPy vector of a word
import requests $ import pandas as pd $ import numpy as np $ import json $ from pandas.io.json import json_normalize
len(tweet_list)
to_be_predicted_Day5 = 25.10814835 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
year15 = driver.find_elements_by_class_name('yr-button')[14] $ year15.click()
year_ago = str(year_before)+"-"+most_recent_list[1]+"-"+most_recent_list[2] $ year_ago#check
print(roc_auc_score(y_test, y_pred))
model.evaluate([X_dense_test, X_conv_test], y_test, batch_size=64)
print clintondf.datetime.max() $ print clintondf.datetime.min()
atdist_info_4x_tabledata = atdist_4x_count_prop_byloc.reset_index() $ create_study_table(atdist_info_4x_tabledata, 'locationType', 'emaResponse', $                    location_remapping, atdist_info_response_list)
WorldBankdf.take(2) $
class_merged.date=pd.to_datetime(class_merged.date) $ class_merged.dtypes
plate_appearances.loc[plate_appearances.batter_name=='ryan howard',].head()
sandag_df_w_pred = sandag_df.reset_index() $ sandag_df_w_pred['homevals_pred'] = pd.Series(y_2_pred) $ sandag_df_w_pred = (sandag_df_w_pred. $                     set_index(['year', 'tract']).sort_index()) $ sandag_df_w_pred.head()
df_breed.head()
prob_new_page = df2['landing_page'].value_counts()[0]/len(df2) $ print (prob_new_page)
comments.head()
crimes_by_type.reset_index(inplace=True)
pca.explained_variance_ratio_
m
df2.query('group =="control"').converted.mean()
df['neg'] = vader_df['neg'] $ df['pos'] = vader_df['pos']
small_big_coeffs = pd.concat([coeffs.sort_values().head(20), $                               coeffs.sort_values().tail(20)])
df_2014['bank_name'] = df_2014.bank_name.str.split(",").str[0] $
df_y_pred = (y_age_predict.as_data_frame()) $ df_y_actual = (y_age_valid.as_data_frame())
import pandas as pd $ import numpy as np $ import os $ import tweepy
pos_freq = {k: pos_tfidf.dfs.get(v) for v, k in pos_dic.id2token.items()} $ sorted(pos_freq.items(), key=lambda x: x[1], reverse=True)
df.iloc[2218]
df.iloc[0]
full_history = pd.DataFrame( $     modifications_per_authors_over_time.reindex(time_range).fillna(0).unstack().reset_index() $ ) $ full_history.head()
class_merged_state=class_merged_hol['state'].unique() $ print(class_merged_state)
nconvert = len(df2[(df2.converted==1)]) $ ntot = len(df2.index) $ prob = nconvert/ntot $ print(prob)
access_token = os.environ.get('access_token', 'Not Set') $ access_token_secret = os.environ.get('access_token_secret', 'Not Set') $ consumer_key = os.environ.get('consumer_key', 'Not Set') $ consumer_secret = os.environ.get('consumer_secret', 'Not Set')
%%time $ data_demo["num_child"] = data_demo["comment_id"].apply(lambda x: data_demo[data_demo["parent_id"]==x].shape[0])
df = pd.read_csv('./dataset/master_06-02-2018(hotposts).csv') $ df.drop(columns='Unnamed: 0', axis=1, inplace=True)
full_data.to_csv('examples/simple/data/varying_data.csv', index=False)
from datetime import datetime $ date = nc.num2date(time, 'hours since 1800-01-01 00:00:0.0') $ ts = pd.Series(date, index = date) $ print(ts.head(), ts.tail())
scoring[scoring.fk_user==137886]
!ls -l {PATH}
df.managers[:10]
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation $ p_diffs[:10]
duplicate_userid = df2.groupby('user_id')['timestamp'].count().sort_values(ascending=False).head(1).index[0] $ duplicate_userid
import datetime, random
corr_coins = ['BTC','ETH','BCH','ETC','XRP','NEO','LTC','IOTA','BTG'] $ mask = np.logical_or.reduce([(df_cryptdex.symbol == coin) for coin in corr_coins]) $ df_cryptdex = df_cryptdex[mask]
melted_total.groupby(['Categories','Neighbourhood']).mean().unstack()['Rating'].ix[top10_categories.index].plot.bar(legend=True,figsize=(10, 5))
movies = (spark.read.format("csv") $ .options(header = True, inferSchema = True) $ .load(home_dir + "movies.csv") $ .cache()) # Keep the dataframe in memory for faster processing 
data = data[['update','available','free','total','name','Long','Lat']]
final_idx = (test_portfolio['date']<=end_idx) & (test_portfolio['date']>=start_idx)
soup = BeautifulSoup(page.text) $ text= soup.get_text() $ print text
df2 = pd.read_csv('ab_data2.csv')
six = [] $ for cat in range(len(four)): $     four[cat]['age_cat']=cat $     six.append(four[cat]['age_cat'])
print(con_key)
start_date = '2017-01-01' $ end_date = '2017-01-10' $ calc_temp = session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\ $                             filter(Measurement.date >= start_date, Measurement.date <= end_date).all() $ calc_temp
pandas.read_csv('Movies_release_date.csv')
eia_extra.head()
shows['cancelled'].value_counts() $
plt.subplots(figsize=(6, 4)) $ sn.barplot(train_session_v2['isNDF'],train_session_v2['similar_listings'])
cvec = CountVectorizer(ngram_range=(1,2), stop_words='english', strip_accents='unicode') $ cvec
df.describe()
dfPriceCalculations = pd.DataFrame(columns=['Peak Price','Base Price']) $ dfPriceCalculations['Peak Price'] = peakPricePerDay $ dfPriceCalculations['Base Price'] = basePricePerDay $ dfPriceCalculations.head() # verify correct creation of consolidated data frame
print(df[0:3],'\n') $ print(df['20180102':'20180103'], '\n') $ x=np.array([0,1,2,3,4]) $ print('Numpy slice x[0:3]=', x[0:3])
np.sum(ser6)
df_2004['bank_name'] = df_2004.bank_name.str.split(",").str[0] $
autos['offer_type'].value_counts()
ab_dataframe.shape
actual_diff=p_t-p_c
unique2 = len(df2['user_id'].unique()) $ print('Number of unique id in df2 dataframe is:{}'.format(unique2))
a.string[a.start(): a.end()]
n_new=df2.query('landing_page == "new_page"').shape[0] $ n_new
injury_df.DL_length.value_counts()
df = pd.DataFrame(recentd, columns=['prcp']) $ df.head(20)
trigram_model = prep.get_trigram_model(from_scratch=False):
row_bk, col3 = bnb2.shape $ ATB = float(row_bk)/float(row) $ ATB
y=df['target'] $ X=df.scores
df_clusters=pd.concat([pd.Series(clusters),df,pd.Series(data)],axis=1)
len(df_repub_2016)
print(convo1.text)
df000001.tail()
from svpol import formats $ from svpol import parser $ from svpol import handler
plot_price(f,'Close') $ plt.legend("full range")
usersDf.hist(column=['followers_count'],bins=50) $
guinea_data3 = guinea_data1[['Description', 'Date', 'Totals']] $ guinea_data3.head()
sql = "SELECT * FROM paudm.cosmos order by paudm_id limit 5 " $ df3 = pd.read_sql(sql,engine)
df_ab_page.head()
applications = pd.read_csv('applications.csv') $ questions = pd.read_csv('questions.csv')
df["sentences"].head()
from sklearn.metrics.scorer import make_scorer $ def myError(y_orig,y_pred): $     error=mean_absolute_error(y_orig, y_pred)/(sum(y_orig)/len(y_orig)) $     return error $ my_scorer = make_scorer(myError, greater_is_better=False)
stock.shape
stemmer = porter.PorterStemmer() $ stemmerL = LancasterStemmer()
from githubid import githubid
df.loc[0:4:2] $
with open('tweet_json.txt', 'r') as content_file: $     content = content_file.read() $ records = json.loads(content) $ df_twitter = pd.DataFrame(records) $ df_twitter.head()
from bmtk.analyzer import nodes_table $ input_nodes_DF = nodes_table(nodes_file, 'FridayHarborBiophysics') $ input_nodes_DF[:5]
trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt) $ val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt) $ md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)
chefdf = pd.merge(chefdf, chef01df,  how='left', left_on=['name','user'], $                   right_on = ['name','user'], suffixes=('','_01'))
twitter_archive_master.describe()
clf = LinearRegression()  # create a classifire object $ clf.fit(xtrain,ytrain) # train data related with fir() method $ accuracy=clf.score(xtest,ytest) # test data related with score() method $ print "the accuracy is "+str(accuracy)
def calculate_dividend_weights(dividends): $     factors = np.cumsum(dividends, axis=0) $     dividend_weights = factors.div(factors.sum(axis=1), axis=0) $     return dividend_weights $ project_tests.test_calculate_dividend_weights(calculate_dividend_weights)
not_aligned = n_rows - (np_tr + old_con) $ not_aligned[0]
stringlike_instance.content = 'changed content'
df_treat= df2[df2['group'] == 'treatment']['converted'].mean() $ print("{} is the probability they converted.Thus, given that an individual was in the treatment group.".format(df_treat))
pystore.list_stores()
newdf["win"] = 1 * (newdf["home_score"] > newdf["away_score"]) + 0.5 * (newdf["home_score"] == newdf["away_score"]) $
tmp_cov_1 = tmp_cov.reset_index(level=0) $ del tmp_cov_1['price_date'] $ tmp_cov_1
df_all_loans = pd.concat(dfs_loan, keys=selected_reporting_dates)
response.json()
df2c = df.query('group == "control" and landing_page == "old_page"')
df.axes
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=["Tweets"]) $ display(data.tail(10))
spark.app.name  App Name $ spark.ui.port   4141 $ spark.master    spark://localhost:7077
prices = pd.read_csv("prices.csv" , parse_dates= ['date'])
metrics.accuracy_score(rf.predict(x_test), y_test)
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ act_diff $ p_diffs = np.array(p_diffs) $ p_diffs $ (act_diff < p_diffs).mean()
feature_cols = ['StoreNumber', 'BottlesSold', 'VolumeSoldGallons', 'SaleDollars_q1', 'Sold_div_Sales']
a = df_protest.loc[0:10, 'start_date'].weekday()
df.query('(group == "treatment") != (landing_page == "new_page")').user_id.count()
generate_chart(roc_curve, # we imported this method 2 cells above. $                "", # filename is blank. $                "injured", # the name of the report is "injured." $                [('a', our_nb_classifier), ]) # not sure what 'a' is for. $ plt.show()
df4[df4['converted']==1].count()[0]/df4.count()[0]
links = [] $ for item in soup.find_all('div',class_='excerpt'): $     for link in item.find_all('a'): $          links.append(link.attrs['href'])
n_new = df2_treatment.shape[0] $ n_new
soup=bs(site.text,"html")
zero_rev_acc_opps.columns
k1.head()
df = get_mta_data(4)  #load the last 3 weeks of MTA data starting from (2016, 9, 17) $ df.head(5)
URL = 'https://api.blockchain.info/charts/transactions-per-second?timespan=5weeks&rollingAverage=8hours&format=json'
import dill as dpickle
posts['ViewCount'].agg([np.mean, np.median])
logit_countries = sm.Logit(new_sf['converted'],new_sf[['country_UK', 'country_US', 'intercept']]) #to check the impact of categorical values on the convertion $ logit_countries = logit_countries.fit() $ logit_countries.summary()
secclintondf['tags'] = secclintondf.text.apply(lambda s: TextBlob(s).tags)
clf = svm.SVR() $
OldPage = df2.query('landing_page=="old_page"')['user_id'].count() $ OldPage
from sklearn.model_selection import cross_val_score $ from sklearn.ensemble import GradientBoostingClassifier $ forest_clf.score(X_test, Y_test) # test socre $
control_df = df2.query('group == "control"') $ (control_df['converted'] == 1).mean()
merged1 = pd.merge(left=merged1, right=meeting_status, how='left', left_on='MeetingStatusId', right_on='Id')
nonzero = grades.drop(grades.index[(grades.Mark == 0) | (grades.ATAR == 0)]) $ nonzero.plot(x='ATAR', y='Mark', color='red', kind='scatter')
df.shape
print 'However, passing a single value doesn\'t work because that\'s interpreted as a column' $ msft['2012-01-01']
line = next(file.generateParsedLines()) $ line
%matplotlib inline $ data4.plot()
alpha_range = 10.**np.arange(-2, 3) $ alpha_range
twitter_archive_enhanced_clean = twitter_archive_enhanced_clean[twitter_archive_enhanced_clean.in_reply_to_user_id.isna()] $ twitter_archive_enhanced_clean = twitter_archive_enhanced_clean.drop(['in_reply_to_status_id','in_reply_to_user_id'], axis = 1)
access_logs_df.select('ipAddress').distinct().count()
model.most_similar("best")
conn = pymysql.connect(host='mysql.guaminsects.net',user='readonlyguest',passwd='readonlypassword',db='oryctes') $ df_obs = pd.io.sql.read_sql(sql, conn) $ conn.close()
np.__version__
prediction_df = pd.DataFrame(y_pred, columns=["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]) $ prediction_df.head(15)
properati['state_name'].value_counts()
df.head(1)
recommendation_df.drop(['contest_id', 'rank'], axis = 1, inplace = True)
oppose.amount.sum()
lm=sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'CA', 'UK']]) $ results=lm.fit() $ results.summary() $
df2.query('group == "control"').converted.sum()/df2.query('group == "control"').converted.count()
reg.score(X_test,Y_test)
pca=decomposition.PCA() $ stocks_pca_t3= pca.fit_transform(stocks_pca_m3)
clean_rates.info()
df2[df2["user_id"] == 773192]
print(cfd_index['wm']) $ print(cfd_index['hr']) $ print(cfd_index['fy'])
df_students.columns.tolist()
print(model.wv.similarity('human', 'party')) $ print(model.wv.similarity('tree', 'murder'))
overallYearBuilt = pd.get_dummies(dfFull.YearBuilt)
Obama_raw.head(3)
for genre in genres: $     df[genre] = df.Genre.apply(lambda x : 1 if genre in x else 0) $ df.head()
k['percent_of_total'] = k['count']/len(train)
f, ax = plt.subplots() $ ax.set_ylim(ymax=1200); $ ax.set_xlabel('Shift duration [h]'); $ ax.set_ylabel('Count'); $ df['duration'].astype('timedelta64[h]').hist(bins=75, ax=ax);
df.head()
t1 = pd.to_datetime('20:24:27 10/05/2018', format='%H:%M:%S %d/%m/%Y') $ t2 = pd.to_datetime('21:24:27 10/05/2018', format='%H:%M:%S %d/%m/%Y')
commits_per_year1 = corrected_log["timestamp"].groupby(corrected_log["timestamp"].dt.year).count() $ commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq="AS")).count() $ print(commits_per_year1.head(5)) $ commits_per_year.head(5)
old_numbers = [10,8,3,1,5,-5,2,-15,-4,5,-2,-1,-3,-5] $ new_numbers = pd.Series(old_numbers).replace(range(1,10,1),0) $ pd.concat([pd.Series(old_numbers),pd.Series(new_numbers)],axis=1)
p_diffs_m = np.array(p_diffs).mean()
data['Agency'].unique()
len(tweets)
ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3]) $ ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6]) $ pd.concat([ser1, ser2])
class ProcessData: $         self.result['sum'] = self.datacont.sum() $         means = [x.mean() for x in self.datacont.data] $         self.result['mean'] = means $         return self.result
from matplotlib import pyplot as plt $ temp = reader.monitor.select_column("TM_T_SIPM") $ plt.hist(temp, bins=100)
X=df.titles $ y=df.target
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
faux = clean_rates.text.str.contains('\d+\.\d+/\d+(\.\d+)?') $ clean_rates[faux]
temp_cat.codes
import numpy as np $ import pandas as pd
df2.user_id.nunique()
df_archive_clean["source"] = df_archive_clean["source"].replace('<a href="http://twitter.com" rel="nofollow">Twitter Web Client</a>', $                                                                "Twitter Web Client")
df['log_price'] = np.log(df['price_doc']) $ df.head()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ z_score, p_value
active_sample_sizes = non_blocking_df_save_or_load_csv( $     active_raw_sample_sizes, $     "{0}/active_sample_sizes_13".format(fs_prefix))
def plot_q_table(q_table): $
