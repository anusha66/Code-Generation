os . kill ( os . getpid ( ) , signal . SIGUSR1 )
bytes . fromhex ( _STR_ ) . decode ( _STR_ )
all ( x == myList [ 0 ] for x in myList )
print ( _STR_ % ( 20 , _STR_ , 20 , _STR_ ) )
d . decode ( _STR_ ) . encode ( _STR_ )
res = { k : v for k , v in list ( kwargs . items ( ) ) if v is not None }
res = dict ( ( k , v ) for k , v in kwargs . items ( ) if v is not None )
subprocess . check_output ( _STR_ , shell = True )
_STR_ . join ( [ _STR_ , _STR_ , _STR_ ] )
pd . Series ( list ( set ( s1 ) . intersection ( set ( s2 ) ) ) )
client . send ( _STR_ )
then = datetime . datetime . strptime ( when , _STR_ ) . date ( )
inputString . split ( _STR_ )
_STR_ . split ( _STR_ )
_STR_ . join ( str ( x ) for x in b )
Entry . objects . filter ( ) [ : 1 ] . get ( )
a . sum ( axis = 1 )
warnings . simplefilter ( _STR_ )
print ( _STR_ . join ( map ( str , l ) ) )
subprocess . call ( [ _STR_ , _STR_ , _STR_ ] )
time . strptime ( _STR_ , _STR_ )
my_float = float ( my_string . replace ( _STR_ , _STR_ ) )
float ( _STR_ . replace ( _STR_ , _STR_ ) )
sys . path . append ( _STR_ )
re . split ( _STR_ , _STR_ )
file = open ( _STR_ , _STR_ )
urllib . request . urlretrieve ( _STR_ , _STR_ )
u = urllib . request . urlopen ( url ) #NEWLINE# f = open ( file_name , 'wb' ) #NEWLINE# meta = u . info ( ) #NEWLINE# file_size = int ( meta . getheaders ( 'Content-Length' ) [ 0 ] ) #NEWLINE# print ( 'Downloading:#SPACE#%s#SPACE#Bytes:#SPACE#%s' % ( file_name , file_size ) ) #NEWLINE# file_size_dl = 0 #NEWLINE# block_sz = 8192 #NEWLINE# while True : #NEWLINE# #INDENT# buffer = u . read ( block_sz ) #NEWLINE# #INDENT# if not buffer : #NEWLINE# #INDENT# #INDENT# break #NEWLINE# #INDENT# file_size_dl += len ( buffer ) #NEWLINE# #INDENT# f . write ( buffer ) #NEWLINE# #INDENT# status = '%10d#SPACE##SPACE#[%3.2f%%]' % ( file_size_dl , file_size_dl * 100.0 / #NEWLINE# file_size ) #NEWLINE# #INDENT# status = status + chr ( 8 ) * ( len ( status ) + 1 ) #NEWLINE# #INDENT# print ( status , end = '#SPACE#' ) #NEWLINE# f . close ( )
response = urllib . request . urlopen ( _STR_ ) #NEWLINE# html = response . read ( )
r = requests . get ( url )
response = requests . get ( url , stream = True ) #NEWLINE# with open ( '10MB' , 'wb' ) as handle : #NEWLINE# #INDENT# for data in tqdm ( response . iter_content ( ) ) : #NEWLINE# #INDENT# #INDENT# handle . write ( data )
parser . add_argument ( _STR_ , action = _STR_ , version = _STR_ )
{ i : d [ i ] for i in d if i != _STR_ }
pd . merge ( split_df , csv_df , on = [ _STR_ ] , suffixes = ( _STR_ , _STR_ ) )
s . split ( _STR_ , 4 )
input ( _STR_ )
app . run ( debug = True )
pickle . dump ( mylist , open ( _STR_ , _STR_ ) )
scipy . tensordot ( P , T , axes = [ 1 , 1 ] ) . swapaxes ( 0 , 1 )
numpy . zeros ( ( 3 , 3 , 3 ) )
_STR_ . join ( content . split ( _STR_ ) [ : - 1 ] )
x = np . asarray ( x ) . reshape ( 1 , - 1 ) [ ( 0 ) , : ]
sum ( sum ( i ) if isinstance ( i , list ) else i for i in L )
struct . unpack ( _STR_ , _STR_ . decode ( _STR_ ) ) [ 0 ]
my_dict . update ( ( x , y * 2 ) for x , y in list ( my_dict . items ( ) ) )
subprocess . call ( _STR_ , shell = True )
_STR_ . join ( l )
myList = _STR_ . join ( map ( str , myList ) )
list ( reversed ( list ( range ( 10 ) ) ) )
print ( _STR_ . replace ( _STR_ , _STR_ ) )
_STR_ . join ( s . split ( _STR_ ) [ : : - 1 ] )
datetime . datetime . fromtimestamp ( s ) . strftime ( _STR_ )
time . strftime ( _STR_ , time . gmtime ( 1236472051807 / 1000.0 ) )
( datetime . datetime . now ( ) - datetime . timedelta ( days = 7 ) ) . date ( )
print ( sum ( row [ column ] for row in data ) )
[ sum ( row [ i ] for row in array ) for i in range ( len ( array [ 0 ] ) ) ]
base64 . b64encode ( bytes ( _STR_ , _STR_ ) )
dict ( ( k , [ d [ k ] for d in dicts ] ) for k in dicts [ 0 ] )
{ k : [ d [ k ] for d in dicts ] for k in dicts [ 0 ] }
request . args [ _STR_ ]
[ k for k , v in list ( Counter ( mylist ) . items ( ) ) if v > 1 ]
sys . path . insert ( 1 , os . path . join ( os . path . dirname ( __file__ ) , _STR_ ) )
sys . path . append ( os . path . join ( os . path . dirname ( __file__ ) , _STR_ ) )
db . execute ( _STR_ , ( None , ) )
[ image for menuitem in list_of_menuitems for image in menuitem ]
a . extend ( b )
a . extend ( list ( b ) )
np . savetxt ( _STR_ , df . values , fmt = _STR_ )
df . to_csv ( _STR_ , header = None , index = None , sep = _STR_ , mode = _STR_ )
print ( x . rpartition ( _STR_ ) [ 0 ] )
print ( x . rsplit ( _STR_ , 1 ) [ 0 ] )
ftp . storlines ( _STR_ + filename , open ( filename , _STR_ ) )
browser . execute_script ( _STR_ )
np . maximum ( [ 2 , 3 , 4 ] , [ 1 , 5 , 2 ] )
print ( l [ 3 : ] + l [ : 3 ] )
for fn in os . listdir ( _STR_ ) : #NEWLINE# #INDENT# if os . path . isfile ( fn ) : #NEWLINE# #INDENT# #INDENT# pass
for root , dirs , filenames in os . walk ( source ) : #NEWLINE# #INDENT# for f in filenames : #NEWLINE# #INDENT# #INDENT# pass
[ int ( 1000 * random . random ( ) ) for i in range ( 10000 ) ]
datetime . datetime . now ( ) . strftime ( _STR_ )
db . GqlQuery ( _STR_ , foo . key ( ) )
df . b . str . contains ( _STR_ )
print ( _STR_ . join ( _STR_ . join ( str ( col ) for col in row ) for row in tab ) )
df . set_index ( list ( _STR_ ) ) . drop ( tuples , errors = _STR_ ) . reset_index ( )
_STR_ . format ( self . goals , self . penalties )
_STR_ . format ( self . goals , self . penalties )
_STR_ . format ( self )
[ int ( _STR_ . join ( str ( d ) for d in x ) ) for x in L ]
[ _STR_ . join ( str ( d ) for d in x ) for x in L ]
L = [ int ( _STR_ . join ( [ str ( y ) for y in x ] ) ) for x in L ]
myfile . write ( _STR_ . join ( lines ) )
[ x for x in [ _STR_ , _STR_ , _STR_ , _STR_ ] if _STR_ not in x and _STR_ not in x ]
text = re . sub ( _STR_ , _STR_ , text )
df . astype ( bool ) . sum ( axis = 1 )
re . search ( _STR_ , _STR_ )
_STR_ . split ( )
print ( re . search ( _STR_ , line ) . group ( 0 ) )
open ( filename , _STR_ ) . close ( )
datetime . datetime . strptime ( string_date , _STR_ )
[ index for index , item in enumerate ( thelist ) if item [ 0 ] == _STR_ ]
re . sub ( _STR_ , _STR_ , text ) . lower ( ) . strip ( )
re . sub ( _STR_ , _STR_ , text ) . lower ( ) . strip ( )
plt . plot ( x , y , label = _STR_ )
plt . plot ( x , y , label = _STR_ )
[ x for x in mylist if len ( x ) == 3 ]
lst = [ Object ( ) for _ in range ( 100 ) ]
lst = [ Object ( ) for i in range ( 100 ) ]
self . driver . find_element_by_css_selector ( _STR_ ) . get_attribute ( _STR_ )
df1 . merge ( df2 , on = _STR_ )
_STR_ % ( str1 , _STR_ )
[ x . strip ( ) for x in _STR_ . split ( _STR_ ) ]
if not os . path . exists ( directory ) : #NEWLINE# #INDENT# os . makedirs ( directory )
try : #NEWLINE# #INDENT# os . makedirs ( path ) #NEWLINE# except OSError : #NEWLINE# #INDENT# if not os . path . isdir ( path ) : #NEWLINE# #INDENT# #INDENT# raise
distutils . dir_util . mkpath ( path )
try : #NEWLINE# #INDENT# os . makedirs ( path ) #NEWLINE# except OSError as exception : #NEWLINE# #INDENT# if exception . errno != errno . EEXIST : #NEWLINE# #INDENT# #INDENT# raise
re . sub ( _STR_ , _STR_ , text )
re . sub ( _STR_ , _STR_ , _STR_ )
_STR_ . join ( [ x for x in _STR_ if x . isdigit ( ) ] )
print ( soup . find ( _STR_ ) . string )
records = dict ( ( record [ _STR_ ] , record ) for record in cursor )
np . concatenate ( ( A , B ) )
np . vstack ( ( A , B ) )
os . stat ( filepath ) . st_size
l . count ( _STR_ )
Counter ( l )
[ [ x , l . count ( x ) ] for x in set ( l ) ]
dict ( ( x , l . count ( x ) ) for x in set ( l ) )
l . count ( _STR_ )
shutil . copy ( srcfile , dstdir )
max ( k for k , v in x . items ( ) if v != 0 )
( k for k , v in x . items ( ) if v != 0 )
max ( k for k , v in x . items ( ) if v != 0 )
file . seek ( 0 )
df [ _STR_ ] = np . where ( df [ _STR_ ] . isnull , df [ _STR_ ] , df [ _STR_ ] )
del d [ _STR_ ]
MyModel . objects . update ( timestamp = F ( _STR_ ) + timedelta ( days = 36524.25 ) )
[ _STR_ ] + [ _STR_ ] + [ _STR_ ]
str ( int ( x ) + 1 ) . zfill ( len ( x ) )
all ( df . index [ : - 1 ] <= df . index [ 1 : ] )
list ( t )
tuple ( l )
level1 = map ( list , level1 )
pprint . pprint ( dataobject , logFile )
df . loc [ df [ _STR_ ] ]
df . iloc [ np . flatnonzero ( df [ _STR_ ] ) ]
df [ df [ _STR_ ] == True ] . index . tolist ( )
df [ df [ _STR_ ] ] . index . tolist ( )
os . chdir ( owd )
c . execute ( _STR_ , ( testfield , ) )
_STR_ . decode ( _STR_ )
raw_string . decode ( _STR_ )
raw_byte_string . decode ( _STR_ )
[ m . group ( 0 ) for m in re . finditer ( _STR_ , s ) ]
plt . scatter ( np . random . randn ( 100 ) , np . random . randn ( 100 ) , facecolors = _STR_ )
plt . plot ( np . random . randn ( 100 ) , np . random . randn ( 100 ) , _STR_ , mfc = _STR_ )
soup . find ( _STR_ , id = _STR_ ) . decompose ( )
df [ df [ _STR_ ] . str . contains ( _STR_ ) ]
df . reset_index ( level = 0 , inplace = True )
df [ _STR_ ] = df . index
df . reset_index ( level = [ _STR_ , _STR_ ] )
[ x [ : : - 1 ] for x in b ]
np . array ( [ zip ( x , y ) for x , y in zip ( a , b ) ] )
np . array ( zip ( a . ravel ( ) , b . ravel ( ) ) , dtype = _STR_ ) . reshape ( a . shape )
_STR_ . join ( [ str ( i ) for i in list_of_ints ] )
requests . post ( url , data = DATA , headers = HEADERS_DICT , auth = ( username , password ) )
_STR_ . rfind ( _STR_ )
print ( [ item for item in [ 1 , 2 , 3 ] ] )
[ ( x [ _STR_ ] , x [ _STR_ ] ) for x in d ]
print ( os . path . splitext ( os . path . basename ( _STR_ ) ) [ 0 ] )
dict ( x [ i : i + 2 ] for i in range ( 0 , len ( x ) , 2 ) )
values = sum ( [ [ _STR_ , _STR_ , _STR_ ] , [ _STR_ , _STR_ , _STR_ ] , [ _STR_ , _STR_ , _STR_ ] ] , [ ] )
df = df [ ( df [ _STR_ ] >= 99 ) & ( df [ _STR_ ] <= 101 ) ]
df . replace ( { _STR_ : _STR_ } , regex = True )
df . replace ( { _STR_ : _STR_ } , regex = True )
[ ( x + y ) for x , y in zip ( word , word [ 1 : ] ) ]
list ( map ( lambda x , y : x + y , word [ : - 1 ] , word [ 1 : ] ) )
print ( re . findall ( _STR_ , myString ) )
print ( re . search ( _STR_ , myString ) . group ( _STR_ ) )
re . sub ( _STR_ , _STR_ , mystring )
pd . date_range ( _STR_ , freq = _STR_ , periods = 13 )
matrix = [ [ a , b ] , [ c , d ] , [ e , f ] ]
mystring . replace ( _STR_ , _STR_ )
os . path . abspath ( _STR_ )
_STR_ . join ( my_string . split ( ) )
os . path . splitext ( filename ) [ 0 ]
[ sum ( l [ : i ] ) for i , _ in enumerate ( l ) ]
_STR_ . replace ( _STR_ , _STR_ ) . split ( _STR_ )
np . random . shuffle ( np . transpose ( r ) )
df [ _STR_ ] = df [ _STR_ ]
list ( data [ _STR_ ] [ _STR_ ] . values ( ) ) [ 0 ] [ _STR_ ] [ 0 ] [ _STR_ ]
all ( predicate ( x ) for x in string )
os . statvfs ( _STR_ ) . f_files - os . statvfs ( _STR_ ) . f_ffree
cursor . fetchone ( ) [ 0 ]
user_list = [ int ( number ) for number in user_input . split ( _STR_ ) ]
[ int ( s ) for s in user . split ( _STR_ ) ]
sorted ( list , key = lambda x : ( x [ 0 ] , - x [ 1 ] ) )
ut . sort ( key = cmpfun , reverse = True )
ut . sort ( key = lambda x : x . count , reverse = True )
ut . sort ( key = lambda x : x . count , reverse = True )
driver . find_element_by_partial_link_text ( _STR_ ) . click ( )
driver . findElement ( By . linkText ( _STR_ ) ) . click ( )
driver . find_element_by_link_text ( _STR_ ) . click ( )
_STR_ + str ( i )
df . sort_values ( [ _STR_ , _STR_ ] )
open ( _STR_ , _STR_ ) . write ( _STR_ + open ( _STR_ ) . read ( ) )
l . sort ( key = lambda t : len ( t [ 1 ] ) , reverse = True )
re . findall ( _STR_ , s )
bool ( re . search ( _STR_ , _STR_ ) )
list ( set ( t ) )
list ( set ( source_list ) )
list ( OrderedDict . fromkeys ( _STR_ ) )
numpy . array ( a ) . reshape ( - 1 ) . tolist ( )
numpy . array ( a ) [ 0 ] . tolist ( )
print ( soup . find ( text = _STR_ ) . findNext ( _STR_ ) . contents [ 0 ] )
_STR_ . join ( [ ( _STR_ % t ) for t in l ] )
_STR_ . join ( [ ( _STR_ % ( t [ 0 ] , t [ 1 ] ) ) for t in l ] )
driver . execute_script ( _STR_ )
[ i for i in teststr if re . search ( _STR_ , i ) ]
df [ _STR_ ] [ ( df [ _STR_ ] > 50 ) & ( df [ _STR_ ] == 900 ) ]
sorted ( o . items ( ) )
sorted ( d )
sorted ( d . items ( ) )
int ( _STR_ )
int ( )
T2 = [ map ( int , x ) for x in T1 ]
subprocess . call ( [ _STR_ ] )
subprocess . call ( [ _STR_ ] )
[ val for pair in zip ( l1 , l2 ) for val in pair ]
encoded = base64 . b64encode ( _STR_ )
encoded = _STR_ . encode ( _STR_ )
lol = list ( csv . reader ( open ( _STR_ , _STR_ ) , delimiter = _STR_ ) )
getattr ( my_object , my_str )
print ( dict ( zip ( LD [ 0 ] , zip ( * [ list ( d . values ( ) ) for d in LD ] ) ) ) )
sum ( [ pair [ 0 ] for pair in list_of_pairs ] )
d = ast . literal_eval ( _STR_ )
[ word for word in mystring . split ( ) if word . startswith ( _STR_ ) ]
text = re . sub ( _STR_ , _STR_ , text , flags = re . MULTILINE )
np . where ( np . in1d ( A , [ 1 , 3 , 4 ] ) . reshape ( A . shape ) , A , 0 )
np . mean ( a , axis = 1 )
subprocess . call ( [ _STR_ , _STR_ , _STR_ ] )
subprocess . call ( _STR_ , shell = True )
writer . writeheader ( )
df . fillna ( df . mean ( axis = 1 ) , axis = 1 )
time . strftime ( _STR_ , time . localtime ( 1347517370 ) )
super ( Derived , cls ) . do ( a )
a [ np . where ( ( a [ : , ( 0 ) ] == 0 ) * ( a [ : , ( 1 ) ] == 1 ) ) ]
re . split ( _STR_ , _STR_ )
len ( max ( words , key = len ) )
result [ 0 ] [ _STR_ ]
[ line . split ( ) for line in open ( _STR_ ) ]
res = dict ( ( v , k ) for k , v in a . items ( ) )
new_file = open ( _STR_ , _STR_ )
df . groupby ( [ _STR_ , _STR_ ] ) [ _STR_ ] . nunique ( ) . reset_index ( )
any ( key . startswith ( _STR_ ) for key in dict1 )
[ value for key , value in list ( dict1 . items ( ) ) if key . startswith ( _STR_ ) ]
pd . DataFrame ( { _STR_ : sf . index , _STR_ : sf . values } )
print ( _STR_ . join ( map ( str , list ) ) )
print ( _STR_ . encode ( _STR_ ) )
_STR_ . encode ( _STR_ ) . decode ( _STR_ )
image = image . resize ( ( x , y ) , Image . ANTIALIAS )
re . findall ( _STR_ , s )
print ( _STR_ . format ( 1.0 / 3 * 100 ) )
mylist . sort ( key = lambda x : x [ _STR_ ] )
l . sort ( key = lambda x : x [ _STR_ ] )
l . sort ( key = lambda x : ( x [ _STR_ ] , x [ _STR_ ] , x [ _STR_ ] ) )
heapq . nlargest ( 10 , range ( len ( l1 ) ) , key = lambda i : abs ( l1 [ i ] - l2 [ i ] ) )
soup . find_all ( _STR_ , { _STR_ : _STR_ } )
df . to_sql ( _STR_ , engine , schema = _STR_ )
brackets = re . sub ( _STR_ , _STR_ , s )
list ( dict ( ( x [ 0 ] , x ) for x in L ) . values ( ) )
[ line . rstrip ( _STR_ ) for line in file ]
[ i for i , x in enumerate ( testlist ) if x == 1 ]
[ i for i , x in enumerate ( testlist ) if x == 1 ]
for i in [ i for i , x in enumerate ( testlist ) if x == 1 ] : #NEWLINE# #INDENT# pass
for i in ( i for i , x in enumerate ( testlist ) if x == 1 ) : #NEWLINE# #INDENT# pass
gen = ( i for i , x in enumerate ( testlist ) if x == 1 ) #NEWLINE# for i in gen : #NEWLINE# #INDENT# pass
print ( testlist . index ( element ) )
try : #NEWLINE# #INDENT# print ( testlist . index ( element ) ) #NEWLINE# except ValueError : #NEWLINE# #INDENT# pass
max ( lis , key = lambda item : item [ 1 ] ) [ 0 ]
max ( lis , key = itemgetter ( 1 ) ) [ 0 ]
time . sleep ( 1 )
_STR_ . join ( _STR_ + _STR_ . join ( i ) + _STR_ for i in L )
b = models . CharField ( max_length = 7 , default = _STR_ , editable = False )
sorted ( list5 , lambda x : ( degree ( x ) , x ) )
sorted ( list5 , key = lambda vertex : ( degree ( vertex ) , vertex ) )
( n for n in [ 1 , 2 , 3 , 5 ] )
newlist = [ v for i , v in enumerate ( oldlist ) if i not in removelist ]
f = open ( _STR_ , _STR_ )
getattr ( obj , _STR_ )
from functools import reduce #NEWLINE# reduce ( lambda a , b : a + b , ( ( 'aa' , ) , ( 'bb' , ) , ( 'cc' , ) ) )
map ( lambda a : a [ 0 ] , ( ( _STR_ , ) , ( _STR_ , ) , ( _STR_ , ) ) )
df [ _STR_ ] . replace ( _STR_ , _STR_ , inplace = True )
zip ( * [ ( _STR_ , 1 ) , ( _STR_ , 2 ) , ( _STR_ , 3 ) , ( _STR_ , 4 ) ] )
zip ( * [ ( _STR_ , 1 ) , ( _STR_ , 2 ) , ( _STR_ , 3 ) , ( _STR_ , 4 ) ] )
result = [ a for a , b in original ] , [ b for a , b in original ]
result = ( a for a , b in original ) , ( b for a , b in original )
zip ( * [ ( _STR_ , 1 ) , ( _STR_ , 2 ) , ( _STR_ , 3 ) , ( _STR_ , 4 ) , ( _STR_ , ) ] )
map ( None , * [ ( _STR_ , 1 ) , ( _STR_ , 2 ) , ( _STR_ , 3 ) , ( _STR_ , 4 ) , ( _STR_ , ) ] )
json . dumps ( Decimal ( _STR_ ) )
d [ _STR_ ] = _STR_
data . update ( { _STR_ : 1 } )
data . update ( dict ( a = 1 ) )
data . update ( a = 1 )
max ( [ max ( i ) for i in matrix ] )
answer = str ( round ( answer , 2 ) )
ip = re . findall ( _STR_ , s )
df . groupby ( _STR_ ) . filter ( lambda x : len ( x ) > 1 )
[ x for x in myfile . splitlines ( ) if x != _STR_ ]
lst = map ( int , open ( _STR_ ) . readlines ( ) )
plt . colorbar ( mappable = mappable , cax = ax3 )
Counter ( _STR_ . join ( df [ _STR_ ] ) . split ( ) ) . most_common ( 100 )
re . findall ( _STR_ , text )
list ( itertools . combinations ( ( 1 , 2 , 3 ) , 2 ) )
datetime . now ( pytz . utc )
list2 = [ x for x in list1 if x != [ ] ]
list2 = [ x for x in list1 if x ]
return HttpResponse ( data , mimetype = _STR_ )
re . findall ( _STR_ , example_str )
re . findall ( _STR_ , example_str )
re . findall ( _STR_ , _STR_ )
re . findall ( _STR_ , _STR_ )
re . findall ( _STR_ , _STR_ )
elements = [ _STR_ . format ( element ) for element in elements ]
subprocess . Popen ( [ _STR_ , _STR_ ] )
[ mydict [ x ] for x in mykeys ]
dict ( [ ( _STR_ , _STR_ ) , ( _STR_ , 22 ) ] )
data . reshape ( - 1 , j ) . mean ( axis = 1 ) . reshape ( data . shape [ 0 ] , - 1 )
print ( s . encode ( _STR_ ) . replace ( _STR_ , _STR_ ) )
re . split ( _STR_ , s )
df . plot ( kind = _STR_ , stacked = True )
{ i [ 1 ] : i [ 0 ] for i in list ( myDictionary . items ( ) ) }
[ i for i , j in enumerate ( myList ) if _STR_ in j . lower ( ) or _STR_ in j . lower ( ) ]
isinstance ( obj , str )
isinstance ( o , str )
type ( o ) is str
isinstance ( o , str )
isinstance ( obj_to_test , str )
list2 . extend ( list1 )
list1 . extend ( mylog )
c . extend ( a )
for line in mylog : #NEWLINE# #INDENT# list1 . append ( line )
b . append ( ( a [ 0 ] [ 0 ] , a [ 0 ] [ 2 ] ) )
app . config [ _STR_ ] = _STR_
pd . DataFrame ( out . tolist ( ) , columns = [ _STR_ , _STR_ ] , index = out . index )
[ x for x in range ( len ( stocks_list ) ) if stocks_list [ x ] == _STR_ ]
ax . set_xticklabels ( labels , rotation = 45 )
re . sub ( _STR_ , _STR_ , s )
os . path . basename ( os . path . dirname ( os . path . realpath ( __file__ ) ) )
print ( re . findall ( _STR_ , str ) )
re . split ( _STR_ , input )
re . split ( _STR_ , input )
r = requests . post ( url , files = files , headers = headers , data = data )
open ( _STR_ , _STR_ ) . write ( bytes_ )
[ dct [ k ] for k in lst ]
x . set_index ( _STR_ ) . index . get_duplicates ( )
round ( 1.923328437452 , 3 )
sorted ( li , key = lambda x : datetime . strptime ( x [ 1 ] , _STR_ ) , reverse = True )
ax . set_rlabel_position ( 135 )
os . path . isabs ( my_path )
len ( list ( yourdict . keys ( ) ) )
len ( set ( open ( yourdictfile ) . read ( ) . split ( ) ) )
df . groupby ( _STR_ ) . first ( )
pd . concat ( [ df [ 0 ] . apply ( pd . Series ) , df [ 1 ] ] , axis = 1 )
re . findall ( _STR_ , data )
subprocess . Popen ( [ _STR_ ] )
q . put ( ( - n , n ) )
df [ _STR_ ] . plot ( kind = _STR_ , color = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] )
re . findall ( _STR_ , data )
len ( my_list )
len ( l )
len ( s )
len ( my_tuple )
len ( my_string )
_STR_ . decode ( _STR_ )
_STR_ . replace ( _STR_ , _STR_ ) . replace ( _STR_ , _STR_ ) . replace ( _STR_ , _STR_ )
shutil . rmtree ( _STR_ )
data [ _STR_ ] = data [ _STR_ ] . apply ( lambda x : x . weekday ( ) )
sorted ( x , key = x . get , reverse = True )
sorted ( list ( x . items ( ) ) , key = lambda pair : pair [ 1 ] , reverse = True )
np . vstack ( ( a , b ) )
print ( concatenate ( ( a , b ) , axis = 0 ) )
print ( concatenate ( ( a , b ) , axis = 1 ) )
c = np . r_ [ a [ ( None ) , : ] , b [ ( None ) , : ] ]
np . array ( ( a , b ) )
print ( socket . getaddrinfo ( _STR_ , 80 ) )
df . xs ( _STR_ , level = _STR_ , drop_level = False )
return HttpResponse ( _STR_ , status = 401 )
Flask ( __name__ , template_folder = _STR_ )
session . execute ( _STR_ )
c2 . sort ( key = lambda row : row [ 2 ] )
c2 . sort ( key = lambda row : ( row [ 2 ] , row [ 1 ] , row [ 0 ] ) )
c2 . sort ( key = lambda row : ( row [ 2 ] , row [ 1 ] ) )
matplotlib . rc ( _STR_ , ** { _STR_ : _STR_ , _STR_ : _STR_ } )
df [ _STR_ ] . apply ( lambda x : x . toordinal ( ) )
element . get_attribute ( _STR_ )
df . index . get_loc ( _STR_ )
os . system ( _STR_ )
my_dict . update ( { _STR_ : 1 } )
my_list = [ ]
my_list . append ( 12 )
myList . insert ( 0 , _STR_ )
_STR_ . replace ( _STR_ , _STR_ ) . decode ( _STR_ )
df [ df . columns [ - 1 ] ]
df . loc [ df [ _STR_ ] == _STR_ , _STR_ ] . values [ 0 ]
np . column_stack ( ( [ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] ) )
type ( i )
type ( v )
type ( v )
type ( v )
type ( v )
print ( type ( variable_name ) )
next ( itertools . islice ( range ( 10 ) , 5 , 5 + 1 ) )
print ( _STR_ . format ( word ) )
_STR_ . join ( list )
y = [ [ ] for n in range ( 2 ) ]
data = [ line . strip ( ) for line in open ( _STR_ , _STR_ ) ]
_STR_ . join ( [ char for char in _STR_ if char != _STR_ ] )
re . sub ( _STR_ , _STR_ , _STR_ )
_STR_ . replace ( _STR_ , _STR_ )
_STR_ . join ( [ char for char in _STR_ if char != _STR_ ] )
df . dropna ( subset = [ 1 ] )
[ x for x in myList if x . n == 30 ]
nums = [ int ( x ) for x in intstringlist ]
map ( int , eval ( input ( _STR_ ) ) )
sys . stdout . write ( _STR_ )
int ( round ( 2.51 * 100 ) )
os . chdir ( _STR_ ) #NEWLINE# for file in glob . glob ( '*.txt' ) : #NEWLINE# #INDENT# pass
for file in os . listdir ( _STR_ ) : #NEWLINE# #INDENT# if file . endswith ( '.txt' ) : #NEWLINE# #INDENT# #INDENT# pass
for root , dirs , files in os . walk ( _STR_ ) : #NEWLINE# #INDENT# for file in files : #NEWLINE# #INDENT# #INDENT# if file . endswith ( '.txt' ) : #NEWLINE# #INDENT# #INDENT# #INDENT# pass
df . plot ( legend = False )
for i in range ( 256 ) : #NEWLINE# #INDENT# for j in range ( 256 ) : #NEWLINE# #INDENT# #INDENT# ip = '192.168.%d.%d' % ( i , j ) #NEWLINE# #INDENT# #INDENT# print ( ip )
for i , j in product ( list ( range ( 256 ) ) , list ( range ( 256 ) ) ) : #NEWLINE# #INDENT# pass
generator = iter_iprange ( _STR_ , _STR_ , step = 1 )
sum ( 1 << i for i , b in enumerate ( x ) if b )
target . write ( _STR_ % ( line1 , line2 , line3 ) )
[ y for x in data for y in ( x if isinstance ( x , list ) else [ x ] ) ]
print ( _STR_ . encode ( _STR_ ) )
_STR_ . join ( s . rsplit ( _STR_ , 1 ) )
( x [ 1 : ] + x [ : - 1 ] ) / 2
x [ : - 1 ] + ( x [ 1 : ] - x [ : - 1 ] ) / 2
arr = numpy . fromiter ( codecs . open ( _STR_ , encoding = _STR_ ) , dtype = _STR_ )
l = sorted ( l , key = itemgetter ( _STR_ ) , reverse = True )
l = sorted ( l , key = lambda a : a [ _STR_ ] , reverse = True )
df . loc [ df [ 0 ] . str . contains ( _STR_ ) ]
re . search ( _STR_ , your_string ) . group ( 1 )
[ d . strftime ( _STR_ ) for d in pandas . date_range ( _STR_ , _STR_ ) ]
_STR_ . count ( _STR_ )
json . loads ( request . body )
urllib . request . urlretrieve ( url , file_name )
text . split ( )
text . split ( _STR_ )
line . split ( )
[ re . sub ( _STR_ , _STR_ , i ) for i in s ]
sorted ( list_of_strings , key = lambda s : s . split ( _STR_ ) [ 1 ] )
subprocess . check_call ( _STR_ , shell = True )
[ element for element in lst if isinstance ( element , int ) ]
[ element for element in lst if not isinstance ( element , str ) ]
newlist = sorted ( list_to_be_sorted , key = lambda k : k [ _STR_ ] )
newlist = sorted ( l , key = itemgetter ( _STR_ ) , reverse = True )
list_of_dicts . sort ( key = operator . itemgetter ( _STR_ ) )
list_of_dicts . sort ( key = operator . itemgetter ( _STR_ ) )
df . groupby ( _STR_ ) . sum ( ) . sort ( _STR_ , ascending = False )
_STR_ . join ( trans [ _STR_ ] )
_STR_ . join ( [ _STR_ , _STR_ , _STR_ , _STR_ ] )
json . load ( urllib . request . urlopen ( _STR_ ) )
[ x for x in sents if not x . startswith ( _STR_ ) and not x . startswith ( _STR_ ) ]
Entry . objects . filter ( pub_date__contains = _STR_ )
list . sort ( key = lambda item : ( item [ _STR_ ] , item [ _STR_ ] ) )
( t - datetime . datetime ( 1970 , 1 , 1 ) ) . total_seconds ( )
re . sub ( _STR_ , _STR_ , _STR_ )
import imp #NEWLINE# imp . reload ( module )
struct . unpack ( _STR_ , struct . pack ( _STR_ , number ) )
numlist = [ float ( x ) for x in numlist ]
df . to_csv ( filename , index = False )
json_data = json . loads ( unescaped )
[ chr ( i ) for i in range ( 127 ) ]
newFile . write ( struct . pack ( _STR_ , * newFileBytes ) )
re . sub ( _STR_ , _STR_ , string )
list ( dict . keys ( ) ) [ - 1 ]
print ( _STR_ , file = f )
f = open ( _STR_ , _STR_ ) #NEWLINE# f . write ( 'hi#SPACE#there\n' ) #NEWLINE# f . close ( )
with open ( _STR_ , _STR_ ) as the_file : #NEWLINE# #INDENT# the_file . write ( 'Hello\n' )
s . encode ( _STR_ )
AuthorizedEmail . objects . filter ( group = group ) . order_by ( _STR_ ) [ 0 ]
re . findall ( _STR_ , text )
re . findall ( _STR_ , text )
os . system ( _STR_ , _STR_ )
b . sort ( key = lambda x : x [ 1 ] [ 2 ] )
list ( cf . get_range ( ) . get_keys ( ) )
datetime . datetime . now ( )
next ( i for i , x in enumerate ( lst ) if not isinstance ( x , bool ) and x == 1 )
a [ : ] = [ ( x - 13 ) for x in a ]
random . choice ( os . listdir ( _STR_ ) )
max ( x . min ( ) , x . max ( ) , key = abs )
re . findall ( _STR_ , s , re . MULTILINE | re . DOTALL )
re . findall ( _STR_ , document )
mystring . replace ( _STR_ , _STR_ ) . split ( _STR_ )
open ( path , _STR_ )
[ [ sum ( item ) for item in zip ( * items ) ] for items in zip ( * data ) ]
a [ : , ( np . newaxis ) ]
