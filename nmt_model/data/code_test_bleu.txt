print ( pd . DataFrame ( test_matrix ) . head ( ) )
df = pd . read_excel ( _STR_ )
cars = cars . drop ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 )   $
RF . fit ( X_train , Y_train )
apple . asfreq ( _STR_ , method = _STR_ )   $ apple . head ( )
df2 [ _STR_ ] = pd . get_dummies ( df [ _STR_ ] ) [ _STR_ ]   $ df2 . head ( )
res = joinresult . set_index ( [ _STR_ , _STR_ ] )   $ print ( res )
print ( _STR_ , p_diff )   $ p_greater_than_diff = ( float ( len ( greater_than_diff ) ) / len ( p_diffs ) )   $ print ( _STR_ , p_greater_than_diff )   $ print ( _STR_ . format ( p_greater_than_diff * 100 ) )
data = drive . CreateFile ( { _STR_ : _STR_ } )   $ data . GetContentFile ( _STR_ )   $ test_data = drive . CreateFile ( { _STR_ : _STR_ } )   $ test_data . GetContentFile ( _STR_ )   $
sp = openmc . StatePoint ( _STR_ )
pre_strategy = people_person . date < _STR_
df_tte_all [ df_tte_all [ _STR_ ] == _STR_ ]
def find_similar ( matrix , index , top_n = 10 ) :   $ cosine_similarities = linear_kernel ( matrix [ index : index + 1 ] , matrix ) . flatten ( )   $ related_docs_indices = [ i for i in cosine_similarities . argsort ( ) [ : : - 1 ] if i != index ]   $ return [ ( index , cosine_similarities [ index ] ) for index in related_docs_indices ] [ 0 : top_n ]
df_tweets = pd . DataFrame ( tweets )   $ df_tweets
yc_new3 = yc_new2 [ yc_new2 . tipPC < 100 ]
tree_features_df [ ~ tree_features_df [ _STR_ ] . isin ( manager . image_df [ _STR_ ] ) ] #Let's look at what's missing:
ybar_min = dfNiwot [ _STR_ ] . mean ( )   $ ybar_max = dfNiwot [ _STR_ ] . mean ( )   $ print ( _STR_ . format ( ybar_min ) )   $ print ( _STR_ . format ( ybar_max ) )
dtrain , dval , evals = xgb_md . get_train_eval_ds ( )
import google . datalab . storage as storage
df_only_headline = df_tweets [ ( df_tweets [ _STR_ ] . str . len ( ) == 0 ) & ( df_tweets [ _STR_ ] . apply (   $ lambda x : len ( set ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ) . intersection ( set ( x ) ) ) == 0   $ ) ) ]   $ headline_date_count = df_only_headline . groupby ( [ _STR_ , _STR_ ] ) . size ( )   $ headline_date_count [ headline_date_count > 1 ]
taxiData . Trip_distance . size
X = pd . merge ( X , meal_is_interactive [ [ _STR_ , _STR_ ] ] , on = _STR_ , how = _STR_ )
url = form_url ( _STR_ , orderBy = _STR_ )   $ response = requests . get ( url , headers = headers )   $ print_enumeration ( response )
disposition_df . shape
aug2014 . start_time , aug2014 . end_time   $
master_list . sort_values ( by = _STR_ , ascending = False ) . head ( 10 )
n_new = len ( df2 . query ( _STR_ ) )   $ print ( _STR_ . format ( n_new ) )
import statsmodels . api as sm   $ convert_old = sum ( df2 . query ( _STR_ ) [ _STR_ ] )   $ convert_new = sum ( df2 . query ( _STR_ ) [ _STR_ ] )   $ print ( _STR_ )   $ print ( _STR_ )
h = tclab . Historian ( ( ( _STR_ , lambda : [ 0 , 0 , 0 , 0 ] ) ,   $ ( _STR_ , None ) ,   $ ( _STR_ , None ) ,   $ ( _STR_ , None ) ) , dbfile = _STR_ )
ferrocarriles_caba = pd . read_csv ( _STR_ , sep = _STR_ , error_bad_lines = False , low_memory = False )   $ ferrocarriles_caba . info ( )
y_pred = rdf_model . predict ( X_test )   $ precision , recall , fscore , support = score ( y_test , y_pred , pos_label = 4 , average = _STR_ )
df . groupby ( _STR_ ) [ _STR_ ] . mean ( )
from statsmodels . stats . diagnostic import acorr_ljungbox   $
df . describe ( include = [ _STR_ ] )
ohe_feats = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ for f in ohe_feats :   $ df_all_dummy = pd . get_dummies ( df_all [ f ] , prefix = f )   $ df_all = df_all . drop ( [ f ] , axis = 1 )   $ df_all = pd . concat ( ( df_all , df_all_dummy ) , axis = 1 )
pd . Series . loc ?
x_train = scaler . transform ( x_train )
data = open ( _STR_ ) . read ( )
S_lumpedTopmodel . basin_par . filename
p_diffs = [ ]   $ diffs = np . random . binomial ( n_new , p_new , 10000 ) / n_new - np . random . binomial ( n_old , p_old , 10000 ) / n_old   $ p_diffs . append ( diffs )
run txt2pdf . py - o _STR_ _STR_
df . dropna ( inplace = True )   $ df . shape
a4_dims = ( 15 , 15 )   $ fig , ax = plt . subplots ( figsize = a4_dims )   $ seaborn . barplot ( data = keywords_dataframe , x = _STR_ , y = _STR_ , ax = ax )
autos [ _STR_ ] . value_counts ( ) . head ( )
f = np . linspace ( 0 , np . pi , 100 )   $ f = np . sin ( f )   $ print ( _STR_ , f )
df_archive_clean . to_csv ( _STR_ , encoding = _STR_ , index = False )
con = sqlite3 . connect ( _STR_ )   $ df = pd . read_sql_query ( _STR_ , con )   $ con . close ( )   $ df
mike = pd . read_csv ( _STR_ , delim_whitespace = True , header = None , names = [ _STR_ , _STR_ , _STR_ , _STR_ ] )   $ mike [ _STR_ ] = [ float ( Angle ( i , u . hr ) . to_string ( unit = u . degree , precision = 5 , decimal = True ) ) for i in mike . ra ]   $ mike [ _STR_ ] = [ float ( Angle ( i , u . deg ) . to_string ( unit = u . degree , precision = 5 , decimal = True ) ) for i in mike . dec ]   $
display ( _STR_ , _STR_ ,   $ _STR_ )
pd . cut ( londonDFSubset [ _STR_ ] , bins = 5 , include_lowest = True )
( violations08Acounts / boro_counts ) . plot ( kind = _STR_ )
autos . head ( )   $
df_everything_about_DRGs [ ( df_everything_about_DRGs [ _STR_ ] == _STR_ ) ] . index . tolist ( )
df1 . query ( _STR_ ) . shape [ 0 ] +   \   $ df1 . query ( _STR_ ) . shape [ 0 ]
new_page_converted = np . random . binomial ( nnew , pnew )
ordered . set_index ( [ 0 ] , inplace = True )   $ odds = ordered [ _STR_ ] . plot ( kind = _STR_ , figsize = ( 20 , 5 ) , title = _STR_ , rot = 45 , legend = True )   $ odds . set_ylabel ( _STR_ , fontsize = 15 )   $ plt . show ( )
neg_freq = { k : neg_tfidf . dfs . get ( v ) for v , k in neg_dic . id2token . items ( ) }   $ sorted ( neg_freq . items ( ) , key = lambda x : x [ 1 ] , reverse = True )
tbl = pd . merge ( df , tbl , on = [ _STR_ , _STR_ ] )   $ tbl . tail ( n = 1 )
week1_df = courses [ 0 ]   $ week2_df = courses [ 1 ]   $ week1_df . append ( week2_df )
dfRegMet = dfRegMet [ dfRegMet [ _STR_ ] < - 33.015542 ]
shape_file = ( _STR_ )
model . train ( x = mtcars_filtered . col_names , training_frame = mtcars_filtered )
t . reset_index ( ) . corr ( )
reddit . head ( )
league = pd . read_sql_query ( _STR_ , conn ) # don't forget to specify the connection $ print(league.shape) $ league.head()
mv_lens = pd . merge ( movies , ratings )
display ( _STR_ , _STR_ , _STR_ )
post_date = djb_soup . findAll ( _STR_ , { _STR_ : _STR_ } )   $ for x in post_date :   $ print ( x . text )   $
logistic_mod_time = sm . Logit ( df3 [ _STR_ ] , df3 [ [ _STR_ , _STR_ , _STR_ ] ] )   $ results_time = logistic_mod_time . fit ( )   $ results_time . summary ( )
people . sort_values ( by = _STR_ , inplace = True )   $ people
products = soup . find ( _STR_ , class_ = _STR_ )   $ hemispheres = products . find_all ( _STR_ , class_ = _STR_ )
weather_subset = weather_mean [ [ _STR_ , _STR_ , _STR_ ] ]   $ weather_subset . plot ( kind = _STR_ , subplots = True , figsize = ( 10 , 6 ) , fontsize = _STR_ ) ;
sns . countplot ( auto_new . Hand_Drive )
selfharmmm_final_df = mf . compile_combo_dfs ( epoch3_df , _STR_ , selfharmmm_dictionary , nmf_cv_df , nmf_tfidf_df , lsa_cv_df , lsa_tfidf_df , lda_cv_df , lda_tfidf_df )
out = query . get_dataset ( db , id = ds_info [ _STR_ ] [ 0 ] )
twitter . name . value_counts ( )   $
teams = pd . unique ( results [ [ _STR_ , _STR_ ] ] . values . ravel ( ) )   $ teams
pop_dog = timedog_df . groupby ( _STR_ ) [ [ _STR_ , _STR_ ] ] . mean ( )   $ pop_dog . head ( )
cell_df [ _STR_ ] = cell_df [ _STR_ ] . astype ( _STR_ )   $ y = np . asarray ( cell_df [ _STR_ ] )   $ y [ 0 : 5 ]
pandas . DataFrame (   $ get_child_column_data ( observations_node ) +   $ get_child_column_data ( observations_ext_node )   $ )
bo . loc [ bo [ _STR_ ] == 1 , _STR_ ] [ 0 ]
old_page_converted = [ ]   $ for _ in range ( n_old ) :   $ b = np . random . binomial ( 1 , p_old )   $ old_page_converted . append ( b )   $ old_page_converted = np . asarray ( old_page_converted )
smap = folium . Map ( location = [ site_dct [ _STR_ ] , site_dct [ _STR_ ] ] , tiles = _STR_ , zoom_start = 16 )
twitter_merged_data . hist ( column = _STR_ , bins = 30 ) ;   $ plt . title ( _STR_ )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ ) ;
data = r . json ( )   $ data
url = _STR_   $ html_txt = urllib . request . urlopen ( url ) . read ( )   $ dom = lxml . html . fromstring ( html_txt )   $ [ line for line in dom . xpath ( _STR_ ) ] [ 0 : 10 ]
address = pd . read_sql_query ( _STR_ , engine )   $ address . head ( )
charge_counts = df . groupby ( _STR_ ) [ _STR_ ] . count ( )
fig , ax = plt . subplots ( figsize = ( 8 , 6 ) )   $ df_master [ _STR_ ] . value_counts ( ) . head ( 20 ) . plot ( kind = _STR_ , color = _STR_ , ax = ax , edgecolor = [ _STR_ ] * len ( names ) )   $ ax . set_facecolor ( _STR_ )   $ plt . show ( ) ;
result_df [ _STR_ ] . value_counts ( )
arr1d = np . linspace ( start = 0 , stop = 100 , num = 10 , dtype = np . int8 )   $ arr1d
! wget https : // download . pytorch . org / tutorial / faces . zip   $   ! unzip faces . zip
grid_heatmap . head ( )
news_df . drop_duplicates ( subset = [ _STR_ , _STR_ , _STR_ , _STR_ ] , inplace = True )
df_enhanced . head ( 5 )
n_booths = len ( bthlst )   $ n_faulty = df_bug [ _STR_ ] . unique ( ) . size   $ n_fit = n_booths - n_faulty   $ print _STR_ , n_booths , _STR_ , n_faulty , _STR_
no_psc_and_psc = active_psc_records [ ( active_psc_records . company_number . isin ( active_psc_statements [ active_psc_statements . statement == _STR_ ] \   $ . company_number ) ) ] [ _STR_ ]   $ print ( _STR_ )   $ active_companies [ active_companies . CompanyNumber . isin ( no_psc_and_psc ) ] . to_csv ( _STR_ )   $ active_companies [ active_companies . CompanyNumber . isin ( no_psc_and_psc ) ] [ [ _STR_ , _STR_ , _STR_ ] ] . head ( 5 )
category = [ ]   $ for i in range ( avg_preds . shape [ 0 ] ) :   $ category . append ( np . argmax ( avg_preds [ i ] ) )   $ type ( category ) , len ( category ) , category [ 0 : 10 ]
df_p = df_proc1 . groupby ( [ _STR_ , _STR_ , _STR_ , _STR_ ] , as_index = False ) . agg ( np . sum )   $ df_p1 = df_p . loc [ : , [ _STR_ , _STR_ , _STR_ , _STR_ ] ]   $ ivtdiff = lambda x : abs ( x . shift ( 1 ) - x )   $ df_p1 [ _STR_ ] = ( df_p1 . groupby ( _STR_ ) [ _STR_ ] . transform ( ivtdiff ) ) . dt . days   $ df_p1 [ _STR_ ] = df_p1 [ _STR_ ]
df . groupby ( _STR_ ) [ _STR_ ] . nunique ( ) . reset_index ( ) . rename (   $ columns = { _STR_ : _STR_ } ) . head ( 10 )
zipShp . head ( )
mpl . style . use ( _STR_ )
date_crawled_count_norm . describe ( )
cpi_sdmx . translate_expression ( _STR_ )
ds_eval = FileDataStream . read_csv ( eval_file , collapse = False , header = False , names = columns , numeric_dtype = np . float32 , sep = _STR_ , na_values = [ _STR_ ] , keep_default_na = False )
get_response ( _STR_ )
autos [ _STR_ ] . value_counts ( ) . sort_index ( ascending = False ) . head ( 20 )
f = open ( _STR_ , _STR_ )   $ aa = f . read ( )   $ aa
conn . close ( )
( df2 [ df2 [ _STR_ ] == _STR_ ] . user_id . count ( ) ) / 290585
table = pd . crosstab ( df [ _STR_ ] , df [ _STR_ ] , normalize = True )   $
print ( _STR_ )   $ relationships = lift . get_all_enterprise_relationships ( )   $ print ( len ( relationships ) )   $ df = json_normalize ( relationships )   $ df . reindex ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 ) [ 0 : 5 ]
print ( data . shape )   $ print ( data . columns )   $ data . head ( )
score [ _STR_ ] = np . where ( ( score [ _STR_ ] == 1 ) & ( score [ _STR_ ] == 0 ) , 1 , 0 )   $ score [ _STR_ ] = np . where ( ( score [ _STR_ ] == 0 ) & ( score [ _STR_ ] == 1 ) , 1 , 0 )   $ score [ _STR_ ] = np . where ( ( score [ _STR_ ] == 1 ) & ( score [ _STR_ ] == 1 ) , 1 , 0 )   $ score [ _STR_ ] = np . where ( ( score [ _STR_ ] == 0 ) & ( score [ _STR_ ] == 0 ) , 1 , 0 )
def datetimeampm2datetime ( text ) :   $ try :   $ return pd . to_datetime ( text , format = _STR_ )   $ except AttributeError :   $ return text
iris_new [ _STR_ ] = iris_new [ _STR_ ] * iris_new [ _STR_ ]   $ iris_new [ _STR_ ] . quantile ( [ 0.25 , 0.5 , 0.75 ] )
pd_aux2 . describe ( )
y_cat = aldf [ _STR_ ]   $ y_cat . shape
df2_clean [ _STR_ ] = df2_clean [ _STR_ ] . str . title ( )   $ df2_clean [ _STR_ ] = df2_clean [ _STR_ ] . str . title ( )   $ df2_clean [ _STR_ ] = df2_clean [ _STR_ ] . str . title ( )
autos [ _STR_ ] . value_counts ( )
lm = sm . Logit ( df2 [ _STR_ ] , df2 [ [ _STR_ , _STR_ ] ] )   $ r = lm . fit ( )
final_topbikes . groupby ( by = final_topbikes . index . weekday ) [ _STR_ ] . count ( ) . plot ( kind = _STR_ , figsize = ( 6 , 3 ) )
df . query ( _STR_ ) . index
e_p_b_two = cfs_df [ cfs_df . Beat == _STR_ ]   $ e_p_b_two = e_p_b_two . groupby ( e_p_b_two . TimeCreate ) . size ( ) . reset_index ( )
bedr_intercept , bedr_slope = simple_linear_regression ( train_data [ _STR_ ] , train_data [ _STR_ ] )   $ print ( _STR_ . format ( bedr_intercept ) )   $ print ( _STR_ . format ( bedr_slope ) )
token2id , id2token , _ = index . get_dictionary ( )   $ print ( list ( id2token . items ( ) ) [ : 15 ] )
frequent_authors = authors [ ( authors [ _STR_ ] >= 100 ) & ( authors [ _STR_ ] >= 0.3 ) ]   $ frequent_authors . shape [ 0 ]
df_potholes . groupby ( df_potholes . index . weekday ) [ _STR_ ] . count ( ) . plot ( kind = _STR_ )   $
print ( pd . DataFrame ( test_matrix ) . head ( ) )
ok . grade ( _STR_ )
df_geo . columns = [ _STR_ , _STR_ , _STR_ , _STR_ ]   $ df_geo [ _STR_ ] = pd . to_numeric ( df_geo . Tract ) / 100.   $ df_geo [ [ _STR_ , _STR_ ] ] = df_geo [ [ _STR_ , _STR_ ] ] . apply ( pd . to_numeric )
p_new = df2 [ _STR_ ] . mean ( )   $ p_old = p_new   $ print ( _STR_ , p_new , _STR_ , p_old )
autos [ _STR_ ] = ( autos [ _STR_ ]   $ . str . replace ( _STR_ , _STR_ )   $ . str . replace ( _STR_ , _STR_ )   $ . astype ( float )   $ )
data . loc [ ( 80 , slice ( _STR_ , _STR_ ) , _STR_ ) , : ] . iloc [ : , 0 : 4 ]
np . exp ( 0.0150 )
@ pyimport pandas_datareader . data as pdrd   $ stocks = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]     $ df = pdrd . get_data_yahoo ( stocks , _STR_ , interval = _STR_ )   $
props . head ( )
naive_preds = np . repeat ( np . mean ( y_tr ) , X_val . shape [ 0 ] )   $ np . sqrt ( metrics . mean_squared_error ( y_val , naive_preds ) )
fig = plt . subplot ( 1 , 1 , 1 )   $ fig . plot ( track . speed_gps , [ sqrt ( track [ _STR_ ] [ i ] ) for i in range ( track . shape [ 0 ] ) ] , _STR_ )   $ fig . figure . set_size_inches ( 16 , 16 )   $ fig . axis ( ( 0 , 7 , 0 , 7 ) )   $
leaderboard = pd . read_json ( _STR_ )   $ leaderboard [ _STR_ ] = pd . to_numeric ( leaderboard [ _STR_ ] , errors = _STR_ )   $ leaderboard . head ( )
fig = m . plot_components ( forecast ) ;
def get_list_tot_likes ( the_posts ) :   $ list_tot_likes = [ ]   $ for i in list_Media_ID :   $ list_tot_likes . append ( the_posts [ i ] [ _STR_ ] [ - 1 ] [ _STR_ ] )   $ return list_tot_likes
print ( _STR_ % dataset . description )   $ print ( _STR_ % len ( dataset . data ) )   $ print ( _STR_ % len ( dataset . target_names ) )
table_rows = driver . find_elements_by_tag_name ( _STR_ ) [ 24 ] . find_elements_by_tag_name ( _STR_ )   $
twitter_archive_clean [ twitter_archive_clean [ _STR_ ] . isnull ( ) == False ] . shape [ 0 ]
df2 . query ( _STR_ ) [ _STR_ ] . count ( ) / df2 . shape [ 0 ]
sNew = pd . Series ( [ 1 , 2 , 3 , 4 , 5 ] ,   $ index = pd . date_range ( _STR_ , periods = 5 ) )   $ df [ _STR_ ] = sNew   $ df
dataset . head ( )
df3 = df2 . merge ( c , on = _STR_ , how = _STR_ )   $ df3 . head ( )
old_page_converted = np . random . binomial ( nold , nullrate )   $ old_page_converted
sum ( contractor . state_id . isnull ( ) ) #the count of missing state_id value is 0 $ contractor.state_id.value_counts() #The state_id columns do not have missing data
trend_de . head ( )
ref_dict = reference_frame . set_index ( _STR_ ) [ _STR_ ] . to_dict ( )   $ patient_group_dict = patient_group . set_index ( _STR_ ) [ _STR_ ] . to_dict ( )
df = pd . DataFrame ( one_year_prcp , columns = [ _STR_ , _STR_ , _STR_ , _STR_ ] )   $ df . head ( )
LUM . plot_time_all ( all_lum_binned )
log_mod_interact = sm . Logit ( df_new_npage [ _STR_ ] , df_new_npage [ [ _STR_ , _STR_ , _STR_ ] ] )   $ log_mod_interact_results = log_mod_interact . fit ( )   $ log_mod_interact_results . summary ( )
pipe = pc . PipelineControl ( data_path = _STR_ ,   $ prediction_path = _STR_ ,   $ retraining_flag = True ,   $ sliding_window_size = 500 )   $ pipe . runPipeline ( )
users [ users [ _STR_ ] < ( pd . Timestamp . today ( ) - pd . DateOffset ( years = 1 ) ) ] [ _STR_ ] . plot ( kind = _STR_ )   $ plt . title ( _STR_ )   $ plt . xlim ( ( 0 , 2070 ) )   $ plt . xlabel ( _STR_ )
import pandas as pd   $ import matplotlib . pyplot as plt
decoder_model_inference . save ( _STR_ )
np . corrcoef ( total_sales . ratio . values , total_sales . sales_change_growth . values )
STD_reorder_stats . describe ( )
df . replace ( _STR_ , 51529 , inplace = True )   $ df [ df [ _STR_ ] == 51529 ] . head ( )
daily = hourly . asfreq ( _STR_ )   $ daily
% sql   \   $ SELECT twitter . tweet_text FROM twitter   \   $ WHERE twitter . tweet_text LIKE _STR_ ;
df . head ( 12 )
train_reduced = train_pos . union ( train_neg ) . orderBy ( func . rand ( seed = seed ) )   $ train_reduced . cache ( )   $ validation = validation . cache ( )   $ print ( _STR_ , train_reduced . count ( ) )   $ print ( _STR_ , validation . count ( ) )
from matplotlib . pyplot import figure   $ figure ( num = None , figsize = ( 17 , 4 ) , dpi = 80 , facecolor = _STR_ , edgecolor = _STR_ )   $ genders = list ( gender_freq_hist . index )   $ frequencies = list ( gender_freq_hist . gender_freq )   $ plt . bar ( genders , frequencies )
dtm . shape
merged_data [ _STR_ ] = merged_data [ _STR_ ] . dt . day   $ merged_data [ _STR_ ] = merged_data [ _STR_ ] . dt . month   $ merged_data [ _STR_ ] = merged_data [ _STR_ ] . dt . year
from sklearn . model_selection import train_test_split   $ test_data_size = 0.25   $ seed = 8   $ X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = test_data_size , random_state = seed )   $ X_train . head ( )
duration = tia [ _STR_ ] . apply ( lambda x : dt . date ( 2018 , 1 , 30 ) - x . date ( ) )
pd . value_counts ( preds ) . plot . bar ( )
print ( model . summary ( ) )
def datetimeinvertedstring2datetime ( text ) :   $ try :   $ return pd . to_datetime ( text , format = _STR_ )   $ except AttributeError :   $ return text
df_spl [ _STR_ ] = df_spl [ _STR_ ] . apply ( lambda x : translate ( x , _STR_ ) )   $ df_spl [ _STR_ ] = df_spl [ _STR_ ] . apply ( lambda x : translate ( x , _STR_ ) )
df2 . drop_duplicates ( _STR_ , keep = _STR_ , inplace = True )   $ df2 [ df2 [ _STR_ ] == repeated_id ]
from sklearn . cross_validation import train_test_split   $ x = my_df . text   $ y = my_df . target   $ SEED = 2000   $ x_train , x_validation_and_test , y_train , y_validation_and_test = train_test_split ( x , y , test_size = .10 , random_state = SEED )
spark . stop ( )
StockData . loc [ StockData . Date < StartDate , _STR_ ] = _STR_   $ print ( _STR_ . format ( len ( StockData . loc [ StockData . Set == _STR_ ] ) ) )   $ print ( _STR_ . format ( len ( StockData . loc [ StockData . Set == _STR_ ] ) ) )
train [ train . url . isnull ( ) ] . head ( )
b_dist = bnbx [ ( bnbx [ _STR_ ] < 80 ) & ( bnbx [ _STR_ ] >= 18 ) ]   $ bnbx [ _STR_ ] = bnbx . age . fillna ( 28 )   $
df_c . head ( )
dt = datetime . datetime . today ( )   $ td = datetime . timedelta ( hours = 6 , minutes = 42 , microseconds = 123456 )   $ dt + td
topics = set ( )   $ df [ _STR_ ] . apply ( lambda x : topics . update ( x ) )   $ topics = tuple ( topics )   $ topics = sorted ( topics )   $ print ( _STR_ )
ad_groups_zero_impr . groupby (   $ [ _STR_ , _STR_ ]   $ ) . groups
lsi . print_topics ( 2 )
df [ df [ _STR_ ] == _STR_ ] . query ( _STR_ ) . shape
s = pd . Series ( [ 99 , 5 , 60 ] , index = [ _STR_ , _STR_ , _STR_ ] )   $ df1 . append ( s , ignore_index = True ) # append the series to the data frame. The line 4 is appended $
sns . distplot ( data . song_freq )
y3 , X3 = patsy . dmatrices ( _STR_ , data = df , return_type = _STR_ )   $ model = sm . OLS ( y3 , X3 , missing = _STR_ )   $ fit3 = model . fit ( )   $ fit3 . summary ( )
latest_df . classification_id . nunique ( ) == len ( latest_df )
col_names = list ( zip ( df_test . columns , df_train . columns ) )   $ for cn in col_names :   $ assert cn [ 0 ] == cn [ 1 ]
auth = tweepy . OAuthHandler ( CONSUMER_KEY , CONSUMER_SECRET )   $ auth . set_access_token ( ACCESS_TOKEN , ACCESS_SECRET )   $ api = tweepy . API ( auth )   $ public_tweets = api . home_timeline ( )   $ data = pd . DataFrame ( data = [ tweet . text for tweet in public_tweets ] , columns = [ _STR_ ] )
bigram_sentences = LineSentence ( bigram_sentences_filepath )   $ for bigram_sentence in it . islice ( bigram_sentences , 230 , 240 ) :   $ print _STR_ . join ( bigram_sentence )   $ print _STR_
today_ = datetime . date . today ( ) . strftime ( _STR_ )   $ today_ = _STR_   $ print ( _STR_ + today_ )
def remove_from_word_list ( original_list , list_subset ) :   $ for each in list_subset :   $ idx = original_list . index ( each )   $ del original_list [ idx ]   $ return original_list
save_data = ( D1 , D2 )   $ import pickle   $ with open ( _STR_ , _STR_ ) as f :   $ pickle . dump ( save_data , f )
del merged_portfolio_sp_latest_YTD [ _STR_ ]   $ merged_portfolio_sp_latest_YTD . rename ( columns = { _STR_ : _STR_ } , inplace = True )   $ merged_portfolio_sp_latest_YTD . head ( )
BallBerry_ET_Combine = pd . concat ( [ BallBerry_rootDistExp_1 , BallBerry_rootDistExp_0_5 , BallBerry_rootDistExp_0_25 ] , axis = 1 )   $ BallBerry_ET_Combine . columns = [ _STR_ , _STR_ , _STR_ ]
user . loc [ _STR_ , _STR_ ]
clean_trips . to_csv ( path_or_buf = _STR_ , sep = _STR_ , index_label = False )
avg_reorder_days = prior_orders . groupby ( [ _STR_ ] ) [ _STR_ ] . aggregate ( _STR_ ) . reset_index ( name = _STR_ )   $ avg_reorder_days . head ( )
days_alive = ( datetime . datetime . today ( ) - datetime . datetime ( 1981 , 6 , 11 ) )   $ days_alive . days
met . T . plot ( kind = _STR_ , figsize = ( 5 , 3 ) , xlim = ( 0 , 1 ) )   $ met
twitter_data . tweet_id . unique ( ) . size
df [ _STR_ ] = pd . to_datetime ( df [ _STR_ ] )   $ df . sort_values ( _STR_ , inplace = True )   $ df . reset_index ( drop = True , inplace = True )   $ df . head ( )
train . OPTION . value_counts ( )
typ = [ f . dataType for f in smpl_join . schema . fields ]
data = data . dropna ( ) ; data
missing_nums = df . isnull ( ) . sum ( )   $ print ( missing_nums . iloc [ missing_nums . nonzero ( ) [ 0 ] ] )
import pandas as pd   $ import numpy as np   $ from sklearn . tree import DecisionTreeRegressor
pres_df [ _STR_ ] = pres_df [ _STR_ ] . map ( lambda x : x [ 0 ] )   $ pres_df [ _STR_ ] . head ( )
model . delete_temporary_training_data ( keep_doctags_vectors = True , keep_inference = True )
prod_paudm . columns . values
import pandas as pd   $ pd . read_table ( _STR_ , parse_dates = { _STR_ : [ 0 , 1 , 2 ] } , delim_whitespace = True , na_values = [ _STR_ ] , index_col = _STR_ )   $
def compute_sentence_length_variance ( text_list ) :   $ return np . var ( [ len ( x ) for x in text_list ] )   $ compute_sentence_length_variance ( [ _STR_ , _STR_ , _STR_ , _STR_ ] )
temp = df2 . landing_page . value_counts ( )   $ prob = temp / temp . sum ( )   $ print _STR_ , prob [ 0 ]
utils . read_sas_write_hdf ( source_paths , data_dir , _STR_ , downcast = False , verbose = False )
invoice_hub_dropper = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ for col in invoice_hub_dropper :   $ print invoice_hub [ col ] . value_counts ( )
% % timeit   $ with tb . open_file ( filename = _STR_ , mode = _STR_ ) as f :   $ table = f . get_node ( where = _STR_ )   $ rows = table . where ( _STR_ )   $ amounts = [ x [ _STR_ ] for x in rows ]
cur . fetchall ( )
trainDF . head ( )
autos . columns
beirut [ _STR_ ] . std ( ) , summer [ _STR_ ] . std ( )
bob = np . array ( problems ) [ : , 1 ] . tolist ( )
exportID [ _STR_ ] = exportID [ _STR_ ] + exportID [ _STR_ ]
sc . stop ( )
% matplotlib inline   $ import numpy as np   $ import matplotlib . pyplot as plt   $ import openmc   $ import openmc . mgxs as mgxs
df = df . reset_index ( )   $ df . irlco . sum ( )
time_string = _STR_   $ delta_hours = 1   $ start_time = pd . to_datetime ( time_string )
np . array ( p_diffs )   $ plt . hist ( p_diffs ) ;   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )   $ plt . title ( _STR_ )   $
df . head ( )
df . drop ( _STR_ , axis = 1 , inplace = True ) ; #drops the match column above
pulledTweets_df . sentiment_predicted_lr . value_counts ( ) . plot ( kind = _STR_ ,   $ title = _STR_ )   $ plt . savefig ( _STR_ + _STR_ )
df [ _STR_ ] . apply ( returnCategory ) . value_counts ( )
nzi = pd . notnull ( train_data [ _STR_ ] ) . sum ( )   $ nzr = ( revenue [ _STR_ ] > 0 ) . sum ( )   $ print ( _STR_ , nzi , _STR_ , nzi / train_data . shape [ 0 ] )   $ print ( _STR_ , nzr , _STR_ , nzr / revenue . shape [ 0 ] )
tablename = _STR_   $ pd . read_csv ( read_inserted_table ( dumpfile , tablename ) ,   $ delimiter = _STR_ ,   $ error_bad_lines = False ) . head ( 10 )
old_page_converted = np . random . binomial ( n_old , p_old )   $ print ( _STR_ . format ( old_page_converted ) )
plt . hist ( p_diffs )   $ plt . axvline ( x = 0.000913 , color = _STR_ )
resdf = resdf . drop ( [ _STR_ ] , axis = 1 )   $ resdf . head ( 3 )   $
a = bnbAx [ bnbAx [ _STR_ ] == _STR_ ] . first_browser . value_counts ( ) / len ( bnbAx [ bnbAx [ _STR_ ] == _STR_ ] )   $ a . head ( )
grouper = visits . groupby ( ( visits . address , visits . dba_name ) )
cassession . builtins . serverstatus ( )
loans_act_20150430_xirr = cashflows_act_investor_20150430 . groupby ( _STR_ ) . apply ( lambda x : xirr ( x . payment , x . dcf ) )
autos [ _STR_ ] . describe ( )
product_time = nbar_clean [ [ _STR_ , _STR_ ] ] . to_dataframe ( ) #Add time and product to dataframe $ product_time.index = product_time.index + pd.Timedelta(hours=10) #Roughly convert to local time $ product_time.index = product_time.index.map(lambda t: t.strftime('%Y-%m-%d')) #Remove Hours/Minutes Seconds by formatting into a string
X_train , X_test , y_train , y_test = train_test_split ( X_s_n , y_sc , test_size = 0.5 , random_state = 42 )
df . loc [ 1 , _STR_ ] = _STR_   $ df
f_counts_hour_ip = spark . read . csv ( os . path . join ( mungepath , _STR_ ) , header = True )   $ print ( _STR_ % f_counts_hour_ip . count ( ) )
evaluator . plot_confusion_matrix ( normalize = True ,   $ title = _STR_ ,   $ print_confusion_matrix = False ,   $ figsize = ( 8 , 8 ) ,   $ colors = None )
data . dropna ( how = _STR_ )
top_headlines = newsapi . get_top_headlines ( q = _STR_ , sources = _STR_ , language = _STR_ )   $ top_headlines [ _STR_ ] [ 0 ]
% matplotlib notebook   $ import matplotlib . pyplot as plt
df [ _STR_ ] = df . activity_date_time_c . dt . to_period ( _STR_ )
V0 = 17.6639   $ r = 0.01
twitter_archive_df_clean [ _STR_ ] . sort_values ( ascending = False ) . head ( )
clf = svm . SVR ( )   $
aqmdata . describe ( )
df [ _STR_ ] . head ( )
order_data . shape
logit = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ ] ] )   $ results = logit . fit ( )   $ results . summary ( )
save_n_load_df ( promo_df , _STR_ )
props . value_counts ( )
old_page_converted = np . random . choice ( np . arange ( 2 ) , size = n_old , p = [ ( 1 - p_old ) , p_old ] )   $
df_link_match_unique . to_csv ( _STR_ , index = False )
summer [ [ _STR_ , _STR_ ] ] . plot ( grid = True , figsize = ( 10 , 5 ) )
twitter_ar . rating_num . value_counts ( )
df_2007 . dropna ( inplace = True )   $ df_2007
with open ( _STR_ , encoding = _STR_ ) as data_file :   $ json_data3 = j . load ( data_file )
run txt2pdf . py - o _STR_ _STR_
zip_file . filelist
np . exp ( results . params )
vi = vio2016 . groupby ( [ _STR_ , _STR_ ] ) . count ( ) . reset_index ( )   $ pd . merge ( vio2016 , vi , on = [ _STR_ , _STR_ ] )   $ vi = vi . iloc [ : , : 3 ]   $ vi   $ vio2016 = pd . merge ( vio2016 , vi , on = [ _STR_ , _STR_ ] )
! head . . / . . / data / msft_modified . csv
predictDT_on_test = model_dt . predict ( X_test ) # model.predict_proba(X_test) # for predicting in probabilities $ predictDT_on_test $ predictRF_on_test = model_rf.predict(X_test) # model.predict_proba(X_test) # for predicting in probabilities $ predictRF_on_test
test_data . tail ( )
df [ [ _STR_ , _STR_ ] ] . plot ( figsize = ( 15 , 5 ) ) ;
df2 . query ( _STR_ ) . user_id . size
df_reg = injuries_hour [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ]   $ df_reg [ _STR_ ] = pd . to_datetime ( df_reg . date_time ) . dt . hour   $ df_reg . head ( )   $
data_issues_transitions . head ( )
_STR_ . find ( _STR_ )
for row in my_df_small . iterrows ( ) :   $ print ( row )
image_predictions_df . head ( )
japandata [ _STR_ ] = japandata . Credit_2NonFinSec * 10 ** 9 / japandata . GDP / japandata . Yen2USD
mean_fn = ( lambda x : np . mean ( x ) )
au . clear_dir ( _STR_ )
bacteria2 = pd . Series ( bacteria_dict ,   $ index = [ _STR_ , _STR_ ,   $ _STR_ , _STR_ ] )   $ bacteria2
pd . DataFrame ( result , columns = schema )
no_hyph = df_nona [ df_nona [ _STR_ ] \   $ . apply ( lambda x : len ( x . split ( _STR_ ) ) < 2 ) ] [ _STR_ ] . str . lower ( )   $ no_hyph = no_hyph [ no_hyph . apply ( lambda x : x . split ( ) [ - 1 ] != _STR_ ) ] . replace ( repl_dir )
n_old = df2 [ df2 [ _STR_ ] == _STR_ ] . count ( ) [ 0 ]   $ n_old
train [ _STR_ ] = train [ _STR_ ] . map ( lambda x : 1 if x . weekday ( ) in [ 5 , 6 ] else 0 )   $ test [ _STR_ ] = test [ _STR_ ] . map ( lambda x : 1 if x . weekday ( ) in [ 5 , 6 ] else 0 )
uniqueCreatedTimeByUser = firstWeekUserMerged [ [ _STR_ , _STR_ ] ] . drop_duplicates ( )   $ uniqueCreatedTimeByUser . head ( 5 )
big_df_count = big_df_count . reset_index ( )   $ big_df_avg = big_df_avg . reset_index ( )
z_score , p_value = sm . stats . proportions_ztest ( np . array ( [ convert_new , convert_old ] ) , np . array ( [ n_new , n_old ] ) , alternative = _STR_ )
io2 = ioDF . copy ( )
compound_sub2 = compound_sub2 . append ( compound_wdate_df4 )
df . eval ( _STR_ , inplace = True )   $ df . head ( )
pd . Series ( [ _STR_ , _STR_ , _STR_ ] ) . dtypes
print ( _STR_ , Actual_diff )   $ p_greater_than_diff = len ( greater_than_diff ) / len ( p_diffs )   $ print ( _STR_ , p_greater_than_diff )   $ print ( _STR_ . format ( p_greater_than_diff * 100 ) )
test_df . head ( 10 )
print ( ( X_train_all . select_dtypes ( include = [ _STR_ ] ) . columns . values ) )
cols = [ _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ df = pd . DataFrame ( columns = cols )
train = pd . read_csv ( _STR_ , low_memory = False , index_col = _STR_ )   $ if kaggle :   $ if sim == False :   $ test = pd . read_csv ( _STR_ , low_memory = False , index_col = _STR_ )   $ res = pd . read_csv ( _STR_ , low_memory = False , index_col = _STR_ )
len ( conditions . unique ( ) )
median = np . percentile ( df [ _STR_ ] , 50 )   $ per_75 = np . percentile ( df [ _STR_ ] , 75 )   $ print ( _STR_ , median )   $ print ( _STR_ , per_75 )
year8 = driver . find_elements_by_class_name ( _STR_ ) [ 7 ]   $ year8 . click ( )
@ pyimport numpy as np   $ a = PyObject ( np . array ( 1 : 10 ) )   $ jpyArr = a [ : reshape ] ( 5 , 2 ) # uses the Python reshape method for an ndarray object (not Julia's reshape function)
df . loc [ monthMask , _STR_ ] = df [ _STR_ ] + 1
df3 [ _STR_ ] . value_counts ( )
df_trips . to_csv ( _STR_ , index = False )   $ df_planets . to_csv ( _STR_ , index = False )   $ df_pilots . to_csv ( _STR_ , index = False )   $ df_passengers . to_csv ( _STR_ , index = False )
sns . heatmap ( viscov ,   $ cmap = _STR_ )
obs_diff_mean = new_page_converted . mean ( ) - old_page_converted . mean ( )   $ obs_diff_mean
control_conv_prob = df2 . loc [ ( df2 [ _STR_ ] == _STR_ ) , _STR_ ] . mean ( )   $ control_conv_prob
TEXT . numericalize ( [ md . trn_ds [ 0 ] . text [ : 12 ] ] )
payment = pd . get_dummies ( auto_new . Payment_Option )   $ payment . head ( )
y_pred1 = linreg . predict ( X_test )   $ print np . sqrt ( metrics . mean_squared_error ( y_test , y_pred1 ) )
Station = Base . classes . station   $ Measurement = Base . classes . measurement   $
a . keys ( ) - b . keys ( )
import sys   $ reload ( sys )   $ sys . setdefaultencoding ( _STR_ )
df2 [ df2 . duplicated ( _STR_ ) == True ] . user_id
knn . fit ( train [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] , train [ _STR_ ] )
df [ _STR_ ] = df [ _STR_ ] . rolling ( window = 5 ) . apply ( lambda x : np . sum ( x ) )
non_grad_age_mean = records3 [ records3 [ _STR_ ] == _STR_ ] [ _STR_ ] . mean ( )   $ non_grad_age_mean
git_log . info ( )
all_df . describe ( )
df . head ( 3 )
n_new = df2 . query ( _STR_ ) . user_id . count ( )   $ n_new
logit_mod = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ ] ] )   $ results = logit_mod . fit ( )   $ results . summary ( )
df = pd . read_csv ( _STR_ , parse_dates = [ _STR_ ] , encoding = _STR_ )
np . sin ( df * np . pi / 4 )
probs = alg . predict_proba ( X_test )   $
vol = pd . DataFrame ( raw , columns = [ _STR_ ] )   $ vol . head ( )
unique = df2 . user_id . nunique   $ unique ( )
from sklearn . metrics import log_loss   $ y_pred_proba = y_pred [ _STR_ ]   $ log_loss ( y_test , y_pred_proba )
data . head ( )
autos . describe ( include = _STR_ )
df2 . query ( _STR_ )
pickle . dump ( tfidf_fitted , open ( _STR_ , _STR_ ) )
autos [ _STR_ ] = autos [ _STR_ ] . str . replace ( _STR_ , _STR_ ) . str . replace ( _STR_ , _STR_ ) . astype ( int )   $ autos . rename ( { _STR_ : _STR_ } , axis = 1 , inplace = True )
df2 [ _STR_ ] = 1   $ df2 [ _STR_ ] = pd . get_dummies ( df2 [ _STR_ ] ) [ _STR_ ]   $ df2 . head ( )
print ( model . wv . similarity ( _STR_ , _STR_ ) )   $ print ( model . wv . similarity ( _STR_ , _STR_ ) )
df . location_id . shift ( ) . head ( )
result2 = df . query ( _STR_ )   $ np . allclose ( result1 , result2 )
obj . drop ( _STR_ , inplace = True )
old_page_converted = np . random . choice ( [ 0 , 1 ] , p = [ 1 - p_old , p_old ] , size = n_old )
fld = _STR_   $ df = df . sort_values ( [ _STR_ , _STR_ ] )   $ get_elapsed ( fld , _STR_ )   $ df = df . sort_values ( [ _STR_ , _STR_ ] , ascending = [ True , False ] )   $ get_elapsed ( fld , _STR_ )
wod_df_loc . to_csv ( _STR_ )   $ wod_df_date . to_csv ( _STR_ )   $ hurricanes_df . to_csv ( _STR_ )
master_df [ _STR_ ] = master_df [ _STR_ ] . str . match ( _STR_ ) #creates new boolean column $ human = gen_filter(master_df, 'human?', True) $ mouse = gen_filter(master_df, 'human?', False) $ mouse
converted_control = df2 . query ( _STR_ ) [ _STR_ ] . mean ( )   $ print ( _STR_ . format ( converted_control ) )
def KL ( a , b ) :   $ a = np . asarray ( a , dtype = np . float )   $ b = np . asarray ( b , dtype = np . float )   $ return np . sum ( np . where ( a != 0 , a * np . log ( a / b ) , 0 ) )
vi_ok [ _STR_ ] . values
df1 = pd . DataFrame ( data , index = [ _STR_ , _STR_ , _STR_ , _STR_ ] )   $ print ( df1 )
df_actor [ df_actor . last_name . str . contains ( _STR_ ) ] . sort_values ( by = [ _STR_ , _STR_ ] )
y = df [ _STR_ ] . values   $ y [ 0 : 5 ]
cdate = [ x for x in building_pa . columns if _STR_ in x ]   $ cdate   $
countries_df = pd . read_csv ( _STR_ )   $ df_new = countries_df . set_index ( _STR_ ) . join ( df2 . set_index ( _STR_ ) , how = _STR_ )   $ df_new . groupby ( _STR_ ) . count ( )
pandas_ds = pandas . DataFrame ( data_list , columns = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] )   $ pandas_ds . columns . values . tolist ( )
blah = sub_df [ _STR_ ] . apply ( sentiment_scores )
output = _STR_   $ cursor . execute ( output )   $
clf_RGF_tfidf = RGFClassifier ( max_leaf = 240 ,   $ algorithm = _STR_ ,   $ test_interval = 100 ,   $ verbose = False , ) . fit ( X_traincv_tfidf , y_traincv_tfidf )
ls_other_columns = df_uro . loc [ : , ls_both ] . columns
path = _STR_   $ df = pd . read_excel ( path , sheetname = _STR_ )   $ df . head ( 5 )
df_transactions . info ( )
tweet_text2 = tweets2 [ _STR_ ] . values   $ clean_text2 = [ preprocess_text ( x , fix_unicode = True , lowercase = True , no_urls = True , no_emails = True , no_phone_numbers = True , no_currency_symbols = True ,   $ no_punct = True , no_accents = True )   $ for x in tweet_text2 ]
new_df = new_df . set_index ( _STR_ )
items2 = [ { _STR_ : 20 , _STR_ : 30 , _STR_ : 35 } ,   $ { _STR_ : 10 , _STR_ : 50 , _STR_ : 15 , _STR_ : 5 } ]   $ store_items = pd . DataFrame ( items2 )   $ store_items
hexbin = sns . jointplot ( x = _STR_ , y = _STR_ , data = dta , kind = _STR_ )   $
df2 [ ids . isin ( ids [ ids . duplicated ( ) ] ) ]
np . linspace ( 0 , 10 , 5 ) #5 equally spaced points between 0 and 10
url = _STR_   $ response = requests . get ( url )   $ soup = bs ( response . text , _STR_ )   $ print ( soup . prettify ( ) )
one_station [ _STR_ ] = one_station [ _STR_ ] . apply ( dt . weekday )   $ one_station . head ( )
output2 . count ( )
df1_cols = df1 . columns . tolist ( )   $ print ( df1_cols )
new_group . get_group ( _STR_ )
with open ( _STR_ , _STR_ ) as file :   $ lines = file . readlines ( )   $
keto = pmol . df [ pmol . df [ _STR_ ] == _STR_ ]   $ print ( _STR_ % keto . shape [ 0 ] )   $ keto
austin [ _STR_ ] = austin [ _STR_ ] . dt . round ( _STR_ ) . dt . hour   $ df1 = austin . pivot_table ( index = _STR_ , columns = _STR_ , values = _STR_ , aggfunc = _STR_ )   $ df1 = df1 . reindex_axis ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 )   $ plt . figure ( figsize = ( 16 , 5 ) )   $ sns . heatmap ( df1 , cmap = _STR_ ) . set_title ( _STR_ )
import plotly   $ plotly . tools . set_credentials_file ( username = _STR_ , api_key = _STR_ )
result = Valid_events . copy ( )   $ result . insert ( loc = 1 , column = _STR_ , value = Vy_pred )   $ result
precipitation_df . dropna ( inplace = True )   $ precipitation_df . head ( )
time_hour_for_file_name = 0 #datetime.datetime.now().time().hour
pd . crosstab ( aqi [ _STR_ ] , aqi [ _STR_ ] , normalize = _STR_ , margins = True ) . plot ( kind = _STR_ , stacked = True ) ;
X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 42 )
tweets_df . name . describe ( )
df_h1b_nyc_ft . pw_unit_1 . value_counts ( )
data = data_train . append ( data_test )
df [ _STR_ ] . nunique ( )
print _STR_ , lostintranslation . info ( ) [ _STR_ ] [ 0 ] [ _STR_ ]   $ print _STR_ , lostintranslation . info ( ) [ _STR_ ] [ 0 ] [ _STR_ ]
segmentData . lead_source . value_counts ( )
bigdf_read . loc [ bigdf_read [ _STR_ ] . isnull ( ) ]
intervention_history . reset_index ( inplace = True )   $ intervention_history . set_index ( [ _STR_ , _STR_ ] , inplace = True )
k1 . head ( )
abc = Grouping_Year_DRG_discharges_payments . groupby ( [ _STR_ , _STR_ ] ) . get_group ( ( 2015 , 871 ) )   $ abc . head ( )
education_2011_2015 . head ( )
components3 = pd . DataFrame ( pca . components_ , columns = [ _STR_ , _STR_ , _STR_ ] )
df_notnew = df . query ( _STR_ )   $ df_3 = df_notnew . query ( _STR_ )   $ df_3 . nunique ( )   $
df . head ( )
from sklearn . model_selection import train_test_split   $ X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.1 ) # 30% for testing, 70% for training $ X_train.sample(5)
plt . plot ( ds_issm [ _STR_ ] , ds_issm [ _STR_ ] , _STR_ )   $ plt . title ( _STR_ )   $ plt . ylabel ( _STR_ )   $ plt . xlabel ( _STR_ )   $ plt . show ( )
df2 . query ( _STR_ ) . converted . sum ( ) / df2 . query ( _STR_ ) . converted . count ( )
tweet_archive_clean . head ( 30 )
train . pivot_table ( values = _STR_ , index = _STR_ , aggfunc = np . mean )
print ( festivals . at [ 2 , _STR_ ] )
z = np . round ( sum ( successful_users . values ( ) ) / len ( positive_results ) , 3 ) * 100   $ print ( _STR_ . format ( z ) )
dfcounts . head ( )
from sklearn . model_selection import KFold   $ cv = KFold ( n_splits = 100 , random_state = None , shuffle = True )   $ estimator = Ridge ( alpha = 30000 )   $ plot_learning_curve ( estimator , _STR_ , X_std , y , cv = cv , train_sizes = np . linspace ( 0.2 , 1.0 , 10 ) )
df [ _STR_ ] = pd . to_datetime ( df . date )   $ df . set_index ( _STR_ , inplace = True )
merged_data = pd . merge ( trip_data , weather_data , left_on = [ _STR_ , _STR_ ] , right_on = [ _STR_ , _STR_ ] )
corr = df_pivot . corr ( )
df_all_users [ _STR_ ] = df_all_users [ _STR_ ] . apply ( lambda x : str ( x . lower ( ) . strip ( ) ) )   $ df_all_users [ _STR_ ] = df_all_users [ _STR_ ] . astype ( str )
free_data . groupby ( _STR_ ) [ _STR_ ] . mean ( )
sorted_m3 = m3 . ravel ( )   $ sorted_m3 [ : : - 1 ] . sort ( )   $ sorted_m3 = sorted_m3 . reshape ( 2 , 2 )   $ print ( _STR_ , sorted_m3 )
likes . head ( 3 )
valid_countries = set ( countries [ _STR_ ] . tolist ( ) )   $ allteams = allteams & valid_countries   $ print ( len ( allteams ) )
print ( df2 [ _STR_ ] . value_counts ( ) [ 0 ] / len ( df2 ) )
fig = plt . figure ( )   $ ax = fig . add_subplot ( 1 , 1 , 1 )   $ ax . scatter ( filtered_df [ _STR_ ] , filtered_df [ _STR_ ] )   $ plt . show ( )
run txt2pdf . py - o _STR_ _STR_
trend_de = googletrend [ googletrend . file == _STR_ ]   $ trend_de . head ( )
ndvi_us = ndvi_nc . variables [ _STR_ ] [ 0 , lat_li : lat_ui , lon_li : lon_ui ]   $ np . shape ( ndvi_us )
series1 . cov ?
s = pd . Series ( [ 1 , 3 , 5 , np . nan , 6 , 8 ] )   $ print ( s )
print ( datetime . now ( ) - timedelta ( hours = 1 ) )   $ print ( datetime . now ( ) - timedelta ( days = 3 ) )   $ print ( datetime . now ( ) + timedelta ( days = 368 , seconds = 2 ) )
logit_country = sm . Logit ( df3 [ _STR_ ] , df3 [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ results3 = logit_country . fit ( )
offices = pd . read_csv ( _STR_ )
global batch_sz   $ print ( batch_sz )   $ modelrest = trainModel ( model , trX . reshape ( - 1 , 24 , 1 ) , trY . reshape ( - 1 , 1 ) , 1 , batch_sz , 2 )
infile = open ( _STR_ , _STR_ )   $ contents = infile . read ( )   $ soup = BeautifulSoup ( contents , _STR_ )   $ infile . close ( )
run txt2pdf . py - o _STR_ _STR_
xx = list ( building_pa_prc_zip_loc [ _STR_ ] . unique ( ) )   $ aux_list = [ ]   $ for x in xx :   $ aux_list . append ( ( x , set ( building_pa_prc_zip_loc . permit_type_definition [ building_pa_prc_zip_loc [ _STR_ ] == x ] ) ) )   $ aux_list
autos [ _STR_ ] . value_counts ( )
df_Q123 . head ( )
aa = _STR_   $ aa . split ( _STR_ ) [ 1 ] [ : - 1 ]
len ( train_data [ train_data . fuelType == _STR_ ] )
lm = Logit ( y , X )   $ res = lm . fit ( )
v = variables_df [ variables_df [ _STR_ ] == _STR_ ]   $ variableID = v . index [ 0 ]   $ results = read . getResults ( siteid = siteID , variableid = variableID , type = _STR_ )   $ resultIDList = [ x . ResultID for x in results ]   $ len ( resultIDList )
dcrime_gb . to_csv ( processed_path + _STR_ )
dfs [ 1 ] . head ( )
x = K . placeholder ( dtype = _STR_ , shape = X_train . shape )   $ target = K . placeholder ( dtype = _STR_ , shape = Y_train . shape )   $ W = K . variable ( np . random . rand ( dims , nb_classes ) )   $ b = K . variable ( np . random . rand ( nb_classes ) )
df_goog . Open . resample ( _STR_ ) . plot ( )   $ df_goog . Open . asfreq ( _STR_ , method = _STR_ ) . plot ( )
df_dummy = pd . get_dummies ( data = df_country , columns = [ _STR_ ] )   $ df3 = df2 . set_index ( _STR_ ) . join ( df_dummy . set_index ( _STR_ ) )   $ df3 . head ( )
nullrate = df2 [ df2 . converted == 1 ] . count ( ) [ 0 ] / df2 . count ( ) [ 0 ]   $ nullrate
for row in selfharmm_topic_names_df . iloc [ 4 ] :   $ print ( row )
retweets = df [ ~ df [ _STR_ ] . isnull ( ) ]   $ retweet_pairs = retweets [ [ _STR_ , _STR_ , _STR_ ] ] . groupby ( [ _STR_ , _STR_ ] ) . agg ( { _STR_ : _STR_ } ) . rename ( columns = { _STR_ : _STR_ } )   $ retweet_pairs . reset_index ( inplace = True )   $ retweet_pairs . sort_values ( _STR_ , ascending = False ) . head ( )
np_array = np . random . randint ( 1 , 10 , size = 16 ) . reshape ( 4 , 4 )   $ print ( np_array )   $ df = pd . DataFrame ( np_array )   $ df
df = w . data_handler . get_all_column_data_df ( )   $ df . columns   $ df [ df . SEA_AREA_NAME . isnull ( ) ] . loc [ : , [ _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ]   $
state_samples = np . array ( [ env . observation_space . sample ( ) for i in range ( 10 ) ] )   $ discretized_state_samples = np . array ( [ discretize ( sample , state_grid ) for sample in state_samples ] )   $ visualize_samples ( state_samples , discretized_state_samples , state_grid ,   $ env . observation_space . low , env . observation_space . high )   $ plt . xlabel ( _STR_ ) ; plt . ylabel ( _STR_ ) ; # axis labels for MountainCar-v0 state space
Results_kNN1000d . to_csv ( _STR_ , index = False )
merged . head ( )
def calc_p ( array ) :   $ p = array . mean ( )   $ return p
X_train_dtm = pd . DataFrame ( cvec . fit_transform ( X_train . title ) . todense ( ) , columns = cvec . get_feature_names ( ) )
n_new , n_old = df2 [ _STR_ ] . value_counts ( )   $ print _STR_ , n_new
df1 . shape , df2 . shape , df3 . shape
% run word2vec2tensor . py - i / tmp / doc_tensor . w2v - o cornell - movie
sum ( data . days_repayment . isnull ( ) )
len ( df2 . query ( _STR_ ) ) / len ( df2 )
to_be_predicted_Day5 = 31.23097277   $ predicted_new = ridge . predict ( to_be_predicted_Day5 )   $ predicted_new
dti = pd . to_datetime ( [ _STR_ , _STR_ , _STR_ , None ] )   $ dti
scipy . stats . kruskal ( df3 [ _STR_ ] , df4 [ _STR_ ] )
for j in test_dum . columns :   $ if j not in train_dum . columns :   $ print j
1 / np . exp ( - 0.0150 )   $
quandl . ApiConfig . api_key = input ( _STR_ )   $ exchange = _STR_
my_tree_one = tree . DecisionTreeClassifier ( )   $ my_tree_one = my_tree_one . fit ( features_one , target )
two_day_sample . set_index ( _STR_ , inplace = True )
print ( _STR_ , change . max ( ) )
browser . click_link_by_id ( _STR_ )
for dataset in full_data :   $ dataset [ _STR_ ] = dataset [ _STR_ ] . fillna ( train [ _STR_ ] . median ( ) )   $ train [ _STR_ ] = pd . qcut ( train [ _STR_ ] , 4 )   $ print ( train [ [ _STR_ , _STR_ ] ] . groupby ( [ _STR_ ] , as_index = False ) . mean ( ) )
np_cities = np . array ( cities )
train_session . isnull ( ) . sum ( ) / train_session . shape [ 0 ]
station_count = session . query ( Station ) . count ( )   $ station_count
pred . sample ( 10 )
pd . DataFrame ( data2 . groupby ( _STR_ ) . joy . mean ( ) )
lr = K . constant ( 0.01 )   $ grads = K . gradients ( loss , [ W , b ] )   $ updates = [ ( W , W - lr * grads [ 0 ] ) , ( b , b - lr * grads [ 1 ] ) ]
autos [ autos [ _STR_ ] . between ( 50 , 1000000 ) ] [ _STR_ ] . describe ( )
df . T . describe ( ) . T [ _STR_ ] . plot ( )
station_count = measure_df [ _STR_ ] . nunique ( )   $ station_names = measure_df [ _STR_ ] . unique ( )   $ print ( station_count )   $ print ( station_names )
autos [ _STR_ ] . value_counts ( normalize = True ) . head ( 10 )
gameids . get_new_data ( Season = _STR_ , PlayerOrTeam = _STR_ )   $ gameids . _set_api_parameters ( Sorter = _STR_ )   $ gameids2017 = pd . DataFrame ( gameids . game_list ( ) )   $ gameids2017 . head ( ) [ _STR_ ]
df . isnull ( ) . sum ( )
new_logit_mod = sm . Logit ( df_comb [ _STR_ ] , df_comb [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ new_results = new_logit_mod . fit ( )   $ new_results . summary ( )
import pickle   $ with open ( _STR_ , _STR_ ) as f :   $ tweets , bible_data = pickle . load ( f )
abc . to_csv ( _STR_ , index = False )
autos = autos [ autos [ _STR_ ] . between ( 1 , 150000 ) ]
grouped_dpt . groups # groups
logits = tf . layers . dense ( embed , len ( vocab ) , activation = None ,   $ kernel_initializer = tf . random_normal_initializer ( ) )
from sklearn . metrics import logloss   $ loss = log_loss ( y_val , val_pred , rf . classes_ )   $ mae = sum ( abs ( np . subtract ( y_val , val_pred ) ) ) / len ( y_val )
p_control_conv = df2 . query ( _STR_ ) [ _STR_ ] . mean ( )   $ p_control_conv
twitter_df_clean [ _STR_ ] = _STR_   $ twitter_df_clean . iloc [ 191 , 13 ] = _STR_   $ twitter_df_clean . iloc [ 200 , 13 ] = _STR_   $ twitter_df_clean . iloc [ 191 , 7 ] = _STR_   $ twitter_df_clean . iloc [ 200 , 7 ] = _STR_
linear_prediction = linear_model . predict ( X_test )   $ RandomForestRegressor_prediction = RandomForestRegressor_model . predict ( X_test )   $ group_B_predict = ( linear_prediction + RandomForestRegressor_prediction ) / 2
frame = pd . DataFrame ( data )
df [ _STR_ ] . describe ( )
4.832214765100671 * 298
summary = records . describe ( include = _STR_ )   $ summary
ol . data [ _STR_ ]
combined_factor_df [ _STR_ ] = 1   $ logistic_model = sm . Logit ( combined_factor_df [ _STR_ ] , combined_df [ [ _STR_ , _STR_ , _STR_ ] ] )   $ result = logistic_model . fit ( )
store . head ( )
old_page_converted = np . random . choice ( [ 1 , 0 ] , size = n_old , p = [ np . mean ( [ p_new , p_old ] ) , ( 1 - np . mean ( [ p_new , p_old ] ) ) ] ) . mean ( )   $ old_page_converted
vect = CountVectorizer ( ngram_range = ( 1 , 2 ) )   $ vect . fit_transform ( [ _STR_ , _STR_ ] ) . toarray ( ) , vect . vocabulary_
top20_mosttweeted = top20_df [ _STR_ ] . value_counts ( )   $ top20_mosttweeted = pd . DataFrame ( top20_mosttweeted ) . reset_index ( )   $ top20_mosttweeted . columns = [ _STR_ , _STR_ ]
titanic . survived . head ( )
event_sdf . toPandas ( ) . head ( )
rddRow = rdd . map ( lambda f : Row ( col1 = f ) )   $ df = spark . createDataFrame ( rddRow )   $ df . show ( )
engine = create_engine ( _STR_ )
txns . describe ( )
node_0_value = ( input_data * weights [ _STR_ ] ) . sum ( )   $ node_1_value = ( input_data * weights [ _STR_ ] ) . sum ( )   $ hidden_layer_outputs = np . array ( [ node_0_value , node_1_value ] )   $ output = ( hidden_layer_outputs * weights [ _STR_ ] ) . sum ( )   $ print ( output )   $
yeardf . head ( )
print ( train . age . describe ( ) )   $ print ( len ( train [ train [ _STR_ ] > 90 ] ) ) #jc: 1.2% of age above 90, drop these? team decision $ train[train["age"]>90] $
import statsmodels . api as sm   $ import pandas as pd   $ from patsy import dmatrices
fig , ax = plt . subplots ( figsize = ( 10 , 8 ) )   $ fig = arma_res . plot_predict ( start = 8000 , end = 16000 , ax = ax )   $ legend = ax . legend ( loc = _STR_ )   $ plt . title ( _STR_ )
display ( data . head ( 10000 ) )
from sklearn . metrics import r2_score   $ predictions = automl_feat . predict ( X_test )   $ print ( _STR_ , r2_score ( y_test , predictions ) )
np . where ( [ min ( BID_PLANS_df . iloc [ i ] [ _STR_ ] ) != BID_PLANS_df . iloc [ i ] [ _STR_ ] [ 0 ] for i in range ( len ( BID_PLANS_df ) ) ] )
numeric_cols = [ _STR_ , _STR_ , _STR_ , _STR_ ]   $ filtered_df [ numeric_cols ] . head ( 5 )
df = pd . read_csv ( _STR_ , delimiter = _STR_ )   $ df   $ df . dtypes
tw_clean . sort_values ( _STR_ ) . tail ( 1 )
output = lmp . run ( lammps_exe , _STR_ , return_style = _STR_ )
for df in reader . iterate_over_events ( ) :   $ break   $ df
df2 . reindex_like ( df1 , method = _STR_ )
revs = data_df . text   $ stars = data_df . stars
test_data_features_tfidf . shape
df2 [ _STR_ ] . value_counts ( )
api . get_data ( a = 1 )
dataframe . groupby ( _STR_ ) . daily_worker_count . agg ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] )
for row in cursor . columns ( table = _STR_ ) :   $ print ( row . column_name )
pd . set_option ( _STR_ , None )
datetime . datetime ( 2018 , 2 , 18 )
print ( _STR_ , len ( sakhalin_filtered . genus . unique ( ) ) )
unique_urls . sort_values ( _STR_ , ascending = False ) [ 0 : 50 ] [ [ _STR_ , _STR_ ] ]
df . loc [ _STR_ ] [ _STR_ ]
from sklearn . neural_network import MLPClassifier   $ algorithm = MLPClassifier ( )
price_counts = price . value_counts ( )   $ print ( price_counts )   $ price_counts . sort_index ( ascending = False )   $
to_be_predicted_Day3 = 36.48931367   $ predicted_new = ridge . predict ( to_be_predicted_Day3 )   $ predicted_new
null_desc = raw_data [ ( raw_data [ _STR_ ] . isna ( ) ) ]   $ null_desc . T
mod = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ results = mod . fit ( )   $ results . summary ( )
csvfile = _STR_   $ resdf = pd . read_csv ( csvfile )   $ resdf . info ( )
data . loc [ ( 100 , slice ( None ) , _STR_ ) , : ] . iloc [ 0 : 5 , 0 : 5 ]
fires . columns = map ( str . lower , fires . columns )   $ fires . columns = map ( lambda x : x . replace ( _STR_ , _STR_ ) , fires . columns )   $ fires . head ( 2 ) . T
posts . describe ( )
m_vals = np . linspace ( m_true - 3 , m_true + 3 , 100 )   $ c_vals = np . linspace ( c_true - 3 , c_true + 3 , 100 )
ax = mario_game . groupby ( _STR_ ) [ _STR_ ] . sum ( ) . plot ( )   $ ax . set_ylabel ( _STR_ )
tweets . info ( )
driver . get ( _STR_ )   $
d6 = d5 . T   $ d6
df . dtypes
print ( tfidf_svd_v2 . explained_variance_ratio_ . sum ( ) )
texts = [ re . sub ( _STR_ , _STR_ , text ) for text in texts ]   $ texts = [ re . sub ( _STR_ , _STR_ , text ) for text in texts ]
autos [ _STR_ ] . describe ( )
predictions_count = model_count_NB . predict ( X_test_count )   $ predictions_tfidf = model_tfidf_NB . predict ( X_test_tfidf )
pd . MultiIndex . from_arrays ( [ [ _STR_ , _STR_ , _STR_ , _STR_ ] , [ 1 , 2 , 1 , 2 ] ] )
knn_reg . score ( x_test , y_test )
start = pd . Timestamp ( _STR_ )   $ finish = pd . Timestamp ( _STR_ )   $ time_index = pd . date_range ( start , finish , freq = _STR_ )
counts . index
train . groupby ( by = _STR_ ) . mean ( ) . head ( )
df = df [ [ target_column ] ] . copy ( )   $ base_col = _STR_   $ df . rename ( columns = { target_column : base_col } , inplace = True )
hdf5_file . close
tuna_pos_cnt = tuna_pos . count ( ) * 100   $ print _STR_ \   $ . format ( tuna_pos_cnt , tuna_pos_cnt * 1. / dau )
start_sp = datetime . datetime ( 2013 , 1 , 1 )   $ end_sp = datetime . datetime ( 2018 , 3 , 9 )   $ end_of_last_year = datetime . datetime ( 2017 , 12 , 29 )   $ stocks_start = datetime . datetime ( 2013 , 1 , 1 )   $ stocks_end = datetime . datetime ( 2018 , 3 , 9 )
from pyspark . mllib . evaluation import MulticlassMetrics   $ predictionAndLabel = predictions . select ( _STR_ , _STR_ ) . rdd   $ metrics = MulticlassMetrics ( predictionAndLabel )   $ print metrics . confusionMatrix ( )
scores , metrics = pipeline . test ( ds_eval , _STR_ )   $ print ( _STR_ )   $ display ( metrics )
gbm_grid_rand = H2OGridSearch ( model = model_to_grid ,   $ grid_id = grid_id_name_rand ,   $ hyper_params = gbm_hyperparams_rand ,   $ search_criteria = search_criteria )   $ gbm_grid_rand . train ( x = predictor_columns , y = target , training_frame = train , validation_frame = valid , seed = 1234 )
ks_particpants = kick_projects . groupby ( [ _STR_ , _STR_ , _STR_ , _STR_ ] ) . count ( )   $ ks_particpants = ks_particpants [ [ _STR_ ] ]   $ ks_particpants . reset_index ( inplace = True )
df_new [ [ _STR_ , _STR_ , _STR_ ] ] = pd . get_dummies ( df_new [ _STR_ ] )   $ df_new . head ( )
table_rows = driver . find_elements_by_tag_name ( _STR_ ) [ 4 ] . find_elements_by_tag_name ( _STR_ )   $
conn . fetch ( table = dict ( name = _STR_ , caslib = _STR_ ) , to = 5 )
X = pd . merge ( X , at_menu_id_granularity , left_on = _STR_ , right_on = _STR_ , how = _STR_ )   $ del X [ _STR_ ]
reddit_comments_data . groupBy ( _STR_ ) . agg ( { _STR_ : _STR_ } ) . orderBy ( _STR_ , ascending = False ) . show ( )
data . dropna ( )
train_X , test_X = pd . get_dummies ( train_X ) , pd . get_dummies ( test_X )   $ fin_model = GradientBoostingRegressor ( n_estimators = 105 , max_depth = 8 )   $ fin_model . fit ( train_X , train_Y )   $ Predictions = fin_model . predict ( test_X )   $ Predictions
dfg = df [ df . source == _STR_ ]   $ dfg [ _STR_ ] = first_cluster . labels
from pysumma . utils import utils
to_drop = [ _STR_ , _STR_ ,   $ _STR_ , _STR_ ,   $ _STR_ ]   $ df_clean = df_clean . drop ( to_drop , axis = 1 )
newdf = pd . DataFrame ( { _STR_ : sf . index , _STR_ : sf . values } )
df . loc [ index_name ]
dd2 = cfs . diff_abundance ( _STR_ , _STR_ , _STR_ , random_seed = 2018 )
cursor = db . cursor ( )
print pd . concat ( [ s1 , s4 ] , axis = 1 )
knn_grid . fit ( X_train , y_train )
rt . head ( )
countries_df = pd . read_csv ( _STR_ )   $ df_new = countries_df . set_index ( _STR_ ) . join ( df2 . set_index ( _STR_ ) , how = _STR_ )   $ df_new . head ( )
def get_year ( date ) :   $ try :   $ return re . match ( _STR_ , date ) . group ( 1 )   $ except :   $ return np . nan
sst = f . variables [ _STR_ ]   $ sst
df_y_pred = ( y_age_predict . as_data_frame ( ) )   $ df_y_actual = ( y_age_valid . as_data_frame ( ) )
query_date = datetime . now ( ) . strftime ( _STR_ )
misk_df . head ( 3 )
crsr = cnxn . cursor ( )
convers_con . time_difference . mean ( )
im_clean [ _STR_ ] = im_clean [ _STR_ ] . str . replace ( _STR_ , _STR_ )   $ im_clean [ _STR_ ] = im_clean [ _STR_ ] . str . replace ( _STR_ , _STR_ )   $ im_clean [ _STR_ ] = im_clean [ _STR_ ] . str . replace ( _STR_ , _STR_ )
pandas_candlestick_ohlc ( apple . loc [ _STR_ : _STR_ , : ] , otherseries = [ _STR_ , _STR_ , _STR_ ] )
% % time   $ like_plaintiff = geocoded_df . apply ( lambda x : pratio ( x [ _STR_ ] , x [ _STR_ ] ) , axis = 1 )   $ like_defendant = geocoded_df . apply ( lambda x : pratio ( x [ _STR_ ] , x [ _STR_ ] ) , axis = 1 )
workspace_uuid = ekos . get_unique_id_for_alias ( user_id , _STR_ )
data . fillna ( method = _STR_ )
p_new = df2 . converted . mean ( )   $ p_new
city_holidays_df = ph . loc [ ph [ _STR_ ] == 1 , [ _STR_ , _STR_ , _STR_ ] ] . copy ( ) . reset_index ( drop = True )   $ state_holidays_df = ph . loc [ ph [ _STR_ ] == 1 , [ _STR_ , _STR_ , _STR_ ] ] . copy ( ) . reset_index ( drop = True )   $ nat_holidays_df = ph . loc [ ph [ _STR_ ] == 1 , [ _STR_ , _STR_ ] ] . copy ( ) . reset_index ( drop = True )   $ nat_events_df = ph . loc [ ph [ _STR_ ] == 1 , [ _STR_ , _STR_ ] ] . copy ( ) . reset_index ( drop = True )
tokendata = tokendata . fillna ( 0 )
bc . info ( )
valence_df . head ( )
logit_mod = sm . Logit ( df3_new [ _STR_ ] , df3_new [ [ _STR_ , _STR_ , _STR_ ] ] )   $ results = logit_mod . fit ( )   $ results . summary ( )
print ( v_to_c . time . mean ( ) )
print ( _STR_ )
bigdf [ _STR_ ] = bigdf [ _STR_ ] . apply ( lambda x : x . replace ( _STR_ , _STR_ ) )
df . to_csv ( _STR_ )
PredClass = ForecastModel ( _STR_ , last_dt = _STR_ )
dfSF . head ( )
df [ _STR_ ] . mean ( )
saem_base_url = _STR_   $ saem_women = _STR_   $ saem_women_save = _STR_
df_final_edited_10 = df_final_edited . loc [ df_final [ _STR_ ] == 10 ]   $
cutoff_times . groupby ( _STR_ ) [ _STR_ ] . sum ( ) . sort_values ( ) . tail ( )
app_label = train [ _STR_ ] . unique ( )   $ print ( app_label )
% % timeit   $ scipy . optimize . root ( globals ( ) [ function_name ] , 2 )
pn_and_qty_no_duplicates = dict ( [ ( key , 0 ) for key in pn_and_qty [ _STR_ ] ] )   $ pns = pn_and_qty [ _STR_ ]   $ qtys = pn_and_qty [ _STR_ ]   $ for pn , qty in itertools . izip ( pns , qtys ) :   $ pn_and_qty_no_duplicates [ pn ] += qty
unnormalized_q_i_j = np . exp ( - pairwise_squared_distances_i_j )   $ q_i_j = unnormalized_q_i_j / unnormalized_q_i_j . sum ( axis = 1 )   $ f , ( left , right ) = plt . subplots ( 1 , 2 , figsize = ( 8 , 8 ) )   $ plot_matrix ( p_ij , ax = left , title = _STR_ , vmin = 0 , vmax = 1 )   $ plot_matrix ( q_i_j , ax = right , title = _STR_ , vmin = 0 , vmax = 1 )
reddit . multireddit ( _STR_ , _STR_ )
AAPL . iloc [ : , 0 : 4 ] . plot . hist ( bins = 25 )
import datetime   $ now = datetime . datetime . now ( )   $ diff = now - datetime . timedelta ( hours = 7 )   $ diff . strftime ( _STR_ )
gas_df [ _STR_ ] = pd . to_datetime ( gas_df [ _STR_ ] + _STR_ + gas_df [ _STR_ ] , format = _STR_ )   $ del gas_df [ _STR_ ]   $ gas_df . head ( )
cars . dtypes
autos [ _STR_ ] . unique ( )
chinapostnumber = len ( df [ df . province < 99 ] )   $ chinapostnumber
df . track_popularity . hist ( )
df2 = tier1_df . reset_index ( )   $ df2 = df2 . rename ( columns = { _STR_ : _STR_ , _STR_ : _STR_ } )
d1 = pd . DataFrame ( { _STR_ : [ _STR_ , _STR_ , _STR_ ] , _STR_ : [ 704352 , 673184 , 8537673 ] } )   $ d2 = pd . DataFrame ( { _STR_ : [ _STR_ , _STR_ , _STR_ ] , _STR_ : [ 48.42 , 468.48 , 142.5 ] } )   $ pd . merge ( d1 , d2 )
gDate_vEnergy [ _STR_ ] = gDate_vEnergy . sum ( axis = 1 )   $ gDate_vEnergy
df . head ( )
df2 = df2 . drop_duplicates ( subset = [ _STR_ ] )
df2 . query ( _STR_ )
twitter_archive_clean . drop ( _STR_ , axis = 1 , inplace = True )
print ( _STR_ . format ( dup_id . user_id [ 0 ] ) )
session . commit ( )   $ session . close ( )
df_treatment = df2 [ df2 . group == _STR_ ]   $ sum ( df_treatment . converted == 1 ) / df_treatment . shape [ 0 ]
sent . index . name = _STR_   $ idk = tidy_format . join ( sent , how = _STR_ , on = _STR_ )   $ trump [ _STR_ ] = idk . groupby ( [ _STR_ ] ) . sum ( ) [ _STR_ ]   $ trump [ _STR_ ] = trump [ _STR_ ] . fillna ( 0 )   $ idk   $
donors . loc [ donors [ _STR_ ] == 606 , _STR_ ] . value_counts ( )
googletrend . loc [ googletrend . State == _STR_ , _STR_ ] . head ( )
to_be_predicted_Day5 = 21.20401743   $ predicted_new = ridge . predict ( to_be_predicted_Day5 )   $ predicted_new
df_a . join ( df_b , lsuffix = _STR_ , rsuffix = _STR_ ) # overlapping columns
csvData [ _STR_ ] = pd . to_datetime ( csvData [ _STR_ ] , format = _STR_ , errors = _STR_ )   $ csvData [ _STR_ ] = pd . to_datetime ( csvData [ _STR_ ] , format = _STR_ , errors = _STR_ )   $ csvData [ _STR_ ] = [ d . strftime ( _STR_ ) if not pd . isnull ( d ) else _STR_ for d in csvData [ _STR_ ] ]   $ csvData [ _STR_ ] = [ d . strftime ( _STR_ ) if not pd . isnull ( d ) else _STR_ for d in csvData [ _STR_ ] ]
users_nan = ( users . isnull ( ) . sum ( ) / users . shape [ 0 ] ) * 100   $ users_nan
reg_logit = LogisticRegression ( random_state = 1984 , C = 0.01 )   $ reg_logit . fit ( x_train_advanced , y_train_advanced )
autos [ _STR_ ] . unique ( ) . shape
day = lambda x : x . split ( _STR_ )   $ emotion_big_df [ _STR_ ] = emotion_big_df [ _STR_ ] . apply ( day )
SANDAG_age_df . loc [ ( 2012 , _STR_ ) ]
df = pd . read_csv ( _STR_ )   $ df . head ( 5 )
bd . index . name
df_cs [ _STR_ ] = df_cs [ _STR_ ] . apply ( lambda x : text_cleaners ( x ) )
contin_map_train = contin_preproc ( joined_train )   $ contin_map_valid = contin_preproc ( joined_valid )   $ contin_map_test = contin_preproc ( joined_test_df )   $ contin_cols = contin_map_train . shape [ 1 ]
df_matric = pd . read_excel ( _STR_ )
df_prep6 = df_prep ( df6 )   $ df_prep6_ = pd . DataFrame ( { _STR_ : df_prep6 . index , _STR_ : df_prep6 . values } , index = pd . to_datetime ( df_prep6 . index ) )
from scipy import stats   $ ben_fin [ _STR_ ] = ben_final . groupby ( [ _STR_ ] ) . agg ( { _STR_ : lambda x : stats . mode ( x ) [ 0 ] } )
print len ( data [ data [ _STR_ ] . notnull ( ) ] )
results = session . query ( func . count ( Station . station ) ) . all ( )   $ for result in results :   $ print ( result )
df_tweet_clean = df_tweet . copy ( )   $ df_image_clean = df_image . copy ( )   $ df_clean = df . copy ( )
import matplotlib . pyplot as plt   $ % matplotlib inline
education_data . index = multi_index
selected = features [ features . importance > 0.03 ]   $ selected . sort_values ( by = _STR_ , ascending = False )
query . all ( ) #all() returns a list:
print ( sl . two_measures . sum ( ) )   $ print ( sl . second_measurement . sum ( ) )
@ functions . udf   $ def lowercase ( text ) :   $
X_final_test_3 = X_test_best_coef [ [ c for c in X_test_best_coef . columns if _STR_ not in c ] ]   $ X_training_3 = X_training_best_coef [ [ c for c in X_training_best_coef . columns if _STR_ not in c ] ]
import string   $ def text_process ( text ) :   $ nopunc = [ char for char in text if char not in string . punctuation ]   $ nopunc = _STR_ . join ( nopunc )   $ return [ word for word in nopunc . split ( ) if word . lower ( ) not in stopwords . words ( _STR_ ) ]
density . tail ( )
sdof_resp ( ) # arguments aren't necessary to use the defaults.
df_tweet_json . info ( )
np . exp ( rmse_CSCO )
log_mod = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ ] ] )   $ result = log_mod . fit ( )   $ result . summary ( )
test = spark . read . csv ( os . path . join ( datapath , _STR_ ) , header = True )   $ print ( _STR_ % test . count ( ) )
df . head ( )
z_values , labels = vae_latent ( nn_vae , mnist_train_loader )   $ plt . plot ( z_values [ : , 0 ] , z_values [ : , 1 ] , _STR_ )
df . groupby ( _STR_ ) . sum ( )
all_data_grouped = all_data . groupby ( [ _STR_ , _STR_ ] )   $ daily_cases = all_data_grouped [ _STR_ ] . sum ( )   $ daily_cases . head ( 10 )
df_mes = df_mes [ df_mes [ _STR_ ] == 0.3 ]   $ df_mes . shape [ 0 ]
bwd . head ( 10 )
cohort_retention_df . fillna ( 0 , inplace = True )
df . asfreq ( _STR_ , method = _STR_ )
from fastai . plots import *   $ list_paths = [ _STR_ , _STR_ ]   $ plots_from_files ( list_paths , titles = [ _STR_ , _STR_ ] )
d = corpora . Dictionary . load ( fps . dictionary_fp )   $ c = CorpStreamer ( d , fps . corp_lst_fp , inc_title = _STR_ )   $ bow_c = BOWCorpStreamer ( d , fps . corp_lst_fp , inc_title = _STR_ )
def error ( line , data ) :   $ err = np . sum ( ( data [ : , 1 ] - ( line [ 0 ] * data [ : , 0 ] + line [ 1 ] ) ) ** 2 )   $ return err
data . content
df . head ( )
tweet_ids_twitter_archive . shape [ 0 ]
hasGun = [ _STR_ in tweet for tweet in allData . text ]   $ pd . options . display . max_colwidth = 280   $ pprint ( allData [ hasGun ] . text )   $ pd . options . display . max_colwidth = 50
model_rf = pipeline_rf . fit ( train_data )
old_page_converted = np . random . choice ( [ 1 , 0 ] , size = n_old , p = [ p_old , ( 1 - p_old ) ] )   $ print ( len ( old_page_converted ) )
X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 4 )   $ print ( _STR_ , X_train . shape , y_train . shape )   $ print ( _STR_ , X_test . shape , y_test . shape )
print ( _STR_ )   $ print ( len ( all_attack [ _STR_ ] ) )   $ groups = all_attack [ _STR_ ]   $ df = json_normalize ( groups )   $ df . reindex ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 ) [ 0 : 5 ]
model = models . LdaModel ( corpus , id2word = dictionary , num_topics = 100 )
autos [ _STR_ ] . value_counts ( )
cohort [ _STR_ ] . sum ( )
datacamp . shape
def calc_temps ( start_date , end_date ) :   $ return session . query ( func . min ( Measurement . tobs ) , func . avg ( Measurement . tobs ) , func . max ( Measurement . tobs ) ) . \   $ filter ( Measurement . date >= start_date ) . filter ( Measurement . date <= end_date ) . all ( )   $ print ( calc_temps ( _STR_ , _STR_ ) )
ac_tr_prepared . shape
len ( df . user_id . unique ( ) ) #using unique function to find unique dataset
data_2017_12_14_iberia_negative . reset_index ( ) . text_2 . str . split ( expand = True ) . stack ( ) . value_counts ( ) . head ( )
p = pd . Period ( _STR_ , freq = _STR_ )
labels = [ _STR_ , _STR_ ]   $ shares = [ broken_count , all_count - broken_count ]   $ plt . pie ( shares , explode = ( 0.2 , 0 ) , labels = labels , autopct = _STR_ , shadow = True , )   $ plt . show ( )
df . query ( _STR_ ) . count ( ) [ 0 ]
prob_new_page = len ( df2 . query ( _STR_ ) ) / df2 . shape [ 0 ]   $ print ( _STR_ . format ( prob_new_page ) )
from sklearn . linear_model import LogisticRegression
for c in ccc :   $ notc = ccc [ ccc != c ]   $ for n in notc :   $ ctc [ c ] [ n ] = float ( rtc . loc [ : , rtc . columns . str . contains ( c ) == True ] [ rtc . index . str . contains ( n ) ] . sum ( ) . sum ( ) )
jobs_data . drop_duplicates ( subset = _STR_ , keep = _STR_ , inplace = True )
df2 [ _STR_ ] = 1   $ df2 [ [ _STR_ , _STR_ ] ] = pd . get_dummies ( df2 [ _STR_ ] )
act_diff = df [ df [ _STR_ ] == _STR_ ] [ _STR_ ] . mean ( ) - df [ df [ _STR_ ] == _STR_ ] [ _STR_ ] . mean ( )   $ act_diff   $ p_diffs = np . array ( p_diffs )   $ p_diffs   $ ( act_diff < p_diffs ) . mean ( )
f_ip_os_minute_clicks = spark . read . csv ( os . path . join ( mungepath , _STR_ ) , header = True )   $ print ( _STR_ % f_ip_os_minute_clicks . count ( ) )
users [ _STR_ ] = pd . to_datetime ( users [ _STR_ ] , infer_datetime_format = True )   $ users [ _STR_ ] = pd . to_datetime ( users [ _STR_ ] , format = _STR_ )
tweet_clean . drop ( _STR_ , axis = 1 , inplace = True )
df_mas [ _STR_ ] = map ( lambda x : x . upper ( ) , df_mas [ _STR_ ] )   $ df_mas [ _STR_ ] = map ( lambda x : x . upper ( ) , df_mas [ _STR_ ] )   $ df_mas [ _STR_ ] = map ( lambda x : x . upper ( ) , df_mas [ _STR_ ] )
df_con = pd . concat ( [ df_1 , Xt ] , axis = 1 )
merged_portfolio_sp_latest_YTD_sp = merged_portfolio_sp_latest_YTD_sp . sort_values ( by = _STR_ , ascending = True )   $ merged_portfolio_sp_latest_YTD_sp
xgb = XGBClassifier ( base_score = 0.5 , colsample_bylevel = 1 , colsample_bytree = 1 ,   $ gamma = 0.1 , learning_rate = 0.1 , max_delta_step = 0 , max_depth = 5 ,   $ min_child_weight = 3 , missing = None , n_estimators = 100 , nthread = - 1 ,   $ objective = _STR_ , reg_alpha = 0 , reg_lambda = 1 ,   $ scale_pos_weight = 1 , seed = 0 , silent = True , subsample = 1 )
top_10_4 = scores . loc [ 20 ] . argsort ( ) [ : : - 1 ] [ : 11 ]   $ trunc_df . loc [ list ( top_10_4 ) ]
tweet_archive_enhanced_clean [ tweet_archive_enhanced_clean [ _STR_ ] == tweet_archive_enhanced_clean [ _STR_ ] . max ( ) ]
beta_XOM , alpha_XOM = np . polyfit ( daily_returns [ _STR_ ] , daily_returns [ _STR_ ] , 1 )   $ print ( beta_XOM , alpha_XOM )
with pd . HDFStore ( os . path . join ( data_dir , _STR_ ) ) as hdf :   $ print ( hdf . keys ( ) )
% % sql result_set <<   $ SELECT query_id , state , query   $ FROM runtime . queries   $ LIMIT 2
np . __version__
committees_NNN . info ( )
df . head ( )
for col in grouped_df . get_group ( _STR_ ) . columns :   $ print col
eth = pd . read_csv ( _STR_ )   $ eth . head ( )
autos [ _STR_ ] . value_counts ( normalize = True , dropna = False )   $
y1 = df1 [ _STR_ ]   $ x1 = range ( len ( df1 [ _STR_ ] ) )   $ plt . bar ( x1 , y1 )   $ plt . plot ( x1 , y1 )   $ plt . grid ( )
from sklearn . preprocessing import OneHotEncoder   $ encoder = OneHotEncoder ( ) # Create encoder object $ categDF_encoded = encoder.fit_transform(categDF) # Can't convert this to dense array: too large!
y_predict = clf . predict ( X_test )   $ print ( accuracy_score ( y_test , y_predict ) * 100 )
knn . fit ( data [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ,   \   $ _STR_ , _STR_ , _STR_ ] ] ,   \   $ data [ [ _STR_ ] ] )
most_common_tags = sorted ( tags_counts . items ( ) , key = lambda x : x [ 1 ] , reverse = True ) [ : 3 ]   $ most_common_words = sorted ( words_counts . items ( ) , key = lambda x : x [ 1 ] , reverse = True ) [ : 3 ]   $ grader . submit_tag ( _STR_ , _STR_ % ( _STR_ . join ( tag for tag , _ in most_common_tags ) ,   $ _STR_ . join ( word for word , _ in most_common_words ) ) )
lr = 118813   $ loan_requests [ loan_requests . id_loan_request == lr ]
properati . drop ( inplace = True , labels = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 )
autos [ [ _STR_ , _STR_ , _STR_ ] ] . head ( 5 )
cast_data . columns
counts_df . describe ( )
movies [ _STR_ ] = pd . to_numeric ( movies [ _STR_ ] , errors = _STR_ )
pd . to_datetime ( df )
temp = data . merge ( right = data [ [ _STR_ , _STR_ ] ] . groupby ( _STR_ ) . count ( ) . reset_index ( ) . rename ( columns = { _STR_ : _STR_ } ) , how = _STR_ , on = _STR_ )   $ data [ _STR_ ] = temp [ _STR_ ]
X = bow_transformer . transform ( X )
train_size = int ( train_ . shape [ 0 ] * 0.8 )   $ sentiment_train_tweets = [ ( tweet , sentiment ) for tweet , sentiment in train_ [ [ _STR_ , _STR_ ] ] . values [ : train_size ] ]   $ sentiment_train_tweets_full = [ ( tweet , sentiment ) for tweet , sentiment in train_ [ [ _STR_ , _STR_ ] ] . values ]   $ sentiment_validation_tweets = [ ( tweet , sentiment ) for tweet , sentiment in train_ [ [ _STR_ , _STR_ ] ] . values [ train_size : ] ]   $ sentiment_test_tweets = [ ( tweet , sentiment ) for tweet , sentiment in test_ [ [ _STR_ , _STR_ ] ] . values ]
df [ _STR_ ] . value_counts ( )
sc = SparkContext ( appName = _STR_ )     $ sc . setLogLevel ( _STR_ )
daily_averages . groupby ( _STR_ ) . mean ( ) . plot . bar ( ylim = ( 0 , 4000 ) )
total_rows_in_treatment = ( df2 [ _STR_ ] == _STR_ ) . sum ( )   $ rows_converted = len ( ( df2 [ ( df2 [ _STR_ ] == _STR_ ) & ( df2 [ _STR_ ] == 1 ) ] ) )   $ rows_converted / total_rows_in_treatment
founddocs = fs . find ( { _STR_ : { _STR_ : _STR_ } } )   $ print founddocs . count ( )
Nold = df2 . query ( _STR_ ) . user_id . count ( )   $ Nold
0 * np . nan
cols = list ( df . columns )   $ print ( len ( cols ) )   $ print ( cols )
weather_yvr_dt [ _STR_ ] = pd . to_datetime ( weather_yvr_dt [ _STR_ ] )
crimes_all . info ( null_counts = True )
temp = pd . to_datetime ( df_all [ _STR_ ] , unit = _STR_ )   $ temp = pd . DatetimeIndex ( temp )   $ print ( temp [ 0 : 2 ] )
uber_15 [ _STR_ ] . value_counts ( ) . head ( )
data . name . duplicated ( keep = False ) . sum ( )
pd . concat
properati . loc [ : , _STR_ ]
df2 . head ( )
os_br_colmns = list ( dbdata )   $ os_br_keep = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ os_br_colmns = [ e for e in os_br_colmns if e not in os_br_keep ]   $ dbdata . drop ( dbdata [ os_br_colmns ] , axis = 1 , inplace = True )   $ dbdata
def set_trainability ( model , trainable = False ) :   $ model . trainable = trainable   $ for layer in model . layers :   $ layer . trainable = trainable
obj = pd . Series ( [ 4 , 7 , 3 , - 2 ] )
repos . forked_from = pd . to_numeric ( repos . forked_from )
m = get_rnn_classifier ( bptt , 20 * 70 , c , vs , emb_sz = em_sz , n_hid = nh , n_layers = nl , pad_token = 1 ,   $ layers = [ em_sz * 3 , 50 , c ] , drops = [ dps [ 4 ] , 0.1 ] ,   $ dropouti = dps [ 0 ] , wdrop = dps [ 1 ] , dropoute = dps [ 2 ] , dropouth = dps [ 3 ] )
tbl [ tbl . msno . duplicated ( ) ]
np . nan == np . nan
df [ ( df . salary >= 30000 ) & ( df . year == 2017 ) ]
autos [ _STR_ ] . value_counts ( ) . head ( )
system = am . load ( _STR_ , _STR_ )   $ print ( system )
move_1_herald = sale_lost ( breakfastlunchdinner . iloc [ 1 , 1 ] , 10 )   $ print ( _STR_ + str ( move_34p34h34h - move_1_herald ) )
from pandas . tseries . offsets import BDay   $ pd . date_range ( _STR_ , periods = 5 , freq = BDay ( ) )
gbc . get_params ( ) [ _STR_ ]
import pickle   $ pickle . dump ( knn_reg , open ( _STR_ , _STR_ ) )
print ( y . shape )   $ print ( X . shape )
df2 [ ( ( df2 [ _STR_ ] == _STR_ ) == ( df2 [ _STR_ ] == _STR_ ) ) == False ] . shape [ 0 ]
scaled = scaled . join ( vol , how = _STR_ )
s . iloc [ : 3 ]
crime = crime [ crime . Sex != _STR_ ]
vow [ _STR_ ] = vow . index . day   $ vow [ _STR_ ] = vow . index . month   $ vow [ _STR_ ] = vow . index . year
articles_by_pub . head ( )
% % time   $ text_tw = txt_tweets   $ with open ( _STR_ , _STR_ ) as f :   $ for t in text_tw :   $ f . write ( t + _STR_ )
yr , mo , dd = 2012 , 12 , 21   $ dt . date ( yr , mo , dd )   $ dt . date ( 2012 , 12 , 21 )
players_df . head ( )
Date = pd . to_datetime ( crimes [ _STR_ ] )   $ crimes . index = Date   $ crimes . sort_index ( inplace = True )
itemTable [ _STR_ ] = itemTable [ _STR_ ] . map ( project_link )
twitter_archive_master = pd . merge ( df_clean4 , df_image_tweet2 , on = _STR_ , how = _STR_ )   $ twitter_archive_master . head ( )
df = pd . DataFrame ( { _STR_ : { 0 : _STR_ , 1 : _STR_ , 2 : _STR_ } ,   $ _STR_ : { 0 : 1 , 1 : 3 , 2 : 5 } ,   $ _STR_ : { 0 : 2 , 1 : 4 , 2 : 6 } } )   $ df
data . sort_index ( inplace = True )
dic1 = { _STR_ : [ 1 , 2 , 3 ] , _STR_ : [ _STR_ , _STR_ , _STR_ ] , _STR_ : [ _STR_ , _STR_ , _STR_ ] }   $ ex3 = pd . DataFrame ( dic1 )   $ ex3
full_act_data . plot ( figsize = ( 20 , 8 ) ) ;
unique_users = len ( df [ _STR_ ] . unique ( ) )   $ print ( _STR_ . format ( unique_users ) )
summed . interpolate ( ) # notice all the details in the interpolation of the three columns
from statsmodels . api import Logit   $ model = Logit ( df2 [ _STR_ ] , df2 [ [ _STR_ , _STR_ ] ] )   $ results = model . fit ( )
df_new1 . shape [ 0 ] + df_new2 . shape [ 0 ]   $ print ( _STR_ . format ( df_new1 . shape [ 0 ] + df_new2 . shape [ 0 ] ) )
data = open ( _STR_ ) . read ( )   $ print ( data [ 0 ] )
contractor_merge = pd . merge ( contractor_clean , state_lookup ,   $ on = [ _STR_ ] , how = _STR_ )
vip_reason . to_csv ( _STR_ )
ax = plt . subplot ( 111 )   $ ax . bar ( date_frequency . index , date_frequency . data )   $ ax . xaxis_date ( )   $ plt . show ( )
import statsmodels . api as sm   $ z_score , p_value = sm . stats . proportions_ztest ( [ 135 , 47 ] , [ 1781 , 1443 ] )
print ( _STR_ )   $ df . head ( 2 )
post_sentiment_df_saved = non_blocking_df_save_or_load (   $ post_sentiment_df ,   $ _STR_ . format ( fs_prefix ) )
crimes . describe ( )
df2 . converted [ df2 . group == _STR_ ] . mean ( )
df [ _STR_ ] = df [ _STR_ ] . apply ( lambda x : _STR_ . join ( x ) )
df_data . EXAME . value_counts ( )
sub1 . drop ( _STR_ , axis = 1 , inplace = True )
mod1 = sm . Logit ( ab_new [ _STR_ ] , ab_new [ [ _STR_ , _STR_ , _STR_ ] ] ) # checking if converted depends on these columns - intercept, CA, US $ result1 = mod1.fit() #fitting the new model $ result1.summary()
flight6 . count ( )
df2 = df . copy ( )   $ df2 = df2 [ ( df2 [ _STR_ ] == _STR_ ) & ( df2 [ _STR_ ] == _STR_ ) |   $ ( ( df2 [ _STR_ ] == _STR_ ) & ( df2 [ _STR_ ] == _STR_ ) ) ]
[ x_train . shape , y_train . shape , x_test . shape , y_test . shape ]
fig , axs = plt . subplots ( )   $ axs . plot ( points_df . index , points_df . Lat , _STR_ )   $ fig . autofmt_xdate ( )
df7 = df [ df [ _STR_ ] == 1 ]   $ print ( _STR_ )   $ display ( df7 . groupby ( _STR_ ) [ _STR_ , _STR_ , _STR_ ] . mean ( ) . reset_index ( )   \   $ . rename ( columns = { _STR_ : _STR_ , _STR_ : _STR_ , _STR_ : _STR_ ,   \   $ _STR_ : _STR_ } ) )
sub1 [ _STR_ ] = sub1 . groupby ( [ _STR_ , _STR_ ] ) [ _STR_ ] . transform ( _STR_ )
i = nums . index   $ print ( type ( i ) , i )
s = pd . Series ( rng )
autos [ _STR_ ] . value_counts ( ) . sort_index ( ascending = False ) . head ( 20 )   $
print ( _STR_ , df . Open . max ( ) )   $ print ( _STR_ , df . Open . min ( ) )
transactions [ ( transactions . transaction_date < datetime . strptime ( _STR_ , _STR_ ) ) ]
train . target . sum ( ) == data . shape [ 0 ]
df2 [ _STR_ ] = 1   $ df2 [ [ _STR_ , _STR_ ] ] = pd . get_dummies ( df2 [ _STR_ ] )
hist = pd . merge ( users , transactions . groupby ( _STR_ ) . first ( ) . reset_index ( ) , how = _STR_ , on = _STR_ )   $ hist . head ( )
ayush_sean = df_ayush . join ( df_sean , how = _STR_ ) . fillna ( 0 )   $ ayush_sean
df_train [ _STR_ ] = df_train [ _STR_ ] . dt . day_name ( )   $ df_test [ _STR_ ] = df_train [ _STR_ ] . dt . day_name ( )   $
for Quarter , Data in StockData . groupby ( _STR_ ) :   $ print ( _STR_ . format ( Quarter ) )   $ print ( _STR_ . format ( len ( Data ) ) )   $ print ( Data [ StockNames ] . mean ( ) )   $ print ( )
pd . DataFrame ( lm . coef_ , X . columns , columns = [ _STR_ ] )
Y = _STR_   $ dogscats_h2o [ Y ] = dogscats_h2o [ Y ] . asfactor ( )   $
station_most_active = session . query ( Measurement . station , Station . name ) . group_by ( Measurement . station ) . \   $ order_by ( func . count ( Measurement . tobs ) . desc ( ) ) . first ( )   $ station_most_active
print ( spike_tweets . iloc [ 5000 , 2 ] )
df . injured . value_counts ( )
df . replace ( { 0 : 3 , 1 : 1 } , 99 )
[ k for val in train_x [ 0 ] for k , v in words . items ( ) if v == val ]
kick_data = k_var_state [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ]   $ kick_data . head ( )
s = select ( [ employee ] )   $ result = conn . execute ( s )   $ row = result . fetchall ( )   $ print ( row )
department_df_sub . sum ( axis = 0 ) # non-apply equivalent
flight_cancels . head ( )
multi_col_lvl_df . applymap ( lambda x : np . nan if np . isnan ( x ) else str ( round ( x / 1000 , 2 ) ) + _STR_ ) . head ( 10 )
autos . columns
x0 = np . linspace ( 0 , 5.5 , 200 )   $ pred_1 = 5 * x0 - 20   $ pred_2 = x0 - 1.8   $ pred_3 = 0.1 * x0 + 0.5   $
R16 = INQ2016 . Create_Date . dt . month . value_counts ( ) . sort_index ( )   $ R17 = INQ2017 . Create_Date . dt . month . value_counts ( ) . sort_index ( )   $ R18 = INQ2018 . Create_Date . dt . month . value_counts ( ) . sort_index ( )
data [ _STR_ ] = data [ _STR_ ] . map ( str ) + _STR_ + data [ _STR_ ] . map ( str ) + _STR_ + data [ _STR_ ] . map ( str )           $ data . head ( 5 )
twentyth_movie_mscore = movie_containers [ 20 ] . find ( _STR_ , class_ = _STR_ )   $ print ( sixth_movie_mscore )
! python OpenSeq2Seq / run . py - - config_file = OpenSeq2Seq / example_configs / nmt . json - - logdir = . / nmt - - mode = infer - - inference_out = pred . txt
air_reserve . head ( )
import numpy as np   $ disaster_tables = pd . read_html ( _STR_ , header = 0 )   $ explotions = disaster_tables [ 4 ] # reading from html
data = pd . read_csv ( _STR_ )   $ data . head ( )
import pickle   $ output = open ( _STR_ , _STR_ )   $ pickle . dump ( chambers , output )   $ output . close ( )
bounds . max_latitude
tree_features_df [ _STR_ ] . describe ( )
airbnb_df . groupby ( _STR_ ) . agg ( { _STR_ : [ np . mean , np . median ] , _STR_ : max , _STR_ : np . median } )
pred_probas_over = log_reg_over . predict_proba ( X_test )
df1 . fillna ( value = 5. )
old_page_converted = np . random . choice ( 2 , n_old , p = [ 1 - p_old , p_old ] )   $ print ( old_page_converted )
h . ix [ _STR_ ] . plot ( )
active_station_data = session . query ( Measurement . station , func . count ( Measurement . id ) ) . \   $ filter ( Measurement . station == Station . station ) . \   $ group_by ( Measurement . station ) . order_by ( func . count ( Measurement . id ) . desc ( ) ) . all ( )   $ active_station_data
df2 . drop ( [ _STR_ ] , axis = 1 , inplace = True )   $
failures . head ( )
stn_cnt_df = pd . DataFrame ( stations_des , columns = [ _STR_ , _STR_ ] )   $ stn_cnt_df . head ( )
for col in b_list . columns :   $ print ( _STR_ )
rf_pred = prediction_model . predict ( x_test_scaled )
walmart . end_time
a = a . sort_values ( _STR_ )
df = pd . read_csv ( _STR_ )   $ df = df . drop ( df . columns [ [ 0 ] ] , axis = 1 )   $ print ( shape ( df ) )   $ df [ _STR_ ] = maximum ( 0 , sign ( df [ _STR_ ] ) )   $
df . head ( )
ridgemodel = linear_model . Ridge ( alpha = 1 )   $ fit2 = ridgemodel . fit ( X , y )
print ( _STR_ , gs . best_estimator_ . steps [ 0 ] [ 1 ] . best_idx_ )
import matplotlib . pyplot as plt   $ plt . plot ( df [ _STR_ ] , _STR_ )   $ plt . show ( )
autos [ _STR_ ] . unique ( ) . shape
results_1dRichards , output_R = S_1dRichards . execute ( run_suffix = _STR_ , run_option = _STR_ )
lsi_tfidf . print_topics ( 10 )
HERBARIUM_SEARCH_URL = _STR_
scores . shape
rfc . fit ( Bow_X_train , Bow_y_train )   $ print ( _STR_ , rfc . score ( Bow_X_train , Bow_y_train ) )   $ print ( _STR_ , rfc . score ( Bow_X_test , Bow_y_test ) )   $ print ( _STR_ , cross_val_score ( rfc , Bow_X_test , Bow_y_test , cv = 5 ) )
df . dtypes
p_newpage = df2 . query ( _STR_ ) . user_id . nunique ( ) / df2 . user_id . nunique ( )   $ p_newpage
autos [ _STR_ ] . value_counts ( normalize = True ) . sort_index ( ascending = True ) . head ( 10 )
df_comb [ [ _STR_ , _STR_ , _STR_ ] ] = pd . get_dummies ( df_comb [ _STR_ ] )   $ df_comb . head ( )
df_gene [ _STR_ ] = df_gene [ _STR_ ] . dt . date   $ df_gene . head ( )
airbnb = scores . loc [ 94 ]   $ airbnb_10 = airbnb . argsort ( ) [ : : - 1 ] [ : 11 ]
temp_cat . describe ( )
def execute_statement ( query , database ) :   $ conn = lite . connect ( database )   $ result = pd . read_sql_query ( query , conn )   $ conn . close ( )   $ return result
df2 [ df2 [ _STR_ ] == 1 ] . count ( ) [ 1 ] / df2 . shape [ 0 ]
pold_null = ( df2 [ _STR_ ] == 1 ) . mean ( )   $ pold_null
df1 = pd . DataFrame ( { _STR_ : [ _STR_ , _STR_ ] ,   $ _STR_ : [ _STR_ , _STR_ ] } , index = [ 1 , 2 ] )   $ df2 = pd . DataFrame ( { _STR_ : [ _STR_ , _STR_ ] ,   $ _STR_ : [ _STR_ , _STR_ ] } , index = [ 3 , 4 ] )   $ pd . concat ( [ df1 , df2 ] )
catalog_df = week1_df . append ( week2_df )   $ catalog_df
for idx , row in df_trips . iterrows ( ) :   $ pilot_created = df_pilots . loc [ row [ _STR_ ] , _STR_ ]   $ passenger_created = df_passengers . loc [ row [ _STR_ ] , _STR_ ]   $ min_trip = max ( pilot_created , passenger_created )   $ df_trips . loc [ idx , _STR_ ] = np . random . randint ( min_trip , max_trip )
from sklearn . model_selection import KFold   $ cv = KFold ( n_splits = 200 , random_state = None , shuffle = True )   $ estimator = Ridge ( alpha = 10000 )   $ plot_learning_curve ( estimator , _STR_ , X_std , y , cv = cv , train_sizes = np . linspace ( 0.2 , 1.0 , 10 ) )
t0 = time ( )   $ model = MatrixFactorizationModel . load ( sc , _STR_ )   $ t1 = time ( )   $ print ( _STR_ % ( t1 - t0 ) )
autos_real = autos . loc [ ( autos [ _STR_ ] > 1900 ) & ( autos [ _STR_ ] < 2016 ) , : ]
text_classifier . get_step_params_by_name ( _STR_ )
y = df [ _STR_ ]   $ X = df [ _STR_ ]   $ cvec = CountVectorizer ( stop_words = _STR_ )   $ X = pd . DataFrame ( cvec . fit_transform ( X ) . todense ( ) ,   $ columns = cvec . get_feature_names ( ) )
merged = df2 . set_index ( _STR_ ) . join ( countries . set_index ( _STR_ ) )   $ merged . head ( )
train [ _STR_ ] = ( ( train . url . isnull ( ) ) & ( train . title . str . contains ( _STR_ ) ) ) . astype ( int )   $ train . groupby ( _STR_ ) . popular . mean ( )
merged . head ( )
pd . to_datetime ( next_period_date ) . date ( )
logit_control = sm . Logit ( df2 [ _STR_ ] , df2 [ [ _STR_ , _STR_ ] ] )   $ result_control = logit_control . fit ( )
df2_daily . set_index ( _STR_ ) . resample ( _STR_ , how = _STR_ )
sorted ( entity_relations . items ( ) , key = lambda x : x [ 1 ] , reverse = True )
df . info ( )
df_archive [ _STR_ ] . unique ( ) [ 0 : 10 ]
all_text [ all_text . index == _STR_ ] . sum ( ) . sort_values ( ascending = False ) . head ( 25 )
import statsmodels . api as sm   $ log_mod = sm . Logit ( df2 [ _STR_ ] , df2 [ [ _STR_ , _STR_ ] ] )   $ results = log_mod . fit ( )
lr . fit ( X_train_dtm , y_train )
probs = model2 . predict_proba ( x_test )   $ print probs
new_page_converted = np . random . binomial ( 1 , p = p_new , size = n_new )   $ new_page_converted
treatment_conv = conv_ind . query ( _STR_ ) . shape [ 0 ]   $ treatment_group = df2 . query ( _STR_ ) . shape [ 0 ]   $ print ( _STR_ . format ( treatment_conv / treatment_group ) )
% matplotlib notebook   $ df_var . toPandas ( ) . plot ( )
df_image_clean . info ( )
ab_df . isnull ( ) . sum ( )
auto_new . Hand_Drive . unique ( )
elms_all_0604 = pd . read_excel ( cwd + _STR_ )   $ elms_all_0604 [ _STR_ ] = [ datetime . date ( int ( str ( x ) [ 0 : 4 ] ) , int ( str ( x ) [ 5 : 7 ] ) , int ( str ( x ) [ 8 : 10 ] ) )   $ for x in elms_all_0604 . ORIG_DATE . values ]
df2 [ [ _STR_ , _STR_ ] ] = pd . get_dummies ( df2 [ _STR_ ] )   $ df2 = df2 . drop ( _STR_ , axis = 1 )   $ df2 . head ( )
fig , ax = plt . subplots ( )   $ ax . scatter ( df . artist_popularity , df . artist_followers )   $ ax . set_title ( _STR_ )   $ ax . set_xlabel ( _STR_ )   $ ax . set_ylabel ( _STR_ )
my_df_small . head ( )
doc_duration = doc_duration . resample ( _STR_ ) . sum ( )   $ RN_PA_duration = RN_PA_duration . resample ( _STR_ ) . sum ( )   $ therapist_duration = therapist_duration . resample ( _STR_ ) . sum ( )
store_items . fillna ( 0 )
args = mfclient . XmlStringWriter ( _STR_ )   $ args . add ( _STR_ , _STR_ )   $ args . add ( _STR_ , _STR_ )   $ args . add ( _STR_ , _STR_ )   $ libraries_query = con . execute ( _STR_ , args . doc_text ( ) )
mean2 = df2 [ _STR_ ] . mean ( )   $ mean2
from IPython . display import HTML , display
( df [ _STR_ ] ) . mean ( ) #conversion rate
df2 [ _STR_ ] [ df2 [ _STR_ ] == _STR_ ] . count ( ) / 290584
df_final_ . state . value_counts ( )   $
dump . head ( )
country_dummies = pd . get_dummies ( df2 [ _STR_ ] )
shirt_1 . discount ( .3 )
filepath = os . path . join ( _STR_ , _STR_ )   $ data_nuclear_CH = pd . read_csv (   $ filepath , encoding = _STR_ , header = 0 , index_col = None )
url1 = _STR_ + API_KEY   $ r1 = requests . get ( url1 )
k150_bets_under = [ x [ 1 ] > .6 for x in pred_probas_under_k150 ]
% % timeit - n1 - r2   $ tags = { }   $ for i , el in tqdm ( rentals [ _STR_ ] . iteritems ( ) ) :   $ tags [ i ] = get_rental_concession_2 ( el )   $ T2 = pd . Series ( tags )
df_clean [ _STR_ ] = df_clean [ _STR_ ] . astype ( _STR_ )   $ image_clean [ _STR_ ] = image_clean [ _STR_ ] . astype ( _STR_ )   $ tweet_clean [ _STR_ ] = tweet_clean [ _STR_ ] . astype ( _STR_ )
tree_features_df [ _STR_ ] . isin ( manager . image_df [ _STR_ ] ) . describe ( )   $
daily_returns = ( port_cum_ret - 1 ) . resample ( _STR_ ) . last ( )   $ daily_returns . index = pd . to_datetime ( daily_returns . index . astype ( str ) )
pd . Series ( STATE_COLORS . values ( ) ) . value_counts ( )
! pip3 install http : // download . pytorch . org / whl / cu80 / torch - 0.3 .0 . post4 - cp36 - cp36m - linux_x86_64 . whl   $   ! pip3 install torchvision
df_master = pd . read_csv ( _STR_ )
links_df . to_csv ( _STR_ , index = False )
autos [ _STR_ ] . sort_values ( ascending = False ) . head ( 15 )
import logging   $ logging . basicConfig ( format = _STR_ , level = logging . INFO )   $ rootLogger = logging . getLogger ( )   $ rootLogger . setLevel ( logging . INFO )
session . query ( func . min ( Measurement . tobs ) , func . max ( Measurement . tobs ) , func . avg ( Measurement . tobs ) ) . filter ( Measurement . station == _STR_ ) . all ( )
probability_of_winning = (   $ data_for_model [ data_for_model [ _STR_ ] == 1 ] . count ( ) ) / data_for_model [ _STR_ ] . count ( )   $ probability_of_winning
np . exp ( - 0.0408 ) , np . exp ( 0.0099 )
df_copy = df_groups . join ( df_events . groupby ( [ _STR_ ] ) . created . count ( ) , how = _STR_ , on = _STR_ , lsuffix = _STR_ , rsuffix = _STR_ )   $ df_copy = df_copy . sort_values ( by = _STR_ , ascending = False )   $ df_copy . head ( )
trading . df . tail ( )
pconversion = df2 [ _STR_ ] . mean ( )   $ pconversion
species_count . to_excel ( _STR_ )   $ print ( _STR_ )
unitech_df . dtypes
df_payout = jcp . df_payout . copy ( ) #exotic bets payouts in easier form $ df_result = jcp.df.copy() #race results dataframe
plt . rcParams [ _STR_ ] = False   $ dta_55 . plot ( figsize = ( 15 , 5 ) )   $ plt . show ( )
groupby_imputation = taxi_hourly_df . groupby ( [ taxi_hourly_df . index . month , taxi_hourly_df . index . dayofweek , taxi_hourly_df . index . hour ] ) . mean ( )
df_from_json = pd . read_json ( _STR_ )   $ df_from_json . head ( 5 )
ia = imdb . IMDb ( )   $ response_imdb = ia . search_movie ( _STR_ )   $ response_imdb [ 0 ]   $ for s in response_imdb :   $ print ( s [ _STR_ ] , s . movieID )
content_input_count_hist = cached . map ( lambda x : x [ 1 ] [ _STR_ ] ) . histogram ( range ( 20 ) )   $ draw_histogram ( _STR_ , content_input_count_hist )
df2 [ _STR_ ] . value_counts ( )
prop_users_convert = ( df [ _STR_ ] . mean ( ) ) * 100   $ print ( _STR_ . format ( prop_users_convert ) )
import statsmodels . api as sm   $ convert_old = df2 . query ( _STR_ ) . shape [ 0 ]   $ convert_new = df2 . query ( _STR_ ) . shape [ 0 ]   $ n_old = df2 . shape [ 0 ] - df2 . query ( _STR_ ) . shape [ 0 ]   $ n_new = df2 . query ( _STR_ ) . shape [ 0 ]   $
import matplotlib . pyplot as plt   $ % matplotlib inline   $ import seaborn as sns ; sns . set ( )   $ bikedataframe [ _STR_ ] = y_predit   $ bikedataframe [ [ _STR_ , _STR_ ] ] . plot ( figsize = ( 24 , 16 ) , alpha = 0.5 )
Obama_raw . head ( 3 )
Raw_Forecast . to_csv ( _STR_ , encoding = _STR_ , index = False )
df . sort_values ( by = _STR_ )
! ls - l ~ / . dw / cache / data - society / the - simpsons - by - the - data / latest / data
vacancies [ _STR_ ] = vacancies [ _STR_ ] . apply ( lambda x : x . weekday ( ) + 1 )   $ vacancies [ _STR_ ] = vacancies [ _STR_ ] . apply ( lambda x : x . hour )
df_protest . columns
from sklearn . model_selection import GridSearchCV   $ param_grid = { _STR_ : [ 0.05 , 0.1 ] , _STR_ : [ 40 , 60 , 80 ] }
au . show_frequent_items ( mentions_df , user_names , _STR_ , k = 10 )
data . groupby ( pandas . TimeGrouper ( key = _STR_ , freq = _STR_ )   $ ) . count ( ) [ _STR_ ]
s_n_s_epb_two . rename ( columns = { 0 : _STR_ } , inplace = True )
probs_test = F . softmax ( V ( torch . Tensor ( log_preds_test ) ) ) ;   $
for _word , _count in sorted ( vectorizer . vocabulary_ . items ( ) , key = itemgetter ( 1 ) , reverse = True ) [ : 20 ] :   $ print ( _word , end = _STR_ )   $ print ( _count )
fraud_df = pd . read_csv ( _STR_ )   $ ip_df = pd . read_csv ( _STR_ )
logit4 = sm . Logit ( df3 [ _STR_ ] , df3 [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ result4 = logit4 . fit ( )   $ result4 . summary ( )
final . head ( )
df_twitter_copy . at [ 2335 , _STR_ ] = _STR_   $ df_twitter_copy . at [ 2335 , _STR_ ] = _STR_
interact_lm = sm . Logit ( df3 . converted , df3 [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ interact_result = interact_lm . fit ( )   $ interact_result . summary ( )
QUIDS_wide . drop ( labels = [ _STR_ , _STR_ ] , axis = 1 , inplace = True )
AAPL . info ( )
df = df [ - df . Address . str . contains ( _STR_ ) ]   $ df = df [ df . Address . str . count ( _STR_ ) == 3 ]
age . iloc [ [ 1 , 3 , 0 ] ]
cum_sum_percentage_payments = list ( np . cumsum ( percent_of_total_list ) )   $ print ( _STR_ , type ( cum_sum_percentage_payments ) )   $ cum_sum_percentage_payments [ : 4 ]   $ [ i for i , v in enumerate ( cum_sum_percentage_payments ) if v > 50 ] [ : 10 ]
rankImportance = importanceDF . sort_values ( by = _STR_ , ascending = False )
import numpy as np   $ cities . reindex ( np . random . permutation ( cities . index ) )
datatest . loc [ datatest . place_name == _STR_ , _STR_ ] = - 34.706311   $ datatest . loc [ datatest . place_name == _STR_ , _STR_ ] = - 58.483025
retailDf . registerTempTable ( _STR_ )   $ sqlContext . sql ( _STR_ ) . toPandas ( )
dcAutos [ _STR_ ] . count ( )
def num_missing ( x ) :   $ return sum ( x . isnull ( ) )   $ print ( filtered_df . apply ( num_missing , axis = 0 ) )
c = pd . read_csv ( _STR_ )   $ df3 = df2 . merge ( c , on = _STR_ , how = _STR_ )   $ df3 . head ( )
description = pd . read_csv ( _STR_ , sep = _STR_ , header = None )
missing_info = list ( df_users_first_transaction . columns [ df_users_first_transaction . isnull ( ) . any ( ) ] )   $ missing_info
sns . barplot ( x = _STR_ , y = _STR_ , data = group_by_month )
wuxia_request = requests . get ( _STR_ )   $ wuxia_soup = bs4 . BeautifulSoup ( wuxia_request . text , _STR_ )   $ wuxia_ptags = wuxia_soup . find ( itemprop = _STR_ ) . findAll ( _STR_ )   $ for w_ptag in wuxia_ptags :   $ print ( w_ptag . text )
df_new [ _STR_ ] = 1   $ log_mod = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ results = log_mod . fit ( )   $ results . summary ( )
date_reach_goal = pd . DatetimeIndex ( df [ _STR_ ] [ [ 0 ] ] ) + pd . DateOffset ( days = round ( x ) )   $ date_reach_goal
import datetime   $ print ( _STR_ , datetime . datetime . now ( ) )
df2 . query ( _STR_ ) . shape [ 0 ] / df2 . query ( _STR_ ) . shape [ 0 ]
movies = ( spark . read . format ( _STR_ )   $ . options ( header = True , inferSchema = True )   $ . load ( home_dir + _STR_ )   $ . cache ( ) ) # Keep the dataframe in memory for faster processing
run txt2pdf . py - o _STR_ _STR_
df . head ( 5 )
df2 = df . drop ( control_wrong . index )   $ df2 . drop ( treatment_wrong . index , inplace = True )
def output ( x_tensor , num_outputs ) :   $ return tf . layers . dense ( x_tensor , num_outputs , activation = None )   $ tests . test_output ( output )
to_be_predicted_Day2 = 48.33846849   $ predicted_new = ridge . predict ( to_be_predicted_Day2 )   $ predicted_new
trips_data [ _STR_ ] . plot ( ) # pandas will interact with matplotlib  - default is linechart
expenses_df . melt ( id_vars = [ _STR_ , _STR_ ] , value_vars = [ _STR_ ] )
columns = [ _STR_ ]   $ columns . extend ( list ( cassession . CASTable ( _STR_ ) . fetch ( ) . Fetch [ _STR_ ] ) )   $ columns = _STR_ . join ( [ _STR_ . format ( x ) for x in columns ] )   $ cassession . fedsql . execdirect ( query )
df [ _STR_ ] [ _STR_ ] . mean ( )
dfWeek = dfWeek . groupby ( [ _STR_ , _STR_ ,       $ _STR_ , _STR_ , _STR_ ,     $ _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ ] ) [ [ _STR_ , _STR_ ] ] . sum ( ) . reset_index ( )
df = df . set_index ( _STR_ )
plt . imshow ( corr_matrix , cmap = _STR_ )   $ plt . show ( )
( pd . DataFrame ( click_condition_meta . groupby ( _STR_ ) . count ( ) ) ) . sort_values ( _STR_ , ascending = False ) . head ( 4 )
bacteria2 = pd . Series ( bacteria_dict ,   $ index = [ _STR_ , _STR_ , _STR_ , _STR_ ] )   $ bacteria2
all_tables_df . OBJECT_TYPE . count ( )
autos [ _STR_ ] . unique ( )
sel1 = [ Measurement . date ,   $ func . sum ( Measurement . prcp ) ]   $ all_prcp = session . query ( * sel1 ) . group_by ( Measurement . date ) . all ( )   $
autos = autos . fillna ( { _STR_ : _STR_ } )
I decided to merge the snow data into one snowtotal field that merges snowfall and snow depth measurements .
r [ _STR_ ] . plot ( )
dt . date ( )   $ dt . time ( )
np . exp ( results1 . params )
treino [ treino . sentiment == 1 ] . count ( ) #Tweets com sentimento positivo
novVisits = df_visits [ df_visits . datestamp == _STR_ ]   $ print _STR_ , novVisits . user_visits . values . sum ( ) , _STR_   $ df_totalVisits_day = df_visits . groupby ( [ _STR_ ] , as_index = False ) . sum ( )   $ df_totalVisits_day . sort ( _STR_ , ascending = [ 0 ] ) . head ( )
pd . date_range ( _STR_ , _STR_ )
reddit_comments_data . groupBy ( _STR_ ) . agg ( { _STR_ : _STR_ } ) . orderBy ( _STR_ ) . show ( )
logit_mod = sm . Logit ( df2 [ _STR_ ] , df2 [ [ _STR_ , _STR_ ] ] )   $ results = logit_mod . fit ( )   $ results . summary ( )
logreg = LogisticRegression ( )   $ logreg . fit ( X_train , y_train )
Tue_index = pd . DatetimeIndex ( pivoted . T [ labels == 1 ] . index ) . strftime ( _STR_ ) == _STR_
con = sqlite3 . connect ( _STR_ )   $ df = pd . read_sql_query ( _STR_ , con )   $ con . close ( )   $ df
ins [ _STR_ ] = ins [ _STR_ ] . dt . year   $ ins . head ( 5 )
tweets [ _STR_ ] = ( ( tweets [ _STR_ ] / 7 ) + 1 ) . apply ( np . floor )   $ tweets [ _STR_ ] = tweets [ _STR_ ] + 1 - ( tweets [ _STR_ ] - 1 ) * 7   $ tweets [ _STR_ ] = tweets [ _STR_ ] - ( tweets [ _STR_ ] - 1 ) * 168
pd . DataFrame ( eth_df )
df_users_6 . loc [ df_users_6 [ _STR_ ] == _STR_ , _STR_ ] = _STR_   $ df_users_6 . loc [ df_users_6 [ _STR_ ] == _STR_ , _STR_ ] = _STR_   $ df_users_6 . loc [ df_users_6 [ _STR_ ] == _STR_ , _STR_ ] = _STR_
df_new . to_csv ( _STR_ )
access_logs_df . printSchema ( )
california_averages = california . groupby ( _STR_ ) [ _STR_ ] . mean ( )   $ california_averages . plot ( y = _STR_ , x = _STR_ )
finals . loc [ ( finals [ _STR_ ] == 0 ) & ( finals [ _STR_ ] == 0 ) & ( finals [ _STR_ ] == 0 ) &   $ ( finals [ _STR_ ] == 0 ) & ( finals [ _STR_ ] == 1 ) , _STR_ ] = _STR_
mask = [ ( row in pn_qty [ pn ] [ _STR_ ] ) for row in table_1c . iterrows ( ) ]
df . head ( )
docs_by_topic = tfidfnmf_topics . groupby ( _STR_ )
serious_count_final = 0   $ for row in data :   $ if re . search ( _STR_ , row [ 0 ] ) :   $ serious_count_final = serious_count_final + 1   $ print ( _STR_ + str ( serious_count_final ) )
X_train = train . iloc [ : , features_index ]   $ y_train = train [ _STR_ ]
model . clusterCenters ( )
metadata2 = pd . read_csv ( _STR_ )   $ metadata2
Which_Years_for_each_DRG . loc [ 345 ]   $
engine . execute ( _STR_ ) . fetchall ( )
import statsmodels . formula . api as sm   $ sm_lm = sm . ols ( data = dat , formula = _STR_ ) . fit ( )   $ print ( sm_lm . summary ( ) )   $ print ( _STR_ )   $ print ( sm_lm . params )
X_train , X_test , y_train , y_test = train_test_split ( data . drop ( _STR_ , axis = 1 ) , data [ _STR_ ] , test_size = 0.33 )   $ print ( X_train . shape )   $ print ( y_train . shape )   $ print ( X_test . shape )   $ print ( y_test . shape )
df . info ( )
country = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ for i in country :   $ notus . loc [ notus [ _STR_ ] == i , _STR_ ] = i   $ notus . loc [ notus [ _STR_ ] == i , _STR_ ] = i
df2 . index   $
convert = df [ _STR_ ] . mean ( )   $ print ( _STR_ . format ( round ( convert , 4 ) ) )
station_df . head ( 10 )
logodds . drop_duplicates ( ) . sort_values ( by = [ _STR_ ] ) . plot ( kind = _STR_ )
df . drop ( labels = [ _STR_ , _STR_ , _STR_ ] , axis = 1 , inplace = True )
ab_df . info ( )   $
raw_df . shape
survey . head ( )
plt . figure ( figsize = ( 8 , 5 ) )   $ train_df . groupby ( _STR_ ) . favs . median ( ) . plot . bar ( )   $ plt . title ( _STR_ )   $ plt . xticks ( rotation = _STR_ ) ;
data = pd . Series ( [ 0.25 , 0.5 , 0.75 , 1.0 ] ,   $ index = [ _STR_ , _STR_ , _STR_ , _STR_ ] )   $ data
pf_rdd = sc . parallelize ( [ ( _STR_ , _STR_ , 1. ) , ( _STR_ , _STR_ , 2. ) , ( _STR_ , _STR_ , 0.2 ) , ( _STR_ , _STR_ , - 0.8 ) ] )   $ dfpf = sql . createDataFrame ( pf_rdd , [ _STR_ , _STR_ , _STR_ ] )
svm_tunned . fit ( X_train , y_train )
df . head ( )
plt . plot ( ages , weights , _STR_ , alpha = 0.1 )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )   $
categoryList = df [ _STR_ ] . apply ( returnCategory ) . unique ( )
df . to_csv ( _STR_ , index = False )
prop_conv = df2 [ df2 [ _STR_ ] == 1 ] . shape [ 0 ] / df2 . shape [ 0 ]   $ prop_conv
p_old = df2 [ _STR_ ] . mean ( )   $ print _STR_ , p_old
calls_df . groupby ( [ _STR_ ] ) [ _STR_ ] . mean ( )
for i , correct in enumerate ( correct [ : 9 ] ) :   $ plt . subplot ( 3 , 3 , i + 1 )   $ plt . imshow ( X_test [ correct ] . reshape ( 28 , 28 ) , cmap = _STR_ , interpolation = _STR_ )   $ plt . title ( _STR_ . format ( predicted_classes [ correct ] , y_true [ correct ] ) )   $ plt . tight_layout ( )
merged_df . reset_index ( _STR_ , inplace = True )
pd . concat ( [ A , B ] , axis = 1 )
cashflows_act_arrears_investor [ ( cashflows_act_arrears_investor . id_loan == 675 ) & ( cashflows_act_arrears_investor . fk_user_investor == 38 ) ] . to_clipboard ( )
timelog = timelog . drop ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 )
rsi = rs . index [ 120 ]   $ rsi
country_with_least_expectancy = le_data . idxmin ( axis = 0 )   $ country_with_least_expectancy
% % time   $ df [ _STR_ ] = pd . to_datetime ( df [ _STR_ ] , format = _STR_ )
most_recent_investment . head ( )   $ most_recent_investment . columns = [ _STR_ , _STR_ ]
titanic . loc [ index_strong_outliers , : ] . head ( )
1 / np . exp ( - 0.0175 ) , 1 / np . exp ( - 0.0469 ) , np . exp ( - 0.0057 ) , np . exp ( 0.0314 )
for k , d in dataset_dict . items ( ) [ : 10 ] :   $ print _STR_ , k   $ print _STR_ , d   $ print _STR_
from scipy . stats import norm   $ norm . ppf ( 1 - ( 0.05 / 2 ) )
result = customer_visitors . groupby ( _STR_ ) . mean ( ) . astype ( int )   $ result   $
import matplotlib . pyplot as plt   $ df [ [ _STR_ , _STR_ ] ] . set_index ( _STR_ ) . plot ( )
twitter_data = { }   $ for target_user in target_user_list :   $ public_tweets = api . user_timeline ( target_user , count = 100 , result_type = _STR_ )   $ twitter_data [ target_user ] = public_tweets
users = list ( db . osm . aggregate ( [ { _STR_ : { _STR_ : _STR_ , _STR_ : { _STR_ : 1 } } } , { _STR_ : { _STR_ : - 1 } } ] ) )   $ print _STR_ , len ( users )   $ users [ : 80 ]   $
df_twitter_copy = df_twitter_copy . drop ( [ _STR_ , _STR_ , _STR_ ] , axis = 1 )
train . head ( 3 )
df_estimates_false . groupby ( _STR_ ) . count ( )   $
temperature . head ( )
iowa . describe ( )
df_mas [ _STR_ ] = df_mas . rating_numerator + _STR_ + df_mas . rating_denominator
df_master . to_csv ( _STR_ , encoding = _STR_ , index = False )
df . to_csv ( _STR_ )
data2 [ _STR_ ] = data2 . sales
np . ones ( [ 2 , 3 ] ) #There's also np.zeros, and np.empty (which results in an uninitialized array).
l = pd . read_sql_query ( QUERY , conn )   $ l   $
df . drop ( [ _STR_ ] , axis = 1 , inplace = True )
df2 = df [ ( ( df [ _STR_ ] == _STR_ ) == ( df [ _STR_ ] == _STR_ ) ) == True ] . copy ( )   $ df2 . shape
split_pct = 0.75   $ train_test_cut_period = int ( len ( data ) * split_pct ) #split point $ train_set = data[:train_test_cut_period].copy() $ test_set = data[train_test_cut_period:].copy()
df3 [ [ _STR_ , _STR_ , _STR_ ] ] = pd . get_dummies ( df3 [ _STR_ ] )   $ df3 . head ( )   $
df [ _STR_ ] = pd . to_datetime ( df [ _STR_ ] , errors = _STR_ )   $ df . head ( )
FAFRPOSRequest = requests . get ( FAFRPOS_pdf )   $ print ( FAFRPOSRequest . text [ : 10000 ] )
sns . heatmap ( temp_us )
df4 . dtypes
train_Features , test_Features , train_species , test_species = train_test_split ( Features , species , train_size = 0.5 , random_state = 0 )
merged . isnull ( ) . sum ( )
dsd_cpi = _STR_   $ xml_cpi = requests . get ( dsd_cpi ) . content   $ tree_cpi = etree . fromstring ( xml_cpi )
from sklearn . feature_extraction . text import TfidfVectorizer   $ from sklearn . neighbors import KNeighborsClassifier   $ tfidf_vectorizer = TfidfVectorizer ( stop_words = _STR_ , max_df = 0.7 )
temperature_2016_df = pd . DataFrame ( Temperature_year ) #.set_index('date') $ temperature_2016_df.head()
walk . resample ( _STR_ ) . first ( )
print ( _STR_ )   $ relationships = lift . get_all_pre_relationships ( )   $ print ( len ( relationships ) )   $ df = json_normalize ( relationships )   $ df . reindex ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 ) [ 0 : 5 ]
df . info ( )   $ total_rows = df . shape [ 0 ]   $ print ( _STR_ . format ( total_rows ) )
from sentiment_impact import measure_impact   $ ex0 = np . array ( [ .01 , - .03 , - .04 , .5 , .6 , .8 , .82 ] )   $ plt . plot ( range ( len ( ex0 ) ) , ex0 )   $ plt . show ( )   $ print ( _STR_ . format ( measure_impact ( ex0 ) ) )
musk . tail ( )
score = log_reg . score ( X_test , y_test )   $ print ( score )
a = df2 [ _STR_ ] == _STR_   $ n_new = df2 [ a ]   $ n_new = n_new . converted . count ( )   $ print ( _STR_ . format ( n_new ) )
df . head ( 20 )
g = train_df . groupby ( _STR_ )   $ g . describe ( include = _STR_ )
baseball_subdomain_id = 4   $ url = form_url ( _STR_ )   $ response = requests . get ( url , headers = headers )   $ print_body ( response , skip_audit_info = True )
df_subset . boxplot ( column = _STR_ , by = _STR_ , rot = 90 )   $ plt . show ( )
autos [ _STR_ ] . unique ( ) . shape
df . dtypes
f_counts_week_os . show ( 1 )
import test_package . package_within_package
jpl_url = _STR_   $ browser . visit ( jpl_url )
crimes . head ( )
print ( _STR_ )   $ print ( len ( all_attack [ _STR_ ] ) )   $ techniques = all_attack [ _STR_ ]   $ df = json_normalize ( techniques )   $ df . reindex ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 ) [ 0 : 5 ]
print ( df_result . head ( 50 ) )
for column in list1 :   $ election_data [ column ] . fillna ( election_data . groupby ( _STR_ ) [ column ] . transform ( _STR_ ) , inplace = True )   $
l = data2Scaled . corr ( ) [ _STR_ ]   $ pd . DataFrame ( data = { _STR_ : l . index . values [ : - 1 ] , _STR_ : l . values [ : - 1 ] } , index = ( range ( 1 , 19 ) ) )
building_pa_prc_shrink . dtypes
df . last_name . isnull ( ) . sum ( )
inputPath1 = _STR_   $ vertices = sqlContext . read . options ( header = _STR_ , inferSchema = _STR_ ) . csv ( inputPath1 )   $ vertices . show ( 5 )
autos [ _STR_ ] . value_counts ( ) . sort_index ( ascending = False ) . head ( 20 )
youTubearray = youTubeTitles . values   $ youTubearray
lemmed = [ WordNetLemmatizer ( ) . lemmatize ( w , pos = _STR_ ) for w in lemmed ]   $ print ( lemmed )
df . groupby ( _STR_ ) [ _STR_ ] . nunique ( )
url = _STR_   $ browser . visit ( url )
train_b , valid_b , test_b = df_b . split_frame ( [ 0.7 , 0.15 ] , seed = 1234 )   $ valid_b . summary ( )
df_train [ _STR_ ] . unique ( )
df_ad_airings_4 . columns
tweets_clean . rating_numerator . value_counts ( )
print ( _STR_ , datetime . datetime . utcnow ( ) , _STR_ ) # timezone can't be detected from browser
treatment [ _STR_ ] . sum ( ) / treatment . shape [ 0 ]
IMDB_LABEL = data . Field ( sequential = False )   $ splits = torchtext . datasets . IMDB . splits ( TEXT , IMDB_LABEL , _STR_ )
from sqlalchemy import update   $ stmt = update ( employee ) . where ( employee . c . id == 1 ) . values ( age = 49 )   $ stmt1 = update ( employee ) . where ( employee . c . id == 1 ) . values ( marital_status = _STR_ )   $ conn . execute ( stmt )   $ conn . execute ( stmt1 )
re_json = search_response . json ( )
label = LabelEncoder ( )   $ important_tweets [ _STR_ ] = label . fit_transform ( important_tweets [ _STR_ ] )
g = pd . read_csv ( _STR_ )   $ g . head ( 2 )
df_columns [ df_columns . index . month . isin ( [ 11 , 12 , 1 ] ) ] [ _STR_ ] . value_counts ( ) . head ( )   $
df [ _STR_ ] = df [ _STR_ ] . apply ( nltk . word_tokenize )   $ print ( df [ _STR_ ] )
df_parties = df [ df [ _STR_ ] == _STR_ ]   $ df_parties . groupby ( df_parties . index . hour ) [ _STR_ ] . count ( ) . plot ( kind = _STR_ )   $
df2 . query ( _STR_ )
data . loc [ data . rooms . notnull ( ) , _STR_ ] = data [ _STR_ ] * data [ _STR_ ]
pd . Series ( sales , index = index )
linkNYC = gpd . GeoDataFrame ( linkNYC , geometry = _STR_ )   $ linkNYC . head ( )
venues_df . tail ( 15 )
print ( df [ _STR_ ] . nunique ( ) )
age . sort_index ( )
tia [ _STR_ ] = pd . to_datetime ( tia [ _STR_ ] )
import pandas as pd   $ tuner = sagemaker . HyperparameterTuningJobAnalytics ( tuning_job_name )   $ full_df = tuner . dataframe ( )   $ full_df
difference = ( actual_diff < p_diffs ) . mean ( )   $ perc = difference * 100   $ print ( _STR_ . format ( perc ) )
train . corr ( )
sales [ _STR_ ] = pd . to_datetime ( sales [ _STR_ ] , format = _STR_ )
log_2 . columns
X_train . info ( )
mean_price = pd . Series ( average_price )   $ mean_mileage = pd . Series ( average_mileage )   $ top_autos_stats = pd . DataFrame ( mean_price , columns = [ _STR_ ] )   $ top_autos_stats [ _STR_ ] = mean_mileage   $ top_autos_stats
data_households = { }   $ for household_name , household_dict in households . items ( ) :   $ data_households [ household_name ] = pd . read_pickle ( _STR_ + household_dict [ _STR_ ] + _STR_ )
n_treatment = df2 . query ( _STR_ ) . shape [ 0 ]   $ n_treatment
plt . figure ( figsize = ( 6 , 6 ) )   $ kmeans . fit_predict ( X_std )   $ plt . scatter ( X_std [ : , 0 ] , X_std [ : , 1 ] , c = kmeans . labels_ , cmap = _STR_ )
update_date = groups . max ( )
data_spd = pd . DataFrame ( )   $ data_spd [ _STR_ ] = np . array ( tweet_spd )   $ data_spd . head ( n = 3 )
from keras . utils import np_utils   $ y_label_train_OneHot = np_utils . to_categorical ( y_resampled )   $ y_label_test_OneHot = np_utils . to_categorical ( Vycnn , 4 )   $ y_label_test_OneHot . shape
processed_tweets_with_obs = pd . read_csv ( _STR_ )   $ processed_tweets_with_obs . head ( )
df_new [ _STR_ ] . sum ( )
df [ _STR_ ] = df [ _STR_ ] . apply ( lambda x : days [ x ] )
frame . apply ( f )
df [ _STR_ ] . unique ( )
df [ _STR_ ] = df . created_at . astype ( np . int64 )   $ tweetVolume ( df )   $ print ( _STR_ )
g = logs . groupby ( logs . fm_ip )   $ type ( g )   $
x = pd . Series ( range ( 2 ) , dtype = int )   $ x
autos [ _STR_ ] . str [ : 10 ] . value_counts ( normalize = True , dropna = False ) . sort_index ( ascending = True )
s . index
contribs . head ( )
autos [ _STR_ ] = autos [ _STR_ ] . str [ : 10 ] . str . replace ( _STR_ , _STR_ ) . astype ( int )   $ autos [ _STR_ ] = autos [ _STR_ ] . str [ : 10 ] . str . replace ( _STR_ , _STR_ ) . astype ( int )   $ autos [ _STR_ ] = autos [ _STR_ ] . str [ : 10 ] . str . replace ( _STR_ , _STR_ ) . astype ( int )   $ autos . head ( )
total_sales = total_sales . dropna ( ) # drop any Na values from the stores that closed
pd . Series ( [ 100 , 100 ] ) . rank ( )
building_pa_prc . describe ( include = _STR_ )
df . is_shift . value_counts ( )
mnnb = MultinomialNB ( )   $ mnnb . fit ( X_train_dtm , y_train )
new_items = [ { _STR_ : 20 , _STR_ : 30 , _STR_ : 35 , _STR_ : 4 } ]   $ new_store = pd . DataFrame ( new_items , index = [ _STR_ ] )   $ new_store
data = pd . read_csv ( _STR_ , sep = _STR_ , header = None , index_col = 0 )   $ data . columns = [ _STR_ , _STR_ , _STR_ ]
ip_clean . tweet_id . dtype
n_new = treatment . shape [ 0 ]   $ print ( n_new )
import sys   $ sys . path . append ( _STR_ )   $ from common . download_utils import download_week1_resources   $ download_week1_resources ( )
lt = time . localtime ( )   $ print ( _STR_ . format ( lt . tm_hour , lt . tm_min , lt . tm_sec ) )   $ print ( time . asctime ( ) )
speeches_df4 . shape
q_all_pathdep = c . retrieve_query ( _STR_ )   $ q_all_pathdep . metadata ( )
ridgemodel . score ( X , y )
df4 [ _STR_ ] = 1   $ logistic_reg = sm . Logit ( df4 [ _STR_ ] , df4 [ [ _STR_ , _STR_ ] ] )   $ results = logistic_reg . fit ( )   $ results . summary ( )
feature_cols = list ( train . columns [ 7 : - 1 ] )   $ feature_cols
log_returns = np . log ( pf_data / pf_data . shift ( 1 ) )
plt . bar ( 0 , data , width = 1 , yerr = [ [ data - error [ 0 ] ] , [ error [ 1 ] - data ] ] )   $ plt . title ( _STR_ )   $ plt . ylabel ( _STR_ )   $ plt . xticks ( [ ] )   $ plt . show ( )
csvFile = open ( _STR_ , _STR_ )   $ csvWriter = csv . writer ( csvFile )
scores_firstq = np . percentile ( raw_scores , 25 )   $ scores_thirdq = np . percentile ( raw_scores , 75 )   $ print ( _STR_ . format ( scores_firstq , scores_thirdq ) )
now = pd . Timestamp ( _STR_ )   $ now , now . tz is None
cityID = _STR_   $ for tweet in tweepy . Cursor ( api . search , q = q , contained_within = cityID , wait_on_rate_limit = True , lang = _STR_ ) . items ( 500 ) :         $ Dallas . append ( tweet )
sim_diff = new_page_converted . mean ( ) - old_page_converted . mean ( )   $ sim_diff
trigram_dictionary = prep . get_corpus_dict ( from_scratch = False ) :
results_simpleResistance , out_file1 = S . execute ( run_suffix = _STR_ , run_option = _STR_ )
nullCity . groupby ( _STR_ )
active_num_authors_by_project = active_distinct_authors_latest_commit . groupBy ( _STR_ ) . agg ( F . count ( _STR_ ) . alias ( _STR_ ) )   $ active_num_authors_by_project . cache ( )   $ active_num_authors_by_project . show ( )
new_columns = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ print ( len ( new_columns ) == len ( df . columns ) )   $ df . columns = new_columns   $ df . head ( 2 )
corpus [ _STR_ ] = corpus [ _STR_ ] . apply ( stem_func )
dict_urls = json . loads ( df . loc [ row , _STR_ ] )   $ pprint ( dict_urls )
for active_add_date in daterange :   $ active_add_rows = active_df [ active_df [ _STR_ ] == active_add_date ]   $ cohort_active_activated_df . loc [ active_add_date , active_add_date : ] = len ( active_add_rows )
df = pd . read_pickle ( _STR_ + sep + _STR_ + sep + _STR_ )
plt . hist ( threeoneone_census_complaints [ threeoneone_census_complaints [ _STR_ ] > 0 ] [ _STR_ ] , bins = 100 )   $ plt . show ( )
search [ _STR_ ] = ( search [ _STR_ ] - search [ _STR_ ] ) . dt . days
all_df . info ( )
sns . set_style ( _STR_ )   $ sns . distplot ( data_final [ _STR_ ] , kde = False , color = _STR_ ) #, bins=20) $
df [ _STR_ ] . nunique ( )
cfModel . fit ( [ uids , mids ] , rates , nb_epoch = 50 , validation_split = .1 , callbacks = callbacks , verbose = 2 )
new_page_converted = np . random . choice ( [ 1 , 0 ] , size = n_new , p = [ p_mean , ( 1 - p_mean ) ] )   $ new_page_converted . mean ( )
from sklearn import tree   $ clf = tree . DecisionTreeClassifier ( criterion = _STR_ , splitter = _STR_ , max_depth = 3 , min_samples_split = 2 , max_features = 4 )   $ clf = clf . fit ( X , predict )   $ tree . export_graphviz ( clf , out_file = _STR_ , feature_names = features )
dictOfWellDf = pd . read_pickle ( _STR_ )
damd = pd . read_csv ( _STR_ )   $ damd . columns
unique_top_tracks = df_track . merge ( df_artist , on = [ _STR_ , _STR_ ] ) . sort_values (   $ by = _STR_ ,   $ ascending = False ) [ [ _STR_ , _STR_ , _STR_ ] ] . drop_duplicates ( subset = _STR_ ,   $ keep = _STR_ ) . head ( 5 )   $ unique_top_tracks
filtered_df [ filtered_df [ _STR_ ] == 0 ] . shape [ 0 ]
archive . rating_numerator . value_counts ( )
dtm = vectorizer . fit_transform ( df [ _STR_ ] )
t0 = time . time ( )   $ model . evaluate ( x = X_dev . reshape ( shapeX ) , y = Y_dev , verbose = False )   $ dt = time . time ( ) - t0   $ print ( _STR_ . format ( len ( X_dev ) / dt ) )   $
manager . image_df [ manager . image_df [ _STR_ ] == _STR_ ]   $
plt . rcParams [ _STR_ ] = False   $ dta_57 . plot ( figsize = ( 15 , 5 ) )   $ plt . show ( )
users . rename ( columns = { _STR_ : _STR_ } , inplace = True )   $ users . head ( )
df2 [ _STR_ ] = 1   $ df2 [ _STR_ ] = pd . get_dummies ( df2 [ _STR_ ] ) [ _STR_ ]   $ df2 . head ( )
full_act_data = steps . join ( heart , how = _STR_ )
twitter_ar = pd . read_csv ( _STR_ )
ind = np . arange ( len ( feature_importances ) )   $ plt . bar ( ind , feature_importances , .35 )
trips_data [ _STR_ ] = pd . to_datetime ( trips_data [ _STR_ ] )   $ trips_data [ _STR_ ] = trips_data [ _STR_ ] . dt . weekday
dts = [ _STR_ , _STR_ ]   $ dates = pd . to_datetime ( dts , dayfirst = True )   $ dates
log_mod_results . summary ( )
clf = LogisticRegression ( fit_intercept = True ) . fit ( X , y )
df_new = countries_df . set_index ( _STR_ ) . join ( df . set_index ( _STR_ ) , how = _STR_ )   $ df_new . head ( )
import time   $ time . time ( )
treatment_conv = df2 . query ( _STR_ ) [ _STR_ ] . mean ( )   $ treatment_conv
df_CLEAN1A [ _STR_ ] . min ( )
treatment_set , treatment_mapping , max_treatment_length = parse_treatment_definitons ( open ( _STR_ , _STR_ ) )
top50 = pd . DataFrame ( popCon . groupby ( by = _STR_ ) . sum ( ) )   $ top50 . columns = [ _STR_ ]   $ top50 . sort_values ( by = _STR_ , ascending = False ) . head ( 50 )
change_high_low_list = [ ]   $ for m in data2 :   $ if m [ 2 ] != None and m [ 3 ] != None :   $ change = m [ 2 ] - m [ 3 ]   $ change_high_low_list . append ( change )
HOU = pd . read_excel ( url_HOU ,   $ skiprows = 8 )
import pandas as pd   $ df = pd . read_csv ( _STR_ )   $ df . head ( )
import pandas as pd   $ col_names = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ df = pd . read_csv ( _STR_ , sep = _STR_ , header = None , names = col_names , compression = _STR_ , comment = _STR_ , low_memory = False )
data = pd . Series ( [ 0.25 , 0.5 , 0.75 , 1.0 ] ,   $ index = [ 2 , 5 , 3 , 7 ] )   $ data
avg_per_seat_price [ _STR_ ] # This is the average price of a BAL PSL across all transactions.
movie2000rating . to_csv ( _STR_ )
df1 . join ( df2 )
austin . head ( )
df_master . head ( )
con = presto . Connection ( host = cred [ _STR_ ] , port = 80 )
conglist = billstargs . congress . tolist ( )   $ billlist = billstargs . bill_id . tolist ( )   $ billlist = [ s [ : - 4 ] for s in billlist ] # Origninal data had a 4 digit add-on to denote Congress number. Hence the -4. $ print len(billlist)
autos . loc [ : , _STR_ ] = pd . to_datetime ( autos . loc [ : , _STR_ ] )   $ autos . loc [ : , _STR_ ] = pd . to_datetime ( autos . loc [ : , _STR_ ] )
active_fire_zone_df = census_pd_complete [ census_pd_complete . county . isin ( [ _STR_ , _STR_ ] ) ]   $ active_fire_zone_df   $
rows , columns = df . shape #. To return the number of cells $ print('There are {} total records in the dataset with {} no.of columns in it.'.format(rows-1, columns))
from sklearn . model_selection import KFold   $ cv = KFold ( n_splits = 20 , random_state = None , shuffle = True )   $ estimator = Ridge ( alpha = 3500 )   $ plot_learning_curve ( estimator , _STR_ , X_std , y , cv = cv , train_sizes = np . linspace ( 0.2 , 1.0 , 10 ) )
summary . groupby ( 0 ) . count ( ) . sort_values ( by = _STR_ , ascending = False ) . head ( 10 )
sentiment_df [ _STR_ ] = text   $ sentiment_df . head ( )
df = pd . read_json ( _STR_ )   $
df_master . drop ( _STR_ , axis = 1 , inplace = True )
to_be_predicted_Day5 = 14.80836429   $ predicted_new = ridge . predict ( to_be_predicted_Day5 )   $ predicted_new
! ptdump - av _STR_
spacy_url = _STR_   $ iframe = _STR_ . format ( spacy_url )   $ HTML ( iframe )
sample_item = [ df_stars . iloc [ 0 ] . business_id ]   $ content_rec . recommend_from_interactions ( sample_item )
content_values = cached . map ( lambda x : ( x [ 1 ] [ _STR_ ] , x [ 1 ] [ _STR_ ] ) ) . countByValue ( )
intqrange = transit_df [ _STR_ ] . quantile ( 0.75 ) - transit_df [ _STR_ ] . quantile ( 0.25 )   $ discard = ( transit_df [ _STR_ ] < 0 ) | ( transit_df [ _STR_ ] > 5 * intqrange )   $ transit_df = transit_df . loc [ ~ discard ]   $ transit_df = transit_df . dropna ( )
ok . auth ( )
def basicWeeklyMovePredict ( inpData ) :   $ m , c = np . polyfit ( inpData [ _STR_ ] [ - 14 : - 7 ] , inpData [ _STR_ ] [ - 14 : - 7 ] , 1 )   $ prediction = c + m * ( inpData [ _STR_ ] [ - 1 ] + 7 )   $ return prediction
comps_df . drop_duplicates ( [ _STR_ , _STR_ ] , inplace = True )
pred_labels = lasso . predict ( test_data )   $ print ( _STR_ . format ( lasso . score ( train_data , train_labels ) ) )   $ print ( _STR_ . format ( lasso . score ( test_data , test_labels ) ) )   $ print ( _STR_ . format ( np . sum ( lasso . coef_ != 0 ) ) )
df_sample = df . sample ( frac = 0.1 , replace = True ) . reset_index ( )   $ df_sample   $ X_train_ , X_test_ , y_train_ , y_test_ = sample_split ( df_sample [ selected_feature ] )
df_download_node = pd . merge ( df_download , df_node , left_on = _STR_ , right_on = _STR_ , how = _STR_ )   $ df_download_node = df_download_node [ df_download_node [ _STR_ ] . notnull ( ) ]   $ df_download_node = df_download_node [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ]   $ df_download_node = df_download_node [ df_download_node [ _STR_ ] >= _STR_ ]   $ df_download_node = df_download_node . rename ( columns = { _STR_ : _STR_ , _STR_ : _STR_ } )
cur . execute ( _STR_ ) # LIMIT 10;") $ for r in cur.fetchall(): $    print(r)
log_mod_dweek = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ log_mod_dweek_results = log_mod_dweek . fit ( )   $ log_mod_dweek_results . summary ( )
data [ [ _STR_ , _STR_ ] ] . plot ( subplots = True , color = _STR_ , figsize = ( 16 , 10 ) )
scores , metrics = pipeline . test ( ds_valid , _STR_ )   $ print ( _STR_ )   $ display ( metrics )
Returning = downsample_data ( Cleaneddata )   $ New = downsample_data1 ( Cleaneddata )   $ New . head ( )
Measurement_data = session . query ( Measurement ) . first ( )   $ Measurement_data . __dict__
b = np . zeros ( 60000000 )   $ print b . shape
import statsmodels . api as sm   $ convert_old = sum ( df2 [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] )   $ convert_new = sum ( df2 [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] )   $ n_old = df2 [ ( df2 [ _STR_ ] == _STR_ ) ] . shape [ 0 ]   $ n_new = df2 [ ( df2 [ _STR_ ] == _STR_ ) ] . shape [ 0 ]
df2 = f . query ( _STR_ )
tweet_archive_clean = tweet_archive_df . copy ( )
print ( _STR_ , len ( sakhalin_filtered . species_id . unique ( ) ) )
df . head ( )
soup = bs ( response . text , _STR_ )
to_be_predicted_Day4 = 14.85340501   $ predicted_new = ridge . predict ( to_be_predicted_Day4 )   $ predicted_new
df_mas . dog_stage . value_counts ( )
plt . scatter ( litters [ _STR_ ] , litters [ _STR_ ] )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )   $ plt . title ( _STR_ )   $ plt . show ( )
df . info ( )
df2 . drop_duplicates ( subset = _STR_ , keep = _STR_ , inplace = True )
% % time   $ pred = M_NB_model . predict ( X_test_term )
bb = data . DataReader ( name = _STR_ , data_source = _STR_   $ , start = _STR_ , end = _STR_ )
dft [ pd . datetime ( 2013 , 1 , 1 , 10 , 12 , 0 ) : pd . datetime ( 2013 , 2 , 28 , 10 , 12 , 0 ) ]
country_dummy_list = np . unique ( df_new [ _STR_ ] . values )   $ df_new [ [ _STR_ , _STR_ ] ] = pd . get_dummies ( df_new [ _STR_ ] ) # for group $ df_new[country_dummy_list] = pd.get_dummies(df_new["country"]) # for country $ df_new["intercept"] = 1 $ df_new.head()
df . tail ( )
color = dict ( boxes = _STR_ , whiskers = _STR_ , medians = _STR_ , caps = _STR_ )   $ data [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] . plot . box ( color = color )
df_twitter_extract_copy = df_twitter_extract_copy . drop ( _STR_ , axis = 1 )
old_page_converted = np . random . choice ( [ 1 , 0 ] , size = n_old , p = [ p_old , ( 1 - p_old ) ] )   $ print ( _STR_ , len ( old_page_converted ) )
df2 [ df2 . landing_page == _STR_ ] . count ( ) [ 0 ] / df2 . count ( ) [ 0 ]
! pip install scikit - learn == 0.17 .1
pd . concat ( [ city_loc , city_pop ] , axis = 1 )
bacteria2 . mean ( skipna = False )
topUserItemDocs . to_pickle ( _STR_ )
df . loc [ 0 , _STR_ ] [ - 50 : ]
sns . boxplot ( autodf . powerPS )
predictions_table . where ( F . col ( _STR_ ) == 1 ) . count ( )
url_PIT = _STR_
df2 [ [ _STR_ , _STR_ ] ] = pd . get_dummies ( df2 [ _STR_ ] )   $ df2 = df2 . drop ( _STR_ , axis = 1 )   $ df2 [ _STR_ ] = 1   $ df2 . head ( )
weather_data2 = pd . read_csv ( _STR_ ) ; weather_data2 . head ( )
( null_values > pop_diff ) . mean ( )
cityID = _STR_   $ for tweet in tweepy . Cursor ( api . search , q = q , contained_within = cityID , wait_on_rate_limit = True , lang = _STR_ ) . items ( 500 ) :         $ Arlington . append ( tweet )
print ( autoData . reduce ( lambda x , y : x if len ( x ) < len ( y ) else y ) )
import sqlalchemy   $ from sqlalchemy . ext . automap import automap_base   $ from sqlalchemy . orm import Session   $ from sqlalchemy import create_engine , func   $ from sqlalchemy import desc
f_counts_week_device . show ( 1 )
dci [ _STR_ ] . max ( )
% run process_twitter2tokens . py - i . . / data / Training_promotion_people . csv - ot . . / data / Training_promotion_people . txt - oc . . / data / Training_promotion_people_tokenized . csv - co text
cohort_retention_df . columns
print finalData . shape   $ print X . shape   $ print dat . shape   $ print Stockholm_data . shape
poverty . columns = poverty . iloc [ 0 , : ]
plt . hist ( p_diffs ) ;   $ plt . xlabel ( _STR_ ) ;   $ plt . ylabel ( _STR_ ) ;   $ plt . title ( _STR_ ) ;
p_converted_user2 = df2 . query ( _STR_ ) . user_id . nunique ( ) / df2 . user_id . nunique ( )   $ p_converted_user2
z_score , p_value = sm . stats . proportions_ztest ( [ convert_old , convert_new ] , [ n_old , n_new ] , alternative = _STR_ )   $ print ( z_score , p_value )
df_date = df . copy ( )
m . plot ( forecast )   $ plt . title ( _STR_ , fontsize = 20 )
pantab . frame_to_hyper ( active_with_return , _STR_ )
df = pd . read_csv ( _STR_ )   $
len ( train_data [ train_data . fuelType == _STR_ ] )
os . getcwd ( )
num_users_w_pets = len ( pets_pet [ _STR_ ] . unique ( ) )   $ pct_users_w_pets = round ( num_users_w_pets / num_unique_users * 100. , 2 )   $ print ( _STR_ + str ( pct_users_w_pets ) + _STR_ )
df_index_demo = df_index_demo . set_index ( _STR_ )   $ df_index_demo
p_conv_treat - p_conv_ctrl # !! Keep in mind that later, under the null, we are assuming this difference to be 0 anyways.
! ls . . / data / imsa - cbf / | tail
penalties . head ( 2 )
bufferdf . Fare_amount [ ( bufferdf . Fare_amount == 2 ) | ( bufferdf . Fare_amount == 3 ) | ( bufferdf . Fare_amount == 4 ) ] . apply ( int ) . size
df_os [ df_os [ _STR_ ] == _STR_ ]
df [ _STR_ ] . describe ( )
rain = session . query ( Measurements . date , Measurements . prcp ) . \   $ filter ( Measurements . date > last_year ) . \   $ order_by ( Measurements . date ) . all ( )
trainingSummary = lrmodel . summary
friday_means = friday . groupby ( friday [ _STR_ ] ) . mean ( )   $ friday_means = friday_means . reset_index ( )
wrd_clean [ _STR_ ] . value_counts ( ) [ : 10 ]
p_diffs = np . array ( p_diffs )   $ plt . hist ( p_diffs )   $ plt . axvline ( d , color = _STR_ ) #draw a line on x-axis of value @d
autos = autos . rename ( columns = { _STR_ : _STR_ } )
df [ _STR_ ] . count ( ) - df . query ( _STR_ ) [ _STR_ ] . count ( )
conv_learner . ConvLearner . pretrained ( arch , data , ps = 0 , precompute = True , xtra_fc = [ ] )
import IPython   $ print ( IPython . sys_info ( ) )   $   ! pip freeze
df [ df [ _STR_ ] == 1 ] . shape [ 0 ] / df . user_id . nunique ( )
df2 . to_csv ( _STR_ , index = False )
data . iloc [ : , 10 : 14 ] = data . iloc [ : , 10 : 14 ] . fillna ( _STR_ ) # Overall_Credit_Status, Delivery_Block, Billing_Block, Block_flag $ data.iloc[:,26] = data.iloc[:,26].fillna("0")   # state $ data.dropna(how='any',axis='rows',inplace=True) # district
df . index + pd . DateOffset ( months = 2 , days = 5 )
df2 = pd . read_csv ( _STR_ ,   $ usecols = [ _STR_ , _STR_ ] ,   $ index_col = [ _STR_ ] )   $ df2 . head ( )
t [ _STR_ ] = t [ _STR_ ] . apply ( lambda x : x * 100 / 202 )
autos = autos [ autos [ _STR_ ] . between ( 1900 , 2016 ) ]   $ autos [ _STR_ ] . value_counts ( normalize = True ) . sort_index ( )
autos [ _STR_ ] = autos [ _STR_ ] . str . replace ( _STR_ , _STR_ ) . str . replace ( _STR_ , _STR_ ) . astype ( int )   $ autos . rename ( { _STR_ : _STR_ } , axis = 1 , inplace = True )   $ autos [ _STR_ ] . head ( )
prcp_scores = session . query ( measurements . date , func . avg ( measurements . prcp ) ) . \   $ filter ( measurements . date >= year_ago ) . \   $ group_by ( measurements . date ) . all ( )   $ prcp_scores
t . microsecond
to_be_predicted_Day2 = 81.77623296   $ predicted_new = ridge . predict ( to_be_predicted_Day2 )   $ predicted_new
idxs = dataset . get_cv_idxs ( n , val_pct = 150000 / n )   $ joined_samp = joined . iloc [ idxs ] . set_index ( _STR_ )   $ samp_size = len ( joined_samp ) ; samp_size
port_perf = calc_port_performance ( r_clean . values , hist_alloc . values )   $ pdf = pd . DataFrame ( port_perf , index = r_clean . index , columns = [ dwld_key + _STR_ ] )
plt . figure ( figsize = ( 12 , 6 ) )   $ sns . barplot ( x = _STR_ , y = _STR_ , data = discConvpct )   $ plt . xticks ( rotation = 90 )   $ plt . title ( _STR_ ) ;
sns . barplot ( k . index , k . percent_of_total )
lm = sm . Logit ( df [ _STR_ ] , df [ [ _STR_ , _STR_ ] ] )   $ results = lm . fit ( )   $ results . summary ( )   $
train . head ( )
search_results = gis . content . search ( _STR_ ,   $ _STR_ )   $ major_cities_item = search_results [ 0 ]   $ major_cities_item
r . json ( ) [ _STR_ ] [ _STR_ ] [ 0 ]   $
plt . scatter ( my_df [ _STR_ ] , my_df [ _STR_ ] )   $ plt . axhline ( 0.5 )   $ plt . axvline ( 0.5 )   $ plt . show ( )
fig , ax = plt . subplots ( )   $ typesub2017 [ _STR_ ] . plot ( ax = ax , title = _STR_ , lw = 0.7 )   $ ax . set_ylabel ( _STR_ )   $ ax . set_xlabel ( _STR_ )
cleaned_texts = df . cleaned_text . apply ( lambda x : _STR_ . join ( x ) )
dapr_538 = pd . read_csv ( _STR_ , parse_dates = [ _STR_ ] )   $ dapr_538 . head ( 3 )
x . loc [ x . loc [ : , _STR_ ] > 0.6 , _STR_ ]
data_full = pd . get_dummies ( data_full , columns = [ _STR_ ] )
df . shape
df_new [ [ _STR_ , _STR_ , _STR_ ] ] = pd . get_dummies ( df_new [ _STR_ ] )   $ df_new . head ( )
link = soup . find ( _STR_ ) . find ( _STR_ ) [ _STR_ ]   $ link
testObjDocs . outDF [ 984 : 991 ] ## as expected - bad rows dropped but we now have an indexing issue
glm_binom_feat_2 = H2OGeneralizedLinearEstimator ( family = _STR_ , solver = _STR_ , model_id = _STR_ , Lambda = 0.001 )   $ glm_binom_feat_2 . train ( covtype_X , covtype_y , training_frame = train_bf , validation_frame = valid_bf )
movies = pd . read_csv ( _STR_ , encoding = _STR_ ) # Loading in the dataset
index = similarities . MatrixSimilarity ( lsi [ corpus ] )   $ index . save ( _STR_ )
print ( _STR_ . format ( popt_axial_brace_saddle [ 1 ] [ 0 ] ) )   $ perr = np . sqrt ( np . diag ( pcov_axial_brace_saddle [ 1 ] ) ) [ 0 ]   $ print ( _STR_ . format ( perr ) )
df2 . query ( _STR_ ) . count ( ) [ 0 ] / df2 . count ( ) [ 0 ]
playlist = sp . user_playlist ( spotify_url . split ( _STR_ ) [ 4 ] , spotify_url . split ( _STR_ ) [ 6 ] )   $ pd . io . json . json_normalize ( playlist )
tweet_image_predictions_clean [ _STR_ ] = tweet_image_predictions_clean [ _STR_ ] . astype ( _STR_ )
Y_lin_reg = lin_reg . predict ( X )   $ from sklearn import metrics   $ print ( _STR_ , np . sqrt ( metrics . mean_squared_error ( Y , Y_lin_reg ) ) )   $ print ( _STR_ , metrics . explained_variance_score ( Y , Y_lin_reg ) )
rounds_df = rounds [ rounds . company_uuid . isin ( df . uuid ) ] . copy ( )   $ rounds_df = rounds_df [ ( rounds_df . announced_year >= 1990 ) & ( rounds_df . announced_year <= 2016 ) ] . copy ( )
plt . pie ( total_ride , explode = explode , autopct = _STR_ , labels = labels , colors = colors , shadow = True , startangle = 140 )   $ plt . show ( )
train_df = train_df . drop ( [ _STR_ , _STR_ ] , axis = 1 )   $ test_df = test_df . drop ( [ _STR_ ] , axis = 1 )   $ combine = [ train_df , test_df ]   $ train_df . shape , test_df . shape
deployment_details = client . deployments . create ( model_guid , name = _STR_ )
new_page_converted = np . random . binomial ( n_new , convert_rate_p_new )
against = merged [ merged . committee_position == _STR_ ]   $ support = merged [ merged . committee_position == _STR_ ]
df1 [ _STR_ ] = df1 [ _STR_ ] . dt . dayofweek   $ df1 [ _STR_ ] = df1 [ _STR_ ] . apply ( lambda x : 1 if ( x > 3 ) else 0 )   $ df1 [ _STR_ ] . replace ( to_replace = [ _STR_ , _STR_ ] , value = [ 0 , 1 ] , inplace = True )   $ Feature1 = df1 [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ]   $ Feature1 = pd . concat ( [ Feature1 , pd . get_dummies ( df1 [ _STR_ ] ) ] , axis = 1 )   $
threeoneone_census_complaints [ _STR_ ] = np . log ( threeoneone_census_complaints [ _STR_ ] + 1 )   $ threeoneone_census_complaints [ _STR_ ] = np . log ( threeoneone_census_complaints [ _STR_ ] + 1 )   $ threeoneone_census_complaints [ _STR_ ] = threeoneone_census_complaints [ _STR_ ] . astype ( float )
etsamples_100hz . loc [ etsamples_100hz . eyetracker == _STR_ , _STR_ ] = etsamples_100hz . query ( _STR_ ) . pa_norm . values - etsamples_100hz . query ( _STR_ ) . pa_norm . values
from astropy . coordinates import EarthLocation , AltAz   $ paris = EarthLocation ( lat = 48.8567 * u . deg , lon = 2.3508 * u . deg )   $ crab_altaz = c2 . transform_to ( AltAz ( obstime = now , location = paris ) )   $ print ( crab_altaz )
s1 = [ pairs [ _STR_ ] . loc [ 1 : 2261 : 2 ] [ 2 * i + 1 ] - pairs . loc [ 0 : 2261 : 2 , _STR_ ] [ 2 * i ] for i in np . arange ( 1131 ) ]   $ plt . hist ( s1 , bins = np . arange ( - 30 , 40 , 1 ) )   $ plt . show ( )
df . columns # what are all te
user_tweet_count_df . head ( )
archive_clean . info ( )
def generate_weighted_returns ( returns , weights ) :   $ assert returns . index . equals ( weights . index )   $ assert returns . columns . equals ( weights . columns )   $ return None   $ project_tests . test_generate_weighted_returns ( generate_weighted_returns )
bo = pd . read_csv ( _STR_ )
multi_var = sm . OLS ( df2 [ _STR_ ] , df2 [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ multi_var_results = multi_var . fit ( )   $ multi_var_results . summary ( )
plt . subplots ( figsize = ( 6 , 4 ) )   $ sn . barplot ( train_session_v2 [ _STR_ ] , train_session_v2 [ _STR_ ] )
baseball_h . loc [ ( 2007 , _STR_ , _STR_ ) ]
gap = test . date . min ( ) - train . date . max ( )   $ gap
conv_prob = df2 . converted . mean ( )   $ print ( _STR_ . format ( conv_prob ) )
teams_list = pd . read_csv ( _STR_ ) [ _STR_ ] . tolist ( )
pd . pivot_table ( expenses_df , values = _STR_ , index = _STR_ , aggfunc = np . sum )
yeardf = pd . DataFrame ( ( 1 + newdf [ _STR_ ] / 100 ) . resample ( _STR_ ) . prod ( ) )
products_with_nulls = len ( nulls [ _STR_ ] . unique ( ) )   $ all_products = len ( items [ _STR_ ] . unique ( ) )   $ products_with_nulls / all_products
next ( iter ( md . trn_dl ) )
svm_parameters = [ { _STR_ : [ _STR_ , _STR_ , _STR_ , _STR_ ] ,   $ _STR_ : [ 1.0 , 0.5 , 0.25 , 0.1 , 0.05 , 0.01 ] ,   $ _STR_ : [ _STR_ , { 0 : weights [ 0 ] , 1 : weights [ 1 ] } ] } ]
new_model = gensim . models . KeyedVectors . load_word2vec_format ( path_database + _STR_ , binary = True )
url = form_url ( _STR_ )   $ response = requests . get ( url , headers = headers )   $ print_body ( response , max_array_components = 3 )
df . drop ( todrop2 , inplace = True )
old_page_converted = np . random . choice ( [ 0 , 1 ] , size = n_old , p = [ 1 - p_old , p_old ] )   $ print ( old_page_converted . mean ( ) )
df_A [ df_A . index . str . endswith ( _STR_ ) ]
emojis_db = pd . read_csv ( _STR_ )   $ emojis_db . head ( )
df_pca = pd . DataFrame ( pca . transform ( df_norm ) , columns = labels )   $ df_pca . head ( )
dates = joined [ _STR_ ] . drop_duplicates ( ) ; dates [ 0 ]
import tweepy   $ authentication = tweepy . OAuthHandler ( config . consumer_key , config . consumer_secret )   $ authentication . set_access_token ( config . access_token , config . access_secret )   $ api = tweepy . API ( authentication )
MATTHEW_gb_user = matthew . groupby ( _STR_ )
PYR . head ( )
hour_of_day14 = uber_14 [ _STR_ ] . value_counts ( ) . sort_index ( ) . to_frame ( )
ozzy . buddy . doginfo ( )
df . count ( )
data = allocate_equities ( allocs = [ 0.25 , 0.25 , 0.25 , 0.25 ] )   $ data . plot ( )   $ plt . show ( )
np . shape ( ndvi_coarse )
breaches . groupby ( [ _STR_ , _STR_ ] ) . sum ( )
df_q = pd . read_sql ( query , conn , index_col = None )   $ df_q . head ( 5 )
grouped_by_day_df . to_csv ( _STR_ , index = False )
mnb . fit ( Bow_X_train , Bow_y_train )   $ print ( _STR_ , mnb . score ( Bow_X_train , Bow_y_train ) )   $ print ( _STR_ , mnb . score ( Bow_X_test , Bow_y_test ) )   $ print ( _STR_ , cross_val_score ( mnb , Bow_X_test , Bow_y_test , cv = 5 ) )
ax = perf2 . prices . to_drawdown_series ( ) . plot ( )
df_apps = pd . read_csv ( _STR_ , index_col = None )
orgs = pd . merge ( orgs , org_descs , on = _STR_ )
merge . committee_position . value_counts ( )
from pyspark . sql . functions import udf   $ from pyspark . sql . types import DoubleType , StructType   $ def get_pred ( probability ) :   $ return ( round ( probability . toArray ( ) . tolist ( ) [ 1 ] , 6 ) )   $ get_pred_udf = udf ( get_pred , DoubleType ( ) )
dffreq = dfpolicies . groupby ( [ _STR_ ] ) [ _STR_ ] . count ( ) . to_frame ( )
ad_nlp = df [ [ _STR_ , _STR_ ] ]   $ ad_nlp . head ( )
outlier = all_simband_data [ all_simband_data . subject_id == _STR_ ]
autos [ _STR_ ] . str [ : 10 ] . value_counts ( ) . sort_index ( )
print ( filecounts . to_string ( ) )
rddScaledScores = RDDTestScorees . map ( lambda entry : ( entry [ 1 ] * 0.9 ) )   $
plt . scatter ( my_df [ _STR_ ] , my_df [ _STR_ ] )   $ plt . show ( )
from sklearn . linear_model import Ridge   $ ridge = Ridge ( alpha = 2500 )   $ ridge . fit ( X_train_std , y_train )   $ print ( _STR_ % np . mean ( ( ridge . predict ( X_test_std ) - y_test ) ** 2 ) )   $ print ( _STR_ , ridge . score ( X_test_std , y_test ) )
df . iat [ 1 , 1 ]
autos . loc [ conditions , _STR_ ] = np . nan   $ autos . dropna ( subset = [ _STR_ ] , inplace = True )
treatment_cr = df2 . query ( _STR_ ) . user_id . nunique ( ) / df2 . user_id . nunique ( )
years = data . set_index ( _STR_ )   $ years . head ( )   $ years . tail ( )
Y_train_lab = le . fit_transform ( Y_train )   $ Y_valid_lab = le . transform ( Y_valid )
np . isnan ( StockData ) . sum ( ) . sum ( )
% % time   $ stmt = text ( _STR_ )   $ stmt = stmt . bindparams ( date_filter = datetime . now ( ) - timedelta ( days = 100 ) )   $ allquery = engine . execute ( stmt )   $ allqueryDF = pd . DataFrame ( allquery . fetchall ( ) )
srcdf = srcdf . merge ( md , on = _STR_ , how = _STR_ )   $ srcdf
t0 = time ( )   $ km = KMeans ( n_clusters = 10 , random_state = 0 ) . fit ( data )   $ print ( _STR_ % km )   $ print ( )   $ print ( )   $
graf_counts = graf_counts . reset_index ( )
model . get_config ( )
df [ _STR_ ] = df [ _STR_ ]   $ df [ _STR_ ] . iloc [ df [ df [ _STR_ ] == _STR_ ] . index ] = 30   $ df [ _STR_ ] = df [ _STR_ ] / df [ _STR_ ]   $ df [ _STR_ ] = df [ _STR_ ] / df [ _STR_ ]   $ df [ _STR_ ] = df [ _STR_ ] / df [ _STR_ ]
count_id = df . groupby ( _STR_ ) . agg ( { _STR_ : _STR_ } )   $ top_sub = count_id . sort_values ( _STR_ , ascending = False ) . head ( )   $ top_sub
df_centered . select ( _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ) . orderBy ( _STR_ , _STR_ ) . limit ( 20 ) . toPandas ( )
techmeme [ _STR_ ] = pd . to_datetime ( techmeme . date )   $ techmeme . date = techmeme . date_time . apply ( lambda x : x . date ( ) )   $ techmeme . extra_sources = techmeme . extra_sources . apply ( lambda x : ast . literal_eval ( x ) )
aml . leader . save_mojo ( )
week_day_frequency = my_df [ _STR_ ] . value_counts ( )   $ ax = plt . subplot ( 111 )   $ ax . bar ( week_day_frequency . index , week_day_frequency . data )   $ plt . show ( )
some_df = sqlContext . createDataFrame ( some_rdd )   $ some_df . printSchema ( )
df . shape [ 1 ]
df . isnull ( ) . values . any ( )
! head data / train_users_2 . csv
temp_df = Ralston . TMAX   $ for i in range ( 10 ) :   $ temp_df = temp_df + 5   $ print ( _STR_ , i + 1 , _STR_ , temp_df . mean ( ) , temp_df . std ( ) )
def make_soup ( url ) :   $ html = urlopen ( url ) . read ( )   $ return BeautifulSoup ( html , _STR_ )
Station . __table__
fil = df2 [ _STR_ ] . isin ( [ _STR_ , _STR_ , _STR_ ] ) # Using isin for filtering $ fil
twitter_archive_with_json = pd . merge ( twitter_archive_clean , tweet_json_clean , how = _STR_ , on = [ _STR_ ] )   $
headers = ( { _STR_ : _STR_ } )
dfwithplace = df [ ~ df [ _STR_ ] . isnull ( ) ]
pd . options . display . max_columns = 100   $ X_train . head ( 2 )
transactions . merge ( transactions , left_on = [ _STR_ ] , right_on = [ _STR_ ] , how = _STR_ )   $
class_merged_state = class_merged_hol [ _STR_ ] . unique ( )   $ print ( class_merged_state )
df_merged [ _STR_ ] = 1   $ log_mod = sm . Logit ( df_merged [ _STR_ ] , df_merged [ [ _STR_ , _STR_ , _STR_ ] ] )   $ results = log_mod . fit ( )   $ results . summary ( )
index_df = index_ts [ [ _STR_ , _STR_ , _STR_ ] ] . copy ( ) . set_index ( _STR_ )   $ index_df . head ( )
df . dropna ( inplace = True )
run txt2pdf . py - o _STR_ _STR_
merge = pd . merge ( left = INC , right = weather , how = _STR_ , left_on = _STR_ , right_on = _STR_ )   $ merge . head ( )   $
df_dates_final = df_merged . groupby ( df_merged . columns [ 0 ] ) . min ( )   $ df_dates_final . head ( )
df2 . query ( _STR_ )
df . corr ( )
visual_recognition = VisualRecognitionV3 (   $ _STR_ ,   $ iam_api_key = os . environ [ _STR_ ] )
client . get_list_users ( )
logit = sm . Logit ( df2 [ _STR_ ] , df2 [ [ _STR_ , _STR_ ] ] )
pd . read_json ( _STR_ ) . shape
submit = perf_test [ [ _STR_ ] ]   $ submit [ _STR_ ] = log_reg_pred   $ submit . head ( )
df . shape
def mean_absolute_percentage_error ( y_true , y_pred ) :   $ return np . mean ( np . abs ( ( y_true - y_pred ) / y_true ) ) * 100
df [ _STR_ ] = df [ _STR_ ] . apply ( lambda x : ( x [ 0 ] ) )
propnames . value_counts ( ) . reset_index ( )
print ( _STR_ . format ( df2 . converted . mean ( ) ) )
df_new . head ( )
sum ( df2 . landing_page == _STR_ ) . astype ( _STR_ ) / 290584
top_10_authors = git_log [ _STR_ ] . value_counts ( ) . head ( 10 )   $ top_10_authors
finalSymbolsList = strippedSymbolsList . apply ( getFinalSymbol )
dti . freq
coins_mcap_today = mcap_mat . iloc [ - 2 ]   $ coins_mcap_today = coins_mcap_today . sort_values ( ascending = False )
y_train . value_counts ( )
temps_df . Missoula > 82
archive_df . loc [ ( archive_df . name == _STR_ ) |   $ ( archive_df . name . str . islower ( ) ) , _STR_ ] = _STR_
all_data = pd . concat ( [ concat , ti_mar [ ti_mar [ _STR_ ] == _STR_ ] ] )   $ all_data . groupby ( _STR_ ) . size ( )   $
top_songs [ top_songs [ _STR_ ] . isnull ( ) ] [ _STR_ ] . unique ( )   $
step_threshold = 300   $ pax_raw [ pax_raw . paxstep < step_threshold ] . paxstep . hist ( bins = 30 )
from sklearn . metrics import accuracy_score   $ y_pred = rnd_clf . predict ( X_test )   $ accuracy_score ( y_test , y_pred )
new_page_converted = np . random . choice ( [ 1 , 0 ] , size = df_newlen , p = [ pnew , ( 1 - pnew ) ] )   $
new_page = df2 . query ( _STR_ ) . shape [ 0 ]   $ p4 = float ( new_page / df2 . shape [ 0 ] )   $ print ( _STR_ . format ( p4 ) )
refcell_clear_df = irradiance_clear_df . iloc [ : , : 3 ]   $ pyranometer_clear_df = irradiance_clear_df . iloc [ : , - 1 ]
print ( _STR_ , today . day )   $ print ( _STR_ , today . month )   $ print ( _STR_ , today . year )
print ( _STR_ , store . isnull ( ) . sum ( ) . sum ( ) )
epiweek = 201702   $ cid10 = CID10 [ _STR_ ]   $ geocode = rio_id   $ df_forecast_model = pd . read_sql ( sql , con = engine )   $ df_forecast_model
git_log . timestamp . dt . hour . head ( )
s = pd . Series ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] )   $ s . str . lower ( )
set ( top5_best_fan . keys ( ) )
future_dates_df = pd . DataFrame ( index = future_dates [ 1 : ] , columns = dfs . columns )
pnew = df2 [ _STR_ ] . mean ( )   $ print ( pnew ) #creating the mean converted values as pnew
ser = pd . DataFrame ( { _STR_ : dates , _STR_ : [ 0 ] * len ( dates ) } )   $ ser
df . columns
sample_sizes . show ( )
from calendar import day_name   $ pd . DataFrame ( { _STR_ : [ day_name [ i ] for i in range ( 7 ) ] ,   $ _STR_ : noaa_data [ _STR_ ] . groupby ( noaa_data . index . dayofweek ) . mean ( ) } ,   $ columns = [ _STR_ , _STR_ ]   $ ) . set_index ( _STR_ ) . T
tips . sample ( 5 ) . reset_index ( drop = True )
session . query (   $ Country . country_id , Country . country   $ ) . filter (   $ Country . country . in_ ( [ _STR_ , _STR_ , _STR_ ] )   $ ) . frame ( )
snow . select ( _STR_ ) . to_excel ( _STR_ , index = False )
print total . shape [ 0 ] , total . dropna ( ) . shape [ 0 ]   $ total . head ( )
import statsmodels . api as sm   $ convert_old = df2 . query ( _STR_ ) . user_id . nunique ( ) #  we had this from part I Q4. $ convert_new = df2.query('group == "treatment" and converted == 1').user_id.nunique() # this one too. $ n_old = df2.query('group == "control"').user_id.nunique() $ n_new = df2.query('group == "treatment"').user_id.nunique()
print ( lr . score ( testing_array , y_test ) )
print ( pd . crosstab ( classes , classificacoes_treino , rownames = [ _STR_ ] , colnames = [ _STR_ ] , margins = True ) )
joined_sampled_and_infered = sampled_contirbutors_human_agg_by_gender_and_proj_df . join (   $ relevant_info_agg_by_gender_and_proj_df ,   $ on = _STR_ )
disag_filename = join ( data_dir , _STR_ )   $ output = HDFDataStore ( disag_filename , _STR_ )   $ co . disaggregate ( mains , output )   $ output . close ( )
df1 [ _STR_ ] . replace ( _STR_ , _STR_ , inplace = True )   $ df2 [ _STR_ ] . replace ( _STR_ , _STR_ , inplace = True )
users_converted = df [ _STR_ ] . mean ( ) * 100   $ print ( _STR_ . format ( round ( users_converted , 2 ) ) )
Tip_percentage_of_total_fare = taxiData . Tip_amount / taxiData2 . Fare_amount
data [ _STR_ ] = ( data [ _STR_ ] . rolling ( min_periods = 1 , window = 5 ) . sum ( ) ) / 4   $ data . tail ( )
coins_mcap_today [ 50 : ] . index
for column in df_categorical :   $ df_categorical [ column ] . unique ( )
tweet . lang   $ tweet . text   $ tweet . retweet_count   $ tweet . place   $ tweet . geo
users [ _STR_ ] = ( users [ _STR_ ] - users [ _STR_ ] ) / np . timedelta64 ( 1 , _STR_ )
from sqlalchemy . sql import select
answers_df = pd . io . json . json_normalize ( raw_annotations_df . answers . tolist ( ) )   $ answers_df . tail ( 5 )
df . resample ( _STR_ , how = _STR_ )
number_of_commits = git_log [ _STR_ ] . count ( )   $ number_of_authors = git_log [ _STR_ ] . nunique ( )   $ print ( _STR_ % ( number_of_authors , number_of_commits ) )
import statsmodels . api as sm   $ train_cols = df2 . columns [ 5 : 7 ]   $ train_cols   $ logit = sm . Logit ( df2 [ _STR_ ] , df2 [ train_cols ] )   $ result = logit . fit ( )   $
contour_sakhalin . shape
( details . Popularity == 0 ) . value_counts ( )
_ = ok . submit ( )
conf_matrix = confusion_matrix ( training_test_labels , lr . predict ( preproc_training_test ) , labels = [ 1 , 2 , 3 , 4 , 5 ] )   $ conf_matrix
% % bash   $ mkdir sample   $ gsutil cp _STR_ sample / train . csv   $ gsutil cp _STR_ sample / valid . csv
new_page_converted = np . random . binomial ( 1 , 0.1196 , 145310 )   $ new_page_converted
dummy_df [ _STR_ ] = ( dummy_df [ _STR_ ] . shift ( - 1440 ) - dummy_df [ _STR_ ] ) . fillna ( method = _STR_ )   $ dummy_df [ _STR_ ] = ( dummy_df [ _STR_ ] . shift ( - 2880 ) - dummy_df [ _STR_ ] ) . fillna ( method = _STR_ )   $ dummy_df [ _STR_ ] = ( dummy_df [ _STR_ ] . shift ( - 4320 ) - dummy_df [ _STR_ ] ) . fillna ( method = _STR_ )
len ( orig_android_tweets [ orig_android_tweets [ _STR_ ] . dt . year == 2016 ] )
cur . execute ( _STR_ )
df_arch_clean . loc [ df_arch_clean . tweet_id == _STR_ , _STR_ ]   $
tweet_clean . retweeted_status . value_counts ( )
session . query ( Measurement . station , Station . name , Station . latitude , Station . longitude , Station . elevation , func . sum ( Measurement . prcp ) ) \   $ . group_by ( Measurement . station ) . order_by ( func . sum ( Measurement . prcp ) . desc ( ) ) \   $ . filter ( Measurement . date >= start_date ) . filter ( Measurement . date <= end_date ) . all ( )
Gun = G . to_undirected ( )   $ politiciansPartyDict = nx . get_node_attributes ( Gun , _STR_ )   $ partitions = community . best_partition ( Gun , weight = _STR_ )
print ( tipsDF . describe ( ) )
countries_df = pd . read_csv ( _STR_ )
g = df_usa [ _STR_ ] . values   $ p = df_usa [ _STR_ ] . values   $ df_usa [ _STR_ ] = g / ( p * 1000 )   $ df_usa . round ( decimals = 2 )
calls_df [ calls_df [ _STR_ ] == 5419196969 ] . head ( )
msk = np . random . rand ( len ( df ) ) < 0.8   $ train = cdf [ msk ]   $ test = cdf [ ~ msk ]
ab_df2 [ ( ( ab_df2 [ _STR_ ] == _STR_ ) == ( ab_df2 [ _STR_ ] == _STR_ ) ) == False ] . shape [ 0 ]
pd . Timestamp ( _STR_ )
pred . head ( rows = 3 )
BDAY_PAIR_qthis = BDAY_PAIR_df [ pd . to_datetime ( BDAY_PAIR_df . created_at ) >= qthis ]
np . exp ( results2 . params )
for i in range ( - 5 , 0 , 1 ) :   $ data [ _STR_ ] = data [ _STR_ ] . shift ( - i )   $ data = data . dropna ( )   $ data . head ( )
tweets [ _STR_ ] . value_counts ( )
pd . read_csv ( _STR_ , nrows = 4 )
from datetime import datetime   $ xml_in [ _STR_ ] = ( datetime . now ( ) - xml_in [ _STR_ ] ) . astype ( _STR_ )
df . dtypes . head ( 20 )
poptweets = dict ( df [ [ _STR_ , _STR_ ] ] . groupby ( _STR_ ) [ _STR_ ] . sum ( ) )   $ poptweets = pd . DataFrame ( list ( poptweets . items ( ) ) , columns = [ _STR_ , _STR_ ] ) . sort_values ( _STR_ , ascending = False ) [ 0 : 20 ]   $ poptweets . to_csv ( _STR_ , index = False )
dfRegMet2014 = dfRegMet [ dfRegMet . index . year == 2014 ]
p_hat = 5. / 8.   $ freq_prob = ( 1 - p_hat ) ** 3   $ print ( _STR_ % freq_prob )
df [ _STR_ : _STR_ ] [ _STR_ ] . value_counts ( ) . head ( 5 )
cols_to_export = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ mentions_df . to_csv ( _STR_ , columns = cols_to_export , sep = _STR_ , index = False )
treat = ab_data . query ( _STR_ )   $ contr = ab_data . query ( _STR_ )   $ contr [ _STR_ ] . value_counts ( ) , treat [ _STR_ ] . value_counts ( )
A = rng . randint ( 10 , size = ( 3 , 4 ) )   $ A
high_rev_acc_opps_net . head ( )
q_all_count . results ( ) [ _STR_ ] . sum ( )
def pos_to_lemmatizer_pos ( pos_initial ) :                 $ if pos_initial != _STR_ :   $ return pos_initial . lower ( )   $ else :   $ return _STR_
store_items . pop ( _STR_ )   $ store_items
log_mod_m = sm . Logit ( df_countries [ _STR_ ] , df_countries [ [ _STR_ , _STR_ , _STR_ ] ] )   $ results_m = log_mod_m . fit ( )   $ print ( _STR_ )   $
( p_diffs > act_diff ) . mean ( )
df2_trt = df2 . query ( _STR_ ) ;   $ df2_trt [ _STR_ ] . mean ( )
py . iplot ( data_predict . groupby ( freq ) . sum ( ) [ [ _STR_ , _STR_ ] ] . iplot ( asFigure = True ,   $ kind = _STR_ , xTitle = _STR_ , yTitle = _STR_ , title = _STR_ ) )
submit . tail ( )
df [ _STR_ ] = pd . to_numeric ( df . units_purchased )   $ df [ _STR_ ] = pd . to_numeric ( df . total_spend , errors = _STR_ )
volume_yearly = vol . groupby ( vol . index . year ) . sum ( )
y = abc . loc [ : 10 , _STR_ ]   $ x = abc . loc [ : 10 , _STR_ ]   $ plot_top_sites_DRG ( 871 , 2016 , x , y )
ix = df_test . index [ 0 ]   $ ix
telemetry_feat . head ( )
results . columns = [ _STR_ , _STR_ ]   $ results . index = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ print ( results )
row_concat = pd . concat ( [ uber1 , uber2 , uber3 ] )   $ print ( row_concat . shape )   $ print ( row_concat . head ( ) )   $ print ( row_concat . tail ( ) )   $
y . value_counts ( )
X_train , X_test , y_train , y_test = train_test_split ( features , classification_price , test_size = 0.2 )
df [ _STR_ ] . nunique ( )
old_page_converted = np . random . choice ( [ 0 , 1 ] , size = n_old , p = [ 1 - p_old , p_old ] )   $ old_page_converted
den . shape
print ( _STR_ , donations [ _STR_ ] . median ( ) )   $
airbnb_df = pd . read_csv ( _STR_ )   $ airbnb_df . head ( )
print ( data . shape )   $ data = data . drop_duplicates ( )   $ print ( data . shape )
pd . DataFrame ( features [ _STR_ ] . head ( 10 ) )
df_world_map . plot ( )
step_counts . index = pd . date_range ( _STR_ , periods = step_counts . size )   $ print ( step_counts )
Measurements = Base . classes . measurements # Map measurements class $ Stations = Base.classes.stations # Map stations class
len ( vectorizer . get_stop_words ( ) )
withDups . ix [ _STR_ ]
df_best_chart . pivot_table ( values = _STR_ , columns = _STR_ , index = _STR_ ) . plot ( )
len ( df_characters ) , len ( df . groupby ( _STR_ ) [ _STR_ ] . nunique ( ) . reset_index ( ) )
gMapAddrDat . location = _STR_
tweets_raw = tweets_raw . drop ( axis = 1 , labels = [ _STR_ , _STR_ ] )   $ tweets_raw = tweets_raw . dropna ( )
local_tz = pytz . timezone ( _STR_ )
predictions = knn . predict ( test [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] )
pd . date_range ( start , end , freq = _STR_ )
n_old = df2 . query ( _STR_ ) . shape [ 0 ]   $ n_old
html_table = df . to_html ( )   $ html_table
therm_fiss_rate = openmc . Tally ( name = _STR_ )   $ therm_fiss_rate . scores = [ _STR_ ]   $ therm_fiss_rate . filters = [ openmc . EnergyFilter ( [ 0. , 0.625 ] ) ]   $ tallies_file . append ( therm_fiss_rate )
daily_constituent_count = QTU_pipeline . groupby ( level = 0 ) . sum ( )   $ QTU_pipeline . groupby ( level = 0 ) . median ( ) . describe ( )
train . unique_items
df . head ( )
properati [ _STR_ ] . value_counts ( dropna = False )
twitter_data_v2 [ ~ twitter_data_v2 . tweet_id . isin ( tweet_data_v2 . tweet_id ) ]
many_bdays = [ ix for ix in BDAY_PAIR . index if len ( np . unique ( BDAY_PAIR . loc [ ix , _STR_ ] ) ) > 1 ]
flight6 . coalesce ( 2 ) . write . parquet ( _STR_ )
autos [ autos . registration_year > 2016 ]
df_r2 . loc [ df_r2 [ _STR_ ] . isin ( [ customer ] ) ]
arr2d = np . arange ( 30 , dtype = np . float32 ) . reshape ( 10 , 3 )   $ arr2d
airbnb_df [ _STR_ ] . value_counts ( dropna = False )
def episode_finished ( r ) :   $ print ( _STR_ . format ( ep = r . episode , ts = r . episode_timestep ,   $ reward = r . episode_rewards [ - 1 ] ) )   $ return True
table1 . head ( 10 )
df . median ( )
a . count ( )
df = df . dropna ( axis = 1 , how = _STR_ )
count_vectorizer = CountVectorizer ( min_df = 10 , ngram_range = ( 1 , 2 ) , max_df = .5 ,   $ stop_words = nltk_stopwords , token_pattern = _STR_ )   $ count_vectorizer . fit ( final_tweets )
y_pred = gscv . best_estimator_ . predict ( x_test )
unique_brand_dict = { }   $ for b in brand_unique :   $ mean_price = autos . loc [ autos [ _STR_ ] == b , _STR_ ] . mean ( )   $ unique_brand_dict [ b ] = mean_price   $ print ( unique_brand_dict )         $
from pysumma . Simulation import Simulation   $ from pysumma . Plotting import Plotting
f0 . exclude_list_filter
( grades > 5 ) . all ( )
ls = [ bbc , cnn , abc ]   $ print ( _STR_ )   $ for tweet in abc [ : 5 ] :   $ print ( tweet . text )   $ print ( )
p_salary = portland_census2 . drop ( portland_census2 . index [ : 49 ] )   $ p_salary . head ( 3 )
import seaborn as sns   $ sns . barplot ( y = _STR_ , x = _STR_ , data = rankImportance , palette = _STR_ )
iplot ( data . groupby ( _STR_ ) . size ( ) . sort_values ( ascending = False ) [ : 20 ] . iplot ( asFigure = True , dimensions = ( 750 , 500 ) , kind = _STR_ ) )
weather_df . loc [ weather_df [ _STR_ ] . isin ( [ _STR_ , _STR_ , _STR_ , _STR_ ] ) , [ _STR_ , _STR_ ] ] = np . NaN   $ weather_df = weather_df . fillna ( method = _STR_ )
from pyspark . sql . functions import udf   $ from pyspark . sql . types import DoubleType , StructType   $ def get_pred ( probability ) :   $ return ( round ( probability . toArray ( ) . tolist ( ) [ 1 ] , 6 ) )   $ get_pred_udf = udf ( get_pred , DoubleType ( ) )
corn_vege . size ( )
print ( _STR_ , regr2 . coef_ )   $ print ( _STR_ , regr2 . intercept_ )
my_gempro . find_disulfide_bridges ( representatives_only = False )
df = pd . melt ( dfa , id_vars = _STR_ , value_vars = vars2 )   $ print ( df . head ( 5 ) )   $ print ( df . tail ( 5 ) )   $
Pop_df [ _STR_ ] = 0.0   $ for i in range ( 26 ) :   $ Pop_df . Year [ i ] = Pop_df . Date [ i ]   $
query = _STR_   $ c . execute ( query )   $ results = c . fetchall ( )   $ print results [ 0 ] [ 0 ]
y_test_over [ fm_bet_over ] . sum ( )
len ( submission . ImageId . values )
goodreads_users_df . isnull ( ) . sum ( )
b = pd . read_sql_query ( q , conn )   $ b
classified_queensland_data = queensland_data . assign ( topic = topic_preds )   $ classified_queensland_data . loc [ classified_queensland_data [ _STR_ ] == _STR_ ]
df_weather . loc [ : , _STR_ ] = df_weather . events . apply ( lambda x : _STR_ if x == _STR_ else x )   $ df_weather . events . fillna ( value = _STR_ , inplace = True )   $ print ( _STR_ )   $ evnts = [ str ( x ) for x in df_weather . events . unique ( ) ]   $ print ( _STR_ . join ( [ str ( i + 1 ) + _STR_ + evnts [ i ] + _STR_ for i in range ( len ( evnts ) ) ] ) )
from sklearn . metrics import classification_report   $ labels = [ _STR_ , _STR_ , _STR_ ]   $ y_pred = nbc . predict ( d_test_sc )   $ print ( classification_report ( l_test , y_pred ,   \   $ target_names = labels ) )
merged2 [ _STR_ ] = merged2 [ _STR_ ] / 60.0
obs_diff = new_page_converted . mean ( ) - old_page_converted . mean ( )   $ obs_diff
df . head ( )
df [ _STR_ ] . describe ( )
temp_df . columns = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]
import matplotlib . pyplot as plt   $ % matplotlib inline
general_volume [ _STR_ ] = general_volume . ds . astype ( _STR_ )   $ joined_df = general_volume . merge ( forecast , left_on = _STR_ , right_on = _STR_ )
barcelona . dtypes
plt . show ( )
all_311_requests . to_csv ( _STR_ , encoding = _STR_ , index = False )
combined_df = pd . merge ( sales_data , returns_data , how = _STR_ , on = _STR_ )   $ print ( combined_df )
p_new = df2 . query ( _STR_ ) . user_id . nunique ( ) / df2 . user_id . nunique ( )   $ print ( _STR_ . format ( round ( p_new , 4 ) ) )
components = pd . DataFrame ( pca . components_ , columns = [ _STR_ , _STR_ , _STR_ ] )
merged_df = pd . concat ( [ taxi_weather_df , seats_per_hour ] , axis = 1 )
filtered_brewery [ [ _STR_ , _STR_ , _STR_ ] ] [ filtered_brewery . brewery_name == top_three [ 0 ] [ 0 ] ]
plt . imshow ( result )   $ plt . show ( )
TrainData_ForLogistic . columns
eve_contr = df_eve . query ( _STR_ ) . converted . mean ( )   $ eve_contr
y = valid [ _STR_ ]   $ y_pred = valid . merge ( median_15 , on = _STR_ , how = _STR_ ) [ _STR_ ]   $ smape_fast ( y , y_pred )
round ( ( timelog . seconds . sum ( ) / 60 / 60 / 24 ) , 1 )
for i in range ( 10 ) :   $ ypred = model . predict ( np . random . randint ( 0 , 1 , ( 1 , 36 , 1 ) ) )   $ res = [ int_to_char [ n ] for n in word ]   $ print ( _STR_ . join ( res ) )
week_df = df . groupby ( df [ _STR_ ] . dt . weekday_name ) . count ( )   $ print ( week_df [ _STR_ ] )
tree = DecisionTreeClassifier ( criterion = _STR_ )   $ model = tree . fit ( X_train_total , y_train )   $ model . score ( X_test_total_checked , y_test )
df0901 . head ( )
dataset = dataset . join ( longest_date_each_costumer , [ _STR_ ] , rsuffix = _STR_ )   $ dataset . rename ( columns = { _STR_ : _STR_ } , inplace = True )   $ dataset . head ( 3 )
df_new [ [ _STR_ , _STR_ , _STR_ ] ] = pd . get_dummies ( df_new [ _STR_ ] )
pops = [ ]   $ for country in data_countries . country_destination . values :   $ print _STR_ % ( country , sum ( data_age_gender_bkts [ data_age_gender_bkts . country_destination == country ] . population_in_thousands . values ) )   $ pops . append ( sum ( data_age_gender_bkts [ data_age_gender_bkts . country_destination == country ] . population_in_thousands . values ) )   $ data_countries [ _STR_ ] = pops   $
df2 . query ( _STR_ ) . count ( ) [ _STR_ ]
msft . loc [ _STR_ ]
from azureml . core . webservice import AciWebservice   $ aciconfig = AciWebservice . deploy_configuration ( cpu_cores = 1 ,   $ memory_gb = 4 ,   $ tags = [ TICKER , _STR_ , _STR_ ] ,   $ description = _STR_ + TICKER + _STR_ )
tsla . info ( )
tweet1 . user . screen_name
out_file = _STR_   $ with open ( out_file , _STR_ ) as fout :   $ fout . write ( _STR_ )   $ fout . write ( _STR_ )
keras_entity_recognizer . fit ( df_train )
unicode = text . encode ( _STR_ )   $ print ( unicode )   $ unicode . find ( _STR_ )
tlen . plot ( figsize = ( 16 , 4 ) , color = _STR_ )
scores . head ( )
testObjDocs . outDF [ 975 : 1000 ] ## spot check run on batches of 25 records to find the bad ones (b/2 900 and 1000) $
model . fit ( train_words , fb_train . popular )
print ( train_df [ train_df . author . isnull ( ) ] . shape [ 0 ] )   $ print ( train_df . shape [ 0 ] )
print ( _STR_ , groceries . loc [ [ _STR_ , _STR_ ] ] )
train_set . head ( 10 )
tokenization_url = _STR_   $ iframe = _STR_ . format ( tokenization_url )   $ HTML ( iframe )
df_th = df . loc [ df [ _STR_ ] == _STR_ ]   $ df_th
X = pd . get_dummies ( reddit [ _STR_ ] )   $ y = reddit [ _STR_ ]   $ from sklearn . model_selection import train_test_split   $ X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 42 )
stamp . strftime ( _STR_ )
for i in by_periods . index [ : 5 ] :   $ print ( _STR_ . format ( i . start_time , i . end_time , by_periods [ i ] ) )
cars = autos [ _STR_ ] . value_counts ( ) . head ( 7 ) . index
len ( df ) == df . timestamp . nunique ( )
df_uk = df_new [ df_new [ _STR_ ] == _STR_ ]   $ df_uk [ _STR_ ] . mean ( )
engine = create_engine ( _STR_ )   $ conn = engine . connect ( )
p_diffs = np . array ( p_diffs )   $ ( p_diffs > real_diff ) . mean ( )
X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 )
print ( _STR_ . format ( popt_axial_chord_crown [ 2 ] [ 0 ] ) )   $ perr = np . sqrt ( np . diag ( pcov_axial_chord_crown [ 2 ] ) ) [ 0 ]   $ print ( _STR_ . format ( perr ) )
print sqlContext . sql ( query ) . toPandas ( )
pandas . concat ( sheets ) . info ( )
new_page_converted = df_treatment_and_new_page . sample ( no_of_samples_treatment )   $ p_new = new_page_converted . converted . mean ( )   $ p_new
Date1yrago = dt . date . today ( ) - dt . timedelta ( days = 365 )   $ print ( Date1yrago )   $
SST = ( ( y . mean ( ) - test . readingScore ) ** 2 ) . sum ( )   $ print SST
log_mod = sm . Logit ( df2 [ _STR_ ] , df2 [ [ _STR_ , _STR_ ] ] )
data2 . index = list ( range ( len ( data2 . index ) ) )
sorted ( json_dict , key = lambda x : x [ 0 ] )   $ ( result , date1 , date2 ) = max ( [ ( abs ( u [ 4 ] - v [ 4 ] ) , u [ 0 ] , v [ 0 ] ) for u , v in zip ( json_dict , json_dict [ 1 : ] ) ] )   $ print ( _STR_ . format ( date1 , date2 , result ) )   $
clintondf = pd . DataFrame ( HC , columns = [ _STR_ , _STR_ , _STR_ , _STR_ ] )
print ( r . json ( ) )
pd . Series ( 0 , index = days )
temp_series . plot ( label = _STR_ )   $ temp_series_freq_15min . plot ( label = _STR_ )   $ plt . legend ( )   $ plt . show ( )
train . head ( 5 )
print ( _STR_ , regr . coef_ )   $ print ( _STR_ , regr . intercept_ )
df_archive [ _STR_ ] . value_counts ( )
slack = Slacker ( slack_api_token )   $ channels = slack . channels . list ( )   $ for channel in channels . body [ _STR_ ] :   $ print ( _STR_ )
n_old = df2 [ ( df2 [ _STR_ ] == _STR_ ) ] . shape [ 0 ]   $ n_old
reddit_comments_data . groupBy ( _STR_ ) . agg ( { _STR_ : _STR_ } ) . orderBy ( _STR_ , ascending = False ) . show ( )
to_be_predicted_Day2 = 26.69068092   $ predicted_new = ridge . predict ( to_be_predicted_Day2 )   $ predicted_new
m4 = m3 . flatten ( ) #converting the array in 1-D $ m5 = np.sort(m4, axis = None)[::-1]      #arranging in descending order $ sorted_m3 = np.reshape(m5, (2, 4))      #reshaping it to 2-D $ print("sorted m3: ", sorted_m3)
inspector = inspect ( engine )   $ inspector . get_table_names ( )
tem = list ( mars_facts [ 0 ] . values ( ) )   $ tem
melted [ ( melted . Date == _STR_ ) & ( melted . Symbol == _STR_ ) ]
df_new . groupby ( [ _STR_ , _STR_ ] ) [ _STR_ ] . value_counts ( )
old_page_converted = np . random . choice ( [ 0 , 1 ] , size = n_old , p = [ ( 1 - p_old ) , p_old ] )   $ old_page_converted . mean ( )
util . get_rebalance_date ( start_idx , end_idx , _STR_ )
non_na_df . groupby ( _STR_ ) . count ( ) [ _STR_ ] . plot ( kind = _STR_ )   $ plt . tight_layout ( )   $ plt . title ( _STR_ )   $ plt . ylabel ( _STR_ )   $ plt . xlabel ( _STR_ )
learn . lr_find ( lrs / 1000 )   $ learn . sched . plot ( )
df_regression [ _STR_ ] = 1   $ df_regression [ [ _STR_ , _STR_ ] ] = pd . get_dummies ( df_regression [ _STR_ ] )   $ df_regression . drop ( [ _STR_ ] , axis = 1 , inplace = True )   $ df_regression . head ( )
popCon . sort_values ( by = _STR_ , ascending = False ) . head ( 9 ) . plot ( kind = _STR_ )
print ( autos [ _STR_ ] . str [ : 10 ] . value_counts ( normalize = True , dropna = False ) . sort_index ( ) )   $ print ( autos [ _STR_ ] . str [ : 10 ] . value_counts ( normalize = True , dropna = False ) . shape )
np . sum ( all_data [ _STR_ ] . isnull ( ) . values )
tweets_master_df . to_csv ( _STR_ , header = True )
! head - n 10 mbox . txt
X = pd . get_dummies ( X , drop_first = True )   $ X . head ( )
yc_new4 . describe ( )
lmscore . summary2 ( )
temp = df . groupby ( _STR_ ) . cust_id . count ( )   $ temp = pd . DataFrame ( temp )   $ temp . columns . values [ 0 ] = _STR_   $ temp = temp1 . sort_values ( by = [ _STR_ ] , ascending = False )   $ temp . head ( )
data_sets = { }   $ data_sets [ _STR_ ] = pd . read_pickle ( _STR_ )   $ data_sets [ _STR_ ] = pd . read_pickle ( _STR_ )
plt . figure ( figsize = ( 7 , 7 ) )   $ plt . scatter ( x_9d [ : , 0 ] , x_9d [ : , 1 ] , c = _STR_ , alpha = 0.5 )   $ plt . ylim ( - 10 , 30 )   $ plt . show ( )
bad_df = df . index . isin ( [ 5 , 12 , 23 , 56 ] )   $ df [ ~ bad_df ] . head ( )
to_be_predicted_Day4 = 55.27537048   $ predicted_new = ridge . predict ( to_be_predicted_Day4 )   $ predicted_new
dataframe . head ( )
autos = autos [ autos [ _STR_ ] . between ( 1900 , 2016 ) ]   $ autos [ _STR_ ] . value_counts ( normalize = True ) . head ( 10 )
parser . HHParser . regexp . pattern
dfNYC . head ( )
datetime . now ( ) . toordinal ( ) / 365   $
pd . cut ( tips . tip , np . r_ [ 0 , 1 , 5 , np . inf ] ,   $ labels = [ _STR_ , _STR_ , _STR_ ] ) . sample ( 10 )
typesub2017 . isnull ( ) . sum ( )
weights = { _STR_ : 68 , _STR_ : 83 , _STR_ : 86 , _STR_ : 68 }   $ s3 = pd . Series ( weights )   $ s3
logit3 = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ result3 = logit3 . fit ( )   $ result3 . summary2 ( )
top_brands = ( autos [ _STR_ ] . value_counts ( )   $ . sort_values ( ascending = False ) . head ( 6 ) . index )   $ print ( top_brands )
num_new_user = len ( df2 [ df2 [ _STR_ ] == _STR_ ] )   $ prob_new_user = num_new_user / ( df2 . shape [ 0 ] )   $ print ( _STR_ . format ( prob_new_user ) )
winter = df [ df . index . month . isin ( [ 12 , 1 , 2 ] ) ]
( test . groupby ( by = _STR_ ) . mean ( ) - train . groupby ( by = _STR_ ) . mean ( ) ) . plot ( figsize = ( 15 , 5 ) )
df . CATS_Counter . head ( )
control_converted = df2 [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] . mean ( )   $ control_converted
es_dictindex = _STR_ . format ( city . lower ( ) )   $ es_dicttype = _STR_   $ es . createOrReplaceIndex ( es_dictindex )
arma_mod50 = sm . tsa . ARMA ( dta_713 , ( 5 , 0 ) ) . fit ( disp = False )   $ print ( arma_mod50 . params )
cityID = _STR_   $ for tweet in tweepy . Cursor ( api . search , q = q , contained_within = cityID , wait_on_rate_limit = True , lang = _STR_ ) . items ( 500 ) :         $ Fort_Wayne . append ( tweet )
sns . distplot ( utility_patents_subset_df . prosecution_period , color = _STR_ )   $ plt . show ( )
print ( _STR_ . format ( data [ _STR_ ] [ likes ] ) )   $ print ( _STR_ . format ( likes_max ) )   $ print ( _STR_ . format ( data [ _STR_ ] [ likes ] ) )   $ print ( _STR_ . format ( s = data [ _STR_ ] [ likes ] ) )
from h2o . estimators . gbm import H2OGradientBoostingEstimator   $ gbm_model = H2OGradientBoostingEstimator ( model_id = _STR_ , distribution = _STR_ )   $ gbm_model . train ( x = predictor_columns , y = target , training_frame = train , validation_frame = valid )
oecd_site = _STR_   $ pd . read_html ( oecd_site )
CryptoComm . head ( )
members_df_all [ _STR_ ] = [ ( 2018 - x . year ) for x in pd . to_datetime ( members_df_all [ _STR_ ] ) ]
leadConvpct . tail ( )
df_sched . iloc [ : , 1 : ] = df_sched . iloc [ : , 1 : ] . apply ( lambda x : x . str [ : 10 ] )
mydata [ mydata . isnull ( ) . any ( axis = 1 ) ]
print _STR_ % ( time2close_2013 . mean ( ) / 3600.0 )   $ print _STR_ % ( time2close_2013 . median ( ) / 3600.0 )   $ print _STR_   $ print time2close_2013 . quantile ( [ 0.25 , 0.5 , 0.75 ] )
real_diff = df2 . query ( _STR_ ) . converted . sum ( ) / df2 . query ( _STR_ ) [ _STR_ ] . count ( ) - df2 . query ( _STR_ ) . converted . sum ( ) / df2 . query ( _STR_ ) . user_id . nunique ( )   $ real_diff
first_commit_timestamp = pd . to_datetime ( _STR_ )   $ last_commit_timestamp = pd . to_datetime ( _STR_ )   $ corrected_log = git_log [ ( git_log [ _STR_ ] >= first_commit_timestamp ) &   $ ( git_log [ _STR_ ] <= last_commit_timestamp ) ]   $ corrected_log [ _STR_ ] . describe ( )
autos [ _STR_ ] . value_counts ( )
twitter_archive_master = pd . read_csv ( _STR_ )
mfp_boss . build_prior ( )
_ , pval_2_sided = sm . stats . proportions_ztest ( [ convert_new , convert_old ] , [ n_new , n_old ] , alternative = _STR_ )   $ pval_2_sided
demo . plot_weights ( l , x_test )
x_axis = output [ _STR_ ]
df_ad_airings_5 [ _STR_ ] . value_counts ( )
views_data = pd . read_json ( _STR_ , lines = True , chunksize = 1000000 )   $ views_data_clean = pd . DataFrame ( [ ] , columns = [ _STR_ , _STR_ , _STR_ ] )   $ for views_chunk in views_data :   $ views_chunk = views_chunk . drop ( [ _STR_ , _STR_ , _STR_ ] , axis = 1 )   $ views_data_clean = views_data_clean . append ( views_chunk . loc [ views_chunk . browser_id . isin ( unique_buying_browsers ) ] )
from sklearn . feature_extraction . text import CountVectorizer   $ vectorizer = CountVectorizer ( stop_words = _STR_ )   $ X = vectorizer . fit_transform ( tmp )
import pandas as pd   $ from datetime import timedelta   $ % matplotlib inline   $ vow = pd . read_csv ( _STR_ )   $ vow . head ( )
sentence = _STR_   $ sentiment_score ( sentence )
intervention_train . columns
results . params
to_be_predicted_Day5 = 21.38790489   $ predicted_new = ridge . predict ( to_be_predicted_Day5 )   $ predicted_new
students . columns
reviews . region_1 . fillna ( _STR_ ) . value_counts ( )
sns . violinplot ( x = _STR_ , y = _STR_ , hue = _STR_ , data = titanic , split = True )
autos = autos [ autos [ _STR_ ] . between ( 1900 , 2016 ) ]   $ autos [ _STR_ ] . value_counts ( normalize = True ) . head ( 10 )
discGrouped = discovery . groupby ( [ _STR_ , _STR_ ] ) . opportunity_stage . count ( )   $ discConvpct = discGrouped . groupby ( level = [ 0 ] ) . apply ( lambda x : 100 * x / float ( x . sum ( ) ) ) ;
autos [ _STR_ ] . value_counts ( ) . sort_index ( ) . head ( 20 )
print ( stock . head ( ) )   $ print ( maxi . head ( ) )
style . available
n_new = ( df2 [ _STR_ ] == _STR_ ) . sum ( )   $ n_new
run txt2pdf . py - o _STR_ _STR_
print ( autos [ _STR_ ] . unique ( ) . shape )   $ print ( autos [ _STR_ ] . describe ( ) )   $ print ( autos [ _STR_ ] . value_counts ( ) )
exceldmh = dmh . PandasDataFrame ( exceldf )
fraud_data_updated = fraud_data_updated . drop ( [ _STR_ , _STR_ , _STR_ ] , axis = 1 )   $
model . wv . syn0 . shape
df3 = make_df ( _STR_ , [ 0 , 1 ] )   $ df4 = make_df ( _STR_ , [ 0 , 1 ] )   $ display ( _STR_ , _STR_ , _STR_ )
from pandas_datareader . famafrench import get_available_datasets   $ import pandas_datareader . data as web   $ len ( get_available_datasets ( ) )   $
P = 2 * numpy . pi / freqs
conn . execute ( sql )   $ conn . execute ( sql )
df_protest . columns = df_protest . columns . str . lower ( )
lr_cv . fit ( X_train , y_train )
repos_ids = pd . read_sql ( _STR_ , con )
pres_df . shape
sns . set ( )   $ sns . distplot ( np . log ( df_input_clean . toPandas ( ) . Resp_time ) , kde = True , color = _STR_ )
df . dropna ( axis = 0 , inplace = True )   $ df . reset_index ( inplace = True , drop = True )   $ df . shape
from scripts . model import *
new_page_converted = np . random . binomial ( n_new , p_new )
U M M
daily_sales . plot ( _STR_ , _STR_ , kind = _STR_ , color = _STR_ , figsize = ( 15 , 5 ) )
link_pattern = _STR_
daily_ret_b_mean = daily_ret_b . mean ( )   $ daily_ret_b_mean
train_corpus = [ _STR_ . join ( [ t for t in reuters . words ( train_doc [ t ] ) ] )   $ for t in range ( len ( train_doc ) ) ]   $ print ( _STR_ . format ( train_corpus [ 0 ] [ : 100 ] ) )
top_songs = top_songs . dropna ( axis = 0 , how = _STR_ )
df2 [ _STR_ ] = df [ _STR_ ] . map ( lambda x : 1 if pd . isnull ( x ) == False else 0 )
autos = autos [ autos [ _STR_ ] . between ( 1900 , 2016 ) ]   $ autos [ _STR_ ] . value_counts ( normalize = True , ascending = False ) . head ( 20 )
df [ [ _STR_ , _STR_ , _STR_ ] ] [ df . favorite_count == np . min ( df . favorite_count ) ]
grouped . sort_index ( by = _STR_ ) . tail ( )
df = ek . get_timeseries ( [ _STR_ ] ,   $ start_date = _STR_ ,     $ end_date = _STR_ )   $ df
% time lr = lr . fit ( train_4 , y_train )
data . groupby ( [ _STR_ , _STR_ ] ) [ _STR_ ] . agg ( [ _STR_ ] ) . pivot_table (   $ _STR_ , index = _STR_ , columns = _STR_ , fill_value = 0 ) . resample ( _STR_ ) . sum ( ) . plot ( y = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , legend = False ) ;
import engarde . decorators as ed
pd . DataFrame ( d , index = [ _STR_ , _STR_ ] , columns = [ _STR_ , _STR_ ] )
inspector = inspect ( engine )   $ inspector . get_table_names ( )   $
df [ df [ _STR_ ] == False ] [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] . sort_values ( by = _STR_ , ascending = False ) . head ( )
autos [ _STR_ ] . head ( 10 )
df . to_csv ( _STR_ )
print _STR_   $ print _STR_   $ pd . to_datetime ( [ _STR_ , _STR_ ] , errors = _STR_ )
df [ _STR_ ] . groupby ( by = df . index . month . isin [ 6 , 7 , 8 ] ) . counts ( )
old_page_converted = np . random . binomial ( 1 , conv_mean , 145274 )
dta . head ( 5 )
id_range = range ( results . index . values . min ( ) , results . index . values . max ( ) )   $ results . reindex ( id_range ) . head ( )   $
df . groupby ( _STR_ ) [ _STR_ ] . mean ( )
df = df [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ]   $ print ( df . head ( ) )
seq2seq_model . summary ( )
s . ix [ 2 : 5 ] . mean ( )
learner . save ( _STR_ )
now = datetime . datetime . utcnow ( ) . isoformat ( ) + _STR_   $ CONFERENCE_START = _STR_   $ LEVEL_LIST = [ _STR_ , _STR_ , _STR_ ]
sqlContext . sql ( _STR_ ) . show ( )
actual_diff = df2 [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] . mean ( ) - df2 [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] . mean ( )   $ ( p_diffs > actual_diff ) . mean ( )
keywords_qs = pd . read_excel (   $ _STR_   $ )   $ keywords_qs
cust_data1 = cust_data . assign ( No_of_30_Plus_DPD = cust_data . No_of_30_59_DPD + cust_data . No_of_60_89_DPD + cust_data . No_of_90_DPD ,   $ MonthlySavings = cust_data . MonthlyIncome * 0.15 )
col = _STR_   $ tmp_df . loc [ tmp_df [ col ] == _STR_ , col ] = float ( _STR_ )
autos . price . value_counts ( ) . sort_index ( ascending = True ) . head ( 20 )
my_gempro . kegg_mapping_and_metadata ( kegg_organism_code = _STR_ )   $ print ( _STR_ , my_gempro . missing_kegg_mapping )   $ my_gempro . df_kegg_metadata . head ( )
df_subset . info ( )
final_df_test = dt_features_test . join ( tree_c_features_test )
try :   $ alldata . to_csv ( csv_name , sep = _STR_ )   $ print ( _STR_ )   $ except :   $ print ( _STR_ )
tweet_list = [ ]   $ for line in f :   $ tweet_list . append ( line . split ( _STR_ ) )
df2 [ df2 . user_id . duplicated ( keep = False ) ]
prop_new_page = df2 [ df2 [ _STR_ ] == _STR_ ] . shape [ 0 ] / df2 . shape [ 0 ]   $ prop_new_page
train = K . function ( inputs = [ x , target ] , outputs = [ loss ] , updates = updates )
del merged_portfolio_sp [ _STR_ ]   $ merged_portfolio_sp . rename ( columns = { _STR_ : _STR_ , _STR_ : _STR_   $ , _STR_ : _STR_ } , inplace = True )   $ merged_portfolio_sp . head ( )
sns . pairplot ( segmentData , diag_kind = _STR_ , hue = _STR_ ) ;
p_old = df2 [ df2 [ _STR_ ] == 1 ] . shape [ 0 ] / df2 . shape [ 0 ]   $ p_old
is_wrong_timestamp = blame . timestamp < initial_commit   $ wrong_timestamps = blame [ is_wrong_timestamp ]   $ blame . timestamp = blame . timestamp . clip ( initial_commit )   $ len ( wrong_timestamps )
multi_counts = [ ( w , word_counts [ w ] ) for w in word_counts if word_counts [ w ] > 0.05 ]   $ multi_counts = sorted ( multi_counts , key = lambda w : w [ 1 ] , reverse = True )   $ multi_counts
possible_bots = users . query ( _STR_ )   $ possible_bots . shape
a = pd . read_sql_query ( q , conn )   $ a   $
td + pd . tseries . offsets . Minute ( 15 )
assert len ( [ col for col in cols_to_drop if col in twitter_archive_clean . columns ] ) == 0
url = _STR_   $ html_txt = urllib . request . urlopen ( url ) . read ( )   $ dom = lxml . html . fromstring ( html_txt )   $ for link in dom . xpath ( _STR_ ) : # select the url in href for all a tags(links) $     print(link)
merged_NNN . sort_values ( _STR_ , ascending = False )
sns . set ( )   $ sns . distplot ( df_input_clean . toPandas ( ) . Resp_time , kde = True , color = _STR_ )
cells_file = _STR_   $ rates_file = configure [ _STR_ ] [ _STR_ ]   $ plot_rates_popnet ( cells_file , rates_file , model_keys = _STR_ )
f1 = ( df [ _STR_ ] >= 2000 )   $ f2 = ( df [ _STR_ ] . isin ( [ _STR_ , _STR_ ] ) )   $ interesting_teams = teams_sorted_by_wins [ f1 & f2 ]   $ interesting_teams . head ( 20 )
y . head ( )
tfav2 = tfav . values   $ SA2 = SA1 * 100000
for col in full . columns :   $ print ( full [ col ] . value_counts ( ) )
p = pd . Period ( _STR_ , freq = _STR_ )
mini_csv_data_df = non_blocking_df_save_or_load_csv (   $ mini_csv_data_df ,   $ _STR_ . format ( fs_prefix ) )
with open ( _STR_ ) as f :   $ tweets = json . load ( f )   $ tweets [ : 2 ]
column = inspector . get_columns ( _STR_ )   $ for c in column :   $ print ( c [ _STR_ ] , c [ _STR_ ] )
predictions . select ( _STR_ , _STR_ , _STR_ ) . show ( )   $
print ( _STR_ )   $ print ( len ( all_enterprise [ _STR_ ] ) )   $ df = all_enterprise [ _STR_ ]   $ df = json_normalize ( df )   $ df . reindex ( [ _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 ) [ 0 : 5 ]
df_res = sqlContext . createDataFrame ( aaa )
Measurements = Base . classes . hawaii_measurement   $ Stations = Base . classes . hawaii_station   $
fg = sns . FacetGrid ( data = motion_at_home_df [ motion_at_home_df [ _STR_ ] == 0 ] , hue = _STR_ , aspect = 5 ) # $ fg.map(plt.scatter, 'time', 'state').add_legend()
pd . crosstab ( data . rate_marriage , data . affair . astype ( bool ) ) . plot ( kind = _STR_ )   $ plt . title ( _STR_ )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )   $ plt . show ( )
kick_projects . loc [ : , _STR_ ] = kick_projects [ _STR_ ] / kick_projects [ _STR_ ] # Pledged amount as a percentage of goal. $ kick_projects.loc[kick_projects['backers'] == 0, 'backers'] = 1 $ kick_projects.loc[:,'pledge_per_backer'] = kick_projects['pledged'] / kick_projects['backers'] # Pledged amount per backer.
unique_speaker_id = speeches_cleaned [ _STR_ ] . unique ( )   $ print ( len ( unique_speaker_id ) )
pd . set_option ( _STR_ , 120 )   $ pd . set_option ( _STR_ , 300 )   $ df [ ( df [ _STR_ ] == _STR_ ) & ( df [ _STR_ ] . str . lower ( ) . str . contains ( _STR_ ) | ( df [ _STR_ ] . str . lower ( ) . str . contains ( _STR_ ) ) ) ]
import re   $ pbptweets . loc [ pbptweets [ _STR_ ] . apply ( lambda x : any ( re . findall ( _STR_ , x ) ) ) ] [ [ _STR_ , _STR_ , _STR_ ] ]
% run _STR_
pd . DataFrame ( [ m . params for m in models ] , index = model_dates ) . T
df . groupby ( _STR_ ) [ _STR_ ] . nunique ( ) . agg ( [ _STR_ , _STR_ , _STR_ ] )
import gc   $ del full_data   $ gc . collect ( )
grid_id_flat = grid_id_array . flatten ( )
obs_diff = ( sum ( ( df2 . group == _STR_ ) & ( df2 . converted == 1 ) ) / sum ( df2 . group == _STR_ ) ) - ( sum ( ( df2 . group == _STR_ ) & ( df2 . converted == 1 ) ) / sum ( df2 . group == _STR_ ) )   $ obs_diff
df_weather = df_weather [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] . copy ( )
ek . get_news_headlines ( _STR_ , date_from = _STR_ , date_to = _STR_ )
salesdec [ _STR_ ] = salesdec [ _STR_ ]   $ salesdec [ _STR_ ] . isnull ( ) . any ( )   $
All_tweet_data_v2 . name [ ( All_tweet_data_v2 . name . str . contains ( _STR_ ) ) ] . value_counts ( )
rain = session . query ( Measurement . date , Measurement . prcp ) . \   $ filter ( Measurement . date > last_year ) . \   $ order_by ( Measurement . date ) . all ( )   $ print ( rain )
df_clean3 . loc [ 979 , _STR_ ]
all_test_times_dates . head ( )
daily_cases . unstack ( ) . T . fillna ( 0 ) . cumsum ( ) . plot ( )
journeys_scored . loc [ anomaly_condition2 , [ _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ ] ]
precip_df . rename ( columns = { _STR_ : _STR_ } , inplace = True )   $ precip_df . set_index ( _STR_ , inplace = True )   $ precip_df . head ( )
Z = np . random . randint ( 0 , 10 , ( 3 , 3 ) )   $ print ( Z )   $ print ( Z [ Z [ : , 1 ] . argsort ( ) ] )
print ( len ( a . intersection ( b ) ) )
p_diffs = np . array ( p_diffs )   $ ( p_diffs > diff1 ) . mean ( )   $
model = pd . get_dummies ( auto_new . CarModel )   $ model = model . ix [ : , [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ]   $ model . head ( )
bb . head ( )
autos [ ( ( autos . price > 500000 ) & ~ ( autos . name . str . contains ( _STR_ ) ) ) ]   $ autos = autos [ ~ ( ( autos . price > 500000 ) & ~ ( autos . name . str . contains ( _STR_ ) ) ) ] #antimasking $ autos.shape #stripped off 12 rows
print ( len ( df_concat ) ) #Prints number of rows. $ df_concat.head()
from sklearn . preprocessing import LabelEncoder
stack_with_kfold_cv [ _STR_ ] = gb_best . predict ( train [ col ] )   $ stack_with_kfold_cv [ _STR_ ] = knn_best . predict ( train [ col ] )   $ stack_with_kfold_cv [ _STR_ ] = xgb_best . predict ( train [ col ] )   $ stack_with_kfold_cv [ _STR_ ] = rf_best . predict ( train [ col ] )   $ stack_with_kfold_cv [ _STR_ ] = lgb_best . predict ( train [ col ] )
piotroski_univ_sets = soup . select ( _STR_ )   $ piotroski_univ_sets
b_list . head ( 2 )
archive_clean . drop ( [ _STR_ ,   $ _STR_ ,   $ ] ,   $ axis = 1 , inplace = True )
total_students_with_passing_math_score = len ( df_students . loc [ df_students [ _STR_ ] > 69 ] )   $ total_students_with_passing_math_score
week44 = week43 . rename ( columns = { 308 : _STR_ } )   $ stocks = stocks . rename ( columns = { _STR_ : _STR_ , _STR_ : _STR_ } )   $ week44 = pd . merge ( stocks , week44 , on = [ _STR_ , _STR_ ] )   $ week44 . drop_duplicates ( subset = _STR_ , inplace = True )
print ( r . text [ 0 : 500 ] )
regression_model . score ( X_test , y_test )
pd . Series ( labels , y ) . value_counts ( )   $
cursor . execute ( _STR_ )   $
xmlData [ xmlData [ _STR_ ] . str . match ( _STR_ ) ] [ [ _STR_ , _STR_ , _STR_ ] ]
train_users_pd . loc [ train_users_pd [ _STR_ ] == _STR_ , _STR_ ] = _STR_   $ train_users_pd . loc [ train_users_pd [ _STR_ ] == _STR_ , _STR_ ] = _STR_
prob_ZeroFill = LogisticModel_ZeroFill . predict_proba ( Test )   $ prob_ZeroFill
to_be_predicted_Day1 = 54.45   $ predicted_new = ridge . predict ( to_be_predicted_Day1 )   $ predicted_new
jobs_data1 = json_normalize ( json_data1 [ _STR_ ] )   $ jobs_data1 . head ( 5 )
ticks = data . ix [ : , [ _STR_ , _STR_ ] ]   $ ticks . head ( )
df_raw_tweet = pd . read_csv ( _STR_ , encoding = _STR_ )   $ print ( df_raw_tweet . head ( ) )
data . date . values
data . head ( )
tweets_clean . to_csv ( _STR_ )   $ images_clean . to_csv ( _STR_ )
melted_total . groupby ( [ _STR_ , _STR_ ] ) . mean ( ) . unstack ( ) [ _STR_ ] . ix [ top10_categories . index ] . plot . bar ( legend = True , figsize = ( 10 , 5 ) )
df = df . sort_values ( [ _STR_ , _STR_ ] , ascending = [ True , False ] )
yr15 = likes [ likes . year == 2015 ]   $ len ( yr15 )
y_hat_lr = lr . predict ( train_4 )
with open ( _STR_ , _STR_ ) as f :   $ pickle . dump ( TotalNameEvents , f )   $
type ( git_log . timestamp [ 0 ] )
fname_messages = _STR_   $ messages = load_messages ( fname_messages ) # messages data $ messages.head()
new = ( df2 [ _STR_ ] == _STR_ ) . mean ( )   $ new   $
df2 . query ( _STR_ ) . count ( ) [ 0 ] / df2 . shape [ 0 ]
y_pred = log_reg . predict ( X_test )
def plot ( x , y ) :   $ plt . figure ( figsize = ( 10 , 3 ) )   $ plt . plot ( x , y , marker = _STR_ , linewidth = 0 )   $ plt . ylim ( - 1000 , )
db . tweets . count ( )
df . loc [ ( df . company == False ) & ( df . other == False ) , _STR_ ] = df [ ( df . company == False ) &   $ ( df . other == False ) ] . name . isin ( other_list )
lr2 = LogisticRegression ( solver = _STR_ , multi_class = _STR_ , random_state = 20 , max_iter = 1000 )   $ lr2 . fit ( X , y )
df2 [ ( ( df2 [ _STR_ ] == _STR_ ) == ( df2 [ _STR_ ] == _STR_ ) ) == False ] . shape [ 0 ]
cm = confusion_matrix ( y_true , y_pred )   $ cm_normalized = cm . astype ( _STR_ ) / cm . sum ( axis = 1 ) [ : , np . newaxis ]   $ plot_confusion_matrix ( cm_normalized , labels = y_labels , title = _STR_ )
temperature = session . query ( measurement . date , measurement . tobs ) . \   $ filter ( measurement . date >= _STR_ ) . \   $ filter ( measurement . station == _STR_ ) . \   $ order_by ( measurement . date ) . all ( )   $ temperature
df2 = df . query ( _STR_ )   $ df2 . head ( 2 )
df . max ( )
s519397_df [ _STR_ ] . max ( )
df . plot . scatter ( x = _STR_ , y = _STR_ , title = _STR_ , color = _STR_ )
ABT_tip . shape
clf = OneVsRestClassifier ( estimator = linear_model . SGDClassifier ( loss = _STR_ , penalty = _STR_ ,   $ alpha = 0.001 , fit_intercept = True , n_iter = 10 ,   $ shuffle = True , verbose = 1 , epsilon = 0.1 , n_jobs = - 1 ,   $ random_state = SVM_SEED , learning_rate = _STR_ , eta0 = 0.0 ,   $ class_weight = None , warm_start = False ) , n_jobs = 1 )   $
p_diff_mean = p_diffs . mean ( )
tz_tmp = timezone_df . groupby ( _STR_ ) . agg ( { _STR_ : { _STR_ : _STR_ } } )   $ tz_tmp . columns = tz_tmp . columns . get_level_values ( 1 )   $ tz_tmp . reset_index ( inplace = True )
predictions . show ( )
N_old = df2 . query ( _STR_ ) [ _STR_ ] . count ( )   $ print ( _STR_ . format ( N_old ) )
autos [ _STR_ ] . unique ( ) . shape
df_new [ [ _STR_ , _STR_ , _STR_ ] ] = pd . get_dummies ( df_new [ _STR_ ] )
sel = [ Measurement . date ,   $ Measurement . tobs ]   $ day_prcp = session . query ( * sel ) . filter ( ( Measurement . date > year_ago ) ) . all ( )   $
to_be_predicted_Day1 = 47.75   $ predicted_new = ridge . predict ( to_be_predicted_Day1 )   $ predicted_new
blocks . head ( )
max ( ( stock - mini ) . abs ( ) )
np . sum ( ttt [ ttt > 1 ] ) * 1. / len ( ttt )
with twitter_file as f :   $ first_line = f . readline ( )   $ print ( first_line )   $ second_line = f . readline ( )   $ print ( second_line )
data . columns = data . columns . str . replace ( _STR_ , _STR_ )   $ data . columns [ data . columns != data . columns . str . extract ( _STR_ ) ]
festivals = festivals [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ]   $ list ( festivals . columns . values )
df2 [ df2 [ _STR_ ] . duplicated ( keep = False ) ]
dt_features [ _STR_ ] = pd . to_datetime ( dt_features [ _STR_ ] , unit = _STR_ )
merged = pd . merge ( prop , contribs , on = _STR_ )
words_hash_sk = [ term for term in words_sk if term . startswith ( _STR_ ) ]   $ corpus_tweets_streamed_keyword . append ( ( _STR_ , len ( words_hash_sk ) ) ) # update corpus comparison $ print('List and total number of hashtags: ', len(words_hash_sk)) #, set(terms_hash_stream))
plt . style . use ( _STR_ )   $
autos = autos [ autos [ _STR_ ] . between ( 70000 , 150000 ) ]   $ autos [ _STR_ ] . hist ( )
df_ids . to_csv ( _STR_ )
daily_df = all_turnstiles . groupby ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , as_index = False ) . sum ( )   $ daily_df . head ( )
data [ _STR_ ] . shape
new_page_converted = np . random . choice ( [ 0 , 1 ] , size = n_new , p = [ ( 1 - p_new ) , p_new ] )
U M M
proportion = df [ df [ _STR_ ] == 1 ] . user_id . nunique ( ) / number_rows   $ print ( _STR_ . format ( proportion ) )
df = gpd . GeoDataFrame ( df )
with open ( os . path . expanduser ( _STR_ ) ) as f :   $ creds = yaml . load ( f )
data_final . head ( )
df2 . info ( )
active_stations = session . query ( Measurement . station , func . count ( Measurement . tobs ) ) . group_by ( Measurement . station ) . \   $ order_by ( func . count ( Measurement . tobs ) . desc ( ) ) . all ( )   $ busiest_station = active_stations [ 0 ] [ 0 ]         $ print ( _STR_ , busiest_station )   $ active_stations
r . json ( ) [ _STR_ ] [ _STR_ ]
cfs_df = pd . read_csv ( path + _STR_ )
df_new [ _STR_ ] = 1   $ df_new [ [ _STR_ , _STR_ ] ] = pd . get_dummies ( df_new [ _STR_ ] )   $ df_new = df_new . drop ( _STR_ , axis = 1 )   $ df_new . head ( )   $ df_new [ [ _STR_ , _STR_ , _STR_ ] ] = pd . get_dummies ( df_new [ _STR_ ] )
lb = articles [ _STR_ ] . map ( len ) . quantile ( .025 )   $ ub = articles [ _STR_ ] . map ( len ) . quantile ( .975 )   $ articles = articles . loc [ ( articles [ _STR_ ] . map ( len ) > lb ) & ( articles [ _STR_ ] . map ( len ) < ub ) , : ]
plt . plot ( spks [ : , 0 ] , spks [ : , 1 ] , _STR_ )   $ plt . xlim ( 0 , config_file [ _STR_ ] [ _STR_ ] )   $ plt . ylim ( - 1 , 7.5 )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )
loan_stats [ _STR_ ] . table ( )
x_normalized = pd . DataFrame ( x_scaled )
df2 . nunique ( )
join_a . count ( )
from sklearn . feature_selection import RFE   $ rfe = RFE ( lm , 10 )
plt . figure ( figsize = ( 16 , 10 ) )   $ plt . clf ( )   $ sns . boxplot ( x = _STR_ , y = _STR_ , hue = _STR_ , data = df )   $ plt . xticks ( rotation = 90 )
NYG_analysis2 = NYG_analysis [ _STR_ ] . mean ( )
click_condition_meta [ _STR_ ] = np . where ( ( pd . isnull ( click_condition_meta [ _STR_ ] ) ) ,   $ _STR_ , click_condition_meta . geo_country )
df2 . query ( _STR_ ) [ _STR_ ] . mean ( )
trn_df . dtypes
to_be_predicted_Day3 = 22.34206034   $ predicted_new = ridge . predict ( to_be_predicted_Day3 )   $ predicted_new
tweet_archive_enhanced_clean [ _STR_ ] = tweet_archive_enhanced_clean [ _STR_ ] . astype ( str )   $
log_model . coef_
merged = merged . rename ( columns = { _STR_ : _STR_ } )
iris . head ( ) . iloc [ : , : 1 ]
Log_model . fit ( ) . summary ( )
ayush = relevant_data [ relevant_data [ _STR_ ] == _STR_ ]   $ ayush [ _STR_ ] . value_counts ( ) . plot ( kind = _STR_ )
df_prep3 = df_prep ( df3 )   $ df_prep3_ = pd . DataFrame ( { _STR_ : df_prep3 . index , _STR_ : df_prep3 . values } , index = pd . to_datetime ( df_prep3 . index ) )
coins = get_coin_list ( )   $ COIN_DB = pd . DataFrame . from_dict ( coins , orient = _STR_ )   $ print ( COIN_DB . head ( ) )
Y_mat = Y_df1 . interest . as_matrix ( )   $ Y_mat [ : 5 ]
tobs_info = ( session . query ( Measurement . date , Measurement . tobs ) . \   $ filter ( Measurement . station == _STR_ ) . \   $ filter ( Measurement . date >= _STR_ ) . all ( ) )   $ tobs_info
svm_tunned . best_params_
ps . to_timestamp ( _STR_ , how = _STR_ )
na_df . isna ( ) # check elements that are missing
df . describe ( )
events [ [ _STR_ , _STR_ ] ] . head ( )
df_R . tail ( )
s1 . values
y = df_cond [ _STR_ ]   $ y = y . replace ( _STR_ , 0 ) . replace ( _STR_ , 1 ) . replace ( _STR_ , 1 )
bremen . info ( )
df2 [ [ _STR_ , _STR_ ] ] = pd . get_dummies ( df2 [ _STR_ ] )   $ df2 . head ( )
df2 . query ( _STR_ ) . count ( ) . user_id
fb_vec = cv . fit_transform ( [ ftfy . ftfy ( fb_cleaned ) ] )
y_know . shape
p_new = df2 [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] . mean ( )   $ print ( _STR_ , p_new )
fashion . shape
np . exp ( - 0.0140 ) , 1 / np . exp ( - 0.0140 )
df = df . set_index ( _STR_ ) . join ( df2 . set_index ( _STR_ ) ) ;
output = _STR_   $ cursor . execute ( output )   $
omdb_df . info ( )
plt . figure ( figsize = [ 14 , 6 ] )   $ plt . plot ( actual1 [ : 200 ] , _STR_ )   $ plt . plot ( benchmark21 [ : 200 ] , _STR_ )   $ plt . plot ( pred1 [ : 200 ] , _STR_ )
not_in_misk = not_in_misk . copy ( ) . sort_values ( _STR_ )
df = pd . DataFrame ( np . random . randn ( 4 , 3 ) , columns = [ _STR_ , _STR_ , _STR_ ] )
sample_single_contrl = control_group . sample ( size_control , replace = True )   $ old_page_converted = ( sample_single_contrl [ _STR_ ] == 1 ) . sum ( ) / sample_single_contrl . user_id . count ( )   $ old_page_converted
by_day_by_hour14 = uber_14 . groupby ( [ _STR_ , _STR_ ] ) . size ( )   $ by_day_by_hour14 . head ( )
dataset . hist ( )
num_of_dtmodel_pred . keys ( )
graf_train = pd . concat ( [ graf_train , train_embedding ] , axis = 1 )
small_frame . rbind ( small_frame )
from pyspark import SparkContext   $ from pyspark . streaming import StreamingContext   $ import json   $ import time   $ from datetime import datetime
pd . merge ( df1 , def2 , ... )   $ pd . concat ( [ df1 , df2 , df3 , ... ] )
new_page_converted = np . random . choice ( [ 1 , 0 ] , size = n_new , p = [ p_mean , ( 1 - p_mean ) ] , replace = True )
df . loc [ _STR_ ] > 0
sia = SIA ( )   $ sentiment = stock [ _STR_ ] . apply ( sia . polarity_scores )   $ sent = pd . DataFrame ( list ( sentiment ) )   $ sent . index = stock . index
% timeit pd . to_numeric ( df_census . loc [ : , _STR_ ] )
news_dict_df . to_json ( _STR_ )
tweet . author
df . to_csv ( _STR_ )
bruins . head ( )
token_entity_type = [ token . ent_type_ for token in parsed_review ]   $ token_entity_iob = [ token . ent_iob_ for token in parsed_review ]   $ pd . DataFrame ( zip ( token_text , token_entity_type , token_entity_iob ) ,   $ columns = [ _STR_ , _STR_ , _STR_ ] )
np . eye ( 3 , dtype = _STR_ ) #Identity matrix. float64 is the default dtype and can be omitted
sum ( clean_rates . text . str . contains ( _STR_ ) )
gmm2 = mixture . GMM ( n_components = num_comps )   $ fit2 = gmm2 . fit_predict ( X )   $ mdf [ _STR_ ] = fit2   $ gmm2df = mdf . copy ( )   $ print _STR_ , gmm2 . bic ( X )
words = bow_converter . get_feature_names ( )   $ len ( words )
( df2 [ _STR_ ] == _STR_ ) . mean ( )
visitors . head ( 5 )
df [ df . isna ( ) ] . count ( )
print ( _STR_ )   $ sb . regplot ( y_pred , errors , fit_reg = False )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )
df2 = pd . read_csv ( _STR_ )
autos [ _STR_ ] . describe ( )
stock_list = df_release [ _STR_ ] . unique ( ) #plan to loop for each ticker, fiscal_year and fiscal_quarter $ print(stock_list) $ print(len(stock_list)) $ df_release_pivot = df_release.set_index(['ticker', 'fiscal_year', 'fiscal_quarter']).sort_index() $ df_release_pivot
average_polarity = pd . concat ( [ average_polarity_2012 , average_polarity_2013 , average_polarity_2014   $ , average_polarity_2015 , average_polarity_2016 ] , axis = 1 )
y_test_over [ rfc_bet_over ] . mean ( )
mars_weather = current_weather_info [ 0 ] . text   $ mars_weather
yc_sd . columns
print ( _STR_ )   $ print ( len ( all_mobile [ _STR_ ] ) )   $ df = all_mobile [ _STR_ ]   $ df = json_normalize ( df )   $ df . reindex ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 ) [ 0 : 5 ]
customer_visitors_new . index . levels [ 1 ]
pred_probas_over_fm = gs_from_model . predict_proba ( X_test )   $ fm_bet_over = [ x [ 1 ] > .62 for x in pred_probas_over_fm ]
yelp_class . shape
print ( _STR_ . format ( len ( atloc_opp_dist [ _STR_ ] . unique ( ) ) ) )
data_activ [ _STR_ ] = data_activ [ _STR_ ] . apply ( lambda x : datetime . strptime ( x , _STR_ ) if not pd . isnull ( x ) else _STR_ )   $ data_activ [ _STR_ ] = data_activ [ _STR_ ] . apply ( lambda x : x . strftime ( _STR_ ) if not pd . isnull ( x ) else _STR_ )   $ data_activ [ _STR_ ] = data_activ [ _STR_ ] . apply ( lambda x : x . strftime ( _STR_ ) if not pd . isnull ( x ) else _STR_ )
combined_df . head ( 2 )
n_old = df2 . query ( _STR_ ) [ _STR_ ] . count ( )   $ n_old
( autos [ _STR_ ]   $ . value_counts ( )   $ . head ( )   $ . sort_index ( ascending = False ) )
lm_2 = sm . Logit ( new_df2 [ _STR_ ] , new_df2 [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ reg_lm2 = lm_2 . fit ( )   $ reg_lm2 . summary ( )
merged_index_year = merged2 . index . year
pd . isnull ( doglist )
for tweet in tweepy . Cursor ( api . search , q = _STR_ % place_id_L ) . items ( maxitems ) :   $ detected = detect ( tweet . text )   $ conn . commit ( )
BAL = pd . read_excel ( url_BAL ,   $ skiprows = 8 ) # This reads in each team's respective spreadsheet, ignoring the header.
highlandpark_potholes = data_311 [ potholes & highland_park ]   $ print ( highlandpark_potholes . shape )   $ highlandpark_potholes . head ( )
ts . resample ( _STR_ ) . sum ( )
df . groupby ( _STR_ ) [ _STR_ ] . mean ( )
random . randrange ( 0 , 101 , 5 )
mentions_count = mentions . value_counts ( ) . head ( 10 )   $ mentions_count
print ( _STR_ % len ( joined . loc [ joined [ _STR_ ] . isnull ( ) ] ) )
job_requirements . ndim   $
events . schema
import numpy as np   $ preTest = [ 4 , 4 , 31 , 2 , 3 ] ,   $ postTest = [ 25 , 25 , 57 , 62 , 70 ]   $ print np . mean ( preTest )   $ print np . mean ( postTest )
logit_ctries = sm . Logit ( df_final [ _STR_ ] ,   $ df_final [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ result_ctries = logit_ctries . fit ( )   $ result_ctries . summary ( )
linear . delete_endpoint ( )
news . date . dt . date . value_counts ( ) . plot ( linewidth = 0.5 , figsize = ( 16 , 5 ) , title = _STR_ ) ;
dfPre = df . loc [ _STR_ : _STR_ ]   $ dfPost = df . loc [ _STR_ : _STR_ ]
day = lambda x : datetime . date ( int ( x [ 0 ] ) , int ( x [ 1 ] ) , int ( x [ 2 ] ) ) . weekday ( )   $ emotion_big_df [ _STR_ ] = emotion_big_df [ _STR_ ] . apply ( day )
nba_df . head ( 10 )
print ( _STR_ , len ( df2 . user_id . unique ( ) ) )   $ print ( _STR_ , len ( df2 ) - len ( df2 . user_id . unique ( ) ) )
avg_per_seat_price_seasonsandteams [ _STR_ , _STR_ ] # This is the average price of PSLs before BAL won the SuperBowl.
actual_payments . iso_date . unique ( )
df . head ( )
twitter_df_clean . drop ( twitter_df_clean [ twitter_df_clean . retweeted_status_id . notnull ( ) ] . index , inplace = True )
offseason16 = ALL [ ( ALL . index > _STR_ ) & ( ALL . index < _STR_ ) ]
a400hz = hc . table ( _STR_ )   \   $ . where ( _STR_ . format ( start_date , end_date ) )   \   $ . persist ( )
bb_df . isnull ( ) . any ( )
hr [ _STR_ ] = hr [ _STR_ ] - time_delta   $ hr [ _STR_ ] = hr [ _STR_ ] - time_delta   $ bp [ _STR_ ] = bp [ _STR_ ] - time_delta   $ bp [ _STR_ ] = bp [ _STR_ ] - time_delta   $ bp . head ( )
damd [ _STR_ ] . groupby ( by = damd [ _STR_ ] . dt . year ) . count ( ) . plot . bar ( figsize = ( 5 , 6 ) , title = _STR_ ) . grid ( True , axis = _STR_ )
df_test . dtypes
df_userIds = pd . DataFrame ( { _STR_ : user_ids , _STR_ : 1 } )   $ df_productIds = pd . DataFrame ( { _STR_ : product_ids , _STR_ : 1 } )   $
dule2 . drop ( columns = [ _STR_ ] , inplace = True )   $ dule2
results_df . plot ( kind = _STR_ , stacked = False , x_compat = True , alpha = .2 )   $ plt . show ( )
dfU = df . drop_duplicates ( _STR_ )   $ dfU . info ( )
df2 = pd . read_csv ( _STR_ )
day_of_week15 . to_excel ( writer , index = True , sheet_name = _STR_ )
dfMonth = dfDay . copy ( deep = True )
type ( rng . asi8 )
df_new [ _STR_ ] = df_new [ _STR_ ] . replace ( ( _STR_ , _STR_ , _STR_ ) , ( 0 , 0 , 1 ) )   $ lm_ca = sm . OLS ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ ] ] )   $ results_ca = lm_ca . fit ( )   $ results_ca . summary ( )
n_old = len ( df2 . query ( _STR_ ) )   $ n_old
archive_df_clean = archive_df_clean . drop ( columns = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] )
train_users = pd . read_csv ( _STR_ )
def load_file ( file_name ) :   $ with codecs . open ( file_name , _STR_ , _STR_ ) as infile :   $ text = infile . read ( )   $ return text
print ( sp . pi )   $ print ( _STR_ . format ( sp . pi ) )   $ print ( _STR_ . format ( sp . pi ) )   $ print ( _STR_ . format ( sp . pi ) )
filtered_df [ _STR_ ] . describe ( )
all_weekdays = pd . date_range ( start = start , end = end , freq = _STR_ )
model . most_similar ( [ trial ] )
ts . asfreq ( pd . tseries . offsets . BDay ( ) )
ux . html_utils . get_webpage_meta ( urls [ 0 ] )
classifier = LogisticRegression ( random_state = 0 )   $ classifier . fit ( X_train , y_train )
df_mod . head ( )
g = mixture . GMM ( n_components = 3 )   $ g . fit ( cluster )
purchases [ _STR_ ] . value_counts ( )
for col in missing_info :   $ percent_missing = data [ data [ col ] . isnull ( ) == True ] . shape [ 0 ] / data . shape [ 0 ]   $ print ( _STR_ . format ( col , percent_missing ) )
df [ df . isnull ( ) . any ( axis = 1 ) ]
end_date = [ USER_PLANS_df . loc [ cid , _STR_ ] [ np . argmax ( USER_PLANS_df . loc [ cid , _STR_ ] ) ] if USER_PLANS_df . loc [ cid , _STR_ ] [ np . argmax ( USER_PLANS_df . loc [ cid , _STR_ ] ) ] == False else USER_PLANS_df . loc [ cid , _STR_ ] [ np . argmax ( USER_PLANS_df . loc [ cid , _STR_ ] ) ] if USER_PLANS_df . loc [ cid , _STR_ ] [ np . argmax ( USER_PLANS_df . loc [ cid , _STR_ ] ) ] == True else None for cid in churned_ix ]
access_token = os . environ . get ( _STR_ , _STR_ )   $ access_token_secret = os . environ . get ( _STR_ , _STR_ )   $ consumer_key = os . environ . get ( _STR_ , _STR_ )   $ consumer_secret = os . environ . get ( _STR_ , _STR_ )
aggreg1 . to_excel ( _STR_ , index = False )
df_json_tweets [ _STR_ ] = df_json_tweets [ _STR_ ] . apply ( lambda time : time . strftime ( _STR_ ) )   $ df_json_tweets [ _STR_ ] = df_json_tweets [ _STR_ ] . apply ( lambda time : time . strftime ( _STR_ ) )
location = boto . s3 . connection . Location . USWest2
token_receiveavg = token_receivecnt . groupby ( _STR_ ) . agg ( { _STR_ : mean_except_outlier } ) . reset_index ( )
df = ( data . loc [ data [ _STR_ ] == False ] ) . drop ( [ _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 ) . sort_values ( ascending = True , by = _STR_ )   $ display ( HTML ( df . to_html ( ) ) )
goog . sort_values ( _STR_ , inplace = True )   $ goog . set_index ( _STR_ , inplace = True )   $ goog
rsp . json ( ) [ _STR_ ] == 0
np . random . seed ( 123456 )   $ dates = pd . date_range ( _STR_ , periods = 10 )   $ s1 = pd . Series ( np . random . randn ( 10 ) , dates )   $ s1 [ : 5 ]
pres_df [ _STR_ ] [ 0 ] . split ( _STR_ )
print ( status . created_at )
sms = df_sms . groupby ( _STR_ ) . agg ( { _STR_ : _STR_ , _STR_ : _STR_ , _STR_ : _STR_ } )   $ sms [ _STR_ ] = sms . messageCount * sms_value_R   $ sms
tweet_image_predictions_clean . info ( )
df . neu . shape
atdist_4x_positive = atdist_4x [ atdist_4x [ _STR_ ] . isin ( [ _STR_ , _STR_ ] ) ]   $ atdist_4x_positive . merge ( atloc_4x , how = _STR_ , on = [ _STR_ , _STR_ ] )
categoryByCountry_df = youtube_df [ youtube_df [ _STR_ ] . isin ( category_namesC ) ]   $ categoryByCountry_dfMax = categoryByCountry_df . loc [ categoryByCountry_df . groupby ( [ _STR_ ] ) [ _STR_ ] . idxmax ( ) ]   $ categoryByCountry_dfMax
train_cols = data . columns [ 1 : ]
iterables = [ eia_total_monthly . index . levels [ 0 ] , range ( 2001 , 2017 ) , range ( 1 , 13 ) ]   $ index = pd . MultiIndex . from_product ( iterables = iterables , names = [ _STR_ , _STR_ , _STR_ ] )   $ eia_extra = pd . DataFrame ( index = index , columns = [ _STR_ , _STR_ ,   $ _STR_ ] )
tweet_counts_by_month . tweet_count . plot ( )
df_master . to_csv ( _STR_ , index_label = False )
has_stage_archive = twitter_archive_master [ twitter_archive_master [ _STR_ ] == 1 ]   $ for column in has_stage_archive . iloc [ : , 8 : 12 ] . columns :   $ avg_rating = has_stage_archive [ has_stage_archive [ column ] == 1 ] [ _STR_ ] . mean ( )   $ print ( _STR_ . format ( column , avg_rating ) )
for each in range ( len ( sample_index ) ) :   $ honeypot_df [ _STR_ ] . replace ( sample_index [ each ] , sample_regular [ each ] , inplace = True )
total_sales [ _STR_ ] = ( sales_diff [ _STR_ ] . values / sales_diff [ _STR_ ] . values )
gr_e2 = df . query ( _STR_ )
dbData . head ( 3 )
calls_df . pivot_table ( [ _STR_ ] , [ _STR_ ] , aggfunc = _STR_ )
uniqueTags = np . unique ( catList )
p = pd . Period ( _STR_ , freq = _STR_ )
rentals [ _STR_ ] = rentals [ _STR_ ] . str . lower ( )
pop_dog = twitter_archive_master . groupby ( _STR_ ) [ _STR_ ] . mean ( ) . reset_index ( )   $ pop_dog . sort_values ( _STR_ , ascending = False ) . head ( )
old_page_converted = np . random . choice ( [ 1 , 0 ] , size = n_old , p = [ p_old , ( 1 - p_old ) ] )   $ print ( len ( old_page_converted ) )
bg_df2 . index # list all the row labels
import pandas as pd   $ data2 = pd . read_csv ( _STR_ , compression = _STR_ , header = 0 , sep = _STR_ , quotechar = _STR_ )   $
df . text [ df . text . str . len ( ) == df . text . str . len ( ) . min ( ) ]
StockData . to_csv ( StockDataFile + _STR_ , compression = _STR_ , index = False )
merged = pd . merge ( prop , contribs , on = _STR_ )
firstday_df = firstday_df . sort_values ( by = _STR_ , ascending = True )   $ firstday_df
bet_over = [ x [ 1 ] > .64 for x in pred_probas_over ]
name = contractor . groupby ( _STR_ ) [ _STR_ ] . nunique ( )   $ print ( name [ name > 1 ] )
! h5ls - r _STR_
new_page_converted = np . random . choice ( [ 0 , 1 ] , size = n_new , p = [ 1 - p_new , p_new ] )   $ new_page_converted . mean ( )
unique_users = df [ _STR_ ] . unique ( ) . shape [ 0 ]   $ unique_users   $
data = [ _STR_ , _STR_ , _STR_ , _STR_ ]   $ [ s . capitalize ( ) for s in data ]
twitter_archive_enhanced_clean = twitter_archive_enhanced_clean . drop ( [ _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 )   $ twitter_archive_enhanced_clean . head ( )
precip_df . describe ( )
rng2017 = pd . date_range ( _STR_ , periods = 12 , freq = _STR_ )   $ rng2018 = pd . date_range ( _STR_ , periods = 12 , freq = _STR_ )   $ rng2019 = pd . date_range ( _STR_ , periods = 12 , freq = _STR_ )
testmx2 = testmx . toarray ( )
crimes . info ( null_counts = True )
obj4 = pd . Series ( sdata , index = [ _STR_ , _STR_ , _STR_ , _STR_ ] )
pf = df   $ pf = pf [ ( pf [ _STR_ ] == _STR_ ) ]   $ pf = pf [ ~ pf [ _STR_ ] . isnull ( ) ]   $ pf = pf [ pf . elapsed < datetime . timedelta ( minutes = 70 ) ]   $ pf [ _STR_ ] = pf [ _STR_ ] . astype ( _STR_ )
segmentData [ _STR_ ] = segmentData . lead_converted_date . dt . to_period ( _STR_ )   $ segmentData [ _STR_ ] = segmentData . opportunity_qualified_date . dt . to_period ( _STR_ )   $ segmentData [ _STR_ ] = segmentData . lead_created_date . dt . to_period ( _STR_ )
df_questionable_2 = pd . merge ( left = df_questionable , left_on = _STR_ ,   $ right = df_usnpl_one_hot , right_on = _STR_ , how = _STR_ )
indata_dir = _STR_   $ indata = _STR_   $ result = cassession . loadTable ( indata_dir + _STR_ + indata + _STR_ , casout = indata )
df_tweet_json_clean . info ( )
print ( y_train . status . value_counts ( ) )   $ print ( y_valid . status . value_counts ( ) )   $ print ( y_test . status . value_counts ( ) )
for x in range ( 0 , len ( list_of_genre_1990s_clean ) ) :   $ list_of_genre_1990s_clean [ x ] = re . sub ( _STR_ , _STR_ , list_of_genre_1990s_clean [ x ] ) . strip ( _STR_ ) [ 2 : ]
Moscow_food . delivery_address . value_counts ( )
print ( _STR_ , Ralston [ _STR_ ] . mean ( ) - Ralston [ _STR_ ] . mean ( ) )
Columns = StockData . columns . values   $ Columns [ - 4 : ] = [ _STR_ . format ( N ) for N in range ( 1 , 5 ) ]   $ StockData . columns = Columns   $ print ( StockData . columns )
dream = str ( dreamSoup . body ) . split ( _STR_ ) [ 1 ] . split ( _STR_ ) [ 0 ]   $ user = re . search ( _STR_ , dreamSoup . title . text ) . group ( 1 )   $ daysPassed = re . search ( _STR_ , str ( dreamSoup . body ) ) . group ( 1 )   $
print ( attend_with . sum ( axis = 0 ) . sum ( ) )   $ print ( )   $ print ( attend_with . sum ( axis = 0 ) )
df2 . columns [ df2 . columns . str . upper ( ) . str . contains ( _STR_ ) ]
blacklist = vacancies [ ( vacancies . hour >= 20 ) | ( vacancies . hour < 10 ) | ( vacancies . weekday > 5 ) ]   $ blacklist . sort_values ( _STR_ ) [ [ _STR_ , _STR_ , _STR_ ] ]
temp . groupby ( temp ) . size ( ) . to_frame ( ) . rename ( columns = { 0 : _STR_ } ) . head ( 4 )
cur_a . callproc ( _STR_ ,   $ ( _STR_ , _STR_ ) )
df . drop ( remove_cols , axis = 1 , inplace = True )
! head - 24 fremont . csv
add_plane_data . registerTempTable ( _STR_ )
df_new [ _STR_ ] = df_new [ _STR_ ] * df_new [ _STR_ ]   $ df_new [ _STR_ ] = df_new [ _STR_ ] * df_new [ _STR_ ]   $ df_new . head ( )
mean = np . mean ( data [ _STR_ ] )   $ print ( _STR_ . format ( mean ) )
session_df . groupby ( by = _STR_ ) . agg ( { _STR_ : [ _STR_ , _STR_ , _STR_ ] } )
merged_daily = data . set_index ( _STR_ ) . resample ( _STR_ ) . size ( )   $ merged_monthly_mean = merged_daily . resample ( _STR_ ) . mean ( )   $ merged_monthly_mean . index = merged_monthly_mean . index . strftime ( _STR_ )
tweet_not_dog = stacked_image_predictions [ stacked_image_predictions [ _STR_ ] == False ] . index   $ stacked_image_predictions . drop ( tweet_not_dog , inplace = True )   $ stacked_image_predictions . reset_index ( drop = True , inplace = True )   $ stacked_image_predictions . head ( 10 )
for row in selfharmm_topic_names_df . iloc [ 5 ] :   $ print ( row )
snow . drop_table ( _STR_ )
search_df . info ( )
min ( TestData . Lead_Creation_Date_clean ) , max ( TestData . Lead_Creation_Date_clean )
seaborn . countplot ( vacancies . weekday )
from h2o . estimators . glm import H2OGeneralizedLinearEstimator   $ glm_model = H2OGeneralizedLinearEstimator ( model_id = _STR_ , family = _STR_ )   $ glm_model . train ( x = predictor_columns , y = target , training_frame = train , validation_frame = valid )
from sklearn . model_selection import StratifiedShuffleSplit   $ splitObject = StratifiedShuffleSplit ( n_splits = 1 , test_size = 0.2 , random_state = 42 )   $ for train_index , test_index in splitObject . split ( features , target ) :   $ X_train , X_test = features [ train_index ] , features [ test_index ]   $ y_train , y_test = target [ train_index ] , target [ test_index ]
pytrends . get_historical_interest ( kw_list , year_start = 2018 , month_start = 1 , day_start = 1 , hour_start = 0 , year_end = 2018 , month_end = 2 , day_end = 1 , hour_end = 0 , cat = 0 , geo = _STR_ , gprop = _STR_ , sleep = 10 )
dfNYC . head ( )
projFile = _STR_   $ schedFile = _STR_   $ budFile = _STR_
train . info ( )
s . find_all ( _STR_ )
target . shape
d = datetime . datetime ( 2018 , 11 , 12 , 12 )   $ for post in posts . find ( { _STR_ : { _STR_ : d } } ) . sort ( _STR_ ) :   $ pprint . pprint ( post )
ccl [ _STR_ ] = pd . to_datetime ( ccl . Date , format = _STR_ , errors = _STR_ )
dtypes = { _STR_ : np . str , _STR_ : np . int64 , _STR_ : np . int64 }   $ parse_dates = [ _STR_ ]   $ transactions = pd . read_csv ( _STR_ , dtype = dtypes , parse_dates = parse_dates ) # opens the csv file $ print("Rows and columns:",transactions.shape) $ pd.DataFrame.head(transactions)
kwargs = { _STR_ : xgb . XGBRegressor , _STR_ : mean_squared_error   $ , _STR_ : X_train , _STR_ : y_train , _STR_ : X_dev , _STR_ : y_dev }
X_future = sandag_df . values
df . iloc [ [ 1 , 2 , 5 ] , [ 0 , 3 ] ]
df_usa [ _STR_ ] = ( ( df_usa [ _STR_ ] * 1000 ) / df_usa [ _STR_ ] ) . round ( 2 )   $ df_usa
mpl . rc ( _STR_ , figsize = ( 6 , 3.5 ) )   $ prophet_model . plot ( forecast , uncertainty = True )   $ plt . show ;
reddit_comments_data . groupby ( _STR_ ) . count ( ) . orderBy ( _STR_ , ascending = False ) . show ( 100 , truncate = False )
data . sort_values ( by = _STR_ , ascending = False ) . head ( 4 )
top10 = git_blame [ git_blame . knowing ] . author . value_counts ( ) . head ( 10 )   $ top10
pulledTweets_df . head ( 20 )
dfClientes . shape
y_ = df [ _STR_ ] . replace ( to_replace = [ _STR_ , _STR_ ] , value = [ 1 , 0 ] )   $ y_ [ 0 : 5 ]
dfs = pd . read_html ( _STR_ , header = 0 )   $ dfs [ 4 ]
week23 = week22 . rename ( columns = { 161 : _STR_ } )   $ stocks = stocks . rename ( columns = { _STR_ : _STR_ , _STR_ : _STR_ } )   $ week23 = pd . merge ( stocks , week23 , on = [ _STR_ , _STR_ ] )   $ week23 . drop_duplicates ( subset = _STR_ , inplace = True )
def sale_lost ( count , minutes ) :   $ crepe_per_min = count // ( 4 * 60 ) * 0.002   $ crepes_lost = crepe_per_min * minutes   $ return crepes_lost
X = reddit [ _STR_ ] . values   $ y = reddit [ _STR_ ]   $ X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.33 , random_state = 42 )
! tar - xvzf rossmann . tgz
students . iloc [ 8 ]
my_file = open ( _STR_ )   $ my_file . close ( )
df_selected . filter ( _STR_ ) . toPandas ( )
df2 . groupby ( [ _STR_ ] ) [ _STR_ ] . mean ( )
x_eff = 30 * u . mbarn   $ density = 1 * u . cm ** - 3   $ interaction_time = ( density * x_eff * cst . c ) ** - 1   $ interaction_time . to ( _STR_ )
( df2 . query ( _STR_ ) [ _STR_ ] == 1 ) . mean ( )
new_page_converted . mean ( )
tw_clean [ tw_clean . name == _STR_ ]
countries_df = pd . read_csv ( _STR_ )   $ df_new = countries_df . set_index ( _STR_ ) . join ( df2 . set_index ( _STR_ ) , how = _STR_ )
print _STR_ % ( rounds . shape [ 0 ] , rounds_df . shape [ 0 ] )
import tqdm as tqdm   $ import os   $ import time   $ for file in tqdm . tqdm ( os . listdir ( _STR_ ) ) :   $ time . sleep ( 0.5 )
testObjDocs . outDF . tail ( 10 ) ## new records on the end ... still need to delete the bad ones
df_measures_users . head ( )
len ( df [ ( df [ _STR_ ] == _STR_ ) & ( df . index . month . isin ( [ 12 , 1 , 2 ] ) ) ] )
from sklearn . preprocessing import StandardScaler   $ from sklearn . decomposition import PCA   $ X_std = StandardScaler ( ) . fit_transform ( X )   $ pca = PCA ( n_components = 2 )   $ x_9d = pca . fit_transform ( X_std )   $
tweets = pd . read_csv ( _STR_ )   $ tweets [ _STR_ ] = _STR_   $ tweets [ _STR_ ] = np . NaN
ripple_github_issues_df [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] . head ( )
interests_groupby_user = df_interests [ _STR_ ] . groupby ( df_interests [ _STR_ ] )   $ lst_user_interests = [ [ name , group . tolist ( ) ] for name , group in interests_groupby_user ]   $ lst_user_interests [ 1 ]
twitter_archive_clean [ ~ twitter_archive_clean [ _STR_ ] . isnull ( ) ]
american_train_model . print_topics ( num_topics = 10 , num_words = 10 )
promo_df . drop ( _STR_ , 1 , inplace = True )
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_3 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_2 . drop ( [ _STR_ ] , axis = 1 )   $
results = pd . read_csv ( _STR_ , index_col = None )   $ number_of_posts = results [ _STR_ ] . count ( )   $ print ( _STR_ . format ( number_of_posts ) )
df [ _STR_ ] = ( df [ _STR_ ] - df [ _STR_ ] ) / df [ _STR_ ] * 100.0   $
predictors = wages . drop ( _STR_ , axis = 1 ) . values   $ print ( predictors . shape )   $ predictors
cust_demo . sample ( n = 700 , replace = False ) . duplicated ( ) . value_counts ( )
before_sentiment = si . overall_sentiment ( before )   $ during_sentiment = si . overall_sentiment ( during )   $ after_sentiment = si . overall_sentiment ( after )
Meter1 . ModeSet ( _STR_ )
df . head ( )
predictions_clean . tweet_id = predictions_clean . tweet_id . astype ( str )
p_new = df2 . converted . mean ( )   $ p_new
sum ( df2 [ _STR_ ] ) / df2 . shape [ 0 ]
df2_dummy = df2_dummy . drop ( [ _STR_ ] , axis = 1 )   $ df2_dummy . tail ( 1 )
print ( _STR_ ,   $ ab_file2 [ ab_file2 [ _STR_ ] == _STR_ ] [ _STR_ ] . mean ( ) )
url = _STR_   $ r = requests . get ( url )
! python csv_to_tfrecords . py - - csv_input = images / train / train_labels . csv - - image_dir = images / train - - output_path = train . record
pickle . dump ( nmf_tfidf_df , open ( _STR_ , _STR_ ) )
bigrams = nltk . bigrams ( tweet_no_stop )   $ word_freq = nltk . FreqDist ( bigrams )
wrd_clean [ _STR_ ] . describe ( )
df = pd . read_csv ( _STR_ , na_values = [ _STR_ ] )
def sigmoid_gradient ( z ) :     $ return np . multiply ( sigmoid ( z ) , ( 1 - sigmoid ( z ) ) )
yc_new1 = yc_new1 [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ]   $ yc_new1 . head ( )
def rmprecent ( s ) :   $ return s . split ( _STR_ ) [ 0 ]   $ hsi [ _STR_ ] = ( hsi [ _STR_ ] . apply ( rmprecent ) . astype ( _STR_ ) >= 1 ) . astype ( _STR_ )
final_log_mod = sm . Logit ( df_comb [ _STR_ ] , df_comb [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ results_final_log_mod = final_log_mod . fit ( )   $ results_final_log_mod . summary ( )
diff_pnew_pold = ( new_page_converted . mean ( ) ) - ( old_page_converted . mean ( ) )   $ print ( _STR_ . format ( diff_pnew_pold ) )
df2 = df2 . drop ( df2 . index [ 2893 ] )
rf = RandomForestClassifier ( n_estimators = 50 , random_state = 42 )   $ scores = cross_val_score ( rf , X , y , cv = skf )   $ print _STR_ , scores   $ print _STR_ , scores . mean ( )
fp7_proj . shape , fp7_part . shape , fp7 . shape
plt . scatter ( x , y )   $ plt . plot ( x , np . dot ( x_15 , linear . coef_ ) + linear . intercept_ , c = _STR_ )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )
loans_df . earliest_cr_line . value_counts ( )
male = crime . loc [ crime [ _STR_ ] == _STR_ ]   $ male . head ( 3 )
df . loc [ _STR_ : _STR_ , [ _STR_ , _STR_ ] ] # Selecting by label
price2017 = price2017 . rename ( columns = { _STR_ : _STR_ } )
import numpy as np   $ import pandas as pd   $ import matplotlib . pyplot as plt   $ % matplotlib inline   $
a2 . head ( )
df . drop ( [ _STR_ , _STR_ ] , axis = 1 , inplace = True )
train_data [ _STR_ ] = train_data [ _STR_ ] . apply ( get_integer2 )   $ test_data [ _STR_ ] = test_data [ _STR_ ] . apply ( get_integer2 )   $ del train_data [ _STR_ ]   $ del test_data [ _STR_ ]
autos [ _STR_ ] . value_counts ( normalize = True )
squares = pd . Series ( [ 1 , 4 , 9 , 16 , 25 ] ,   $ index = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] )   $ squares
pd . merge ( left = city_loc , right = city_pop , on = _STR_ , how = _STR_ )
us . loc [ us [ _STR_ ] . str . len ( ) == 2 , _STR_ ] = us . loc [ us [ _STR_ ] . str . len ( ) == 2 , _STR_ ]   $ us . loc [ us [ _STR_ ] . str . len ( ) == 2 , _STR_ ] = _STR_   $ us [ _STR_ ] . value_counts ( dropna = False )
guinea_data1 = guinea_data [ guinea_data . Description . isin ( [ _STR_ ,   $ _STR_ ] ) ]
venues . to_csv ( _STR_ , header = False , index = False )
Lab7_RevenueEPS0 . head ( )
train [ _STR_ ] = ( train . day >= 5 ) . astype ( int )   $ train . groupby ( _STR_ ) . popular . mean ( )
GBR . fit ( X_train , Y_train )
params = { _STR_ : [ 8 , 8 ] , _STR_ : _STR_ , _STR_ : 12.0 , _STR_ : 2 }   $ plot_decomposition ( therapist_duration , params = params , freq = 31 , title = _STR_ )
data . text . str . contains ( _STR_ ) . resample ( _STR_ ) . sum ( ) . plot ( )
BroncosBillsPct . merge ( BroncosBillsTweets , how = _STR_ , on = _STR_ )
import json   $ pos = json . loads ( r . content )   $ pos
len ( package . resources )
df_clean = pd . merge ( left = df_enhanced , right = df_json , left_on = _STR_ , right_on = _STR_ , how = _STR_ )   $ df_clean = df_clean . drop ( [ _STR_ , _STR_ ] , axis = 1 )   $ df_master = pd . merge ( left = df_clean , right = df_breed , left_on = _STR_ , right_on = _STR_ , how = _STR_ )
itemTable . iloc [ 0 : 30 ]
users . dtypes
session . query ( Measurements . station , func . count ( Measurements . date ) )   \   $ . group_by ( Measurements . station ) . order_by ( func . count ( Measurements . date ) . desc ( ) ) . all ( )
image_clean = image_clean . drop_duplicates ( subset = [ _STR_ ] , keep = _STR_ )
pred_probas_rfc_under = gs_rfc_under . predict_proba ( X_test )
import unicodedata   $ def clean_unicode ( unicode_str ) : #Credit to Yingling $     return unicodedata.normalize("NFKD", unicode_str) $ indeed['summary_clean'] = indeed['summary'].apply(lambda x: x.replace('\n',' ')) $ indeed['summary_clean'] = indeed['summary_clean'].apply(clean_unicode) $
df_coor . head ( 20 )
s1 = pd . Series ( [ 7.3 , - 2.5 , 3.4 , 1.5 ] , index = [ _STR_ , _STR_ , _STR_ , _STR_ ] )
df [ df [ _STR_ ] == _STR_ ] . query ( _STR_ ) . index . values   $ df2 = df2 . drop ( df [ df [ _STR_ ] == _STR_ ] . query ( _STR_ ) . index . values )
df2 [ _STR_ ] = 1   $ df2 [ [ _STR_ , _STR_ ] ] = pd . get_dummies ( df2 [ _STR_ ] )   $ df2 = df2 . drop ( _STR_ , axis = 1 )   $ df2 . head ( )
my_df . dropna ( inplace = True )   $ my_df . reset_index ( drop = True , inplace = True )   $ my_df . info ( )
Barcelona = api . search ( geocode = _STR_ , count = 300 )   $ data3 = pd . DataFrame ( data = [ tweet . text for tweet in Barcelona ] , columns = [ _STR_ ] )
X_eval . shape
news_period_df . loc [ 3 , _STR_ ]
data = { _STR_ : pd . Series ( data = [ 2 , 300.000 ] ) ,   $ _STR_ : pd . Series ( data = [ 6 , 2 ] ) }   $ job_requirements = pd . DataFrame ( data )   $ job_requirements
evaluator = MulticlassClassificationEvaluator (   $ labelCol = _STR_ , predictionCol = _STR_ , metricName = _STR_ )   $ recall = evaluator . evaluate ( predictions )   $ print ( _STR_ % ( recall ) )
def remove_no_reaction ( df ) :   $ tmp = df [ ( df [ _STR_ ] . notnull ( ) ) ]   $ return tmp [ tmp [ _STR_ ] >= 200 ]   $ dataset_test = remove_no_reaction ( dataset_test )   $ dataset_test . loc [ : , _STR_ ] = dataset_test . loc [ : , _STR_ ] . astype ( int )
null_name = raw_data [ ( raw_data [ _STR_ ] . isna ( ) ) ]   $ null_name . T
n_old = ( df2 [ _STR_ ] == _STR_ ) . sum ( )   $ n_old
df [ df [ _STR_ ] . isin ( [ _STR_ , _STR_ ] ) ] . count ( )
file_dir = os . path . dirname ( os . path . abspath ( _STR_ ) )   $ parent_dir = os . path . dirname ( file_dir )   $ newPath = os . path . join ( parent_dir , _STR_ )   $ dfleavetimes = pd . read_csv ( newPath , delimiter = _STR_ , index_col = False )
for row in selfharmm_topic_names_df . iloc [ 2 ] :   $ print ( row )
df . iloc [ 0 ] [ _STR_ ]
df2_new = countries_df . join ( df2 . set_index ( _STR_ ) , how = _STR_ )
properati [ properati [ _STR_ ] == _STR_ ] [ _STR_ ] . value_counts ( dropna = False )
s = type . __new__ ( type , _STR_ , ( ) , { _STR_ : 1 } )
from sklearn . model_selection import cross_val_score   $ accuracies = cross_val_score ( LogisticRegression ( ) , X , y , cv = 10 )   $ print accuracies . mean ( )   $ print 1 - y . mean ( )
for i in station_distance [ _STR_ ] :   $ start_lat . append ( i )   $ for i in station_distance [ _STR_ ] :   $ start_lon . append ( i )
test = weekly [ : 7 ] . sort_values ( by = [ _STR_ ] )   $ plt . figure ( figsize = ( 10 , 3 ) )   $ plt . plot ( test [ _STR_ ] , test [ _STR_ ] )
df . columns
B = pd . DataFrame ( rng . randint ( 0 , 10 , ( 3 , 3 ) ) , columns = list ( _STR_ ) )   $ B
lReligion = list ( db . osm . find ( { _STR_ : { _STR_ : 1 } } ) )   $ print _STR_ , len ( lReligion )   $ lReligion [ : 5 ]
approved [ _STR_ ] = True   $ approved . head ( )
df1 = pd . DataFrame ( )   $ df1 [ _STR_ ] = np . random . randint ( low = 20000 , high = 30000 , size = 100 )   $ df1 [ _STR_ ] = np . random . randint ( low = 20000 , high = 40000 , size = 100 )   $ df1 . index = pd . date_range ( _STR_ , periods = 100 , freq = _STR_ )
lsa_tfidf_topic6_sample_precision = mf . get_precision_score ( lsa_tfidf_topic6_sample , lsa_tfidf_topic6_sample_fp_list )   $ lsa_tfidf_topic6_sample_precision
ign = ign . rename ( columns = { _STR_ : _STR_ , _STR_ : _STR_ } )   $
df2 [ _STR_ ] = 1   $ df2 [ _STR_ ] = pd . get_dummies ( df [ _STR_ ] ) [ _STR_ ]   $ df2 . head ( )
s = pd . Series ( [ 1 , 2 , 3 ] )   $ s . loc [ _STR_ ] = _STR_   $ s
new_page = np . random . binomial ( 1 , p , df2 . query ( _STR_ ) . shape [ 0 ] )
df_all . xs ( 299 , level = _STR_ )
from IPython . core . display import HTML   $ def css_styling ( ) :   $ styles = open ( _STR_ , _STR_ ) . read ( )   $ return HTML ( styles )   $ css_styling ( )
df_loan2 [ df_loan2 . fk_loan == 36 ]
df2 = df2 . drop_duplicates ( subset = [ _STR_ ] , keep = _STR_ )   $ df2 . shape
tmi = indices ( tmaggr , _STR_ , _STR_ )   $ tmi . head ( )
df_select_cats = df_select . copy ( )   $ df_select_cats = df_select_cats . groupby ( [ _STR_ ] , as_index = False ) . mean ( )   $ df_select_cats
from sklearn . model_selection import train_test_split   $ X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.1 ) # 30% for testing, 70% for training $ X_train.sample(5)
pd . value_counts ( ac [ _STR_ ] . values , sort = True , ascending = False )
p_diffs = np . array ( p_diffs )
df_new = df_new . join ( pd . get_dummies ( df_new [ _STR_ ] ) )
props . prop_name
SVM_yhat = SVM . predict ( test_X )   $ print ( _STR_ % jaccard_similarity_score ( test_y , SVM_yhat ) )   $ print ( _STR_ % f1_score ( test_y , SVM_yhat , average = _STR_ ) )
poverty_2011_2015 . columns . name = None
kick_projects = df_kick [ ( df_kick [ _STR_ ] == _STR_ ) | ( df_kick [ _STR_ ] == _STR_ ) ]   $ kick_projects [ _STR_ ] = ( kick_projects [ _STR_ ] == _STR_ ) . astype ( int )
plt . hist ( traindata . rating )   $ plt . title ( _STR_ ) ;
B2 = B . get_step_object ( _STR_ )   $ B2 . load_indicator_settings_filters ( )
print ( _STR_ )   $ df = df . withColumn ( _STR_ , df [ _STR_ ] . cast ( _STR_ ) ) # elasticsearch needs datetimes in a string type $ es.saveToEs(df,index=es_dataindex,doctype=es_datatype)
df_image_clean [ _STR_ ] = df_image_clean [ _STR_ ] . astype ( _STR_ )
lines_to_read = 100   $ with open ( dumpfile ) as myfile :   $ firstlines = myfile . readlines ( ) [ 0 : lines_to_read ] #put here the interval you want $     for x in firstlines: $         print(x.strip())
colors = ( _STR_ , _STR_ , _STR_ , _STR_ , _STR_ )   $ lgd = zip ( tweet_df [ _STR_ ] . unique ( ) , colors )   $
df_parties . groupby ( df_parties . index . weekday ) [ _STR_ ] . count ( ) . plot ( kind = _STR_ )   $
pivoted . T . shape   $
df_license_appl . head ( 2 )
log_mod = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ ] ] )   $ results = log_mod . fit ( )   $ results . summary ( )
trn_lm = np . array ( [ [ stoi [ o ] for o in p ] for p in tok_trn ] )   $ val_lm = np . array ( [ [ stoi [ o ] for o in p ] for p in tok_val ] )
train , validate , test = np . split ( cnn_fox_data . sample ( frac = 1 ) , [ int ( .6 * len ( cnn_fox_data ) ) , int ( .8 * len ( cnn_fox_data ) ) ] )
from sklearn . decomposition import LatentDirichletAllocation   $ lda = LatentDirichletAllocation ( n_topics = 10 ,   $ random_state = 123 ,   $ learning_method = _STR_ )   $ X_topics = lda . fit_transform ( X )
geocoded_df [ list ( filter ( lambda x : x . endswith ( _STR_ ) , geocoded_df . columns ) ) ] =   \   $ geocoded_df [ list ( filter ( lambda x : x . endswith ( _STR_ ) , geocoded_df . columns ) ) ] . apply ( pd . to_datetime )
tallies_file = openmc . Tallies ( )
weather_mean . iloc [ 5 : 15 : 3 , : ]
archive_copy . head ( )
nba_df . loc [ ( nba_df [ _STR_ ] == 2015 ) & ( nba_df [ _STR_ ] == _STR_ ) & ( nba_df [ _STR_ ] == _STR_ ) & ( nba_df [ _STR_ ] == 82 ) , _STR_ ] = avg_att_2015_BOS   $ nba_df . loc [ ( nba_df [ _STR_ ] == 2015 ) & ( nba_df [ _STR_ ] == _STR_ ) & ( nba_df [ _STR_ ] == _STR_ ) & ( nba_df [ _STR_ ] == 82 ) , _STR_ ] = avg_att_2015_MIL
df [ _STR_ ] = pd . to_datetime ( df [ _STR_ ] )   $ df . head ( )
len ( df_events ) , len ( df_events . group_id . unique ( ) )
lm = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ ] ] )   $ result = lm . fit ( )   $ result . summary ( )
pd . concat ( [ pd . DataFrame ( y_pred_rsskb , columns = [ _STR_ ] , index = y_test . index ) , y_test ] , axis = 1 ) # Test predictions
df = pd . read_csv ( _STR_ +   $ _STR_ +   $ _STR_ +   $ _STR_ )   $ df [ : 5 ]
days = [ start_date ]   $ while start_date != end_date :   $ start_date += dt . timedelta ( days = 1 )   $ days . append ( start_date )
got_data . sort_values ( _STR_ , ascending = False , inplace = True )
rchillipivot = pd . pivot_table ( files8 , index = _STR_ ,   $ aggfunc = { _STR_ : _STR_ , _STR_ : _STR_ } )   $ rchilli = pd . DataFrame ( rchillipivot . to_records ( ) )   $ rchilli = rchilli . rename ( columns = { _STR_ : _STR_ , _STR_ : _STR_ , _STR_ : _STR_ } )     $ rchilli . head ( )
print ( stock_data . shape )   $ print ( _STR_ , stock_data . shape [ 0 ] )   $ print ( _STR_ , stock_data . shape [ 1 ] )   $
converted = df [ _STR_ ] . value_counts ( ) [ 1 ]   $ total = df [ _STR_ ] . count ( )   $ total_converted = ( converted / total ) * 100   $ print ( _STR_   $ _STR_ . format ( converted , total_converted ) )
df . sample ( 5 )
dft = df2 . T   $ dft
display_all ( train_data . head ( 5 ) . transpose ( ) )
df_test . info ( )
mismatch1 = ( ab_df [ _STR_ ] == _STR_ ) & ( ab_df [ _STR_ ] == _STR_ )   $ mismatch2 = ( ab_df [ _STR_ ] == _STR_ ) & ( ab_df [ _STR_ ] == _STR_ )   $ print ( ab_df [ mismatch1 ] . shape [ 0 ] + ab_df [ mismatch2 ] . shape [ 0 ] )
loan_fundings [ ( loan_fundings . fk_loan == 36 ) ] . T . to_clipboard ( )
df_melt = pd . melt ( frame = df , id_vars = _STR_ ,   $ value_vars = [ _STR_ , _STR_ ] ,   $ var_name = _STR_ ,   $ value_name = _STR_ )   $ df_melt
grid_lat = np . arange ( np . min ( lat_us ) , np . max ( lat_us ) , 1 )   $ grid_lon = np . arange ( np . min ( lon_us ) , np . max ( lon_us ) , 1 )   $ glons , glats = np . meshgrid ( grid_lon , grid_lat )
print ( df2 . shape )   $ print ( countries . shape )
with tf . name_scope ( _STR_ ) :   $ correct = tf . nn . in_top_k ( logits , y , 1 )   $ accuracy = tf . reduce_mean ( tf . cast ( correct , tf . float32 ) )   $ accuracy_summary = tf . summary . scalar ( _STR_ , accuracy )
df . info ( )
import requests   $ import json   $ r = requests . get ( _STR_ )   $ res = r . json ( )   $ print ( json . dumps ( res , indent = 4 ) )
user_summary_df [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] . describe ( )
df_merge = df_clean . merge ( df_twitter_clean , how = _STR_ , left_on = _STR_ , right_on = _STR_ )   $ df_merge . drop ( _STR_ , axis = 1 , inplace = True )
submission_full [ [ _STR_ ] ] . max ( )
np . sum ( van_true )
archive_df_clean [ _STR_ ] = archive_df_clean . rating_numerator . astype ( float )
not_in_df = df [ ( df [ _STR_ ] == _STR_ ) & ( df [ _STR_ ] == _STR_ ) ]   $ df_not_t_n = df [ ( df [ _STR_ ] == _STR_ ) & ( df [ _STR_ ] == _STR_ ) ]   $ mismatch = len ( not_in_df ) + len ( df_not_t_n )   $ mismatch_df = pd . concat ( [ not_in_df , df_not_t_n ] )   $ mismatch
y = df . values   $ y . size
df2 . loc [ df2 [ _STR_ ] == 773192 , ]
def get_list_tot_vidviews ( the_posts ) :   $ list_tot_vidviews = [ ]   $ for i in list_Media_ID :   $ list_tot_vidviews . append ( the_posts [ i ] [ _STR_ ] [ - 1 ] [ _STR_ ] )   $ return list_tot_vidviews
ux . html_utils . get_webpage_description ( urls [ 0 ] )
result [ _STR_ ] . describe ( )
user = api . get_user ( ids [ 0 ] )
new_cases . columns = [ _STR_ , _STR_ , _STR_ ]
coins_top10today = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ initialdate_per_coin . T [ coins_top10today ]
df_genre = df [ _STR_ ] . to_frame ( )   $ print ( df_genre )
gearbox_list = list ( set ( train_data . gearbox ) )
by_area [ _STR_ ] . hist ( bins = 20 , histtype = _STR_ , stacked = True , linewidth = 3 )   $ plt . legend ( [ _STR_ , _STR_ , _STR_ ] ) ;
df [ _STR_ ] = df [ _STR_ ] . apply ( lambda x : 2 * x )
brands = autos [ _STR_ ] . value_counts ( ) [ autos [ _STR_ ] . value_counts ( ) > 1000 ] . index
df . describe ( )
dfb_train = train . date_first_booking   $ dfb_test = test . date_first_booking
print ( _STR_ )   $ print ( _STR_ )   $ start = input ( )   $ print ( _STR_ )   $ end = input ( )
cfd_nums = [ vs for word in wsj   $ for vs in re . findall ( _STR_ , word . lower ( ) ) ]   $ cfd = nltk . ConditionalFreqDist ( cfd_nums )   $ df_cfd = pd . DataFrame ( cfd ) . fillna ( value = 0 ) . astype ( dtype = int )   $ df_cfd
data2 . info ( )
y4 , X4 = patsy . dmatrices ( _STR_ , data = df , return_type = _STR_ )   $ model = sm . OLS ( y4 , X4 , missing = _STR_ )   $ fit4 = model . fit ( )   $ fit4 . summary ( )
new_albums . head ( )
while len ( new_tweets ) > 0 :   $ new_tweets = api . user_timeline ( screen_name = _STR_ , count = 200 , max_id = oldest )   $ alltweets . extend ( new_tweets )   $ oldest = alltweets [ - 1 ] . id - 1
df2 [ df2 . duplicated ( subset = _STR_ ) ]
faux = clean_rates . text . str . contains ( _STR_ )   $ clean_rates [ faux ]
b_cal_q1 . columns
np . save ( LM_PATH / _STR_ / _STR_ , tok_trn )   $ np . save ( LM_PATH / _STR_ / _STR_ , tok_val )
from sklearn . metrics import roc_auc_score   $ print classification_report ( ada_pred , y_test )   $ print _STR_ , roc_auc_score ( ada_pred , y_test )
titanic . shape
gbctest = pd . DataFrame ( labeled_features . loc [ labeled_features [ _STR_ ] >= pd . to_datetime ( _STR_ ) ] )   $ gbctest [ _STR_ ] = model . predict ( test_x )
df = pd . DataFrame ( emails )   $ df
train . head ( )
reddit_comments_data . groupby ( _STR_ ) . count ( ) . orderBy ( _STR_ , ascending = False ) . show ( 100 , truncate = False )
h1 = re . findall ( _STR_ , html )   $ print ( h1 )   $ h1 = re . findall ( _STR_ , html )   $ print ( h1 )
df [ 0 : 17 ] = np . nan   $ df . iloc [ 40 : 90 ] = np . nan   $ df . iloc [ 40 : 90 ] . head ( )
cp311 [ _STR_ ] = cp311 . created_date . dt . to_period ( _STR_ )   $ cp311 . set_index ( _STR_ , inplace = True )   $ cp311 . head ( 2 )
df_precipitation . head ( )
print mike . ra [ 0 ]   $ a = Angle ( mike . ra [ 0 ] , u . hr ) . to_string ( unit = u . degree , precision = 5 , decimal = True )   $ print float ( a )   $ b = Angle ( a , u . deg ) . to_string ( unit = u . hr , sep = _STR_ , precision = 1 , pad = True )   $ print b
facts_url = _STR_
df . to_csv ( save_dir )
converted_controlusers2 = float ( df2 . query ( _STR_ ) [ _STR_ ] . nunique ( ) )   $ treat_users2 = float ( df2 . query ( _STR_ ) [ _STR_ ] . nunique ( ) )   $ tp2 = converted_controlusers2 / treat_users2   $ print ( _STR_ . format ( tp2 ) )
page_interaction_log = sm . Logit ( df4 [ _STR_ ] , df4 [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ page_interaction_result = page_interaction_log . fit ( )
trump [ _STR_ ] = trump [ _STR_ ] . str . replace ( _STR_ , _STR_ ) . str . replace ( _STR_ , _STR_ )   $ trump [ _STR_ ] . value_counts ( ) . plot ( kind = _STR_ )   $ plt . ylabel ( _STR_ ) ;
tipsDF = tipsDF . drop ( tipsDF . columns [ 0 ] , axis = 1 )
display ( heading ( _STR_ ) ,   $ set ( sheets_with_bad_column_names . keys ( ) ) )
print ( X_train . shape , y_train . shape , X_val . shape , y_val . shape )
df_archive . sample ( 10 )
import statsmodels . api as sm   $ logit = sm . Logit ( df2 [ _STR_ ] , df2 [ [ _STR_ , _STR_ ] ] )   $ results = logit . fit ( )
pd . crosstab ( train . TYPE_BI , train . TYPE_UT )
print df . shape [ 0 ] + noloc_df . shape [ 0 ]
old_page_converted = np . random . choice ( [ 0 , 1 ] , size = n_old , p = [ 1 - p_old , p_old ] )   $ len ( old_page_converted )
traintrain = train . loc [ : 11815 * 188 - 1 , : ]   $ trainvalidation = train . loc [ 11815 * 196 : , : ]
temps_df . Difference [ 1 : 4 ]
var_df . resample ( _STR_ ) . sum ( )
notus [ _STR_ ] = notus [ _STR_ ] . str . replace ( _STR_ , _STR_ )   $ notus [ _STR_ ] = notus [ _STR_ ] . str . replace ( _STR_ , _STR_ )
potholes = df [ df [ _STR_ ] == _STR_ ]   $ potholes . groupby ( potholes . index . hour ) [ _STR_ ] . count ( ) . plot ( y = _STR_ )
list_of_genre_1990s = list ( )   $ respond = requests . get ( _STR_ )   $ soup = BeautifulSoup ( respond . text )   $ l = soup . find_all ( _STR_ )   $ list_of_genre_1990s . append ( l [ 3 ] . text . split ( _STR_ ) )
df . shape
pd . DataFrame ( np . random . rand ( 3 , 2 ) ,   $ columns = [ _STR_ , _STR_ ] ,   $ index = [ _STR_ , _STR_ , _STR_ ] )
df2 [ ( ( df2 [ _STR_ ] == _STR_ ) == ( df2 [ _STR_ ] == _STR_ ) ) == False ] . shape [ 0 ]
end_time = dt . datetime . now ( )   $ ( end_time - start_time ) . total_seconds ( )
pd . DataFrame ( lostintranslation . credits ( ) [ _STR_ ] ) [ [ _STR_ , _STR_ , _STR_ ] ] . head ( )
to_be_predicted_Day2 = 14.78510842   $ predicted_new = ridge . predict ( to_be_predicted_Day2 )   $ predicted_new
list ( df_users_first_transaction . dropna ( thresh = int ( df_users_first_transaction . shape [ 0 ] * .9 ) , axis = 1 ) . columns )
data [ _STR_ ] = data [ _STR_ ] . astype ( _STR_ )
df = json_normalize ( j [ _STR_ ] , _STR_ )   $ df . columns = col_names   $ df . head ( )
model_info = kipoi_veff . ModelInfoExtractor ( model , Dataloader )   $ vcf_to_region = kipoi_veff . SnvCenteredRg ( model_info )
todaysFollowers . head ( )
dset = xr . open_dataset ( os . path . join ( opath , filename ) )
to_be_predicted_Day4 = 34.13664722   $ predicted_new = ridge . predict ( to_be_predicted_Day4 )   $ predicted_new
top_songs [ _STR_ ] . isnull ( ) . sum ( )
gen = dta [ dta . b == 1 ]
feature_col = ibm_hr_final . columns   $ feature_col
contractor [ contractor [ _STR_ ] . isin ( [ 139 , 140 , 228 , 236 , 238 ] ) ]
noNulls . orderBy ( sort_a_asc ) . show ( 5 )
df = pd . read_csv ( _STR_ , usecols = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] )   $ df . head ( )
import folium   $ map_osm = folium . Map ( location = [ 1.3521 , 103.8198 ] , zoom_start = 12 , tiles = _STR_ )   $ def adding_to_map ( lbs ) :   $ return folium . CircleMarker ( location = [ lbs [ 5 ] , lbs [ 6 ] ] , radius = 12 , color = _STR_ , fill_color = _STR_ ) . add_to ( map_osm )
doc_duration . head ( )
df2 . iloc [ 4 , 3 ] = np . nan
path = _STR_   $ df = pd . read_csv ( path , sep = _STR_ , na_values = [ _STR_ ] )   $ df . head ( 5 )
% % timeit   $ scipy . optimize . fsolve ( globals ( ) [ function_name ] , 2 )
events_enriched_df = pd . merge ( events_df [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ]   $ , groups_topics_unique_df [ [ _STR_ , _STR_ , _STR_ ] ]   $ , on = _STR_ , how = _STR_ )
reddit . head ( 2 )
df2 [ _STR_ ] . value_counts ( ) . plot ( kind = _STR_ )   $ print ( df2 [ _STR_ ] . value_counts ( ) )
ad_source . to_csv ( _STR_ )
precip = session . query ( Precip . date , Precip . prcp , Precip . station ) . filter ( Precip . date >= date_ly ) #.filter(Precip.station == 'USC00511918')
pd . options . display . max_rows = 100   $ pd . options . display . max_colwidth = 300   $ all_tweets . iloc [ : 30 , 7 : 8 ] . style . set_properties ( ** { _STR_ : _STR_ } )
news_title = soup . find ( _STR_ , _STR_ , _STR_ ) . text   $ news_p = soup . find ( _STR_ , _STR_ ) . text   $ news_title   $ news_p
most_freq = data [ ( data [ _STR_ ] == 103.93700000000001 ) & ( data [ _STR_ ] == 1.34721 ) ]
browser . quit ( )
reduced_trips_data = trips_data . loc [ abs ( trips_data . duration - trips_data . duration . mean ( ) ) <= ( 3 * trips_data . duration . std ( ) ) ]   $
X_train [ _STR_ ] . unique ( ) . shape
tt_final [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] = tt_final [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] . astype ( int )   $ tt_final . info ( )
df2_new . head ( )
df_raw = pd . read_csv ( _STR_ , encoding = _STR_ )   $ df = df_raw . copy ( ) #just to keep raw data in memory to later uses $ print 'list of columns available in our dataset: ' $ df.columns
actual_diff = df2 [ df2 . group == _STR_ ] [ _STR_ ] . mean ( ) - df2 [ df2 . group == _STR_ ] [ _STR_ ] . mean ( )   $ actual_diff
df2 [ df2 . landing_page == _STR_ ] . shape [ 0 ] / df2 . shape [ 0 ]
shortcodes = [ ]   $ for i in range ( 35 ) :   $ shortcodes . append ( df [ 0 : 40 ] [ _STR_ ] [ i ] [ - 11 : ] )   $ shortcodes [ 0 ]   $ len ( shortcodes )
tlen . plot ( figsize = ( 16 , 4 ) , color = _STR_ ) ;
tobs_start_date = dt . datetime . strptime ( latest_date , _STR_ ) - dt . timedelta ( days = 365 )   $ tobs_start_date = tobs_start_date . strftime ( _STR_ )
plt . plot ( times )
df . columns
df1 . unstack ( )
with open ( _STR_ , _STR_ ) as picklefile :   $ pickle . dump ( X_train , picklefile )   $
from sklearn . model_selection import KFold   $ cv = KFold ( n_splits = 200 , random_state = None , shuffle = True )   $ estimator = Ridge ( alpha = 20000 )   $ plot_learning_curve ( estimator , _STR_ , X_std , y , cv = cv , train_sizes = np . linspace ( 0.2 , 1.0 , 10 ) )
df_cod2 = df_cod . copy ( )   $ df_cod2 = df_cod2 . dropna ( )
Pold = df2 . converted . mean ( )   $ Pold
autos = autos [ autos [ _STR_ ] . between ( 1 , 350000 ) ]   $ autos [ _STR_ ] . describe ( )
universe = [ _STR_ , _STR_ , _STR_ ]   $ price_data = yahoo_finance . download_quotes ( universe )
k = pd . read_sql_query ( QUERY , conn )   $ k
from scipy . stats import norm   $ norm . ppf ( 1 - ( 0.05 ) )
from crowdtruth . configuration import DefaultConfig
bag = vect . fit_transform ( df [ _STR_ ] ) . toarray ( )   $ bag
df . query ( _STR_ ) . shape
trunc_df . iloc [ 94 ]
scaler = MinMaxScaler ( )   $ data [ intFeatures ] = scaler . fit_transform ( data [ intFeatures ] )
df_lm . keys ( )
os . chdir ( _STR_ )
( - 1 * close_month ) . loc [ month ] . nlargest ( 2 )
today = _STR_   $ past = pd . to_datetime ( today ) - pd . DateOffset ( years = 18 )   $ print ( past )   $ birth_dates [ _STR_ ] . loc [ birth_dates [ _STR_ ] <= past ] . head ( )
pd . read_csv ( _STR_ , parse_dates = [ 0 ] ) . head ( )
len ( df2 . loc [ ( df2 [ _STR_ ] == _STR_ ) & ( df2 [ _STR_ ] == 1 ) ] [ _STR_ ] ) / len ( df2 . loc [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] )
df1 = tier1_df . reset_index ( )   $ df1 = df1 . rename ( columns = { _STR_ : _STR_ , _STR_ : _STR_ } )
party_type_crosstab . plot . bar ( x = _STR_ , y = _STR_ , legend = False )
results_simpleResistance , out_file1 = S . execute ( run_suffix = _STR_ , run_option = _STR_ )
zipincome . ZIPCODE [ : 10 ]
import pymms   $ mysdc = pymms . MrMMS_SDC_API ( start_date = _STR_ , end_date = _STR_ ,   $ data_root = _STR_ )
nums . reverse ( ) # reverses the item order $ nums
columns = inspector . get_columns ( _STR_ )   $ for c in columns :   $ print ( c [ _STR_ ] , c [ _STR_ ] )
pd . pivot_table ( more_grades , index = _STR_ )
np . concatenate ( [ np . random . random ( 5 ) , np . random . random ( 5 ) ] )
opt_fn = partial ( optim . Adam , betas = ( 0.7 , 0.99 ) )
countries = pd . read_csv ( _STR_ )
qualification . qual_conversion . value_counts ( )
regr = LinearRegression ( )   $ regr . fit ( X , y )   $ fl = [ _STR_ . format ( x ) for x in list ( regr . coef_ ) ]   $ for feature , coef in zip ( X . columns , fl ) :   $ print ( feature , coef )
the_drg_number = 66   $ idx = df_providers [ ( df_providers [ _STR_ ] == 2011 ) &   \   $ ( df_providers [ _STR_ ] == the_drg_number ) ] . index . tolist ( )   $ print ( _STR_ , len ( idx ) , _STR_ , the_drg_number )   $ print ( _STR_ , np . max ( df_providers . loc [ idx , _STR_ ] ) )
df . groupby ( _STR_ ) [ _STR_ ] . value_counts ( ) . groupby ( level = 0 , group_keys = False ) . nlargest ( 5 )
free_data [ free_data . educ >= 5 ] . groupby ( _STR_ ) . describe ( ) . stack ( 1 )
df_joy . describe ( )
mediaMask = results [ _STR_ ] == _STR_   $ hydroMask = ~ results [ _STR_ ] . isin ( ( _STR_ , _STR_ , _STR_ , _STR_ ) )   $ charMask = results [ _STR_ ] == _STR_   $ sampFracMask = results [ _STR_ ] == _STR_
freq_df = pd . DataFrame ( freq , columns = [ _STR_ , _STR_ ] )   $ freq_df . set_index ( _STR_ , inplace = True , )   $ freq_df . head ( )
airline_tw_collec = mongo . mongoDB_read_collection ( mongoDBname , collec_name )
fin_coins_r . isnull ( ) . sum ( )
predictions = loaded_text_classifier . predict ( df_test )   $ loaded_evaluator = loaded_text_classifier . evaluate ( predictions )   $ loaded_evaluator . get_metrics ( _STR_ )
df_new [ _STR_ ] , df_new [ _STR_ ] = df_new [ _STR_ ] * df_new [ _STR_ ] , df_new [ _STR_ ] * df_new [ _STR_ ]   $ df_new . head ( )
autos = pandas . read_csv ( _STR_ , encoding = _STR_ )
pattern = re . compile ( _STR_ )
spacy_nlp = spacy . load ( _STR_ )
df3 = pd . merge ( df2_new , country_df , on = [ _STR_ ] )   $ df3 . head ( )
df . groupby ( _STR_ ) [ _STR_ ] . nunique ( ) . hist ( )
df_input_clean . fillna ( - 99999 , subset = [ _STR_ ] ) . filter ( _STR_ ) . count ( )
street_freq = train . groupby ( _STR_ ) [ _STR_ ] . count ( )   $ train = train . join ( street_freq , on = _STR_ , rsuffix = _STR_ )
orig_ct = len ( dfd )   $ dfd = dfd . query ( _STR_ )   $ print ( len ( dfd ) - orig_ct , _STR_ )
s = pd . Series ( )   $ s
df3 = pd . read_csv ( _STR_ )
gpCreditCard . Passenger_count . describe ( )
a = [ ]   $ a . append ( 1 )   $ a . append ( 2 )   $ a
hour_of_day15 . to_excel ( writer , index = True , sheet_name = _STR_ )
rules . sort_values ( [ _STR_ ] , ascending = False )
re . findall ( _STR_ , slices [ 4 ] )
conversion_prob = df2 [ _STR_ ] . mean ( )   $ conversion_prob
male_journalists_retweet_summary_df [ [ _STR_ ] ] . describe ( )
df [ _STR_ ] = km . labels_
knn_10 . fit ( X_train , y_train )
pd . merge ( df1 , df3 , how = _STR_ )
df_ca [ df_ca [ _STR_ ] == 1 ] [ _STR_ ] . mean ( )
regressor = sm . Logit ( df3 [ _STR_ ] , df3 [ [ _STR_ , _STR_ ] ] )   $ result = regressor . fit ( )
top_20_breeds = tweet_archive_master [ _STR_ ] . value_counts ( ) . index [ : 20 ]
df . drop ( _STR_ , axis = 1 , inplace = True )
ab_new = countries_df . set_index ( _STR_ ) . join ( ab_file2 . set_index ( _STR_ ) , how = _STR_ )   $ ab_new . head ( )
list ( Users_first_tran . dropna ( thresh = int ( Users_first_tran . shape [ 0 ] * .9 ) , axis = 1 ) . columns )
testing . replace ( _STR_ , _STR_ )
t_cont_prob = df2 [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] . mean ( ) * 100   $ output = round ( t_cont_prob , 2 )   $ print ( _STR_ . format ( output ) )
df_twitter_copy . loc [ 707 ] . text
nodes . info ( )
g = sns . catplot ( x = _STR_ , col = _STR_ ,   $ data = data , kind = _STR_ ,   $ height = 5 , aspect = .9 ) ;
import statsmodels . api as sm   $ logit = sm . Logit ( df [ _STR_ ] , df [ [ _STR_ , _STR_ ] ] )
Temperature_year = session . query ( Measurements . date , Measurements . tobs )   \   $ . filter ( Measurements . date >= _STR_ ) . filter ( Measurements . date <= _STR_ )   \   $ . filter ( Measurements . station == _STR_ ) . all ( )   $ Temperature_year
readMe = open ( _STR_ , _STR_ ) . read ( )   $ readMeList = open ( _STR_ , _STR_ ) . readlines ( )   $ print ( _STR_ , readMe , _STR_ )   $ print ( _STR_ , readMeList )
props . prop_name . value_counts ( )
df_joined = df2 . join ( df_countries . set_index ( _STR_ ) , on = _STR_ ) # joining the 2 data frames on the 'user_id' column $ df_joined.head()
totals = df . groupby ( [ _STR_ , _STR_ ] ) . count ( ) [ _STR_ ] . unstack ( ) . reset_index ( )
X_test . shape
Probas2 = pd . DataFrame ( estimator . predict_proba ( X2 ) , columns = [ _STR_ , _STR_ ] )   $ joined2 = pd . merge ( tweets2 , Probas2 , left_index = True , right_index = True )
print ( a . find ( _STR_ ) )   $ print ( a . rfind ( _STR_ ) )   $ print ( a . find ( _STR_ ) )   $ print ( a . find ( _STR_ ) )
print ( mostRecentTweets . shape )   $ print ( tweetsOverall . shape )   $ print ( tidy . shape )   $ tidy . to_csv ( _STR_ , index = False )
df_goog . head ( )
import statsmodels . api as sm   $ convert_old = df2 . query ( _STR_ ) . shape [ 0 ]   $ convert_new = df2 . query ( _STR_ ) . shape [ 0 ]   $ n_old = df2 . query ( _STR_ ) . shape [ 0 ]   $ n_new = df2 . query ( _STR_ ) . shape [ 0 ]
pd . set_option ( _STR_ , False ) # render Series and DataFrame as text, not HTML $ pd.set_option( 'display.max_column', 10)    # number of columns $ pd.set_option( 'display.max_rows', 10)     # number of rows $ pd.set_option( 'display.width', 80)        # number of characters per row
rng_pytz . tz
results [ results [ _STR_ ] == _STR_ ] [ _STR_ ] . value_counts ( )
model_sm . params . sort_values ( ascending = False )
X_train_all = pd . concat ( [ X_train_df , X_train . drop ( _STR_ , axis = 1 ) ] , axis = 1 )   $ X_test_all = pd . concat ( [ X_test_df , X_test . drop ( _STR_ , axis = 1 ) ] , axis = 1 )
mentions_df_raw = pd . DataFrame ( mentions_raw , columns = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] )   $ print ( len ( mentions_df_raw ) )
facts_metrics . id . nunique ( )
with tf . Session ( ) as sess :   $ print ( sess . run ( tf_tensor ) )   $ print ( sess . run ( tf_tensor [ 0 ] ) )   $ print ( sess . run ( tf_tensor [ 2 ] ) )
cutoff_times = generate_labels ( _STR_ , trans ,   $ label_type = _STR_ , churn_period = 30 )   $ cutoff_times [ cutoff_times [ _STR_ ] == 1 ] . head ( )
frames = [ df_prep17_ , df_prep16_ , df_prep15_ , df_prep14_ , df_prep13_ , df_prep12_ , df_prep11_ , df_prep10_ ,   $ df_prep9_ , df_prep8_ , df_prep7_ , df_prep6_ , df_prep5_ , df_prep4_ , df_prep3_ , df_prep2_ , df_prep1_ ,   $ df_prep0_ , df_prep99_ , df_prep98_ ]   $ df = pd . concat ( frames )
contribs = pd . read_csv ( _STR_ )
r . loc [ 0 , _STR_ ]
len ( set ( df . title ) )
! grep - A 20 _STR_ taxifare / trainer / model . py
new_converted_simulation = np . random . binomial ( n_new , new_page_converted . mean ( ) , 10000 ) / n_new   $ old_converted_simulation = np . random . binomial ( n_old , old_page_converted . mean ( ) , 10000 ) / n_old   $ p_diffs = new_converted_simulation - old_converted_simulation
print ( np . min ( ndvi ) , np . mean ( ndvi ) , np . max ( ndvi ) )
techmeme . news_text = techmeme . news_text + _STR_   $ techmeme = techmeme . groupby ( _STR_ ) [ _STR_ , _STR_ ] . sum ( )
table_names = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ table_list = [ pd . read_csv ( _STR_ , low_memory = False ) for fname in table_names ]   $ for table in table_list : display ( table . head ( ) )
df_main = pd . merge ( df_clean , api_clean , on = _STR_ , how = _STR_ )
h = sess . get_historical_data ( [ _STR_ , _STR_ ] , [ _STR_ , _STR_ ] )
plt . scatter ( df [ _STR_ ] , df [ _STR_ ] )   $ plt . show ( )
ratings = tweet_archive_clean . text . str . extractall ( _STR_ )   $ ratings . numerator = ratings . numerator . astype ( float )   $ ratings . denominator = ratings . denominator . astype ( float )
j = r . json ( )   $
firstround = pd . read_csv ( _STR_ )   $ firstround . head ( )
import pandas as pd   $ file_name = _STR_   $ sample_survey = pd . read_csv ( file_name . format ( 2010 ) , index_col = 0 )   $ sample_survey . head ( )
test_preds = lr . predict ( test_features )
len ( data_get . keys ( ) )
calls_temp = pd . DataFrame ( calls_df [ _STR_ ] )   $ calls_temp = pd . DataFrame ( KNN ( k = 3 ) . complete ( calls_temp ) , columns = calls_temp . columns )   $ calls_df = calls_df . drop ( [ _STR_ ] , axis = 1 )   $ calls_df [ _STR_ ] = calls_temp [ _STR_ ]   $ calls_df . head ( )
mean = ...   $ print ( _STR_ . format ( mean ) )
sh_max_df . tobs = sh_max_df . tobs . astype ( float )
print ( _STR_ )   $ print ( data [ ( data [ _STR_ ] == _STR_ ) & ( data [ _STR_ ] < 30 ) ] [ _STR_ ] . count ( ) / a )
autos = autos [ ( autos [ _STR_ ] > 0 ) & ( autos [ _STR_ ] <= 350000 ) ]   $ autos [ _STR_ ] . describe ( )
( df2 . landing_page == _STR_ ) . mean ( )
print ( _STR_ . format ( get_residual_sum_of_squares ( example_model , test_data , test_data [ _STR_ ] ) ) )
malenew = malebyphase . rename ( columns = { _STR_ : _STR_ } )   $ malenew . head ( 3 )
final_grades_clean = final_grades_clean . dropna ( axis = 1 , how = _STR_ )   $ final_grades_clean
top_brands = autos [ _STR_ ] . value_counts ( ascending = False , normalize = True ) > 0.05   $ top_brands = top_brands [ top_brands [ top_brands . index ] == True ]   $ print ( top_brands )   $ print ( top_brands . index . shape )
pd . Series ( np . random . randn ( 3 ) )
kickstarter [ kickstarter [ _STR_ ] . isnull ( ) ] [ 0 : 10 ]
data_df . desc [ 17 ]
twitter_archive [ twitter_archive [ _STR_ ] . apply ( len ) < 3 ]
df2 [ _STR_ ] = 1   $ df2 [ _STR_ ] = np . where ( df2 [ _STR_ ] == _STR_ , 1 , 0 )   $ df2 . head ( )
pd . crosstab ( index = sample [ _STR_ ] , columns = _STR_ ) . plot . barh ( rot = 0 )
pro_result = pro_table . sort_values ( _STR_ , ascending = False )   $ pro_result . head ( )
stmt = _STR_ % ( start , end )   $ pd . read_sql ( stmt , engine ) [ _STR_ ] . values
model_ADP = ARIMA ( ADP_array , ( 2 , 2 , 1 ) ) . fit ( )   $
T = price_mat . shape [ 0 ]   $ nr_coins = price_mat . shape [ 1 ]   $ price_mat . shape
df . to_csv ( path , encoding = _STR_ , index = False )
from scipy . stats import norm   $ print ( norm . cdf ( z_score ) )   $ print ( norm . ppf ( 1 - ( 0.05 / 2 ) ) )   $
bad_comments [ 3 ] . replace ( _STR_ , _STR_ )
from goatools import obo_parser   $ oboUrl = _STR_   $ obo = obo_parser . GODag ( oboUrl , optional_attrs = [ _STR_ ] )
datasets_ref = pd . read_csv ( _STR_ , sep = _STR_ )   $ datasets_slug_id = datasets_ref . set_index ( _STR_ ) [ _STR_ ] . to_dict ( )   $ datasets_id_slug = datasets_ref . set_index ( _STR_ ) [ _STR_ ] . to_dict ( )
print ( _STR_ , p_diff )   $ p_greater = greater / len ( p_d )   $ print ( _STR_ , p_greater )   $ print ( _STR_ . format ( p_greater * 100 ) )
got_data . head ( 10 )
result . summary ( title = _STR_ )
df_h1b_mv_ft . pw_1 . describe ( )
df_imputed = pd . DataFrame ( df_imput )
tobs_results = session . query ( Measurement . station , Measurement . tobs ) . \   $ filter ( Measurement . date . between ( _STR_ , _STR_ ) ) . \   $ order_by ( Measurement . tobs . desc ( ) ) . all ( )   $ tobs_results
test = datatest [ datatest [ _STR_ ] . isnull ( ) ]   $ train = datatest [ datatest [ _STR_ ] . notnull ( ) ]
run txt2pdf . py - o _STR_ _STR_
import nltk   $ from nltk . stem import PorterStemmer   $ import string
turnstiles_df = turnstiles_df . drop ( [ _STR_ , _STR_ ] , axis = 1 , errors = _STR_ )
reason_for_visit . info ( )
df . set_index ( _STR_ , inplace = True )   $ df
group = sdf . groupBy ( _STR_ , window ( _STR_ , _STR_ ) ) . agg ( sum ( _STR_ ) . alias ( _STR_ ) )   $ sdf_resampled = group . select ( group . window . start . alias ( _STR_ ) , group . window . end . alias ( _STR_ ) , _STR_ , _STR_ ) . orderBy ( _STR_ , ascending = True )   $ sdf_resampled . printSchema ( )   $ sdf_resampled . show ( )
treatment_old . index
from IPython . core . display import display , HTML   $ htmlFile = _STR_   $ display ( HTML ( htmlFile ) )   $
spark_df . registerTempTable ( _STR_ )
missing_sample . dropna ( axis = 1 )
priors_product = pd . merge ( priors , products , on = _STR_ )   $ priors_product . head ( )
dbl2 = dbl . reset_index ( )   $ dbl2 [ _STR_ ] = dbl2 . groupby ( _STR_ ) . create_date . transform ( min )   $ dbl2 [ _STR_ ] = dbl2 . create_date == dbl2 . first_date   $ dbl2 . hist ( _STR_ , by = _STR_ )   $
sns . violinplot ( autos [ _STR_ ] ) ;
for ( method , group ) in planets . groupby ( _STR_ ) :   $ print ( _STR_ . format ( method , group . shape ) )
most_active = df . sort_values ( _STR_ ) . reset_index ( drop = True )   $ most_active . head ( 5 )
v_to_c [ _STR_ ] = v_to_c . checkout_time - v_to_c . visit_time
gdax_trans_btc . plot ( kind = _STR_ , x = _STR_ , y = _STR_ , grid = True ) ;
lm3 = sm . OLS ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ res = lm3 . fit ( )   $ res . summary ( )
testing_active_listing = Binarizer ( 10000 )   $ testing_active_listing . fit ( X_test [ _STR_ ] . values . reshape ( - 1 , 1 ) )   $ testing_active_listing_dummy = testing_active_listing . transform (   $ X_test [ _STR_ ] . values . reshape ( - 1 , 1 ) )
peakPricePerDay = dfEPEXpeak . groupby ( dfEPEXpeak . index . date ) . aggregate ( _STR_ ) [ _STR_ ]   $ peakPricePerDay . head ( ) # verify calculation
logs . fm_ip . unique ( )
i = _STR_   $ stock_ts = pd . read_sql ( _STR_ % i , engine )   $ stock_df = stock_ts [ [ _STR_ , _STR_ ] ] . copy ( ) . set_index ( _STR_ )   $ stock_df . rename ( columns = { _STR_ : i } , inplace = True )
all_311_requests = pd . concat ( [ fifteen , sixteen , seventeen , eighteen ] , axis = 0 ) . sort_index ( )
for key , value in r . json ( ) [ _STR_ ] . items ( ) :   $ print ( key , _STR_ , value , _STR_ )
df_amznnews_clsfd_2tick = df_amznnews_clsfd_2tick . set_index ( _STR_ )   $ df_amznnews_clsfd_2tick . info ( )
df3 = df2 . set_index ( _STR_ ) . join ( countries . set_index ( _STR_ ) )   $ df3 . head ( )
len ( [ premieSn for premieSn in SCN_BDAY . scn_age if premieSn < 0 ] ) / SCN_BDAY . scn_age . count ( )
list ( r_ord . values ( ) ) [ 2 ]   $
unique_buying_browsers = transactions . browser_id . unique ( )
len ( apple . resample ( _STR_ ) . mean ( ) )
Lab7_RevenueEPS . to_csv ( _STR_ , index = False )   $
from scipy . stats import norm   $ z_score_sig = norm . cdf ( z_score )   $ critical_value = norm . ppf ( 1 - ( 0.05 / 2 ) )   $ print ( _STR_ , round ( z_score_sig , 4 ) )   $ print ( _STR_ , round ( critical_value , 4 ) )
df_null_acct_name [ _STR_ ] . unique ( )
expiry = datetime . date ( 2015 , 1 , 5 )   $ msft_calls = pd . io . data . Options ( _STR_ , _STR_ ) . get_call_data ( expiry = expiry )   $ msft_calls . iloc [ 0 : 5 , 0 : 5 ]
class Example :   $ self . instance_var = _STR_   $ def class_method ( ) :   $
res_c_i = res_c . merge ( interval_c . to_frame ( ) , left_index = True , right_index = True )
autos . price . value_counts ( ) . sort_index ( ascending = False ) . head ( 20 )
control = df2 [ df2 [ _STR_ ] == _STR_ ]   $ size_control = control . shape [ 0 ]   $ prop_conv_control = control [ control [ _STR_ ] == 1 ] . shape [ 0 ] / control . shape [ 0 ]   $ prop_conv_control
df = pd . read_csv ( _STR_ )   $ df
df2 = df . copy ( )
weather [ _STR_ ] = np . maximum ( weather . power_output - weather . PV_noise , 0 )
num_of_converted = df2 [ df2 . converted == 1 ]   $ p_new = len ( num_of_converted ) / len ( df2 )   $ p_new
stock_data . shape
df_predictions . head ( )
exiftool - csv - createdate - modifydate cisnwf6 / Cisnwf6_cycle3 . MP4 > cisnwf6 . csv
returns = generate_returns ( close )   $ project_helper . plot_returns ( returns , _STR_ )
df2 . shape
df . columns
df . info ( )
mysdc . sc = _STR_   $ mysdc . instr = _STR_   $ mysdc . mode = _STR_   $ mysdc . level = _STR_   $ file = mysdc . Download ( )
bp . head ( )
df_dummies = pd . get_dummies ( df_onc_no_metac [ ls_other_columns ] )   $ dummy_colnames = df_dummies . columns   $ dummy_colnames = [ clean_string ( colname ) for colname in dummy_colnames ]   $ df_dummies . columns = dummy_colnames
notes . head ( )
import pandas as pd   $ df = pd . read_csv ( _STR_ )   $ df . dropna ( how = _STR_ , inplace = True ) #to drop if any value in the row has a nan $ print "Dimensiones en el dataset: ",df.shape $ df.head()
cur . execute ( _STR_ )   $ cur . fetchall ( ) # fetch all the results of the query
def save_combined_df ( df_to_save , file_name ) :   $ df_to_save . to_csv ( _STR_ )
print ( autos [ _STR_ ] . unique ( ) . shape )   $ print ( autos [ _STR_ ] . describe ( ) )   $ print ( autos [ _STR_ ] . value_counts ( ) . head ( 15 ) )
df3 . values
len ( orders . user_id . unique ( ) )
pd . Timestamp ( 2015 , 11 , 11 )
plt . hist ( drt_avg16 , bins = 20 , align = _STR_ ) ;   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )   $ plt . title ( _STR_ ) ;
tips_train = tips . sample ( frac = 0.8 , random_state = 123 )   $ tips_test = tips . loc [ tips . index . difference ( tips_train . index ) , : ]   $ tips_train . shape , tips_test . shape , tips . shape
op = [ ]   $ for row in json_data [ _STR_ ] [ _STR_ ] :   $ op . append ( row [ 1 ] )   $ print ( max ( op ) , min ( op ) )
temp_long_df . tail ( )
q3_results = session . query ( Stations . name , Measurements . tobs ) . filter ( Stations . station == Measurements . station , ) \   $ . filter ( Stations . name == _STR_ ) . all ( )   $ q3_results
val = np . zeros ( len ( dates ) )   $ sample = pd . DataFrame ( val , index = dates )   $ sample [ _STR_ ] = sample . index
sns . pairplot ( df , vars = [ _STR_ , _STR_ , _STR_ , _STR_ ] , hue = _STR_ ) ;
average_polarity = pd . DataFrame ( )   $ count_polarity = pd . DataFrame ( )
t [ np . argmax ( sp_rec ) ]
print ( ozzy . name )
_STR_ . replace ( _STR_ , _STR_ )
print d . variables [ _STR_ ]
table_rows = driver . find_elements_by_tag_name ( _STR_ ) [ 28 ] . find_elements_by_tag_name ( _STR_ )   $
df_t = df . query ( _STR_ )   $ df_c = df . query ( _STR_ )   $ n_mis = df_t . query ( _STR_ ) . user_id . count ( ) + df_c . query ( _STR_ ) . user_id . count ( )   $ n_mis
df_users . to_csv ( _STR_ , index = False )
df . groupby ( _STR_ ) [ _STR_ ] . count ( ) . sort_values ( ascending = False ) [ : 10 ]
condos . head ( )
ekos . load_data ( user_id = user_id , unique_id = workspace_uuid )
a_list . append ( another_list )   $ a_list
stocks . describe ( )
tweet_archive_enhanced_clean = tweet_archive_enhanced_clean [ tweet_archive_enhanced_clean [ _STR_ ] . isnull ( ) ]   $ tweet_archive_enhanced_clean = tweet_archive_enhanced_clean [ tweet_archive_enhanced_clean [ _STR_ ] . isnull ( ) ]
df_western = df [ df [ _STR_ ] . str . contains ( _STR_ ) ]
! hdfs dfs - cat 32 ordered_results - output / part - 0000 * > 32 ordered_results - output . txt   $   ! tail 32 ordered_results - output . txt
countries_df = pd . read_csv ( _STR_ )   $ df_new = countries_df . set_index ( _STR_ ) . join ( df2 . set_index ( _STR_ ) , how = _STR_ )   $ print ( df_new [ _STR_ ] . unique ( ) )   $ df_new . head ( )
p_value = ( normal_dist < diff ) . mean ( )   $ p_value
q3 = pd . Period ( _STR_ , freq = _STR_ )
from tqdm import tqdm   $ from time import sleep   $ for i in tqdm ( range ( 100 ) ) :   $ sleep ( 0.1 )
df_mas [ _STR_ ] = df_mas [ _STR_ ] . astype ( float )   $ df_mas [ _STR_ ] = df_mas [ _STR_ ] . astype ( float )
days = np . array ( [ _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ ,   $ _STR_ ] )   $ data_vi [ _STR_ ] = days [ data_vi . index . weekday ]
df_clean [ _STR_ ] . dtype
% time pax_raw = pd . read_hdf ( os . path . join ( data_dir , hdf_path ) , _STR_ )
print ( np . min ( rhum ) , np . mean ( rhum ) , np . max ( rhum ) )
pd . crosstab ( aqi [ _STR_ ] , aqi [ _STR_ ] , normalize = _STR_ , margins = True )
t_ave = ( weather . TMAX + weather . TMIN ) / 2   $ weather [ _STR_ ] = t_ave   $ weather . info ( )
pd . concat ( pieces )
transform = am . tools . axes_check ( np . array ( [ x_axis , y_axis , z_axis ] ) )   $ b = transform . dot ( burgers )   $ print ( _STR_ , b )
sel_stats = [ func . min ( Measurement . tobs ) , func . max ( Measurement . tobs ) , func . avg ( Measurement . tobs ) ]   $ session . query ( * sel_stats ) . filter ( Measurement . station == station_with_highest_observations ) . all ( )
print ( _STR_ . format ( popt_axial_brace_saddle [ 2 ] [ 0 ] ) )   $ perr = np . sqrt ( np . diag ( pcov_axial_brace_saddle [ 2 ] ) ) [ 0 ]   $ print ( _STR_ . format ( perr ) )
input_data . snow_depth . value_counts ( )
print ( np . min ( prec ) , np . mean ( prec ) , np . max ( prec ) )
lots . ix [ - 1 ]
cryptos . to_json ( )   $ cryptos . to_csv ( )
new_df . head ( )
pivoted . T [ labels == 1 ] . T . plot ( legend = False , alpha = 0.1 ) ;
dat [ dat . zip . isnull ( ) ]
autos = autos . drop ( _STR_ , 1 )
for j in top_tracks :   $ top_tracks [ j ] = [ ( i . split ( _STR_ ) [ 0 ] + _STR_ + i . split ( _STR_ ) [ 1 ] + _STR_ + i . split ( _STR_ ) [ 2 ] + _STR_ + i . split ( _STR_ ) [ 3 ] ) for i in top_tracks [ j ] ]   $
dfData [ _STR_ ] = [ _STR_ if ( type ( x ) == NoneType ) else x for x in dfData [ _STR_ ] ] # 'Standard Sale' is the most common. $ dfData['financing'] = ['Conventional' if (type(x) == NoneType) else x for x in dfData['financing']]  # 'Conventional' is the most common. $ dfData.head(10)
df . isnull ( ) . sum ( )
data [ ( data [ _STR_ ] == _STR_ ) & ( data [ _STR_ ] == _STR_ ) ] . comment_body . head ( 15 )   $
temp_df . plot . hist ( by = _STR_ , bins = 12 )   $ plt . show ( )   $ plt . savefig ( _STR_ , bbox_inches = _STR_ )
data . shape
df . count ( )
pipeline . fit ( fb_train . message , fb_train . popular )
def format_sample ( x ) :   $ data = json . loads ( x )   $ data [ _STR_ ] = datetime . fromtimestamp ( data [ _STR_ ] ) . strftime ( _STR_ )   $ data [ _STR_ ] = data . pop ( _STR_ )   $ return ( data [ _STR_ ] , json . dumps ( data ) )
data_df [ data_df [ _STR_ ] == 21 ] [ _STR_ ] . min ( )
input_data = pd . merge ( mit , weather , on = _STR_ )
def plot_fi ( fi ) : return fi . plot ( _STR_ , _STR_ , _STR_ , figsize = ( 6 , 7 ) , legend = False )   $ fi = pd . DataFrame ( { _STR_ : X_data . columns , _STR_ : m . feature_importances_ }   $ ) . sort_values ( _STR_ , ascending = False )   $ plot_fi ( fi )   $ plt . show ( )
positive_review_transformed = bow_transformer . transform ( [ positive_review ] )   $ nb . predict ( positive_review_transformed ) [ 0 ]
print ( loadedModelArtifact . name )   $ print ( saved_model . uid )
store . delete_collection ( _STR_ )
m_test = merged_test [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] . set_index ( _STR_ )   $ m_test . shape
returned_orders_data = combined_df . loc [ combined_df [ _STR_ ] == _STR_ ]   $ print ( returned_orders_data )
to_be_predicted_Day3 = 25.10910355   $ predicted_new = ridge . predict ( to_be_predicted_Day3 )   $ predicted_new
rf_vect = RandomForestClassifier ( n_estimators = 50 , random_state = 42 , n_jobs = - 1 )   $ scores = cross_val_score ( rf_vect , X , y , cv = skf )   $ print _STR_ , scores   $ print _STR_ , scores . mean ( )
df2 = df . drop ( df . query ( _STR_ ) . index )   $ df2 = df2 . drop ( df2 . query ( _STR_ ) . index )
ind = ind . drop ( _STR_ , axis = 1 )
duplicate_row_indexes = list ( duplicate_rows . index )   $ duplicate_row_indexes
local_sea_level_stations . columns = [ name . strip ( ) . replace ( _STR_ , _STR_ )   $ for name in local_sea_level_stations . columns ]   $ local_sea_level_stations . columns
urls = r . json ( )   $ urls
trn_y = train_small_sample . is_attributed   $ val_y = val_small_sample . is_attributed
legLastOnly = leg [ leg [ _STR_ ] . isnull ( ) ]
data . fillna ( { _STR_ : 2013 , _STR_ : 2 } )
lastDay = lobbyFrame [ _STR_ ] . max ( ) # This is the end date of the line chart!! $ print(lastDay)
time . gmtime ( )
Meter1 . QurreyProgress ( debug = True )
for dataset in full_data :   $ dataset [ _STR_ ] = dataset [ _STR_ ] . fillna ( _STR_ )   $ print ( train [ [ _STR_ , _STR_ ] ] . groupby ( [ _STR_ ] , as_index = False ) . mean ( ) )
df_twitter_archive_copy . stage . dtype
ma = ffinal . groupby ( [ _STR_ , _STR_ , _STR_ ] ) [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] . sum ( ) . reset_index ( )   $ mb = ffinal . groupby ( [ _STR_ , _STR_ , _STR_ ] ) [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] . sum ( ) . reset_index ( )   $ ma . columns = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ mb . columns = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ ma . head ( )
cFrame [ _STR_ ] = cFrame . groupby ( _STR_ ) [ _STR_ ] . apply ( lambda x : x . cumsum ( ) )   $ cFrame . head ( 20 )
wine_reviews . shape # To check the number of rows and columns in our datset
agg_dict = { _STR_ : [ _STR_ , _STR_ , _STR_ ] ,   $ _STR_ : [ _STR_ , _STR_ ] }   $ weather_all . groupby ( _STR_ ) . agg ( agg_dict )
precipitation_df . plot ( )   $
labels = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ tweet_df = pd . DataFrame ( tweet_list , columns = labels )
unstacked . head ( 10 )
len ( test_dict . keys ( ) )
train . head ( )
Lab7_Equifax . to_csv ( _STR_ , index = False )
data [ data . name == _STR_ ] . head ( )
df . dropna ( subset = [ _STR_ ] , how = _STR_ )   $ df = remove_duplicate_index ( df )
multipoint_conn_pivot = pivot_condition_time ( multipoint_df , _STR_ , _STR_ ,   $ ( _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ) )   $ multipoint_conn_pivot
dist = np . sum ( bag , axis = 0 )   $ dist
new_orgs . shape
appointments = pd . read_csv ( _STR_ )
df_combined . dtypes   $
USvideos . tail ( 10 )
df3 . info ( )
best_ids_df = topics_data [ topics_data . id . isin ( best_ids ) ]   $ display ( best_ids_df )
df1 . io_state [ 2 ]
x_train = dfTrain [ [ _STR_ ] ] . iloc [ : nb_samples ]   $ x_test = dfTest [ [ _STR_ ] ]
teams1 = list ( df1 . Home . unique ( ) )   $ teams2 = list ( df2 . Home . unique ( ) )   $ set ( teams1 ) - set ( teams2 )
month3 = oanda . get_history ( instrument_1 ,   $ start = _STR_ ,   $ end = _STR_ ,   $ granularity = _STR_ ,   $ price = _STR_ )
df_new [ _STR_ ] . value_counts ( )
TrainData . drop ( [ _STR_ , _STR_ ] , axis = 1 ) . to_csv ( _STR_ , index = False )   $ TestData . drop ( [ _STR_ , _STR_ ] , axis = 1 ) . to_csv ( _STR_ , index = False )
data . loc [ data . LRank == _STR_ , [ _STR_ ] ] = np . nan
asfr . describe ( )
matthew [ _STR_ ] = matthew . text . apply ( lambda text : pd . Series ( [ x in text for x in SOCIAL_NETWORKS ] ) . any ( ) )   $ matthew [ _STR_ ] = matthew . text . apply ( lambda text : pd . Series ( [ x in text for x in DECISION_MAKING ] ) . any ( ) )   $ matthew [ _STR_ ] = matthew . text . apply ( lambda text : pd . Series ( [ x in text for x in ADAPTIVE_CAPACITY ] ) . any ( ) )
client = MongoClient ( _STR_ , 27017 )   $ db = client . renthop   $ collection = db . listings   $ pp_bold ( _STR_ . format ( collection . count ( ) ) )
query = session . query ( Measurement )   $ rain = query . filter ( Measurement . date >= year_ago_str ) . all ( )   $ print ( len ( rain ) )
df = pd . concat ( [ df , gender_dummies ] , axis = 1 )   $ df . head ( 2 )
df_usa = df_usa . pivot ( columns = _STR_ , values = _STR_ )   $ df_usa . head ( )
print ( broadband . shape )   $ print ( broadband . head ( 10 ) )
image_file = pd . read_csv ( _STR_ , sep = _STR_ )
dfAbes = df [ df [ _STR_ ] . apply ( lambda x : x . split ( _STR_ ) [ 0 ] ) == _STR_ ]
f_ip_os_clicks . show ( 1 )
contractor = pd . read_csv ( _STR_ )   $ state_lookup = pd . read_csv ( _STR_ )
df1 = add_percentiles ( df )   $ df1 . head ( )
to_be_predicted_Day4 = 26.69300296   $ predicted_new = ridge . predict ( to_be_predicted_Day4 )   $ predicted_new
class Widget : # same as "class Widget(object):" $         print(self.__class__)  # __class__ is the easy way to get at an object's class $     @staticmethod $     def print_class():  # Static method as it has no 'self' parameter $
new_page_converted = np . random . choice ( [ 1 , 0 ] , size = n_new , p = [ convert_rate_new , ( 1 - convert_rate_new ) ] )   $ new_page_converted . mean ( )
pd . Series ( sales , index = pd . date_range ( start = _STR_ , periods = len ( sales ) , freq = _STR_ ) )
load2017 [ _STR_ ] = load2017 [ _STR_ ] . str . slice ( 0 , 10 )   $ load2017 [ _STR_ ] = pd . to_datetime ( load2017 [ _STR_ ] )   $ load2017 [ _STR_ ] . head ( )
vulnerability_histogram = cved_df . groupby ( by = [ _STR_ ] ) [ _STR_ ] . count ( )   $ vulnerability_histogram
df2 [ df2 [ _STR_ ] == _STR_ ] . user_id . count ( ) / df2 . user_id . count ( )
print ( _STR_ , lda_model . log_perplexity ( corpus ) ) # a measure of how good the model is. lower the better. $ coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v') $ coherence_lda = coherence_model_lda.get_coherence() $ print('\nCoherence Score: ', coherence_lda)
df2 [ _STR_ ] = 1   $ df2 . head ( )
plt . title ( _STR_ )   $ aggregated_parent . plot ( kind = _STR_ , figsize = ( 15 , 7 ) )
price_series = pd . Series ( brand_mean_prices )   $ df = pd . DataFrame ( price_series , columns = [ _STR_ ] )   $ km_series = pd . Series ( brand_mean_km )   $ df [ _STR_ ] = km_series   $
df_goog . index . min ( ) , df_goog . index . max ( )
print ( df . apply ( np . cumsum ) )
taxiData2 . loc [ taxiData2 . Fare_amount <= 0 , _STR_ ] = 1
_STR_ . format ( 100000.154787895444 )
bp_medication_df . head ( )
df = pd . DataFrame ( bmp_series , columns = [ _STR_ ] )   $ df
browser . click_link_by_partial_text ( _STR_ )
cityID = _STR_   $ for tweet in tweepy . Cursor ( api . search , q = q , contained_within = cityID , wait_on_rate_limit = True , lang = _STR_ ) . items ( 500 ) :         $ New_York . append ( tweet )
d = r . json ( )   $ d
geo_db . head ( )
y_val_predicted_labels_tfidf = classifier_tfidf . predict ( X_val_tfidf )   $ y_val_predicted_scores_tfidf = classifier_tfidf . decision_function ( X_val_tfidf )
print ( _STR_ , df_usa . isnull ( ) . sum ( ) )   $ df_usa = df_usa . fillna ( 0 )   $ print ( _STR_ )   $ print ( _STR_ , df_usa . isnull ( ) . sum ( ) )
from pandas import ExcelWriter   $ with ExcelWriter ( _STR_ ) as writer :   $ aapl . to_excel ( writer , sheet_name = _STR_ )   $ df . to_excel ( writer , sheet_name = _STR_ )
my_image = tf . placeholder ( _STR_ , [ None , None , 3 ] )   $ slice = tf . slice ( my_image , [ 10 , 0 , 0 ] , [ 16 , - 1 , - 1 ] )
forecast_data = ( forecast_data   $ . merge ( test_data )   $ )   $ forecast_data . head ( )
bc = pd . read_csv ( _STR_ )
tweet_archive . describe ( )
master_list [ master_list [ _STR_ ] >= 5 ] [ _STR_ ] . describe ( )
import seaborn as sns   $ sns . lmplot ( x = _STR_ , y = _STR_ , data = data , fit_reg = True )
bad_dtype_df . index . set_levels ( [ pd . to_datetime ( bad_dtype_df . index . levels [ 0 ] ) , bad_dtype_df . index . levels [ 1 ] ,   $ bad_dtype_df . index . levels [ 2 ] , bad_dtype_df . index . levels [ 3 ] ,   $ bad_dtype_df . index . levels [ 4 ] , bad_dtype_df . index . levels [ 5 ] ] ,   $ inplace = True )   $ index_level_dtypes ( bad_dtype_df )
df2 [ df2 . user_id == 773192 ]
! wget https : // raw . githubusercontent . com / sunilmallya / mxnet - notebooks / master / python / tutorials / data / p2 - east - 1 b . csv
df . filter ( ~ a_sevenPointSeven ) . count ( )
logs [ _STR_ ] . resample ( _STR_ ) . count ( ) . plot ( )   $ plt . show ( )
sc . stop ( )
df2 [ [ _STR_ , _STR_ ] ] = pd . get_dummies ( df2 [ _STR_ ] )   $ df2 = df2 . drop ( _STR_ , axis = 1 )   $ df2 . head ( )
df1 [ _STR_ ] = ( df1 [ _STR_ ] - df1 [ _STR_ ] ) / df1 [ _STR_ ]   $ df1 [ _STR_ ] . head ( )
sen_dat = twt_data . join ( sentiment_data [ [ _STR_ , _STR_ , _STR_ ] ] )
decoder_input = tf . reshape ( caps2_output_masked ,   $ [ - 1 , caps2_n_caps * caps2_n_dims ] ,   $ name = _STR_ )
areas_dataframe [ areas_dataframe . company_id == cid ]
tt_final [ ( tt_final . p1_dog == True ) ] [ _STR_ ] . value_counts ( ) . head ( 10 )
html = browser . html   $ soup = bs ( html , _STR_ )   $ url_hemispheres_link = soup . find_all ( _STR_ , class_ = _STR_ )   $ url_hemispheres_link
df_vec_sums = df_vec . sum ( )   $ df_vec_sums = df_vec_sums . sort_values ( ascending = False )   $ dftops = df_vec_sums [ df_vec_sums >= 8000 ]   $ dftops
data [ _STR_ ] = pd . DatetimeIndex ( data [ _STR_ ] )   $ data . head ( )
archive_version = _STR_ # i.e. '2017-11-10'
fullData = pd . concat ( [ tweets , sentiment_df ] , axis = 1 )   $ fullData . head ( )
df [ _STR_ ] = df [ _STR_ ] . apply ( lambda tweet : TextBlob ( tweet ) . sentiment [ 0 ] )
check_measurements . __dict__
support . groupby ( [ _STR_ , _STR_ ] ) [ _STR_ ] . sum ( ) . reset_index ( ) . sort_values ( _STR_ , ascending = False )
df2 [ _STR_ ] . nunique ( )
negative = _STR_   $ negative = pd . read_table ( negative , encoding = _STR_ )
temp_series = pd . Series ( temperatures , dates )   $ temp_series
import statsmodels . api as sm   $ convert_old = df2 . query ( _STR_ ) [ _STR_ ] . sum ( )   $ convert_old   $
results = [ ]   $ for tweet in tweepy . Cursor ( api . search , q = _STR_ ) . items ( 100 ) :   $ results . append ( tweet )
df . columns
df_all_wells_basic . info ( )
df_datecols = df_uro . select_dtypes ( include = [ _STR_ ] ) . drop ( columns = [ _STR_ , _STR_ ] )   $ df_datecols . shape
browser = webdriver . Chrome ( _STR_ )   $ browser . get ( url )   $ time . sleep ( 3 )   $ html = browser . page_source   $ soup = BeautifulSoup ( html , _STR_ )   $
df . loc [ df . category == 1022200 , _STR_ ] = _STR_
data . loc [ data . expenses > 150000 , _STR_ ] = np . NaN
test . shape
users = pd . read_sql_table ( _STR_ , con = engine )   $ customers = users [ users . role == 0 ]   $ cleaners = users [ users . role == 1 ]   $ leads = users [ users . role == 3 ]   $ applicants = users [ users . role == 4 ]
print ( api . VerifyCredentials ( ) )
train = pd . read_csv ( _STR_ )   $ test = pd . read_csv ( _STR_ )   $ test [ _STR_ ] = test [ _STR_ ] . astype ( _STR_ )   $ train [ _STR_ ] = train [ _STR_ ] . astype ( _STR_ )
data = data [ ~ data [ _STR_ ] . isnull ( ) ]   $ print ( _STR_ , len ( data ) )
df1 [ _STR_ ] = pd . to_datetime ( df1 [ _STR_ ] , format = _STR_ ) . dt . hour # to create a new column with the hour information $ df1.head()
df . columns
trips_data . groupby ( _STR_ ) . duration . mean ( ) . reset_index ( ) . plot . scatter ( _STR_ , _STR_ )   $ plt . show ( ) # It can be seen that average duration per trip is higher on weekends
nr_rev = _STR_   $ vectorizer . transform ( [ nr_rev ] )   $ M_NB_model . predict_proba ( vectorizer . transform ( [ nr_rev ] ) )
date = datetime . datetime . now ( )
df_countries . head ( )
df_vow [ _STR_ ] . unique ( )
for i in range ( 0 , 10 ) :   $ topics = model . show_topic ( i , 10 )   $ print _STR_ % ( i , ( _STR_ . join ( [ str ( word [ 0 ] ) for word in topics ] ) ) )
from sklearn . metrics import confusion_matrix   $ confusion_matrix = confusion_matrix ( y_test , y_pred_clf )   $ print ( confusion_matrix )
train . readingScore [ train . male == 1 ] . mean ( )
print ( a )   $ print ( a . index ( _STR_ ) ) # Get index of first matching entry; throws exception if not found $ print(a.count('cat'))  # Count the number of instances of an element
mb . swaplevel ( _STR_ , _STR_ ) . head ( )
list ( db . fs . files . find ( ) )
df . head ( 2 )
dfNiwot = df . loc [ df [ _STR_ ] == _STR_ ] . copy ( )   $ dfNiwot . head ( )
graf_train = graf_train . drop ( _STR_ , axis = 1 )
import regex as re   $ from nltk . corpus import stopwords
tweets [ 0 ] . _json . keys ( )
contractor_clean [ contractor_clean [ _STR_ ] . isin ( [ 382 , 383 , 384 , 385 , 386 , 387 ] ) ] [ _STR_ ]
bthlst = list ( df_bthlst [ df_bthlst [ _STR_ ] . isin ( bth_dlst ) ] [ _STR_ ] )
show_max = 20   $ columns_meta = list ( set ( click_condition_meta . columns ) - set ( not_fit ) - set ( clicking_conditions . columns ) )   $ for c in columns_meta :   $ print ( c , _STR_ , _STR_ , len ( click_condition_meta [ c ] . unique ( ) ) , _STR_ ,   $ list ( click_condition_meta [ c ] . unique ( ) ) [ : show_max ] , _STR_ )
All_tweet_data = pd . merge ( All_tweet_data , Imagenes_data_v2 , on = _STR_ , how = _STR_ )   $
yt . get_subscriptions ( channel_id , key )
print ( _STR_ + sys . version )   $ print ( _STR_ + pd . __version__ )   $ print ( _STR_ + matplotlib . __version__ )   $ print ( _STR_ + np . __version__ )
prob_temp = df2 . query ( _STR_ ) [ _STR_ ] . mean ( ) * 100   $ print ( _STR_ . format ( round ( prob_temp , 2 ) ) )
pold = df2 . converted . mean ( )   $ pold
twitter_ar . to_csv ( _STR_ , index = False )
date + pd . to_timedelta ( np . arange ( 12 ) , _STR_ )
df [ _STR_ ] = np . array ( [ sentiment_finder_partial ( comment ) for comment in df [ _STR_ ] ] )
index_of_wrong_mapped_cols = df . query ( _STR_ ) . index
sns . pairplot ( subset_sessions_summary , hue = _STR_ )
df = pd . read_json ( _STR_ , orient = _STR_ )
query = _STR_   $ df = pd . read_sql ( query , session )   $ df . head ( 10 )
joined = join_df ( joined , googletrend , [ _STR_ , _STR_ , _STR_ ] )   $ joined_test = join_df ( joined_test , googletrend , [ _STR_ , _STR_ , _STR_ ] )   $ sum ( joined [ _STR_ ] . isnull ( ) ) , sum ( joined_test [ _STR_ ] . isnull ( ) )
pd . Series ( [ 4 , np . nan , 7 , np . nan , - 3 , 2 ] ) . sort_values ( )
plt . hist ( null_value ) ;   $ plt . axvline ( x = obs_diff , color = _STR_ ) ;
df [ df . index . month . isin ( [ 5 , 6 , 7 ] ) ] [ _STR_ ] . value_counts ( ) . head ( )
kd915_filtered . head ( )
person = _STR_   $ person_counts = df [ df [ _STR_ ] == person ] . groupby ( TimeGrouper ( freq = _STR_ ) ) . agg ( { _STR_ : _STR_ } ) . rename ( columns = { _STR_ : _STR_ } )   $ person_counts . reset_index ( inplace = True )   $ person_counts . head ( )
rollcorr_daily [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] . plot ( )   $ plt . show ( )
print ( df_train . shape )   $ df_train . head ( 10 )
for idx , match_id in full_dataset . match_id . iteritems ( ) :   $ date = data . loc [ data . match_id == match_id , _STR_ ] . tolist ( ) [ 0 ]   $ full_dataset . loc [ idx , _STR_ ] = date
for i in range ( 0 , df1 . columns . size - 1 ) :   $ print ( _STR_ + df1 . columns [ i ] + _STR_ )   $ print ( df1 . iloc [ : , i ] . unique ( ) )   $ print ( _STR_ )
loan_stats [ _STR_ ] . head ( rows = 2 )
df [ df [ _STR_ ] ] . sample ( 10 )
temp_df = temp_df . reset_index ( ) [ [ _STR_ , _STR_ ] ]
flight6 . printSchema ( )
steemit_accounts = unique_urls [ ( unique_urls . domain == domain ) & ( unique_urls . url . str . contains ( _STR_ ) ) ]   $ steemit_accounts . sort_values ( _STR_ , ascending = False ) [ 0 : 50 ] [ [ _STR_ , _STR_ ] ]
n_new = df2 [ df2 [ _STR_ ] == _STR_ ] . count ( ) [ 0 ]   $ n_new
coin_mean > .5
by_area [ _STR_ ] . plot ( ) ; plt . legend ( ) ;
name = df_titanic [ _STR_ ]   $ print ( name . describe ( ) )
pax_raw . info ( )
engine = create_engine ( _STR_ )
autos [ _STR_ ] . describe ( )
session . query ( Measurement . station , func . count ( Measurement . tobs ) ) . \   $ group_by ( Measurement . station ) . \   $ order_by ( func . count ( Measurement . tobs ) . desc ( ) ) . all ( )   $
zip_1_sns . Date = zip_1_sns . Date . str . replace ( _STR_ , _STR_ )   $ zip_1_sns . Date = pd . to_datetime ( zip_1_sns . Date , format = _STR_ )   $ zip_2_sns . Date = zip_2_sns . Date . str . replace ( _STR_ , _STR_ )   $ zip_2_sns . Date = pd . to_datetime ( zip_2_sns . Date , format = _STR_ )
fin_coins_r . shape , fin_r_monthly . index . min ( ) , r_top10_mat_na . index . min ( ) , fundret . index . min ( )
twitter_archive . info ( )
weather . loc [ weather . NAME == _STR_ ] . boxplot ( column = _STR_ ) ;
df4 [ df4 [ _STR_ ] == 1 ] . count ( ) [ 0 ] / df4 . count ( ) [ 0 ]
autos [ _STR_ ] . head ( )
df . shape
p_new = df2 [ _STR_ ] . mean ( )   $ print ( p_new )
nu_fission_rates = fuel_rxn_rates . get_slice ( scores = [ _STR_ ] )   $ nu_fission_rates . get_pandas_dataframe ( )
z1 . interval . isnull ( ) . sum ( )
print ( _STR_ , sum ( np . sum ( np . array ( tokendata . isnull ( ) ) , axis = 1 ) ) )
delte1 = [ ]   $ for i in or_list1 :   $ if ( i not in and_list1 ) :   $ delte1 . append ( i )   $
crime_df . info ( )
injury_df [ _STR_ ] = injury_df [ _STR_ ] . map ( lambda x : x . replace ( _STR_ , _STR_ ) )   $ injury_df [ _STR_ ] . head ( )
train [ _STR_ ] . tail ( 5 )
print ( sentences [ 75 ] )   $ analyzer . polarity_scores ( sentences [ 75 ] )
bad_dates = mapped . filter ( lambda row : ( row [ 3 ] == - 2 or row [ 4 ] == - 2 ) )   $
evaluation_2 = api . create_evaluation ( ensemble_2 , test_dataset )   $ api . ok ( evaluation_2 )
if using_sample :   $ pax_raw . to_hdf ( os . path . join ( data_dir , hdf_path ) , _STR_ )   $ else :   $ pax_raw . to_hdf ( os . path . join ( data_dir , hdf_path ) , _STR_ )
dt_features [ _STR_ ] = pd . to_datetime ( dt_features [ _STR_ ] , unit = _STR_ )
manager . image_df [ _STR_ ] . isin ( tree_features_df [ _STR_ ] ) . describe ( )
rain_df = pd . DataFrame ( rain )   $ rain_df . head ( )
tweet_df [ tweet_df . total == 1 ] . sum ( ) . drop ( _STR_ )
item1 = { _STR_ : ( _STR_ , _STR_ , _STR_ , _STR_ ) , _STR_ : np . random . randint ( 0 , 4 , 4 ) }   $ item2 = { _STR_ : np . random . randint ( 0 , 4 , 5 ) , _STR_ : np . random . randint ( 0 , 4 , 5 ) }   $ pd . Panel ( { _STR_ : item1 , _STR_ : item2 } )
print ( _STR_ , df2 . converted . mean ( ) )
image_path1 = _STR_   $ img1 = pil_image . open ( image_path1 )   $ img1 = img1 . convert ( _STR_ )
rng = pd . date_range ( _STR_ , _STR_ )
df . groupby ( [ _STR_ , _STR_ ] ) [ _STR_ ] . nlargest ( 5 )   $
public_table = public_table . rename ( columns = { _STR_ : _STR_ , _STR_ : _STR_ } )
vio2016 = vio2016 . rename ( index = str , columns = { _STR_ : _STR_ } )   $ ins2016 = pd . merge ( ins2016 , vi , on = [ _STR_ , _STR_ ] ) . rename ( index = str , columns = { _STR_ : _STR_ } )   $ ins2016
df2 [ [ _STR_ , _STR_ , _STR_ ] ] = pd . get_dummies ( df2 [ _STR_ ] )   $ df2 . head ( 10 )
df . sort_values ( _STR_ , ascending = True ) . head ( )
df_user_extract_copy . info ( )
df2_c = df2 . query ( _STR_ )   $ df2_c . query ( _STR_ ) . user_id . count ( ) / df2_c . user_id . count ( )
calls_df [ _STR_ ] . value_counts ( )
Z = np . linspace ( 0 , 1 , 11 , endpoint = False ) [ 1 : ]   $ print ( Z )
autos = autos [ autos [ _STR_ ] . between ( 1900 , 2100 ) ]
tweets_clean [ _STR_ ] = tweets_clean [ _STR_ ] . astype ( _STR_ )
config_path = os . path . join ( config_root , model_name )   $ config = Config . from_config_file ( config_path )   $ model = seed_model_from_config ( config )
users [ _STR_ ] = [ user in adopted for user in users . object_id . values ]
crimes [ [ _STR_ , _STR_ ] ] . head ( )
ndvi_change = ndvi_of_interest02 - ndvi_of_interest   $ ndvi_change . attrs [ _STR_ ] = affine
train [ [ _STR_ , _STR_ ] + columns ] . head ( )
display_all ( train_data [ [ _STR_ , _STR_ ] ] . isnull ( ) . sum ( ) / len ( train_data ) )
image_predictions_copy = image_predictions_copy [ image_predictions_copy . p1_dog == True ]
learn . save ( _STR_ )
naive_bayes_classifier = nltk . NaiveBayesClassifier . train ( train_set )
test_float [ _STR_ ] = 0   $ test_float . loc [ test_float . description . str . contains ( _STR_ , na = False ) , _STR_ ] = 1   $ test_float . balcon . value_counts ( )
charge = reader . select_column ( _STR_ )   $ charge = charge . values # Convert from Pandas Series to numpy array $ charge
df2 = df . copy ( )   $ df2 . columns
df . query ( _STR_ ) . shape [ 0 ] +   \   $ df . query ( _STR_ ) . shape [ 0 ]
banks [ 0 ] . head ( 2 ) . to_html ( _STR_ )   $   ! head - 30 failed_banks . html
autos [ _STR_ ] . unique ( )
recommendationTable_df = recommendationTable_df . sort_values ( ascending = False )   $ recommendationTable_df . head ( )
hmeq . head ( )
df [ _STR_ ] . value_counts ( ) . sort_values ( ascending = True ) . plot ( kind = _STR_ )   $
os . environ [ _STR_ ] = _STR_ + s3_key_merge   $ os . environ [ _STR_ ] = _STR_   $   ! earthengine upload image - - asset_id = $ asset_id   $ gs_key
hrefs = soup . find_all ( _STR_ , href = _STR_ )   $ hrefs
df_lm . filter ( regex = _STR_ ) . boxplot ( by = _STR_ , figsize = ( 10 , 10 ) , showfliers = False )
df2 . query ( _STR_ )   $
x_normalized . index = intersections_irr . index
print ( gs_rfc_over . best_score_ )   $ print ( gs_rfc_over . best_params_ )
top_10_authors = git_log . author . value_counts ( ) . head ( 10 )   $ top_10_authors
n_old = df2 [ df2 [ _STR_ ] == _STR_ ] . shape [ 0 ]   $ print ( n_old )
irmaFlorida = pd . read_csv ( _STR_ , index_col = _STR_ )   $ irmaFlorida
print ( len ( coins_infund ) , _STR_ ,   $ len ( intersection ) , _STR_ )
airbnb_df . loc [ 0 : 5 , [ _STR_ , _STR_ ] ]
df_btc . head ( )
% % time   $ df = pd . read_pickle ( _STR_ )
df . isnull ( ) . sum ( ) / len ( df )
feature_list = [ _STR_ , _STR_ , _STR_ , _STR_ ]   $ vecAssembler = VectorAssembler ( inputCols = feature_list , outputCol = _STR_ )   $ feature_sel = vecAssembler . transform ( all_feature )
print ( _STR_ + color . RED + color . BOLD + os . getcwd ( ) + color . END )   $
np . exp ( - 0.0408 ) , np . exp ( 0.0099 )
for ix , file in enumerate ( s3_key_origs ) :   $ s3_download . meta . client . download_file ( s3_bucket , file , local_orig_keys [ ix ] )
non_empty_crash_data_df = pd . read_csv ( _STR_ )   $ non_empty_crash_data_df . drop ( _STR_ , axis = 1 , inplace = True )   $ non_empty_crash_data_df
cfs_df . TimeArrive = pd . to_datetime ( cfs_df . TimeArrive )   $ cfs_df . TimeCreate = pd . to_datetime ( cfs_df . TimeCreate )   $ cfs_df . TimeClosed = pd . to_datetime ( cfs_df . TimeClosed )   $ cfs_df . TimeDispatch = pd . to_datetime ( cfs_df . TimeDispatch )
with open ( _STR_ , _STR_ ) as fd :   $ IMDB_dftouse_dict = json . load ( fd )
sales = graphlab . SFrame ( _STR_ )   $ sales . head ( )
data [ _STR_ ] = data [ _STR_ ] . apply ( lambda x : datetime . datetime .   $ strptime ( x , _STR_ ) )   $ data . info ( )
train . shape
wrd_api_clean = wrd_api . copy ( )   $ wrd_api_clean = wrd_api_clean . transpose ( ) . sort_index ( ascending = True )   $ wrd_api_clean = wrd_api_clean [ [ _STR_ , _STR_ , _STR_ ] ]   $ wrd_api_clean . rename ( columns = { _STR_ : _STR_ } , inplace = True )
old_page_converted = np . random . choice ( [ 1 , 0 ] , size = nold , p = [ pmean , ( 1 - pmean ) ] )   $ old_page_converted . mean ( )
client = foursquare . Foursquare ( client_id = _STR_ , client_secret = _STR_ , redirect_uri = _STR_ )   $ auth_uri = client . oauth . auth_url ( )
run txt2pdf . py - o _STR_ _STR_
pd . options . display . max_colwidth = 200   $ data . sort_values ( by = _STR_ , ascending = False ) [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] [ 0 : 19 ]
diff = prob_convert_t - prob_convert_c   $ plt . hist ( p_diffs ) ;   $ plt . axvline ( x = diff , color = _STR_ ) ;   $ plt . title ( _STR_ ) ;
github_data . drop_duplicates ( _STR_ ) . user_type . value_counts ( )
y_pred = lin_clf . predict ( X_train_scaled )   $ accuracy_score ( y_train , y_pred )
n_old = ( df2 [ _STR_ ] == _STR_ ) . sum ( )   $ n_old
dfX = df2 . query ( _STR_ )   $ control_convert = dfX . converted . sum ( ) / dfX . count ( ) [ 0 ]   $ control_convert
job_requirements . values
Dxs_summarized = { }   $ for Dx in Dxs_info . keys ( ) :   $ pprint ( Dxs_info [ Dx ] [ 3 ] )   $ Dxs_summarized [ Dx ] = input ( _STR_ )
autos [ _STR_ ] . describe ( )
old_page_converted = np . random . choice ( [ 0 , 1 ] , size = nold , p = [ 1 - rate_pold_null , rate_pold_null ] )   $ print ( len ( old_page_converted ) )
stop_words = set ( stopwords . words ( _STR_ ) )   $ stop_words . update ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] )   $ for doc in document :   $ list_of_words = [ i . lower ( ) for i in wordpunct_tokenize ( doc ) if i . lower ( ) not in stop_words ]   $ stop_words . update ( list_of_words )
features = features . drop ( _STR_ , axis = 1 )   $ features = features . rename ( columns = { _STR_ : _STR_ } )   $ features . head ( 2 )
df_new [ _STR_ ] = 1   $ lm = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ res = lm . fit ( )   $ res . summary ( )
df2 = df . query ( _STR_ )
rddScaledScores . reduce ( lambda s1 , s2 : s1 + s2 ) / rddScaledScores . count ( )
pd . DataFrame ( { _STR_ : [ _STR_ , _STR_ ] , _STR_ : [ _STR_ , _STR_ ] } )
! pip install - - upgrade git + https : // github . com / GeneralMills / pytrends   $   ! pip install lxml
pipeline = Pipeline ( stages = stages_with_naive_bayes )   $ model = pipeline . fit ( trainingData )   $ predictions = model . transform ( testData )
new_page_converted . mean ( ) - old_page_converted . mean ( )
old_page_converted = np . random . choice ( [ 1 , 0 ] , size = nold , p = [ p_mean , ( 1 - p_mean ) ] )   $ print ( len ( old_page_converted ) )
df . shape
solar_corr = rets . corr ( )   $ print ( solar_corr )
user_profiles = [   $ { _STR_ : 218 , _STR_ : _STR_ } ,   $ { _STR_ : 219 , _STR_ : _STR_ } ]   $ result = db . profiles . insert_many ( user_profiles )   $
df = pd . DataFrame ( file . generateRecords ( ) )   $ df . head ( )
X_train_all = pd . concat ( [ X_train_df , X_train . drop ( _STR_ , axis = 1 ) ] , axis = 1 )   $ X_test_all = pd . concat ( [ X_test_df , X_test . drop ( _STR_ , axis = 1 ) ] , axis = 1 )
def diff ( x ) :   $ return max ( x ) - min ( x )   $ ins_named [ _STR_ ] . groupby ( ins_named [ _STR_ ] ) . agg ( diff ) . to_frame ( ) . sort_values ( _STR_ , ascending = False )
old_page_converted = np . random . binomial ( n_old , p_old , 10000 ) / n_old   $ print ( old_page_converted )
sns . regplot ( x = np . arange ( - len ( my_tweet_df [ my_tweet_df [ _STR_ ] == _STR_ ] ) , 0 , 1 ) , y = my_tweet_df [ my_tweet_df [ _STR_ ] == _STR_ ] [ _STR_ ] , fit_reg = False , marker = _STR_ , scatter_kws = { _STR_ : _STR_ , _STR_ : 0.8 , _STR_ : 100 } )   $ ax = plt . gca ( )   $ ax . set_title ( _STR_ , fontsize = 12 )   $ plt . savefig ( _STR_ )
prepared [ [ _STR_ , _STR_ , _STR_ ] ] . describe ( )
data_2018 = data_2018 . reset_index ( )
print ( data . iloc [ 6 , 0 ] )
data = data . loc [ ( data . place_with_parent_names . str . contains ( _STR_ ) ) |   \   $ ( data . place_with_parent_names . str . contains ( _STR_ ) ) ]
data . values [ 0 ]
bwd . drop ( _STR_ , 1 , inplace = True )   $ bwd . reset_index ( inplace = True )
autos [ _STR_ ] . str [ : 10 ] . value_counts ( normalize = True , dropna = False ) . sort_index ( )
p_diffs = [ ]   $ for _ in range ( 10000 ) :   $ samp = df2 . sample ( df2 . shape [ 0 ] , replace = True )   $ p_diffs . append ( samp . query ( _STR_ ) . converted . mean ( ) - \   $ samp . query ( _STR_ ) . converted . mean ( ) )   $
pd_aux = pd . DataFrame ( building_pa_prc_shrink [ [ _STR_ , _STR_ ] ] [ filt_M ] )   $ pd_aux . sort_values ( by = [ _STR_ ] , inplace = True )   $ pd_aux . head ( 20 )
print ( len ( plan [ _STR_ ] [ _STR_ ] ) )   $ print ( plan [ _STR_ ] [ _STR_ ] [ 0 ] . keys ( ) )
for col in time_cols :   $ df [ col ] = df [ col ] . astype ( _STR_ )
station_count = session . query ( Stations . id ) . count ( )   $ print ( _STR_ )
df_new [ _STR_ ] = df_new [ _STR_ ] * df_new [ _STR_ ]   $ df_new [ _STR_ ] = df_new [ _STR_ ] * df_new [ _STR_ ]   $ log_mod = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ results = log_mod . fit ( )   $ results . summary ( )
archive_df . rating_numerator . value_counts ( sort = False )
df2 . query ( _STR_ ) . user_id . nunique ( )
df_combined . head ( 5 )
odds . tail ( 3 )
cutoff = 0.75   $ base_filename = _STR_
reg_logit = LogisticRegression ( random_state = 1984 , C = 0.01 )   $ reg_logit . fit ( x_train_advanced , y_train_advanced )
uber_15 [ _STR_ ] . value_counts ( )
case_mask = ( lower_vars . str . contains ( _STR_ )   $ & ( lower_vars . str . contains ( _STR_ ) | lower_vars . str . contains ( _STR_ ) )   $ & ~ lower_vars . str . contains ( _STR_ )   $ & ~ lower_vars . str . contains ( _STR_ ) )
issue_category_mapping = pd . read_csv ( _STR_ +   $ _STR_ +   $ _STR_ )   $ issue_category_mapping . head ( 5 ) # Same result as @TomAugspurger
hobbies_learned . findall ( _STR_ )
pn_qty [ pn ] [ _STR_ ] . append ( table_store . ix [ 0 ] )
n_neg = n_pos * 10   $ train_neg = train_sample . filter ( col ( _STR_ ) == 0 ) . orderBy ( func . rand ( seed = seed ) ) . limit ( n_neg ) . cache ( )   $ print ( _STR_ , n_neg )
! less _STR_
df_input_clean . filter ( _STR_ ) . count ( )
metrics . adjusted_rand_score ( labels , km . labels_ )
image_predictions . status_code
df . groupby ( _STR_ ) [ _STR_ ] . max ( )
nb = naive_bayes . GaussianNB ( )   $ cv_score = cross_val_score ( nb , X_train , y_train , cv = 10 )
df2 [ _STR_ ] = 1   $ df2 [ _STR_ ] = pd . get_dummies ( df2 [ _STR_ ] ) [ _STR_ ]   $ df2 [ _STR_ ] . mean ( )
df_state_votes . hill_trump_diff . hist ( bins = np . arange ( - 100 , 105 , 5 ) )
srctake = sourcetake . stack ( ) . to_frame ( )   $ srctake . rename ( columns = { _STR_ : _STR_ } , inplace = True )   $ srctake . reset_index ( inplace = True )   $ srctake [ _STR_ ] = pd . to_numeric ( srctake [ _STR_ ] , errors = _STR_ )   $ srctake = srctake [ ( srctake [ _STR_ ] <= datetime . today ( ) . year ) & ( srctake [ _STR_ ] >= 1000 ) ]
intersections_final_for_update_no_dupe . info ( )
( autos [ _STR_ ] . str [ : 10 ]   $ . value_counts ( normalize = True )   $ . sort_index ( ascending = True )   $ )
titanic . count ( )
menu_dishes_about_vect = menu_dishes_about_vectorizer . fit_transform ( menu_dishes_to_analyze [ _STR_ ] )   $ menu_dishes_about_feature_names = menu_dishes_about_vectorizer . get_feature_names ( )   $ menu_dishes_about_latent_features = run_sklearn_nmf ( menu_dishes_about_vect , menu_dishes_about_feature_names , 10 )
with open ( os . path . join ( _STR_ , csvfile ) , _STR_ ) as f :   $ data2 = csv . DictReader ( f )   $ for row in data2 :   $ print ( row )
pax_raw = pax_raw . merge ( n_user_days , on = _STR_ , how = _STR_ )
authors_count . sort_values ( _STR_ , ascending = False ) . head ( )
unigram_feats = sentim_analyzer . unigram_word_feats ( allNeg , min_freq = 4 )   $ len ( unigram_feats )   $ sentim_analyzer . add_feat_extractor ( extract_unigram_feats , unigrams = unigram_feats )
f_lr_hash_modeling2 . cache ( )
twitter_ar = twitter_ar [ twitter_ar . rating_num <= 17.0 ]
group = class_merged . groupby ( [ _STR_ ] , as_index = False )   $ daily_sales = pd . DataFrame ( group [ _STR_ ] . agg ( _STR_ ) )   $ pd . DataFrame . head ( daily_sales )
print _STR_ , db . command ( _STR_ ) [ _STR_ ] / 1024 , _STR_   $ print
kd915 = pd . read_csv ( _STR_ )
autos [ _STR_ ] . value_counts ( ) . sort_index ( ascending = False ) . head ( 10 )
url = form_url ( _STR_ , orderBy = _STR_ )   $ response = requests . get ( url , headers = headers )   $ print_enumeration ( response . json ( ) [ _STR_ ] )
measure . dtypes
np . sum ( df [ _STR_ ] . isnull ( ) )
data1_new = data1_new . reset_index ( drop = True )   $ data1_new . head ( )
StockData . describe ( )
from sklearn . ensemble import RandomForestRegressor   $ start_fit = time . time ( )   $ reg1 = RandomForestRegressor ( random_state = rs , n_estimators = 200 , n_jobs = - 1 )   $ reg1 . fit ( X_train , y_train . ravel ( ) ) ;   $ end_fit = time . time ( )
pd . Series ( topic_preds ) . value_counts ( normalize = True ) . plot . barh ( )   $ plt . title ( _STR_ , fontsize = 20 )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )   $ plt . show ( )
y_predicted = fit3 . predict ( X3 )   $ plt . plot ( y3 , y_predicted , _STR_ )   $ plt . title ( _STR_ )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )
df = df . selectExpr ( _STR_ , _STR_ , _STR_ , _STR_ , _STR_ )   $ df . show ( )
sns . swarmplot ( x = _STR_ , y = _STR_ , data = df5 )   $ plt . title ( _STR_ )   $ plt . ylim ( - 0.05 , 0.2 )   $ plt . show ( )
totalConvs_month = pd . DataFrame ( df_convs_master . groupby ( [ _STR_ , _STR_ ] , as_index = False ) . sum ( ) )   $ totalConvs_month . drop ( _STR_ , axis = 1 , inplace = True )   $ print _STR_ , _STR_ , _STR_ , totalConvs_month . shape   $ totalConvs_month
df_train . index = pd . to_datetime ( df_train [ _STR_ ] )   $ df_train . drop ( _STR_ , axis = 1 , inplace = True )   $ df_test . index = pd . to_datetime ( df_test [ _STR_ ] )   $ df_test . drop ( _STR_ , axis = 1 , inplace = True )
df [ df [ _STR_ ] == _STR_ ] [ _STR_ ] . value_counts ( )
def collect_tweet_text ( tizibika ) :   $ tweets = [ ]   $ for tweet in tizibika [ _STR_ ] :   $ tweets . append ( tweet )   $ return tweets   $
data = data . reset_index ( drop = True )
twitter_ar . shape
a_df . size   $ b_df . size
affair_children = pd . crosstab ( data . children , data . affair . astype ( bool ) )   $ affair_children . div ( affair_children . sum ( 1 ) . astype ( float ) , axis = 0 ) . plot ( kind = _STR_ , stacked = True )   $ plt . title ( _STR_ )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )
df2 [ df2 [ _STR_ ] == _STR_ ] . head ( )
df . drop ( df . query ( _STR_ ) . index , axis = 0 , inplace = True )   $
cleaned2 [ _STR_ ] . head ( 10 )
zn_array = np . loadtxt ( os . path . join ( _STR_ , _STR_ ) )   $ plt . imshow ( zn_array )
original_sports = pd . Series ( { _STR_ : _STR_ ,   $ _STR_ : _STR_ ,   $ _STR_ : _STR_ ,   $ _STR_ : _STR_ } )   $ original_sports
len ( list ( set ( have_seen_two_versions ) & set ( df_click . user_session . unique ( ) ) ) )
f_counts_week_channel = spark . read . csv ( os . path . join ( mungepath , _STR_ ) , header = True )   $ print ( _STR_ % f_counts_week_channel . count ( ) )
year_prcp_df = pd . read_sql_query ( year_prcp . statement , engine , index_col = _STR_ )   $ year_prcp_df . head ( )
csvDF . columns = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]
predict . predict_score ( _STR_ )
df_old = df2 . query ( _STR_ )   $ n_old = df_old . shape [ 0 ]   $ n_old
contribs . iloc [ imin ]
df_merge . columns
tweets = [ ]   $ for page in tweepy . Cursor ( api . user_timeline , screen_name = _STR_ ) . pages ( ) :   $ tweets . append ( page )
df [ df . tweet_id . isin ( [ 1299 , 1298 , 1300 , 1301 ] ) ] . sort_values ( by = _STR_ ) [ [ _STR_ , _STR_ , _STR_ ] ]
companyNeg . columns
state_lookup . info ( )
daily = data . resample ( _STR_ ) . sum ( )   $ daily . rolling ( 30 , center = True ) . sum ( ) . plot ( style = [ _STR_ , _STR_ , _STR_ ] )   $ plt . ylabel ( _STR_ )
model . summary ( )
jobs . loc [ ( jobs . MAXCPUS == 600 ) & ( jobs . GPU == 0 ) ] . groupby ( _STR_ ) . JobID . count ( ) . sort_values ( ascending = False )
transactions [ ~ transactions [ _STR_ ] . isin ( dfTemp [ _STR_ ] . values ) ]
import matplotlib . pyplot as plt   $ import numpy as np   $ plt . plot ( np . random . rand ( 50 ) . cumsum ( ) )
from sklearn . feature_selection import RFE   $ rfe = RFE ( classifier , 7 )   $ rfe = rfe . fit ( data , affair )   $ print ( rfe . support_ )   $ print ( rfe . ranking_ )   $
schumer . head ( )
ax1 = sns . boxplot ( x = tweets_clean [ _STR_ ] )   $ ax1 . set ( ylabel = _STR_ )
plt . hist ( commits . log_commits , bins = 50 )   $ plt . xlim ( ( 1 , 8 ) )   $ plt . title ( _STR_ )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )   $
active . info ( )
num_plots = plot . regression_contour_sgd ( x , y , diagrams = _STR_ )
csv_paths = sorted ( [ os . path . join ( data_dir , fname ) for fname in os . listdir ( data_dir )   $ if fname . endswith ( _STR_ ) ] )
df1 = pd . DataFrame ( list ( range ( 20 ) ) )
df . dtypes . head ( 20 )
new_page_conv = np . random . binomial ( n_new , p_new )   $
states . index
geometry = [ Point ( xy ) for xy in zip ( g [ _STR_ ] , g [ _STR_ ] ) ]   $ crs = { _STR_ : _STR_ }   $ g_geo = gpd . GeoDataFrame ( g , crs = crs , geometry = geometry )
tt_final = ttarc_clean . merge ( tt_json_clean , left_on = _STR_ , right_on = _STR_ , how = _STR_ )   $ tt_final = tt_final . merge ( imgp_clean , left_on = _STR_ , right_on = _STR_ , how = _STR_ )   $ tt_final . timestamp = pd . to_datetime ( tt_final . timestamp , infer_datetime_format = True )   $ tt_final . info ( )
eval_df = pd . DataFrame ( predictions , index = model_dates ) . T
sns . lmplot ( x = _STR_ , y = _STR_ , data = twitter_moves , lowess = True , size = 8 , aspect = 1.5 )
dataset = pd . read_csv ( _STR_ , encoding = _STR_ ) . fillna ( 0 )   $ dataset . head ( )
[ np . exp ( 0.0507 ) , np . exp ( 0.0408 ) ]
journalists_mention_summary_df = journalist_mention_summary ( journalists_mention_df )   $ journalists_mention_summary_df . to_csv ( _STR_ )   $ journalists_mention_summary_df [ journalist_mention_summary_fields ] . head ( 25 )
tweet_archive_clean = tweet_archive_clean [ tweet_archive_clean [ _STR_ ] . isnull ( ) ]
df2 . date . value_counts ( ) . sort_index ( axis = 0 )
df2 . head ( )
df . sort_values ( [ _STR_ ] , ascending = True ) . head ( 5 )
train . groupby ( [ _STR_ ] ) [ _STR_ ] . mean ( ) . reset_index ( )
speakers . whylisten = speakers . whylisten . apply ( lambda x : re . sub ( _STR_ , _STR_ , x ) )
unsorted_df . sort_values ( by = _STR_ , ascending = False )
edges = list ( zip ( edgereader . source , edgereader . target ) ) #create tuple for every pair from the dataset
list . sort ( )   $ print ( list )
( data [ _STR_ ] > 6 ) . sum ( )
click_condition_meta [ _STR_ ] = np . where ( click_condition_meta . user_id == _STR_ , _STR_ , click_condition_meta . dvce_type )
twitter_archive_df [ _STR_ ] . value_counts ( ascending = True ) [ : 10 ]
df_unique_users_treatment = df2 [ df2 [ _STR_ ] == _STR_ ]   $ n_unique_users_treatment = len ( df_unique_users_treatment )   $ n_conversion_treatment = len ( df_unique_users_treatment [ df_unique_users_treatment [ _STR_ ] == 1 ] )   $ probability_treatment = n_conversion_treatment / n_unique_users_treatment   $ print ( _STR_ . format ( probability_treatment ) )
datAll [ _STR_ ] . value_counts ( dropna = False )
def unix_to_datetime ( i ) :   $ u = datetime . datetime . fromtimestamp (   $ int ( i )   $ ) . strftime ( _STR_ )   $ return u
print ( a )   $ a . reverse ( )     $ print ( a )
regression_models = ws . models ( tag = TICKER )   $ for m in regression_models :   $ print ( _STR_ , m . name , _STR_ , m . version , _STR_ , m . description , m . tags )
df_archive_clean . drop ( [ _STR_ , _STR_ ] , axis = 1 , inplace = True )
groceries = pd . Series ( data = [ 30 , 6 , _STR_ , _STR_ ] , index = [ _STR_ , _STR_ , _STR_ , _STR_ ] )   $ print ( groceries )   $ print ( groceries * 2 )   $
new_texas_city . tail ( 10 )
round ( final_topbikes [ _STR_ ] . sum ( ) )
n = 13   $ df [ _STR_ ] . pct_change ( n ) . mean ( )
from microsoftml_scikit . utils . exports import dot_export_pipeline   $ dot_vis = dot_export_pipeline ( pipeline , ds_train )   $ print ( dot_vis )
dfSavours = df [ df [ _STR_ ] . apply ( lambda x : x . split ( _STR_ ) [ 0 ] ) == _STR_ ]
fb = pd . read_json ( json . dumps ( res_json [ _STR_ ] [ _STR_ ] ) )
print ( today . strftime ( _STR_ ) )
cp311 [ [ _STR_ ] ] . groupby ( [ cp311 . borough ] ) . count ( )
np . exp ( results_1 . params )
horror_readings . head ( )
a = re . match ( _STR_ , _STR_ )
def remove_data_outside_window ( df ) :   $ window_start = datetime . datetime ( 2018 , 4 , 1 , 23 , 0 )   $ window_end = datetime . datetime ( 2018 , 4 , 2 , 0 , 0 )   $ return df [ ( df . client_event_time >= window_start ) & ( df . client_event_time < window_end ) ]
gender . value_counts ( )
tt_json . describe ( )
print ( XGBClassifier . feature_importances_ )
df . rename ( columns = { _STR_ : _STR_ } , inplace = True )
puppos = df1_clean [ df1_clean . type == _STR_ ] . count ( ) [ 0 ]   $ puppers = df1_clean [ df1_clean . type == _STR_ ] . count ( ) [ 0 ]   $ doggos = df1_clean [ df1_clean . type == _STR_ ] . count ( ) [ 0 ]   $ floofers = df1_clean [ df1_clean . type == _STR_ ] . count ( ) [ 0 ]   $ puppos , puppers , doggos , floofers
s = pd . Series ( [ 1 , 2.3 , np . nan , _STR_ ] )   $ s
from google . colab import files   $ uploaded = files . upload ( )
clean_train_df = pd . DataFrame ( parse_data ( train_df [ _STR_ ] . tolist ( ) ) )
pd . read_html ( driver . page_source ) [ 0 ]
ca_cities_list = sorted ( df_h1b [ df_h1b . lca_case_employer_state == _STR_ ] . lca_case_workloc1_city . unique ( ) )
rdd_from_df = sqlContext . sql ( _STR_ )
model . doesnt_match ( _STR_ . split ( ) )   $
data [ _STR_ ] = pd . rolling_mean ( data [ _STR_ ] , 500 )
import os   $ % matplotlib inline   $ import pandas as pd   $ import seaborn as sns   $ import matplotlib . pyplot as plt
% % time   $ max_key = max ( r_dict . keys ( ) , key = get_daily_chg )   $ print ( _STR_ + str ( get_daily_chg ( max_key ) ) )
pd . date_range ( start , periods = 5 , freq = _STR_ )
df . drop ( columns = [ _STR_ , _STR_ ] ) # drop multiple columns
infinity . head ( 10 )
def calc_temps ( start_date , end_date ) :   $ return session . query ( func . min ( Measurement . tobs ) , func . avg ( Measurement . tobs ) , func . max ( Measurement . tobs ) ) . \   $ filter ( Measurement . date >= start_date ) . filter ( Measurement . date <= end_date ) . all ( )   $ print ( calc_temps ( _STR_ , _STR_ ) )
s1 = pd . Series ( [ True , False , False , True ] )   $ s1
threeoneone_geo = threeoneone_geo . to_crs ( { _STR_ : _STR_ } )   $ census_withdata = census_withdata . to_crs ( { _STR_ : _STR_ } )
morning_rush = last_year [ ( last_year [ _STR_ ] . dt . weekday < 5 ) & ( last_year [ _STR_ ] . dt . hour > 5 ) & ( last_year [ _STR_ ] . dt . hour < 10 ) ]   $ print ( morning_rush . shape )   $ last_year . shape
n_user_days . hist ( )   $ plt . axvline ( x = 4 , c = _STR_ , linestyle = _STR_ )
Test . SetFlowValues ( 6.5 , 9.5 , 0.45 )
lookup_sentiment ( _STR_ )
r = requests . get ( _STR_ )   $
df2 = df1 . join ( df1_stdev , how = _STR_ )
df . groupby ( _STR_ ) [ [ _STR_ , _STR_ ] ] . agg ( np . sum )
df . plot ( ) ;
metadata [ _STR_ ] = refl [ _STR_ ] [ _STR_ ] [ _STR_ ] . value   $ metadata
price_by_brand = { }   $ for b in brands :   $ price_by_brand [ b ] = autos . loc [ autos [ _STR_ ] == b , _STR_ ] . mean ( )   $ price_by_brand
temp_df = weather_mean [ [ _STR_ ] ]   $ temp_df . head ( )
print ( _STR_ , env . action_space )   $ print ( _STR_ )   $ print ( np . array ( [ env . action_space . sample ( ) for i in range ( 10 ) ] ) )
df = df . dropna ( subset = [ _STR_ ] )
from sklearn . preprocessing import StandardScaler   $ import seaborn as sns   $ import matplotlib . pyplot as plt   $ % matplotlib inline   $ import re
inCSV = dataDir + _STR_   $ df = pd . read_csv ( inCSV )   $ df = df [ [ _STR_ , _STR_ , _STR_ ] ]   $ df . columns = [ _STR_ , _STR_ , _STR_ ] #just delcare col names here $ df.to_csv(dataDir+'processing/new-york_new-york_points.csv', index=False)
features = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ X = train_df [ features ] . iloc [ : , 0 : ] . values
sorted_precip = index_date_df . sort_values ( by = [ _STR_ ] )   $ sorted_precip . head ( )
from datetime import datetime   $ pd . Timestamp ( datetime ( 2015 , 11 , 11 ) )
ridge3 = linear_model . Ridge ( alpha = 1 )   $ ridge3 . fit ( x3 , y )   $ ( ridge3 . coef_ , ridge3 . intercept_ )
transfer_duplicates . apply ( lambda row : smoother_function_part2 ( row [ _STR_ ] , row [ _STR_ ] , row [ _STR_ ] , row [ _STR_ ] ) , axis = 1 ) ;
df2 [ df2 [ _STR_ ] == 1 ] . shape [ 0 ] / df2 . shape [ 0 ]
other_text_feature = vect1 . fit_transform ( talks [ _STR_ ] . as_matrix ( ) )
top_supporters = support . groupby ( [ _STR_ , _STR_ ]   $ ) . amount . sum ( ) . reset_index ( ) . sort_values ( _STR_ , ascending = False ) . head ( 10 )
import random   $ sample = all_data_merge . sample ( n = 1000 , replace = True )   $ sample . to_csv ( _STR_ , index = False , sep = _STR_ , encoding = _STR_ )
from pyspark . sql . types import *
weather . info ( )
run txt2pdf . py - o _STR_ _STR_
test1 . head ( 10 )
normal , malicious = plots . top_n_ports ( 10 )   $ print ( normal )   $ print ( malicious )
f1_A = lv_workspace . get_data_filter_object ( step = 1 , subset = _STR_ )   $ f1_A . include_list_filter
def count_non_null ( df , fld ) :   $ has_field = np . logical_not ( pd . isnull ( df [ fld ] ) )   $ return has_field . sum ( ) , df . shape [ 0 ]
my_df [ _STR_ ] = df_user . groupby ( _STR_ ) [ _STR_ ] . nunique ( )   $ my_df [ _STR_ ] = df_user . groupby ( _STR_ ) [ _STR_ ] . nunique ( )
data . drop ( [ _STR_ , _STR_ ] )
data = pd . read_csv ( _STR_ , index_col = 0 )
all_cards . loc [ _STR_ ]
new_page_converted . mean ( ) - old_page_converted . mean ( )
df2 [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] . mean ( )
dfX = df2 . query ( _STR_ )   $ treatment_convert = dfX . converted . sum ( ) / dfX . count ( ) [ 0 ]   $ treatment_convert
dfg = dfg . set_index ( [ _STR_ , _STR_ ] )   $
def print_groups ( groupobject ) :   $ for name , group in groupobject :   $ print ( name )   $ print ( group . head ( ) )
tw . head ( )
df_clean . info ( )
print ( twitter_data_v2 . shape )   $ print ( tweet_data_v2 . shape )   $ print ( Imagenes_data_v2 . shape )   $
df . query ( _STR_ ) . count ( ) [ 0 ] + df . query ( _STR_ ) . count ( ) [ 0 ]   $
data_get [ _STR_ ] . Close . plot ( )
average_range = df [ _STR_ ] . mean ( )
hours [ _STR_ ] = linreg . predict ( X )   $ plt . scatter ( hours . hour , hours . start )   $ plt . plot ( hours . hour , hours . pred , color = _STR_ )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )   $
station_total = station_by_week . reset_index ( ) . groupby ( [ _STR_ ] ) [ _STR_ ] . sum ( ) . reset_index ( )   $ station_total = station_total . sort ( _STR_ , ascending = [ 0 ] )   $ print _STR_   $ station_total . head ( 10 ) . set_index ( _STR_ )
df [ _STR_ ] . value_counts ( ) . to_frame ( ) . head ( 20 ) . index #select top 20 country names
df . convert_objects ( convert_dates = _STR_ , convert_numeric = True )
psy_df3 = QLESQ . merge ( psy_df2 , on = _STR_ , how = _STR_ ) # I want to keep all Ss from psy_df
df2 . rename ( columns = { _STR_ : _STR_ } , inplace = True )   $ df2 . drop ( _STR_ , axis = 1 , inplace = True )   $ df2 . head ( )
from sklearn . ensemble import RandomForestClassifier   $ clf = RandomForestClassifier ( max_depth = 2 , random_state = 0 )   $ clf . fit ( X_tr [ 0 : len ( X_train ) - 40 - 1 ] , y_ls )   $ print ( clf . feature_importances_ )
df . head ( ) . to_json ( _STR_ )   $   ! cat . . / . . / data / stocks . json
newfile . insert ( 0 , _STR_ , _STR_ )   $ newfile . insert ( 1 , _STR_ , _STR_ )
y_pred_submit = rfr . predict ( x_test )   $ np . savetxt ( _STR_ , y_pred_submit , _STR_ , delimiter = _STR_ )
ttt = np . argmax ( out_df [ [ _STR_ , _STR_ , _STR_ ] ] . as_matrix ( ) , axis = 1 )
rf = RandomForestClassifier ( )   $ rf . fit ( X_train , y_train )   $ rf . score ( X_test , y_test )
df2_treatment = df2 . query ( _STR_ )   $ p_treatment = df2_treatment [ _STR_ ] . sum ( ) / len ( df2_treatment )   $ p_treatment
educ_freqs = [ 0.112 , 0.291 , 0.1889 , 0.096 , 0.202 , 0.111 ]
cur . execute ( _STR_ )   $ cur . execute ( _STR_ )   $ for r in cur . fetchall ( ) :   $ print ( r )
inflation = pd . read_csv ( _STR_ )   $ print inflation
austin [ austin [ _STR_ ] . isnull ( ) ] . index . tolist ( )
np . log ( bacteria )
requests . get ( _STR_   $ ) . json ( )
def median_of_column ( path , col_name ) :   $ with TablesBigMatrixReader ( path ) as reader :   $ col_indices = reader . get_col_indices_by_name ( [ col_name ] )   $ return np . median ( reader [ : , col_indices ] )
train = hn [ hn . created_at < july_1 ] . copy ( )   $ new = hn [ hn . created_at >= july_1 ] . copy ( )
results . coordinates
fig , axs = plot_partial_dependence ( clf , X_test [ X_test . age_well_years != 274 ] , [ 7 , 8 ] , feature_names = X_test . columns , grid_resolution = 70 , n_cols = 7 )   $ fig , axs = plot_partial_dependence ( clf , X_test [ X_test . age_well_years != 274 ] , [ 10 ] , feature_names = X_test . columns , grid_resolution = 70 , n_cols = 7 )   $
from IPython . display import HTML   $
urlb = _STR_   $ urls = _STR_   $ page = urlopen ( urlb + urls )   $ soup = BeautifulSoup ( page , _STR_ )
a_list . remove ( 1 )   $ a_list
rain_period = sf_small_grouped [ sf_small_grouped [ _STR_ ] == 1 ] . start_date
plt . show ( )
df . head ( )
print ( lr_gd . best_params_ )   $ print ( lr_gd . best_score_ )
pd . set_option ( _STR_ , lambda x : _STR_ % x )   $ autos [ _STR_ ] . describe ( )
mr = dd . from_pandas ( rentals , npartitions = 3 )
df2 . shape
import logging   $ logging . basicConfig ( format = _STR_ , level = logging . WARNING )
pysqldf ( _STR_ )
def get_gender ( name ) :   $ urlprefix = _STR_ # Define url prefix $     r = requests.get(urlprefix + name.lower()) $     return r.json()['gender']
dates_list = list ( dates_list )   $ dateset = pd . DataFrame ( { _STR_ : dates_list , _STR_ : _STR_ } )   $ dateset = dateset . sort_values ( ascending = True , by = _STR_ )   $ dateset . head ( )
df . airline . unique ( )
my_gempro . set_representative_structure ( )   $ my_gempro . df_representative_structures . head ( )
dataA = pd . read_csv ( _STR_ , header = None )
np . array ( p_diffs )   $ plt . hist ( p_diffs ) ;
gpCreditCard . Hour_of_day . describe ( )
kochdf . head ( 10 )
ab_CA = 1 / np . exp ( - 0.0469 )   $ ab_UK = 1 / np . exp ( .0314 )   $ print ( ab_CA )   $ print ( ab_UK )
movie_df . head ( )
actual_diff = actual_pnew - actual_pold   $ ( p_diffs > actual_diff ) . mean ( )
df_country_join = df_country . set_index ( _STR_ ) . join ( df2 . set_index ( _STR_ ) , how = _STR_ )   $ print ( df_country_join . head ( ) )
full . groupby ( [ _STR_ ] ) [ _STR_ ] . agg ( { _STR_ : _STR_ , _STR_ : _STR_ , _STR_ : _STR_ } ) . sort_values ( by = _STR_ , ascending = False )   $
old_prob = gb . count ( ) . values / sum ( gb . count ( ) . values )
from sklearn import tree   $ model = tree . DecisionTreeClassifier ( )   $ print ( _STR_ )   $ reg_analysis ( model , X_train , X_test , y_train , y_test )
print ( _STR_ , data . created_at . max ( ) )   $ print ( _STR_ , data . created_at . min ( ) )
countries = pd . read_csv ( _STR_ )   $ countries . head ( )
U M M
lasso2 = Lasso ( alpha = 0.0002 )   $ lasso2 . fit ( train_data , train_labels )
all_indicators . ix [ : , 0 : 1 ]
df . shape
df . info ( )
features_df . shape
search [ _STR_ ] = search . apply ( trip_start_weekday_m , axis = 1 )
tw_clean = tw_clean . drop ( tw_clean [ tw_clean . text . str . contains ( _STR_ ) ] . index )
df_l = df_vu . append ( df_cb ) #creating a new dataset
open_users [ _STR_ ] = open_users [ _STR_ ] . map ( lambda name : name in approved_users )
import requests   $ election_results = _STR_   $ response = requests . get ( election_results )
total_payments = np . sum ( sortdf [ _STR_ ] ) / 1000000   $ percent_of_total = lambda x : round ( x * 100 / total_payments , 1 )   $ percent_of_total_list = list ( map ( percent_of_total , my_list ) )   $ percent_of_total_list [ : 10 ]
row_len = df . query ( _STR_ ) . shape [ 0 ]   $ print ( _STR_ , row_len )
treat_convert = df2 . query ( _STR_ ) [ _STR_ ] . mean ( )   $ treat_convert
n_old = df2 [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] . count ( )   $ n_old
autos [ _STR_ ] . str [ : 10 ] . value_counts ( normalize = True , dropna = False ) . sort_index ( ) [ : 40 ] . plot ( kind = _STR_ , title = _STR_ , colormap = _STR_ )
mean_sea_level . plot ( kind = _STR_ , figsize = ( 12 , 8 ) )
attend_with . columns = [ _STR_ + str ( col ) for col in attend_with . columns ]
from sklearn . feature_extraction . text import TfidfVectorizer   $ tfidf = TfidfVectorizer ( ngram_range = ( 2 , 3 ) , stop_words = _STR_ , max_features = 2000 )   $ tfmod = tfidf . fit_transform ( aldf [ _STR_ ] )   $ tfidfdummiee = pd . DataFrame ( tfmod . todense ( ) , columns = tfidf . get_feature_names ( ) )   $ tfidfdummiee . head ( )
df_protest . duration . max ( )
print ( _STR_ . format ( np . mean ( data3 [ _STR_ ] ) ) )
items = pd . read_sql_query ( _STR_ , con = engine )
baseball . sum ( )
from ipywidgets import interact , interactive , fixed , interact_manual   $ import ipywidgets as widgets   $ from IPython . display import display   $ import numpy as np   $ from scipy . optimize import curve_fit as cf
df_mes = df_mes [ df_mes [ _STR_ ] >= 0 ]   $ df_mes . shape [ 0 ]
plt . pie ( slices , labels = activities , colors = [ _STR_ , _STR_ , _STR_ ] , shadow = True , explode = ( 0 , 0.1 , 0 ) , startangle = 90 )   $ plt . show ( )
logit2 = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ result1 = logit2 . fit ( )   $ result1 . summary ( )
oppstagepct . loc [ 0 ] . plot . box ( by = _STR_ )
X_train = X_train . to_frame ( )   $ X_test = X_test . to_frame ( )
new_user_project . to_csv ( _STR_ , index = False )
wikipedia_meritocracy = _STR_   $ meritocracy_save = _STR_
coefs = pd . DataFrame ( logreg . coef_ [ 0 ] , index = X . columns , columns = [ _STR_ ] )   $ coefs [ _STR_ ] = np . exp ( coefs [ _STR_ ] )   $ coefs . sort_values ( by = _STR_ , ascending = False , inplace = True )   $ coefs . head ( 10 )
df_protest . columns . tolist ( )
df . rename ( columns = { _STR_ : _STR_ } , inplace = False ) # Since the change is only temporary, it is not getting reflected in the df output $ df.head(2)
df_transactions [ _STR_ ] = df_transactions [ _STR_ ] . clip_lower ( 0 )
for row in soup . select ( _STR_ ) :   $ print ( row [ _STR_ ] )
def RMSLE ( y , pred ) :   $ return metrics . mean_squared_error ( y , pred ) ** 0.5
y = new [ _STR_ ]   $ x = new . drop ( _STR_ , axis = 1 )
au . find_some_docs ( ao18_qual_coll , limit = 3 )
alg2 = RandomForestClassifier ( )   $ alg2 . fit ( X_train , y_train )   $ probs = alg2 . predict_proba ( X_test )   $ score = log_loss ( y_test , probs )   $ print ( score )
properati [ properati [ _STR_ ] == _STR_ ] [ [ _STR_ , _STR_ ] ]
df . loc [ d [ 5 ] , _STR_ ]
data = pd . read_csv ( _STR_ , index_col = 0 )   $ data . head ( )
autos [ _STR_ ] . describe ( )
autos = autos [ autos [ _STR_ ] . between ( 1970 , 2016 ) ]   $ autos [ _STR_ ] . describe ( )
df = df1 . append ( df2 , ignore_index = True )   $ df
other_text_feature . shape
datetime ( 2005 , 12 , 31 , 10 , 0 , 0 ) < p . end_time # WAT?!
d . visualize ( )
InfinityWars_Predictions = infinity . join ( InfinityWars_PRED_df ) . encode ( _STR_ ) . strip ( )   $ InfinityWars_Predictions . columns = [ _STR_ , _STR_ , _STR_ ]   $ InfinityWars_Predictions = InfinityWars_Predictions . drop ( [ _STR_ ] , axis = 1 )
df2 [ _STR_ ] . mean ( )
plt . scatter ( aqi [ _STR_ ] , aqi [ _STR_ ] , alpha = 0.2 )   $ plt . xlabel ( _STR_ ) ; plt . ylabel ( _STR_ ) ;
access_logs_df = access_logs_df . cache ( )
iowa . head ( )
df_countries . isnull ( ) . sum ( )
df2 . shape [ 0 ]
price_counts = autos [ _STR_ ] . value_counts ( )   $ print ( price_counts . sort_index ( ascending = True ) )
merged . sort_values ( _STR_ , ascending = False )
df_new [ _STR_ ] = u . answers . count
sub1 . shape
res = dfs . join ( dfpf , dfpf . rf == dfs . rf ) . select ( dfs . rf , dfpf . qty , dfs . date , dfs . neutral , dfs . scenarios )
fig = ax . get_figure ( )   $ fig . savefig ( _STR_ )
exp_budget_vote = sorted_budget_biggest . groupby ( [ _STR_ ] ) [ _STR_ ] . mean ( )
daily_averages = pivoted . resample ( _STR_ ) . mean ( )
import nltk . corpus   $ import nltk . stem . porter   $ import re
jn = urllib . request . urlopen ( _STR_ ) . read ( )   $ df = pd . read_json ( jn )
autos . brand . value_counts ( normalize = True )
ved = pd . read_excel ( _STR_ , sheet_name = _STR_ , usecols = _STR_ , header = 12 , skipfooter = 4 )
a = np . arange ( 0.5 , 10.5 , 0.5 )   $ a
np . exp ( - 0.0140 )
segments . seg_length . apply ( np . log ) . hist ( bins = 100 )
es . indices . refresh ( index = _STR_ )
df_links = df_links [ df_links [ _STR_ ] != _STR_ ]
% % R   $ flightsDB $ CARRIER_CODE < - as . numeric ( as . factor ( flightsDB $ UNIQUE_CARRIER ) )
avg_per_seat_price_inoroffseason_teams [ _STR_ , _STR_ ] # This is the average across BAL's off-season prices.
df2 [ _STR_ ] = 1   $ df2 [ [ _STR_ , _STR_ ] ] = pd . get_dummies ( df2 [ _STR_ ] ) # create dummy variable columns for 'group' $ df2['ab_page'] = df2['treatment']
145311.000000 / df2 . shape [ 0 ]   $
autos [ _STR_ ] . value_counts ( )
occurrences = np . asarray ( vectorized_text_labeled . sum ( axis = 0 ) ) . ravel ( )   $ terms = ( occurrences )   $ counts_df = pd . DataFrame ( { _STR_ : terms , _STR_ : occurrences } ) . sort_values ( _STR_ , ascending = False )   $ counts_df
tweet_archive_clean [ _STR_ ] = tweet_archive_clean [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] . apply ( lambda x : _STR_ . join ( x ) , axis = 1 )
flight . printSchema ( )   $ flight . show ( 2 , truncate = False )   $ display ( flight . count ( ) )
import test_package . print_hello_function_container   $ import test_package . print_hello_class_container   $ import test_package . print_hello_direct # note that  the paths should include root (i.e., package name) $
df . zip . replace ( 56201 , 52601 , inplace = True )
import statsmodels . api as sm   $ convert_old = sum ( df2 [ df2 . group == _STR_ ] . converted )   $ convert_new = sum ( df2 [ df2 . group == _STR_ ] . converted )   $ n_old = df2 [ df2 . group == _STR_ ] . user_id . count ( )   $ n_new = df2 [ df2 . group == _STR_ ] . user_id . count ( )
! head samples . csv
sess . get_data ( _STR_ , _STR_ , index = _STR_ )
S . modeloutput_obj . add_variable ( _STR_ )
columns = inspector . get_columns ( _STR_ )   $ for c in columns :   $ print ( c [ _STR_ ] , c [ _STR_ ] )
forecast_df [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] = 0   $ for ind , row in SANDAG_age_df [ SANDAG_age_df [ _STR_ ] == _STR_ ] . iterrows ( ) :   $ forecast_df . loc [ ind , _STR_ ] = row [ _STR_ ]   $ forecast_df . head ( )
ben_final [ ben_final [ _STR_ ] . str . contains ( _STR_ ) ]
! ls | grep _STR_ # this is the file
d311_gb . head ( )
column_datasets = { _STR_ : ColumnarDataset . from_data_frame ( trn_df , cat_vars , trn_y ) ,   $ _STR_ : ColumnarDataset . from_data_frame ( val_df , cat_vars , val_y ) }
print ( _STR_ + str ( max ( ( data [ _STR_ ] - data [ _STR_ ] ) . abs ( ) ) ) )
desc_stats . to_excel ( _STR_ )
print ( _STR_ )   $ print ( _STR_ )
df . query ( _STR_ ) [ _STR_ ] . count ( ) / df . shape [ 0 ]
ab_file2 [ _STR_ ] = 1 #creating an intercept column with all values 1 $ ab_file2[['ab_page']] = pd.get_dummies(ab_file2['group'])[['treatment']] # a dummy variable column
sns . barplot ( x = _STR_ , y = _STR_ , data = words_df . head ( 15 ) )
new_page_converted = np . random . choice ( [ 1 , 0 ] , size = n_new , p = [ p_new , ( 1 - p_new ) ] )   $ new_page_converted
df3 [ _STR_ ] . plot ( kind = _STR_ , bins = 12 )   $ plt . show ( )
trigram_bow_filepath = paths . trigram_bow_filepath
liberia_data3 [ _STR_ ] . apply ( pd . to_datetime )
z_score , p_value = sm . stats . proportions_ztest ( [ convert_old , convert_new ] , [ n_old , n_new ] , alternative = _STR_ )
newdf . reset_index ( inplace = True )   $ newdf [ _STR_ ] = newdf [ _STR_ ] . apply ( lambda x : str ( x ) [ 0 : 7 ] )   $ newdf . set_index ( _STR_ , inplace = True )
kochdf = pd . merge ( kochdf , koch11df , how = _STR_ , left_on = [ _STR_ , _STR_ ] ,   $ right_on = [ _STR_ , _STR_ ] , suffixes = ( _STR_ , _STR_ ) )   $ kochdf . info ( )
pold = df2 [ _STR_ ] . mean ( )   $ pold
print ( type ( google_stock ) )   $ print ( google_stock . shape )
soup = BeautifulSoup ( response . text , _STR_ )
All_tweet_data_v2 . name [ ( All_tweet_data_v2 . name . str . len ( ) < 3 ) & ( All_tweet_data_v2 . name . str . contains ( _STR_ ) ) ]
fraud_data_updated . head ( )
test_pl = tweet_en [ tweet_en [ _STR_ ] . apply ( lambda x : _STR_ in x ) ]   $ test_pl = tweet_en [ tweet_en [ _STR_ ] != _STR_ ]
measure_val_2010_to_2013 . count ( )
tweet_archive_clean . info ( )
data = np . linspace ( 0 , 1 , 201 )   $ print ( data )
df2 . iloc [ 2862 ]
lm = sm . Logit ( df4 [ _STR_ ] , df4 [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ results = lm . fit ( )   $ results . summary ( )
df . drop ( df . query ( _STR_ ) . index , inplace = True )   $ df . drop ( df . query ( _STR_ ) . index , inplace = True )   $
own_star . to_pickle ( _STR_ )
model_w = sm . formula . ols ( _STR_ , data = df ) . fit ( )   $ anova_w_table = sm . stats . anova_lm ( model_w , typ = 1 )   $ anova_w_table . round ( 3 )
log_mod = LogisticRegression ( )   $ log_mod . fit ( x_train , y_train )   $ y_preds = log_mod . predict ( x_test )
import numpy as np   $ data = np . random . normal ( 0.0 , 1.0 , 1000000 )   $ np . testing . assert_almost_equal ( np . mean ( data ) , 0.0 , decimal = 2 )
dum = pd . get_dummies ( TrainData , sparse = True , drop_first = True , dummy_na = True ,   $ columns = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ ] )   $ dum . head ( )
SEA_analysis2 = SEA_analysis [ _STR_ ] . mean ( )
nold = df2 . query ( _STR_ ) . count ( ) [ 0 ]   $ nold
incidents_yes_no = [ ]   $ for i in range ( len ( incidents_time_index ) ) :   $ teststamp = incidents_time_index [ i ]   $ incidents_yes_no . append ( len ( incidents [ ( incidents [ _STR_ ] <= teststamp ) & ( incidents [ _STR_ ] >= teststamp ) ] ) )
pprint . pprint ( posts . find_one ( { _STR_ : _STR_ } ) )
google_stock [ _STR_ ] . describe ( )
df_train = pd . merge ( df_train , df_group_by2 , on = _STR_ , how = _STR_ )   $ df_test = pd . merge ( df_test , df_group_by2 , on = _STR_ , how = _STR_ )
% matplotlib inline   $ plt . scatter ( newdf [ _STR_ ] , newdf [ _STR_ ] )
wrd_full [ _STR_ ] = wrd_full [ _STR_ ] . apply ( lambda x : datetime . strptime ( x , _STR_ ) . year )
with open ( _STR_ ) as f :   $ size = len ( [ 0 for _ in f ] )   $ print ( _STR_ . format ( size ) )
format = lambda x : x [ _STR_ ] + x [ _STR_ ]   $ df2 . apply ( format , axis = 1 )
target_google = people_person [ people_person [ _STR_ ] == _STR_ ]   $ target_google . head ( )
df_protest . loc [ : , df_protest . columns [ df_protest . dtypes == _STR_ ] . tolist ( ) ]
def times_contacted_min_max_std ( df , sample_id ) :   $ rec = df [ df [ _STR_ ] == sample_id ]   $ return rec [ _STR_ ] . min ( ) , rec [ _STR_ ] . max ( ) , rec [ _STR_ ] . std ( )
s_n_s_epb_one . index = s_n_s_epb_one . Date   $ del s_n_s_epb_one [ _STR_ ]   $
datatest = pd . concat ( [ train , test , rest ] )
users . to_csv ( _STR_ , index = False )   $ repos_users . to_csv ( _STR_ , index = False )   $ repos_users . to_csv ( _STR_ , index = False )
autos [ _STR_ ] . str [ : 10 ] . value_counts ( dropna = False , normalize = True ) . sort_index ( )
faa_data_pandas . shape
p_diffs = [ ]   $ for i in range ( 10000 ) :   $ new_page_converted = np . random . binomial ( 1 , p_new , n_new )   $ old_page_converted = np . random . binomial ( 1 , p_old , n_old )   $ p_diffs . append ( new_page_converted . mean ( ) - old_page_converted . mean ( ) )
df . query ( _STR_ ) . shape [ 0 ]
df [ df [ _STR_ ] == 1 ] . groupby ( _STR_ ) . nunique ( ) . shape [ 0 ] / df . nunique ( ) [ _STR_ ]
print len ( word_freq_df )   $ word_freq_df . head ( )
def get_datetime_from_microsecond ( timestamp ) :   $ return time . strftime ( _STR_ , time . localtime ( timestamp / 1000 ) )
df . info ( )
data [ _STR_ ] . value_counts ( )
df_data . show ( )
from sklearn . metrics import accuracy_score , classification_report   $ score = 100.0 * accuracy_score ( y_test , predicted )   $ print ( _STR_ )   $ print ( classification_report ( y_test , predicted ) )
df . head ( )
dfx = df . reindex ( index = dates [ 0 : 4 ] , columns = list ( df . columns ) + [ _STR_ ] )   $ print ( dfx , _STR_ )   $ dfx . loc [ dates [ 0 ] : dates [ 1 ] , _STR_ ] = 1   $ print ( dfx , _STR_ )
df = sqlContext . inferSchema ( my_data )
pred4 = nba_pred_modelv1 . predict ( g4 )   $ prob4 = nba_pred_modelv1 . predict_proba ( g4 )   $ print ( pred4 )   $ print ( prob4 )
cached . count ( )
to_be_predicted_Day5 = 43.4545817   $ predicted_new = ridge . predict ( to_be_predicted_Day5 )   $ predicted_new
autos [ _STR_ ] . describe ( )
df1 = df . drop ( [ _STR_ , _STR_ , _STR_ ] , axis = 1 )   $ df1
print ( data . iloc [ [ 34450 ] ] )
smooth_stanmodel = SMOOTH . compileModel ( ) # to facilitate plotting
lgb_model . fit ( X_train , y_train )
big_df_count . head ( )
df_archive . info ( )
if True == 0 :   $ rr_tmp = rr . replace ( 0 , np . nan ) . apply ( [ np . mean , np . std ] )   $ rr_tmp . T . plot . barh ( )   $ plt . xlabel ( _STR_ )   $ plt . show ( )   $
users_not_odu . groupby ( users [ _STR_ ] > 0 ) . mean ( )
freeways . layers
os . chdir ( _STR_ )
goog . Open . plot ( )   $ goog . Close . plot ( )
plt . figure ( figsize = ( 16 , 8 ) )   $ plt . title ( _STR_ )   $ model_count = sns . countplot ( volkswagen_cars [ _STR_ ] )   $ model = model_count . set_xticklabels ( model_count . get_xticklabels ( ) , rotation = 90 )
vt_daily_flow_abs = df . xs ( _STR_ , level = _STR_ ) . flow [ _STR_ : ] . abs ( ) . rename ( _STR_ ) * 100   $ spy_daily_flow_abs = df . xs ( _STR_ , level = _STR_ ) . flow [ _STR_ : ] . abs ( ) . rename ( _STR_ ) * 100   $ title = _STR_   $ vt_daily_flow_abs . resample ( _STR_ ) . mean ( ) . plot ( legend = True , title = title )   $ spy_daily_flow_abs . resample ( _STR_ ) . mean ( ) . plot ( legend = True )   $
eclf3 = VotingClassifier ( estimators = [ ( _STR_ , alg ) , ( _STR_ , alg7 ) , ( _STR_ , alg2 ) , ( _STR_ , alg6 ) ] , voting = _STR_ )   $ eclf3 . fit ( X_train , y_train )   $ probs = eclf3 . predict_proba ( X_test )   $ score = log_loss ( y_test , probs )   $ print ( score )
name = _STR_   $ full_path_to_file = _STR_
old_page_converted = np . random . binomial ( 1 , prop_users_converted , n_old )
sdf . rdd . first ( ) # ["host"]
df [ df . sentiment == 1 ] . count ( )
ax = users . created_at . hist ( bins = 144 )   $ ax . set_xlabel ( _STR_ )   $ ax . set_ylabel ( _STR_ )   $ ax . set_title ( _STR_ )
from h2o . automl import H2OAutoML   $
errors . mae_vals . idxmin ( errors . mae_vals . min ( ) )
new_page_converted = np . random . choice ( 2 , size = n_new , p = [ 1 - p_new , p_new ] )
dates = np . array ( [ _STR_ , _STR_ ] , dtype = _STR_ )   $ print ( dates )
clf . fit ( X_train , y_train )
train_view . sort_values ( by = 5 , ascending = False ) [ 0 : 10 ]
get_response ( _STR_ )
f_os_hour_clicks = spark . read . csv ( os . path . join ( mungepath , _STR_ ) , header = True )   $ print ( _STR_ % f_os_hour_clicks . count ( ) )
cr_new_null = len ( df2 . query ( _STR_ ) ) / len ( df2 [ _STR_ ] )   $ print ( _STR_ . format ( cr_new_null ) )
prediction_clean . info ( )
oppose_NNN = merged_NNN [ merged_NNN . committee_position == _STR_ ]
Celsius . __repr__ = lambda self : _STR_
print ( soup . prettify ( ) )
target_data = pd . read_csv ( _STR_ , sep = _STR_ )   $ target_data . head ( 5 )
results . summary ( )
train [ _STR_ ] = train [ _STR_ ] . map ( lambda x : 1 if str ( x . date ( ) ) in us_holidays else 0 )   $ test [ _STR_ ] = test [ _STR_ ] . map ( lambda x : 1 if str ( x . date ( ) ) in us_holidays else 0 )
sentencelist = map ( lambda text : list ( jieba . cut ( text , cut_all = True ) ) , michael_df . text )   $ reducedwordlist = reduce ( lambda x , y : x + y , sentencelist )
df = pd . read_csv ( _STR_ , header = None )   $ df . columns
poverty_data = poverty . iloc [ poverty_data_rows , : ]
energy_indices = energy_cpi . drop ( [ _STR_ , _STR_ , _STR_ , _STR_ ] ,   $ axis = 1 ) . set_index ( [ _STR_ , _STR_ ] ) . unstack ( _STR_ )   $ energy_indices
properati . loc [ ( properati [ _STR_ ] == _STR_ ) & ( properati [ _STR_ ] == _STR_ ) , _STR_ ] = _STR_
returns = ( mydata / mydata . shift ( 1 ) ) - 1   $ returns . head ( )
assoc = db . get_associations ( limit = 150 )   $ assoc . head ( )
idxs = get_cv_idxs ( n , val_pct = 150000 / n )   $ joined_samp = joined . iloc [ idxs ] . set_index ( _STR_ )   $ samp_size = len ( joined_samp ) ; samp_size
rf1000 = RandomForestClassifier ( n_estimators = 1000 )   $ rf1000 . fit ( X , y )
random_steps . hist ( ) ;
test . replace ( { _STR_ : { np . nan : 1 } } , inplace = True )
trans = trans . sort_values ( [ _STR_ , _STR_ , _STR_ ] )
df_out = pd . merge ( df_userid , df_Tran , how = _STR_ , on = _STR_ ) [ [ _STR_ , _STR_ ] ]
grouped . get_group ( _STR_ )
df_new [ [ _STR_ , _STR_ , _STR_ ] ] = pd . get_dummies ( df_new [ _STR_ ] )
baseball . hr . sort_values ( )
store_items . dropna ( axis = 1 )
class_test . shape
print ( repos_users . shape )   $ repos_users . head ( )
plt . rcParams [ _STR_ ] = False   $ dta_6201 . plot ( figsize = ( 15 , 5 ) )   $ plt . show ( )
afx_2017_dict = r . json ( )
y = df [ _STR_ ] . values   $ df . drop ( _STR_ , axis = 1 , inplace = True )   $ df . drop ( _STR_ , axis = 1 , inplace = True )   $ X = df . values
from sklearn . base import BaseEstimator , RegressorMixin   $ class MedianReg ( BaseEstimator , RegressorMixin ) :     $ Called when initializing the regression   $
print ( _STR_ . format ( 100 * full [ _STR_ ] . mean ( ) ) )   $
donors [ donors [ _STR_ ] == 606 ] . head ( )
api_clean [ _STR_ ] = api_clean [ _STR_ ] . astype ( str )
% load_ext rpy2 . ipython   $ % Rpush raw_large_grid_df
output = output . sample ( False , 0.1 , seed = 0 )
group_name = lift . get_group_by_alias ( _STR_ )
scr_activated_df = pd . DataFrame ( index = daterange , columns = daterange )
import numpy as np   $ import pandas as pd   $ rainfall = pd . read_csv ( _STR_ ) [ _STR_ ] . values   $ inches = rainfall / 254.0 # 1/10mm -> inches $ inches.shape
num_portfolios = 25   $ results = np . zeros ( ( num_portfolios , 6 ) )
model_CSCO . aic
points = pd . Series ( [ 630 , 25 , 26 , 255 ] ,   $ index = [ _STR_ , _STR_ , _STR_ , _STR_ ] )   $ print ( points )
y_test . shape
groups = openmc . mgxs . EnergyGroups ( )   $ groups . group_edges = np . array ( [ 0. , 0.625 , 20.0e6 ] )
breed_conf = breed_conf . sort_values ( ascending = False )
train = K . function ( inputs = [ x , target ] , outputs = [ loss ] , updates = updates )
p_new = sum ( df2 . converted == 1 ) / 290584   $ p_new
_STR_ . format ( subjectivity . sum ( ) . counts , percentConvert ( subjectivity . loc [ _STR_ ] . percent ) , percentConvert ( subjectivity . loc [ _STR_ ] . percent ) )
df . iloc [ 2 ] # by single row number
requests . get ( wikipedia_marvel_comics )
for i , x in enumerate ( unprocessed_list ) :   $ unprocessed_list [ i ] = x . split ( _STR_ )
df [ y_col ] = df [ _STR_ ] . str . lower ( ) . replace ( repl_dir )   $ df_noblends = df [ df [ y_col ] . replace ( repl_dir ) . str . lower ( ) . isin ( keep_vars ) ]   $ df_noblends [ y_col ] . unique ( ) . size
tweet_place_hist = pd . crosstab ( index = tweets_df [ _STR_ ] , columns = _STR_ )   $ tweet_place_hist [ _STR_ ] = tweet_place_hist [ _STR_ ] * 100 / tweet_place_hist . sum ( ) [ _STR_ ]   $ tweet_place_hist = tweet_place_hist . sort_values ( _STR_ , ascending = False )   $ tweet_place_hist . head ( 10 )
file_path = _STR_   $ sheet = _STR_   $ users = pd . read_excel ( file_path , sheet_name = sheet , usecols = _STR_ )
tweet_archive . head ( 40 )
geocoded_df . loc [ idx , _STR_ ] = geocoded_df . loc [ idx , _STR_ ] - geocoded_df . loc [ idx , _STR_ ]
parks_don_quar = parks_don . groupby ( [ _STR_ ] ) [ [ _STR_ ] ] . sum ( )   $ parks_don_quar . plot ( kind = _STR_ ) ;
features = wine_query . execute ( output_options = bq . QueryOutput . dataframe ( ) ) . result ( )
plt . scatter ( y = dftouse_four . opening_gross , x = dftouse_four . star_avg , s = dftouse_four . review_count )
df_goog . Open
s . ix [ 3 : 6 ] . mean ( )
plt . plot ( dataframe . groupby ( _STR_ ) . daily_worker_count . mean ( ) )   $ plt . show ( )
print ( DataSet_sorted [ _STR_ ] . iloc [ 2 ] )
a_list . append ( 1 )
class_merged = pd . merge ( class_merged , transactions , on = [ _STR_ , _STR_ ] , how = _STR_ )   $ print ( _STR_ , class_merged . shape )   $ pd . DataFrame . head ( class_merged )
from preprocessing_pipeline import preprocessing   $ with open ( _STR_ . format ( version ) , _STR_ ) as file :   $ word_embedder = pickle . load ( file )
yhat = neigh . predict ( X_test )   $ yhat [ 0 : 5 ]
wrong_treatment1 = df . query ( _STR_ )   $ wrong_treatment2 = df . query ( _STR_ )   $ print ( _STR_ + str ( len ( wrong_treatment1 ) ) + str ( len ( wrong_treatment2 ) ) )
df2 . converted . sum ( ) / df2 . converted . count ( )
print ( prop . info ( ) )
from google . cloud import bigtable   $ client = bigtable . Client . from_service_account_json ( JSON_SERVICE_KEY , project = project_id , admin = True )   $ instance = client . instance ( instance_id )   $
df . info ( )
import seaborn as sns ;   $ titanic = sns . load_dataset ( _STR_ )   $ titanic . head ( )
salesfull . head ( )   $ print salesfull . sale . sum ( )   $ print salesfull . full_2016 . sum ( )
score_merkmale = scoring [ [ _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ ] ] . \   $ merge ( merkmale . reset_index ( ) , on = _STR_ )
np . count_nonzero ( np . any ( na_df . isnull ( ) , axis = 0 ) ) # total number of columns with missing values
learning_rate = 0.01   $ with tf . name_scope ( _STR_ ) :   $ optimizer = tf . train . GradientDescentOptimizer ( learning_rate )   $ training_op = optimizer . minimize ( loss )
rankings = pd . read_csv ( _STR_ ) #reading in a csv file
autos [ _STR_ ] . value_counts ( ) . sort_index ( ascending = True ) . head ( 10 )
df3 . sort_values ( _STR_ ) . head ( 2 ) , df3 . sort_values ( _STR_ ) . tail ( 2 )
mig_l1 = mi . groupby ( level = 0 )   $ print_groups ( mig_l1 )
analyze_set . sample ( 5 )
volt_prof_before = pd . read_csv ( _STR_ . format ( feeder = _feeder ) )   $ volt_prof_after = pd . read_csv ( _STR_ . format ( feeder = _feeder ) )
lda = LatentDirichletAllocation (   $ n_topics = n_topics , learning_method = _STR_ , random_state = 0 )   $ lda = lda . fit ( tf )
mini_px = consol_px [ consol_px . columns [ : 10 ] ] . tail ( 5 ) # pricing subset $ as_of = consol_px.index.to_datetime()[-1:] # date as of when we want the weights vector $ print(mini_px.shape, frame, lb, frequency, min_gross, max_gross, min_w, max_w, gamma_val) $ mini_px.shape, as_of
df_new [ _STR_ ] = df_new [ _STR_ ] . replace ( ( _STR_ , _STR_ , _STR_ ) , ( 0 , 0 , 1 ) )   $ lm = sm . OLS ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ ] ] )   $ lm . fit ( ) . summary ( )
model . summary ( )
dfg = Grouping_Year_DRG_discharges_payments . groupby ( [ _STR_ ] ) . \   $ agg ( { _STR_ : [ np . size , np . mean , np . max ] } )   $ dfg . head ( )
len ( users )   $ users . drop_duplicates ( subset = _STR_ , keep = _STR_ , inplace = True )   $ len ( users )
useless_variables = [ _STR_ , _STR_ , _STR_ ]   $ equipment . drop ( useless_variables , axis = 1 , inplace = True )
df_imputed = pd . DataFrame ( df_imput )   $
cats_df . describe ( )
S_lumpedTopmodel . decision_obj . hc_profile . options , S_lumpedTopmodel . decision_obj . hc_profile . value
inspector = inspect ( engine )   $ inspector . get_table_names ( )
df . index
coll . find_one ( { _STR_ : { _STR_ : datetime . datetime ( 2016 , 9 , 5 , 0 , 0 , 0 ) } } )
the_data = tmp_df . applymap ( lambda x : 1 if x > 3 else 0 ) . as_matrix ( )   $ print ( the_data . shape )
start = datetime . now ( )   $ modelrf250 = RandomForestClassifier ( n_estimators = 250 , n_jobs = - 1 )   $ modelrf250 . fit ( Xtr . toarray ( ) , ytr )   $ print ( modelrf250 . score ( Xte . toarray ( ) , yte ) )   $ print ( ( datetime . now ( ) - start ) . seconds )
mom . set_index ( _STR_ , inplace = True )
fig = plotly_plotting ( sentiment_track , results , results_postseason ) # add 'results_postseason' as third argument only if team made playoffs $ py.iplot(fig, filename='sentiment_vs_wins')
str ( datetime . now ( ) )
df_questionable_3 [ df_questionable_3 [ _STR_ ] == 1 ] [ _STR_ ] . value_counts ( )
tweet_df [ _STR_ ] . unique ( )
import dask . dataframe as dd
control_con = df2 [ ( df2 [ _STR_ ] == _STR_ ) & ( df2 [ _STR_ ] == 1 ) ] . shape   $ control_con
lda_tf . print_topics ( num_topics = 10 , num_words = 7 )
test_data_dir = _STR_ . format ( os . sep ) . join ( [ gensim . __path__ [ 0 ] , _STR_ , _STR_ ] ) + os . sep   $ lee_train_file = test_data_dir + _STR_   $ print lee_train_file
log_mod = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ results = log_mod . fit ( )   $ results . summary ( )
print _STR_ % ( sum (   $ cycling_df [ _STR_ ] ) , secToHours ( sum ( cycling_df [ _STR_ ] ) ) )
err = pd . DataFrame ( )   $ for subject in etsamples . subject . unique ( ) :   $ err = pd . concat ( [ err , CALIBRATION . pl_accuracy ( subject ) , CALIBRATION . el_accuracy ( subject ) ] , ignore_index = True )   $ err . loc [ : , _STR_ ] = err . avg . astype ( float )   $ err . loc [ : , _STR_ ] = err . msg_time . astype ( float )
rejected . isnull ( ) . sum ( )
1 / np . exp ( result . params [ 1 ] )
idx = df_providers [ ( df_providers [ _STR_ ] == 2011 ) &   \   $ ( df_providers [ _STR_ ] == 39 ) ] . index . tolist ( )   $
n_new = ( df2 [ df2 [ _STR_ ] == _STR_ ] ) . shape [ 0 ]   $ n_new   $
nb = MultinomialNB ( )   $ nb . fit ( X_train_total , y_train )   $ nb . score ( X_test_total_checked , y_test )
tokens . sort_values ( _STR_ , ascending = False ) . head ( 10 )
pd . merge ( crimes , weather , on = _STR_ , how = _STR_ ) . head ( )
notus [ _STR_ ] . value_counts ( dropna = False )
s3 = pd . Series ( diz , [ _STR_ , _STR_ , _STR_ ] )   $ s3
total_Visits_Convs_month_byMC . loc [ total_Visits_Convs_month_byMC . conversion_rate > 1 ] # outlier - this data may be incorrect $
plt . plot ( time_local , key_press , _STR_ )   $ plt . show ( )
df   \   $ . filter ( df . name == _STR_ ) \   $ . take ( 1 )
adj_close_pivot_merged = pd . merge ( adj_close_pivot , adj_close   $ , on = [ _STR_ , _STR_ ] )   $ adj_close_pivot_merged . head ( )
candidates [ _STR_ ] = candidates . election_date . apply ( get_cycle )
listRating = list ( soup . findAll ( _STR_ , class_ = _STR_ ) )   $ rating = list ( listRating [ 0 ] . children )   $ rating = rating [ 0 ]   $ print ( _STR_ + rating )   $ df_new [ _STR_ ] = rating
df2 [ _STR_ ] = 1   $ df2 [ [ _STR_ , _STR_ ] ] = pd . get_dummies ( df [ _STR_ ] )   $ df2 = df2 . drop ( _STR_ , axis = 1 )   $ df2 . head ( )
event_sdf_pd = event_sdf . toPandas ( )
TEXT . vocab . stoi [ _STR_ ]
import json   $ tweets = [ ]   $ for tw in new_tweets :   $ tweets . append ( tw . text )   $ print ( tweets )
tesla = pd . read_csv ( _STR_ )
my_data_test_path = _STR_   $ my_data_test = np . genfromtxt ( my_data_test_path , delimiter = _STR_ )
for tweet in tweepy . Cursor ( api . search , q = _STR_ , count = 100 , lang = _STR_ , since = _STR_ ) . items ( ) :   $ print ( tweet . created_at , tweet . text )   $ analysis = tb ( tweet . text )   $ print ( analysis . sentiment . polarity )   $ csvWriter . writerow ( [ tweet . created_at , tweet . text . encode ( _STR_ ) ] )   $
print ( test . shape )
urls = [ link . get ( _STR_ ) for link in all_a ]
from sklearn . model_selection import train_test_split
df_meta = pd . read_csv ( f_meta )   $ df_meta . head ( 3 )
algo = tree . DecisionTreeClassifier ( max_features = 4 )   $ train = algo . fit ( X_train , y_train )   $ res = train . predict ( X_val )
df = pd . read_csv ( _STR_ , nrows = 1000000 , usecols = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] )
a_set . union ( another_set )
best_worst = data_df . loc [ ( stars == 5 ) | ( stars == 1 ) , : ]   $ best_worst . head ( )
xgb = XGBClassifier ( objective = _STR_ )   $ xgb . fit ( X_train , y_train )   $ test_predictions = xgb . predict ( X_test )   $ eval_sklearn_model ( y_test , test_predictions , model = xgb , X = X_test )
! ls . .
c = { key : a [ key ] for key in a . keys ( ) - { _STR_ , _STR_ } }   $ c
ts . shift ( 1 , DateOffset ( minutes = 0.5 ) )
full . groupby ( [ _STR_ ] ) [ _STR_ ] . agg ( { _STR_ : _STR_ , _STR_ : _STR_ , _STR_ : _STR_ } ) [ _STR_ ] . plot ( kind = _STR_ , figsize = ( 12 , 4 ) )   $ plt . title ( _STR_ , size = 15 )   $ plt . ylabel ( _STR_ )
revisions = [ revision for revision in page . revisions ( ) ]   $ revisions [ 0 ]
df_characters . head ( 10 )
dataframe . columns = column_names
def initial_trend ( series , slen ) :   $ sum = 0.0   $ for i in range ( slen ) :   $ sum += float ( series [ i + slen ] - series [ i ] ) / slen   $ return sum / slen   $
df_members [ _STR_ ] = pd . cut ( df_members [ _STR_ ] , age_bins , labels = age_groups )
f_ip_device_clicks = spark . read . csv ( os . path . join ( mungepath , _STR_ ) , header = True )   $ print ( _STR_ % f_ip_device_clicks . count ( ) )
print ( autos . groupby ( _STR_ ) . size ( ) )
lm = sm . Logit ( df_joined [ _STR_ ] , df_joined [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ res = lm . fit ( )
t_likes . plot ( title = _STR_ , figsize = ( 12 , 6 ) , legend = True , label = _STR_ ) ;   $ t_retweets . plot ( legend = True , label = _STR_ )
CryptoComm [ _STR_ ] = CryptoComm [ _STR_ ] . apply ( len )   $ CryptoComm [ _STR_ ] = ( CryptoComm . CommentTime - CryptoComm . PostTime ) . total_seconds ( )
plt . figure ( figsize = ( 10 , 10 ) )   $ sns . distplot ( df_nd101_d_b [ df_nd101_d_b [ _STR_ ] > 0 ] . ud730 )
% matplotlib notebook   $ a . toPandas ( ) . plot ( ) ;
apple . index . is_unique
for index , row in df_rand . iterrows ( ) :   $ df_rand . loc [ index , _STR_ ] = row . postcodes in row . ch_postcodes
contractor_merge [ contractor_merge . contractor_bus_name . duplicated ( ) == True ]   $
print pd . concat ( [ s1 , s4 ] , axis = 1 , join = _STR_ )
df_gnis . shape
forecast_df [ _STR_ ] = 0   $ for ind , row in SANDAG_jobs_df . iterrows ( ) :   $ forecast_df . loc [ ind , _STR_ ] += row [ _STR_ ]   $ forecast_df . head ( )
popStationData = session . query ( Measurement . date , Measurement . tobs ) . filter_by ( station = _STR_ ) . filter ( Measurement . date > pastYear ) . all ( )   $ popStationData
stocks = pdread . DataReader ( _STR_ , _STR_ , st , ed )
Convert_Prob = df2 . query ( _STR_ ) . shape [ 0 ] / df2 . shape [ 0 ]   $ print ( _STR_ , Convert_Prob )
! ls . / anaconda3_410   $
train_set . to_csv ( PROCESSED_DATA_DIR + _STR_ , index = False )   $ test_set . to_csv ( PROCESSED_DATA_DIR + _STR_ , index = False )
ytDf_ttrendDate = youtube_df [ _STR_ ]   $ ytDf_views = youtube_df [ _STR_ ]   $ order = np . argsort ( ytDf_ttrendDate )   $ x_trendDate = np . array ( ytDf_ttrendDate ) [ order ]   $ y_views = np . array ( ytDf_views ) [ order ]
dsi_me_1_df . head ( 3 )
themes = _STR_ . join ( [ themes , _STR_ ] )
autos [ _STR_ ] . value_counts ( )
autos [ _STR_ ] . value_counts ( )
airlines = [ h for h in heap if h . company in [ _STR_ , _STR_ , _STR_ ] ]
nar5 = nar4 . merge ( loans [ [ _STR_ , _STR_ , _STR_ ] ] , on = _STR_ )
n_net2 . score ( x_test , y_test )
print ( json_data_2017 [ _STR_ ] . keys ( ) , _STR_ )   $ print ( json_data_2017 [ _STR_ ] [ _STR_ ] )
old_page_converted = np . random . binomial ( 1 , p_old , n_old )   $ sum ( old_page_converted )
print ( autodf . groupby ( _STR_ ) . size ( ) )   $ autodf = autodf [ autodf [ _STR_ ] != _STR_ ]   $ autodf = autodf . drop ( _STR_ , axis = 1 )   $ print ( _STR_ + str ( len ( autodf ) ) )
y_pred = y_pred . argmax ( axis = 1 )
df1_clean . source . unique ( )
last_year = session . query ( Measurement . date , Measurement . prcp ) . filter ( Measurement . date >= _STR_ ) . \   $ filter ( Measurement . date < _STR_ ) . order_by ( Measurement . date ) . all ( )
df_with_metac1 = pd . concat ( [ df_onc_no_metac , df_dummies ] , axis = 1 )
duration_train_df . shape , duration_test_data . shape
big_df_count . head ( )
from pandas . tseries . holiday import *   $ cal = USFederalHolidayCalendar ( )   $ for d in cal . holidays ( start = _STR_ , end = _STR_ ) :   $ print ( d )
imgp_clean = imgp_clean . drop ( imgp_clean [ ( imgp_clean . p1_dog == False ) & ( imgp_clean . p2_dog == False ) & ( imgp_clean . p3_dog == False ) ] . index )   $
train_data , test_data , train_labels , test_labels = train_test_split ( spmat , y_data , test_size = 0.10 , random_state = 42 )
empInfo = pd . read_csv ( _STR_ )   $ empInfo . columns = [ _STR_ , _STR_ , _STR_ , _STR_ ]
trunc_df . iloc [ list ( indices ) ]
fouls_df . to_pickle ( _STR_ )
data . loc [ data . PRECIP . isnull ( ) ]
[ ( v . standard_name ) for v in dsg . data_vars ( ) ]
prob_new_page = df2 . groupby ( _STR_ ) . count ( )   $ prob_new_page [ _STR_ ] [ _STR_ ] / len ( df2 )
contentPTags = obamaSpeechSoup . body . findAll ( _STR_ )   $ for pTag in contentPTags [ : 5 ] :   $ print ( pTag . text )
props . prop_name . value_counts ( ) . reset_index ( )
cities = [ ]   $ for i in places2 . index :   $ cities . append ( places2 [ i ] [ _STR_ ] )
display ( heading ( _STR_ ) ,   $ set ( get_grandchild_nodes ( observations_node ) . keys ( ) ) )
print ( autos [ _STR_ ] . unique ( ) . shape ) #Find out the number of different prices in the dataset
treat_old = df . query ( _STR_ ) . shape [ 0 ]   $ control_new = df . query ( _STR_ ) . shape [ 0 ]   $ misalignment = treat_old + control_new   $ misalignment
tweet_2 = pd . read_json ( _STR_ )
df2 [ ( df2 . group == _STR_ ) & ( df2 . converted == 1 ) ] . shape [ 0 ] / df2 [ df2 . group == _STR_ ] . shape [ 0 ]   $
import statsmodels . api as sm   $ convert_old = df2 . query ( _STR_ ) . shape [ 0 ]   $ convert_new = df2 . query ( _STR_ ) . shape [ 0 ]   $ n_old = df2 . query ( _STR_ ) . shape [ 0 ]   $ n_new = df2 . query ( _STR_ ) . shape [ 0 ]   $
discounts_table . groupby ( [ _STR_ ] , sort = False ) [ _STR_ ] . min ( )
obs_diff = ( df2 . query ( _STR_ ) . converted ) . mean ( ) - ( df2 . query ( _STR_ ) . converted ) . mean ( )   $ plt . hist ( p_diffs ) ;   $ plt . axvline ( x = obs_diff , color = _STR_ ) ;
from sklearn . linear_model import LogisticRegression   $ logmodel = LogisticRegression ( )   $ logmodel . fit ( X , y )
msftAC . tail ( 5 ) , shifted_forward . tail ( 5 )
import pandas as pd   $ cust_df = pd . read_csv ( _STR_ )   $ cust_df . head ( )
train , validation = train_test_split ( df_train , test_size = 0.3 )
if False :   $ plt . scatter ( train . diff_lng , train . duration )   $ plt . show ( )
print ( stats . variation ( project_0_cycle_times ) )   $ print ( stats . variation ( project_1_cycle_times ) )
df [ _STR_ ] = np . nan   $ df [ _STR_ ] = np . nan   $ df [ _STR_ ] = np . nan
df_countries . info ( )
plt . figure ( )   $ dat . VecAvgWindDir . plot ( )
poiModel = Sequential ( )   $ poiModel . add ( Embedding ( num_POI + 1 , output_dim = vec_dim , input_length = 1 ) )   $ poiModel . add ( Reshape ( ( vec_dim , ) ) )
def retrieve_html ( url ) :   $ r = requests . get ( url )   $ return ( r . status_code , r . text )
df . boxplot ( column = _STR_ , by = _STR_ ) ;
pd . DataFrame ( { _STR_ : [ x [ 1 ] for x in HARVEY_92_USERS_DM ] } ) . hist ( bins = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 10 , 100 , 200 , 1000 ] )
print ( _STR_ . format ( df [ _STR_ ] . mean ( ) ) )
dfs [ index_max ] [ dfs [ index_max ] [ _STR_ ] == dfs [ index_max ] [ _STR_ ] . max ( ) ]
unique_users_count2 = df2 . user_id . nunique ( )   $ print ( unique_users_count2 )
plt . style . available
new_page_converted = np . random . choice ( [ 1 , 0 ] , size = n_new , p = [ p_new , ( 1 - p_new ) ] )   $ p_new_page_converted = new_page_converted . mean ( )   $ print ( p_new_page_converted )
m3 . fit ( lrs , 7 , metrics = [ accuracy ] , cycle_len = 2 , cycle_save_name = _STR_ )
df . count ( ) [ 0 ]
g [ _STR_ ] = pd . to_datetime ( g [ _STR_ ] )   $ g [ _STR_ ] = g [ _STR_ ] . dt . year   $ g . head ( 2 )
pprint ( _STR_ )
pn_qty [ pn ] [ _STR_ ] . append ( table_1c . ix [ 0 ] )
log_reg_over . score ( X_train , y_train_over )
tmdb_movies_production_countries_revenue [ _STR_ ] = tmdb_movies_production_countries_revenue [ _STR_ ] . map ( lambda x : x . get ( _STR_ ) )
df . head ( 5 )
top_features = [ ( key , value ) for key , value in feature_set . items ( ) if value >= 100 ]
df . head ( )
overdue_encoder = LabelEncoder ( )   $ overdue_transf = overdue_encoder . fit_transform ( overdue )   $ vectorizer = DictVectorizer ( )   $ features_class_vect = vectorizer . fit_transform ( features_classif )   $ features_class_norm = normalize ( features_class_vect , norm = _STR_ , axis = 1 )
negGroups = list ( neg_tweets . group_id_x )   $ num_convos = len ( set ( negGroups ) )   $ print ( _STR_ )   $ companyNeg = filtered [ filtered . group_id . isin ( negGroups ) ]   $
baseball . loc [ 89521 , _STR_ ]
df_hi_temps . head ( )
archive_df [ archive_df . in_reply_to_status_id . isnull ( ) == False ]
df_email . occurred_at = pd . to_datetime ( df_email . occurred_at )   $ df_email [ _STR_ ] = df_email . occurred_at . dt . week #strftime('%Y-%U')
import datetime   $ datetime . datetime ( 2015 , 12 , 31 , 0 , 0 ) . strftime ( _STR_ )
store_items . isnull ( ) . sum ( )
weather [ weather [ _STR_ ] == _STR_ ] [ _STR_ ] . unique ( )
lastPrediction = thisWeek [ _STR_ ] . count ( ) - 1 + hourlyRates . loc [ hourlyRates [ _STR_ ] == thisWeek [ _STR_ ] [ 0 ] , _STR_ ] . iloc [ 0 ]   $ stddev = hourlyRates . loc [ hourlyRates [ _STR_ ] == thisWeek [ _STR_ ] [ 0 ] , _STR_ ] . iloc [ 0 ]   $ skew = hourlyRates . loc [ hourlyRates [ _STR_ ] == thisWeek [ _STR_ ] [ 0 ] , _STR_ ] . iloc [ 0 ]
counts = df . groupby ( TimeGrouper ( freq = _STR_ ) ) . agg ( { _STR_ : _STR_ } ) . rename ( columns = { _STR_ : _STR_ } )   $ counts . reset_index ( inplace = True )   $ counts . head ( )
x = pd . merge ( ign , salesdec , on = [ _STR_ , _STR_ ] , suffixes = [ _STR_ , _STR_ ] , how = _STR_ , indicator = True )   $ x . head ( )
tweet_json_df_clean . id . dtypes
( temp_df . email != _STR_ ) . sum ( )
lv_workspace . apply_subset_filter ( subset = _STR_ ) # Not handled properly by the IndexHandler
tm [ _STR_ ] . dt . hour . hist ( density = True , color = _STR_ )
tweet_counts_by_month = tweet_archive_master [ [ _STR_ , _STR_ , _STR_ ] ] . groupby ( pd . Grouper ( key = _STR_ , freq = _STR_ ) ) . sum ( )
pipe_lr_2 = make_pipeline ( hvec , lr )   $ pipe_lr_2 . fit ( X_train , y_train )   $ pipe_lr_2 . score ( X_test , y_test )
df_new = pd . concat ( [ df_2001 , df_2002 , df_2003 , df_2004 , df_2007 , df_2008 , df_2009 , df_2010 , df_2011 , df_2012 , df_2013 , df_2014 , df_2015 , df_2016 , df_2017 ] , sort = True )
train [ _STR_ ] = 0   $ train . loc [ pd . notnull ( train . units_purchased ) , _STR_ ] = 1   $ train = train . drop ( [ _STR_ , _STR_ , _STR_ ] , axis = 1 )
all_df . isnull ( ) . any ( )   $
logit = sm . Logit ( df2 . converted , df2 [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ result = logit . fit ( )   $ result . summary ( )
props . info ( )
df_image_clean . loc [ 1024 , : ]
start_date = _STR_   $ end_date = _STR_   $ calc_temp = session . query ( func . min ( Measurement . tobs ) , func . max ( Measurement . tobs ) , func . avg ( Measurement . tobs ) ) . \   $ filter ( Measurement . date >= start_date , Measurement . date <= end_date ) . all ( )   $ calc_temp
move_1_union = sale_lost ( breakfastlunchdinner . iloc [ 1 , 1 ] , 30 )   $ move_2_union = sale_lost ( breakfastlunchdinner . iloc [ 5 , 2 ] , 30 )   $ adjustment_2 = move_1_union + move_2_union   $ print ( _STR_ + str ( move_34p14u34p - adjustment_2 ) )
json_df [ _STR_ ] . value_counts ( )
os . getcwd ( )
df . to_csv ( _STR_ , index = False )
print ( _STR_ . format ( np . mean ( data2 [ _STR_ ] ) ) )
classifier = train_classifier ( articles_train , authors_train , articles_test , authors_test ,   $ LogisticRegression ( ) )   $ print ( classifier . score ( test_features_tokenized , authors_test ) )
np . sum ( df [ _STR_ ] < 0.01 )
data_df . groupby ( _STR_ ) [ _STR_ ] . nunique ( )
tweets_clean . drop ( index = 754011816964026368 , inplace = True )   $ images_clean . drop ( index = 754011816964026368 , inplace = True )
total_delta_nbs = hits_df . iloc [ - 1 ] - hits_df . iloc [ 0 ]   $ total_delta_nbs
PMCIDs_in_demographics = [ d [ _STR_ ] [ _STR_ ] [ _STR_ ] [ _STR_ ] for d in output_dict [ _STR_ ] ]   $ jsondata = json . dumps ( pmcid_in_demographics )   $ f = open ( _STR_ , _STR_ )   $ f . write ( jsondata )   $ f . close ( )
train . head ( 3 )
corpus = st . CorpusFromParsedDocuments ( df_trump_device_non_retweets ,   $ category_col = _STR_ ,   $ parsed_col = _STR_ ) . build ( )
% % time   $ body_pp = processor ( keep_n = 8000 , padding_maxlen = 70 )   $ train_body_vecs = body_pp . fit_transform ( train_body_raw )
len ( [ col_n for col_n in df . columns if _STR_ in col_n ] )   $
df_total . head ( )
df . groupby ( _STR_ ) . max ( )   $
conn_helloDB . execute ( _STR_ )
fraud_data_updated . info ( )
dftops . plot ( kind = _STR_ )
df0 . date = pd . to_datetime ( df0 . date )
clean_rates . cuteness . value_counts ( )
autos . columns
store_time = table_store . get_value ( 0 , _STR_ )
dfHashtags . to_pickle ( data + _STR_ )
_STR_ , len ( df_experiment . user_id . unique ( ) )
station_df = pd . read_sql ( _STR_ , conn )   $ station_df . head ( )
disag_hart = DataSet ( disag_filename )   $ disag_hart_elec = disag_hart . buildings [ building ] . elec
df [ df . index . month . isin ( [ 11 , 12 , 1 , 2 ] ) ] [ _STR_ ] . value_counts ( ) . head ( )
CRnew = df2 . converted . sum ( ) / df2 . count ( ) [ 0 ]   $ CRnew
locationing [ _STR_ ] = locationing . apply ( lambda x : TextBlob ( x [ _STR_ ] ) . sentiment . polarity , axis = 1 )   $ locationing [ _STR_ ] = locationing . apply ( lambda x : TextBlob ( x [ _STR_ ] ) . sentiment . subjectivity , axis = 1 )   $ locationing = locationing . sort_values ( _STR_ , ascending = False ) . drop_duplicates ( [ _STR_ ] , keep = _STR_ )   $ locationing . head ( )
sox . sort ( _STR_ , ascending = True , inplace = True )   $ sox . reset_index ( drop = False , inplace = True )   $ sox . rename ( columns = { _STR_ : _STR_ } , inplace = True )
one_station [ _STR_ ] = pd . to_datetime ( one_station [ _STR_ ] )
df . query ( _STR_ ) . count ( ) [ 0 ] + df . query ( _STR_ ) . count ( ) [ 0 ]
for i in range ( 1 , 10 ) :   $ print ( _STR_ . format ( 10 ** - i , np . sum ( rfc_feat_sel . feature_importances_ >= 10 ** - i ) ) )
df . loc [ : , [ _STR_ , _STR_ ] ] # notice lack of parentheses here!
_STR_ . join ( [ _STR_ , _STR_ ] )
auth = tweepy . OAuthHandler ( consumer_key , consumer_secret )   $ auth . set_access_token ( access_token , access_token_secret )   $ api = tweepy . API ( auth , parser = tweepy . parsers . JSONParser ( ) )
list ( c . find ( ) )
np . random . seed ( 15 )   $ df . sample ( frac = 0.6 ) #randomly pick 60% of rows, without replacement
print ( _STR_ . format ( len ( df2 . user_id . unique ( ) ) ) )   $
top_movies = new_user_recommendations_rating_title_and_count_RDD . filter ( lambda r : r [ 2 ] >= 25 ) . takeOrdered ( 25 , key = lambda x : - x [ 1 ] )   $ print ( _STR_ %   $ _STR_ . join ( map ( str , top_movies ) ) )
retention = grouped . groupby ( level = 0 ) . apply ( cohort_period )
session . query ( Station . station , Station . name , Station . latitude , Station . longitude , Station . elevation ,   \   $ func . sum ( Measurement . prcp ) ) . filter ( Measurement . station == Station . station ) \   $ . filter ( Measurement . date >= _STR_ ) . filter ( Measurement . date < _STR_ ) \   $ . group_by ( Station . station ) . order_by ( func . sum ( Measurement . prcp ) . desc ( ) ) . all ( )
headlines_prep = preprocess ( headlines )   $ y_true = random_sample [ actual_reactions_list ] . as_matrix ( )   $ y_pred = clf . predict ( headlines_prep )   $ y_prob = clf_prob . predict_proba ( headlines_prep )
item = collection . item ( _STR_ )   $ item
from scipy import stats   $ stats . chisqprob = lambda chisq , df : stats . chi2 . sf ( chisq , df )
data . loc [ _STR_ ]
val sorted_requests = requests   $ . map ( pair = > pair . swap )   $ . transform ( rdd = > rdd . sortByKey ( false ) )
initialdate_per_coin = df0 . groupby ( _STR_ ) . date . min ( )   $ initialdate_per_coin . to_csv ( _STR_ )
events_per_day = events_df [ [ _STR_ , _STR_ ] ] . groupby ( _STR_ ) . count ( )   $ events_per_day . rename ( columns = { _STR_ : _STR_ } , inplace = True )   $ events_per_day . reset_index ( inplace = True )
! ls . / container / model
! python . . / . convert_notebook_to_script . py - - input ch08 . ipynb - - output ch08 . py
y = df . price . values   $ plt . hist ( np . log ( y ) )   $ plt . show ( )
train_norm = train_norm . join ( train [ [ _STR_ , _STR_ , _STR_ , _STR_ ] + dummy_features_test ] )   $ test_norm = test_norm . join ( test [ [ _STR_ , _STR_ , _STR_ , _STR_ ] + dummy_features_test ] )
properati . count ( ) . plot ( kind = _STR_ , figsize = ( 20 , 10 ) )
print ( _STR_ + str ( df . isnull ( ) . values . any ( ) ) )
shopping_carts . values
import matplotlib . pyplot as plt   $ import seaborn as sns   $ % matplotlib inline
p_new = new_page_converted . mean ( )   $ p_old = old_page_converted . mean ( )   $ p_new - p_old
popC15 [ popC15 . content == _STR_ ] . sort_values ( by = _STR_ , ascending = False ) . head ( 10 ) . plot ( kind = _STR_ )
df . info ( )
df . corr ( ) [ _STR_ ]
print ( _STR_ )   $ match . head ( 3 )
data . info ( )
fraq_volume_m_coins [ [ _STR_ , _STR_ ] ] . plot ( )   $ plt . ylabel ( _STR_ )   $ plt . show ( )
goog . plot ( y = _STR_ )
from pyspark . ml . evaluation import BinaryClassificationEvaluator   $ evaluator = BinaryClassificationEvaluator ( rawPredictionCol = _STR_ , labelCol = _STR_ , metricName = _STR_ )   $ print _STR_ . format ( evaluator . evaluate ( results ) )
ways . info ( )
from sklearn . model_selection import train_test_split   $ X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.33 , random_state = 101 )
pm_final [ ( pm_final . obs_date < 40 ) & ( pm_final . status == 1 ) ]
df_schools = pd . read_csv ( file1 )   $ df_students = pd . read_csv ( file2 )
order_data . reset_index ( inplace = True )
df_columns [ df_columns [ _STR_ ] . str . contains ( _STR_ ) ] [ _STR_ ] . value_counts ( ) . head ( )   $
print ( _STR_ % mean_squared_error ( y_test , y_pred ) )
maint [ _STR_ ] = pd . to_datetime ( maint [ _STR_ ] , format = _STR_ )   $ maint . count ( )
p_value = ( p_diffs > obs_diff_conversion ) . mean ( )   $ print ( p_value )
data . shape
feature_df = cell_df [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ]   $ X = np . asarray ( feature_df )   $ X [ 0 : 5 ]
ppm_title . token_count_pandas ( ) . head ( )
stars . value_counts ( )
joined . describe ( )   $
df_comment [ _STR_ ] = df_comment . liked_by . apply ( lambda x : len ( x ) )
print ( s2 . keys ( ) )   $ print ( s3 . keys ( ) )   $ s2 + s3
data . describe ( )
df2 . shape
values = pd . DataFrame ( dataSeries . values )   $ df = pd . concat ( [ values . shift ( 1 ) , values ] , axis = 1 )   $ df . columns = [ _STR_ , _STR_ ]   $ result = df . corr ( )   $ print ( result )
ab_dataframe . query ( _STR_ ) . shape [ 0 ]
BroncosBillsTweets [ _STR_ ] = BroncosBillsTweets [ _STR_ ] . apply ( lambda x : x [ : 30 ] )
data_FCInspevnt_latest = data_FCInspevnt_latest . drop_duplicates ( subset = _STR_ , keep = _STR_ )
dfss . head ( )
df_new [ _STR_ ] = 1   $ logit_mod = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ ] ] )   $ results = logit_mod . fit ( )   $ results . summary ( )
twitter_archive_df_clean . columns
sq83 = _STR_   $ sq84 = _STR_
a = 4.5   $ b = 2   $ print ( _STR_ . format ( type ( a ) , type ( b ) ) )   $ a / b
interpolated = bymin . resample ( _STR_ ) . interpolate ( )   $ interpolated
df_merge = df . merge ( df_users , on = _STR_ , how = _STR_ ) [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ]   $ df_merge . to_csv ( _STR_ , index = False )
pred10 = nba_pred_modelv1 . predict ( g10 )   $ prob10 = nba_pred_modelv1 . predict_proba ( g10 )   $ print ( pred10 )   $ print ( prob10 )
from sklearn . metrics import r2_score   $ r2_score ( y_test , pred )   $
df2 [ ( ( df2 [ _STR_ ] == _STR_ ) == ( df2 [ _STR_ ] == _STR_ ) ) == False ] . shape [ 0 ]
x_train , x_test , y_train , y_test = train_test_split ( x , y , test_size = 0.25 , random_state = 42 )
twitter_key = os . environ . get ( _STR_ )   $ twitter_secret = os . environ . get ( _STR_ )   $ twitter_token = os . environ . get ( _STR_ )   $ twitter_token_secret = os . environ . get ( _STR_ )
df [ _STR_ ] = df [ _STR_ ] + df [ _STR_ ]   $ df . head ( 2 )
image_predictions_copy [ image_predictions_copy . p1_dog == True ]
autos . describe ( include = _STR_ ) #include all to get both categorical(non-numeric) and numeric analyses for any columns
negative_visits = cats_df [ cats_df [ _STR_ ] < 0 ] [ _STR_ ]   $ cats_df [ _STR_ ] . iloc [ negative_visits . index ] = True   $ del negative_visits
no_conv , yes_conv = df2 . query ( _STR_ ) . converted . value_counts ( )   $ yes_conv / ( no_conv + yes_conv )
url = _STR_   $ browser . visit ( url )   $ time . sleep ( 3 ) #allow time for page to load $ html = browser.html $ soup = bs(html, 'html.parser') $
meas = Base . classes . measurements   $ station = Base . classes . stations
pd . bdate_range ( start = start , periods = 20 )
path = os . path . join ( _STR_ )   $ epa = pd . read_csv ( path )
ts . asfreq ( pd . tseries . offsets . BDay ( ) , method = _STR_ )
stockdftest . tail ( 3 )
def one_year_csv_writer ( this_year , all_data , path , name ) :   $ surveys_year = all_data [ all_data . year == this_year ]   $ filename = path + name + str ( this_year ) + _STR_   $ surveys_year . to_csv ( filename )   $ one_year_csv_writer ( 1997 , surveys_df , _STR_ , _STR_ )
X = stock . iloc [ 925 : - 1 ] . drop ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , 1 )   $ y = stock . iloc [ 925 : - 1 ] . true_grow
results_frame_b . iloc [ results_frame_b [ _STR_ ] . idxmax ( ) ]
y_pred = nb_pred [ : , 0 ]   $ print ( _STR_ . format ( precision_score ( y_test , y_pred ) ,   $ recall_score ( y_test , y_pred ) ,   $ accuracy ( y_test , y_pred ) ) )
df [ _STR_ ] = df [ _STR_ ] . astype ( int )
rankings_USA [ _STR_ ] = pd . DatetimeIndex ( rankings_USA [ _STR_ ] ) . year   $
import pickle   $ with open ( _STR_ , _STR_ ) as fp :   $ pickle . dump ( list_of_hospitals , fp )   $
import statsmodels . api as sm   $ convert_old = df2 . query ( _STR_ ) [ _STR_ ] . sum ( )   $ convert_new = df2 . query ( _STR_ ) [ _STR_ ] . sum ( )   $ n_old = df2 . query ( _STR_ ) . landing_page . count ( )   $ n_new = df2 . query ( _STR_ ) . landing_page . count ( )
results = crowdtruth . run ( data , config )
! python extract_dl1 . py - h
treatment_convert_rate = df2 . query ( _STR_ ) . converted . mean ( )   $ print ( _STR_ .   $ format ( treatment_convert_rate ) )
import pandas   $ data = pandas . read_excel ( _STR_ , sheetname = _STR_ , parse_dates = [ _STR_ ] , dayfirst = True )   $ data . head ( 2 )
journalist_retweet_gender_summary ( journalists_retweet_df [ journalists_retweet_df . gender == _STR_ ] )
minMaxDate . head ( )
trading_exemption_records . apply ( lambda x : x [ _STR_ ] if not pd . isnull ( x [ _STR_ ] ) else x [ _STR_ ] , axis = 1 ) . value_counts ( )
labels = pd . DataFrame ( { _STR_ : date_range } )   $ labels [ _STR_ ] = labels [ _STR_ ] . shift ( - 1 )   $ labels [ _STR_ ] = customer_id   $ labels . head ( )
reddit_comments_data = reddit_comments_data . withColumn ( _STR_ , senti_udf ( reddit_comments_data . body ) )
X = preprocessing . scale ( X )
neuron_no = 10   $ source_indices_L23exc_L23fs = np . where ( np . array ( conn_L23exc_L23fs . i ) == neuron_no )   $ target_indices_L23exc_L23fs = np . array ( conn_L23exc_L23fs . j ) [ source_indices_L23exc_L23fs ]
cog_simband_times [ cog_simband_times . index == _STR_ ]
player_search_count_df = pd . DataFrame . from_dict ( player_search_count , orient = _STR_ ) . reset_index ( )   $ player_search_count_df . columns = [ _STR_ , _STR_ ]   $ players_df = pd . read_csv ( _STR_ )   $ players_df = pd . merge ( player_search_count_df , players_df , left_on = _STR_ , right_on = _STR_ , how = _STR_ )
changeDate = lambda x : x . weekday ( )   $ sLength = len ( df [ _STR_ ] )   $ Weekdays = pd . Series ( np . random . randn ( sLength ) )   $ Weekdays = df [ _STR_ ] . apply ( changeDate )   $ df = df . assign ( Weekdays = Weekdays . values )
marvelPTags = wikiMarvelSoup . body . findAll ( _STR_ )   $ for pTag in marvelPTags [ : 8 ] :   $ print ( pTag . text )
df = pd . DataFrame ( data = fruits_and_veggies )   $ df
plt . subplots ( figsize = ( 15 , 7 ) )   $ df [ _STR_ ] . hist ( bins = 30 )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ ) ;
weather . head ( 10 )
( null_vals > ( prob2 - prob1 ) ) . mean ( )
df [ ( df . full_sq > 10 ) & ( df . full_sq < 1500 ) ]
autos = autos [ autos [ _STR_ ] . between ( 1900 , 2016 ) ]   $ autos [ _STR_ ] . value_counts ( normalize = True )   $
pres_df [ _STR_ ] . isnull ( ) . sum ( ) , pres_df . shape
df . show ( )
pageviews_tags . info ( )
secclintondf [ _STR_ ] = secclintondf . text . apply ( lambda s : TextBlob ( s ) . tags )
df_new [ _STR_ ] = 1   $ logit = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ ] ] )   $ results = logit . fit ( )   $ results . summary ( )
df_ad_airings_5 . info ( )   $
tweet_json . head ( )
neg_columns = [ word for word in columns if word in negative_words ]   $ dtm_neg = dtm_df [ neg_columns ]   $ dtm_neg [ _STR_ ] = dtm_neg . sum ( axis = 1 )   $ dtm_neg [ _STR_ ]
% % timeit - n 10   $ for group , frame in df . groupby ( _STR_ ) :   $ avg = np . average ( frame [ _STR_ ] )   $ print ( _STR_ + group + _STR_ + str ( avg ) )
import pandas as pd   $ from datetime import timedelta   $ % matplotlib inline   $ df_vow = pd . read_csv ( _STR_ )
print ( _STR_ . format ( * taxiData . shape ) )
user = tweet . author   $ for param in dir ( user ) :   $ if not param . startswith ( _STR_ ) :   $ print ( _STR_ % ( param , eval ( _STR_ + param ) ) )
pair . tail ( )
s . str . split ( _STR_ )
weather . loc [ weather . events == _STR_ , _STR_ ] = _STR_   $ weather . loc [ weather . events . isnull ( ) , _STR_ ] = _STR_
df3 = df [ df . Language == _STR_ ]   $ df3 . drop ( [ _STR_ ] , 1 )
open ( _STR_ , encoding = _STR_ )
collection . delete_snapshot ( _STR_ )   $
tweet_archive_clean = pd . merge ( tweet_archive_clean , info , on = _STR_ , how = _STR_ )
data . groupby ( [ _STR_ ] ) [ _STR_ ] . sum ( )
News_outlets_df . dtypes
import statsmodels . api as sm   $ logit_model = sm . Logit ( y , X )   $ result = logit_model . fit ( )
tweets . sort_values ( by = _STR_ , ascending = True ) . head ( )
image_predictions_clean [ _STR_ ] . value_counts ( )
tce2 . drop ( columns = [ _STR_ ] , inplace = True )   $ tce2
cust_data . sort_values ( by = _STR_ , ascending = False ) . head ( 10 )   $
train_view . sort_values ( by = 4 , ascending = False ) [ 0 : 10 ]
free_data . groupby ( _STR_ ) . mean ( )
cars = cars [ ( cars . yearOfRegistration <= 2016 ) & ( cars . yearOfRegistration >= 1950 ) & ( cars . price >= 100 ) & ( cars . price <= 160000 ) ]
X_train = df . loc [ : 25000 , _STR_ ] . values   $ y_train = df . loc [ : 25000 , _STR_ ] . values   $ X_test = df . loc [ 25000 : , _STR_ ] . values   $ y_test = df . loc [ 25000 : , _STR_ ] . values
num_data = pd . DataFrame ( df [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] , index = None )   $ num_data . head ( )
tw_clean [ tw_clean . text . str . startswith ( _STR_ ) ] . shape [ 0 ]
journalist_retweet_gender_summary ( journalists_retweet_df [ journalists_retweet_df . gender == _STR_ ] )   $
food [ _STR_ ] . head ( )
uber_15 . head ( )
df . describe ( )
def fahr_to_kelvin ( temp ) :   $ temp_kelvin = ( ( temp - 32 ) * 5 / 9 ) + 273.15   $ return temp_kelvin
row_0 . index
brand_counts = autos [ _STR_ ] . value_counts ( normalize = True )   $ most_common = brand_counts [ brand_counts > .05 ] . index   $ print ( most_common )
print ( _STR_ % ( ( 1. - freq_prob ) / freq_prob ) )
compared_resuts . to_csv ( _STR_ )
from sklearn . metrics import f1_score   $ from sklearn . neighbors import KNeighborsClassifier
df [ df . msno == _STR_ ]
rollrank_fxn = lambda x : x . rolling ( 200 , min_periods = 20 ) . apply ( lambda x : pd . Series ( x ) . rank ( pct = True ) [ 0 ] , raw = True )   $ features [ _STR_ ] = prices . groupby ( level = _STR_ ) . volume . apply ( rollrank_fxn )
data = pd . Series ( [ 1 , np . nan , 2 , None , 3 ] , index = list ( _STR_ ) )   $ data
plt . title ( _STR_ )   $ aggregated_content . plot ( kind = _STR_ , figsize = ( 15 , 7 ) )
to_be_predicted_Day2 = 81.77623296   $ predicted_new = ridge . predict ( to_be_predicted_Day2 )   $ predicted_new
archive_copy . info ( )
questions = pd . concat ( [ questions . drop ( _STR_ , axis = 1 ) , ad_source ] , axis = 1 )
Imagenes_data . describe ( )
df_twitter_archive_master = pd . read_csv ( _STR_ )
precipitation_df . plot ( kind = _STR_ , grid = True )   $ plt . title ( _STR_ )   $ plt . xticks ( rotation = 45 )   $ plt . gray ( )   $ plt . show ( )
n_inc_series = 5   $ Data [ _STR_ ] = pd . concat ( [ Data [ _STR_ ] ] * n_inc_series , ignore_index = True )   $ Data [ _STR_ ] . loc [ : , _STR_ ] = Data [ _STR_ ] [ _STR_ ] . apply ( lambda n : randint ( - 39 , - 1 ) )
df = df . sort_values ( by = [ _STR_ , _STR_ ] , axis = 0 , ascending = [ True , False ] )
dayofweek . count ( )
pandas_df = spark . read . format ( _STR_ ) . options ( header = _STR_ ) . load ( _STR_ )   $ pandas_df . head ( )
df . set_index ( _STR_ ) [ _STR_ ] . resample ( _STR_ ) . count ( )
print ( _STR_ , datetime . datetime . now ( ) )
from selenium import webdriver   $ driver = webdriver . Chrome ( )
var_per_portfolio = res . groupBy ( _STR_ , _STR_ ) . sum ( )   $ var_per_portfolio = var_per_portfolio . map ( lambda r : ( r [ 0 ] , r [ 1 ] , r [ 2 ] , float ( var ( np . array ( r [ 3 : ] ) - r [ 2 ] ) ) ) )   $ var_per_portfolio = sql . createDataFrame ( var_per_portfolio , schema = [ _STR_ , _STR_ , _STR_ , _STR_ ] )
autos . head ( )
print ( X_train . iloc [ 0 , ] )   $ print ( cvec . get_feature_names ( ) [ 64 ] , cvec . get_feature_names ( ) [ 94 ] )
cats_df . describe ( )
fin_r_monthly = fin_r . resample ( _STR_ ) . asfreq ( )
match_id ( merkmale , merkmale . Merkmalcode . isin ( [ _STR_ ] ) ) . Merkmalcode . unique ( )
bnb . groupby ( _STR_ )   $
print ( _STR_ , np . sqrt ( fruits ) )
z_values , _ = create_latent ( nn_aae . nn_enc , test_loader )   $ print ( z_values [ : , 0 ] . mean ( ) , z_values [ : , 0 ] . std ( ) )   $ print ( z_values [ : , 1 ] . mean ( ) , z_values [ : , 1 ] . std ( ) )   $ recon_x = create_sample ( nn_aae . nn_dec , z_values )   $ plot_mnist_sample ( recon_x )
tmp_input = pd . DataFrame ( tail . iloc [ - 1 , lookforward_window : ] ) . T . values   $ tmp_input [ 0 ] = scaler . transform ( tmp_input [ 0 ] . reshape ( - 1 , 1 ) ) . reshape ( 1 , - 1 )   $ print ( tmp_input . shape )   $ tmp_input = np . reshape ( tmp_input , ( tmp_input . shape [ 0 ] , 1 , tmp_input . shape [ 1 ] ) )   $ print ( tmp_input . shape )
store_items . dropna ( axis = 0 ) # or store_items.dropna(axis=0, inplace=True)
state_DataFrames_list = [ ]   $ for item in state_keys_list :   $ state_DataFrames_list . append ( state_DataFrames [ item ] )
data . dropna ( )
gene_df [ _STR_ ] = gene_df . end - gene_df . start + 1   $ gene_df [ _STR_ ] . describe ( )
import getpass   $ account = _STR_   $ password = getpass . getpass ( _STR_ )
people . eval ( _STR_ , inplace = True )   $ people
from bmtk . analyzer import nodes_table   $ nodes_table ( _STR_ , _STR_ )
vals2 . sum ( ) , vals2 . min ( ) , vals2 . max ( )
cdata [ cdata . Number_TD > 1 ]
sess . get_data ( _STR_ , _STR_ )
user = toggl . request ( _STR_ )
from IPython . core . interactiveshell import InteractiveShell   $ InteractiveShell . ast_node_interactivity = _STR_
ls_not_numeric = [ not pd . api . types . is_numeric_dtype ( dtype ) for dtype in df_with_metac_with_onc . dtypes ]   $ prog = re . compile ( _STR_ )   $ ls_not_date = [ not bool ( prog . search ( colname ) ) for colname in df_with_metac_with_onc . columns ]   $ ls_both = [ num and date for num , date in zip ( ls_not_numeric , ls_not_date ) ]   $ df_with_metac_with_onc . loc [ : , ls_both ] . nunique ( )
misaligned_row_count = ab_df [ ( ( ab_df [ _STR_ ] == _STR_ ) & ( ab_df [ _STR_ ] == _STR_ ) ) | ( ( ab_df [ _STR_ ] == _STR_ ) & ( ab_df [ _STR_ ] == _STR_ ) ) ] [ _STR_ ] . count ( )   $
url = _STR_   $ r = s . get ( url , params = { _STR_ : _STR_ } )   $ r . status_code
params = { _STR_ : [ 8 , 8 ] , _STR_ : _STR_ , _STR_ : True , _STR_ : _STR_ , _STR_ : 12.0 , _STR_ : 2 }   $ plot_decomposition ( RN_PA_duration , params = params , freq = 31 , title = _STR_ )
df . info ( )
mod = sm . Logit ( new_df [ _STR_ ] , new_df [ [ _STR_ , _STR_ , _STR_ ] ] )   $ results = mod . fit ( )
df = pd . read_pickle ( _STR_ )
spp [ _STR_ ] = spp [ spp . columns [ spp . columns . str . contains ( _STR_ ) == True ] ] . sum ( axis = 1 )   $ spp [ _STR_ ] = spp [ spp . columns [ spp . columns . str . contains ( _STR_ ) == True ] ] . sum ( axis = 1 )
greater_p_diffs = ( p_diffs > ab_data_diff ) . mean ( )   $ greater_p_diffs
print ( _STR_ + str ( len ( tweets_clean . query ( _STR_ ) ) ) )   $ print ( _STR_ + str ( len ( tweets_clean ) ) )   $ tweets_clean = tweets_clean [ tweets_clean . in_reply_to_status_id != tweets_clean . in_reply_to_status_id ]
import time   $ twitter_archive_master . timestamp = twitter_archive_master . timestamp . apply ( lambda x : time . strftime ( _STR_ , time . strptime ( x , _STR_ ) ) )   $ twitter_archive_master . timestamp = np . array ( twitter_archive_master . timestamp , dtype = _STR_ )
P . columns
train_session . head ( )
from pprint import pprint   $ oldest_month = 0   $ trump_tweets   $ oldest_month = trump_tweets [ len ( trump_tweets ) - 1 ] [ _STR_ ] . split ( ) [ 1 ]   $ oldest_month   $
twitter_archive_df_clean . drop ( [ _STR_ , _STR_ ] , axis = 1 , inplace = True )
dd_df [ _STR_ ] . fillna ( _STR_ , inplace = True )
df [ ( ( df [ _STR_ ] == _STR_ ) == ( df [ _STR_ ] == _STR_ ) ) == False ] [ _STR_ ] . count ( )
print ( cv_score )   $ print ( np . mean ( cv_score ) )   $ print ( accuracy_score ( y_train , cv_predict_score ) )
plt . scatter ( ort_avg17 , drt_avg17 ) ;
plt . plot ( sample [ : , 2 ] , sample [ : , 3 ] , _STR_ , alpha = 0.1 )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )   $ print ( _STR_ . format ( sample [ : , 2 ] . mean ( ) ) )   $ print ( _STR_ . format ( sample [ : , 3 ] . mean ( ) ) )
plt . rcParams [ _STR_ ] = ( 20.0 , 10.0 )
mit . groupby ( _STR_ ) . num_commits . sum ( )
rank_meters [ _STR_ ] . most_common ( 11 ) [ 1 : ]
test_df . columns [ test_df . isnull ( ) . any ( ) ] . tolist ( )
twitter_df_clean [ _STR_ ] = pd . to_datetime ( twitter_df_clean [ _STR_ ] )
tweet_image_clean . shape   $
train . shape [ 0 ] + new . shape [ 0 ]
( _STR_ ) . split ( _STR_ )
MICROSACC . plot_default ( microsaccades , subtype = _STR_ ) + ylab ( _STR_ )
restaurantsExcelFile = pd . ExcelFile ( _STR_ ) ;
s . asfreq ( _STR_ )
so_head . index
geometry = openmc . Geometry ( root_universe )
df_sample . shape
top_songs [ _STR_ ] . dtype
fin_p = pd . read_excel ( _STR_ ,   $ index_col = 0 , parse_dates = True )
tmp_df . sample ( 10 )
archive . sample ( 10 )
1.0 * sum ( treatment_df [ _STR_ ] == _STR_ ) / n_valid_users
draft_df . to_pickle ( _STR_ )
SELECTIONS = [ _STR_ ,   $ _STR_ ]   $ MOL2_FILE = _STR_   $ def data_processor ( mol2 ) :   $
from dotce . report import generate_chart
staff . T
data . describe ( ) . T
print len ( com311 )   $ com311 = pd . merge ( com311 , df , how = _STR_ , on = _STR_ )   $ print len ( com311 )
result = conn . execute ( select ( [ users . c . name , users . c . fullname ] ) )
rng = pd . date_range ( _STR_ , _STR_ , freq = _STR_ )   $ rng
build_cost_CO = cpq_CO . groupby ( [ _STR_ , _STR_ ] ) . size ( ) . reset_index ( ) . rename ( columns = { 0 : _STR_ } )   $ build_cost_TX = cpq_TX . groupby ( [ _STR_ , _STR_ ] ) . size ( ) . reset_index ( ) . rename ( columns = { 0 : _STR_ } )   $ build_cost_GA = cpq_GA . groupby ( [ _STR_ , _STR_ ] ) . size ( ) . reset_index ( ) . rename ( columns = { 0 : _STR_ } )
df . iloc [ 0 ]
commits_per_repo = pd . read_pickle ( _STR_ )   $ commits_per_repo . info ( )
logit_modB = sm . Logit ( df_new_log [ _STR_ ] , df_new_log [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ resultsB = logit_modB . fit ( )   $ resultsB . summary ( )
import geopandas as gpd   $ geoLineData = _STR_   $ newYork_gdf = gpd . read_file ( geoLineData )   $ print ( type ( newYork_gdf ) )   $ newYork_gdf . crs = { _STR_ : _STR_ }
ax = gamma_chart [ [ _STR_ , _STR_ , _STR_ ] ] . plot ( x = [ _STR_ ] , secondary_y = [ _STR_ ] , logx = True )   $ ax . set_ylabel ( _STR_ )   $ ax . right_ax . set_ylabel ( _STR_ ) ;
n_old = df2 . query ( _STR_ ) . count ( ) [ 0 ]   $ n_old
impressions = pd . read_sql_query ( _STR_ + RetSqlLimit ( _STR_ , sqlLimit ) , conn )   $ impressions_products = pd . read_sql_query ( _STR_ + RetSqlLimit ( _STR_ , sqlLimit ) , conn )   $ impressions [ _STR_ ] = impressions [ _STR_ ] . apply ( lambda x : x [ : 1 ] )   $ impressions [ _STR_ ] = impressions [ _STR_ ] . apply ( lambda x : x [ 2 : ] )
kfpd . plugin = DataSynthesizerPlugin ( mode = _STR_ )   $ fdf = time_method ( kfpd , verbose = True , rerun_query = False , repetitions = 10 )   $ fdf . head ( )
hm_clean . count ( )
df . converted . mean ( ) #returns average of a value
traded_volumes . sort ( )
twitter_final . groupby ( _STR_ ) [ _STR_ ] . count ( ) . reset_index ( name = _STR_ ) . sort_values ( by = _STR_ , ascending = False )
import matplotlib . pyplot as plt   $ from sklearn . metrics import roc_curve
meals = meals [ meals . id . isin ( tickets [ _STR_ ] . unique ( ) ) ]
num_rows = df . shape [ 0 ]   $ print ( _STR_ , num_rows )
df_vow . dtypes
hourly_df [ _STR_ ] . value_counts ( )
comment_list = sorted ( Counter ( commenters ) . items ( ) , key = lambda x : x [ 1 ] , reverse = True )
plt . hist ( x ) ;
students . weight . plot . hist ( )
X . shape
winter [ winter [ _STR_ ] == _STR_ ] [ _STR_ ] . count ( )
scoring_ind . info ( )
QLESQ = QLESQ [ ( QLESQ [ _STR_ ] == _STR_ ) & ( QLESQ [ _STR_ ] <= 7 ) ]   $ QLESQ . shape
df_loan3 = pd . DataFrame ( df_loan ) . merge ( actual_payments . loc [ actual_payments . iso_date == EOM_date . date ( ) , [ _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ ] ] ,   $ left_index = True , right_on = _STR_ )
df_characters . head ( 10 )
X_train , X_validation , y_train , y_validation = train_test_split ( X_train , y_train , test_size = 0.35 , shuffle = False )
c_df . dtypes
age . loc [ _STR_ ]
df [ df . msno == _STR_ ]
df . ID . dtype
sel_df . to_csv ( _STR_ )
merge [ merge . columns [ 45 ] ] . value_counts ( )   $
learn . fit ( lrs , 1 , wds = wd , cycle_len = 14 , use_clr = ( 32 , 10 ) )
if 0 == 1 :   $ news_title_docs_high_freq_words_df = pd . concat ( [ news_titles_sr , high_freq_words_sr ] , axis = 1 )   $ news_title_docs_high_freq_words_df . columns = [ _STR_ , _STR_ ]   $ news_title_docs_high_freq_words_df . to_pickle ( news_title_docs_high_freq_words_df_pkl )
cols = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ wlwellfeatures = pd . read_csv ( route + _STR_ , header = None ,   $ names = cols , delimiter = _STR_ , error_bad_lines = False )
image = image_soup . find ( _STR_ , class_ = _STR_ )   $ image_url = image . article [ _STR_ ]   $ url = image_url . split ( _STR_ ) [ - 1 ] . split ( _STR_ ) [ 0 ]   $ featured_image_url = _STR_ + _STR_ + url + _STR_   $ print ( featured_image_url )
test_df [ _STR_ ] = np . argmax ( pred , axis = 1 )
from IPython . display import HTML   $ import os   $ video_names = list ( filter ( lambda s : s . endswith ( _STR_ ) , os . listdir ( _STR_ ) ) )   $
df [ _STR_ ] #nos obtiene lacomunda llamada 'field1' $ df.loc[0,'field1']   #obtenemos el valor de la fila '0' y la  comunda 'field1' $ l=df['field1']>59  #se aplica la condicional para cada valor de la columna $ l $ df.loc[df['field1']>79,'field1'] #hacemos un filtro a la columna df.loc[boleano_o_int, 'name_columna'] $
weekly . head ( )
pystore . set_path ( _STR_ )   $ pystore . get_path ( )
df = pd . read_csv ( _STR_ ,   $ parse_dates = [ _STR_ ] ,   $ index_col = _STR_ )   $ df
model . compile ( loss = _STR_ , optimizer = _STR_ , metrics = [ _STR_ ] )
df [ _STR_ ] = vec2 . fit_transform ( df [ _STR_ ] )   $
X_test_tfidf = vectorizer . transform ( X_test )   $ y_pred_train = svr . predict ( X_train_tfidf )   $ y_pred_test = svr . predict ( X_test_tfidf )
b_cal_q1 . columns
autos . drop ( [ _STR_ , _STR_ , _STR_ ] , axis = 1 )
t2 [ _STR_ ] = t2 [ _STR_ ] . str . replace ( _STR_ , _STR_ )   $ t2 [ _STR_ ] = t2 [ _STR_ ] . str . replace ( _STR_ , _STR_ )   $ t2 [ _STR_ ] = t2 [ _STR_ ] . str . replace ( _STR_ , _STR_ )
p_new = len ( df2 . query ( _STR_ ) ) / len ( df2 . index )   $ p_new
twitter_archive_master [ _STR_ ] . value_counts ( )
ls_not_numeric = [ not pd . api . types . is_numeric_dtype ( dtype ) for dtype in df_onc_no_metac . dtypes ]   $ prog = re . compile ( _STR_ )   $ ls_not_date = [ not bool ( prog . search ( colname ) ) for colname in df_onc_no_metac . columns ]   $ ls_both = [ num and date for num , date in zip ( ls_not_numeric , ls_not_date ) ]   $ df_onc_no_metac . loc [ : , ls_both ] . nunique ( )
type ( x . iloc [ 0 ] [ _STR_ ] )
print ( _STR_ , donations [ _STR_ ] . std ( ) )   $ print ( _STR_ , donations [ _STR_ ] . var ( ) )   $ print ( _STR_ , donations [ _STR_ ] . quantile ( [ 0.25 , 0.50 , 0.75 ] ) )
final_ticker_data = avg_data . T
data [ _STR_ ] . plot ( )   $ plt . xlim ( _STR_ , _STR_ ) # change x-axis $ plt.ylim(60,80) # change y-axis $ plt.show() # will be mirrored if loaded $ data = data[(data.t > 519) & (data.t < 564)] $
r = df . rolling ( window = 3 , min_periods = 1 )   $ r [ _STR_ ] . aggregate ( np . sum )
auto . tail ( 15 )
act_p_new = df2 . query ( _STR_ ) [ _STR_ ] . mean ( )   $ act_p_old = df2 . query ( _STR_ ) [ _STR_ ] . mean ( )   $ act_diff = np . array ( act_p_new - act_p_old )   $ ( p_diffs > act_diff ) . mean ( )
import calendar   $ iowa [ _STR_ ] = iowa . Date . apply ( lambda x : calendar . day_name [ x . weekday ( ) ] )   $ total_sales_2015_by_day_of_week = iowa . groupby ( _STR_ ) [ _STR_ ] . sum ( )   $ total_sales_2015_by_day_of_week . plot ( kind = _STR_ )
df_new . rename ( columns = { _STR_ : _STR_ , _STR_ : _STR_ , _STR_ : _STR_ } , inplace = True )   $ df_new . head ( )
oppose . groupby ( [ _STR_ , _STR_ ] ) . amount . sum ( ) . reset_index ( ) . sort_values ( _STR_ , ascending = False ) . head ( 10 )
tweet_df . tweet_created_at . min ( )
avg_monthly_search_volumes = { _STR_ : 301000 , _STR_ : 246000 , _STR_ : 246000 , _STR_ : 165000 , _STR_ : 201000 , _STR_ : 150000 , _STR_ : 201000 , _STR_ : 110000 , _STR_ : 110000 , _STR_ : 500000 }   $ data_countries [ _STR_ ] = data_countries . apply ( lambda x : avg_monthly_search_volumes [ x . country_destination ] , axis = 1 )   $
out_columns = [ primary_temp_column ] + incremental_precip_columns + general_data_columns + wind_dir_columns #columns to include in output $ save_name=Glacier.lower()+ Station + "_15min_"+"LVL2.csv" #filename $ save_pth=os.path.join(save_dir, save_name)
to_be_predicted_Day2 = 34.59184648   $ predicted_new = ridge . predict ( to_be_predicted_Day2 )   $ predicted_new
df2 = df [ ( df . group == _STR_ ) & ( df . landing_page == _STR_ ) | ( df . group == _STR_ ) & ( df . landing_page == _STR_ ) ]   $ df2 . head ( )
for tweet in tweepy . Cursor ( api . search , q = _STR_ , count = 100 , lang = _STR_ , since = _STR_ ) . items ( ) :   $ print ( tweet . created_at , tweet . text )   $ analysis = tb ( tweet . text )   $ print ( analysis . sentiment . polarity )
display ( data . head ( 10 ) )
Pnew = df2 [ _STR_ ] . mean ( )   $ Pnew
pd . Period ( _STR_ , _STR_ )
compiled_data = pd . merge ( sen_dat , bot_data , how = _STR_ , on = [ _STR_ ] )   $ compiled_data = compiled_data . rename ( columns = { _STR_ : _STR_ } )   $ old_compiled_data = compiled_data   $ old_compiled_data = compiled_data
def find_k_worst ( y , y_hat , k = 10 ) :   $ return np . argsort ( y - y_hat ) [ : k ]
plt . style . use ( _STR_ )   $ bb [ _STR_ ] . apply ( rank_performance ) . value_counts ( ) . plot ( kind = _STR_ )
engine . execute ( _STR_ )
condos = condos [ condos . STATUS != _STR_ ]
aapl = np . log ( aapl ) - np . log ( aapl ) . shift ( 1 )   $ aapl . tail ( )
df_predictions [ _STR_ ] . value_counts ( )
ps . sort_index ( ) . tail ( 600 )
df_enhanced [ [ _STR_ ] ] . groupby ( [ _STR_ ] ) [ _STR_ ] . size ( )
_STR_ . rfind ( _STR_ )
followers = api . GetFollowers ( )   $ print ( [ u . screen_name for u in followers ] )
labeled_features . loc [ labeled_features [ _STR_ ] == _STR_ ] [ : 16 ]
with open ( _STR_ ) as f :   $ print ( f . read ( ) )
data = aapl . get_call_data ( expiry = aapl . expiry_dates [ 4 ] )   $ data . iloc [ 0 : 5 : , 0 : 5 ]
plt . scatter ( sing_fam . sqft . values , sing_fam . rp1lndval . values ) ;
mask = ( a + b ) . isnull ( )   $ mask
os . mkdir ( _STR_ )
store_items . fillna ( 0 )
( _train . count ( ) + _test . count ( ) ) == train . count ( )
! wget - nH - r - np - P { PATH } http : // files . fast . ai / models / wt103 /
mb = pd . read_table ( _STR_ , sep = _STR_ )
countries_df = pd . read_csv ( _STR_ )   $ countries_df . head ( ) # we firstly see that we need to match the user_id $
[ [ f . filename , f . file_size ] for f in my_zip . filelist ]
figure . set_ylabel ( _STR_ ) ;
df_countries = pd . read_csv ( _STR_ )   $ df_countries . head ( )
logit_mod_1 = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ results = logit_mod_1 . fit ( )   $ results . summary ( )
bird_data . date_time . head ( )
pd . DataFrame ( { _STR_ : [ x [ 1 ] for x in HARVEY_92_USERS_AC ] } ) . hist ( bins = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 10 , 100 , 200 , 1000 ] )
clean_predictions . columns
sub = pd . DataFrame ( )   $ sub [ _STR_ ] = df [ _STR_ ]   $ for label in [ _STR_ , _STR_ , _STR_ ] :   $ sub [ label ] = y [ : , labels2idx [ label ] ]   $ sub . to_csv ( _STR_ , index = False )
pd . Series ( np . random . randn ( 1000 ) ) . plot ( kind = _STR_ , bins = 20 , color = _STR_ ) ;
to_be_predicted_Day5 = 82.26274693   $ predicted_new = ridge . predict ( to_be_predicted_Day5 )   $ predicted_new
params = { _STR_ : [ 6 , 6 ] , _STR_ : _STR_ , _STR_ : 12.0 , _STR_ : 2 }   $ plot_partial_autocorrelation ( doc_duration , params = params , lags = 30 , alpha = 0.05 ,   \   $ title = _STR_ )
datetime . now ( )
plt . violinplot ( resampled1_groups [ _STR_ ] )
df . head ( )
Customer . withdraw ( jeff , 200.0 )   $ jeff . balance # Shows 700.0
crf . fit ( X_train , Y_train )   $ train_score = crf . score ( X_train , Y_train )   $ print ( _STR_ % ( train_score ) )   $ test_score = crf . score ( X_test , Y_test )   $ print ( _STR_ % ( test_score ) )
imagelist   $ for k in range ( len ( imagelist ) ) :   $ print ( _STR_ + _STR_ + imagelist [ k ] [ : - 4 ] + _STR_ + _STR_ + _STR_ + _STR_ + imagelist [ k ] + _STR_ )
nps . crs
sub_df . shape
tweets [ _STR_ ] . value_counts ( dropna = False )   $ tweets [ _STR_ ] . value_counts ( dropna = False )
df . plot ( kind = _STR_ )   $ plt . ylabel ( _STR_ )
small_movies_file = os . path . join ( dataset , _STR_ , _STR_ )   $ small_movies_raw_data , small_movies_raw_data_header = read_file ( small_movies_file )   $
lst = [ 5 , 1 , 6 , 7 , 4 , 2 , 3 ]   $ mean_for_each_weekday [ _STR_ ] = pd . Series ( lst , index = mean_for_each_weekday . index )   $ mean_for_each_weekday
df_plot_gene = pd . DataFrame ( df_gene [ _STR_ ] . value_counts ( ) )   $ from pylab import rcParams   $ rcParams [ _STR_ ] = 10 , 10   $ ax = df_plot_gene . plot ( style = _STR_ , markevery = 5 , figsize = ( 10 , 7 ) )
ab_data . landing_page . value_counts ( )
previous_month_date = end_date - timedelta ( days = 30 )   $ pr = PullRequests ( github_index ) . get_cardinality ( _STR_ ) . since ( start = previous_month_date ) . until ( end = end_date )   $ get_aggs ( pr )
pivoted = departures . pivot_table ( index = _STR_ , columns = _STR_ , values = _STR_ )
round ( df2 [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] . mean ( ) , 4 )
doc = ao18_qual_coll . find_one ( { _STR_ : _STR_ } )
own_star = pd . merge ( owns , stars , how = _STR_ , on = [ _STR_ , _STR_ ] , suffixes = ( _STR_ , _STR_ ) )   $ own_star . info ( )   $ own_star . head ( )
from scipy import misc   $ image = misc . imread ( _STR_ )
test = test . reset_index ( drop = True )   $ test [ _STR_ ] = pd . Series ( predictions )   $ datatest = pd . concat ( [ train , test ] )   $ datatest = datatest . reset_index ( drop = True )
df [ unique_cols ] . head ( 2 )
with open ( _STR_ , _STR_ ) as f :   $ clixo = json . load ( f )   $ nodes = clixo [ _STR_ ] [ _STR_ ]   $ len ( nodes )
maint [ _STR_ ] = pd . to_datetime ( maint [ _STR_ ] , format = _STR_ )   $ maint [ _STR_ ] = maint [ _STR_ ] . astype ( _STR_ )   $ print ( _STR_ . format ( len ( maint . index ) ) )   $ maint . head ( )
cityID = _STR_   $ for tweet in tweepy . Cursor ( api . search , q = q , contained_within = cityID , wait_on_rate_limit = True , lang = _STR_ ) . items ( 500 ) :         $ San_Antonio . append ( tweet )
results . to_csv ( _STR_ )   $ results . to_pickle ( _STR_ )   $ pd . read_pickle ( _STR_ )
baseball . rank ( ascending = False ) . head ( )
train_data [ _STR_ ] = train_data [ _STR_ ] . apply ( get_integer4 )   $ test_data [ _STR_ ] = test_data [ _STR_ ] . apply ( get_integer4 )   $ del train_data [ _STR_ ]   $ del test_data [ _STR_ ]
import statsmodels . api as sm   $ convert_old = sum ( df2 . query ( _STR_ ) [ _STR_ ] )   $ convert_new = sum ( df2 . query ( _STR_ ) [ _STR_ ] )   $ n_old = df2 . query ( _STR_ ) . shape [ 0 ]   $ n_new = df2 . query ( _STR_ ) . shape [ 0 ]
historicFollowers . head ( )
itemDataWithInt = itemData . join ( itemDataWithIntListIDs , on = _STR_ , how = _STR_ )   $ print itemDataWithInt . shape   $ itemDataWithInt . head ( )
ypred = model . predict ( np . random . random ( ( 300 , 95 , 1 ) ) )
pf = pd . read_csv ( _STR_ )   $ for plyr in inj :   $ pf = pf [ pf . Name != plyr ]   $ pf . head ( )
( datecomp [ _STR_ ] == datecomp [ _STR_ ] ) . value_counts ( )
r [ r [ _STR_ ] == 0.5 ]
data_issues = pd . read_json ( _STR_ , lines = True )
pres_df [ _STR_ ] . dtypes
df [ _STR_ ] = df . amount_initial . str . replace ( _STR_ , _STR_ )   $ df [ _STR_ ] = df . amount_cleanup . str . replace ( _STR_ , _STR_ )   $ df [ _STR_ ] = df . amount_cleanup . astype ( float )
bwd_train = roll_train_df . rolling ( 7 , min_periods = 1 ) . sum ( )   $ bwd_test = roll_test_df . rolling ( 7 , min_periods = 1 ) . sum ( )
regression_line = [ ]   $ for x in xs :   $ regression_line . append ( ( m * x ) + b )
van_final . apply ( lambda x : sum ( x . isna ( ) ) )
for c in [ _STR_ , _STR_ , _STR_ ] :   $ airbnb_df [ c ] = pd . to_datetime ( airbnb_df [ c ] )   $
dummies_country = pd . get_dummies ( df2 [ _STR_ ] )   $ dummies_country . head ( )
train . first_affiliate_tracked . isnull ( ) . sum ( )
train , validation = train_test_split ( df_train , test_size = 0.1 )
df2 = df . loc [ ~ ( ( df . group == _STR_ ) & ( df . landing_page == _STR_ ) ) , : ] . copy ( )   $ df2 = df2 . loc [ ~ ( ( df2 . group == _STR_ ) & ( df2 . landing_page == _STR_ ) ) , : ]   $ df2 . shape
print ( _STR_ )   $ rfc_imp = rfc . feature_importances_   $ features_names = var_num + var_cat   $ for i in range ( 0 , len ( rfc_imp ) ) :   $ print ( features_names [ i ] + _STR_ % _STR_ . format ( rfc_imp [ i ] ) )
df = pd . DataFrame ( { _STR_ : date_list , _STR_ : tmin , _STR_ : tavg , _STR_ : tmax } )
with open ( saem_women_save , mode = _STR_ , encoding = _STR_ ) as f :   $ f . write ( SAEMRequest . text )
mapping = pd . read_excel ( _STR_ )   $ mapping [ _STR_ ] = mapping [ _STR_ ] . astype ( str )   $ all_data [ _STR_ ] = all_data [ _STR_ ] . astype ( str )   $ mapping . rename ( columns = { _STR_ : _STR_ } , inplace = True )   $
grades_array = np . array ( [ [ 8 , 8 , 9 ] , [ 10 , 9 , 9 ] , [ 4 , 8 , 2 ] , [ 9 , 10 , 10 ] ] )   $ grades = pd . DataFrame ( grades_array , columns = [ _STR_ , _STR_ , _STR_ ] , index = [ _STR_ , _STR_ , _STR_ , _STR_ ] )   $ grades
LARGE_GRID . plot_accuracy ( raw_large_grid_df , option = _STR_ )
data . text . str . contains ( _STR_ ) . resample ( _STR_ ) . sum ( ) . plot ( )   $ data . text . str . contains ( _STR_ ) . resample ( _STR_ ) . sum ( ) . plot ( )   $ data . text . str . contains ( _STR_ ) . resample ( _STR_ ) . sum ( ) . plot ( )   $ sns . plt . legend ( [ _STR_ , _STR_ , _STR_ ] )
def sinplot ( flip = 1 ) :   $ x = np . linspace ( 0 , 14 , 100 )   $ for i in range ( 1 , 7 ) :   $ plt . plot ( x , np . sin ( x + i * .5 ) * ( 7 - i ) * flip )
from bs4 import BeautifulSoup   $ example1 = BeautifulSoup ( df . text [ 279 ] , _STR_ )   $ print example1 . get_text ( )
price_data . iloc [ 0 : 5 ]
print ( [ 0.25 , 0.5 , 0.75 , 1 ] )
stories = pd . concat ( [ stories . drop ( [ _STR_ ] , axis = 1 ) ,   $ user_df ] , axis = 1 )
index_df = index_ts [ [ _STR_ , _STR_ , _STR_ ] ] . copy ( ) . set_index ( _STR_ )   $ index_df . head ( )
ratio = result [ _STR_ ] . div ( result [ _STR_ ] )   $ ratio . sort_values ( ascending = False , inplace = True )   $ ratio   $
print ( df . index [ 0 ] )
df = pd . read_csv ( _STR_ )   $ df [ _STR_ ] = pd . to_datetime ( df [ _STR_ ] )   $ df [ _STR_ ] = pd . to_datetime ( df [ _STR_ ] )   $ df . dtypes
van_final = pd . merge ( van , pages , how = _STR_ , on = _STR_ )   $ ben_final = pd . merge ( ben , pages , how = _STR_ , on = _STR_ )   $ van_final . head ( 10 )   $ ben_final . head ( 10 )
def false_positive ( predicted_pivot , test ) :   $ predicted_table = pd . DataFrame ( predicted_pivot . unstack ( ) ) . groupby ( [ _STR_ , _STR_ ] ) . sum ( ) . reset_index ( )   $ predicted_item = predicted_table [ predicted_table [ 0 ] == 1 ]   $ upper = pd . merge ( predicted_item , test , how = _STR_ , on = [ _STR_ , _STR_ ] )   $ return sum ( np . isnan ( upper [ _STR_ ] ) )
df . loc [ df . userTimezone == _STR_ , : ]
df2 = df . query ( _STR_ )
train . cust_id . value_counts ( ) . unique ( )
dates = pd . date_range ( _STR_ , periods = ndays ) . astype ( _STR_ )   $ sim_ret = pd . DataFrame ( sigma * np . random . randn ( ndays , nscen ) + mu , index = dates )   $ sim_closes = ( closes_aapl . iloc [ - 1 ] . AAPL ) * np . exp ( sim_ret . cumsum ( ) )
df_stars [ _STR_ ] . nunique ( ) , df_stars [ _STR_ ] . nunique ( )
subwaydf . iloc [ 154900 : 154915 ] #this low number seems to be because entries and exits resets
df = pd . read_csv ( _STR_ )   $ df . head ( )   $
df = df . loc [ : , [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ]
df_image . info ( )
from IPython . display import SVG   $ from keras . utils . vis_utils import model_to_dot   $ SVG ( model_to_dot ( encoder_model_inference ) . create ( prog = _STR_ , format = _STR_ ) )
df . shape
df . loc [ [ 101 , 103 , 105 ] ]
questions = questions . drop ( questions . index [ [ 9 , 22 ] ] )
event_list = pd . DataFrame ( )   $ event_list [ _STR_ ] = df_events . event_id   $ event_list [ _STR_ ] = df_events . event_start_at   $ event_list
TotalNewPage = df2 . query ( _STR_ ) [ _STR_ ] . count ( )   $ print ( _STR_ , ( TotalNewPage / NewTotalUser ) )
userByCountry_df = youtube_df [ youtube_df [ _STR_ ] . isin ( channel_namesC ) ]
recipes . shape
codes . show ( )
march_2016 = pd . Period ( _STR_ , freq = _STR_ )
tweets_master_df [ tweets_master_df . isnull ( ) . any ( axis = 1 ) ]
reviews . price . astype ( str )
hist = plt . hist ( rf . predict_proba ( validation_data [ Features ] ) [ : , 1 ] , bins = 100 )   $ plt . title ( _STR_ , y = 1.02 , fontsize = 15 )   $ plt . xlabel ( _STR_ , fontsize = 12 )   $ plt . ylabel ( _STR_ , fontsize = 12 )
all_tables_df . OBJECT_NAME
dfz [ _STR_ ] . plot ( color = _STR_ , label = _STR_ )   $ dfz [ _STR_ ] . plot ( color = _STR_ , label = _STR_ )
oil_prices = joined [ [ _STR_ , _STR_ ] ] . copy ( ) . drop_duplicates ( [ _STR_ ] ) . reset_index ( drop = True )   $ oil_prices [ _STR_ ] = oil_prices [ _STR_ ] . pct_change ( periods = 30 ) . fillna ( method = _STR_ )   $ oil_prices [ [ _STR_ , _STR_ ] ] . plot ( )
writer = pd . ExcelWriter ( _STR_ )
store = join_df ( store , store_states , _STR_ )   $ len ( store [ store . State . isnull ( ) ] )
pattern = _STR_   $ matchresult = re . match ( pattern , _STR_ )
def intermediate_cohort ( row ) :   $ if row [ _STR_ ] == _STR_ and row [ _STR_ ] == _STR_ :   $ row [ _STR_ ] = _STR_   $ return row [ _STR_ ]   $ df_users_6 [ _STR_ ] = df_users_6 . apply ( intermediate_cohort , axis = 1 )
df_cond = df . dropna ( subset = [ _STR_ ] )
for feature in sorted_features :   $ importance = feature [ 1 ]   $ print feature [ 0 ]   $ print X . columns [ feature [ 0 ] + 5 ] , importance
for i in compare [ - 1 ] :   $ print ( _STR_ , i )   $ print ( _STR_ , data . view_count [ i ] )   $ data . view_count [ i ] = data . suma [ i ]   $ print ( _STR_ , data . view_count [ i ] )
Feature1 . drop ( [ _STR_ ] , axis = 1 , inplace = True )   $ Feature1 . head ( )
df_M7 = pd . DataFrame ( { _STR_ : dfM . DATE [ 13 : - 12 ] , _STR_ : [ i [ 0 ] for i in M7_actual ] ,   $ _STR_ : [ i [ 0 ] for i in M7_pred ] ,   $ _STR_ : dfM . t [ 13 : - 12 ] } )
dfcopy = df . copy ( )   $ dfcopy . sort_values ( _STR_ ) . head ( )
crime = pd . read_csv ( _STR_ )   $ moon = pd . read_csv ( _STR_ )
autos [ _STR_ ] = autos [ _STR_ ] . astype ( int )   $ autos . rename ( columns = { _STR_ : _STR_ } , inplace = True )   $ autos [ _STR_ ] . head ( )
corpus = matutils . Sparse2Corpus ( counts )
pothole = df [ df [ _STR_ ] == _STR_ ]   $ pothole . groupby ( pothole . index . weekday ) [ _STR_ ] . count ( ) . plot ( )
crimes_by_yr_month_type . reset_index ( inplace = True )
staff [ _STR_ ] = staff . index   $ staff
def stack_unstack ( df , col_target ) :   $ return ( df [ col_target ] . apply ( pd . Series )   $ . stack ( )   $ . reset_index ( level = 1 , drop = True )   $ . to_frame ( col_target ) )
sum ( df . rating_denominator . value_counts ( ) != 10 )
autos [ _STR_ ] . describe ( )
desktop = nvidia . filter ( lambda p : not devices [ int ( p [ _STR_ ] [ _STR_ ] , 16 ) ] . endswith ( _STR_ ) )   $ desktop . map ( lambda p : len ( p [ _STR_ ] ) ) . countByValue ( )
q = tf . distributions . Normal ( loc = q_mu , scale = q_sigma )   $ p = tf . distributions . Normal ( loc = p_mu , scale = p_sigma )
typeof_city = pd . Series ( total_ridepercity [ _STR_ ] )   $ typeby_city = pd . Series ( total_ridepercity [ _STR_ ] )   $ colors = [ _STR_ , _STR_ , _STR_ ]   $ explode = ( 0.1 , 0 , 0 )
RIDs_DXSUM = list ( diagnosis_DXSUM_PDXCONV_ADNIALL [ _STR_ ] . unique ( ) )   $ RIDs_ADSXLIST = list ( diagnosis_ADSXLIST [ _STR_ ] . unique ( ) )   $ RIDs_BLCHANGE = list ( diagnosis_BLCHANGE [ _STR_ ] . unique ( ) )
plt . plot ( pipe . tracking_error )
print ( autos [ _STR_ ] . unique ( ) . shape )   $ print ( autos [ _STR_ ] . describe ( ) )   $ print ( autos [ _STR_ ] . value_counts ( ) . sort_index ( ascending = True ) )
f1_score ( Y_valid , final_valid_pred_nbsvm1 , average = _STR_ , labels = np . unique ( final_valid_pred_nbsvm1 . values ) )
my_date_only_rows = autos [ _STR_ ] . str [ : 10 ] #strip first 9 characters $ my_date_only_rows $
print ( _STR_ % ( df2 . query ( _STR_ ) . shape [ 0 ] / df2 . shape [ 0 ] ) )
s . iloc [ - 3 : ]
df . head ( )
cityID = _STR_   $ for tweet in tweepy . Cursor ( api . search , q = q , contained_within = cityID , wait_on_rate_limit = True , lang = _STR_ ) . items ( 500 ) :         $ Chesapeake . append ( tweet )
plt . hist ( null_vals )   $ plt . axvline ( x = obs_diff , color = _STR_ ) ;
df = pd . read_pickle ( _STR_ )
df . sort_values ( _STR_ ) . head ( 5 )
my_df_loaded = pd . read_csv ( _STR_ , index_col = 0 )   $ my_df_loaded
autos [ _STR_ ] = autos [ _STR_ ] . str [ : 10 ]   $ autos [ _STR_ ] . value_counts ( normalize = True , dropna = False ) . sort_index ( )
df2 [ _STR_ ] = 1   $ df2 [ _STR_ ] = pd . get_dummies ( df2 [ _STR_ ] ) [ _STR_ ]   $ df2 [ [ _STR_ , _STR_ ] ] = pd . get_dummies ( df2 [ _STR_ ] )   $ df2 . head ( )
df_tweet_clean . tweet_id = df_tweet_clean . tweet_id . astype ( str )
ser4 = pd . Series ( dic_a , index = [ _STR_ , _STR_ , _STR_ , _STR_ ] )   $ ser4
prcp_1_df = pd . DataFrame ( prcp_data , columns = [ _STR_ , _STR_ ] )   $ prcp_1_df . set_index ( _STR_ , inplace = True ) # Set the index by date $ prcp_1_df.count()
df_goog . loc [ ( df_goog . index . year == 2015 ) ]
! python csv_to_tfrecords . py - - csv_input = images / test / test_labels . csv - - image_dir = images / test - - output_path = test . record
from sklearn . linear_model import Ridge   $ ridge = Ridge ( alpha = 26500 )   $ ridge . fit ( X_train_std , y_train )   $ print ( _STR_ % np . mean ( ( ridge . predict ( X_test_std ) - y_test ) ** 2 ) )   $ print ( _STR_ , ridge . score ( X_test_std , y_test ) )
df_tweets . reset_index ( ) . to_pickle ( _STR_ )
auth = tweepy . OAuthHandler ( consumer_key , consumer_secret )   $ auth . set_access_token ( access_token , access_token_secret )   $ api = tweepy . API ( auth , parser = tweepy . parsers . JSONParser ( ) )
assert isinstance ( my_zip , zipfile . ZipFile )   $ assert isinstance ( list_names , list )   $ assert all ( [ isinstance ( file , str ) for file in list_names ] )   $
subs_and_comments = sub_df . merge ( sub_comments , on = _STR_ , how = _STR_ )
re . findall ( _STR_ , cfd_index [ _STR_ ] [ 10 ] )
users . body [ _STR_ ]
item_item_rec = graphlab . recommender . item_similarity_recommender . create ( sf_stars ,   $ user_id = _STR_ ,   $ item_id = _STR_ ,   $ target = _STR_ )
print ( _STR_ \   $ , len ( nltk . corpus . brown . words ( ) ) / len ( nltk . corpus . brown . sents ( ) ) )
result = pd . DataFrame ( alg7 . predict_proba ( test [ features ] ) , index = test . index , columns = alg7 . classes_ )   $ result . insert ( 0 , _STR_ , mid )   $ result . head ( )
s = pd . Series ( [ 80 , 2 , 50 ] , index = [ _STR_ , _STR_ , _STR_ ] )   $ print s
my_gempro . get_dssp_annotations ( )
y = np . log1p ( y )   $ y . hist ( grid = True , bins = 50 )
combined_df . to_csv ( _STR_ , index = False )
May = df [ _STR_ ]   $ May [ _STR_ ] . value_counts ( ) . head ( 5 )
num_var = data_type [ data_type != _STR_ ] . index . tolist ( )   $ num_var . remove ( _STR_ )   $ print len ( num_var )
first_commit_timestamp = git_log . iloc [ - 1 ] . timestamp   $ last_commit_timestamp = pd . to_datetime ( _STR_ )   $ corrected_log = git_log [ ( first_commit_timestamp <= git_log . timestamp ) & ( git_log . timestamp <= last_commit_timestamp ) ]   $ corrected_log [ _STR_ ] . describe ( )
investors_df = pd . merge ( investors_df , most_recent_investment , on = _STR_ )
apple [ _STR_ ] . value_counts ( )
typesub2017 = typesub2017 . dropna ( )
population . loc [ _STR_ : _STR_ ]
perf_test = pd . read_csv ( _STR_ )   $ print ( _STR_ , perf_test . shape )   $ perf_test . head ( 50 )
News_paragraphs = Mars_soup . find ( _STR_ , class_ = _STR_ ) . text   $ print ( News_paragraphs )
number_of_commits = git_log [ _STR_ ] . size   $ number_of_authors = git_log . dropna ( how = _STR_ ) [ _STR_ ] . unique ( ) . size   $ print ( _STR_ % ( number_of_authors , number_of_commits ) )
zscore_fun_improved = lambda x : ( x - x . rolling ( window = 200 , min_periods = 20 ) . mean ( ) ) / x . rolling ( window = 200 , min_periods = 20 ) . std ( )   $ features [ _STR_ ] = prices . groupby ( level = _STR_ ) . close . apply ( zscore_fun_improved )   $ features . f10 . unstack ( ) . plot . kde ( title = _STR_ )
rodelar . editCount ( )
win_rows = inputs [ _STR_ ] . sum ( )   $ total_rows = inputs [ _STR_ ] . size   $ perc_wins = win_rows / total_rows * 100   $ print ( _STR_ . format ( win_rows , perc_wins , total_rows ) )
xmlData [ _STR_ ] = pd . to_datetime ( xmlData [ _STR_ ] , format = _STR_ , errors = _STR_ )   $ xmlData [ _STR_ ] . replace ( { _STR_ : _STR_ } , inplace = True )   $ xmlData [ _STR_ ] = pd . to_datetime ( xmlData [ _STR_ ] , format = _STR_ , errors = _STR_ )   $ xmlData [ _STR_ ] = [ d . strftime ( _STR_ ) if not pd . isnull ( d ) else _STR_ for d in xmlData [ _STR_ ] ]   $ xmlData [ _STR_ ] = [ d . strftime ( _STR_ ) if not pd . isnull ( d ) else _STR_ for d in xmlData [ _STR_ ] ]
data . groupby ( _STR_ ) . mean ( )   $
df30458 = df [ df [ _STR_ ] == _STR_ ] #create df with only rows that have 30458 as bikeid $ x = df30458.groupby('end_station_name').count() $ x.sort_values(by= 'tripduration', ascending = False).head() # we use tripduration as a proxy to sort the values $
pr ( _STR_ )   $ tw = pd . read_csv ( filename , sep = _STR_ , encoding = _STR_ , escapechar = _STR_ , names = columns_header ,   $ quoting = csv . QUOTE_NONE , na_values = _STR_ , header = None )   $ pr ( _STR_ )
closing_prices = [ ]   $ for ele in r . json ( ) [ _STR_ ] [ _STR_ ] :   $ closing_prices . append ( ele [ 4 ] )   $ print ( _STR_ . format ( max ( closing_prices ) - min ( closing_prices ) ) )
conn . execute ( _STR_ )
samples_query . display_records ( 10 )
vars2 = [ x for x in dfa . ix [ : , 6 : 54 ] ]   $ vars2
import statsmodels . api as sm   $ log_reg = sm . Logit ( df [ _STR_ ] , df [ [ _STR_ , _STR_ ] ] )
corpus_tf = corpora . MmCorpus ( os . path . join ( outputs , _STR_ ) )   $ corpus_tfidf = corpora . MmCorpus ( os . path . join ( outputs , _STR_ ) )
import datetime as dt   $ Todays_date = dt . date . today ( ) - dt . timedelta ( days = 365 )   $ print ( _STR_ , Todays_date )
import os # To use command line like instructions $ if not os.path.exists(DirSaveOutput): os.makedirs(DirSaveOutput)
df_click . info ( )
df_train . loc [ : , _STR_ ] = df_train . apply ( lambda row : tweet_to_features ( row [ _STR_ ] ) , axis = 1 )   $ display ( df_train . head ( ) )
df2 [ ( ( df2 [ _STR_ ] == _STR_ ) == ( df2 [ _STR_ ] == _STR_ ) ) == False ] . shape [ 0 ]
df_new [ _STR_ ] . value_counts ( )
origin = str ( output_path )   $ dest = Path ( temp_dir . name , _STR_ )   $   ! bigmatrix_repack - - matrix - chunkshape _STR_ - - force   $ output_path   $ dest
plt . plot ( xs , ys )
path = _STR_   $ mydata = pd . read_csv ( path , sep = _STR_ , header = None )   $ mydata . head ( 5 )
sqlContext . sql ( _STR_ ) . toPandas ( )
html = browser . html   $ weather_soup = BeautifulSoup ( html , _STR_ )
sort = df . sort_values ( _STR_ , ascending = False , axis = 0 ) [ : 25 ]   $ sort . loc [ : , [ _STR_ , _STR_ , _STR_ ] ] [ : 5 ]
print ( len ( train_df [ _STR_ ] . unique ( ) ) )
autos [ _STR_ ] . describe ( )
plot_chernoff_data ( df2 , 1e-6 , 1 , _STR_ )   $ plt . savefig ( _STR_ , bbox_inches = _STR_ )
url = _STR_   $ response = requests . get ( url )   $ with open ( _STR_ , _STR_ ) as file :   $ file . write ( response . content )
actual_value_second_measure = pd . DataFrame ( actual_value_second_measure )   $ actual_value_second_measure . replace ( 2 , 1 , inplace = True )
ac [ _STR_ ] . describe ( )
fig , ax = plt . subplots ( )   $ typesub2017 [ _STR_ ] . plot ( ax = ax , title = _STR_ , lw = 0.7 )   $ ax . set_ylabel ( _STR_ )   $ ax . set_xlabel ( _STR_ )
df_data_1 . dtypes
from sklearn . neighbors import KNeighborsClassifier   $ knn = KNeighborsClassifier ( n_neighbors = 101 )   $ knn . fit ( x_train , y_train > 0 )
df . groupby ( _STR_ ) [ _STR_ ] . min ( )
Xs = df_sample . values
auto_new . CarModel . value_counts ( )
reviews [ _STR_ ] = 0   $ reviews . groupby ( [ reviews . country , reviews . variety ] ) . n . count ( ) . sort_values ( ascending = False )
print ( df . info ( ) )   $ pd . isna ( df ) . sum ( )   $
SANDAG_age_df [ SANDAG_age_df [ _STR_ ] == _STR_ ] . head ( )
smpl . count ( )
run txt2pdf . py - o _STR_ _STR_
npath = save_filepath + _STR_   $ hs . addContentToExistingResource ( resource_id , [ npath ] )
event_sdf = events_grouped_no_nulls . sort ( events_grouped_no_nulls [ _STR_ ] . desc ( ) )
from sklearn . cluster import AffinityPropagation , MeanShift , KMeans   $ from sklearn . manifold import TSNE
if True :   $ train = train . round ( { _STR_ : 2 , _STR_ : 2 , _STR_ : 2 , _STR_ : 2 } )   $ test = test . round ( { _STR_ : 2 , _STR_ : 2 , _STR_ : 2 , _STR_ : 2 } )
youthUser1 . head ( )
sql . sql ( _STR_ ) . collect ( )
print weather_data1 . shape   $ print weather_data1 . columns . values
n_old = len ( df2 . query ( _STR_ ) )   $ n_old
plt . hist ( newshows [ _STR_ ] )   $ plt . title ( _STR_ )   $ plt . ylabel ( _STR_ )   $ plt . xlabel ( _STR_ )
closes = pd . concat ( [ msftA01 [ : 3 ] , aaplA01 [ : 3 ] ] , keys = [ _STR_ , _STR_ ] )   $ closes
print ( _STR_ . format ( len ( my_followers ) ) )
user . query ( _STR_ ) . head ( 3 )
_pred = rnd_search_cv . best_estimator_ . predict ( X_train_scaled )   $ accuracy_score ( y_train , y_pred )
tzs = tweets_df [ _STR_ ] . value_counts ( ) [ : 10 ]   $ print ( tzs )   $
sessions_sample = pd . read_csv ( _STR_ , skiprows = skip_idx )
seq2seq_inf . demo_model_predictions ( n = 50 , issue_df = testdf )
my_df_small_T = my_df_small . T   $ my_df_large_T = my_df_large . T
plt . bar ( [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ] , monthly_percent_off )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )
hrefs = tree . xpath ( _STR_ )   $ for href in hrefs :   $ print ( href . text )     $ print ( href . attrib )
ts = pd . Series ( np . random . randn ( 3 ) , dates )   $ ts
supertrimmed = prune_below_degree ( tweetnet , 500 )   $ nx . info ( supertrimmed )   $ supertrimmed_degrees = supertrimmed . degree ( )   $ plt . hist ( supertrimmed_degrees . values ( ) , 100 ) #Display histogram of node degrees in 100 bins
from datetime import datetime   $ from dateutil import parser   $ ind_date = pd . bdate_range ( _STR_ , _STR_ )
def train_classifier ( X_train , y_train , X_test , y_test , classifier ) :   $ train_features = CountVectorizer ( tokenizer = tokenize_and_stem ) . fit_transform ( X_train )   $ classifier . fit ( train_features_tokenized , y_train )   $ return classifier
tweet_data_clean . head ( )
df2 . head ( )
pd . date_range ( start = _STR_ , end = _STR_ , freq = be )
print ( trump . favorite_count . mean ( ) )   $ print ( trump . retweet_count . mean ( ) )
sort_a_asc = noNulls [ _STR_ ] . asc ( )
ethc = ethc [ np . isfinite ( ethc [ _STR_ ] ) ]   $ ethc . info ( )
pd . Series ( np . random . randn ( 5 ) )
np . random . randn ( 6 , 4 )   $
locations = session . query ( Measurement . station , Station . name , func . count ( Measurement . tobs ) ) . \   $ filter ( Measurement . station == Station . station ) . group_by ( Measurement . station ) . order_by ( func . count ( Measurement . tobs ) . desc ( ) ) . all ( )
display ( _STR_ , _STR_ , _STR_ )
df = pd . concat ( [ df1 , df2 [ df2 [ _STR_ ] == _STR_ ] ] )
senateAll . to_csv ( _STR_ )   $ houseAll . to_csv ( _STR_ )
df2 = df2 . drop_duplicates ( _STR_ , keep = _STR_ )
df_errors = pd . DataFrame ( new_errors , columns = [ _STR_ ] )
preprocessor . infer_subtypes ( ) # this function tries to indentify different subtypes of data
m = md . get_learner ( emb_szs , len ( df . columns ) - len ( cat_vars ) ,   $ 0.04 , 1 , [ 1000 , 500 ] , [ 0.001 , 0.01 ] , y_range = y_range )   $ lr = 1e-3
learn . sched . plot_loss ( )
scaler = StandardScaler ( )   $ scaler . fit ( x_train )
articles_by_pub = db . get_sql ( sql )
final . tail ( 3 )
df_data = df_data . drop_duplicates ( df_data . columns . difference ( [ _STR_ ] ) )   $ print _STR_ % len ( df_data )
we_rate_dogs . shape [ 0 ] - clean_rates . shape [ 0 ]
top_songs [ _STR_ ] . isnull ( ) . sum ( )
df . loc [ _STR_ ]
treatment_num = df2 . query ( _STR_ ) . shape [ 0 ]   $ treatment_num
names = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ df_new . timestamp = pd . to_datetime ( df_new . timestamp )   $ df3 = df_new . join ( pd . get_dummies ( df_new . timestamp . dt . weekday_name )   $ . set_index ( df_new . index ) . reindex ( columns = names , fill_value = 0 ) )   $ df3 . head ( )
sub_gene_logical_vector = df . source . isin ( [ _STR_ , _STR_ , _STR_ ] )   $ sub_gene_df = df [ sub_gene_logical_vector ]   $ sub_gene_df . shape
df_l_s = df . sample ( n = 2000 )
ab_new [ _STR_ ] . value_counts ( )   $
trunc_df [ trunc_df . company_name == _STR_ ]
session_summaries = [ ]   $ for summary_loc in fr5_session_summary_locations :   $ summary = FRStimSessionSummary . from_hdf ( summary_loc )   $ session_summaries . append ( summary )
to_be_predicted_Day2 = 55.13755314   $ predicted_new = ridge . predict ( to_be_predicted_Day2 )   $ predicted_new
subreddit = reddit . subreddit ( _STR_ )   $ for submission in reddit . subreddit ( _STR_ ) . hot ( limit = 25 ) :   $ print ( submission . title )
autos [ [ _STR_ ,   $ _STR_ ,   $ _STR_ ,   $ _STR_ , _STR_ ] ] . describe ( include = _STR_ )
X_extra . describe ( include = [ _STR_ ] )
[ ( s . user . name , s . text ) for s in retweets_of_me [ : 5 ] ]
from sklearn . linear_model import Ridge   $ ridge = Ridge ( alpha = 24000 )   $ ridge . fit ( X_train_std , y_train )   $ print ( _STR_ % np . mean ( ( ridge . predict ( X_test_std ) - y_test ) ** 2 ) )   $ print ( _STR_ , ridge . score ( X_test_std , y_test ) )
lq . isnull ( ) . sum ( )
areacodelocs = pd . read_csv ( _STR_ , index_col = None , encoding = _STR_ )   $ areacodelocs . areacode = list ( [ s [ : 3 ] for s in areacodelocs . areacode ] )   $ areacodelocs = areacodelocs . drop ( areacodelocs [ areacodelocs . city . isnull ( ) ] . index )   $ areacodelocs = areacodelocs . drop ( areacodelocs [ areacodelocs . region . isnull ( ) ] . index )   $
df [ _STR_ ] = df [ _STR_ ] + df [ _STR_ ]   $ df [ _STR_ ] = df . apply ( my_function , axis = 1 )   $ df . to_csv [ _STR_ ]   $
df_2018 . shape
print ( x_train . shape )   $ print ( x_test . shape )   $ print ( y_train . shape )   $ print ( y_test . shape )
d = datetime ( 2014 , 8 , 29 )   $ do = pd . DateOffset ( days = 1 )   $ d + do
len ( active_psc_controls [ active_psc_controls . nature_of_control . str . contains ( _STR_ ) ] . company_number . unique ( ) )
df2 . head ( ) # checking dataframe for ab_page column
tips . index
sorted ( df [ _STR_ ] . unique ( ) )
prcp_data_df = pd . DataFrame ( prcp_data , columns = [ _STR_ , _STR_ ] )
x = search . cv_results_ [ _STR_ ] . data   $ score = search . cv_results_ [ _STR_ ]   $ yerr = search . cv_results_ [ _STR_ ]   $ fig , ax = plt . subplots ( figsize = ( 12 , 8 ) )   $ ax . errorbar ( x , score , yerr = yerr ) ;
city_eco = city_pop . copy ( )   $ city_eco [ _STR_ ] = [ 17 , 17 , 34 , 20 ]   $ city_eco
data . head ( )
label_index = np . repeat ( np . array ( labels ) , 3 )   $ community_index = np . repeat ( np . array ( communities ) , 3 )
def trip_start_date ( x ) :   $ return re . search ( _STR_ , x ) . group ( 0 )
finaldf = df . drop ( [ _STR_ , _STR_ ] , axis = 1 )   $ finaldf . head ( )
loan_requests1 . shape
elec . mains ( ) . good_sections ( )
print len ( ind [ _STR_ ] . unique ( ) )   $ print len ( noise_graf [ _STR_ ] . unique ( ) )   $ print len ( noise_graf )
probab_converted = df2 [ _STR_ ] . mean ( )   $ print ( _STR_ . format ( probab_converted ) )
df_final . info ( )
total . first_valid_index ( )
% matplotlib inline   $ closingPrices . plot ( ) ;
( details [ _STR_ ] == 0 ) . value_counts ( )
plot_confusion_matrix ( cm_lr , classes = [ _STR_ , _STR_ ] , normalize = False , title = _STR_ , cmap = plt . cm . Blues )
from sklearn . metrics import roc_curve , auc   $ probs = pipeline . predict_proba ( ogXfinaltemptf )   $ preds = probs [ : , 1 ]   $ fpr , tpr , threshold = roc_curve ( ogy , preds )   $ roc_auc = auc ( fpr , tpr )
people = odm2rest_request ( _STR_ , { _STR_ : _STR_ . join ( orgs ) } )
xml_in_sample1 [ _STR_ ] . nunique ( )
tweet_full_df . to_csv ( _STR_ )
grid . fit ( X_trainfinaltemp , y_train )
desc_order_active_stations = session . query ( Measurement . station , func . count ( Measurement . station ) ) . \   $ group_by ( Measurement . station ) . order_by ( func . count ( Measurement . station ) . desc ( ) ) . all ( )   $ desc_order_active_stations
new . shape
learn . freeze_to ( - 1 )
data_adv = pd . get_dummies ( data_adv , columns = [ _STR_ , _STR_ , _STR_ ] )   $ data_adv = data_adv . drop ( [ _STR_ ] , axis = 1 )
import statsmodels . api as sm
store_items = store_items . drop ( [ _STR_ , _STR_ ] , axis = 0 )   $ store_items
temp = pd . read_csv ( _STR_ )   $ tweets_clean = temp . copy ( )   $ tweets_clean . info ( )   $
yt . get_subscriptions ( channel_id , key , descriptive = True ) [ : 2 ]
new_model = gensim . models . Word2Vec . load ( temp_path ) # open the model
! hdfs dfs - mkdir { HDFS_DIR } / 3.2   $   ! hdfs dfs - mkdir { HDFS_DIR } / 3.2 / input   $   ! hdfs dfs - mkdir { HDFS_DIR } / 3.2 / output
df . index . month . value_counts ( ) . sort_index ( )
cachedf = dir2df ( cachedir , fnpat = _STR_ , addcols = [ _STR_ ] )   $ cachedf
conn_b . commit ( )
score_l50 = score [ ( score [ _STR_ ] < 50 ) & ( score [ _STR_ ] > 0 ) ]     $ score_l50 . shape [ 0 ]
msft . loc [ _STR_ : _STR_ ] [ : 5 ]
print ( Ralston . TMAX . std ( ) , Ralston . TMAXc . std ( ) )
events [ events [ _STR_ ] == _STR_ ] . head ( )
% % bash   $ cd / data / LNG / Hirotaka / ASYN   $ awk _STR_ PPMI1_all > sign . txt
NoOfUniqueUser = df [ _STR_ ] . nunique ( )   $ NoOfUniqueUser
df_unit . shape
flight . selectExpr ( _STR_ ) . duration_hour . cast ( _STR_ ) . show ( )
m . fit ( lr , 2 , metrics = [ exp_rmspe ] , cycle_len = 4 )
df_archive [ _STR_ ] . value_counts ( )
d_aux = [ ]   $ for i in range ( 0 , l2 ) :   $ if i not in seq :   $ d_aux . append ( diversity [ i ] )   $ col . append ( np . array ( d_aux ) )
my_gempro . get_scratch_predictions ( path_to_scratch = _STR_ ,   $ results_dir = my_gempro . data_dir ,   $ num_cores = 4 )
nt . set_index ( _STR_ , inplace = True )
data = df . drop_duplicates ( subset = _STR_ )
df_2017 . dropna ( inplace = True )   $ df_2017
df2 [ _STR_ ] = 1   $ df2 [ [ _STR_ , _STR_ ] ] = pd . get_dummies ( df2 [ _STR_ ] )   $ df2 . head ( )
g8_aggregates . columns = [ _STR_ . join ( col ) for col in g8_aggregates . columns ]   $ g8_aggregates
train_small_data . to_feather ( _STR_ )   $ val_small_data . to_feather ( _STR_ )   $ test . to_feather ( _STR_ )
df4 [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] . astype ( int )   $ df4 . head ( )
html = browser . html   $ soup = bs ( html , _STR_ )
train_df . head ( )
new_dems . shape [ 0 ] # this is how many rows we have...
print ( _STR_ % ( len ( excel_data . columns ) ) )
df . name
test . head ( )
unstacked = retention [ _STR_ ] . unstack ( 1 )
full_contingency = np . array ( pd . crosstab ( index = intervention_train [ _STR_ ] , columns = intervention_train [ _STR_ ] ) )
df = df [ ( df . yearOfRegistration >= 1990 ) & ( df . yearOfRegistration < 2017 ) ]   $ df = df [ ( df . price >= 100 ) & ( df . price <= 100000 ) ]   $ df = df [ ( df . powerPS < 600 ) ]       $ print _STR_ , df . shape
sess . get_data ( [ _STR_ , _STR_ , _STR_ ] , [ _STR_ , _STR_ ] )
top20 . to_csv ( _STR_ )
texts = [ ]   $ r = csv . reader ( open ( _STR_ ) )   $ for i in r :   $ texts . append ( i )     $ len ( texts )
df_con . to_csv ( _STR_ , index = False )
stories = pd . concat ( [ stories , tag_df ] , axis = 1 )
LSI_model = LSI . train ( corpus [ : - 100 ] )
train_pred = rf . predict ( X_train )
file_path = _STR_
graf_counts2 [ _STR_ ] = graf_counts2 [ _STR_ ] . astype ( int )
cig_data . index
sorted . select ( _STR_ ) . toPandas ( ) . hist ( bins = 15 )
lst = data_after_subset_filter . SEA_AREA_NAME . unique ( )   $ print ( _STR_ . format ( _STR_ . join ( lst ) ) )
autos [ _STR_ ] . value_counts ( )
y = yc_new4 [ _STR_ ]   $ x = yc_new4 [ _STR_ ]   $ print ( y . mean ( ) )   $ print ( x . mean ( ) )   $ sns . regplot ( x , y , data = yc_new4 )
test_words . shape
logit_mod_new = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ results_new = logit_mod_new . fit ( )   $ results_new . summary ( )
firstWeekUserMerged [ firstWeekUserMerged . objecttype . isnull ( ) ] . head ( 5 )
to_be_predicted_Day4 = 83.28399652   $ predicted_new = ridge . predict ( to_be_predicted_Day4 )   $ predicted_new
arr_size = reflClean . shape   $ arr_size
print ( _STR_ . format ( df . isnull ( ) . any ( ) . sum ( ) ) )
filter_df [ _STR_ ] . unique ( )
dfRegMet . info ( )
page = requests . get ( _STR_ )
data . loc [ 1 ]
pd . crosstab ( index = mydf . comp , columns = mydf . dept )
df2 = df . drop ( df . columns [ non_null_counts < 1000 ] , axis = 1 ) #by default df.drop will take out rows, axis = 0
twitter_archive_master . groupby ( _STR_ ) [ _STR_ ] . mean ( )
df_only_headline [ df_only_headline [ _STR_ ] . apply ( lambda x : _STR_ in set ( x ) ) ] . groupby ( [ _STR_ , _STR_ ] ) . size ( )
poldnull = df2 . query ( _STR_ ) . shape [ 0 ] / df2 . shape [ 0 ]   $ poldnull
dd2 = cfs . diff_abundance ( _STR_ , _STR_ , _STR_ , alpha = 0.25 )
fast_scatter_xs = fuel_xs . get_values ( filters = [ openmc . EnergyFilter ] ,   $ filter_bins = [ ( ( 0.625 , 20.0e6 ) , ) ] ,   $ scores = [ _STR_ ] )   $ print ( fast_scatter_xs )
ds_cnsm = xr . open_mfdataset ( data_url2 )   $ ds_cnsm = ds_cnsm . swap_dims ( { _STR_ : _STR_ } )   $ ds_cnsm = ds_cnsm . chunk ( { _STR_ : 100 } )   $ ds_cnsm = ds_cnsm . sortby ( _STR_ ) # data from different deployments can overlap so we want to sort all data by time stamp. $ ds_cnsm
df = pd . DataFrame ( results )   $ df = df . rename ( columns = { 0 : _STR_ } )
evaluation_1 = api . create_evaluation ( ensemble_1 , test_dataset )   $ api . ok ( evaluation_1 )
order_data = pd . read_excel ( _STR_ )
gs_from_model . score ( X_test , y_test_over )
df = df [ df . Address . notnull ( ) ]
print ( df2 [ _STR_ ] . value_counts ( ) )
re_split_raw = re . findall ( _STR_ , raw )   $ print ( re_split_raw [ 100 : 150 ] )
price2017 [ _STR_ ] = pd . to_datetime ( price2017 [ _STR_ ] + _STR_ + price2017 [ _STR_ ] )
print ( _STR_ % len ( d . variables [ _STR_ ] ) )   $ print ( _STR_ % len ( d . variables [ _STR_ ] ) )
df [ _STR_ ] = pd . to_datetime ( df [ _STR_ ] , format = _STR_ )
autos [ _STR_ ] . unique ( )
pbptweets = pbptweets . drop_duplicates ( subset = _STR_ , keep = _STR_ )
np . sin ( 1 )
engine . execute ( _STR_ ) . fetchall ( )
circle_companies = pd . DataFrame ( graph . run ( _STR_ ) . data ( ) )   $ active_companies [ active_companies . CompanyNumber . isin ( circle_companies [ _STR_ ] ) ] . to_csv ( _STR_ )   $ len ( circle_companies )
df = df . reset_index ( )   $ df = df . rename ( columns = { _STR_ : _STR_ , _STR_ : _STR_ } )   $ df . head ( )
m2 = pd . merge ( horror_readings , visits , left_on = _STR_ , right_on = _STR_ )   $
% % sql   $ UPDATE facts   $ SET Last_Update_Date_key = hour . hour_key   $ FROM hour   $ WHERE hour . hour = TO_CHAR ( facts . Last_Update_Date , _STR_ )
treatment_df = df . query ( _STR_ )   $ uut_old = treatment_df . query ( _STR_ ) . user_id . nunique ( )   $ uut_new = treatment_df . query ( _STR_ ) . user_id . nunique ( )   $ print ( _STR_ . format ( uut_old ) )   $ print ( _STR_ . format ( uut_new ) )
ol . data
grouped_authors_by_publication . head ( )
options_data . info ( )
fashion . groupby ( fashion . index ) . size ( )
df = pd . read_csv ( _STR_ )
log_mod_countries_2 = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ results_countries_2 = log_mod_countries_2 . fit ( )   $ results_countries_2 . summary ( )
answer1_quandl = quandl . get ( _STR_ , start_date = _STR_ , end_date = _STR_ )   $ answer1_quandl . head ( )
list ( tweets_total [ 0 ] . keys ( ) )
res [ _STR_ ] = res [ _STR_ ] * res [ _STR_ ]   $ res_agg = res . groupby ( _STR_ ) . agg ( { _STR_ : [ _STR_ ] , _STR_ : [ _STR_ ] , _STR_ : [ _STR_ , _STR_ ] } )   $ res_agg . columns = [ _STR_ , _STR_ , _STR_ , _STR_ ]   $ res_agg . reset_index ( inplace = True )   $
twitter_archive [ twitter_archive [ _STR_ ] . astype ( int ) < 10 ]
events_df [ _STR_ ] = events_df [ _STR_ ] . apply ( lambda d : d . replace ( hour = 0 , minute = 0 , second = 0 ) )   $ events_df [ _STR_ ] = events_df [ _STR_ ] . apply ( lambda d : d - datetime . timedelta ( d . weekday ( ) ) )   $ events_df [ _STR_ ] = events_df [ _STR_ ] . apply ( lambda d : d . weekday ( ) )
df . loc [ : , _STR_ ] = df [ _STR_ ] . str . findall ( _STR_ ) . str . join ( _STR_ )   $ df . head ( )
mr . at_time ( _STR_ )
frame2 . ix [ _STR_ ]
print ( pd . isnull ( titanic ) . sum ( ) )   $
props . prop_name . value_counts ( )   $ propnames . value_counts ( )
by_weekday = data . groupby ( data . index . dayofweek ) . mean ( )   $ by_weekday . index = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ by_weekday . plot ( style = [ _STR_ , _STR_ , _STR_ ] ) ;
segmentData . opportunity_stage . value_counts ( )
airbnb_df [ _STR_ ] . fillna ( _STR_ , inplace = True )
plate_appearances = df . sort_values ( [ _STR_ , _STR_ ] , ascending = True ) . groupby ( [ _STR_ ] ) . first ( ) . reset_index ( )
def prob_loss ( ann_ret ) :   $ ann_ret = ann_ret . dropna ( )   $ mask = ( ann_ret < 0.0 )   $ prob = np . sum ( mask ) / len ( mask )   $ return prob
df_vow . head ( )
indeed . shape
dtmodel . fit ( XX_train , yy_train )   $ dtmodel_predictionsX = logmodel . predict ( XX_test )   $ num_of_dtmodel_predX = collections . Counter ( dtmodel_predictionsX )   $ num_of_dtmodel_predX
th = table . find ( _STR_ , text = _STR_ )   $
data . complex_features
unique = len ( df ) - sum ( df [ _STR_ ] . duplicated ( ) )   $ print ( unique )
hashtag_list = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ list_length = hashtag_list . size [ 1 ]   $ for i in range ( list_length ) :   $ DF_list [ i ] . to_csv ( hashtag_list [ i ] )
df2 = df . query ( _STR_ )
data . value [ [ 3 , 4 , 6 ] ] = [ 14 , 21 , 5 ]   $ data
autos [ _STR_ ] . describe ( )
van_final [ _STR_ ] = van_final . groupby ( _STR_ ) [ _STR_ ] . first ( ) . str . contains ( _STR_ )
df . head ( )
df_test_index = df_test_index . drop ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 )   $ df_test_index = df_test_index . iloc [ : , [ 1 , 0 , 2 ] ]   $ df_test_index [ _STR_ ] = df_test_user [ _STR_ ]   $ df_test_index = df_test_index . fillna ( 0 )   $ df_test_index
pres_df . drop ( _STR_ , inplace = True , axis = 1 )   $ pres_df . head ( )
neg_tweets . shape
stop_words = set ( stopwords . words ( _STR_ ) )   $
users . loc [ : , _STR_ ] = users . created_at . dt . date   $ tmp = users . groupby ( [ _STR_ ] ) . size ( ) . reset_index ( )   $ tmp . rename ( columns = { 0 : _STR_ } , inplace = True )   $ ax = sns . scatterplot ( data = tmp , x = _STR_ , y = _STR_ )
p = p . to_crs ( { _STR_ : _STR_ } )
df_kws . plot ( )
token_sendcnt [ _STR_ ] = token_sendcnt . sender   $ token_receivecnt [ _STR_ ] = token_receivecnt . receiver
NYPD_df [ _STR_ ] . value_counts ( ) . head ( )
from scipy . stats import norm   $ z_sig = norm . cdf ( z_score )   $ crit_val = norm . ppf ( 1 - ( 0.05 / 2 ) )   $ print ( z_sig , crit_val )
from sqlalchemy . ext . declarative import declarative_base   $ from sqlalchemy import Column , Integer , String , Float
autos . isnull ( ) . sum ( ) / len ( autos ) * 100
display ( observations_ext_node [ _STR_ ] [ DATA ] . dropna ( ) . head ( 9 ) )
len ( df2 . loc [ ( df2 [ _STR_ ] == _STR_ ) & ( df2 [ _STR_ ] == 1 ) ] [ _STR_ ] ) / len ( df2 . loc [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] )
pt_after = pd . DataFrame . pivot_table ( df_users_6_after , index = [ _STR_ ] , values = [ _STR_ ] , aggfunc = _STR_ , fill_value = 0 )
shown = pd . DataFrame ( data . tasker_id . value_counts ( ) )   $ shown . loc [ shown [ _STR_ ] == 1 ]
! ls | grep - - fixed - strings - - file samples_with_signatures . txt - v | grep log | xargs head - n 20 | head - n 50
test_join . filter ( _STR_ ) . count ( ) / test_forks . count ( )
df . info ( )
timedog_df = tdog_df [ tdog_df . userTimezone . notnull ( ) ]   $ timedog_df = timedog_df [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ]   $ timedog_df . head ( )   $ timedog_df . size
new_page_converted = np . random . choice ( np . arange ( 2 ) , size = n_new , p = [ ( 1 - p_new ) , p_new ] )   $
archive_copy = archive_copy . drop ( [ _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 )
obs_diff = df2 . query ( _STR_ ) . converted . mean ( ) - df2 . query ( _STR_ ) . converted . mean ( )   $ obs_diff   $ p_value = ( p_diffs > obs_diff ) . mean ( )   $ p_value
well_data . fillna ( value = 0 , inplace = True )
learner . lr_find ( start_lr = lrs / 10 , end_lr = lrs * 10 , linear = True )
theta_0 = 0.1 * np . random . randn ( X_train_1 . shape [ 1 ] )   $ theta = gradient_descent ( X_train_1 , y_train , theta_0 , 0.1 , 100 )
p_new = len ( df2 . query ( _STR_ ) ) / len ( df2 )   $ p_new
prob_convert = ( df2 [ _STR_ ] == 1 ) . sum ( ) / unique_users_2   $ prob_convert
twitter_archive_master . to_csv ( _STR_ , index = False )
jobPostDF [ _STR_ ] = jobPostDF . date . astype ( datetime )
df_joy . shape
fat . add_bollinger_bands ( vol , _STR_ , inplace = True )   $ vol = vol . dropna ( )   $ vol . head ( )
print ( df [ _STR_ ] . value_counts ( dropna = False ) )
brand_mileage = { }   $ for brand in brands :   $ mean_mileage = autos [ _STR_ ] [ autos [ _STR_ ] == brand ] . mean ( )   $ brand_mileage [ brand ] = int ( mean_mileage )   $ brand_mileage
df [ _STR_ ] = df [ _STR_ ] . apply ( lambda x : TextBlob ( x ) . sentiment . polarity )   $ df [ _STR_ ] = df [ _STR_ ] . apply ( lambda x : TextBlob ( x ) . sentiment . subjectivity )
plot = seaborn . barplot ( x = _STR_ , y = _STR_ , data = name_sentiments , capsize = .1 )
for i , x in data . iterrows ( ) :   $ if compute_score ( x [ 3 ] ) > 0.05 :   $ print x [ 0 ]
from rl . callbacks import TrainEpisodeLogger   $ class TrainEpisodeLoggerPortfolio ( TrainEpisodeLogger ) :   $
tweetsDf = tweetsDf . fillna ( _STR_ )   $ tweetsDf . country . hist ( )
daily_station_df = ( all_turnstiles . groupby ( [ _STR_ , _STR_ ] , as_index = False )   $ . sum ( )   $ . drop ( [ _STR_ , _STR_ ] , axis = 1 ) )   $ daily_station_df . sample ( 5 )
tt_json = tt_json_df [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ]   $ tt_json . head ( )
dates = [ ]   $ for ann_date in announcement_dates . index :   $ dates . extend ( neighbor_dates ( ann_date ) )   $ dates = pd . Series ( dates )
sales_agg [ _STR_ ] = sales_agg [ _STR_ ] - sales_agg [ _STR_ ]   $ sales_agg . head ( 1 ) #create margin feature
p_diffs = [ ]   $ for _ in range ( 10000 ) :   $ new_page_converted = np . random . choice ( [ 0 , 1 ] , size = n_new , p = [ p_new , 1 - p_new ] ) . mean ( )   $ old_page_converted = np . random . choice ( [ 0 , 1 ] , size = n_old , p = [ p_old , 1 - p_old ] ) . mean ( )   $ p_diffs . append ( new_page_converted - old_page_converted )
more_grades = final_grades_clean . stack ( ) . reset_index ( )   $ more_grades
df3 [ [ _STR_ , _STR_ , _STR_ ] ] = pd . get_dummies ( df3 [ _STR_ ] )   $ logit_model = sm . Logit ( df3 [ _STR_ ] , df3 [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ result = logit_model . fit ( )   $ result . summary ( )
properati [ properati [ _STR_ ] == _STR_ ]
flight . printSchema ( )   $ flightv1_1 . printSchema ( )
consumerKey = _STR_   $ consumerSecret = _STR_   $ auth = tweepy . OAuthHandler ( consumer_key = consumerKey , consumer_secret = consumerSecret )   $ api = tweepy . API ( auth )
df . index   $
pyLDAvis . enable_notebook ( )
p_diffs = np . array ( p_diffs )   $ null_vals = np . random . normal ( 0 , p_diffs . std ( ) , p_diffs . size )
ip . head ( )
df [ _STR_ ] = df [ _STR_ ] . apply ( lambda x : x . week )   $ df [ _STR_ ] = df [ _STR_ ] . apply ( lambda x : x . year )   $ df [ _STR_ ] = df [ _STR_ ] . dt . date   $
% matplotlib inline   $ import matplotlib . pyplot as plt   $ from matplotlib import style   $ style . use ( _STR_ )
df = pandas . read_csv ( _STR_ )
from sklearn . neighbors import KNeighborsClassifier
tw . apply ( lambda x : x [ _STR_ ] + _STR_ + x [ _STR_ ] , axis = 1 ) . value_counts ( )
spearmanr ( merge_ [ _STR_ ] ,   $ merge_ [ _STR_ ] )
basic_smapes = [ vector_smape ( basic_pred [ col ] , real [ col ] ) for col in basic_pred . columns ]   $ complex_smapes = [ vector_smape ( complex_pred [ col ] , real [ col ] ) for col in complex_pred . columns ]
df . reorder_levels ( order = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ) . head ( 3 )
df . loc [ _STR_ ]
df_1 = pd . read_csv ( _STR_ )   $ df_1   $ IDlist = df_1 [ _STR_ ] . tolist ( )   $ IDlist
a [ _STR_ ] = a . apply ( lambda x : x [ _STR_ ] + x [ _STR_ ] , axis = 1 )
df_img_algo_clean = df_img_algo . copy ( )   $ df_img_algo_clean . head ( )
df_drug_counts . dropna ( axis = 1 , thresh = 20 ) . plot ( kind = _STR_ ,   $ figsize = ( 10 , 6 ) )
mb . head ( )
! head - 1 . / data / raw - news_tweets - original / dataset1 / news / 2014 - 11 - 18. txt
y_pred = lr_pred   $ print ( _STR_ . format ( precision_score ( y_test , y_pred ) ,   $ recall_score ( y_test , y_pred ) ,   $ accuracy ( y_test , y_pred ) ) )
x_train . shape
old_page_converted = np . random . choice ( [ 1 , 0 ] , size = n_old , p = [ convert_p_old , ( 1 - convert_p_old ) ] )
d . year
df_group2 = df . query ( _STR_ )
out_df = pd . DataFrame ( pred )   $ out_df . columns = [ _STR_ , _STR_ , _STR_ ]   $ out_df [ _STR_ ] = test_df . listing_id . values
shopping_carts = pd . DataFrame ( items )   $ shopping_carts
y_pred = svm_clf . predict ( X_train_scaled )   $ accuracy_score ( y_train , y_pred )
autos = autos . drop ( _STR_ , 1 )
import pandas as pd   $ distance_from_sun = [ 149.6 , 1433.5 , 227.9 , 108.2 , 778.6 ]   $ planets = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ dist_planets = pd . Series ( index = planets , data = distance_from_sun )   $ print ( dist_planets )
model . wv [ _STR_ ] # raw NumPy vector of a word
idx = index . Index ( )   $ for i , poly in enumerate ( census_tracts_df [ _STR_ ] ) :   $ idx . insert ( i , poly . bounds )
engine . execute ( _STR_ ) . fetchall ( )
df [ _STR_ ] . value_counts ( )
def parse_date ( str_date ) :   $ return dateutil . parser . parse ( str_date )   $ df [ _STR_ ] = df [ _STR_ ] . apply ( parse_date )
df . set_index ( _STR_ , inplace = True )
df_predict = pd . DataFrame ( { _STR_ : [ _STR_ ] } )   $ predictions = loaded_keras_entity_recognizer . predict ( df_predict )   $ predictions [ [ _STR_ , _STR_ , _STR_ ] ]
Image ( filename = _STR_ )
df_2018 . shape
mask = ( youthUser4 [ _STR_ ] > _STR_ ) & ( youthUser4 [ _STR_ ] <= _STR_ )   $ youthUserNov2017 = ( youthUser4 . loc [ mask ] )   $ youthUserNov2017 . head ( )
s4 . unique ( )
for row in session . query ( Measurements ) . limit ( 5 ) . all ( ) :   $ print ( row )
len ( df [ michaelkorsseries ] . userid . unique ( ) )
print ( df . shape )   $ print ( _STR_ . format ( df . shape [ 0 ] ) )
p ( locale . getdefaultlocale )
res . status_code # Checking the HTTPS Response
len ( df2 . query ( _STR_ ) ) / len ( df2 [ _STR_ ] )
lr = LogisticRegression ( random_state = 42 )   $ param_grid = { _STR_ : [ _STR_ , _STR_ ] ,   $ _STR_ : np . logspace ( 0 , 2 , 10 ) }   $ lr_gd = GridSearchCV ( estimator = lr , param_grid = param_grid , cv = 5 , scoring = _STR_ , n_jobs = - 1 )   $ lr_gd . fit ( X_train , y_train )
pd . Series ( [ 1 , True , _STR_ ] )
number_of_values_where_NoData = np . count_nonzero ( reflClean == metadata [ _STR_ ] )   $ number_of_values_where_NoData
plt . hist ( taxiData . Trip_distance , bins = 60 , range = [ 0 , 3 ] )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )   $ plt . title ( _STR_ )   $ plt . grid ( True )
vocab = vectorizer . get_feature_names ( )   $ print ( vocab )
df_goog . sort_values ( _STR_ , inplace = True ) # This is a good idea to sort our values so the indexes ultimately line up $ df_goog.set_index('Date', inplace=True)      # also df_goog.index = df_goog['Date'] works well here $ df_goog.index = df_goog.index.to_datetime()  # Convert to datetime
fact_url = _STR_
ser = pd . Series ( [ _STR_ , _STR_ ] , index = days )   $ ser
sensors_num_df . data . head ( )
df . reset_index ( inplace = True )
grouped_by_date_df = full_df . groupby ( _STR_ ) [ _STR_ ] . count ( ) . reset_index ( ) . copy ( )   $ grouped_by_date_df . columns = [ _STR_ , _STR_ ]   $ grouped_by_date_df . head ( 3 )
forecast = m . predict ( future )   $ forecast [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] . tail ( )
f_os_hour_clicks . show ( 1 )
autos [ _STR_ ] . describe ( )
df_a . join ( df_b , how = _STR_ ) # right join (see above for definition)
entry = [ ]   $ Keys = Dict [ _STR_ ]   $ for date in data :   $ entry . append ( dict ( zip ( Keys , date ) ) )   $ print ( entry [ 0 : 5 ] )
twitter_df_clean . describe ( )
df3 = pd . merge ( df1 , df2 )   $ df3
plus_minus_fxn = lambda x : x . rolling ( 20 ) . sum ( )   $ features [ _STR_ ] = features [ _STR_ ] . groupby ( level = _STR_ ) . apply ( plus_minus_fxn )
grouped_publications_by_author . tail ( 10 )
s_t = np . sqrt ( ( ( n2 - 1 ) * n2 * sd2 + ( n3 - 1 ) * n3 * sd3 ) / ( n2 + n3 - 2 ) )   $ t = ( m3 - m2 ) / ( s_t * np . sqrt ( 1 / n2 + 1 / n3 ) )   $ tscore = stats . t . ppf ( .95 , n2 + n3 - 2 )   $ print ( _STR_ . format ( t , tscore ) )
dates = [ datetime ( 2014 , 8 , 1 ) , datetime ( 2014 , 8 , 2 ) ]   $ ts = pd . Series ( np . random . randn ( 2 ) , dates )   $ ts
secclintondf = secclintondf [ secclintondf . datetime > _STR_ ]
for x in tweets_clean . dog_class . unique ( ) :   $ print ( _STR_ + str ( x ) + _STR_ + str ( tweets_clean [ tweets_clean . dog_class == x ] . favorites_count . mean ( ) ) )
median = df [ _STR_ ] . quantile ( q = 0.5 )   $ print ( median )
df . isnull ( ) . sum ( )
df . head ( )
classify_df = classify_df . drop ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 )   $ classify_df = classify_df . drop ( [ _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 )   $ classify_df . columns
day_counts = ( daily_hashtag . groupBy ( _STR_ , _STR_ , _STR_ )   $ . count ( )   $ . sort ( _STR_ , ascending = False )   $ ) . cache ( )
tuna_neg_cnt = tuna_neg . count ( ) * 100   $ print _STR_ \   $ . format ( tuna_neg_cnt , D0 . isoformat ( ) , tuna_neg_cnt * 1. / dau )
countries_df . nunique ( )
v = sns . factorplot ( data = tweets_df , x = _STR_ , y = _STR_ , kind = _STR_ )   $ plt . xticks ( rotation = 60 )   $
html_table = df . to_html ( )   $ df . to_html ( _STR_ )   $ html_table
items = { _STR_ : pd . Series ( data = [ 245 , 25 , 55 ] , index = [ _STR_ , _STR_ , _STR_ ] ) ,   $ _STR_ : pd . Series ( data = [ 40 , 110 , 500 , 45 ] , index = [ _STR_ , _STR_ , _STR_ , _STR_ ] ) }
xgb . plot_importance ( model_outer )
bnbAx [ bnbAx [ _STR_ ] != _STR_ ] . country_destination . value_counts ( ) . plot . bar ( )
tweetsDf . hist ( column = _STR_ , bins = 15 )
col_short [ _STR_ ] = col_short [ _STR_ ] . astype ( str )   $ df_weather [ _STR_ ] = df_weather [ _STR_ ] . astype ( str )
idx = data [ _STR_ ] [ _STR_ ] . index ( _STR_ )   $ volume = [ day [ idx ] for day in data [ _STR_ ] [ _STR_ ] ]   $ average_volume = sum ( volume ) / len ( volume )   $ print ( _STR_ . format ( average_volume ) )
countries_df = pd . read_csv ( _STR_ )   $ df_new = countries_df . set_index ( _STR_ ) . join ( df2 . set_index ( _STR_ ) , how = _STR_ )   $ df_new . head ( )
df [ _STR_ ] = to_datetime ( df [ _STR_ ] , format = _STR_ )   $ df . head ( )
run txt2pdf . py - o _STR_ _STR_
plt . figure ( figsize = ( 15 , 5 ) )   $ plt . title ( _STR_ )   $ mentions_df [ _STR_ ] . hist ( bins = 500 )   $ plt . show ( )
Log_model2 = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ Log_model2 . fit ( ) . summary ( )
from sklearn import metrics   $ print _STR_ , metrics . mean_absolute_error ( y_test , predictions )   $ print _STR_ , np . sqrt ( metrics . mean_squared_error ( y_test , predictions ) )   $ print _STR_ , metrics . r2_score ( y_test , predictions )
autos [ _STR_ ] . describe ( )
df = pd . DataFrame ( { _STR_ : [ 2017 , 2018 ] , _STR_ : [ 1 , 12 ] , _STR_ : [ 26 , 25 ] } )   $ print ( df )
master_file [ _STR_ ] . value_counts ( )
df2 = tier1_df . reset_index ( )   $ df2 = df2 . rename ( columns = { _STR_ : _STR_ , _STR_ : _STR_ } )   $ df_orig = df2 [ _STR_ ] . to_frame ( )   $ df_orig . index = df2 [ _STR_ ]   $ n = np . int ( df_orig . count ( ) )
rf_v1 . varimp ( use_pandas = True )
aa2 = np . array ( eval ( aa2 ) )   $ aa2
df = titanic3 [ [ _STR_ , _STR_ ] ] . dropna ( )   $ df [ _STR_ ] = df . apply ( lambda row : ord ( row . cabin [ 0 ] ) - 64 , axis = 1 )   $ sns . regplot ( x = df [ _STR_ ] , y = df [ _STR_ ] )
dfRegMet2014 . shape
df_gnis_test = df_gnis . dropna ( axis = 0 , subset = [ _STR_ , _STR_ ] , thresh = 1 )   $ df_gnis_test . shape
from IPython . display import IFrame   $ IFrame ( _STR_ , width = 800 , height = 200 )
outlier_detection . outlier_detection_1d ( cutoff_params = outlier_detection . basic_cutoff ) . head ( 20 )
log_mod_2 = sm . Logit ( df_new [ _STR_ ] , df_new [ [ _STR_ , _STR_ , _STR_ ] ] )   $ result_2 = log_mod_2 . fit ( )   $ result_2 . summary ( )
1 / np . exp ( - 0.0123 )   $
p_diffs = np . array ( p_diffs )   $ p_diff_proportion = ( p_diff_orig < p_diffs ) . mean ( )   $ print ( _STR_ , p_diff_proportion )
pd . crosstab ( df [ _STR_ ] , df [ _STR_ ] )
autos = autos [ autos [ _STR_ ] . between ( 1900 , 2016 ) ]
festivals . head ( 5 )
df2 [ _STR_ ] = 1   $ df2_dummy = pd . get_dummies ( df [ _STR_ ] )   $ df2 [ _STR_ ] = df2_dummy [ _STR_ ]   $ df2 . head ( )
data = data . reindex ( range ( data . shape [ 0 ] ) )
details . head ( 10 )
tweets3 = pd . read_json ( _STR_ )   $ tweets3 . shape
news . tail ( )
morning_rush . iloc [ : 1000 ] [ [ _STR_ , _STR_ ] ] . get_values ( )
data . columns
joined [ _STR_ ] . describe ( )
newstudents = pd . DataFrame ( [ ( 150 , 62 , _STR_ ) , ( 170 , 65 , _STR_ ) ] , columns = [ _STR_ , _STR_ , _STR_ ] , index = [ _STR_ , _STR_ ] )   $ print ( newstudents )
sp = pd . read_csv ( _STR_ )
df_archive_clean [ _STR_ ] . unique ( ) [ 0 : 10 ]
dftop2 . head ( )
questions = pd . concat ( [ questions . drop ( _STR_ , axis = 1 ) , attend_with ] , axis = 1 )
import statsmodels . api as sm   $ convert_old = df2 [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] . sum ( )   $ convert_new = df2 [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] . sum ( )   $ n_old = df2 [ df2 [ _STR_ ] == _STR_ ] . shape [ 0 ]   $ n_new = df2 [ df2 [ _STR_ ] == _STR_ ] . shape [ 0 ]
% timeit _STR_ . join ( [ stemmer ( word ) for word in regex . sub ( _STR_ , _STR_ ) . split ( ) ] )
np . random . seed ( 123456 )   $ ps = pd . Series ( np . random . randn ( 12 ) , mp2013 )   $ ps
consumerKey = _STR_   $ consumerSecret = _STR_   $ auth = tweepy . OAuthHandler ( consumer_key = consumerKey ,   $ consumer_secret = consumerSecret )   $ api = tweepy . API ( auth )
releases [ _STR_ ] . value_counts ( )
df [ _STR_ ] = df . num_comments . map ( lambda x : 1 if x >= med else 0 )
dftop [ _STR_ ] . mean ( )
planets . groupby ( _STR_ ) [ _STR_ ] . describe ( ) . unstack ( )
df . if_fielding_alignment . value_counts ( )
print metrics . confusion_matrix ( y_test , predicted )   $ print metrics . classification_report ( y_test , predicted )
table3 = table3 . drop ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 )
image_url = _STR_   $ image_browser . visit ( image_url )   $ html = image_browser . html
ip_clean [ _STR_ ] . str . replace ( _STR_ , _STR_ ) . str . capitalize ( )
empPands = empDf . toPandas ( )   $ print ( empPands )   $ print ( _STR_ )   $ for index , row in empPands . iterrows ( ) :   $ print ( row [ _STR_ ] )   $
IQuOD_file = _STR_   $ ds_CTD_1988 = xr . open_dataset ( IQuOD_file )
allPeople = read . getPeople ( )   $ pd . DataFrame . from_records ( [ vars ( person ) for person in allPeople ] ) . head ( )
scaled . tail ( ) . T
df_mes . shape [ 0 ]
pbls = subs . problem_id . unique ( )   $ print _STR_ % len ( pbls )
corrected_log . head ( )
user_table_1 . head ( 10 )
top20_mostfav = top20_mostfav . sort_values ( by = _STR_ , ascending = False ) . copy ( )   $ top20_mosttweeted = top20_mosttweeted . sort_values ( by = _STR_ , ascending = False ) . copy ( )
df2 = df2 . drop ( index = 1899 , axis = 0 )
pd . options . display . max_columns = 55
logit . fit ( X_mat , Y_mat )   $ logit . score ( X_mat , Y_mat )
np . array ( _STR_ , dtype = np . datetime64 ) # We use an array just so Jupyter will show us the type details
logit_mod_joined = sm . Logit ( df_joined_dummy . converted ,   \   $ df_joined_dummy [ [ _STR_ , _STR_ , \   $ _STR_ , _STR_ ] ] )
df_users [ _STR_ ] . value_counts ( ) . plot ( _STR_ )
station = pd . read_sql ( _STR_ , conn )   $ station . head ( )
old_page_converted = np . random . binomial ( 1 , p_old , n_old )   $ print ( _STR_ , old_page_converted . mean ( ) )   $ old_page_converted = np . random . choice ( [ 1 , 0 ] , n_old , p = ( p_old , 1 - p_old ) )   $ print ( _STR_ , old_page_converted . mean ( ) )
tweets_gametitle . merge ( winpct [ [ _STR_ , _STR_ , _STR_ ] ] ,   $ how = _STR_ ,   $ left_on = [ _STR_ , _STR_ ] ,   $ right_on = [ _STR_ , _STR_ ] )
confusion_mat = pd . DataFrame ( confusion_matrix ( y_test , y_pred ) ,   $ columns = [ _STR_ , _STR_ ] ,   $ index = [ _STR_ , _STR_ ] )
twitter_archive . source . value_counts ( )
print _STR_ , len ( df3 )   $ print df3 . columns . values   $ df3
tmax_day_2018 . dims
a = test . toarray ( ) #makes an array based on term frequency for the same test sample. $ a
df_resolved_links = pd . DataFrame ( resolved_links )   $ df_resolved_links . tail ( 3 )
df_onc_no_metac . head ( )
dframe_team [ _STR_ ] = dframe_team [ _STR_ ] . dt . year + 1   $ dframe_team [ _STR_ ] = dframe_team . cut_year . map ( draftDates )   $ dframe_team
final_data = pd . read_pickle ( _STR_ )   $ final_data . info ( )   $ final_data . head ( )
sample = data . sample ( frac = 0.1 , replace = False , random_state = 42 )
df = df . groupby ( _STR_ ) . apply ( make_order_number )
algo = make_pipeline ( preprocessing . MinMaxScaler ( ) , svm . LinearSVC ( class_weight = _STR_ ) )   $ scores = cross_val_score ( algo , X , y , cv = 5 , scoring = _STR_ )   $ scores . mean ( )   $ print ( np . round ( scores . mean ( ) , 3 ) )
InfinityWars . to_csv ( _STR_ , encoding = _STR_ )
k1 = data [ [ _STR_ , _STR_ ] ] . groupby ( [ _STR_ ] ) . agg ( _STR_ ) . reset_index ( )   $ k1 . columns = [ _STR_ , _STR_ ]   $ train = train . merge ( k1 , on = [ _STR_ ] , how = _STR_ )   $ test = test . merge ( k1 , on = [ _STR_ ] , how = _STR_ )
m_df = pd . merge ( raw_full_df , pd . DataFrame ( raw_train_y ) , left_index = True , right_index = True )
cand_date_df = pres_date_df . copy ( )   $ cand_date_df . head ( )
f_counts_hour_ip . show ( 1 )
ts_mean [ _STR_ ] = ts_mean [ _STR_ ] / ts_mean [ _STR_ ] * 10000   $ ts_mean [ _STR_ ] = ts_mean [ _STR_ ] * ts_mean [ _STR_ ]   $ ts_mean [ _STR_ ] = ts_mean [ _STR_ ] * ts_mean [ _STR_ ]   $ ts_mean . head ( )
z_score , p_value = sm . stats . proportions_ztest ( count = [ convert_new , convert_old ] , nobs = [ n_new , n_old ] , alternative = _STR_ )   $ print ( z_score , p_value )
from sklearn . model_selection import train_test_split
df2 [ _STR_ ] = df2 [ _STR_ ]   $ df2 = df2 . drop ( [ _STR_ , _STR_ ] , axis = 1 )   $ df2 . head ( )
train_groupped . Visits [ _STR_ ] [ 0 : 5 ]
friends_n_followers [ _STR_ ] . plot ( kind = _STR_ )   $ plt . title ( _STR_ )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )   $ plt . show ( )
conversion_data_normalized = pd . DataFrame ( preprocessing . scale ( conversion_data . values ) )   $ conversion_data_normalized . columns = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ]   $ print _STR_ , conversion_data_normalized . shape , type ( conversion_data_normalized )   $ conversion_data_normalized . head ( )
yc_new1 . rename ( columns = { _STR_ : _STR_ } , inplace = True )   $ yc_new1 . columns
url = _STR_   \   $ _STR_   \   $ . format ( API_KEY )   $ r = requests . get ( url )
! rm tmp . json
traded_volumes = [ ]   $ for ele in r . json ( ) [ _STR_ ] [ _STR_ ] :   $ traded_volumes . append ( ele [ 6 ] )   $ print ( _STR_ . format ( sum ( traded_volumes ) / float ( len ( traded_volumes ) ) ) )
conn . rollback ( )
if not np . any ( cust [ _STR_ ] == 1 ) :   $ labels [ _STR_ ] = 0   $ labels [ _STR_ ] = np . nan
festivals = pd . read_csv ( _STR_ )   $ festivals_clean = pd . read_csv ( _STR_ )   $ print ( festivals . head ( 7 ) )   $ print ( festivals_clean . head ( 7 ) )
intersections . info ( )
X_age_train , X_age_test , y_age_train , y_age_test = train_test_split ( X_age , y_age , train_size = 0.25 )
PIT_analysis2 = PIT_analysis [ _STR_ ] . mean ( )
Z = np . random . random ( ( 10 , 3 ) )   $ zmin , zmax = Z . min ( ) , Z . max ( )   $ print ( ( Z - zmin ) / ( zmax - Z ) )   $ print ( Z . ptp )
df . head ( )
import statsmodels . api as sm   $ log_regression = sm . Logit ( df2 [ _STR_ ] , df2 [ [ _STR_ , _STR_ ] ] )   $ output = log_regression . fit ( )
df_pol_t = pd . concat ( [ df_pol_matrix_df , df_pol_t . drop ( _STR_ , axis = 1 ) ] , axis = 1 )   $
y_newpage = df2 [ _STR_ ] . count ( )   $ prob_newpage = x_newpage / y_newpage   $ prob_newpage   $
y_label_test_OneHot . shape
datetime . now ( )
train_embedding = train_embedding . rename ( { _STR_ : _STR_ } , axis = 1 )
dtree = tree . DecisionTreeClassifier ( max_depth = MAX_DEPTH )   $ dtree = dtree . fit ( X_train , y_train )   $ print ( _STR_ )   $ predicted_labels = dtree . predict ( X_test )   $ answer_labels = y_test
day_of_year14 . to_excel ( writer , index = True , sheet_name = _STR_ )
colNames = colNames . str . replace ( _STR_ , _STR_ )   $ colNames = colNames . str . replace ( _STR_ , _STR_ )   $ colNames = colNames . str . replace ( _STR_ , _STR_ )   $ colNames = colNames . str . replace ( _STR_ , _STR_ )   $ colNames = colNames . str . lower ( )
df . loc [ df [ _STR_ ] > 27 , [ _STR_ , _STR_ ] ]
df . head ( )
plt . hist ( df15 [ _STR_ ] , bins = 20 , range = ( 0 , 100 ) )   $ plt . show ( )
twitter_archive_master . info ( )
auth = tweepy . OAuthHandler ( consumer_key , consumer_secret ) # autheticate Twitter $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
fullDf . level . value_counts ( )   $
austin . shape   $
months = { datetime ( 2000 , i , 1 ) . strftime ( _STR_ ) : i for i in range ( 1 , 13 ) }   $ df [ _STR_ ] = df [ _STR_ ] . map ( months )
iso_join . head ( )
ab . shape [ 0 ]
from scipy . stats import norm   $ norm . ppf ( 1 - ( 0.05 / 2 ) )   $
df . truncate ( before = _STR_ , after = _STR_ )
data . to_csv ( _STR_ ) #To_excel exists as well.
import statsmodels . api as sm   $ convert_old = df2 [ ( df2 [ _STR_ ] == 1 ) & ( df2 [ _STR_ ] == _STR_ ) ] . shape [ 0 ]   $ convert_new = df2 [ ( df2 [ _STR_ ] == 1 ) & ( df2 [ _STR_ ] == _STR_ ) ] . shape [ 0 ]   $ n_old = df2 [ df2 [ _STR_ ] == _STR_ ] . shape [ 0 ]   $ n_new = df2 [ df2 [ _STR_ ] == _STR_ ] . shape [ 0 ]
len ( chefdf . name )
df [ _STR_ ] = df . apply ( lambda x : urllib . parse . quote_plus ( base_search_url + x [ _STR_ ] + x [ _STR_ ] , safe = _STR_ ) , axis = 1 )
print ( data . describe ( ) )
print ( _STR_ , twitter_data . doggo . value_counts ( ) [ 1 ] , _STR_ )   $ print ( _STR_ , twitter_data . doggo . value_counts ( ) [ 1 ] , _STR_ )   $ print ( _STR_ , twitter_data . pupper . value_counts ( ) [ 1 ] , _STR_ )   $ print ( _STR_ , twitter_data . puppo . value_counts ( ) [ 1 ] , _STR_ )   $ twitter_data . doggo . value_counts ( ) [ 1 ] + twitter_data . doggo . value_counts ( ) [ 1 ] + twitter_data . pupper . value_counts ( ) [ 1 ] + twitter_data . puppo . value_counts ( ) [ 1 ]
rf . score ( X_train , y_train )
state_party_df = pd . concat ( state_DataFrames_list , axis = 1 , join = _STR_ )   $ state_party_df . columns = state_keys_list   $ state_party_df . head ( 20 )
rvs1 = stats . norm . rvs ( loc = 92576890.092929989 , scale = 0.1 , size = 500 )   $ rvs2 = stats . norm . rvs ( loc = 242523522.74525771 , scale = 0.2 , size = 500 )   $ stats . ttest_ind ( rvs1 , rvs2 )
op_ = model . predict ( [ q1_data1 , q2_data2 ] )
intervention_test . reset_index ( inplace = True )   $ intervention_test . set_index ( [ _STR_ , _STR_ ] , inplace = True )
df_spend = df_tx_claims . groupby ( [ _STR_ , _STR_ ] ) . agg ( { _STR_ : [ np . sum ] } )
df [ ( ( df [ _STR_ ] == _STR_ ) == ( df [ _STR_ ] == _STR_ ) ) == False ] . shape [ 0 ]
y_pred = m . predict ( X_valid )   $ cnf_matrix = metrics . confusion_matrix ( y_valid , y_pred )   $ cnf_matrix
df_students . head ( )
support . amount . sum ( )
last_date_of_ppt = session . query ( Measurement . date ) . order_by ( Measurement . date . desc ( ) ) . first ( )   $ last_date_of_ppt   $
scores . describe ( )
with open ( _STR_ , _STR_ ) as inputFile :   $ header = next ( inputFile )   $ for line in inputFile :   $ line = line . rstrip ( ) . split ( _STR_ )   $ print ( line )
temp1 = grp1 . createVariable ( _STR_ , np . float64 , ( _STR_ , _STR_ , _STR_ ) , zlib = True )   $ temp2 = grp2 . createVariable ( _STR_ , np . float64 , ( _STR_ , _STR_ , _STR_ ) , zlib = True )   $ for grp in ncfile . groups . items ( ) : # shows that each group now contains 1 variable $     print(grp)
df . index . strftime ( _STR_ )
df = df . join ( d , how = _STR_ )
r = requests . get ( _STR_ . format ( API_KEY ) , auth = ( _STR_ , _STR_ ) )
df . plot ( subplots = True )   $ plt . show ( )
Jarvis_resistance_simulation_1 = Jarvis_ET_Combine [ _STR_ ]   $ Jarvis_resistance_simulation_0_5 = Jarvis_ET_Combine [ _STR_ ]   $ Jarvis_resistance_simulation_0_25 = Jarvis_ET_Combine [ _STR_ ]
datatest . info ( )
df2 . converted . value_counts ( normalize = True ) [ 1 ]
P_treatment = 17264 / 145310   $ print ( _STR_ % P_treatment )
lda_tf . show_topic ( 5 )
p_new = new_page_converted . mean ( )   $ p_old = old_page_converted . mean ( )   $ p_new - p_old   $
m . end_time
segmentData . head ( )
sns . lmplot ( x = _STR_ , y = _STR_ , data = twitter_moves , lowess = True , size = 8 , aspect = 1.5 )
logit_country = sm . Logit ( df_latest [ _STR_ ] , df_latest [ [ _STR_ , _STR_ , _STR_ ] ] )   $ results = logit_country . fit ( )
df . shape
for df in ( joined , joined_test ) :   $ df [ _STR_ ] = pd . to_datetime ( df . apply ( lambda x : Week (   $ x . Promo2SinceYear , x . Promo2SinceWeek ) . monday ( ) , axis = 1 ) . astype ( pd . datetime ) )   $ df [ _STR_ ] = df . Date . subtract ( df [ _STR_ ] ) . dt . days
ForPlot = TotalNameEvents . sort_values ( by = _STR_ , ascending = False ) . head ( 200 )
df_call = pd . read_csv ( _STR_ , index_col = None )
from sklearn . naive_bayes import MultinomialNB   $ nb = MultinomialNB ( )
logistic_mod_page = sm . Logit ( df2 [ _STR_ ] , df2 [ [ _STR_ , _STR_ ] ] )   $ results_page = logistic_mod_page . fit ( )
df . sort_values ( _STR_ , inplace = True )   $ df . head ( )
df_tweet . head ( )
df3 = df . ix [ 0 : 3 , [ _STR_ , _STR_ ] ]   $ df3 . columns = [ _STR_ , _STR_ ]   $ df4 = df . ix [ 4 : 6 , [ _STR_ ] ]   $ df4 . columns = [ _STR_ ]   $ print df3 , _STR_ , df4
engine . execute ( _STR_ ) . fetchall ( )
not_lining_up = df . query ( _STR_ ) . count ( )   $ not_lining_up   $
plt . bar ( icecream . Flavor , icecream . Price )   $ plt . ylabel ( _STR_ )   $ plt . xlabel ( _STR_ )   $ plt . show ( )
import pandas as pd   $ url = _STR_   $ df = pd . read_csv ( url , header = None , sep = _STR_ )   $ df . dropna ( how = _STR_ , inplace = True ) #to drop if any value in the row has a nan $ df.head()
news_tweets_pd . to_csv ( _STR_ , index = False )
start_date = datetime . datetime ( year = 2017 , month = 1 , day = 1 )   $ stop_date = datetime . datetime ( year = 2017 , month = 8 , day = 23 )
! wget _STR_ - P . . / . . / Data /
rent_db . boxplot ( column = _STR_ , by = _STR_ )
len ( df_groups [ _STR_ ] . unique ( ) )
loadRetailData = sc . textFile ( _STR_ )   $ for row in loadRetailData . take ( 5 ) :   $ print row
apostrophe_match = _STR_   $ conjunctions = re . findall ( apostrophe_match , text )   $ tokens = [ ( word [ : - 3 ] , word [ - 3 : ] ) for word in conjunctions ]   $ print ( tokens )
full_globe_temp [ full_globe_temp == - 999.000 ] = np . nan   $ full_globe_temp . tail ( )
data2 = data . iloc [ 1 : 4 ]   $ data2
sex_dummy = pd . get_dummies ( fraud_data_updated [ _STR_ ] )   $ fraud_data_updated = pd . concat ( [ fraud_data_updated , sex_dummy ] , axis = 1 )
df_sub_headline = df_tweets [ ( df_tweets [ _STR_ ] . str . len ( ) > 0 ) ]   $ sub_headline_date_count = df_sub_headline . groupby ( [ _STR_ , _STR_ , _STR_ ] ) . size ( )   $ sub_headline_date_count [ sub_headline_date_count > 1 ] . index . get_level_values ( 1 )
new_dems . newDate [ new_dems . Sanders . isnull ( ) ]   $
nold = len ( df2 . query ( _STR_ ) )   $ nold
sub_gene_df . sample ( 10 )
facts_df = pd . read_html ( str ( table ) )
autos . drop ( [ _STR_ , _STR_ , _STR_ ] , axis = 1 )
fpr_a = ( grid_pr_fires . sort_values ( [ _STR_ , _STR_ ] , ascending = [ False , True ] ) [ _STR_ ]   $ . values . reshape ( 26 , 59 ) )
pd . concat ( [ df , delivery_dummy ] , axis = 1 )
p_new = df2 [ df2 [ _STR_ ] == 1 ] . shape [ 0 ] / df2 . shape [ 0 ]   $ p_new
collect . get_iterator ( )
codes = access_logs_df . groupBy ( _STR_ ) . count ( )
to_be_predicted_Day3 = 48.60496872   $ predicted_new = ridge . predict ( to_be_predicted_Day3 )   $ predicted_new
twitter_archive_clean [ _STR_ ] = pd . to_datetime ( twitter_archive_clean [ _STR_ ] )
DummyDataframe = DummyDataframe . set_index ( _STR_ ) . sort_index ( )   $ DummyDataframe = DummyDataframe . groupby ( _STR_ ) . sum ( )
sq = Square ( ( 0 , 0 ) , 10 )   $ print ( _STR_ )   $ print ( _STR_ )
pd . options . display . float_format = _STR_ . format
df . head ( )
df [ df [ _STR_ ] == _STR_ ] [ _STR_ ] . value_counts ( ) . head ( )
from pyspark . ml . evaluation import MulticlassClassificationEvaluator   $ evaluator = MulticlassClassificationEvaluator ( labelCol = _STR_ , predictionCol = _STR_ )   $ accuracy = evaluator . evaluate ( predictions )   $ print _STR_ , accuracy
df . sample ( 20 )
overallFireplaces = pd . get_dummies ( dfFull . Fireplaces )
def page_to_df ( page ) :   $ table = page . extract_table ( )   $ lines = table [ 1 : ]   $ return pd . DataFrame ( lines , columns = cols )
data . info ( )
Y_train . iloc [ 0 : 5 ]
twitter_archive_master . tweet_id = twitter_archive_master . tweet_id . astype ( _STR_ )
df_selection = df_selection . dropna ( how = _STR_ )
test . info ( )
results = [ ]   $ for tweet in tweepy . Cursor ( api . search , q = _STR_ ) . items ( 100 ) :   $ results . append ( tweet )   $ len ( results )
lin_pred = lin . predict ( x_test )   $ lin_pred [ : 5 ]
print ( today . strftime ( _STR_ ) )
x . drop ( range ( 2 ) )
X_train , X_test , y_train , y_test = train_test_split ( X_svd ,   $ y_encode ,   $ test_size = 0.2 ,   $ random_state = 42 )   $
co . steady_states
df = df [ pd . notnull ( df [ _STR_ ] ) ]
auth = tweepy . OAuthHandler ( consumer_key , consumer_secret )   $ auth . set_access_token ( access_token , access_token_secret )   $ api = tweepy . API ( auth )
tmdb_movies [ _STR_ ] = tmdb_movies [ _STR_ ] . dt . weekday_name
counts = df2 [ _STR_ ] . value_counts ( )   $ counts
final_names = [ x . replace ( primary_temp_column , _STR_ ) for x in out_columns ]   $ final_names = [ x . replace ( _STR_ , _STR_ ) for x in final_names ]   $ save_dat . columns = final_names   $ save_dat . index . name = _STR_
df_wm . to_csv ( _STR_ , encoding = _STR_ , index = False )   $
pd . concat ( [ city_loc , city_pop ] , join = _STR_ )
train . pivot_table ( values = _STR_ , index = _STR_ , aggfunc = np . mean )
all_data [ all_data [ _STR_ ] . isnull ( ) . values ]
df_bud = pd . read_csv ( budFile , usecols = budCols ,   $ dtype = bud_dtypes )
df_archive_clean [ _STR_ ] . sample ( 5 )
df3 = pd . DataFrame ( q3_results , columns = [ _STR_ , _STR_ ] )   $ df3 [ _STR_ ] = df3 [ _STR_ ] . astype ( float )   $ df3 . dtypes
plt . scatter ( y_test , y_test_pred , alpha = 0.05 )   $ plt . ylabel ( _STR_ )   $ plt . xlabel ( _STR_ ) ;   $ plt . title ( _STR_ ) ;
print ( type ( full_globe_temp ) )   $ print ( full_globe_temp . dtype )   $ print ( full_globe_temp . shape )   $ print ( full_globe_temp . nbytes )
df2 = df . drop ( ( df [ ( df . group == _STR_ ) & ( df . landing_page == _STR_ ) ] . index ) | ( df [ ( df . group == _STR_ ) & ( df . landing_page == _STR_ ) ] . index ) )
version = str ( int ( time . time ( ) ) )
da = DataFrame ( res . A , columns = vectorizer . get_feature_names ( ) )
df3 = pd . read_csv ( _STR_ )   $ df3 . head ( )
df2 [ df2 . converted == 1 ] . shape [ 0 ] / df2 . shape [ 0 ]
df [ _STR_ ] . value_counts ( )
grade_levels = [ textstat . textstat . textstat . flesch_kincaid_grade ( has_text . ix [ index ] . text ) for index in range ( len ( has_text ) ) ]
testing_array = np . concatenate ( ( testing_active_listing_dummy , test_pending_ratio ) , axis = 1 )
df . query ( _STR_ ) . group . count ( ) + df . query ( _STR_ ) . group . count ( )
df_characters [ _STR_ ] . agg ( [ _STR_ , _STR_ , _STR_ ] )
lv_workspace . get_data_filter_info ( step = 1 , subset = _STR_ )
autos [ _STR_ ] . describe ( )
from sklearn . model_selection import cross_val_score
df2 [ _STR_ ] = df2 [ _STR_ ] * df2 [ _STR_ ]   $ df2 [ _STR_ ] = df2 [ _STR_ ] * df2 [ _STR_ ]   $ model = sm . Logit ( df2 [ _STR_ ] , df2 [ [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] ] )   $ result = model . fit ( )   $ result . summary ( )
data . columns = [ _STR_ , _STR_ ]   $ data [ _STR_ ] = data . eval ( _STR_ )
def percentConvert ( value ) :   $ full = value * 100   $ return _STR_ . format ( full )
tweet_json . describe ( )
datetime . now ( ) . toordinal ( ) - datetime ( 1987 , 1 , 4 ) . toordinal ( )
avg_att_2015_BOS = nba_df . loc [ ( nba_df [ _STR_ ] == 2015 ) & ( nba_df [ _STR_ ] == _STR_ ) , _STR_ ] . mean ( )   $ round ( avg_att_2015_BOS , 0 )
> ljan1 = pd . read_csv ( _STR_ , names = [ _STR_ ] )   $ matched_id = [ np . where ( ( final0 [ _STR_ ] == i ) == True ) [ 0 ] [ 0 ] for i in jan1 [ _STR_ ] ]   $ final1 = final0 . drop ( matched_id )   $ final1 . head ( )   $ print final0 . shape , final1 . shape
df3 [ df3 [ _STR_ ] == _STR_ ] . head ( )
reviews = np . array ( tf . review )   $ reviews_vector = vectorizer . transform ( reviews )   $ predictions = clf . predict ( reviews_vector )   $
from sklearn . metrics import confusion_matrix   $ confusion_matrix ( y_true = df [ _STR_ ] ,   $ y_pred = df [ _STR_ ] )
def select_range ( df , min_value , max_value , col = _STR_ ) :   $ df2 = df . loc [ ( df != 0 ) . all ( axis = 1 ) , : ]   $ return df2 [ df [ col ] . between ( min_value , max_value , inclusive = False ) ]
pd . Timestamp ( _STR_ )
df_new_log = pd . get_dummies ( df_new , columns = [ _STR_ , _STR_ , _STR_ ] ) #create dummies $ df_new_log = df_new_log.drop(['landing_page_old_page','landing_page_new_page','group_control','country_CA'],axis=1) #drop unnecessary columns $ df_new_log['intercept'] = 1 $ df_new_log.head()
geo_db . describe ( )
ser = pd . Series ( [ _STR_ , _STR_ , _STR_ , _STR_ ] , index = [ 5 , 6 , 7 , 8 ] )   $ ser
sns . pairplot ( data , hue = _STR_ , size = 3 )
celtics . reset_index ( drop = False , inplace = True )   $ celtics . rename ( columns = { _STR_ : _STR_ } , inplace = True )
assert df_total . isna ( ) . any ( ) . any ( ) == False
df_h1b_ft_US = df_h1b [ map ( lambda x : x [ 1 ] . lca_case_workloc1_state in states   $ and x [ 1 ] . full_time_pos == _STR_   $ and ( x [ 1 ] . status == _STR_ or x [ 1 ] . status == _STR_ ) ,   $ df_h1b . iterrows ( ) ) ]   $ df_h1b_ft_US = df_h1b_ft_US . drop ( [ 33052 , 44675 , 45556 , 48967 , 51744 , 53898 , 135911 , 188139 , 225285 , 332969 , 366595 ] )
conn . autocommit = True   $ c = conn . cursor ( )
final_result = final_result [ pd . notnull ( final_result [ _STR_ ] ) ]   $ final_result = final_result . drop_duplicates ( subset = _STR_ ,   $ keep = _STR_ )   $ len ( final_result )
Base = automap_base ( )   $ Base . prepare ( engine , reflect = True )   $ Base . classes . keys ( )   $
data_full = data_full . dropna ( )
same_destination = trips [ trips [ _STR_ ] == trips [ _STR_ ] ]   $ same_destination = same_destination [ same_destination [ _STR_ ] > 120 ]   $ same_destination = same_destination [ same_destination [ _STR_ ] < 3600 ]   $ same_destination . sort_values ( by = _STR_ , ascending = True )
persort = percent . abs ( ) . sort_values ( ascending = False )
response = requests . get ( form_url ( _STR_ ) )   $ print_body ( response , max_array_components = 3 )
sh_max_df = pd . DataFrame ( np . array ( sh_results ) , columns = ( [ _STR_ , _STR_ ] ) )   $ sh_max_df
like_stats = USvideos . groupby ( _STR_ ) . agg ( { _STR_ : [ np . size , np . mean ] } )   $ atleast_5 = like_stats [ _STR_ ] [ _STR_ ] >= 5   $ like_stats [ atleast_5 ] . sort_values ( [ ( _STR_ , _STR_ ) ] , ascending = False ) [ : 10 ]   $ like_stats [ atleast_5 ] . sort_values ( [ ( _STR_ , _STR_ ) ] , ascending = True ) [ 0 : 10 ]
rf = RandomForestClassifier ( )   $ rfparams = { _STR_ : 42 , _STR_ : 8 , _STR_ : _STR_ ,   $ _STR_ : 8 , _STR_ : 300 , _STR_ : 0.2 }   $ rf . set_params ( ** rfparams )
ticks . head ( )
for key , value in sample_dic . iteritems ( ) :   $ print value
ed = [ _STR_ , _STR_ , _STR_ ]   $ school = wk_output [ wk_output . explain . str . contains ( _STR_ . join ( ed ) ) ]   $ school . shape   $ school . to_csv ( _STR_ )
len ( donald_trump_tweets [ _STR_ ] . value_counts ( ) )
import requests   $ import pprint   $ pp = pprint . PrettyPrinter ( indent = 4 )   $ from collections import OrderedDict , defaultdict , namedtuple
y_predict = [ round ( ii [ 0 ] ) for ii in model . predict ( x ) ]   $ deviate = [ 0.5 for aa , bb in zip ( y , y_predict ) if aa == bb ]   $ plt . figure ( )   $ plt . plot ( y , marker = _STR_ , linestyle = _STR_ )   $ plt . plot ( deviate , marker = _STR_ , markersize = 1 , linestyle = _STR_ , color = _STR_ )   $
for col in Y_train_df . columns :   $ nbsvm_models [ col ] = NbSvmClassifier ( C = 10 , dual = True ) . fit ( X_train_cont_doc , Y_train_df [ col ] )
df_archive [ _STR_ ] . value_counts ( )
one_test_pvalue = 1 - 0.19 / 2   $ one_test_pvalue
nar4 = nar3 . merge ( avg_yield [ [ _STR_ ] ] , left_on = _STR_ , right_index = True )
autos [ _STR_ ] . describe ( )
print pd . pivot_table ( data = df ,   $ index = _STR_ ,   $ columns = _STR_ ,   $ values = _STR_ ,   $ aggfunc = _STR_ )
df [ df [ _STR_ ] == _STR_ ] . groupby ( by = df [ df [ _STR_ ] == _STR_ ] . index . hour ) . count ( ) . plot ( y = _STR_ )
test = full_orig . set_index ( _STR_ ) [ _STR_ ]   $ X_test = test . drop ( [ _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 )
tweets . head ( )
bad_tc_isbns = pd . read_table ( _STR_ )   $ bad_tc_isbns = bad_tc_isbns [ _STR_ ]   $ print ( bad_tc_isbns . size )
X = preprocessing . scale ( X )
final [ _STR_ ] = final [ _STR_ ] . fillna ( 0 )
feature_importances [ : 10 ] . plot . bar ( )
os . getcwd ( )
first_commit_timestamp = git_log . loc [ git_log [ _STR_ ] == _STR_ , _STR_ ] . min ( )   $ last_commit_timestamp = pd . to_datetime ( _STR_ )   $ corrected_log = git_log . loc [ ( git_log [ _STR_ ] >= first_commit_timestamp ) & ( git_log [ _STR_ ] <= last_commit_timestamp ) ] . copy ( )   $ corrected_log [ _STR_ ] . describe ( )
tweets2 . text [ 0 ]
df_ratings . shape
cats_out = outcome . loc [ outcome [ _STR_ ] == _STR_ ]   $ cats_out . shape
df2 [ _STR_ ] = df2 [ _STR_ ] . map ( lambda x : x . lower ( ) )
beijing [ _STR_ ] = beijing [ _STR_ ] . astype ( _STR_ )
texts = [ [ token for token in text . split ( ) if frequency [ token ] > 1 and token not in nltk_stops ]   $ for text in documents ]   $ texts
df2 [ df2 . duplicated ( [ _STR_ ] , keep = False ) ] #find row information for the repeat user_id
from sklearn . neighbors import KNeighborsClassifier
nitrogen [ _STR_ ] . unique ( )
rowsToSkip = list ( range ( 28 ) )   $ rowsToSkip . append ( 29 )
data_scrapped = pd . read_csv ( _STR_ )
df . ix [ 1 : 4 ]
merged_data [ _STR_ ] = merged_data [ _STR_ ] . apply ( lambda x : 1 if x != _STR_ else 0 )
store_items = store_items . append ( new_store )   $ store_items
save ( p_nmf2 , _STR_ )
try :   $ cur_a . execute ( _STR_ )   $ except Exception as e :   $ print ( _STR_ , e )
shifted_backwards . tail ( 5 )
goog . head ( )
order_item_merge . to_csv ( _STR_ , index = False )
autos . rename ( { _STR_ : _STR_ } , axis = 1 , inplace = True )
import pandas as pd   $ df = pd . DataFrame ( )
fig , ax = plt . subplots ( figsize = ( 8 , 6 ) , dpi = 75 )   $ f1 = passes [ _STR_ ] . value_counts ( ) . plot ( kind = _STR_ )   $ f1 . set ( title = _STR_ , xlabel = _STR_ , ylabel = _STR_ )   $ plt . show ( )
from sklearn . pipeline import Pipeline   $ pipeline = Pipeline ( [   $ ( _STR_ , cv ) ,   $ ( _STR_ , model )       $ ] )
goog . plot ( alpha = 0.5 , styple = _STR_ )   $ goog . resample ( _STR_ ) . mean ( ) . plot ( stype = _STR_ )   $ goog . asfreq ( _STR_ ) . plot ( stype = _STR_ )   $ plt . legend ( [ _STR_ , _STR_ , _STR_ ] , loc = _STR_ )
df_birth . head ( )
df1 . columns = df2 . columns   $ df = pd . concat ( [ df1 , df2 ] )   $ df . head ( )
no_outliers_forecast_exp3 [ ( no_outliers_forecast_exp3 . index >= _STR_ ) & ( no_outliers_forecast_exp3 . index <= _STR_ ) ] . astype ( int )
import statsmodels . api as sm   $ convert_old = df2 . query ( _STR_ ) . count ( ) [ 0 ]   $ convert_new = df2 . query ( _STR_ ) . count ( ) [ 0 ]   $ n_old = df2 [ df2 [ _STR_ ] == _STR_ ] . count ( ) [ 0 ]   $ n_new = df2 [ df2 [ _STR_ ] == _STR_ ] . count ( ) [ 0 ]
m = md . get_learner ( emb_szs , len ( df . columns ) - len ( cat_vars ) ,   $ 0.04 , 1 , [ 1000 , 500 ] , [ 0.001 , 0.01 ] , y_range = y_range )   $ lr = 1e-3
lr . fit ( X_train , y_train )
a . iloc [ 3 ]
ds_issm = xr . open_dataset ( data_url1 )   $ ds_issm = ds_issm . swap_dims ( { _STR_ : _STR_ } )   $ ds_issm
fin_r . index
cfs = cfs . sort_samples ( _STR_ )
np . random . seed ( 123 )   $ b = pd . Series ( np . round ( np . random . uniform ( 0 , 1 , 10 ) , 2 ) )   $ c = b . copy ( )   $ b . index = np . random . permutation ( np . r_ [ 0 : 10 ] )   $ b
df . info ( )   $ df . isnull ( ) . sum ( ) #Counts all null values
data . show ( )
RNPA_new = RNPA [ RNPA [ _STR_ ] . str . contains ( _STR_ ) ]   $ RNPA_existing = RNPA [ ~ RNPA [ _STR_ ] . str . contains ( _STR_ ) ]
predicted_live = model . predict ( X_live )   $ predicted_live
len ( [ baby for baby in BDAY_PAIR_df . pair_age if baby < 3 ] )
grp1 = df . query ( _STR_ )   $ print ( _STR_ . format ( len ( grp1 ) ) )   $ grp2 = df . query ( _STR_ )   $ print ( _STR_ . format ( len ( grp2 ) ) )   $ print ( _STR_ . format ( len ( grp1 ) + len ( grp2 ) ) )
test_df . labels . mean ( )
test_bkk2 . reset_index ( inplace = True )   $ test_kyo2 . reset_index ( inplace = True )
S . decision_obj . stomResist . value = _STR_   $ S . decision_obj . stomResist . value
authors_with_name = authors_grouped_by_id_saved . select (   $ _STR_ , parse_name_info_udf ( _STR_ ) . alias ( _STR_ ) ) . cache ( )
with open ( _STR_ , _STR_ ) as piccle2 :   $ pickle . dump ( nmf_new , piccle2 )
pd . read_pickle ( _STR_ )
count_authors_with_given_numer_publications = data_final . groupby ( _STR_ , as_index = False ) [ _STR_ ] . count ( )   $ count_authors_with_given_numer_publications . columns = [ _STR_ , _STR_ ]   $ count_authors_with_given_numer_publications . head ( 20 )
df4 = df3 . merge ( df_countries , on = _STR_ )
old_page_converted = np . random . choice ( 2 , size = n_old , p = ( p_old , 1 - p_old ) )   $ old_page_converted
df2 . user_id . count ( ) == df2 . user_id . nunique ( )
pres_df [ _STR_ ] . tail ( )
pt = to_plot_df [ _STR_ ] . plot ( kind = _STR_ , figsize = ( 20 , 10 ) , rot = 0 , fontsize = 20 )   $ plt . xlabel ( pt . get_xlabel ( ) , fontsize = 22 )   $ plt . ylabel ( _STR_ , fontsize = 22 )   $ plt . title ( _STR_ , fontsize = 30 )
df_data . VITIMAFATAL . value_counts ( )
sentences = open ( _STR_ , _STR_ ) . readlines ( )
plt . hist ( reddit [ _STR_ ] , range = ( 20 , 1000 ) )   $ plt . xlabel ( _STR_ , fontsize = _STR_ )   $ plt . ylabel ( _STR_ , fontsize = _STR_ )   $ plt . title ( _STR_ , fontsize = _STR_ )   $ plt . show ( )
uusers = df [ _STR_ ] . nunique ( )   $ print ( _STR_ . format ( uusers ) )
live_weights . describe ( )
plot_data = df [ _STR_ ]   $ sns . kdeplot ( plot_data , bw = 100 )   $ plt . show ( )
model = models . TfidfModel ( corpus , normalize = True )
df2 = df [ ( ( df [ _STR_ ] == _STR_ ) == ( df [ _STR_ ] == _STR_ ) ) == True ]   $ df2 . head ( )
subred_num_avg . head ( 10 )
print ( a [ 0 ] ) # Implemented by __getitem__ $ a[0] = "t"  # No can do; strings are immutable.
suburban_ride_total = suburban_type_df . groupby ( [ _STR_ ] ) . count ( ) [ _STR_ ]   $ suburban_ride_total . head ( )   $
converted = df2 . query ( _STR_ ) . user_id . count ( )   $ total = df2 . query ( _STR_ ) . user_id . count ( )   $ converted / total
cust_data . head ( 3 )
p . to_timestamp ( _STR_ , _STR_ )
twitter_archive_clean = twitter_archive_clean [ ~ twitter_archive_clean . expanded_urls . isnull ( ) ]
xmlData . to_csv ( _STR_ , index = False )
number_unpaids . plot . scatter ( x = _STR_ , y = _STR_ , figsize = ( 20 , 10 ) )
import bikescore   $ bikescore . init ( )   $ testsavedmodel ( )   $
vol = vol . fillna ( 0. )   $ vol . head ( 20 )
nx . draw ( H , pos = nx . spring_layout ( H ) )   $ plt . show ( )
cur . execute ( _STR_ )   $ result = cur . fetchall ( )   $
import matplotlib . pylab as plt   $ lists = r_close . items ( ) # sorted by key, return a list of tuples $ x, y = zip(*lists) # unpack a list of pairs into two tuples $ plt.plot(x, y) $ plt.show()
startTime_user = train [ _STR_ ] . min ( )   $ y = train [ _STR_ ] . astype ( int )
twitter_df_clean = pd . merge ( twitter_df_clean , tweet_json_df ,   $ on = [ _STR_ ] , how = _STR_ )
df2 [ _STR_ ] = 1   $ df2 [ [ _STR_ , _STR_ ] ] = pd . get_dummies ( df2 [ _STR_ ] )   $
top_songs [ _STR_ ] = pd . to_datetime ( top_songs [ _STR_ ] )
tweet_file = _STR_
print ( _STR_ . format ( prediction_multiple [ 0 ] ) )   $ print ( _STR_ . format ( np . sum ( ( prediction_multiple - test_output_multiple ) ** 2 ) ) )
conversion_rate_all_pages = df2 . query ( _STR_ ) . shape [ 0 ] / df2 . shape [ 0 ]   $ print ( _STR_ . format ( conversion_rate_all_pages ) )
pres_df . ix [ 362865 ] # how to get row info by index number
joined = pd . read_feather ( _STR_ )   $ joined_test = pd . read_feather ( _STR_ )
autos . rename ( { _STR_ : _STR_ ,   $ _STR_ : _STR_ ,   $ _STR_ : _STR_ ,   $ _STR_ : _STR_ } ,   $ axis = 1 , inplace = True )
financial_crisis . iloc [ - 2 ]
% matplotlib inline   $ import numpy as np   $ import seaborn as sns   $ import scipy . stats as stats
hashtags [ _STR_ ] . str . lower ( ) . value_counts ( )
df = pd . DataFrame ( { _STR_ : [ 10 , 20 , 30 , 40 , 50 , 2000 ] ,   $ _STR_ : [ 1000 , 0 , 30 , 40 , 50 , 60 ] } )   $ df
plt . hist ( index_temp_df . tobs , bins = 12 )   $ plt . xlabel ( _STR_ )   $ plt . ylabel ( _STR_ )   $ plt . title ( _STR_ + most_active )   $ plt . show ( )
bad_iv_post = options_frame [ np . isnan ( options_frame [ _STR_ ] ) ]
fig = plt . figure ( figsize = ( 10 , 4 ) )   $ ax = fig . add_subplot ( 111 )   $ ax = resid_713 . plot ( ax = ax ) ;
oppstage = segmentData [ [ _STR_ , _STR_ , _STR_ ] ] . pivot_table (   $ index = [ _STR_ , _STR_ , _STR_ ] , aggfunc = len , fill_value = 0 ) . reset_index ( )
print ( _STR_ + color . RED + color . BOLD + os . getcwd ( ) + color . END )   $ assert os . getcwd ( ) . split ( _STR_ ) [ len ( os . getcwd ( ) . split ( _STR_ ) ) - 1 ] == str ( today ) , _STR_   $ imagelist = [ i for i in os . listdir ( ) if i . endswith ( _STR_ ) ]   $ imagelist
stations_info_dict = dict ( zip ( stations_info . name , stations_info . station ) )   $ stations_info_dict
df = pd . read_sql ( _STR_ , con = conn )   $ df . head ( )
diffs = np . array ( p_diffs )   $ null_vals = np . random . normal ( 0 , diffs . std ( ) , diffs . size )   $ plt . hist ( null_vals ) ;   $ plt . axvline ( x = obs_diff , color = _STR_ ) ;
len ( df_json . query ( _STR_ ) )
session . query ( Actor . first_name , Actor . last_name ) . frame ( ) . head ( )
twitter_archive_clean [ _STR_ ] = twitter_archive_clean . text . str . extract ( _STR_ , expand = True )   $
p_old = df2 . query ( _STR_ ) . shape [ 0 ] / df2 . query ( _STR_ ) . shape [ 0 ]
df = pd . read_csv ( _STR_ , nrows = 50000 )
op_add_comms [ _STR_ ] = op_add_comms [ _STR_ ] . apply ( lambda x : any ( substring in x . lower ( ) for substring in thanks ) )
df_archive_clean [ [ _STR_ , _STR_ ] ] . sample ( 5 )
n_new = len ( df2 . query ( _STR_ ) )                       $ print ( _STR_ , n_new )
infinity . to_csv ( _STR_ , encoding = _STR_ )
season12 = ALL [ ( ALL . index >= _STR_ ) & ( ALL . index <= _STR_ ) ]
X_train_df = pd . DataFrame ( X_train_matrix . todense ( ) ,   $ columns = tvec . get_feature_names ( ) ,   $ index = X_train . index )
idx = payments_all_yrs [ payments_all_yrs [ _STR_ ] > 0 ] . index . tolist ( )   $ len ( idx )   $
result = pd . DataFrame ( result )
df . to_excel ( _STR_ )
correct = y_test . eq ( p_y > 0.5 )   $ total_correct = sum ( correct )   $ print ( _STR_ , total_correct , _STR_ , len ( y_test ) , _STR_ , float ( total_correct ) / len ( y_test ) , _STR_ )
data [ _STR_ ] = pd . to_datetime ( data [ _STR_ ] )   $ data [ _STR_ ] = pd . to_datetime ( data [ _STR_ ] )   $ data . head ( 5 )   $
merkmale . xs ( 99550 , level = _STR_ ) . to_clipboard ( )
negGroups2 = list ( neg_tweets . group_id_x )   $ num_convos = len ( set ( negGroups ) )   $ print ( _STR_ )   $ companyNeg2 = filtered [ filtered . group_id . isin ( negGroups2 ) ]
birth_dates . head ( 3 )
df_ad_airings_filter_3 . to_pickle ( _STR_ )
treatment_conversion = df2 [ ( df2 [ _STR_ ] == _STR_ ) & ( df2 [ _STR_ ] == 1 ) ] . count ( )   $ total_treatment = df2 [ ( df2 [ _STR_ ] == _STR_ ) ] . count ( )   $ print ( treatment_conversion / total_treatment )
df = pd . read_sql ( _STR_ , con = conn_b )   $ df . head ( 15 )
random_forest . fit ( train , train_labels )   $ feature_importance_values = random_forest . feature_importances_   $ feature_importances = pd . DataFrame ( { _STR_ : features , _STR_ : feature_importance_values } )   $ predictions = random_forest . predict ( test )
taxi_hourly_df . shape
y_pred = model . predict ( x_test , batch_size = 1024 , verbose = 1 )
df2 . groupby ( _STR_ ) . count ( )
url_img = _STR_   $ browser . visit ( url_img )
df [ _STR_ ] = df . len_convo . apply ( lambda x : 1 if x > 1 else 0 )
bigdf_read = pd . read_csv ( _STR_ , index_col = 0 )
s_empty = pd . Series ( )   $ s_empty . empty
tobs_data = session . query ( Measurement . date , Measurement . tobs ) . \   $ filter ( sqlalchemy . and_ ( Measurement . date <= latest_date , Measurement . date >= tobs_start_date ) ) . \   $ filter ( Measurement . station == most_active_station ) . all ( )
import pandas as pd   $ import numpy as np   $ import matplotlib . pyplot as plt
( keys . shape , keys_0611 . shape )
images . img_num . value_counts ( )
dashdata [ _STR_ ] . max
libraries_df . tail ( )
df_2018 . isnull ( ) . sum ( )
train = read_data ( _STR_ )   $ validation = read_data ( _STR_ )   $ test = pd . read_csv ( _STR_ , sep = _STR_ )
print ( _STR_ )   $ techniques = lift . get_all_pre_techniques ( )   $ print ( len ( techniques ) )   $ df = json_normalize ( techniques )   $ df . reindex ( [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ] , axis = 1 ) [ 0 : 5 ]
Precipitation_DF . describe ( )
during [ _STR_ ] = during . groupby ( _STR_ ) [ _STR_ ] . transform ( pd . Series . value_counts )   $ during . sort ( _STR_ , ascending = False )   $ during . hashtags . dropna ( ) . head ( )
df . to_csv ( _STR_ )
url = _STR_   $ browser . visit ( url )
yelp_dataframe . head ( 2 )
df2 [ ( df2 . group == _STR_ ) ] . converted . mean ( )
df2 [ _STR_ ] . mean ( )
% matplotlib inline   $ import seaborn as sns     $ sns . set_style ( _STR_ )
full_clean_df . sample ( 1 )
lgbm = lgb . LGBMRegressor ( )   $ lgbmrscv = model_selection . RandomizedSearchCV ( lgbm , params , n_iter = 10 , cv = tscv , n_jobs = - 1 )     $ lgbmrscv . fit ( train [ col ] , np . log1p ( train [ _STR_ ] . values ) )   $ print ( lgbmrscv . best_params_ )
from sklearn . manifold import TSNE   $ tsne_model = TSNE ( n_components = 2 , verbose = 1 , random_state = 0 , angle = .99 , init = _STR_ , metric = _STR_ )   $ tsne_nmf = tsne_model . fit_transform ( nmf_doc_top )
monthly_sales = sales . groupby ( [ _STR_ , _STR_ , _STR_ ] ) . agg ( { _STR_ : _STR_ } )   $ monthly_sales . rename ( columns = { _STR_ : _STR_ } , inplace = True )   $ monthly_sales . reset_index ( level = [ _STR_ , _STR_ , _STR_ ] , inplace = True )   $ monthly_sales . head ( )
from sklearn . feature_extraction . text import CountVectorizer   $ from sklearn . decomposition import LatentDirichletAllocation
my_refinedquery = _STR_   $ refined_df = get_lims_dataframe ( my_refinedquery )   $ refined_df . tail ( )
autos . columns = [ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ , _STR_ , _STR_ , _STR_ ,   $ _STR_ ]
( autos [ _STR_ ]   $ . str [ : 10 ]   $ . value_counts ( normalize = True , dropna = False )   $ . sort_index ( )   $ )
raw_df . head ( 5 )
plt . scatter ( x = df [ _STR_ ] , y = df [ _STR_ ] ) ;
today = datetime . now ( )   $ dates = list ( pd . date_range ( join_date , today ) )   $ print ( _STR_ + str ( len ( dates ) ) ) # days since joining
fb . head ( )
appointments . shape
merged_portfolio_sp_latest_YTD_sp [ _STR_ ] = merged_portfolio_sp_latest_YTD_sp [ _STR_ ] . cumsum ( )   $ merged_portfolio_sp_latest_YTD_sp [ _STR_ ] = merged_portfolio_sp_latest_YTD_sp [ _STR_ ] . cumsum ( )   $ merged_portfolio_sp_latest_YTD_sp [ _STR_ ] = merged_portfolio_sp_latest_YTD_sp [ _STR_ ] . cumsum ( )   $ merged_portfolio_sp_latest_YTD_sp [ _STR_ ] = merged_portfolio_sp_latest_YTD_sp [ _STR_ ] / merged_portfolio_sp_latest_YTD_sp [ _STR_ ]   $ merged_portfolio_sp_latest_YTD_sp . head ( )   $
import sys   $   ! conda install - - yes - - prefix { sys . prefix } pandas - datareader
old_page_converted = np . random . choice ( [ 0 , 1 ] , size = n_old , p = [ 1 - p_old , p_old ] )   $ print ( old_page_converted )
tweet_archive_clean . loc [ 2436 ]
vip_reason = vip_reason . drop ( [ _STR_ , _STR_ ] , axis = 1 )   $ vip_reason = vip_reason . drop ( vip_reason . columns [ 0 ] , axis = 1 )
df [ _STR_ ] = np . array ( [ lemmatiz ( comment ) for comment in df [ _STR_ ] ] )
for zone in zones :   $ filename = _STR_ % zone + _STR_   $ z_query = _STR_ % ( zone )   $ joined . query ( z_query ) [ [ _STR_ , _STR_ , _STR_ , _STR_ ] ] . to_file ( filename , driver = _STR_ )
data_table . to_csv ( _STR_ , sep = _STR_ , encoding = _STR_ )
model_data [ _STR_ ] = [ 1 if x in final_dict else 0 for x in model_data . object_id ]   $ print ( model_data )
data . loc [ data . density > 10 , [ _STR_ , _STR_ ] ]
met . T . plot ( kind = _STR_ , figsize = ( 5 , 3 ) , xlim = ( 0 , 1 ) )   $ met
joined_data = official_data . join ( social_disorder ) . join ( positive_amentities ) . join ( negative_amentities ) . reset_index ( )   $ joined_data
tip_sample . describe ( )
start_date = session . query ( Measurement . date ) . order_by ( Measurement . date . desc ( ) ) . first ( )   $ start_date
rand_bg = np . random . uniform ( low = 2.5 , high = 19 , size = ( 21 , ) ) # create a data sample of random numbers $ rand_bg_s = pd.Series(rand_bg) $ bg_s = rand_bg_s.round(1) # reduce to one decimal places to it's a little easier to read $ bg_s # bg_s name confers 'bg' and 'Series', a nice short name that reminds you it's a Series
popular_programs = challange_1 [ _STR_ ] . value_counts ( )   $ popular_programs
repeated_user = df2 . groupby ( [ _STR_ ] ) . size ( ) . idxmax ( )   $ print ( repeated_user )
df_train [ 0 : 5 ] . T
xgb_pred = gbm . predict ( X_test )
lm = sm . Logit ( sub_df2 [ _STR_ ] , sub_df2 [ [ _STR_ , _STR_ , _STR_ ] ] )   $ results = lm . fit ( )   $ results . summary ( )
sm . stats . proportions_ztest ( [ convert_old , convert_new ] , [ n_old , n_new ] , alternative = _STR_ )
def str_merge ( str1 , str2 ) :   $ return _STR_ . format ( str1 , str2 )
haw [ _STR_ ] = haw [ _STR_ ] * haw [ _STR_ ] * 8.34 / 1000000
! head - n 2 Consumer_Complaints . csv
evaluator . plot_confusion_matrix ( normalize = False ,   $ title = _STR_ ,   $ print_confusion_matrix = False ,   $ figsize = ( 8 , 8 ) ,   $ colors = None )
autos . dropna ( subset = [ _STR_ ] , inplace = True )   $ autos . shape
twitter_archive_df_clean [ _STR_ ] . isnull ( ) . any ( )
x_axis = [ _STR_ ]   $ plt . bar ( x_axis , tobs_mean , width = 0.35 , yerr = ( tobs_max - tobs_min ) )   $ plt . title ( _STR_ )   $ plt . ylabel ( _STR_ )   $ plt . show ( )
df . columns = [ _STR_ , _STR_ ]
df . rating_numerator . value_counts ( )
% % time   $ with tb . open_file ( filename = _STR_ , mode = _STR_ ) as f :   $ carray = f . create_carray ( where = _STR_ , name = _STR_ , atom = tb . Float32Atom ( ) , shape = x . shape )   $ carray [ : ] = x ** 3 + 0.5 * x ** 2 - x
rr . tail ( 3 )
df . describe ( )
x = raw_valid . copy ( )   $ x [ _STR_ ] = np . std ( preds , axis = 0 )   $ x [ _STR_ ] = np . mean ( preds , axis = 0 )
popularity . info ( )
import plotly . graph_objs as go   $ import plotly . plotly as py
joined = load_df ( _STR_ )
actual_diff = p_treatment - p_control   $ ( p_diffs > actual_diff ) . mean ( )
df98 = pd . read_csv ( _STR_ )
html_table_marsfacts = df . to_html ( )   $ html_table_marsfacts
df . loc [ _STR_ : _STR_ , _STR_ : _STR_ ]
df2 . shape   $ df2 . query ( _STR_ )   $
frames = [ k_var , k_var1 ]   $ k_var_concat = pd . concat ( frames )   $ g = k_var_concat . groupby ( [ _STR_ ] )   $ size = g . size ( )   $ size [ size > 1 ] . head ( )
logit = sm . Logit ( df4 [ _STR_ ] , df4 [ [ _STR_ , _STR_ , _STR_ ] ] )   $ result = logit . fit ( )
house_data [ _STR_ ] . nunique ( )
import statsmodels . api as sm   $ convert_old = sum ( df2 [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] )   $ convert_new = sum ( df2 [ df2 [ _STR_ ] == _STR_ ] [ _STR_ ] )   $ n_old = sum ( df2 [ _STR_ ] == _STR_ )   $ n_new = sum ( df2 [ _STR_ ] == _STR_ )
raw_df = raw_df . reset_index ( )   $ raw_df = raw_df . rename ( index = str , columns = { _STR_ : _STR_ , 1 : _STR_ , 2 : _STR_ , 3 : _STR_ } )   $ raw_df . index . names = [ _STR_ ]
from sklearn import svm   $ clf = svm . SVC ( kernel = _STR_ )   $ clf . fit ( X_train , y_train )
html = browser . html   $ news_soup = BeautifulSoup ( html , _STR_ )   $
