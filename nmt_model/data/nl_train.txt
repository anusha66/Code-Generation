#'Rochester': '2409d5aabed47f79'
# Use `engine.execute` to select and display the first 10 rows from the table
# removing un-activated users 
#admissions api connection
# use pandas to_datetime() function to convert string column to datetime
# Display of first 10 elements from dataframe:
# Dimensions. #
# Combine each stomatal resistance parameterizations # add label 
#'Newark': 'fa3435044b52ecc7'
# What is the median?
# Grabs the last date entry in the data table
# Determine the column index for 'Close'
# one way to tokenize:
# drop low because it is really common
# grabs the first row of our data table
# And visualizing...
#get average-to-date for each PA / player to avoid info leakage
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# accediendo a una columna por el atributo
#sns.heatmap(corrmat, vmax=.8, square=True);
# pick your favorite classfication model
# RN/PA decomposition
#Limit data to those with more than 10 years data and still operating
# Clean up
#train.drop('timePeriodStart',1,inplace=True)
# get the current date and time (now)
# favorite table # for row in table_rows: #     print(row.text)
# a number (float)
# 降低采样频率 以一小时为单位显示总量变化
## strip whitespace in column headers
# Take the raw data and filter out a single person just to check if the data looks reasonable.
# T :: it will transform the table and make rows as columns and columns as rows
#Save
# Extract the mean of lenghts:
#move the lat and long columns to be next to the address #You can rearrange columns directly by specifying their order: #df = df[['a', 'y', 'b', 'x']]
# we simplify the purchase history to only include user_id & product_id
# Drop columns with all NAN #c_df.head()
# check inserted records
# Querying by Team by Game (only shows one team's stats)
# let's declare those files above as variables in Python.
# Show Figure
# Print the RESULT LIST so far ###print(f"RESULTS LIST: '{results_list}'") # Prepare DataFrame for the bar plot
# Number of dialogue
# Look at first x rows in the table
# Create one dimensional NumPy array from a list
# show last 5 rows of data frame
# pytz
# Create and display the first scatter plot
# Basic search for '#Australia'
# столбец salary заполнен на треть
#### Results for 09-05-2016
### Create the necessary dummy variables
#tweetsPorFecha['tweetCreated']=tweetsPorFecha['tweetCreated'].map(lambda x: x.strftime('%Y/%m/%d') if x else '')
# To insert a document into a collection we can use the insert_one() method:
# Find the probability indiviual control group they converted
# overall info
# What does this look like with 15 topics (potentially the knee)
# Clean, tokenize, and apply padding / truncating such that each document length = 70 #  also, retain only the top 8,000 words in the vocabulary and set the remaining words #  to 1 which will become common index for rare words 
# show a sample of the sub gene dataframe
#Add a column
# Load raw hashrate data over the last two years # Downloaded from https://blockchain.info/charts/hash-rate
# Create engine using the `demographics.sqlite` database file
# 17. Create a new Data Frame with columns named cat and score from your results in part (16),  # for the column with the largest score and the actual score respectively.
# all data on June 3rd of 2015:
# Plug sentiment in to df_tick
#1 Collect data from the Franfurt Stock Exchange, for the ticker AFX_X,  # for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD).
# Check the list of column
#json.dump(fp, youtube_urls, separators=(',', ':'))
# 4. What was the largest change in any one day (based on High and Low price)? # Alternative solution using list comprehension.
#A DataFrame has a second index, representing the columns:
#persistência dos dados
#save data to a csv file
# A:
# 詳細結果の取得
# df_4_test.dtypes
#find any numbers within the posts
#bins='log', cmap='inferno'
# Find out Shape of the data
#combine so all charges have a proper unblended rate
#.apply(myutilObj.my_tokenizer)
#Export Canadian Tire file to local machine
#info_final_gente = pd.merge(info_final, avisos_detalles, on = 'idaviso', how = 'left') #info_final_avisos = pd.merge(avisos_detalles, info_final, on = 'idaviso', how = 'left')
# Collect data from the Fankfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017
#@title Default title text
# приведем слова к нормальной форме
# get artists whose name contains "Aerosmith"
#writing year month and size in MB into CSV
# show dataframe first rows
# A:
#Use the read_html function in Pandas to automatically scrape tabular data of mars facts from the page.
# concatenate first three rows of above dataframes
SQL = """SELECT * FROM github_issues_2"""$
# read grid id to subset # unique
#delete * gender type
# Confusion Matrix
# Convert created_utc to readable format
# First, import the relevant modules
# Authentication
#DATA_ROOT = 'data/workshop-content18/5-cloudpbx/data/cloudpbx_sample_data_10k/' #CSV_FILE_PATH = os.path.join('locn-filtered.csv')
# Fill the address from table [EDW].[SHIPTO_DIM]: CustomerID = '0000100273'
#Example2: Import File from URL
# Word frequency for terms only (no hashtags, no mentions)
#Read data into a pandas DataFrame. Sorry this also takes a moment, approx half a minute
# initialize your estimator # train your model
#Constructing a model on a scaled dataset
# Retrieve data from Mars Weather # Create BeautifulSoup object; parse with 'html.parser'
#print key["API_KEY"], key["API_SECRET"], key["ACCESS_TOKEN"], key["ACCESS_TOKEN_SECRET"]
# date 컬럼 포맷 수정 
# There are not any null values
# Import studies text file
# Save data to Excel
#selecting a subset of the rows so it fits in slide.
# Write to CSV
# import fire predictor # remove fires with missing values # greater than 5000 acres
# set the new url and go to the site
# create a DatetimeIndex using a time zone
#au.plot_user_popularity(very_pop_df, day_list)
# Sample five random movies
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# subset temp to us
# default behavior is dayfirst=False
#lets plot the time series
# Collect data from 2017
# get the index labels, aka row labels
def to_date(s, year):$     '''Returns a date from the datetime library from a string like \'January 1\''''$
# drop duplicates and unnecessary column # set `keep='last'` to keep the last duplicate.
# Initialize MGXS Library with OpenMC statepoint data
# Logistic regression confusion matrix
# Even if you pause the display of tweets, your stream is still connected to Twitter! # To disconnect (for example, if you want to change which words you are searching for),  # use the disconnect() function.
# Rename the unnamed column to 'time'
#creating home,Opp_Team,Win columns from Match_up
# Merge files by station
# review small df window after engineering lagged features/target vector(s)
# Export to csv
qry = '''\$ DROP TABLE IF EXISTS lucace.disc_667_stage_10_daily_active_users$ '''$
# In Pandas DF # Average all polarities by news source # View the polarities
#df_weather.Date = pd.to_datetime(df_weather.Date).dt.date
# check option and selected method of (27) choice of thermal conductivity representation for soil in Decision file
#for post in posts.find({"author": "Mike"}):
# select a strike to plot # get the call options # set the strike as the index so pandas plots nicely
#select the call option with the fifth expiry date
# costumise dates with time info to midnight convention:
# Precipitation Analysis # - Design a query to retrieve the last 12 months of precipitation data.
# посмотрим на полученные значения
# merging zipincome with zip_dest
# Build models
# Filter rows by considering "name" and maximum "capacity # Apply general template of columns
# Score the predictions
class Square(Rectangle):$     """ Simple circle, inheriting from Rectangle """$
# Create the hawaii sqlite engine created in database_engineering.ipynb file # Declare the base as automap_base # Use the Base class to reflect the database tables
# NLTK's punkt includes a pre-trained tokenizer for english which can # be used to transform (split) new paragraph observations into sentences
#Have to run later, no national events in training set
# load dataset
# change appointmentduration to hours
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# Convert each 'tweets' DataFrame to a list of dicts
# checking the easist way of looking the files you have in the directory
#print(highlight(json.dumps(jcustomer_info, indent=4, sort_keys=True), JsonLexer(), TerminalTrueColorFormatter())) # remove the comment from the beginning of this line to see the full results
#print first ten rows of theft data
#Read in the results.csv
# check if any values are present
# read the original frame in from cache (pickle)
# Let's make some recommendations
# printing parameters AIC,BIC and HQIC
# The following command retrieves the symbols and price for all stocks with a price less than 10 and greater than 0
# These are only hashtags that have value_counts greater than 4
#pivot table showing the Age and affair status
# Importing the built-in logging module
# Append distance to an empty list # Iterate thru get_distance to return the distance between coordinates
# Creating another lamba function to find spaces and split the each value
# This will print the most similar words present in the model
#Group by the status values, and then describe..
# Establish a new column for person's age at designation
# делаем по инструкции с сайта NVidia
# Variables:
#Charlotte
#Save figure
# get the gene names # show to value of a random selection of the attribute column of 10 records
#
# count number of each unique values
## Writing to csv
#---Connecting to a MongoDB client---
# Select row of data by index name
#format the trend date from string format of mm/dd/yy to date  #format mm-dd-yy, for correct datatype assignment
# # Perform a query to retrieve the data and precipitation scores # Save the query results as a Pandas DataFrame and set the index to the date column # Sort the dataframe by date
# hierarchical index in conjunction with sorting # (note that here we should have converted the day to ordinal) # tips.set_index(["sex","day"]).sort_values(["sex", "day"]) works the same
# worst 5 breweries - expanded to ignore the "garbage beer" breweries at the very bottom
# Number of ratings
#'Washington': '01fbe706f872cb32'
# We display the first 10 elements of the dataframe: #display(data2.head(10))
# Apply the wrangling and cleaning function
# write to csv the cluster selection
# Golf KOL pred
# Only co-occurence > 1
# Get the title of Guido's webpage: guido_title # Print the title of Guido's webpage to the shell
#== By Label: row range by data index, column range by column names 
#Grouped by data in a dataframe groupby
#Now let's get the genres of every movie in our original dataframe #And drop the unnecessary information
# Save DF to json file
# AUC 
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
# a series' index
# how big is the largest collection?
#torch.manual_seed(args.seed) #if args.cuda: #    torch.cuda.manual_seed(args.seed)
# Highest & Lowest opening price
# initialize the twitter API object
# For binary classification, response should be a factor
# Tells us how significant our z-score is # Tells us what our critical value at 95% confidence
q = '''$     DROP VIEW customer_2;$ '''$
# praw object for script to use Reddit API
# Add a new column that we will use for groupby operations
#Seasonality calculation
# baseline accuracy
# first print the Dataset object to see what we've got # close the Dataset.
# Load query data into a dataframe
# Now let's see how this works in a base case, and with the labor # force growth rate boosted by 2% per year in the alternative case...
#There are too many document types to plot on one chart so select the types with highest maximum
# accediendo a una fila por posición entera y obteniendo una serie
# Distances table
#Clipping outliers/wierd values (Conditional Imputation)
# Likes vs retweets visualization:
# tips.tail() # tips.sample()
# remove colums with all na
# One hot encoding
# you can convert your data frame index to period index
# Pipeline class # Create and return an empty Pipeline
# common words from the reviews
# Nasa Mars News URL to Scrape
# add prefix to column names
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
# I will add both and see which makes the most sense
# # Getting back the objects:
# Define the demonstration data. #base_df.drop(['index'], inplace=True)
# Create the inspector and connect it to the engine # Collect the names of tables within the database
#this will upsample:
# We'll use gain first because that is the most useful for my purposes (saving more than $5 is necessary as it is cost of trade)
# Value of old_page_converted
# Lower value.
# Make a list of all Terms per season
## Amount paid
# results = soup.find_all('li', class_='rollover_description_inner')
# reset the index to the date
# Device
# Create an array of 10 random integers between 1 and 100
# concat df and coming_next_reason
#mongo returns a generator, which we call cursor
# Set file names for train and test data
# Divide each number of each countries columns by it's annual maximum 
# Add chanel column to users table
#validation set accuracy
#Test to make sure connection is working.
# Movies with the lowest RottenTomatoes rating
#x=dataframe['Date']
#client.create_database(dbname)
# get indices for any other keys that are part of data
# We're plotting them all at once so... there are going to be some visual issues. # Volume distorts the visualization because those values are much larger than the prices. # Let's put all those other ones on a plot.
#Most common title words 
#get the mean prediction across all 
#msft['2012-01-03'] while this will not work for dataframe, this syntax can work for series
# Task 1 : Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD) #Collect data from the Frankfurt Stock Exchange, for the ticker AFX_X for the whole year 2017
# create grid id array
# Find the div that will allow you to retrieve the news title # Use this html, do not go back to the website to look for it # Latest news title
# Setting up plotting in Jupyter notebooks # plot the data # ... YOUR CODE FOR TASK 8 ...
# A one liner as I wanted to explore using islice and thought it should be possible
# make the date series into a dataframe with the key 
#we also don't really care about how the members join
#白噪聲檢查 #返回統計量和p值
#WARNING: Long execution time - 20-30 mins
# Use tally arithmetic to ensure that the absorption- and scattering-to-total MGXS ratios sum to unity # The sum ratio is a derived tally which can generate Pandas DataFrames for inspection
# writing it to csv #pred = pd.DataFrame(predictions, columns=["rating"]).to_csv("24nov.csv", index=False) #predictions.to_csv("submission.csv", columns=["id","rating"], index_lable=tf.id)
# устанавливаем патч
# import pyLDAvis # import pyLDAvis.gensim # import warnings
#Make sure no missing geometry
# Examining the paragraphs found in the body / the body data
## get core and xtra lists into dataframes for merging into eppd
#create a new master dataframe by concatanating the original master and dummies dataframe
#urls of the 10 top commented posts
# len(xres3) # len(xres3.items()) # object.keys(xres).length
# first thought is to check out some trump tweets
#l4exc_l4exc
# KNN model 
# Group by labels to display how the clusters were formed.
## Store the imputed data for further processing
# Sort tweets by date
# Assign directory paths and SQLite file name # sqlite_pth = os.path.join(dpth, os.path.pardir, "data", dbname_sqlite)
# Display the new number of thermal distesses
#Extract years and months into a new columns in our dataframe #Have a look...
#Outliers
# Find stops which are in in stops but not in oz_stops
#getting the summary of the above model
# parse the sentence
# unemp rate for this april 2018 was 4.1
#     print(screen_name)
# went through the Quandl documentation and determined that I needed to find the Franfurt Stock Exchange dataset code # as well as add start_date and end_date parameters to make sure the API request via HTTP GET was done properly
# Missing value due to disabled value
# Stip off the year and save a list of %m-%d strings
# Using ARMA model
#dftemp = df1.where(df1['Area'] == 'Iceland') ...... where command keeps array size same so not used here
# Dropping the Id column from the dataframe # Checking the shape of the dataframe
# Solution 2 : numpy diff function
# remove unwanted fields
#df['LinkedAccountId'].apply(lambda x: x in acct_dict print(acct_dict[x])) #df['LinkedAccountId'].apply(lambda x:x in acct_dict acct_dict[x])
# by year and in-season vs. offseason
# set the index of all_cards to be the name column, so we can search cards by name
# Assign tie values the maximum rank in the group
# Verify that there aren't any null values
# Structures data library # Columnar data library
# Create a training DataFrame
# We first observe the head data.
# Get column and types Station Table # columns
# SAVE A CSV OF THE NEW SPOTIFY TABLE ("spotify_merged.csv") WHICH NOW FLAGS SONGS THAT REACH NUMBER ONE IN A REGION
# Saving our dataframe to file
# easy to access data # efficiency wise, better (database)
#calculate the mean absolute error
#Los Angeles': '3b77caf94bfc81fe'
#df_pd = df_pd.set_index("timestamp") #df_pd.set_index("timestamp")
# Preprocessing the Date (Created at)
#learner.load_cycle('adam3_10',2)
#And to check the tail
# we don't need file name anymore
#build url  #go to facebook get the GameOfThrone page name and then get the id.
'''Ridge'''$
# Save to file
# convert utc to est? Find out what tz it is first.
#rename state_abbrev from state_lookup table to state_code
# create a DataFrame with the minimum and maximum values of TV
# Create average purchase per product feature
# reducing our "train_4" sparse matrix which has already been scaled # this crashes the kernel at 1000 and 100 components...
# pd.DataFrame(cursor.fetchall(), columns=['User_id','User Name','Following','Followers','TweetCount'])
#sort by Fox in order to create different color scatter plot
# Read the data file.
# plot autocorrelation function for RN/PA data
#create a timestamp to get values from the FAll 2018 term that are prior to Jan 1 2017
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
#Tells us non-null types
# Task 1
# We can also do a crosstab (not sure how much sense there is in this but...):
# graph api call using requests get method.
# Replace key with Googlemap API developer key # https://github.com/googlemaps/google-maps-services-python
# return the dtypes
# Test score is better than the train score - model generalizes well 
#sns.distplot(x)
# shows time stability
# Extract slice - 1
# IQR
# map vip to 1's and 0's
# Visualize
# Find the new_page and 
# adding prefix AD_ to column names
# We'll hold out 10% of our data for validation and leave 90% for training
#Tweepy Cursor handles pagination .. 
##### ndarray of transposed
#we got two new feature .now update the dataframe with updated with new information
# Create and display the second scatter plot
# replace NaN with placeholder value, let's say 99999
# заменяем значения больше 1 на 1
# Put the data into HDFS - adjust path or filename as needed
#this weather station started in 2013
# Instantiate an instance of the classifier we defined above. # This method, 'get_classifier', takes arguments 'self' and 'key' # It then returns 'self.config.classifiers[key]'
# checking the null values in the DF , according to Ben this is the fancy one
# validation 24 hours last day 
#Variable Time is now decimal day time.
### Number 1
# look at all models and topics learned in class
#absolutely useless, must be a better format
#what is th eofficial Water Resources Station Catalogue ID of site '105001'?
# read the json data into a pandas dataframe # set column 'created_at' to the index # convert timestamp index to a datetime index
# These are taken from arxiv paper JH.
# what is the type of the index?
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# let's start with a few simple observations: # - as in the case of dataframes, indexing series with  # integers vs slices/list gives different types of outputs
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
### Retain Only the Top 12 most common wine types
# Get movies not rated by new user # keep just those not on the ID list # Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies
# market cap of fund using W matrix
# Get a list of column names and types # columns
# create a counter to look at freqency of insert_id # filter list to only include ids that have a count greater than 1 # how many insert_ids are duplicated?
# extract full monthly grid
# initiate geoip client
# remove unnecessary columns(0 or 1 non-null value): contributors, coordinates, geo, place
# close the connection
# Get the count of total number of records in the dataframe
# instantiate model object
# specify column type
# 'Shreveport': '4ec71fc3f2579572'
# Example 1 # Example 2
#print(tmp)
# Create pivot table
#### Look at DEPT to make sure it has gone up, it has!
# get 5000 random ids
# Generate auth tokens for Colab
# Colors # Define x-axis and xlabels
def saveDataFrame(df,path):$     '''save DataFerame in csv format'''$
# drop unwanted columns
# read the contents of the file into a DataFrame
# Print the index of airquality_pivot
#start_time = timeit.default_timer() #create a copy of the dataframe to label encode
# Create the Dataset object. # Map features and labels with the parse function.
# to compare score between train / test set.  
# save matrix as csv.  # np.savetxt("foo.csv", g.featureMatrix , delimiter=",") # another way
# ML modules
# Plot the results using DataFrame.plot
# read local csv
# identifying when new_page and treatment don't line up
#https://docs.python.org/3.6/library/datetime.html#strftime-strptime-behavior
#Click here and press Shift+Enter
# descending order
#Convert to numpy array and calculate the p-value
#proportions_ztest returns the z_score, p_value
# Save an image of the chart and print it to the screen
# Plot all columns (default)
# drop duplicates
#Save News Tweets to csv
## convert disctionary to pandas Data Frame for easier manipulation
# This isn't right. There are no long positions. Take another look at the code and fix it.
# Replace Nan's with values from the previous row or column with forward filling # Replace each NaN with the value from the previous value along the given axis
# Get last tweet or mars weather
# Veamos algunas muestras
#Convert titles to a list for analysis
# Your friendly tokenizer # Numpy
# Prepare scoring payload using these random values
# 1. 请将数据导⼊入pandas中，加上列列名
"""$ Check results$ """$
# how many values less than 6?
#dropping! the smallest number
# Articles are contained in a <div class="article--container"> element.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# .ix can be used to look up rows by either the index label or location,  # essentially combining .loc and .iloc in one.
#population = np.random.randint(2, size=(36, 10, 29)) # random population of 10 binary genetic strings
# 6. If we define the "Tasker conversion rate" as the number of times a #    Tasker has been hired, out of the number of times the Tasker has #    been shown, how many Taskers have a conversion rate of 100%
# how outliers many below 1.625?
# This code saves the cleaned salary information back to the main data folder
# chaining head onto the frequency count
#duplicates #Credit:Hector Ian Martinez
#Replace '|' with ',' and then replace ',' with ',\n' to save the entire dataset into an excel
# Create a feature DataFrame for this week's games.
# Select and display the first 5 rows from the table
# align train and test frames
# This new column determines what SP 500 equivalent purchase would have been at purchase date of stock.
# Summary statistics of overall data-columns (where the data-type can make such statistics to happen)
# Step 10: Conduct PCA to limit features to 2 ## (optional) use svd_solver='dense'
# find overall sentiments
# to make plots appear in the notebook
## Using Dow 30 dividend data sourced from yahoo finance
# Initialize reader object: urb_pop_reader # Get the first DataFrame chunk: df_urb_pop # Check out the head of the DataFrame
#Merge every row into one paragraph, then split it by sentences ('.') #Use 'type' to check the data type of globalCitySentences
#d = feedparser.parse('http://g1.globo.com/dynamo/rss2.xml')
#under_sample_data['Diff'] = under_sample_data['Diff'].fillna(0)
#Manchester has so many retweets! Let's see what that tweet is that makes it so good #Looks like it's a video
# better than get_hist_data
# Print each negative tweet
# First, get a list of all of the hemispheres
# Initialization
#  利用bins來設置偏移
# The index name of the previous row can be recovered with:
# creating vip reason df
# create lagged autoregressive features
#calculate the decision tree's mean absolute error
# the off-season.
#Extract the month from the datetime column
#converts to dataframe
# Create the 'DateTime' column in the price2017 DataFrame
# How many ticket in each type
# 末尾が"_at"で終わるunixtimeのカラムをdatetimeに変換する
# Read one site as a CSV
#find all mentions of woman/women within the posts
#Melt with 2 variables
# We wish to iterate over the number themselves
# by year and in-season vs. offseason
# Surveys by day
# rename id tp tweet_id
#run for March users
# Put in the end points for the forecast
# split one review into separate words # remove stop words from review text
#Sharpe Ratio #Menor SD
# Setup Tweepy API Authentication
#KNN Result
# Pulls the first 5 columns from the data
#save new title in a variable
# $로 시작하는데 d+를 해줌으로써 1자리 10의자리 100의자리~ 까지 받을 수 있고 .뒤에 숫자가 또 올지 안올지는모르지만 올수있다.
#Create a series with 100 random numbers
c.execute('''SELECT * FROM artist_db''')$
# Verifying 
#Create dataframe of sentiments
# Requests
#Conver Receipts into dataframe
# print a single tweet json for reference #print(json.dumps(test_tweet[0], sort_keys=True, indent=4))
# target is the handle. # make trump 1 and sanders 0
#Other filtering examples
# 3.2.C OUTPUT/ANSWER
# We display the updated dataframe with the new column:
# distribution plot without the KDE
# less than 500 # more than 500
# a two year range of daily data in a Series # only select those in 2013
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# print some sample training trees
#change the directory
#check the index and shape
# read excel file # only reads first sheet
# Creating a dataframe to house the data
#  This will totally remove all duplicates 
# Loading in the pandas module # Reading in the log file # Printing out the first 5 rows
# Test all of the assumptions
#We parse the date to have a uniform 
## Reading the conn logs for our analysis
# Create pivot table
#get station count, has been checked with measurement station count
# LDA Model 
# TODO: Tune Parameters
#Download the dataset. Takes a while, file is 800MB. Due to a slight complication with jupyter notebooks, #the progress bar is not displayed, so be patient.
# We Create a DataFrame that only has selected items for both Alice and Bob # We display sel_shopping_cart
# recode school_type
# converting to date to pd date time
# Filter outliers in Max Capacity
#Click here and press Shift+Enter
# Construct pandas Series objects
# before merging, both zip columns need to be the same datatype
#Sorting by date
# corr.to_csv("correlationmatrix.csv", sep='\t', encoding='utf-8')
# Number of populate for new Page
# add response variable
# Preview the Data in the station dataframe...as expected from the above...there are 9 unique stations
#bikedataframe = bikedataframe.drop(['DATE', 'REPORTTPYE'], axis=1)
### Create the necessary dummy variables for the country
#Note the as_index option will enable the datasets to be joined. The default behaviour of the groupby method is to make the groupby variable an index.
# find historical data for 2017
# it took the mean of the columns, then the mean of those means # however, s1 and s2 have a different number of terms
# create activity prediction matrix
# save our pca classifier
#Rearranging/ Renaming data for graph
# Or one specific month
# check NA
# Percentage Change Column is created
# df=load_df('rolled_filled_elapsed_events_df.pkl')
# Print percentages:
# Make columns for seasons and terms
# 因為msno有相同的欄位,所以用left_index=True, right_index=True
# show ohlc resampling
# transform the rate into numeric variables
# Using proportions_ztest(count, nobs, value=None, alternative='two-sided', prop_var=False)
# create grid id 1 to 1535 and save as type string
#transit_df_rsmpld = transit_df.reset_index().set_index('DATETIME').resample('1D',how='sum')
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Atlanta
# Use Inspector to print the column names and types for station
# 日時を文字列から時刻オブジェクトへ変換
# drop dry because it is really common
#'Denver': 'b49b3053b5c25bf5'
# How many stations are available in this dataset?
# look at the example from above, what that ticket is actually talking about
# Set up a collection name "test_database" in MongoDB
#join Shopify orders to Baby-camera Pair date 
# df.head().T.head(40)
#You can load json, text and other files using sqlContext #unlinke an RDD, this will attempt to create a schema around the data #self describing data works really well for this
# adding prefix ATTEND_ to column names
#Creating a (pandas) DataFrame # We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
#check for any null values in the trimmed dataset 
#create date variables for title
# mean Closing price for specific month:
# Drop duplicates 2862 in user_id # Verify the drop action
# Run this cell
#df[df['message'].map(len) < 2] #any empty or very small messages?
# Store tables
# Plot just the Dew Point data
# Setup the hyperparameter grid #,'max_depth':[4,6,8,10]
#Drop rows with all NAN
#training the dataset
# Export file as a csv, without the Pandas index, but with the header
# Importing Libraies
#Compute annual Nitrate concentration
#recall: (X_train, y_train), (X_test, y_test) = mnist.load_data()
# Read one site as a DataFrame
#Leitura do data set
# Create similarity measure object in tf-idf space
# The "description" column has the text that we want to analyze 
#data_spd['SA'] = np.array([ analize_sentiment(tweet) for tweet in data_spd['tweets_spd'] ])
#Calculate the lowest opening prices for the stock during 2017
# calculate the mean of the first minute of the walk
# Checking missing values
#Plot bar charts of the three summary tables
# convert crfa_a to numeric value
# Setup Tweepy API Authentication
# We extract the mean of lenghts:
#Convert into a Python dictionary
# Here's a example code snippet you might use to define the feature columns:
# Task C answer check
# Look at our current features df
# df.tail(2)
# Variável para armazenar os tweets com a palavra escolhida
# convert timestamp to a time value
#results = Geocoder.reverse_geocode(df['latitude'][0], df['longitude'][0])
#Calculate the difference in NDVI from before to after cyclone
# Probability an individual recieved new page
# reindex to inlcude all claendar days, using forward fill #fin_r = fin_r.reindex(r_top10_mat.index, method='ffill')
# Now, call the Quandl API and pull out a small sample of the data (only 5 days) to get a glimpse # into the JSON structure that will be return
#In looking at unique values in DESC category, it does not appear to be anything we really care about.
# DataFrameをCSVエクスポート
# Setting index
# best/worst breweries, min x beers
# reduce graph data size
# display specific number of rows from bottom of the file 5 is the default 
#sort lastest_consensus_created_date to find the latest ones.
#using . notation
# to read multiple files just add the names
#shows.to_pickle("ismyshowcancelled_raw_pull.pkl")
# Setup Tweepy API Authentication
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
cur_b.execute('''$ ''')$ conn_b.commit()
#'St. Petersburg': '5d231ed8656fcf5a'
#count of converted values
# read in a (large) convolutional neural network model # this will only work after the CNN model is downloaded (~800MB) # e.g. python -m spacy download en_core_web_lg
#target
#Set site, parameter, and stat codes
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# convert back to spark dataframe
# Concat the two dataframes together columnwise
#TODO: get qty rejected # get ncrs
#scrape just 5 of the threads
# Display the client version number.
##### first examine loc()
#'Oklahoma City': '9531d4e3bbafc09d'
# convert from MSE to RMSE
#API setup
#Cincinnati
# plot the pie chart
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Likes vs retweets visualization:
# Create a session
# Note: Count can't be greater than 200
# Data frame
# search of life expectancy indicators
# remember there are about 250 days of stock trading in a year.
# Create the Estimator
# Concatenate first two and last two rows
# Count terms only (no hashtags, no mentions)
# this list is hard to read
# Drop null values
# get the column names
# Take a peak at the data
# Create the empty table that will contain the sidewalk linestrings.$ sql = """CREATE TABLE approx_medial (id bigserial, LINESTRING geometry); """$
#Create list of top10 topics
# Least favorited 
# Reloading the used classifier
#.dropna(subset=['place_name_long'])
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#Determine summary statistics for the school dataframe
# what are all of the business quarterly end # dates in 2014?
#**Which Tasker has been hired the least?**
# add 5% of noise
# Author: Evgeni Burovski
#Churns cohort x cohort
# 将一个列表作为值传入
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# write clean data to a csv to open in Rstudio
# fig.savefig('toma.png', dpi=300)
# essential imports
#Laredo': '4fd63188b772fc62'
# The mean squared error
#Baton Rouge
# metricsの取得
# do not fit with U.S variable as this will be the baseline
#checking the shape of the age_up70 dataframe
# Extract NoData value and store into dictionary
# convert array values to floats
# media
#import statsmodels.api as sm
#cleaning the dataframe containing dates based on the data we saw 
# 7. (Optional) What was the median trading volume during this year.
# Let's plot this instead
# READ IT IN USING # df = pd.read_csv('citation-data-clean.csv')
#confirm scns_created,associated scns_array in sequential order...should be null
# The clipboard from Excel has the following:
"""$ Print all news title docs$ """$
# Making a copy on which we will perform the transformations
# write the numeric features back to a table
#view result
# find the best combination of the hyperparameters
#List the counts of records by month (easily changed to show year year)
#不推荐使用，要被遗弃了
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# option 4 (ignores and resets index)
#data.dropna(inplace=True)
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# Create a mask True/False of which stopids are shared by both dataframes # trips.loc[trips['tripid'] == 4093258]
# split features and target
# read_csv中指定index_col = 0意思是以第0列为索引 # read_csv中指定parse_dates = True意思是需要解析时间
# for appending rows you can use .rbind()
# Note: used reset_index() here so it's easier to use the 'time' and 'time_2' columns if needed
# Load the spikes # Print the first 10 spikes
#Albuquerque
#open and read the file into a pandas data frame
# Separate response variables from predictors
#df_unique_providers.drop('level_0',axis=1)
#Importing S&P Data
#remove new lines
cur_b.execute('''$     FROM room r INNER JOIN hotel@FIT5148A h ON h.hotel_id=r.hotel_id''')
# Drop those two rows with those indices and you are saying inplace=True, to make sure you are not creating a copy. 
# lr.fit(features_normalized, paid_status_transf)
### START CODE HERE ### ### END CODE HERE ###
# shift so that we're trying to predict tomorrow
#There are too many document types to plot on one chart so select the types with highest maximum
# Sorting the Dataframe based on DealID
# Construct all tallies needed for the multi-group cross section library
# Subset to only Level 1, and those where days_baseline is small (let's say below 7)
#df_insta.dropna()
# Make vwg folder
#CIA, ORGANISATION_CODE and L2_organization_id are the same
#transit_df['EXITS'] = transit_df.apply(fix_exits,axis=1,iqr=intqrange)
# Create database connection
# Creating a sentiment dataframe # Writing sentiment data to a csv file
#We need to load only the stuff between the curly braces
# leadsdf['simpleDate'] = pd.to_datetime(leadsdf["lastEnteredOn"],format="%Y-%m-%d") # leadsdf['simpleDate'] = leadsdf['simpleDate'].dt.strftime('%Y-%m-%d')
#Mars hemispheres # url of page to be scraped
# Verifying that all our shapes are as expected
# PRs opened in the last month
# Get a list of column names and types # columns
# read-in created csv file
# We display the total salary each employee received in all the years they worked for the company
# Read/clean each DataFrame
# create dataFrame from ndarray
#とりあえずCSVを読み込むよ！
#read in level1 CSV
# Top 10 most active companies
# Import data
# busco duplicados en las fechas
# To grab the other needed files, execute this code block command: 
# Retrieve Latest Date - This way we do not have to 'hardcode the date' later in our code...
# Add Rapid Watershed Delineation onto map
# how many unique authors do we have?
#clean the data from NaN values
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# set up tweepy
#and then read in this document to analyze # df = pd.read_excel('SHARE_cleaned_lists.xlsx', index=False)
# Rows where magnitude is greater than 10.
# For Research purpose: 
# does it return True or False from re.match
# Step 8: import PCA
# find the countries that have over 1 billion people. (thats kind of a lot)
# City data
# 13. Print rows 100 to 110 of only the first 3 columns in free1 using only indices.
# plot of the length of tweets vs period of time
#to see what the visititems methods does, type ? at the end:
# load the .env file
# merge  df_detail 里面含 指标 download_sum_num  sum_total  
# Let's get the IDs of the artists whose name contains "unknown"
#'Lubbock': '3f3f6803f117606d'
# check your resources id and copy it, then post simulation results of BallBerry back to HS
# Combine each stomatal resistance parameterizations # add label 
#Apply length function to the review column
# calculating number of commits # calculating number of authors # printing out the results
#Peek into the data
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# use cross_val_score() # ... #
#Pregnant(kittyIndexToOwner[_matronId], _matronId, _sireId, matron.cooldownEndBlock);
# why tweets contain "instantaneamente"?
# estructura bd
# Implement simple regression: Result ~ Input # Display model fit results
#reset the index to the date
#    time_left_for_this_task=120, per_run_time_limit=30, #    tmp_folder='/tmp/autoslearn_regression_house_tmp', #    output_folder='/tmp/autosklearn_regression_house_out'
#this is looking cleaner now!
# We remove the new watches column # we display the modified DataFrame
# # a list of dictionaries containing metadata for cells with reconstructions # a list of cell metadata for cells with reconstructions, download if necessary # cells = ctc.get_cells(require_reconstruction=True)
# plt.subplot(2,2,4)
# concat dummy/indicator variables to dataframe
# Create the deployment
# Printing the content of git_log_excerpt.csv
# check inserted records
# Line equation function # test 0.2634
# Number of missing - You can investigate these more later
#Create two new dataframe views: One with records before Falls Lake an one after
# r squared value
# Setup Tweepy API Authentication #, parser=tweepy.parsers.JSONParser()
# Creating lenghts over time
# Creamos columna de resultados del análisis # Hacemos display de los sentimientos:
#ordered and paired after genesisco
#Export Canadian Tire file to local machine # Get sentiment for walmart tweet
# Set a starting and ending time. #end_t = '2017-12-31 23:59:59'
### Remove all Blends and Wine Types made with more then one Grape
# Install the boto library.
# TODO for now, just delete the song
# combined_df5
# Now, we can save the new data set that includes the headers
#Rename columns
# This command should output closing values for our three # stocks from March 22nd to March 30th.
#Re-ordering the DataFrame
# Create new dataframe that only contains rows with tipPC < 200
# note that we use ascending = False becase we want people iwth largest number of things to get higher ranks
# LENGTH AND SENTIMENT
# reshape the data
#tickers = companies['tweet_ticker'].tolist() #Tweets with the ticker in front come in very slowly. May take a while to build up. But these are official tweets
# Merge two dataframes, the common column is selected aotumatically
#  Invest at inprice, then #  assuming market conditions embedded in poparr: #  ... effectively simulates 5,000 years of bootstrapped daily data.
# make predictions for test data # Convert numpy array to list
# Initialise with a scalar. #
# id is just a identifier, so generate features of data
# get artists whose name contains "Aerosmith" # show two examples
# train file is an extract from history file with target variable added
# this is text processing required for topic modeling with Gensim
#Use a lambda function to assign values to water_year2 based on datetime values
# Export CSV
# load data into H2O object
# we will create the indices from the  # product of these iterators
# are there any values greater than 8?
# index can be obtained for an OrderedDict by using, for example, d.items()[0]
# Create BeautifulSoup object; parse with 'html.parser'
# Variables can be any type of data
# Run this cell to display all buckets.
#df.parse(sheet_name)  # read a specific sheet to DataFrame
# add features to validation # check wether the join operations where right
# Inspecting tables in the sqlite database
# do some sessions show up in one or the other? # looks like it is the case, and some sessions that don't have either (quite a few in fact...)
#     print(screen_name)
# Predict the on the train_data # Predict the on the train_data # Predict the on the train_data
# most delays are on thursdays
# Calculate Probability of new landing page
#== Filling missing value 
# one-hot encoding of categorical variables
# Copy data from the cloud to this instance
# Store the experiment, and display the experiments uid.
# delete by row label
# number of age-mates in different age values
# check is document has been parsed (dependency parsing)
#print (r) #print (data)
# split into conversation and last message dataframes # users_df = df[df.conv_flag==0].drop(['conversations','conv_flag'],axis=1)
# Fill nan with zeros
#Graph the results  # Avg Temperature on your trip \n2016-04-13 to 2016-04-13
# print_full(feedbacks_stress) # print_full(feedbacks_stress)
#url for mars image
#join the data frames
#  - 申万一级 A股
#Change duration of simulation from 3 seconds to 0.5 seconds
# Compute thermal flux utilization factor using tally arithmetic
# Remove these stops.
# Sort data frame by target and tweets ago
# subset temp to us
## create a database (if it doesn't exist)
# View the first five rows of data.
# check fineness of feature_col
# dr_new.index
#Drop rows with invalid data
#
# You can compute the class GPA by #Show pipeline maps i.e. the cluster tracks how the data was created #rddScaledScores?
# We print percentages:
# Filter down to just Ductless units
# To find convert rate for p_old 
# plt.xticks(rotation='45') # 刻度字体转换45度  如上图
# A priori tenemos la misma cantidad de columnas, veamos que se corresponden los nombres 
#Trying to read the csv file with the list of twitter accounts of the fortune 1000 companies #this is a csv file with the twitter accounts of every Forturne 1000 company #I am reading the csv file to a data frame called companies
# Here the benchmark is iShares Nasdaq Biotechnology ETF 
# for our single-sides test, assumed at 95% confidence level: # Here, we take the 95% values as specified in PartII.1
# likes and retweeets
# Show plot
# entfernt Spalte "created_at"
# Backend
# Drop duplicated user # Check the drop worked
# Read the csv file we just created into memory in order to downcast # We can pass type_map to pd.read_csv to specify the types we want
# first 5 rows
# count the number of complaints per month
# fill shipping method names with shipping method ids if there is no shipping method name
# find the max of IMDB column
# Print all of the classes mapped to the Base
# Add a normalizing column for each song in each region to convert to streams per million of populace
## url
# Identify rows which are transfers from Coinbase to GDAX
#Put all data into dataframe and export to csv file
# combine these two equal sized frames
#### Define #### # Replace underscores '_' between words in p1,p2,p3 columns with ' ' #### Code ####
#select a row
# print words without probability
#creating new dataset using query function
# converting the timestamp column # summarizing the converted timestamp column
#== Sorting axis by labels (axis=0:row, axis=1:column)
# 3.Calculate what the highest opening prices were for the stock in this period
# There are 2 '68' columns. # So, adding value to original '68' column and dropping 2nd col
#check
#  透過參數y來指定需求column
# Extract the mean length of the tweets:
# Want to leverage the Company name so need to create dummy variables. 
# Or, we can fill nulls with a default value
# Find quarterly average
# Make spp folder
# take a look at the first two lines #!curl 'https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv' -o Consumer_Complaints.csv
# We are joining the developing dataframe with the sp500 closes again, this time with the latest close for SP.
#import datetime
#inspect dataframe
# show first few rows
# valid
# plot the model error
# results = soup.find_all('div', class_='img')
# set date as datetime
# Dropping categorical columns so that applymap() can run # Applying functionn
#List the stations and observation counts in descending order
#Histogram
# education
# tensorboard = keras.callbacks.TensorBoard(log_dir="../logs", write_graph=True, write_images=True, histogram_freq=1)
#all_mapped = all_data.loc[all_data['rpc'].isin(mapping['rpc']) * all_data['store'].isin(mapping['store'])]
# Enter code to look at datatypes
# create a mosaic graph 
#Convert to dictionary/json format
# To flatten after combined everything. 
# Use Pandas to print the summary statistics for the precipitation data.
#this reads the json files from a specified column. In this case, the gps files from the validation study (+a few others #that we'll have to filter out later by created on datetime.
# Fill missing values with 0s
# Reading JSON
#Retrieve the news paragraph for all NASA articles
# verify that we now have all the float columns we expected
#'Nashville': '00ab941b685334e3'
# Now comes the training and testing
#act_diff = p_new - p_old # compute difference from original dataset ab_data.csv
#now create another label for forcast data that we need
# Checking our shapes. We see that "test" has an extra column, because it still has "id". We'll drop this column next  # after using it to prepare a submission dataframe, where we'll put our predictions
# plot histogram
# What is the sample standard deviation?
# Use a colour-blind friendly colormap, "Paired".
# If the error `No module named 'matplotlib'` happen, execute: # pip3 install matplotlib  # on terminal.
# return NaT for input when unparseable
#finds date of most recent and least recent tweet
#http://www.statsmodels.org/dev/generated/statsmodels.stats.proportion.proportions_ztest.html #Help to calculate p_value easily using statsmodelapi #zstat_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old])
#Remove unnecessary columns
# Novo Dado a ser rankeado # Transforming new_doc into vector (lang_idx, freq)
#预览部分数据
#Put the indeed and tia data together, then clean some more
# Join the SP 500 start of year with current dataframe for SP 500 ytd comparisons to tickers.
# Find number of rows in the dataset.
# create end date column from period index
# Looking at the information saved within a gene
# check if shapes match
# Predict the on the train_data # Predict the on the train_data # Predict the on the train_data
###YOUR CODE HERE###
# print "Loglikelihood:\n", "\n", loglik, "\n \n LL Calc\n", loglikA
# create period range:
# we get 726 rows for our 2 years of data
# Now we can check if there are any significant differences, between the 2 labels,
#1 reset index so that we do not have issues since we loaded multiple files 
#getting dummies for car models
#Remove na's
#params
# I realized through working with this data in the past that Per Seat Price they give is not always correct.
# fig1.show() # Pupper is the most populate stage
# count the frequency of the types of sub-gene elements
"""$ Check relative distribution of data features$ """$
# Let's say you want to get the distinct count of unique entries in the OWNERS # table:
# select column needed
#Initializes the PixieDust monitor
# Delete Rows # drop rows for mismatched treatment groups # drop rows for mismatched control groups
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# Get station count, has been checked with measurement station count
# Import BeautifulSoup into your workspace # Initialize the BeautifulSoup object on a single movie review     
# Use the append() function to join together free_sub and ed_level.  # Are the results the same as in part (4)?  # If not, how could you reproduce the result append() by using concat() or merge()?
# create an isntance of the ModifyModel class 
# Python's SQLite let's you use either '==' or '=', but I think SQL only allows '=', okay?
# Design a query to calculate the total number of stations.
# Checking if it works
# Just to have the sample dataframe here for comparison
## чем отличается фит от предикт - делает фит (обучает модель) и предсказывает
#Reading CSV files in the DIR
# lets turn that list into a spreadsheet
# Save cleaned and encrypted dataset back to csv without indexes
# Change row & col labels
# Preview the first 5 records of the DataFrame using `head()`. # There's also a `tail()` function!
#this is a little on-off switch for my various long run-time operations that are behind a check. # go_no_go = 0  #GO #runtime items will be necessary once each time after the kernel is reset.
# reuse the created period range as index:
# Save the query results as a Pandas DataFrame and set the index to the date column
#  Extract endpoint url and display it.
# Table values are from 'D' column  # Indexed by combination of 'A' & 'B' columns  # Result table has columns for each unique value of 'C' (i.e., 'bar' and 'foo')
### Fit Your Linear Model And Obtain the Results
# Rows can be retrieved by location using .iloc
# Display confusion matrix for the binary classification # example by using our helper function
# Initialize reader object: df_reader # Print two chunks
#create an index to compare against each iteration of tau #deterministic variable lambda
# more properly, should do
# Pickle the 'data' dictionary using the highest protocol available.
# loading in test data as well as the Sample Submission file
# Request content from web page # Set as Beautiful Soup Object
# DATADIC contains 
#### Define #### # Convert tweet_id column from int obejects to string objects# #### Code ####
#Nasdaq 100 Company list
# poo
#To shows columns within dataframe
# To access a particular row, we use the index position:
# checking shapes before fitting a model
query = ''' SELECT tweets_info.created_at, user_mentions, tweets_info.id, tweets_info.user_id, tweets_users.name$             FROM tweets_info INNER JOIN tweets_users $             ON (tweets_info.user_id = tweets_users.id); '''$
# filter(function or None, iterable) --> filter object # Return an iterator yielding those items of iterable for which function(item) # is true. If function is None, return the items that are true.
# rolling sum over data for each hour
# set up interp spline, coordinates need to be 1D arrays, and match the shape of the 3d array.
# Example 1:
# Apply the smoother cleaning function (part 2), by checking indexs in transfer_duplicates DF and then updating BTC DF
#get the mean prediction across all 
# midToto is 13, therefore user can change each layer value from 0 to 12
# List comprehension for reddit post timestamps
# manufacture some loss function # there are n_epochs * n_batchs * batch_size  # recorded values of the loss
# Librerías para integración numérica # Modelo de capitalización continua
# Set file names for train and test data
#we need so we can later import for anaylis the whole dataset. 
# Save the file and run this block
#Design a query to find the most active stations.
### Create the necessary dummy variables
#A quick scan reveals this amazing user description:
# Deleting date again as it's an unnecessary column.  Explaining that new column is the Ticker Start of Year Close.
# Ordeno de mayor a menor la temperatura media
# # convert date columns to datetime 
# results are returned as an iterable list
#ensure column names do not have spaces
# Visualizing causes of death by year # This is hard to read because there are so many different causes of death, but left here for reference
#Calculate vector-average wind direction if not already present
# look at the count of values for the 3 categories.
# extract weekday and publish time from vacancies
# pull out isPorn values for training data (first 2000 youtube title values and first porn title values)
# Merging actuals and backcast
#Save Test Dataset as pickle for later use # the test data in pkl format
#read in mike objects #mike.sort_values(by='RA0')
#Long Beach': '01c060cf466c6ce3'
# creating authentication  # connecting to the Twitter API using the above authentication
# exact match
#Train Model and Predict  
# Retrieving the last 12 months of data, last date - 365
# 컬럼순서 변경
# Converting the results into a dataframe # View the Temperature DataFrame
# subset for only type 
# cur.execute("""SELECT distinct pullid,pullquery from public.datapull_id limit 25""")$ # returns = cur.fetchall()
# examine the start and end times of this period
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#created ticker dataframe (to add column 'ticker' to databreach_2017 dataframe)
# plotting the 20 most frequently used tags
# cisnwf6
# add a EVA as a material
# get the index of the high values in the data
# My gamelogs include 3/29/2000 - 10/02/2016, so I'm going to delete those rows.
# PASS CONDITIONAL GROUPBY??
#Check via customer sample
# load the data # this file has 'numVoters', 'numDems', 'numRepubs' calculated from the voter tables and populated
# Write out the DF as a new CSV file
#SQL Solution$ c.execute('''SELECT MAX(track_duration),artist_name, track_name from track_db''')$
# TASK E ANSWER CHECK
# Define a dictionary with the filename, path, and dbase type. `echo` suppresses output.
#### Define #### # Capitalize the first letter of p1,p2,p3 columns to make it consistent #### Code ####
#https://stackoverflow.com/questions/29722704/fastest-way-to-calculate-average-of-datetime-rows-using-pandas
# Display of first 10 elements from dataframe:
# Calculate the range for each operator by part combination.
# datAll['month'] = datAll['Date'].map(lambda x: x.) # datAll['Year-Mon'] = datAll['Date'].dt.strftime('%b-%Y')
# selected items from alice's cart
# I could drop all the prior day info, because I saw no autocorrelation, but it just seems... odd.
# Train for one epoch, just the embedding layer
# import the data offset types # calculate one business day from 2014-8-31
# use the keyword index
# Show all chars that are predicted to belong to the amount
#获取2014年第3季度的成长能力数据
# you can add arithmatic operations between two defined periods # here we have 2 quaters defined and if you subtract q2-q # result is how many quaters between q2 and q
#Create dataframes
# print min and max date
### Step 17: Dissect just a portion of the data ## The red cluster has a high peak commuter pattern displayed (Commute days)
qry = '''\$ DROP TABLE IF EXISTS lucace.disc_667_stage_00_list_of_users$ '''$
#You can also use the offline_PSI_tweets() function to instatiate your data if its already downloaded ## tweets = offline_PSI_tweets(x)
# How many stations are available in this dataset?  Method #1 using a query from the database:
# Retrieve page with the requests module
# We create a dictionary from a list of Python dictionaries that will number of items at the new store # We create new DataFrame with the new_items and provide and index labeled store 3 # We display the items at the new store
# convert the 3 date columns from str to datetime
# Download the dataset # Split the dataset
# Make predictions using the testing set
# How many stations are available in this dataset?. Calculating # of stations in the full measurement table.
# Total file size (compressed)
# Calculate the morning traffic that fall under the income group for every station
#Count and plot source url
# LOADING LIBRARIES WE MIGHT NEED... # # system
# getting for each game the violance events level and the number of tickets #V.head(10)
# these are the columns we'll base out calculations on.
#groups the dataframe by the result list
# 1. How many recommendation sets are in this data sample?
# pickle pulled data.
# create a variable with the path/name of the file that will contain your unique address list
# FML floating points
#full window width
# set Date column to index
# write out csv file of all data
#Define a dictionary with the numpy functions needed for the groupby that will be completed in the next step.
# Use `engine.execute` to select and output our chosen vacation day-time period of around 15 days. # It picks the first 15 days of january for which data was collected.  (remember any missing days # that are not in sequence may have resulted from the 1400 est data points missing from prcp, etc)
# Import transplant stemsoft data
# count number of fire violation per month in  May 2018 
# Creating a document-term matrix
    """$     Return a new SoundCloud Client object.$     """$
# First row from validation dataset
# For Displaying Data
# FA_indexs
# The climate data for Hawaii is provided through two CSV files. Start by using Python  # and Pandas to inspect the content of these files and clean the data.
#app_ids
# Use the session to query Demographics table and display the first 5 locations ### BEGIN SOLUTION ### END SOLUTION
# iris file #Remove the header line
# group by combination of 'A' and 'B' for taking sum 
# Getting rid of null values is easy too
#Pass the tweets list to 'toDataFrame' to create the DataFrame
# drop scratched
# duplicates #intervention_train.drop_duplicates(inplace=True)
# Make a new empty pandas data frame
#correlation of continuous variables
#Do Data Frame queries
#fitfile = FitFile('fit_files/2912551983.fit') #cycling (example with good gps) #fitfile = FitFile('fit_files/2853423540.fit')#short swim
# 这里不能使用set_index，使用set_index后index是str对象，而不是timestamp，后面画图时无法显示x轴的标注
# We display the updated dataframe with the new column:
#missing value treatment # Check for nulls.
# find historical data for 2011
# add our new function to the class # test our function on the initialized instance
## filtering out the cats
# Create a pandas data frame # We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# fit validation model
#out_columns=out_temp_columns+incremental_precip_columns+general_data_columns+ wind_dir_columns #columns to include in output
#Remove tokens with punctation.
#qry = DBSession.query(User).filter( #        and_(User.birthday <= '1988-01-17', User.birthday >= '1985-01-17'))
#uncomment the next line to install. #!conda install -y pandas-datareader
# frequency of chunk types/labels # from collections import Counter
# Create a legend for the chart
# 181 tweet id in tweet_archive_clean do not exist in tweet_image table
# Abosulte value # Round # help() # type q to quit
# Position: row 1, column 4
# SAVE A CSV WHICH NOW ONLY SHOWS DATA FOR SONGS THAT REACHED NUMBER ONE
# highest
## to get the summary of the counts ##
#%% Read dataframe 
# Lenghts along time
# accediendo a filas y columnas por posiciones enteras
# df=pd.read_csv("/home/midhu/DataScience/nlp/keras-quora-question-pairs/dataset/quora_test.csv")
# 7. 
# Show results
# This step is needed to get the date into the index # This is required if you want to easily plot the data
# merge dataframes
# In total we are using for this model the following amount of coefficients
# Make output folder
# Load pre-calculated stacking fault energy data
#Collecting data from the FSE for AFX_X for the year 2017
# Plot a run chart, using the index for x.
# concatenate by columns
# Take a peak at the data
#data massage newsMood_df #converts 'Timestamp' from string into datetime object # save newsMood_df to csv file in same folder
# Change path delimiters to unix format if necessary
# id and sentiment are categoricals
# 6.What was the average daily trading volume during this year?
# For Displaying Data
# map the count function to each strike where there is a nan implied volatility
#table.to_csv("filenamehere.csv") # Write table to CSV
# ¿En cuántas ciudades he sido etiquetado?... Y ¿Cuántas veces he sido etiquetado en cada ciudad?
# Let's get Apple stock data; Apple's ticker symbol is AAPL # First argument is the series we want, second is the source ("yahoo" for Yahoo! Finance), third is the start date, fourth is the end date
# draw the boxplot of total based on different types
# Armamos un dataframe con los tweets recortados #we will structure the tweets data into a pandas DataFrame
# masked["RT"] = [1 if "RT" in ele else 0 for ele in masked["text"]]
# Get the current date and time # Read the time and date now
# Calculating in and out degrees
# Mars Weather URL to Scrape
# Find the max of the 'Average Fare' to plot on y axis
#Jacksonville': 'd5dbaf62e7106dc4'
#after getting the data we will fill the data
# extract metadata values
# I couldn't get the #s and the dates to match up (1 more line in the counts which is like the indexing) # saved them both to a .csv and then combined the csv worked though
# Remove the 'Date' and 'Time' columns
# The amount of unique user-entries:
# Make columns with country sums
# Creates a list for sentence tokens
# dummying purchase
# Reflect Database into ORM class
# What are the most active stations? # List the stations and the counts in descending order.
# can also show how many times we have scrapped  from this id of the reddit # However, this i found out there are multiple topics
# Cyber Monday
#Remove rows which contain words from blacklist
# we create a new table grouped by city type and driver count and count the number od rides in the city
# Custom the color # Custom the inside plot: options are: “scatter” | “reg” | “resid” | “kde” | “hex”
#Create BeautifulSoup object; parse with 'html.parser'
#dimension #shape
# convert 'Date' from object to datetime data type
#Split the DataSet
# We can create a time series of tweet length like this
#Save Date in Pickle and CSV format
#grouped_by_year_DRG.get_group(level=1)
# Task 2
# check NaNs
##这里resample('M')的意思是从每个月中进行抽样后取平均 #如下列数据中2009-01-31代表抽样为1月的31日，然后对这一天的所有值计算平均
# Which were the 3 most popular trainings? (Trainings with the most learners)
# your code here # every 8th business end of month
# TM Min and Max
# Step 7: Display the shape of the pivoted data
# Say we only want to compare fails and success, we can create a separate view of the dataset that # contains only those states
# Printing the content of git_log_excerpt.csv
# count the NaN values in a columns
# add appropriate prefix and suffix to metadata keys  # get indices for rows where the metadata keywords are present
##Load in officers data
# split features and target
# total size in GBs
# We convert the Date type:
#fill the missing millesime by year of creation date
#Example2: Import File from URL
# Importing the built-in logging module
# group by News_Source # calculate mean on compound scores
# Can use any valid frequency for pd.date_range, such as 'D' or 'M'
# unhide the strings in jupyter notebook # assign all new functions to df # new_df.head(1)
# Get the maximum likely class
#plt.figure()
#Row wise concatenation
# Export
# New ARR Closed Won by Industry and Quarter
# AUC for our recommender system
#fill cohort activation dates
# List the stations and the counts in descending order.
# stock_data['TIMESTAMP'].iloc[1:10]
# Segun se indica en DataDictionaryBuildingPermit.csv permit number es del tipo number pero vemos que lo interpreta como object # De la simple revision se puede observar casos donde el permit number es MXXXXXX y parece estar relacionado con permit_creation_Date
#reading in csv files
# plotly does not work with "dates" in strict sense, therefore we need to chage it again into a string
# December 31, 2017 at 7:03pm from Online Store...unix timestamp datetime funk?
# Create Geometry and set root Universe # Export to "geometry.xml"
# and the first ten records
#create a new column called month of year, and convert the type of data to string variables to prep for dummies
# Convert columns from string to datetime
# Create engine using hawaii.sqlite created in database_engineering steps
#join in the weather for these days! then look at complaint types
#This is what we would expect 
# dropping columns '[', ']', and ','
# tweets with hashtags or user mentions of apple or samsung
# Choose the station with the highest number of temperature observations. # Query the last 12 months of temperature observation data for this station and plot the results as a histogram
#convert html syntax to humanly readable characters
# lgb1.feature_importances_
# We'll also remove the double "_" in DATE__OF_OCCURENCE
# same as above but as different return value type (as series) 
# cisuabg7
#Clean up some bad data at index 11650
# results are returned as an iterable list
# Store our sentiment analysis in a dataframe
#giving the values and index itelf a label
#Frequency_score is float data type, but should be integer
#creating new dataset using query function
# Define the columns and set the index.  
# flight_to_brisbane = flight.where(col("price") > 0 & col("to_city_name") == "brisbane").groupby(flight.search_date_x).count()
#Let's display the text of each tweet we found. #[tweet.text for tweet in tweet_list]
# Replace the API_KEY and API_SECRET with your application's key and secret. # Use AppAuthHandler instead of OAuthHandler to get higher limits.
# get multiple sections with the term fees
# We drop any rows with NaN values
# Filter down to the set with valid licenses #df_valid = pd.read_csv('/home/bmcfee/data/cc_tracks.csv.gz', usecols=[0])['track_id']
# The column names are taken alphabetically from the keys of the dictionary # The rows are taken from the union of the indices. It will use numerical row indices if there are now labels given. # NaN stands for not a number
# remove qstot_12 and qstot_14
# Set some simple earthquake related keywords:  # Collect 100 tweets using the keywords:
# EXPORT FILE # Export the dataframe into an output CSV file located in the folder "output", placed in the same folder as the code # The file name, using a clever time stamp, will show as MM-DD-YY-Output.csv
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Create a "tallies.xml" file for the MGXS Library
# Create a dataframe with the minimum and maximum values of 'x'.
# calculating or setting the year with the most commits to Linux
#Find the largest change between any two days (based on Closing Price)
# apply the tweet extend function # prepared for run # mancano 900 - 1058
# separate out target column, rename w/ generic title
# Get and display model uid.
# Confusion Matrix # https://mingchen0919.github.io/learning-apache-spark/decision-tree-classification.html
#recarguemos el archivo parseando ahora issued_date como date
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Author: Stefan van der Walt
#The number of events hosted by groups 
# 4. What was the largest change in any one day (based on High and Low price)?
# genre_ids is a combination of differents types # We are going to get value that are not numbers
# add a notebook to the resource of summa output # check the resource id on HS that created.
#sc.stop()
#Subset data to alternate temperature columns #Check if any secondary temp sensors exist
#### Test ####
# convert ISO string to datetime
# Create logit_countries object # Fit
# rows only with emoji # df_emoji_rows.head(1)
# NS --> Non Seoul 
# Information of duplicate row user_id
# Count the total number of tweets
# Initialise with a dictionary. #
# I created a csv with: # a binary for whether a day is an offical DC gov workday (per their website), which should inform occupant electricity use, # and the minutes of daylight in a given day (from NOAA), which should inform day v night electricity use and capture seasonality
# !dpkg -i libcudnn7_7.1.3.16-1+cuda8.0_amd64.deb
# Check how many countires there are to determine how many dummy variables are needed
# Load the data
# here's the index
# Add this line if team made postseason:
# describe dataframe
#from pandas.tools.plotting import autocorrelation_plot
# exercise (page ) # 1) extract the superdiagonal # 2) extract all the numbers divisible by 3 using a boolean mask
# start_time :: start time of 2016
#Accuracy on test data
# Firts, create an array of booleans using the labels from db.
# Freeviewing
#search best parameters #%debug
#Bar plot for overall sentiment
#rename column headings
#=== By Label: row selection by 'data index'
# take logarithm of goals
# Import required libraries
# Store the API key as a string - according to PEP8, constants are always named in all upper case
# Creating the term dictionary of our courpus, where every unique term is assigned an index. # Found the output from models is more close to what tickets mean about when not using bigram phrases
# Create a new dataframe for the Airport ID (It does so by testing true/false on date equaling 2018-05-31)
# Inspect compiled dictionary 
# Compute metrics
#convert dictionary to dataframe
#timeStart = datetime.strptime(timeStart, "2015-06-09 00:00:00")
# Create a TensorFlow session and register it with Keras. It will use this session to initialize all the variables
#print(tmp)
# média de todos os prêmios ganhos com os valores não acumulados
# set SUMMA executable file
# Find the link to more details about the Avengers movie and click it
# Weekday
# Change the Date column to the index to facilitate plotting
#Let's bring back our datetime values from index to a column in our dataframe
# scrs < 0m / all scrs, since 3-01-2018
# top 3 styles, minimum of x beers drank
"""this is what worked to create the existing file $ read in SHARE json records and set encoding to utf-8"""$
# merge dataframes
#file path
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Count terms only once, equivalent to Document Frequency
# driver = selenium.webdriver.Chrome(executable_path = "<path to chromedriver>") # This command opens a window in Chrome # driver = selenium.webdriver.Firefox(executable_path = "<path to geckodriver>") # This command opens a window in Firefox
#print_sentiment_scores(twitter_datadf['text'][1]) #analyser.polarity_scores(twitter_datadf['text'][1])['neu']
# get current local time
#added the year 2018 in sublime and excel. created 2 separate files because the date was different format. Next I'll convert the dates to datetimeformat and then merge the to datarames back together
# join in information about occurring words, probabilities etc
# Name of columns
# Showing that the last 35 values have been shifted up and replaced with NaN values
# new_df = pd.DataFrame(columns = ['hour','minute','hourfloat','x','y'])
#set up reddit connection
#Check via customer sample
# goodreads_users_df['joined'].replace('None', np.nan, inplace=True) # goodreads_users_df['age'].replace('None', np.nan, inplace=True)
# remove those that don't have a week 0 value
# Replace NaN values with 0
#List the unique values inthe HydrologicEvent column AND COUNT THE NUMBER OF RECORDS IN EACH
# split out y size # split out predictors
# Create a binary target variable (injured/not injured)
# HITS THE SERVER: downloads data from yahoo for selected EFT
# Rearrange the columns in the name of order
# Add the 95 % confidence interval of the median.
# sort dataframe (hint: use .sort_values(by="column name", inplace=True))
#view the header names for files
# how many unique products do we have listed in this purchase history? 
# 가게 이름과 메뉴 이름을 어떻게 분리하지??
# Once again need to delete the new Date column added as it's redundant to Latest Date.   # Modify Adj Close from the sp dataframe to distinguish it by calling it the SP 500 Latest Close.
# We want to expand these fields into our dataframe. First expand into its own dataframe.
# 네이버 영화 평점 변화 보기 # https://movie.naver.com/movie/sdb/rank/rmovie.nhn?sel=cur&date=20170806
# Merge CSV
#taking log of the column 
#X_train.append(training_active_listing_dummy)
#Lets see what Melt does
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Example:
# export the dataframe
#prob don't need # last_train = train_df.iloc[-1].name # last_train
# check shape
# raw_df = pd.concat([raw_df, pd.DataFrame(np.arange(avg_da).reshape(-1))], axis = 1)
# source vivo 没有tag字段 # df_detail_download_total_time[df_detail_download_total_time['game_name'] == '王者荣耀'].head() # 去掉 vivo_game
# Concatenating the two pd.series
#split the data as described above #Prepare predictors and response columns
#preds = model.predict_proba(X_test_mean)
#Set xgboost parameters and tree type #Use all the defaults for train here, as to not add excess detail #In actual usage, grid-search or other means of parameter tuning appropriate
# generate a Series at one minute intervals
# write your code here
# get value counts
# columns in data # first and last item in data confirming I have all 2017 information
#plt.axis("equal")
##### unless specified otherwise, head and tail are first and last 5 rows #### print(df.tail)
#List the unique values inthe HydrologicEvent column
# Format bill ids to match the bill ids taken in by the API
#'Omaha': 'a84b808ce3f11719'
# Merge train and items and store it on the train variable
# 'counter' is a standard python package. not unique to DOTCE. # it's a container that keeps track of how many times equivalent values are added
# list all scores that aren't null
# Merge mojo and youtube
#reset index to create date as a column 
# Isolate comment_body
#eth['close'] = eth['close'].astype('float64') 
# series have properties: name, dtype, values
# data_folder = '/Users/alex/Documents/ODS/oct_4_2016_dump'
# 7. Print the mean for all columns for each age_cat using groupby.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# credit card payments
# a Jupyter thing to send the encoding to Arvados via CWL.
# Explore our target variable.
# how many rows, columns do we have now?
#Investigate the semantic space
# plt.bar(Daily_Price[0],Daily_Price[1]) # plt.show()
#Upper code was used to produce a .csv file containing all parsed Twitter data #Was uploaded to Dropbox and made available via a public link, lower function downloads that file
# result = pd.concat([Aust_result,Aussie_result]) # result.head(5)
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# tweets_df = pd.read_csv(tweets_filename)
# Change row & col labels
# get 8-16 week forecast new patients # keep only date in index, drop time
# Exchange DB # EXCHANGE_DB.to_csv('exchanges_list.csv')
# Probability of user converting
# Query and select 'date' and 'prcp' for the last 12 months
# log reg
# statistical information related to the data frame
# dt是什么来着？- change the format
# http://w2.weather.gov/climate/xmacis.php?wfo=mtr
# create a series to work with
# follow up @8/14/18 # The web framework gets post_id from the URL and passes it as a string # Convert from string to ObjectId:
# read movies of small dataset
# create list of columns to lag
# rtemoving unused columns
# but it has recovered it up to some tiny epsilon
# Convert the returned JSON object into a Python dictionary
# add a notebook to the resource of summa output # check the resource id on HS that created.
#Averages grouped by price
# run the model giving the output the suffix "1dRichards_docker_develop" and get "results_1dRichards" object
#CON is the CONTACT TERM information #CON=pd.read_csv('C:/Users/sxh706/Desktop/Interactions.by.Month/2018/June/Interactions.Opportunity.Term.csv',skipfooter=5,encoding='latin-1',engine ='python')
# view regrid humidity
#Set pandas so that the dataframe column width will be expanded, so  #we can see more of what is happening in our cleaning steps
# Simulate conversion rates under null hypothesis
#Lastly drop words that are only 1 character post stemming
#My local key:
# Ok, but maybe a bar plot would be better.
#Show all of the tables in database #We'll use the nyctaxi table, created using code similar to https://github.com/toddwschneider/nyc-taxi-data
# set SUMMA executable file
# Using logistic regression because it outputs values between 0 and 1
#drop actual == NaN, since we don't know beat or not...
#df_inventory_santaclara =df_inventory_santaclara.iloc[7:,]            ##remove unnecessary columns
# Store stops in csv for safekeeping
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned #url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=API_KEY&start_date=2017-01-01&end_date=2017-01-02"
# Let's again use the inner join method we used above
# show two examples
# series
#https://radimrehurek.com/gensim/tutorial.html # this makes process visible
# Predict_SVM
# classify the device type # remove remaining device columns
# Smoker descriptive statistics
# Use Pandas to calcualte the summary statistics for the precipitation data
#Checking spread of created vars
# 1 quarter changes regression: earlier half of the sample
#run a sample query
# 6. 
# SAVE A CSV OF THE NEW SPOTIFY TABLE ("hit_tracker.csv") WHICH NOW FLAGS SONGS THAT REACH NUMBER ONE IN A REGION
# delete a column with pop
# Display the results of the Language Stats analysis.
# Look at the head of the dataframe
#Combine The Data
# загрузим данные
#getting mars tweet
# rename non-target columns
# subset to included participants 
# continue training with the loaded model!
# Looking at one tweet object, which has type Status: # example_tweets[0] # ...to get a more easily-readable view.
# better if the data set included a lot more days
# split features and target
# Groups already grouped dataframe by parent index (level=0) AND sums it up # gDate_content_count
# Instantiate the model. # Train the model, passing the x values, and the target (y) # the vectorized variable contains all the test data.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# transform spark dataframe to pandas to print the image
## Converting json to dictionary
# each hour of 31 Dec, 2017
#All the tweets are combined first
# What was the average daily trading volume during this year?
# Ploting mention count and the hashtag count against eachother
# Split the Data to avoid Leakage #splitting into training and test sets
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure #my_data = pd.DataFrame(data['dataset']['data'])
#results = results.append(nflcont.interest_over_time())
# Match Email pattern: 
# What is the number of backers that most success projects have?
# Check the data types for the remaining columns
# Merge the two DataFrames based on DateTime  # using inner join with price data as left (since price data is shorter)
# Checking who all pitch artist to the user.
# check if there is any null value
#Convert titles to a list for further scrubbing
## Ideal max_features was found to be 2,000, but this took too long to run
# Am I currently using a GPU?
# 
# Retrieve page with the requests module # Create BeautifulSoup object; parse with 'lxml'
# Decoder: uses the same weights as encoder.
#big_data_station_mean = big_data_station.EXIT.mean() #print('Min:', big_data_station_mean.min()) #print('Max:', big_data_station_mean.max())
#write this dataframe into csv  #due to the twitter content is not determined, the data form needs extre cleaning steps
### Create the necessary dummy variables
## Simple enough - let's use a loop and our new understanding of dicts to print out the mode and route name:
#  Invest at inprice, then #  assuming market conditions embedded in poparr2: #  ... effectively simulates 5,000 years of bootstrapped hybrid daily data.
# export 
# write scenario from xlsx to data base and run the sceanrio in GAMS
# set SUMMA executable file
# unique identification of project
# Create a bar chart based upon the above data
#train_y = np_utils.to_categorical(train_y, 3) #val_y = np_utils.to_categorical(val_y, 3)
cursor.execute("""SELECT * FROM coindesk""")$
# To call (instantiate) the class
#df_train = df_train.drop(columns=columns[19]) #df_test = df_test.drop(columns=columns[19])
# Add interaction between the pages and countries
# Para que sean las fechas sean más sencillas de manipular conviene # que created_at sea tratada como fecha # La convertimos:
# Use a compound data type for structured arrays
#finding the important features
# Write to CSV
#获取2014年第3季度的偿债能力数据
#R17df.rename({'Create_Date': 'Count-2017'}, axis = 'columns')
#tree_clf.fit(X,y)
# we only need review after 2017-01-01
# 9. 分析周末访问量量是否有增加
# A:
# model for all model visualitation
# Load the query results into a Pandas DataFrame
# use logical indexing to return the dataframe rows where source is GRCh38
# Import inpatient census extract
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# drop nulls created by lag variables (5 nulls per batter. amounts to rougbly 5k observations # in other words, 0.8% of rows lost to creating 5 PA worth of lag columns
#Create a data frame from the Row objects
# Query for last 12 months of precipitation
# df2は、いいね数が20以上存在するものを選ぶ
# Places table
# We create a column with the result of the analysis:
# Make dataframe
# 4. What was the largest change in any one day (based on High and Low price)?
#Youtube videos by country
# We extract the mean of lengths:
# Set the index to time, so when we save the data out we dont have an additional column with increasing numbers 
#Visualize the frequency of #shoolshooting was tweeted
# merge 'yc_depart' with the destination zip codes
# Create BeautifulSoup object; parse with 'html.parser' # Examine the results, then determine element that contains sought info
#pandas #transactions_pandas = pd.read_json('data/transactions.ndjson', lines=True)
# Note A1 is a counted allele and usually minor. (A2 is usually major)
# Inspect counts
#Make dataframe a datetime index
# create a new DataFrame # drop columns we don't plan to use to reduce the overall DataFrame size
#prints a column
#  Select the last 12 months of precipitation data: # Grabs the last date entry in the data table.   
# Load the statepoint file
# check Forcing list data in file manager file
# Создадим таблицы в которых будем хранить требуемую информацию о постах и комментариях
# data count
# read in the csv data into a pandas data frame and set the date as the index # execute the describe() function and transpose the output so that it doesn't overflow the width of the screen
#Sharpe Ratio #Menor SD
#Visit the url for JPL's Featured Space Image here.
# Let's now filter our data
#delete unknown gender type
#Downloading the file to a local folder
# Mapped classes are now created with names by default # matching that of the table name.
#inspect dataframe#inspect 
#data_comparision
# print out details of each variable
# Find the dates
#locate missing value record
# Function inputs # Plot the matrix
#Create empty df template #df = pd.DataFrame(columns=['Link','Title','Publisher','Journal','Authors','RDate','PDate']) #Or import df from pkl file
#checking the dimension of the whole data frame
SQL = """SELECT * FROM github_pull_requests_2"""$
# the 2018 offseason.
#removes the 'b' infront of the tweets that indicate byte format
# Import Dependencies
# Información general del dataset.  # Cuidado, para cada columna son las filas con elementos
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
#probability of conversion in treatment group
# top 5 breweries
#save model
#ignore_index means the indices will not be reset after each append
#df['price'].map(normalizePrice)
# load the data 
# Check is Data is imbalanced
# replace 0's, since they cause potential blow-ups
# rename treatment to ab_page and drop control column
# record observation count for each sensor
# Width # Color # Values as integers
#Aurora
# 4 adding axis to the equation. It will copy vertically
cur.execute("""INSERT INTO coindesk VALUES ('http://google.com', 'google', 'Ian Chen', '2018-11-11 11:11:11')""")$
# Call function and retrieve data
# Here's a pivot table instead - exact same output as a groupby
# observe results
# Range chart statistics. # Calculate the average range.
# save the file to the output
## Feature importances
# Upper value.
# we can use standard type __call__ see below
# QUERY METHOD!!!
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# The protocol version used is detected automatically, so we do not # have to specify it.
# Train the model using the training sets
# Place map
# load random forest classifier
#remove the row that contains a negative value for initiation days
cursor.execute("""DROP TABLE coindesk""")$
# find historical data for 2009
# extract mean of lengths
# 4.
# assert monthly data
#test set accuracy
#The table I created above on the nulls in the dataset let's us know that Completed Date has 101,709 null values, and Permit Number and Current Status do not have any null values. Let's confirm that here. 
# show dataframe first rows
#oh shize, did we ensure the OSes are covered too? will need to improve eventually, but as all are Linux, TTE is ok #doing D2MI_DEV, not as clean? nope - we have RHEL and Linux mixed, fix it later
# Read the solar power production from Data.xlsm
# convert an h2oframe to pandas frame (make sure H2OFrame fits in memory!)
# Author: Eelco Hoogendoorn
### Step 19: Convert dates into days of the week  # a) convert object into DatetimeIndex
#clean df...no nans
# Add cleaned tokens as an attribute to by_tweeter:
# lg = pd.read_csv('awesome_go_logging.csv') # TODO: should have a function for that ....
# calculating number of commits # calculating number of authors # printing out the results
# Largest change between any two days
# View column headings and index
# query to pull the last year of precipitation data
# build the same model, making the 2 steps explicit # can be a non-repeatable, 1-pass generator
# Median calculated on a Series
# First option - the slice object
# All only csv competitions
# Change directory # Create a list with all the files
# Car KOL
# results are returned as an iterable list
# Assuming 0.2% of people that pass buy a crepe # Column "day_sales" in units 'Number of crepes sold per day in that location'
# the apply() function here should probably be split up; it's messy!
# Our classifier is now trained. Wow that was easy. Now we can test it!
# Rounding the numbers to two decimals
# Here we are adding a "date" column that can be used for grouping operations
# builtins.uclresearch_topic = 'HAWKING' # 4828104 entries # builtins.uclresearch_topic = 'NYC' # builtins.uclresearch_topic = 'FLORIDA'
#Distinct
# Now let's see how this works in a base case, and with the labor # force growth rate boosted by 2% per year in the alternative case...
# Find total number of stations
# Below is the list of default classifiers which were included at the beginning of the code # clf_base_default = [clf_base_lr, clf_base_rf, clf_base_nb, clf_base_knn, clf_base_dt, clf_base_svc] # Pass the clf_base_default value to the 'clf' parameter
## Q1. How to print out dictionary values using for loop?
# Second Approach, using Doc2Vec and LSTM for predicting
#let's look at the data! start with weather data: pull for 2010-2017 for the central park station
#'Toledo': '7068dd9474ab6973'
# Descriptive statistics
#select URL to gather info
# Store the Average Sentiments
# training run内の個別テスト結果の取得
# Applying function to "Ideas" 
#Remove columns which are not necessary to answer the question I would like to answer
# plot heatmap of predicted probability for fires
#Example3: Passing the sheetname method by their order
# Using the transpose function to make the rows columns
#Let's first create a new dataframe with only the Permit Number, Current Status, and Completed Date columns from the  #dataset. 
# Missing 1 value for Q3 so filling with 'Other'
# Load data # Print shape
# payment distributed
# the cost is the data gets copied
#URL's of pages
# Drop rows with label 0
# JH uses 14. 8.26: first 4, then 3, then 2. saved model, reloaded 8.27 and set up for 5 epochs. pred on this, 8.27.csv # DONE. fine for now.
# Set up logistic regression # Calculate results
# specifying the index column
#count of users in treatment group
# Create engine using the `hawaii.sqlite` database file
# df1 = pd.read_csv('../output/data/expr_2/expr_2_pcs_nth_0.1_div_101_03-20.csv', comment='#')
# created a dataframe with the text for each article and then combined the output with the metadata into a second dataframe  # dropped url, headline, type_material (all Op_Ed) from the dataset
# label encoding block by their any_spot rankings
#load the correction factor table from the jupyter notebook `MassFlowCFFinder`
#load data  
# We print the number of non-NaN values in our DataFrame
# sort in expiration order    
# getting GEO ID for Philippines
#2
# Convert "date" data to datetime and set index on date
# read the file to test
#Check count of missing city value 
# predict fire size
#output df_SP to a file
#In order to do an inner join, there must be a common key in both datasets.  Base the common key off of the school name. #This cell changes the name of the school column so that the inner join can be completed 
# summaries
#\xa0 is  non-breaking space in Latin1 (ISO 8859-1). replace with a space
#Reading all necessary CSV files from Scrapy in the DIR
# Topic 6 is a strong contender for 'eating'
# save transformed data
# Retrieve page with the requests module # Create BeautifulSoup object; parse with 'html.parser' # Examine the results, then determine element that contains sought info
# Load the training set # Group the taxonomic classes
#URL for NASA Mars News Site
# Now we'll try "device_ip" - 2MM values in the reduced training set
# drop columns where reason for visit name = Follow up, returning patient # existing patient dataframe
# Display the unique values of column L2
#Find average HP by Brand #Add a count 1 to each record and then reduce to find totals of HP and counts
# shift using a DateOffset
# We can use the describe method to inspect our data easily
# Viewing the edge_types file
# Push table to database
# columns
# A:
# Calculate probability of conversion for new page
# check if still missing 
# How many stations are available in this dataset?
# In our case bigram is enough
#Refernce paths for data files
#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
# Read the variation of wind generation from Data.xlsm
# Check everything  has worked
# check inserted records
#fitting the model using logistic regression for preddicting converted column using intercept and ab_page
# Transmission 2030 [GWh], late sprint
# at first I didn't realize this produced the lunch data I wanted, so... # I tried to find another way to extract lunch data # a long story follows but it worked too
#pre-3 mo pairs
#extract the url of background-image 
#### Test ####
#access_logs_parsed.takeSample(False, 10)
# Inspect numeric feature summary statistics 
## How many tables are there in the database
# Print model.summary.
#Select rows where the Confidence indicates estimated:
#create a temp df that has all the sentiments broken out by applying the get_polarity_scores function
# Fitting K-means with 4 clusters
## no.of unique cats taken in by the shelter 
# Try Multinomial Naive Bayes model with TF-IDF # Define X and y # Train test split 
# Check for duplicate col names 
#Fort Wayne': '3877d6c867447819'
# creating DataFrame
# Saving results to pickle # store the data as binary data stream
# total非空
# Create an object to transform the data to fit minmax processor
# This will shift the column index up by the number of periods - here it is 1% of len(df) which rounded up is 35 and # the column values it shifts up will be replaced with NaN
# Subset to only week 0 or over 12 # NOTE: week only available for self-rated, thus this selects self-rated implicitely # Subset to only those colums I will need 
# Your code here
# Modify instance with multiple thermal distresses
# Import dataset
# Start by sorting data from oldest rides to newest
# Seasonal Difference by itself was not enough!
# import pickle # # Getting back the objects:
# Print the info of df_subset
# call download_executable_lubuntu_hs method to download SUMMA Executable from HS, and unzip.
#Save figure
#Retun time and sensor information
# best/worst styles, minimum of 5 beers drank
# df = pd.read_excel("msft.xls", sheetname='aapl) #sheetname
# Predict_gnb
# shift just the index values
# merge
# Defina uma função para salvar
# caution!
# this has been reduced to a Series
#, 'test'
# 3. Calculate what the highest and lowest opening prices were for the stock in this period.
# set the simulation start and finish times
# Make predictions using the testing set
# A:
# Convert titles to string type
# 'itineraries' holds a lot more information, let's start with how many itineraries were returned # and list what keys exit for the first itinerary
# Now query to pull the last YEAR of precipitation data
#df = pd.read_csv('statcast_0817.csv') # using a csv of statcast events April 2015 - Aug. 2018
# Use the create_engine function to create the connection to the target database...Note: this doesn't actually open the conn.
# shape of coarse grid
# model= model.load_weights("model")
# View the game_data shape: (43727, 161)
# confirm review shape
# First let's turn each title into a list of words # Then we'll use the lemmatizer to change plurals to singulars to group them # And rejoin the lists of words to one string for the count vectorizer
# Note: Count can't be greater than 200
# TASK G ANSWER CHECK
# Type in the search bar, and click 'Search'
# print min & max dates for each datetime column
#### Test ####
#X_train['age'] = df_imputed.iloc[:,0]
# Design a query to calculate the total number of stations.
# For doing the same thing as above
#writing yearmonth and counting no of files into csv
#### Test ####
# Inspect the table(s)
# recuperar los índices
# Ensure that the Date column is in datetime format to facilitate plotting
"""$ Run preliminary test of classifier to get baseline accuracy$ """$
#downloaded dataset for daily price
# Create SOUP
#Tweets containing references to Donald Trump
# connect to MongoDB cluster
#I don't need this #type(rptdt)         pandas.core.series.Series
# specify a new set of names for the columns # all lower case, remove space in Adj Close # also, header = 0 skips the header row
# bb type
# Show results
# convert the price data to a dictionary
# Import the built-in logging module and configure it so that Word2Vec creates nice output messages
# Settings for notebook
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
# Define a date range #print(dates[0])
# tokenize all the tweet's text # apply hash tag function to text column # apply at_tag function to text column
# replace NaNs with 0
# Replace NaN in Field4 with "None"
#plt.ylim(200,400);
#build query for last 12 months of prcp data #using the last_date datime variable and relativedelta in filter to get last 12 months
# Open the dataset # Swap the dimensions
# 5. 使用created_at这⼀列的数据作为时间索引
#run for August users
# ETHNICITY_LEVELS[0]=='Asian', so I'm interested in e==0.
# Inspect measurement table
# drop rows where our value of interest is unknown # if you drop the original column you'll lose partial atbats. sometimes it's null for only some pitches. dropping it loses event data.
# Build a KNN classifier # k is chosen to be square root of number of training example
# training 
# ! cp -R keras-yolo3/model_data .
# Filter dataframe without undefined state and assign it back to data
## YOUR CODE HERE
# Load the last statepoint file
# Save the created DataFrames
# Reseting index to turn the multi-index dataframe to a single-index dataframe
# Number of unique products
#information_extraction_pdf = 'https://github.com/KnowledgeLab/content_analysis/raw/data/21.pdf'
# Check the accuracy of the trained model with early stopping
# add up all the lengths
# default is freq='D' for date_range
#You should convert the user_id from object to integer!
# only one file because all the json files were the same {"hello": "world"}
#Export the dataframe to a csv file. 
# Calculate the date 1 year ago from today
# if the file path does not exist, it will be created
#now we create a pandas dataframe for the tweets #print out some examples 
# Now we'll try "device_ip" - 2MM values in the reduced training set
# check types
# Create a dataframe with only the Adj Close column as that's all we need for this analysis.
# reading csv file with first column as index  # Here we have dates as the index unlike numbers starting from zero as in previous case
# Give the chart a title, x label, and y label
# Likes vs retweets graph
# \xba\xba 是‘汉’的 GBK 编码 # mbcs 是不推荐使用的编码，这里仅作测试表明为什么不应该用
"""$ Dates go until 2018 and are mostly unique.$ """$
# Plot all the positiv and negativ of the dummy data # This is an example of not choosing to save the image
#### Number of unique wells based on UWI
# this model doesn't perform as well # i think additional variable will be needed to improve this
# Load the previous query results into a Pandas DataFrame and add the `trip_dates` range as the `date` index
#Join events and groups
#ii. How viral are my posts? #Ans. Get the sum of all the fav_cnt and group by users order by fav_cnt desc; #     The user with most counts has the most viral posts.
# Because of GitHub space limits (no files over 2GB), train data file was split into 5 pieces # Loading the first file with header row to use for column names # Checking the columns present
#create a new label call HL_PCt which is percent volatility and add feture
# save df to csv
# dummying rate_experience 
# 12 months before LAST record entry.
#redshift_admin
# Muestro solo las primers 4 líneas
# shopify_data_merge_msrp.apply(lambda x: x['Discount Amount']/x['MSRP'])
#this is India meri jaan, yaha twitter pe Likes se jada Retweets hote hai,  #So we will do seperate visualization
# have a peek at the data
# extract data values from datatable # add column names # view first couple rows
### Fit Your Linear Model And Obtain the Results
# Take a peak at the data
# your code here
# We won't use these dataframes anymore
# Create a list of links.
# TASK 1
"""$ Load tmp sr pickle for news title docs$ """$
#Use the iloc method 
# TODO : load data 'AMZN.csv'
# lets fit logistic model for the countries, baseline = CA
# Save references to each table
# Drop the rows for last date
# Dummy subreddit
##### concatenate along rows
#Store Definitions in WML Repository
# Printing the content of git_log_excerpt.csv
# if we just want floats in the entries, we do
# URL of page with latest Mars weather tweet to be scraped
# Get some general info on the dataset. # Int is for integer, float for floating point (number with decimal), object for string (text) or mixed types.
# resample to 1 min intervals, then back to 1 sec
# Get rid of the duplicate entry
# Fit pipeline 
# convert retrieved records to a dataframe
#提取评论内容content,评论时间creationTime,是否置顶isTop,商品id referenceId,设备userClientShow，是否手机isMobile #pd.DataFrame(content,columns=columns_name )
# Fill NaNs with zeros
# to 45 minute frequency and forward fill
# Drop missing values (Birth Year)
#the below display is the list of company names and tickers that do not begin with same character
# grab the index of the 50 largest abs(errors) # get the rest of the details from the frame # plot model error against strike
# get mongodb params from environment
# msft calls expiring on 2015-01-17
#import crime data by Chicago wards
# X_test_all = pd.concat([X_test_df, X_test.drop('title', axis=1)], axis=1)
# same as above
# Calculate basic statistics for y by group within w.
# fancy indexing creates a copy
# sort df by label
# Converting the index as date
# Simulate conversion rates under null hypothesis
# group by blocks
# Display the most commonly rated movies
#Review summary stats for the data set
# Create the pie chart based upon the values above # Automatically find the percentages of each part of the pie chart
# true flag (T/F): negative class # predicted flag (T/F): positive class
#Kansas City': '9a974dfc8efb32a0'
#################################### # remove invalid duluth isbns
#Changing the Winner with 0 and 1 #if winner is first Pokemon then Winner is 0 else 1
# open r humidity nc
# bad request use Response.raise_for_status() to track
# Agrupar por día y contar la cantidad de páginas a las que les dí likes
#Rainfall per weather station for previous year's matching dates (based on your trip dates)
#  numpy.setdiff1d() returns the sorted, unique values in ar1 that are not in ar2. # genco ids from paired cameras which do not correspond to shopify order ids
# In _theory_ in preview dataproc Spark UI is force disabled but history fills the gap, except history server isn't started by default :(
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
#s_etf.plot(ax=ax, legend='right')
# Converting date to a DateTime Format
# We create time series for data:
#Auto ML
# Next step: only select the RIDs that overlap across all 3 diagnosis datasets # first make a list containing all the unique RIDs in each dataset
#Read in csv file into two dataframes
# Cast Frequency_score as integer
#Removing intermediate columns created
# we're hoping to predict, is actually the future price. As such, our features are actually:  # current price, high minus low percent, and the percent  # change volatility. The price that is the label shall be the price at some determined point the future
# Read the data file. # y is the column of response values.
#Looking at new table
# Change the number of bins.
'''$     3 - Number of SQLs - Sales Sourced vs. Marketing Sourced$ '''$
#find the end date from first non zero observation for tobs at given station #assuming each month is 4 weeks, 4 weeks * 12 months = 48
# Compute sentiment for each tweet and add the result into a new column: # Display the first 10 elements of the dataframe:
#Humboldt Park, Division Street converted to Humboldt Park and Division Street
# Use Pandas to calcualte the summary statistics for the precipitation data
#tw_sample_df = pd.DataFrame(list(collection_reference.aggregate([{'$sample': {'size': 5}}])))
# add ToT sum by student_program # drop duplicates on student_section, reduce columns
# ph = load_df('mergeable_holidays.pkl')
# Divide each number by each countries annual maximum
# Make vhd folder
### Fit Your Linear Model And Obtain the Results # Ab_page is used, which means, new_page and treatment is the baseline
# 1. free_data = my_df_free1
# ???? why this one does not work???
# item_lookup = shopify_data_simple[['child_sku', 'child_name']].drop_duplicates()
# using the default parameters of the logistic regression
# Numero de filas y columnas en el dataset
# remove outliers
# the `self` function of Engine has an attribute to return the data as a pandas df.
#Remove rows which contain words from blacklist #Remove Duplicates (don't keep any posts which appear more than once)
# pull a subset of data for testing
# custom grids for every degree # vectors # mesh grid array
#  另外來看，可以單純show出某欄位之後直接plot
# A:
# converting to date to pd date time
# Suppresing warnings for a "pretty output."
# resample to 1 sec intervals using forward fill
# Which features play the biggest role in predicting campaign success:
# Generate data
# Instantiate a "coarse" 2-group EnergyGroups object # Instantiate a "fine" 8-group EnergyGroups object
# we cant work with the null value so we replace with them with a negative large number of value
# 2. 检测是否有重复值
# Split the Data to avoid Leakage #splitting into training and test sets
# Probability of treatment group converting
# Approach 2: Creation of  #Class_frame.loc[Class_frame["amused"]>0]
# use SVM to classify
##select whether you want excel or csv
#Read the data into the notebook
# To find convert rate for p_new
# confirm which conda environment you are using - make sure it is one with SpaCy installed # if you have difficulty importing spacy try the following in git bash # conda install ipykernel --name Python3
#First, remind us what our columns are
# only process the first three rows
#merge the 2 df
# Define X and y
# Reflect Database into ORM classes
# Create a Dataframe #tweet_info_pd.to_csv('NewsMood.csv')
# It can sometimes be useful to know which column contains the maximum value for each row. #
# March 2016 was our start period, and the period frequency is months.
#NB: mean accuracy is higher because it is choosing the best prediction row by row. then comparing with actual category.
# should we need to load the model
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Now fill index with the previous values
#creating tweet summary for each news agency
# Build Pie Chart
# statusの確認
# We get descriptive statistics on our stock data
#make a copy of dataframe #converting normal values to log values using numpy's log function to remove anamolies
# Run all sequence to structure alignments
# attempt to pull out any duplicates based on text data
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
#'Detroit': 'b463d3bd6064861b',
#resets the index (for ease of use)
# Creating DF which can be used in models
#Looks like the second file has less columns. Let's take a look at both files and see what we can drop
#Testing % change calculations and trading logic
# get the feature column
#read in the mean rsvp count data collected on Jan 27, 2018
# or we can aggregate the data according to criteria # $sample is a mongo function
# Query for finding the most active stations # List the stations and observation counts in descending order
# first tweet of 9/20/2017
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# create two Series, same start, same periods, same frequencies # each with a different time zone
# df_dummies.head() # df_value.head()
# first 25 shared domains
thecmd = '''curl -X POST -H "Content-Type: application/json" -d '{$   "query": "'''+query+'''"$ }' "http://'''+USERNAME+'''.carto.com/api/v2/sql/job?api_key='''+APIKEY+'''"'''
# Set up the Auth Tweepy object. This takes your credentials from above # and does some authentication with Twitter
# get all puts at strike price of $80 (first four columns only)
# 8. 
# Lets get the data for which we need values #Get all rows and only columns 6 through 54
#creating a metric to see number of competitors for a given project #number of participants in a given category, that launched in the same year and quarter and in the same goal bucket #since the above table has all group by columns created as index, converting them into columns
# Use the Base class to reflect the database tables
# Remove mismatched  rows
#a computer can understand
#review saved data
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Counting the no. commits per year # Listing the first rows
# above upper quartile
# concatenate tc physcial and electronic dataframes into one
# Convert into log2(tpm+0.001)
# heaviest connections in the graph
# Read the variation of heat demand from Data.xlsm
# results are returned as an iterable list
# creating a new dataframe with just the columns I need
#Plot the results as a histogram with bins=12
# 총 50개가 있어서 상황상 딱 맞는다
# Fit the Pipeline on train subsets
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned #r = requests.get('https://www.quandl.com/?apikey=2WWxsMC5KvrYAngyPH-W.json')
#Split Data ###YOUR CODE HERE###
# a function that cleans the text of each tweet by removing removing links and special characters using regex.
###YOUR CODE HERE###
# Therapists decomposition
# Make a pivot table containing ratings indexed by user id and movie id
#Check via customer sample
# on the occasion: when using multiple boolean indexes,  # ** make sure you get the parentheses right! **
# recode column names
# Summarize the scrapped documents from hot.json
# Set the limits of the x axis # Set the limits of the y axis
# Scrape NASA Mars News # URL of page to be scraped
# accediendo a una fila por etiqueta y obteniendo una serie
#Group male data by moon phase
# aprovechamos y salvamos este nuevo set # y recargamos la informacion nuevamente
# merge with main df
#Tweepy Cursor handles pagination .. 
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# for testing
# users
# leave only uncanceled ride
# Save references to each table
# what does the dataframe look like now?
# manually add the intercept
# dateutil
# We create a column with the result of the analysis:
# A:
#preview
# select all rows for a specific column
# Is is okay tha NaN in first_published_at converted to 1970-01-01 00:00:00.000009999 ?
# plt.show()
##### named tuple
# Initialize PyMongo to work with MongoDBs
# look at unique regions that are missing data # this code might be wrong...
#verifying the dataframe contains 100 tweets from each of the news channels
# 返回当前系统所使用的默认字符编码
# calculate a one day offset from 2014-8-29
# Check out columns
# Test
# create grid id 1 to 1534 and save as type string # head and tail
# We can also delete elements using the drop method
#full_stats = df2.groupby('group') #full_stats.describe()
# longest_date_interval.columns = longest_date_interval.columns.droplevel(level=1) # longest_date_interval = longest_date_interval # .remove_unused_levels().droplevel(level=1)
# creating array for ARIMA modelling # ARIMA model #printing parameters
# open netCDF file
# Decode the JSON data into a dictionary: json_data # Print each key-value pair in json_data
# get final event of the PA, as this will say single/hr/k/walk/etc
# score summary
# Setup 
# tensorboard = keras.callbacks.TensorBoard(log_dir="../logs", write_graph=True, write_images=True, histogram_freq=1)
# the attributes it have
# path length for each branch point
#Examine the summary stats of the per_studen_budget column to create bins needed for next analysis. 
# Podemos crear un dataframe como sigue: # Hacemos un display del dataframe:
# output full forecast dataframes to csv
## Check if there is 'None' in their 'userID' column ## Check how many tweets are we left with now
# word2vec expects a list of lists. # Using punkt tokenizer for better splitting of a paragraph into sentences. #nltk.download('popular')
#Save to a local file. First collect the RDD to the master  #and then save as local file. # creating a new file, writing contents to the file, and saving.
# Empty search to ensure it is working # res["hits"]["hits"][-1]
#Events per day
# Checking the datatypes of our submission sample
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
# looks like there aren't any posts with this topic as the max
# Time Conversion
# Keys become column headers, indexes becomes row labels
#rf_enc = OneHotEncoder() #rf_enc.fit(rf.apply(X_train))
# -> Having more severe depression leads to worse outcomes
# Common citation violations in residential over commercial areas
#read value into row, the same session created previously will be used
# Make a new dataframe with the tag lists expanded into columns of Series.
# Save final version here. #json.dump(fp, youtube_urls, separators=(',', ':'))
# update new category names with new (interpolated) category names 
# calculating or setting the year with the most commits to Linux # print the result
# this might run long
#merge with itself? or drop -- are these aliases or distinct babies
# Count the number of stations in the Measurement table
# Instantiate the count vectorizer with an NGram Range from 1 to 3 and english for stop words. # Fit the text and transform it into a vector. This will return a sparse matrix.
# Calculate how many times people talked about IKEA by following function  # Applying same way, count data of other interested cities are saved as following dataset
#'Phoenix': '5c62ffb0f0f3479d'
#df = pd.read_csv('/home/bmcfee/data/vggish-likelihoods-a226b3-maxagg10.csv.gz', index_col=0)
# get english stop words
# Here we are merging the new dataframe with the sp500 adjusted closes since the sp start price based on  # each ticker's acquisition date and sp500 close date. # .set_index('Ticker')
#type(rptdt)     pandas.core.series.Series
# This is the column I was interested in creating.  It should contain the same information that I would get by dummying # all of the lead_mgr values.
# Find the longest name
#query to calculate the total number of stations.
# Select for just the graffiti removal entries
# checking out their unique values, for a single level  # checking out their unique values, for combinations of multiple levels # See answer at https://stackoverflow.com/questions/39080555/pandas-get-level-values-for-multiple-columns
# Scrape JPL Mars Space Images - Featured Image # URL of page to be scraped
# creating a matrix after dropping nulls
#quantitative-categorical-categorical
# -f = to ignore non-existent files, never prompt # -r = to remove directories and their contents recursively # -v = to explain what is being done
# take a look at the data
# common_words
#Temperature - fill gaps under 3 hours before aggregation
# index positions correspond to integer values stored in vocab dictionary
# the above p values 0.1291 and 0.4558 clearly states no significance
# Predict stock prices
# Plot ACF of first difference of each series
# quick and dirty investigation for LTDate
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# Just get rid of those annoying |
# Calculate basic statistics.
# %load ../solutions/sol_2311.py
#query for precipitation data based on date range from most recent to a year before #create data frame from sql query
#Concatenate (or join) the pre and post summary objects
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
# Remove stop words from "words"
# save data to csv
# Importar el modulo data del paquete pandas_datareader. La comunidad lo importa con el nombre de web
# Add a column level for our new measure # Concat it with our original data
# count number of fire  per weekday in May 2018
# How many stations are available in this dataset?
q="""select capital_gain,capital_loss,(capital_gain+' '+capital_loss) Net_capital_Gain from adultData"""$
## чем отличается фит от предикт - делает фит (обучает модель) и предсказывает
#Check shape first
# time between questions posing and first answer
#Initialize modules for plots
# Display the row's columns and data in dictionary format (first row)
# loading data into pandas for inspection
## Now predict on the test set with the training model
# Count the total number of tweets
##### the index which was created in the dataframe
# Inspect master size
#avereage hourly rate
# get life expectancy at birth for all countries from 1980 to 2014
# area plot of their age distribution
# define the data path # load json data # view the first 5 rows of data
#Show the row with the index matching Jan 1st, 1975
#Ah predict already how? I dunno how to put back :X
query = ''' SELECT t2.id, t2.created_at, t2.is_retweet$             FROM tweets_info t1 INNER JOIN tweets_info t2$             ON (t1.retweet_id = t2.id); '''$
# Use Pandas to calcualte the summary statistics for the precipitation data
# examine some of the indicators
# Columns with results
#Mars Images
#Select lang field and counting returned values from the table
# minimum age
# get dummies for country and category type
# converting the timestamp column # summarizing the converted timestamp column
#### Change name and drop [Neighbors_Obj]
# print the HTML for the first result.
# analyze validtation between BallBerry simulation and observation data.
# Reset index
# Integer slicing
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned #"https://www.quandl.com/api/v3/datasets/FSE/AF_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key="+API_KEY
# FAVOURITE TWEET:
# Put the data into HDFS - adjust path or filename as needed
# plot first 10 samples from training data # plot 10 random samples of test data 
# TASK B ANSWER CHECK
# create pca to explain 99% of data # fit our scaled data
# use cross_val_score() # ... #
#df_trimmed.info() #df_trimmed.head() # df_trimmed.eventClassification.unique()
# tencent_df.set_index('date').pivot_table(index='date', columns=['second_type', 'work_year']).fillna('--').rename(columns={'salary_mean':'平均薪资/K'}).to_excel('/Users/monstar/Desktop/tencent.xlsx')
## check whether the parsing succeeded for all the locations
#It is easy to do a plot on this:
# Strange data
# defind observation data
# Annnnddd... now every column has no string datatypes... :) 
# 271 is a memba production (not a photoz) #print "Number of rows with 'production_id'==271   : ", len(np.where(df3['production_id']==271)[0])
# Let say you want to x in index 1
#ADF(resid_6203.values)
# remove outliers
# Need to run this first, apparently: # http://forums.fast.ai/t/understanding-code-error-expected-more-than-1-value-per-channel-when-training/9257/10
# show summary statistics for number of claims
#  利用參數kind來變換圖形模式
# Check for missing values # Extract data where is_attributed == 1 # Check NAs
# вакансия может быть активной в течение месяца
#plt.hist(tag_degrees.values(), 50, normed=True) #Display histogram of node degrees in 100 bins
# Transform percentage into float
# load .csv files created by SCRAPING
# Filtrar solo las columnas que son FLG
#zt = r1_test[r1_test == False].index.tolist()
#Question 4
# Create a Word / Index dictionary, mapping each vocabulary word to # a cluster number                                                                                            
#Lexington-Fayette': '00ae272d6d0d28fe'
# convert jsonData to pandas dataframe
#creating derived metrics/ features #converting the date columns from string to date format #will use it to derive the duration of the project
#Example2: Passing the sheetname method 
# Unique operator values in stops
# Create your stream object with authentication
#### Define #### # Remove all tweet record that have a non-null retweeted_status # #### Code ####
# drop unused columns
#Iterating through the companies data frame for the twitter usernames #printing every twitter username
# Inspect the data to determine values for an implicit enum
# the length of the review
# use a right close
# collecting the violence levels 
# Make ved folder
#Now, let's modify the dataframe to show only those rows that have a "complete" in the Current Status column, since we want to include only those permit applications that are marked as "complete" in our analysis. 
#Save News Tweets to csv
'''Performing Polynomial Features on the Pending Ratio Column'''$
# plt.plot(np.cumsum(pca.explained_variance_ratio_))  # plt.show() 
# accediendo a varias columnas por le label
# flight.describe().show()
# Kind of a hack because of the Spark notebook serialization issues
# the number of reports from the most active station
# One last downcasting check...
# minimum you need to pass to upload a dataset
# OK. Now I want to look again at my dataset head to remind myself of the column names and values for each.
# data=stock_data["CLOSE"].iloc[-100:]
# Option 1 is the generalized solution to reorder the index levels # Note: We're not making an inplace change in this cell, #       but it's worth noting that this method doesn't have an inplace parameter.
#print data
# Convert data into a DataFrame
# empty dictionary to append number of tweets to 
# for l in range(0,20):
#Convert event time to datetime
#feature_set  # uncomment feature_set in order to illustrate.   May require notebook reboot depending on memory. 
# create a copy
#Display a few beginning rows.
#convert pandas DF to HTML file
# create category column # create single name column
#import pandas as pd
# Remove all rows with NaN birthyear
####TEST # b['Compound Score'].mean()
# convert rate for Pnew under the null
#!/usr/bin/python ## dd/mm/yyyy format
# 5.What was the largest change between any two days (based on Closing Price)?
# Make predictions on test data using the Transformer.transform() method.
#Calculate average math score
# mean and variance w/in each group
# Define the Cosine Similarity function
# Obtain the source names for reference
# or # 0 is Monday so 2 is wednesday
# Prepare data
# concatenate tc physcial and electronic dataframes into one
#Example1:
#delete duplicate receipts 80874 duplicate receipts
#dfg.loc[('drg3', 'Red'), :] #dfg.loc[('drg3'), :5]
#users who purchased multiple cameras and at least one of those were a replacement
### Create the necessary dummy variables
#Check the combination of contractor_number and contractor_bus_name are unique #The combination is unique so far.  #Check duplicated rows by contractor_number 
# new_discover_sale_transaction.groupby(by=['Email', 'Created at'])['Total'].sum()
# It seems 1st user will like her profile whereas 2nd one won't.
#Monthly average
## YOUR CODE HERE
# make new column for year. Then drop all rows from 2018
# P value
#Read the header of the dataframe
#### cache during development
# Find top-10 tweeters:
#Example1:
#获取2014年第3季度的业绩报表数据
#We must convert the Polarity to string to be used as a map attribute
# let's compare the three legs of the first itinerary, similarly as we compared the itineraries
# set the index to date
# Concatenate series # Index has to be ignored because the series 's' does not have the index column
# Print all classes mapped to the Base
# This merge results in duplicate columns, marked by _x and _y suffixes:
# remove selected workspace #ekos.delete_workspace(user_id = user_id, unique_id = workspace_uuid, permanently=True)
# Check if users could be in calibration for only some days
# df_2017
# rename Tip_Amt to reflect new value: TipPC
# transform the data
# delete cached result:
# Create new column with clean text # Add a column nwords 
# df.drop_duplicates(subset=['A', 'C'], keep=False) # df.total_idle.value_counts()
#count of converted users in treatment group
#count of users with new_page
# We will join on the email so we need to make sure the email address column is formatted in the same way
# this ensures the plot appears in the web browser
# CHECK THAT LOGIN SUCCEEDED
# Applying the function again to the column 1
# Find the max and min of the 'Total Number of Rides' to plot on x axis
# suburban
# used to check that we have converted the objects into datetime rows from above properly
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# output.coalesce(2).write.parquet("C:\\s3\\20170503_jsonl\\output.parquet")
# check option and selected method of (11) choice of groundwater parameterization in Decision file
#finally 3 hours!
# confirm records were dropped
# result = {'customerID': avg_interval_between_reorders}
# Checking How big the Data set is
# 1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD).
# Suburban cities average fare
# connect to twitter # write up about API object
# group by names  # show the replicated names
# pickle classified data.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# set SUMMA executable file
# Tot number of stations are 9
# what does requirements.txt look like?
# Print summary statistics for the precipitation data.
# loading the processed csv containing the urls for all of the op-ed pieces
# Listing top 10 authors
#Change Dates to string fo manipulation
# Define model and loss
# Put the data into HDFS - adjust path or filename if needed
# might be affected by timezones, so look at just US. # looks like there's a spike around lunch hour (1-2 pm) and around after work 
# identify nulls
#Resetting the index to avoid future issues #Dropping unnecessary issues due to save memory and to avoid issues
# Plot the histogram
# analyze validtation between BallBerry simulation and observation data.
#tweetdf.to_csv("tweets_w_lga.csv")
# Example # Get Apple Stock
#converting all the datetime #segments.st_time.apply(lambda d:datetime.strptime(d, '%m/%d/%y %H:%M'))
# DataFrame objects have a nice plot method
# Run the model with training set 
# Create a column with the result of the sentiment analysis # Display the updated dataframe with the new column:
# For getting all the texts under the div tag
# Lista de palavras para ignorar usando os pacotes do NLTK
# convert the result proxy to a pandas dataframe
### read in IOTC csv file scraped from iccat site (for Chinese vessels only)
#df_low_temps.head()
# movie meta-data
# This will give the total number of words in the vocabolary created from this dataset
# this is an inner join by default
# contamos cuantos elementos tenemos sin valor
# Baseline score:
# read CSV
# Creamos el dataframe con los tweets # Mostramos los primeros 5 tweets en nuestro dataframe
# Translated role
# How many stations are available in this dataset?
# Instead of leaving it as NaN set it to 0, although that's not possible as event cannot have "0" gathering
# are classes balanced? #86.31% will be benchmark for our models
# For retrieving the xml content from the sitemap
# Run this cell to download the data
#(player['events'] == 'triple') | #(player['events'] == 'double'), |
# Getting logs of usre 1
#And let's try this again:
#Count the number of stations
# creating ad_source df
# skip only two lines at the end # engine parameter to force python implementation rather than default c implementation
# Split arrays into "training set" (train_Features, train_species) and "test set" (test_Features, test_species)
# figure points
# Author: Jaime Fernández del Río
#'San Diego': 'a592bd6ceb1319f7'
#Setting the date time as the index allows time slicing
# convert pandas column into matrix to use in ARIMA model
# plt.scatter(f1, f2, c='black', s=7) # Data Normalization
# Initialise with a list. #
# Station and Measurement
#Make predictions using fit model
###YOUR CODE HERE###
#normaliza os dados da versao 2 para range de 10
# cvec_4 top 10 words
# load the model from disk
# get number of unique elements
# Apply the fix_timestamp function to each timestamp row in the gdax_trans DF
# read pickled data.
# create dataframe from list of tuples
#Get the rows has appointment only
# What is the average?
# isin関数によって、customer_idがサンプリングした顧客IDのいずれかに一致した行を抽出 # (target: Series type)
# split the data set into mentor-mentee based on the number of collaborators a person has
# Convert pandas table into html
#ff6042c59f3870e2 -- original hash #fe6046c59f3870e2 -- new hash, off by 1
#test the counts #B4JAN17['Contact_ID'].value_counts().sum() #B4JAN18['Contact_ID'].value_counts().sum()
# select rows using .loc
# creates nodes in a graph # "construction phase"
# Delete a collection snapshot # To delete all snapshots, use: # collection.delete_snapshots()
# 10M word/token corpus  
# # we'll filter out the non-represented classes, sort them, and plot it!
# Read .csv files from kaggle
# vemos los tipos de datos de cada columna ( los que puso pandas por defecto )
# df_session_dummies.head(n=2) # df_session_dummies_drop=df_session_dummies.drop(['created_at','value',],1) # df_session_dummies.head()
# use defaults, let primary key auto-increment, just supply material ID
#see how many counts are negative 
# Use Pandas to print the summary statistics for the precipitation data.
# Creates a subset dataframe from another dataframe
#Selecting the interesting features #Adding a column containing the target
# Create an extractor object: # Create a tweet list as follows: # Print the total number of extracted tweets
# A nice thing about Pandas is that series and dataframe methods output new series and dataframe objects # So we see that the type of value_counts is itself a series:
# open temp nc
#Check for non null values
# Examine the results, then determine element that contains sought info
# sample data: note that Series auto-generate index  # histogram by value_counts
# Build a classifier # k is chosen to be square root of number of training example
"""$ Check shape and confirm that no NA values are present$ """$
# Find users converted propertion
# Only fetchs historical data from a desired symbol # When we are not dealing with equity, we must use the generic method # or qb.History[QuoteBar]("EURUSD", timedelta(30), Resolution.Daily)
# display(flight.select("duration").show()) # display(flight2.select("duration_h", 'duration_m').show()) # flight2.toPandas().head()
# We want to extract 5 topics: # We fit the model to the textual data: 
# change data type # ARR by industry QT
#'Wichita': '1661ada9b2b18024'
#Save current featured image of mars in var featured_img_url
# A look under the hood:
# .loc can deal with a boolean series
# We display the average salary per year
#Count All Tokens
#What does our index look like? 
#Display the dimensions of the data frame
#Group data by year and compute the total flow and count of records #Remove records with fewer than 350 records #Rename the 'sum' column
#Storing the movie information into a pandas dataframe #Storing the user information into a pandas dataframe #Head is a function that gets the first N rows of a dataframe. N's default is 5.
# check Basin Parameter info data in file manager file
# concat df and coming_next_reason
## check to make sure NaN appears for the first row for an SCP
#Getting right time
# new groupby object by department AND city
# sen_data.to_msgpack('full_DataFrame/combined_data/master/sentiment_data_2min.msg') # sen_data2.to_msgpack('full_DataFrame/combined_data/master/sentiment_data_5min.msg')
#(print(len(a[0])) #The lenght of the feature vectore is equal to the number of different words in the test sample.
# Incase the street length was wrong for any street, we take a median for street length for each street
#Sort INT by Contact_ID and Interaction Name
#Calculate average reading score
# Convert the url to df
#What about location since timezone was surprising?
# Since the underlying data of Fundamentals.exchange_id # is of type string, .latest return a Classifier
# used later to find coefPaths # used later to find the original location of the path from non one hot
#READ THE LIST OF NEWSPAPERS
# The SUM produces the number of emails opened # The COUNT produces the total emails sent
# print(type(df)) # print(df.shape) # print(df.head())
#date_str = date.strftime("%Y/%m/%d %H:%M:%S")
# The protocol version used is detected automatically, so we do not # have to specify it.
# loan_stats["revol_util"].describe()
# Getting predictions for train from our training set
SQL = """$ SELECT rating, COUNT(*) FROM film WHERE rating = "PG" OR rating = "G" GROUP BY rating;$ """$
#display(dflong) #dflong.Date.unique()
#cek print head - zona waktu as index
# Timeline for the dataframe
#housing prices
#joined_test[v] = joined_test[v].fillna(0).astype('float32')
# Store link
# last 12 months qty received
# Round capacity values as well as the efficiency estimate to five decimals-
# a path without the root package name does not always work (e.g., it works inPyCharm, but not in Jupyter) # If the subpackage will not be imported later with a separate import command, it can also be included in initial import  # import package_within_package 
#'New York': '27485069891a7938'
#  - 申万一级 H 股
# Create dataframe from the .csv files or simply read in csv files. 
# Create online deployment.
# Verify that an address is valid (i.e. in Google's system)
# Converting my tobs_date list to a dataframe.
# Take a peak at the data
# Read the JSON file into a list of dictionaries
# intersect the two lists (lists are tuples of words and their frequency, we need the first value only)
# Assign NaN to all entries in L3 equal to ' '
# Merge to our features df
#tweetdf[pd.isnull(output)]
# fig.savefig('toma.png', dpi=300)
# Issues open
# Language / culture can have a huge effect on NPS responses; does it here?
# convert dictionary to dataframe
#psy_prepro.head()
# rural
# Plot ACF of first difference of each series
# Import package # Build DataFrame of tweet texts and languages # Print head of DataFrame
# write data to csv
# Logistic Regression on age and subreddit 
# cvec_1 top 10 words
# define path to save model
# pairings < 3m / all pairings
# check if any values are present
#定義沒意義的字
# directory
#Show the results; water_year and water_year2 are the same...
# common words from the reviews
# Printing multiple outputs per cell
# shift forward by 5 hours
# Run the prediction
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# join for plotting purposes
# tweets.apply(lambda row: print(row['tokens']), axis=1)
#plot the histogram
# The wikipedia URL that every article has in common
#Show first 5 tweets
# call edftocsv function
# lets fit logistic model for the countries with ab_page, baseline = CA
# json objects are interactable in the way python dicts are.
#Compute annual Nitrate concentration
# This shows that we guessed 0 for each one of our rows.
#Group by Target, aggregation average by Compound 
# KEGG mapping of gene ids
# getting train and test
# regular expression aka wildcard pattern
#Find Precipitation Data from the Last 12 months
#most active stations over the entire data set. Defaults to descending order of count
#try with a csv
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# OSX Users can run this to open the file in a browser,  # or you can manually find the file and open it in the browser
# write code to drop column below:
# the contents of the date column
# post simulation results of Jarvis back to HS
#Read in the data into the 'sites' dataframe 
# Importing data from the processed 'news' DataFrame. # Define current working DataFrame.
#May-Nov has (31 + 30 + 31 + 31 + 30 + 31 + 30)(24) = 5136 hours in total. There should be 5136 entries
#non_nlp variables  # y=df_combined['subreddit'] # transform the label 
# deciding the cut off
#verifying the counts equal the amount of rows of the dataset
#We see some null values in the gender, city bd registerd_via column, we replace those either with mean(numeric) or mode(categorical)
#Här får jag ibland felmeddelande core has no attribute ParameterMapping 
#coverting a csv file into a dataframe
#Exploring the dataset #We can see thet there are high number of 5-star reviews
#sort index by date (earliest tweet to most recent tweet)
# Retrieve data from Mars Facts # Put into pandas table
# df_lib_con.title = df_lib_con.title.str.replace('\d+', '')
# count at 0
# Use Pandas to calcualte the summary statistics for the precipitation data
#make sure query is up-to-date #query to match BABYID_BABYSN_CHANNELID_PAIRDATE for all channels # API_KEY = 'LcuoHqjZLvxaSPDrhv5VMhcrJUyPVb88RJR69REq'
# predict # round predictions
# get number of non-NaN elements
# The Duplicates have diffrent time fetched the file
# We can save a reference to the bound method and call it later and it will use the right instance
# https://spark.apache.org/docs/latest/ml-classification-regression.html#multinomial-logistic-regression
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Saving Plot
# Remove row using drop ... axis=0
# Calculating  how representative the 2nd df is of the 1st df through the proportion of their count volumes 
# today_
#获取2014年第3季度的现金流量数据
#url for mars news
# check that it works
# Save the dataframe as a csv file to be used elsewhere
#Strip unwanted newlines to clean up the table.
# A:
# Count the number of stations in the Measurement table
#show the shape of dataframe
# and we can see it is a collection of timestamps
# Output a csv file for analyze
# Transform the text to a vector, with the same shape of the trained data.
#Word frequency for all terms (including hashtags and mentions)
#Set the index to be the bus names
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Get the last 100 tweets from BBC, CBS, CNN, Fox, and New York times.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# To use the saved classifier # The above 2 cells can be commeneted once the classsifier has been trained and saved to a file
# again, let's take a look
# Strip/fix columns and index # Take a quick peek to make sure everything's fine
# plt.savefig('Actual number of days between purchases')
#Column clean-up
# Design a query to retrieve the last 12 months of precipitation data and plot the results # Calculate the date 1 year ago from today
# Use Pandas to calcualte the summary statistics for the precipitation data
# Create gensim dictionary object
#Review data in dataframe.
#### Test ####
# Import packages using the import command.  # Chaning my working directory
#getting summary of large dataset
# Create df from rf model predictions of test set data
# Overweight
# Create model # Compile model
# Fixing the datatype 
# few days ago it was 758
# run ltv model
# getting a single document:
#print(dataframe.head())
# use dropna to eliminate rows or columns with NaN values # We drop any rows with NaN values
# We can append a column to our existing index
#example_text_file = 'sometextfile.txt' #stringToWrite = 'A line\nAnother line\nA line with a few unusual symbols \u2421 \u241B \u20A0 \u20A1 \u20A2 \u20A3 \u0D60\n'
# Use list comprehension to create new DataFrame column 'Total Urban Population' # Plot urban population data
# check out how many years out it's been since the measurement. Really two groups, 1 year and 6 years
#happiness_df
#Let's plot a subset of the data in order toget a better sense of it.
# We first name the file that contains our data of interest
# important: 'parse_dates=True' when reading docs with dates!
# create the column timestamp with Hours for our dataframe
# How many tickets in each topic
#将编码后的页面输出为txt文本存储
#Subset columns
# Point가 숫자가 아니다
# Keep only top50 datasets in dict (and convert int to str)
# write arrow table to a single parquet file, just to test it
# cumulative mean of the MC estimates
# Preview the information available for the table
# Padding NAN's
# How many stations are available in this dataset?
# Model persistence: save(), load()
# what aabout this 
# Create engine using the `hawaii.sqlite` database file created in database_engineering steps
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# What about intersection between KBest and elnet only?
# Largest change in any one day (based on High and Low price)?
# Pure Scorers
# get ndarrays columns from the dataset we just uploaded
# tokenizer = lambda key_phrases: key_phrases - должен взять список и отдать список  #(вместо взять сплошной текст и отдать список слов) # preprocessor=lambda x: x - должен взять список и отдать список
#1 we are going to put some other values instead of NA
# Extract endpoint url and display it. #scoring_url = deployment_details['entity']['scoring_url']
# Assign the Measurement class to a variable called station
# distribution plot of X test
# reading a file containing list of US cities, states and counties
# Perform a MWW test # Print the p-value. Note I have specified I want to print a floating point decimal with 15 decimals after the period
#ANSWER TO THE QUESTION take two:
# Looking at one tweet object, which has type Status: 
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
#export df to csv
#create the x,y training and testing variables from the dataframes. The size of the training set will be 80 percent #the size of the testing set will be 20 percent the variables
# plot autocorrelation function for doctors data
# view the entity pairs in descending order
# resampling degree day data. The first line takes the average when a given hour has multiple entries. # The second line generates daily data.
# driver = selenium.webdriver.Chrome(executable_path = "<path to chromedriver>") # This command opens a window in Chrome # driver = selenium.webdriver.Firefox(executable_path = "<path to geckodriver>") # This command opens a window in Firefox # Get the xkcd website
#Renaming a column in files4  #Making  a jobcandidate column in files4 
# apply function to get stats
# Past 10 days
# create new column "age" by subtracting fetched time - created time. # To make it readable, timedelta64 is needed. [m] gives by minutes-long, [h] by hours. 
# Create logit_countries object # Fit
# interactions between treatment page and country
# for game-state variables we care about the beginning of the PA, since that mostly determines strategy
# # # created a dataset that use postcode as index
# read in data from HDF5
#Read first row: In Document Nr 0, word nr x appears 1 time.
# 找出每个游戏 最新更新 的download
#term_freq_df .head()
# Create a Soup i.e  ## A new Column that combines Category, EventName and Location ## for applying TF-IDF
# ! cd /home/ubuntu/s3/flight_1_5/extracted # ! mv /home/ubuntu/s3/flight_1_5/extracted/final_results/*.txt . # -exec runs any command,  {} inserts the filename found, \; marks the end of the exec command.
#3  to carry forward previous day value for a NA value, we use ffill
# we can also name the columns
# Print the first 10 most frequent words found in tweets that mention 'gender'
# Replacing nulls with 0.
#### write our earlier dataframe to csv file
# 14. Create and print a list containing the mean and the value counts of each column in the data frame except the country column.
#Design a query to calculate the total number of stations. #New DF grouped by station with new column 'count'
######################### Combine Data Frames ##############################
# load preprocessed data 2
# get 8-16 week forecast existing patients # keep only date in index, drop time
# Load tips data set # Display several random rows
#show the inferred schema SQL style
# make a GET request # read the content of the server’s response as a string
# For finding all the links
# calculate time between AppointmentCreated and AppointmentDate
# One hot encode features with boolean values 
#The coulumns are our features currently,and adding new column which will predict by shifting to the specified period
# Set storage path # show the new storage path
# in fact this is the same as looking at the group by on count_publications columns and count authors
# saved each to its own date and then loaded/brought them together to one df
# Increase the depth of the GBM on this new, reshaped data # We can increase the AUC a bit, perhaps not worth the problem
# To check wheather our dummy variables are create or not
#precipitation descriptive statistics
# Read the Train and Test files
#Export the data in the DataFrame into a CSV file
#sort by CBS in order to create different color scatter plot
# Iterate through all the columns # This is an example of not choosing to save the image
# Convert release_date column to datetime object
#Check duplicated rows by contractor_number since contactor_number is unique per contractor per jurisdiction
#Great. Now let's use the groupby function to count the number of  permits completed by year and month.
# create a Series of incremental values # index by hour through all of August 2014
# wide to long # view head of final row-wise dataset
#finding the important features
# We'll also rename some of the columns so that the output is cleaner.
#6875 Outlier, Depth = 675
# Save data to Excel
#final.head()
### Step 15: Fit the model and then make predictions ## The labels will identify if we are in the 0 or 1 cluster
#use_data = use_data[(use_data['win_differential'] <= 0.2) | (use_data['win_differential'] >= 0.9)]
# data=stock_data["CLOSE"].iloc[-100:]
#write to data frame
# Group by group_period as 7D, since market cap data points set at weekly
# 'Reno': '4b25aded08900fd8'
# Save to an Excel file with a single worksheet.
#Example1:
# We extract the mean of lenghts:
# Agrupar por Año y contar la cantidad de páginas a las que les dí likes
# determining the first real commit timestamp (by Linus)
# set the target site as a variable
# add features to train_reduced # check wether the join operations where right
# Another sample text
cur.execute("""SELECT title, journalname from public.datapull_title limit 10""")$
# create new dataframe with profile_ids # add feature value column to existing dataframe
'''Here we are creating a new column named old_page_converted with a length equal to the old_pages count$ calculated above and filling them with one's and zero's with the mean probability we calculated above for $ one's and remaing for zeros'''$
# data_air_visit_data['air_store_id'].drop_duplicates().count() # data_hpg_reserve.info
# Add Text Data
# Some care is needed when removing warnings. But for the final version of this notebook it should be safe.
# latest_time_entries from last 9 days
# Take a look at the submitter_user field; it is a dictionary itself.
# transdat = temp_data.loc[1:700,["OPEN", "HIGH", "LOW", "CLOSE"]] # transdat # temp_data.head(400)
# to get vector graphic plots # to get high resolution plots
# Reading data
# create new timestamp column in the bird_data, pass the data form timestamps list and match its index columns 
# Load uber data
# Display of first 10 elements from dataframe:
# Collect data for ticker AFX_X for whole year 2017
# NASA Mars News - title
# mean
# Export data to csv
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#task 6:  What was the average daily trading volume during this year? #Create an empty list named traded_volume to hold the 'Traded Volume' data for the whole year #Calculate the average daily trading volume #Calcula 
# Note: x-axis is the index.
# 2-3-08 belongs to the 2007 Season, including # the given dates, which were start of season and # the Superbowl.
# Joining categorical data back with Ideas DataFrame
# Likes vs retweets visualization:
# 3. Create a new Data Frame called uni containing only rows from free_data which indicate that  # the person attended university or graduate school. Print the value counts for each country.
# The Dataframe
#Save figure
#Show the first 4 rows (again): Note that if we omit the lower bound, it assumes it's zero
# Step1:  get rsIDs for the variants
# concat grid id and temp
#Great. This is what we wanted. Now let's convert the result, a grouped series, back into to a dataframe and rename the last column as "Completed_Permits". 
# display(flightv1_1.select("flight_leg1.carrierSummary").show(100, truncate=False))
# Check that the largest observed value is <= step threshold
#model_w.summary() # For non-categorical X.
# experimentの状況確認
### Step 14: Import Gaussian Mixture model to investigate the PCA plot ## Specify 2 -> for two clusters
# accediendo a una columna por el nombre (label) y obteniendo un dataframe.
# If Q4 = no; add n/a to Q4A # assigning each answer to unique Q list
#We need to define Required = Y mandatory to run experiment
#append the old and new csv
#Import file 'Sample_Superstore_Sales.xlsx' which is in FourFiles.zip
# Test function
# Save the references to each table.
#use_data = use_data[(use_data['win_differential'] <= 0.2) | (use_data['win_differential'] >= 0.9)]
# return a frame eliminating columns with NaN values
#create a Poisson variable called observation. #we combine our data (count data), with our proposed data generation scheme (lambda_)
#in descending order
# questions.loc[questions['zipcode'] == 'Npahel17@gmail.com']
# this method works, to extract the hour of each datetime value
# what percent is that?
# this is the data per subject
#member_pivot
# Download all mapped PDBs and gather the metadata
# display unique values with counts for neighborhood_district
#replace '-'(hyphen) with '_' underscore to make database operation
#sample 1000 events at random
#install XGBoost
# Task 2
# Expand submitter_user into its own df 
#Inspect label balance
# drop previous index
# Load the data from the query into a dataframe
# the target 
#'San Francisco': '5a110d312052166f'
# commit changes
# Let's try this out with "device model" first - only about 7k values
# Instantiate a Materials collection and export to XML
# En esta instancia guardamos una copia de la informacion. Dado que no utilizamos ninguna de las columnas actualmente como index, no guardamos el indice que  genera pandas al importar el csv
# Divide each number of each countries columns by it's annual maximum 
# Get the shape, # of elements, and # of dimensions
# converting the timestamp column # summarizing the converted timestamp column
## train-test split
#Check the dimensions of the dataframe: it's BIG with close to 100k records!
# sort posts by number of likes, and only return posts that contain a tattoo:
# display unique values with counts for council_district
#find the shortest line
#Get my document sample
#model = ols("happy ~ age + income + np.power(age, 2) + np.power(income, 2)", training).fit()
#plate_appearances.loc[plate_appearances.batter_name=='ryan howard',['is_shift','hit','successful_shift', #                                                                   'shift_lag_1', 'shift_lag_2']].head(20)
# Create a scatterplot matrix
# List the deployments
#retriveing data form bitcoinity.org
# Use statsmodels to verify results
# dicts to map numbers to words/labels
# Inner join pax_raw with the rows we want to keep
# のちのちCSVで分析するかもだから、保存しておくよ
# Add the average.
# Thre seems some missing values filling by 0
# save the file to the output
#JPL Mars Space Images - Featured Image
#Extract the year from the datetime index
# generate a series based upon business days
# the number of reports from the most active station# the nu 
#Convert tobs list to data frame #tobs_values_df.rename(columns={'0':'Temperature (Deg. Fah.)'}, inplace=True)
# Extract just the BTC wallet transactions
# groupby on the ID. # SO_reference  #https://stackoverflow.com/questions/14657241/how-do-i-get-a-list-of-all-the-duplicate-items-using-pandas-in-python
# view df info
# Listing top 30 most popular hash tags during the data collection period # "#guncontrol', '#guncontrol.', and '#guncontrolnow' should be count toghether. So should '#marchforourlives' and '#march4ourlives' etc
# Time series for retweet:
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
#joined df_groups and df_events together after computing the number of events created over the past 7 sevens by groups
# All Data
# these are the columns we'll base out calculations on.
# print the p-values for the model coefficients
# check inserted records
# Printing the content of git_log_excerpt.csv # ... YOUR CODE FOR TASK 1 ...
# Go to this link: https://www.quandl.com/data/BCHARTS/ITBITSGD-Bitcoin-Markets-itbitSGD # follow the instructions on quandl's python page, store the results in the coin_data variable
# add active_user column, and marketing_source column
# Moving on to the User Information csv file...
# More data insepction...
# Now we'll try "device_id" - about 700k values
#we extract menas of the length
#status: any includes archived orders
# Plot the daily normals as an area plot with `stacked=False`
# Fill unknown zones with 'unknown'
# See "Special Case: Conversion to List" section for more.
# Using our SQlite database created just prior, (hawaii), # ..  we now create the engine:
## we see that the class ="ref-module-paid-bribe" contains information of all the reports in a page
# Create the Dataset object. # Map features and labels with the parse function.
# We load Google stock data in a DataFrame # We print some information about Google_stock
#### Define #### # Convert tweet_id column from int type to object type # #### Code ####
#Create InstanceType column from Usage Type col
# Let's pull top 5 subreddit in the most number of comments
# Time is stored in a raw computer format # But we can convert it to a datetime object so it's comprehensible. # note OTP returns raw time value with three extra zeros, divide by 1000 to get rid of them
# we're at a good stage to store the dataset
# cisnwh8
# path will need to be changed pending on where the repo is cloned to
# Instantiate the model
#'Santa Ana': '0562e9e53cddf6ec'
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# filter on Urban # len(Urban) # Urban
# Merge the adj close pivot table with the adj_close table in order to grab the date of the Adj Close High (good to know).
# train and evaluate the chunker 
#forecast_range.head()
# the attribute of the function (default parameters)
# 'stoi': 'string to int'
# Wait, a tomato isn't a vegetable! # This method will replace all instances of 'tomato' with 'pea'.
# Make an empty dataframe for the transmission capacities between the countries
# Look at Subreddit and Age features only  # Define X and y
# Extracting user 1 for testing
#replaceing time under an hour with 1 hour 
# Grabs the last date entry in the data table
# make a GET request # read the content of the server’s response as a string
#Check graph styles available
# Calculates how many people DID NOT indicate ageRange
# Create a new dataframe by dropping rows with NA data
# Combine ET for each rootDistExp # add label 
# read csv into data frame
# Use Pandas to calculate the summary statistics for the precipitation data
# Plot scores obtained per episode
# This is what the table looks like
# gDateEnergy_content.count()
# show available waterbodies
# shape of coarse grid
#6d. How many copies of the film Hunchback Impossible exist in the inventory system?
# check option and selected method of (11) choice of groundwater parameterization in Decision file
#Load the json file #Print the parameters    
# Use tally arithmetic to compute the scattering-to-total MGXS ratio # The scattering-to-total ratio is a derived tally which can generate Pandas DataFrames for inspection
#logistic regression to predict converted column using intercept and treatment columns
# 计算 游戏A 全渠道 最新 下载数量之和
#vectorizing words #vect = CountVectorizer(stop_words=stop_words, ngram_range=(2,7), max_df=.2, min_df=.0001)
# rename the values column
#create the linear regression model using the lm.fit function on the x and y training sets
# Read downloaded file into a pandas dataframe.  Note there are weird characters requiring the ISO encoding
# Продемонстрируем результаты
# SMA: 6, 12, 20, 200
# run the model giving the output the suffix "lumpedTopmodel_docker_develop" and get "results_lumpedTopmodel" object
# Store a Library and its MGXS objects in a pickled binary file "mgxs/mgxs.pkl"
# take a look at the first two lines
# Joining two data frames
# create pySUMMA Plotting Object
# Rural cities total drivers
# Pull up the same value in the "cleaned" dataset
# Probably want to create a datetime object for March 31st 2017 # create a logical column with the datetime object and a timedelta object
# Join on the bitcoin price information in euros
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Make predictions. # Select example rows to display.
# from tensorforce import Configuration
# ciscid4
# Find the div that will allow you to retrieve the news paragraph # Use this html, do not go back to the website to look for it # Latest news paragraph
#print(dictionary.token2id['movie']) #verify if these words were in the stopwords list #print(dictionary.token2id['n\'t'])
# Calculate Probability of old landing page
# Requirement #2: Add your code here
# import data
#'Orlando': '55b4f9e5c516e0b6'
# How many stations are available in this dataset?
# So this is just simple pandas!
# get the total number of collaborators
# Get indexes and values
# Calculate total days covered by the tweet file to determine criteria for ignoring tweets
# Find the numeric values: matches # Print the matches
#print(df3) #print(df_list) #print(df3.head())
# Now we need to grab 1 year back, from the last data available (above).. # ... iE we need to get the last 12 months of data, last date - 365:
# view the resulting isochrone shape (can you guess why there are separated geographies?)
# Assign the measurement and station classes to a variable called `Measurement` & `Station`
# Check the accuracy of the trained model
# Describe the variable 'Number of people injured'
#au.plot_user_popularity(very_pop_df, day_list)
#Categorical variables
# Retrieving latest date available
# Determine post age odds ratio 
#statistics_table.to_csv('compiled_data/statistics_table', index=False)
# Remove Dublicate user_id but keep data frame df2
# Use Pandas to calcualte the summary statistics for the precipitation data
# Average those cities together to get the default usage value.
#### Define #### # Change format in the 'time' column # #### Code ####
# load the trained model
# Specify a start point and how many periods should come after.
# How do those 54 recipes look?
# Call the function with lambda_val 0.1, alpha 40, 30 iterations and 10 latent features
#rename columns #print head of load
#Export to CSV:
# maybe a pie chart showing the different users and the magnitude of their contributions is the total number of lines changed/removed/added??
# We remove the store 3 row # we display the modified DataFrame
# save as csv file
# What are the most active stations? # List the stations and the counts in descending order.
# read in data
res = sqlCtx.sql("""SELECT name, stddev(result) as std__of_result$             FROM tempTable$             GROUP BY name""")$
# Calculate distance by coordinates using geopy.distance library
#3b Conversion of columns to proper types. ‘titles’ to string and ‘timestamp’ to an an actual timestamp object.
# Assign classes to a variable
# Design a query to calculate the total number of stations.
# mean() along each row (axis=1)
#gente con mejores estudios busca mas alto nivel
# Add stops which were not in oz_stops but were in stops
# create the path to the data file # the file is named 'test1.csv' # read in the data
# Find the lat/long of a certain address
# No null values, but from a visual check, there are definitely blank strings  # in the region and country series. We should check for those as well.
#  計算股價高於均值的次數 #  利用legend來控制label
# cvec_3 top 10 words
# export the Geodataframe as shapefile
# Create test data for predictions with neural net
# We'll explore the English to Spanish file to get a feel for it
# Identify the abbreviations, as these are not relevant to the SDA corpus.
# looks like someone just manually entered the same datapoint twice
# Merge the station data with census income group data
#query tables to get count of daily report, all temp data is complete for each record, so the count #reflects a count of a station giving temp data, prcp data may or may not have been reported on that date
#Saving unique ratings into a numpy array
# Loading data
# use an index vector to find genes that are longer than 2 million bases # sort the returned values by length # .iloc[::-1] reverses the sort to be max to min
# filtrar información aberrante tmin menor
'''Here we are creating a new column named new_page_converted with a length equal to the new_pages count$ calculated above and filling them with one's and zero's with the mean probability we calculated above for $ one's and remaing for zeros'''$
# Double Check all of the correct rows were removed - this should be 0
# Count terms only (no hashtags, no mentions)
# Calculate various qunatiles.
# Export CSV
# We don't want to join a group that is in a grace period!
# creating attend csv
#### Define #### # Join all three tables using unique key 'tweet_id' # #### Code ####
# As the values for converted are one of the two {0, 1} mean can be taken # to determine the probability
# !cd .. # !curl -O http://www.grouplens.org/system/files/ml-100k.zip # !unzip ml-100k.zip
## Patch of the sky in the Milky way observed by OGLE:
# Find the key of the dict
# Top 10 least active companies
#Example
# p_new under null 
# np.where() is a vectorized if-else function, where a condition is checked for each component of a vector, and the first argument passed is used when the condition holds, and the other passed if it does not # We have 1's for bullish regimes and 0's for everything else. Below I replace bearish regimes's values with -1, and to maintain the rest of the vector, the second argument is apple["Regime"]
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# The DataFrame is created from the RDD or Rows # Infer schema from the first row, create a DataFrame and print the schema
# replacing all 'N,0"' values in the country column with 'NZERO' to avoid discrepancies while one hot encoding
## 간단 분석
# fit tfidf dataset
# alternative: .sample and .difference
# Assign the station class to a variable called station
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#REFLECT DATABASE TABLES INTO THE SQL ALCHEMY ORM # Create an engine to connect to the database.
# review small df window after engineering lagged features/target vector(s)
# Copy data_df and rename columns
# Calculate time laps in Eric's data: # created times variable containing timestamp data for Eric # Create elapsed_time list containing time elapsed between next timestamps 
# Export CSV
# Clean up zones column.  Reduce to either 'Single' or 'Multi'.
# Enter the density of the material that was sieved.
# Save weights from the model, allows prediction without retraining and sharing model with others.
#same result as before!
# Take the mean of these two probabilities
# Set your x and y limits
# probability of conversion regardless of page
# convert the text column from the dataframe to a string
# Create a Beautiful Soup object
#checked values using (len(old_page_converted))
# reindex
# find the table # for row in table_rows: #     print(row.text)
# And of course, we can easily save our new data
# Location; use labeled index # Integer location; use numerical index
# Send and catch the request: r
# view the processed dataset
# Return descriptive statistics on a single column
# customer creation distributed date
# Saving the DataFrame as a .csv file.
# instead of accepting the unwieldy labels we can define them
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure ## Structure of the JSON:
#save as csv
# df_session_dummies.head(n=2) # df_session_dummies_drop=df_session_dummies.drop(['created_at','value',],1)
# Double Check all of the correct rows were removed - this should be 0
# SENTIMENT ANALYSIS BY NEWS SOURCE # To create the plot, first we need to know the exact source names as reported in the file just exported out
#The final recommendation table
# put response data into the data dictionary
#probs_test.detach().data.cpu().numpy()[:, 1].mean()
# read the oracle 10k documents 
#Create date columns #Create month_year columns
# Load JSON as Python Dictionary, for data in 2017
# removed string word comments
# your code
# your code here
#Example 7: Import File from URL
# Drop all rows with missing information
# create_database('mysql://root:root@localhost:3306/stocks_nse')
# It is the same as:
#Boxplot functions
# retweets
# create a column with the result of the analysis # display the updated dataframe with the new column
# Display location of saved model.
# Extract slice - 2
# Combine columns into one stage column
# Data
#lets authenticate with Twitter which means login via code
#Now you can write your dataframe to hyper file, which you can open with tableau directly. Then have fun with your EDA!
# When convinced Raw and Processed data is good, upload to bucket. # Upload raw TMY3 files to bucket.
# Ctrl+C the table output (say 0-3 records of the table above) # Run this...
# Clear out the proc directory
# Group by day
# Declare a base using 'automap_base()' #Use the base class to reflect the database tables
#print(scaled_rip)
#'Durham': 'bced47a0c99c71d0',
# load the query results into a dataframe and pivot on station
# create from dictionary where keys are column names, values are Series
# replace the indexes of the dataframe with the correct header columns by renaming them
# Perform a query to retrieve the data and precipitation scores
# average the predictions, and then round to 0 or 1 # you can also do a weighted average, as long as the weight adds up to 1  # how accurate was the ensemble?
# Run OpenMC
# remove not useful variables
# We see that there are two date/time columns, we'll tell pandas to parse them when loading the full file.
# Merging Twitter data frame and Stock market data frame
# change into datetime format
# Reflect Database into ORM classes
#ls_dataset displays the name, shape, and type of datasets in hdf5 file
# this method displays the predictions on random rows of the holdout set
# Express as a time delta in days using the NumPy function timedelta64
## load isochrone into a geopandas dataframe
#Display shape of df_schools
#Need to put tweet in df with time stamp, sentiment analysis and tweet. 
# Remove rows where 'Delivery block' is empty.
# read matching list
# to calculate the total we only need to get the size (len) for each selection # Underweight
# Drop the unnecessary columns and rearrange columns
#Setting the date time as the index allows time slicing
#Selecting the input file to be read
# Downcast Only # data_file = 'https://alfresco.oceanobservatories.org/alfresco/d/d/workspace/SpacesStore/ef3f532b-7570-43d9-b016-6b58c4429b15/dar24011.asc' # Down and Up Casts
# reset index to date
# How many stations are available in this dataset?
#Join the tables
# Import the csv file with historical neo data and other variables
# We replace NaN values with the next value in the column
# Simulated conversion rate of Pnew uner the null
# Create a GeoPandas GeoDataFrame from Sites DataFrame
# Spark sql will only work with table so register the dataframe access_logs_df as table #rename it as AccessLog as the query given earlier had the table mentioned as such.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned # Use parameter limit = 1 to only return the latest date data
#churns 
# 5. 
#make a copy of dataframe #converting normal values to log values using numpy's log function to remove anamolies
# Make the output Dataframes
# Rebuild network and run simulation using new config parameters 
#In order to do an inner join, there must be a common key in both datasets.  Base the common key off of the school name. #This cell changes the name of the school column so that the inner join can be completed .
# weird but possible
#'Oakland': 'ab2f2fac83aa388d'
# For some reason stats.percentileofscore() throws an error if it's not imported right before being called:
#inspect measurement table
# remove white spaces
#Reading CSV files in the DIR
# Upload the processed files
# Set the x and y limits
#Read csv file into a DataFrame #reviewing dataframe and loaded data
# check Basin variable meta data in file manager file
# create list stores deep cleaned text
# read all data ignore bad lines
# for porability 
# 5. Which Tasker has been hired the most?	 # sample.groupby('tasker_id').size().sort_values(ascending=False) \ #   .reset_index(name='hired')
# This has reduced the dataset by approx. a third:
# logging.getLogger().setLevel(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
# line plot: x-cordinate = index, and y-cordinate = Age
# Returned as a python dictionary# Return 
# C(w) refers to w as a categorical variable.
# `make_pipeline` - credit goes to Harsha
#take a look at the data- this is only first-level broken out json (note the _source column still contains many fields)
# create pySUMMA Plotting Object
# make my acutal outcome column y
# data scraped from http://www.unicode.org/emoji/charts/full-emoji-list.html
# most of the usernames are in 2 words or 3 words.
#Save figure
# 50개의 리스트 중에서 첫 번째 것의 데이터형(type)을 확인하니 # bs4.element.Tag라서 변수내에서 태그 검색이 가능하다
# group your sources and take mean of compound
# drop 'title' and dummy all of it.
# Apply rule of thumbs to top10 
#create a pandas dataframe to store tweets: #display the first 10 elements of the dataframe:
# show that types were conserved even through export
# Print the lat/long
# open file, read it, and tokenize:
# Perform a bsic analysis of variance (ANOVA). # C(x) refers to x as a categorical variable.
#We notice an the max date of 2044-06-01. This date is in the future and seems to be an anamoly.  #Let us check # Bingo!! This is defintely an outlier. It only has one value. Let us drop it
#Creates Dummy from post likes at threshold 500.
# Pandas DataFrame with the Sentiment Analysis results
# Get the tables in the Hawaii Database
#Read tweets approval ratings from csv file
# Statistics on each column
#branch point path length vs radius
#check count of tweets per geography
#split names - in data wrangling
# Critical value at 95% confidence
# Test dataframe
# resample data to weekly
# check Basin variable meta data in file manager file
#Select all the non-null comment fields from the table
# load stop words
# Printing the content of git_log_excerpt.csv
# The ~same simple calculation in Tensorflow
# freq='B' is business days
# dataframe remains # view data descriptions
# I can change the level of the logger.
#huh = soup.find_all('header',class_='gallery_header') #huh
#Create a content_short field for Tf-idf vectorization
# add a new column # plot model error by bid-ask spread # plot model error against strike, many expirations included
#Save figure
# plot new grid
# the names of the columns have become the index # they have been 'pivoted'
#df_series.index = pd.DatetimeIndex(train_frame.index, freq="N")
#'Madison': 'e67427d9b4126602'
#Line plot of log_AAPL
# make predictions
# converting the timestamp column # summarizing the converted timestamp column
#print ("Current date & time " + time.strftime("%c")) #list_of_idxes = [1454, 1852, 2801, 4545, 4759] #list_of_idxes = [3, 4, 56, 62, 63]
#Regression - Forecasting and Predicting
# This will apply the removal of punctuation and stopwords to a string.$ def text_stripper(string):$     '''Takes a string and removes punctuation, capitalisation and stopwords.'''$
#the plot shows that there is almost no correlation between numeric variables
# Next, our dictionary must be converted into a bag-of-words:
# df_2015
#Saving that into a new csv
#type BI et type UT are the same
# print out details of each variable
#print first 10 lines of first shard of train.csv
#Create BeautifulSoup object; parse with 'html' or 'lxml'
# Display dataframe types and usage statistics
#Get the station_id and name for the station has the highest number of observations
# of course ignore unit_sales since missing from test
# Load the data from the query into a dataframe
# We'll hold out 10% of our data for validation and leave 90% for training
#print(data[:5])
# get categorical dataframe
#let's look at temperature across time by max temperature
# Encontrar el 20vo registro de mi DataFrame
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# largest basket:
# Neural net predictions
# get a small_lst for demonstration purposes only
# and that the column labels
# get 8-16 week forecast new patients # keep only date in index, drop time
# ☃☃☃“I like snowmen!”
# 1. Collect data from FSE for AFX_X for the whole year 2017
# Rezeptsammlerin now only has 679 recipes instead of 870:
# now we have negative values, so we need one more transform before we can run MultinomialNB
# Create a new dataframe for the Airport ID (It does so by testing true/false on date equaling 2018-05-31)
# add back metadata for each order # make an additional column with just the year
# predict against the test set # preds = aml.predict(test) # or:
# Split into training and test data: 66% and 33%
# preview the data
# Output File 
# Zürich Kloten station (near Flughafen)
# Creating dummy variables for country column
# remove unwanted fields
# drop extra id column
# Compute and display sample statistics
#url_votes = grouped['net_votes'].agg({'total_votes': lambda x: np.sum(x), 'avg_votes': lambda x: np.sum(x) / len(x)})    
# unigram chunker 
##   Creating data frames directly from CSV
#SAVING TO LOCAL DISK:
# grab the subreddit
# R squared.
#Read Excel file  #https://stackoverflow.com/questions/32591466/python-pandas-how-to-specify-data-types-when-reading-an-excel-file
# Convert the sex column to type 'category' # Convert the smoker column to type 'category' # Print the info of tips
# models trained using gensim implementation of word2vec
#post request to chart_data with chartParams, gets back data
# media de tmin anual
# predict on the same data
# Simulate conversion rates under null hypothesis
#ind_result
# RETRIVE TWIT IN EXTENDED MODE (TO-BE)
# Third option - the cross-section - returns specific values
# Treehouse samples are prefixed with TH (prospective patient) or THR (pediatric research study) # so filter for only these excluding the TCGA and TARGET that are already in our other dataset
# OPTIONALLY, print the keys for each turnstile (in case we need to access it later) # for key, turnstile in turnstiles: #     print(key)
# SAVE A CSV OF THE NEW SPOTIFY TABLE ("song_tracker.csv") WHICH NOW FLAGS SONGS THAT REACH NUMBER ONE IN A REGION
# pandas default datatype given
#print model.words # list of words in dictionary
# Make output directory
# 3.
# checkout cand_id and see if there are duplicates, i.e. if there are differences in how cand_name is formatted
# plots key elements of the universe selected
# Read the Excel files. # The "new" file is the newest, unchanged file from SAP. # The "old" file is the older, modified the previous "new" file.
#  At this point, the user could specify their mean and sigma, #  but we shall use actual values computed from 51-year history:
#join the dataframes together from drugs we used chembl_id vs name 
# YOUR CODE HERE # raise NotImplementedError()
# A:
# Let's see the graph that our regression created. 
# Facilitators
# plot autocorrelation function for first difference of therapist data
# Sort data frame by target and tweets ago #sentiments_df.head()
# determine the order of the AR(p) model w/ partial autocorrelation function, alpha=width of CI
# Sort the data by operator and part.
## Sort each tweet value by its relative timestamp
# What are the most active stations? # List the stations and the counts in descending order.
# Tokenizing your data for use with the model
#analysis_historical['Volitility_90_day'] = analysis_historical.groupby('coin_name')['close_price'].rolling(window=90).std().reset_index(drop=True) #analysis_historical['Volitility_365_day'] = analysis_historical.groupby('coin_name')['close_price'].rolling(window=365).std().reset_index(drop=True)  ## fyi, normally annual volitility is 255 instead of 365 since there are only 255 trading days in a year, but since crypto is all 365 then I leave at 365
# sort from the oldest down to the youngest
sql = """SELECT * FROM $     feed_fetcher_feeditem T1 LIMIT 100$     """$
#output.to_csv("result_modified_97rfcorrect.csv", index=False)
# Use the session to query measurments table and display the first 5 locations
#first cohort
# one time if for converison of list
#checking that we are okay in terms of time sorting
# Read the filtered tweets from the .txt files
#find maximum of Sepal.Length by Species
# view the second isochrone
### 50 příspěvků s nejvíce To se mi líbí.
# get the indices for the rows where the L1 headers are present
# find the relative image url # img_url_rel = image_soup.select_one('figure.lede a img') # img_url_rel
# Predict
# Read the newly created csv back in as "numberOneUnique_df" for later merging with the original "spotify_df"
#Select NDVI for scene after cyclone
# Count terms only (no hashtags, no mentions)
#file_time = str(current_time.time())+'_'+str(current_time.date())
# dict get # returns value if key is in dict, otherwise returns a value of your choice
#Importing the data sets
# one hot encode the component and team columns
# Add two new columns to dataframe
# Utilities
# fit with non-tfidf dataset
# create lookup key in crime dataset
# split the data into training and test
## Now, create a dataframe that only has the subset of crimes with the primary description 'CRIMINAL DAMAGE' # code here:
#Clean the title and date data
#Greensboro': 'a6c257c61f294ec1'
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#pulling artist information
# Run OpenMC!
#Create new dataframes in preparation of feature engineering
# Create linear regression object
### export to chinese_vessels_CLAV.csv file
#create GeoDataFrame for Chicago wards
#festivals['Name']='' #festivals['latitude']='' #festivals['longitude']=''
# Construct the client object # Build the authorization url for your app
# We create time series for data:
# save to hdf
# get one column by attribute, return a Series object
# Creating time stamp format column using the column 'Time stamp'
# generate the Word2Vec model
"""$ Check number of news with empty ‘news_entities’ field per day$ """$
#Return select rows AND columns using loc
#Drop NaN.
# Create df with datetimes range as index
#data
# take a look at the dataset
# Simulating conversion rate times for NULL Hypothesis 
# The above query returns a list of tuples from the measurement 'table' object.  We want to import these tuples into a pandas # dataframe which extracts the values from the tuples for input to the dataframe. # Add column labels to the df.
# Printing the content of git_log_excerpt.csv
# Designed a query to retrieve the last 12 months of temperature observation data (tobs).
# Sort according to EXT, y first, no latter
# check shape
# Make sure there are no duplicate keys
#lastly, let's convert our datetime columns for later use. (We need to tell pandas these are dates)
# create dataframe of matrix  and then inputting the confusion matrix
# get numerical features
# Load the data from the query into a dataframe
# Make five reccommendations to user 1059637 who is the first among the top-10 users in term of number of playcounts # construct set of recommended artists
# all the estimateds_count in the file is >= 3.
# le_data_all.pivot(index='country',columns='year') # examine pivoted data
# 数据统计
# step 1 - create boolean Series # step 2 - do boolean selection
# Count mentions only
# load workspace configuratio from ./aml_config/config.json file
#type(df_concat)
#pd.to_datetime('11/12/2010', format='%m/%d/%Y')
# merged1['Specialty'].isnull()
# drop the identity column from progresql
# Use Pandas to calcualte the summary statistics for the precipitation data
# {0: -7.4305555555555545, 1: -15.097222222222221, 2: -7.263888888888888, 3: -5.097222222222222, 4: 3.402777777777778, 5: 8.069444444444445, 6: 16.569444444444446, 7: 9.736111111111112, 8: -0.7638888888888887, 9: 1.902777777777778, 10: -3.263888888888889, 11: -0.7638888888888887}
#creating a serie from a dicionary #will be created in key sorted order
# Format column AB (column 28) to two decimal places.
# find each of the inconsistent rows in this horrid table, which is now in a new place # for row in table_rows: #     print(row.text)
#ubico por columnas #df['field1']
#Load the query results into a Pandas DataFrame and set the index to the date column.
# cisuabf6
# get the unique number of games that Jimmy did play in
# or  ... s[['c']]
# Conert the numeric columns to integer
#create an extractor object: #create a tweet list as follows:
# Replace each NaN with the value from the previous value along the given axis
# Theresa May indices
# look for seasonal patterns using window=52 - Dr
# get the count of unique locations
# default inner join
# Check for null values for each series
# instantitiate df object | conform to Prophet nomenclature
# write making the worksheet name MSFT
#Change date to datetime
#Remove unnecessary columns
# urban
# open precip nc
# Compute user activity # We are interested in how many playcounts each user has scored.
# your code here
# Here the benchmark is NASDAQ Biotechnology index 
# compute the mean of complaints per quarter... # note this doesn't make sense, but works anyway
# Import and Initialize Sentiment Analyzer
# find historical data for 2012
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# View dataset coordinates
# get head or tail elements
# Specify the input folder and the type of files to load in (glob_string): # The input folder will change depending on which computer/dataset analysing # If analysing data on windows make sure that there is a 'r' before the file name e.g. r'folder/subfolder'
# Tell Bokeh to display plots inside the notebook.
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# preserve
# Convert dictionary to a DataFrame
# Save file to csv
# So pretty much this just shows the annual return of each stock so that -> # you can see which stock has a higher annual rate of return.  # In this case you can see that MSFT is better than all of the others.
#reading the data #combinign the two tables into single table
#50th is 6, 80th is 38, 90th percentile is 183 up_votes, 95th 997 up_votes, 99th is 15170, max 192,674
# Convert text to a Python object using the 'json' package # And now we have a Python list:
# Get down do the acutal data
# YOUR CODE HERE # raise NotImplementedError()
# here is where I filter only for ones (residential tax class)
# How many stations are available in this dataset?
# fit the model
#limit dataframe to only original tweets
#Load CSV
#dropping extra column  #avi_data.drop([avi_data.columns[-1]],axis=1, inplace=True)
# find appropriate contract at the moment of the call
# Save a reference to the stations table as `Station`
#map the dictionary to the new col on index
# Save model.
#find earliest create date
#Removing'userTimezone' which corresponds to 'None'. # Let's also check how many records are we left with now #removed 41 tweets!
# print a sample tree in tuple format
### WCPFC list to start and try to merge IOTC list into it
# combined_df5 merged all uncommon values into a single 'other' var; this is an alternative strategy
#probability of individual with new_page
#785 columns = 1 + 28*28 #reshape into numy array with shape (4199, 1, 28,28)
# remove subjects with more than 50% of missing values
# Convert from reviews to features #X = vectorizer.fit_transform(df.review)
# Use the Base class to reflect the database tables
# Write the file
# Dealing with NaN
# y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag) # y_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)
# View the data type
# use matplotlib to create a bar chart from the dataframe
# We replace all NaN values with 0
# Create a list of lists of the data
# head climate_pred
## remove columns we don't need for model
# first date where nr of coins are c
# run the model giving the output the suffix "rootDistExp"
#Save to a csv file 
#write out the source data onto disk #however we want to write only the records which are duplicates. Better idea to remove the non duplicates.
#read in the group file
# tweet date/times are reported in utc
# tokenize the text
#Merge 2 dataframe
# We drop columns which give us a score for personality type
#assigning meanngful indexes
# sice this took forever, better save the model to disk
# Just my repos
#Merge dates of interest with the Landsat scenes to match dates
# Read the file into a DataFrame: df # Print the head of df
# 3.2.B OUTPUT/ANSWER
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Let's create two Indexes for experimentation
#Which_Years_for_each_DRG.head()
#converting date to datetime format
#Now, build the graph...
# Copy the data of interest - with headers - into clipboard with Ctrl+C # Run this... ## Copy the Excel table to the clipboard first.
# get sum of interceptions by game day
# boolean indexing: with .loc, we can input a boolean series  # as the row index; no need to take the values (in contrast to .iloc)
# Get a list of column names and types # columns
# unigram chunker 
#df2
# accediendo a una columna por el label y obteniendo una serie
# are the observations in train in history ?
#Apply text cleaning function to data frame
# concatenate with the set of resulting columns the intersection of the column names by specifying join='inner'
# summarizing the converted timestamp column
# calculating number of commits # calculating number of authors # printing out the results
#loading the data frame df 
# Pickle the 'data' dictionary using the highest protocol available.
# Create root universe
#Importing rating dataset from Data folder
## Remove Nones and print highest and lowest prices
# create search_weekday column
#  The mean and sigma arguments are the gross descriptors of the GM(2). #  But note here that the sign of the mean here is REVERSED.
# Удаляем строки с пустыми значениями
# Plot in Bar Chart the total number of issues created for every Phase based on thier priorites
# Specify multiple aggregation functions as a list. #
#check the loaded file #Remove unnecessary columns
# Number of tweets by company
#Resetting the index to avoid future issues #Dropping unnecessary issues due to save memory and to avoid issues
#Great. Let's just verify that there are only "complete" permits in this data frame.
# Use Pandas to calcualte the summary statistics for the precipitation data
# differ from ndarray
# using random forest classifier
# Show the node_types file. Note the common column is node_type_id
# Display confusion matrix
# Convert OpenMC's funky ppm to png # Display the materials plot inline
# convert Tip_Amt into Tip percentage
#Like Vs retweets visualization:
# собираем implicit с поддержко CUDA
# Create a scatter plot of likes vs views
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
# I can find out if Katie Hopkins has tweeted more than one time:
# Get genotypes that have associated blood type phenotype
# Of course we can aggregate the other way too
# train a simple logistic regression, and...
# 11. 
# dummying month_bought 
# scrs < 3m / all scrs, since 3-01-2018 # 50% scrs start their scns too early
#df_TempJams['lineString'] = df_TempJams['lineString'].apply(lambda x: loads(x))
#Read tweets approval ratings from csv file
# converting the timestamp column # ts_to_datetime = lambda ts: datetime.datetime.fromtimestamp(int(ts)) # git_log['ts1'] = git_log['timestamp'].apply(ts_to_datetime)
# finding the timezone of the max retweets (cats) #the max is 18857 retweets
# Split data into train and test data
# mean deaths per year:
# load in api key
# read in again, now specify the data column as being the  # index of the resulting DataFrame
# ,test_df=df_test #todo: add a test set
# Use SQLAlchemy automap_base() to reflect your tables into classes # 
# What's the probability that this classification is accurate?
# subset for only amount
# plot fixation durations
#produce statistics for valid/invalid AC
# Create an OpenMOC Geometry from the OpenMC Geometry
#Saving variables
# import Gaussian Naive Bayes
# car break-in  #car=load_data('https://data.sfgov.org/resource/cuks-n6tp') #car.head(5)
# Design a query to retrieve the last 12 months of precipitation data and plot the results
# Score gridsearch logreg model on test data
# KNN model 
#include_WB = ['Norrbottens skärgårds kustvatten'] #lv_workspace.set_data_filter(step=1, subset='B', filter_type='include_list', filter_name='SEA_AREA_NAME', data=include_WB)
# drop the null in is_enabled
# List stopwords
# Saves an image of our chart so that we can view it in a folder
# merge with council data #df7 = pd.merge(df6,df3,how='left',left_on='Date Closed',right_on='MEETING_DATE',suffixes=('_created','_closed'))
#normalize the continuous variables that will impact tripduration
# Sumar una unidad a una columna
# Print all of the classes mapped to the Base by the automap function.
# approximation to avoid a second matrix # multiply the maxarg by factor, sum across and divide by same factor # all matrix elements lower than argmax will contribute very little
# Print the street name
# Create our tokenizer # We will tokenize on character level! # We will NOT remove any characters
# Import the libraries.
# Create a separate data frame that holds the average park attendance for different parks.
# RE.FINDALL
##### index, series pairs
# Movies with the highest RottenTomatoes rating
# Find the station has the highest number of observations
# Example on how to access and edit parameters manually if neccessary
#making python dict
# Not really needed, but nicer plots
# Check whether columns have been created in dataframe
# Generate new feature in our DataFrame for the label
#USvideos['comment_intensity'] = USvideos['comment_count'] / USvideos['views']  #USvideos['thumbs'] = (USvideos['likes'] + USvideos['dislikes']) 
# the finally logistic regression model:
#Joined(train,test)-googletrend for germany (checking if there was any unmatched value in right_t) ##Note: Level of googletrend table for germany is "Year-Week"
# either it opend at high are closed at low
# The number of times the new_page and treatment don't line up. # adding rows where <group> equals 'treatment' and <landing_page> does not equal 'new_page'
#Head shows top 5 rows of ingested data
#よくわからないから、favが多い順に並べる
# show summary statistics for number of claims in utility patent
#Export dataframe to csv
# Name of timestamp to parse on the raw data file
# Create time series for data:
# Check that we don't have any null/nan at this point # Make sure they have identical hugo gene indexes
# Example
# Having some issues saving to pickle, so I'll save to CSV, reupload, and do the downcasting again before I save the pkl #shows.to_csv("ismyshowcancelled_tmp_1.csv")
# filter North American mammals whose genus is "Antilocapra" # query, loop over and print out animals.
#hierarchical Indexing
# subset temp to us
# what is one day from 2014-11-30?
#importing from microsoft excel
# As the values for converted are one of the two {0, 1} mean can be taken # to determine the probability
# Baseline accuracy for this model is percentage of low vs high in the train set
#disconnect to the sql database
# preview the data
# Parse the dimension2 (which contains the customer id and name) and split it into two columns
# imports
#grouping the function by the GICS Sector column using groupby
#read in the member file
# All numeric data is now int64
# We create time series for data:
# Compute the error for every 1000*10*10 generated datapoint. # Then flatten them all out into a list of length 100,000.
# get conversation length
# Setting up dataframe
# # This doesn't give us an accurate base to compare stocks in our portfolio. It's all over the place.
# Save your regex in punct_re
# Slice using just the "outer" index. #
# the maximum age
# Use **statsmodels** to import your regression model. # Instantiate the model, and fit the model using the two columns you created in part b.  # to predict whether or not an individual converts.
# Create dataframe from a dictionary of Pandas Series
# Read the data file and save it to a dataframe called "df".
# Prepare the data
# Fit only to the training data
#Summarize the MeanFlow_cfs data, using default outputs
# Delete the datastore # to delete all datastores in the path, use: # pystore.delete_stores()
#Temperature - fill short gaps < 1hr in all temperature records
# Create pivot table with mean age at death by year
# Save the dataframe to CSV
#happiness_df average by year
# need to fix format issue on 'lastest_consensus_created_date' column.
# sneak peak at data
# simple pandas plot
#Group by the status values, and then describe, and then TRANSPOSE for neater presentation
#how many wells that were good at time of measurement do we predict to fail now?
# To get the total number of unique values for users
#Example 3:
# build the DOM Tree # print the parsed DOM Tree #print lxml.html.tostring(tree)
# boolean indexing: note that with .iloc we *must* take  # the .values # or iris.iloc[iris.iloc[:,0].values>7.5,4]
#CC: 4.2.5: Calculating Daily Mean Speed
# Make country folders
# reading the dataset
# analyze validtation between BallBerry simulation and observation data.
# let's try CPU (20 times slower!)
# counts number of stations
# filter on Suburban # len(Suburban) # Suburban
# exercise (page 161) # extract the data highlighted, *as 2d arrays*
# Convert sentiments to DataFrame
# Add Distinct Users
# Load a single column from table
# Print the external node types
#  ENCOURAGE re-running this cell, like playing with a kaleidoscope: #  Visually see the increased "choppiness" due to GM(2) overlay. #  Local "downtrends" are more likely. 
#'Riverside': '6ba08e404aed471f',
#6  When I want to replace my values with NaN
#获取2014年第3季度的盈利能力数据
#query tables to get count of daily report, all temp data is complete for each record, so the count#query t  #reflects a count of a station giving temp data, prcp data may or may not have been reported on that date
# another way of importing the file
# worst 3 styles, average rating, minimum of x beers drank
# Creating a rate column with the extracted numerical rating  # WHAT AM I DOING?
#word_counts.Tweets.astype(str)
# Arrays for Bar Charts
# Collect the names of tables within the database
## Retrieve all of our original tweet textx, tweet id, user who tweeted, and tweet hashtags. Ignore _id which is  ## MongoDb's id of each record, ie. a 'primary key'.
# - GICS
# Reading data from the rest of csv files.
# Read in our data with more help from the read_csv parameters
# Move this to text as an aside
# Show a sample of the data
# The tags field is another compound field.
# Describe the categorical features
# See how many disease labels overlap
# Create a one-dimensional NumPy array from a range with a specified increment
# Répare une incompatibilité entre scipy 1.0 et statsmodels 0.8.
# Printing the content of git_log_excerpt.csv
# Average daily trading volume
#probably not necessary, but i'm going to drop unblended from Y and then re-add
#Import BeautifulSoup into workspace #Initialize BeautifulSoup object on first comment
# run the model giving the output the suffix "rootDistExp"
#'St. Louis': '0570f015c264cbd9',
# Clean up the merge results
#wikipedia_women_in_medicine = 'https://en.wikipedia.org/wiki/Women_in_medicine' #requests.get(wikipedia_women_in_medicine)
# %load solutions/pipe_results.py
#'Norfolk': 'b004be67b9fd6d8f'
#Data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017
# Pintar la temperatura máx, min, med
#cost for the period
# print(rdc.feature_importances_[113:1000])
# Return Apple stock price
# The two partitions have to be ordered by looking at the extra field in the first line: all A rows need to go before all B rows
# Running and Trainign LDA_3 model on the document term matrix.
# Date limebike started differentiating between bikes & scootscoots
# sort dates in descending order # calculate daily logarithmic return
#here is the data:
# Write data out to an excel file
#So it looks like we can go ahead and delete the duplicates (it's safe)
# Need to convert the created_at to a time stamp
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Transforma datetime para string # Sera utilizado a data em formato de string para aumentar a performance # de busca/insercao de dados do dataFrame usando .at[index, col]
#Design a query to calculate the total number of stations.
# Return the list of movies with the lowest score:
#fmt date for YYYY-MM cohort
#Review summary stats for the data set
# Use SQLAlchemy create_engine to connect to your sqlite database.
# distribution plot of the data
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned # print(r.json())
#List columns being omitted from saved data
# Access local file
#count of converted users with new_page
# Push the sentiment DataFrame to a new CSV file
# Extracting FOX news sentiment from the dataframe
# printing first five rows of the data frame #df.head()  # 5 is the default value. we can ask for any number as shown below
# select an expiration to plot # get the call options # set the strike as the index so pandas plots nicely
# Get the .txt files
# interaktif jupyter
# Create a pandas dataframe as follows: # Display the first 10 elements of the dataframe:
# look at the count of values for the 3 categories.
# fit two standard deviations between -1 and 1
#create a data frame of the actual values on second measurement and our probabilities from the first predictiojn
#'Raleigh': '161d2f18e3a0445a'
# process missing value
# Create a new df copy of df2
# Make predictions on test data using the Transformer.transform() method.
#Wow that is so small!
# explore dict output
#print the r^2 score of the linear regression model to determine the level of accuracy the contains
# fig.savefig('toma.png', dpi=300)
# Check out the actual counts within each wear day bin
# training loss
# Use SQLAlchemy create_engine to connect to your sqlite database. # Create an engine for the hawaii.sqlite database (created in part 2) #
# we can do ?pd.read_csv or just check the  # documentation online since it usually looks nicer ...
# Retrieve latest full mars images
#read in the schedule file
# float으로 변경
# Extract specific values - 2
# X will be a pandas dataframe of all columns except meantempm, feutures
#sample query to verify key works
# set up logging for gensim training
# integer features scaling
# verify the type now is date # in pandas, this is actually a Timestamp
# Create a linearly spaced array of 5 values from 0 to 100
# Use Pandas to calcualte the summary statistics for the precipitation data
# show the first 5 rows of data frame
# Put the data into HDFS - adjust path or filename as needed
# export
# create the prefetch column
# Display of first 10 elements from dataframe:
# Make columns with country sums
# coordinates to a 2 dimension array # check dimensions
# Predicting val_dl to verify that it works
# Convert stock ids & names lists to Pandas DF 
# selecting just the vendor id column
# check if any values are present
# selecting data by col and inx:
# run the model giving the output the suffix "distributedTopmodel_docker_develop" and get "results_distributedTopmodel" object
# let's check
#let's plot the hour of the post against how many retweets to see the optimal time of tweeting to reach others #it looks like, from this sample, the best time to tweet if you want to reach more people is from 11pm to 1am
# check Initial condition data in file manager file
# open Chrome
# Write tweets data frame into a CSV file for later use #dfTweets.to_csv("tweets.csv", encoding='utf-8', index=False) # Read tweets from CSV file into a data frame
# check index
# Design a query to calculate the total number of stations.
# get number of elements
# integridad db
# We display the correlation between columns
# list(.zscan_iter(key))
# Confusion Matrix
# accediendo a varias filas por posición entera
#We have derived important features and now we will dropped the columns to reduce memory usage
# http://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/01_the_machine_learning_landscape.ipynb #x_train = np.float32(x_train) #x_train = x_train.reshape((len(x_train),1))
# Exporting as csv
# saving model
# plot Series obj
#     print('Deleting the {} table.'.format(table_name)) #     connection.delete_table(table_name)
# Define number of Training samples (70 %), Validation (15%) and Testsamples (15%)
# Change our units to 000s for funsies
# quick and dirty investigation for 'RegDateOriginal' and 'RegDate'
# get station count,
#datatype of the columns
# change type of int variables (from float to int, possible because no more nans)
# Load total market cap data
#now drop extra columns
# Sort by co_occurence (max co_occurences in first row of df) # Keep only top10 co-occurence
#Create TFIDF vectors for modeling
# Create a list from 0 to 40 with each step being 0.1 higher than the last
# Look for any remaining missing values per column
# print(X_train.shape)
# Print data from nasa_url to make sure there is data 
# merge with council data #df7 = pd.merge(df6,df3,how='left',left_on='Date Closed',right_on='MEETING_DATE',suffixes=('_created','_closed'))
## create price changes for certain time periods in advance
# Calculate the date 1 year ago from today
# 获取数据 # 获取索引
# future = m.make_future_dataframe(periods=52*3, freq='w') # future_temp = np.random.uniform(df.TempC.min(), df.TempC.max(), size=future.shape) # future['TempC'] = future_temp
# Create Dataframe from Dictionary
#same result 0.995
# \. 으로해야 .으로 인식하기 때문에 # 가격과 주소만 취하기 위해 분리한다.
#Convert the date column into a datetime
#Get rid of 2018, since its not a complete year
# Group data by company to normalize it accordingly.
# initialize authorization instance using credentials
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# this period can be used as index in data frame # index which has period is called priod index
# Gives us general information about the porfolio
# extract lonlat grid # plot
# 2. Convert the returned JSON object into a Python dictionary.
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# display first 10 elements of the dataframe
#ignore_index means the indices will not be reset after each append
# Save our dataframe as a CSV
# indicators = inds_ts  # 为空表示更新所有存在的indicator
#  利用上面定義的函式來取得回傳值 #  如果直接plot是不會有感覺的，因為預設plot的類型為line
#Click here and press Shift+Enter
# palette # sns.set_palette("cubehelix")
# df_2004
# Collect the names of tables within the database
# specify outer join
# First, import the relevant modules
# Setup Tweepy API Authentication
# Similar a un array de 1d, sólo que con índices
# from pyspark.sql import crosstab # from pyspark.sql.functions import * # flight.columns
#Set style
# Get a list of column names and types for Stations Table # columns
# We get descriptive statistics on a single column of our DataFrame
#output
# Create a list of filenames, this will be used to loop over each of the different years netCDF files
# you can define period like starting year is 2011 and 10 quaters after it
#Read the Prometheus JSON BZip data
#Creating more sensible and consumable features => time since promotion
# https://www.reddit.com/r/Bitcoin/comments/
# Hacemos un display del dataframe:
# if user doesn't have executable file or executable file don't work on your local computer, use run_option ='docker_develop' #results_simpleResistance, out_file1 = S.execute(run_suffix="simpleResistance_hs", run_option = 'docker_develop')
# Find the div that will allow you to retrieve mars weather # Print the results to verify it is correct
# insert a test post:
# Visit URL
#Make the graphs prettier (code in tutorial is for an older version) #Turn off pretty print while we're at it
### Step 22: Look deeper into the data for non-weekdays that look like non-commute days ## We see the a pattern of mostly holidays
#Check via customer sample
# change the data type of publicationDate column
# Aggregate for a DataFrame. #
# axis=1 apply function to each row
#Next we will read the data in into an array that we call tweets.
# merge with main df
# Get sentiment for Loblaws tweet #Read loblaws tweets csv file
# ddf.to_csv('new_ddf.csv', index=False)
# rimuove tutte le interruzioni (spero)
# Joining two data frames
# 1 quarter changes regression: full sample
# Visualise the data for the 100'th day in 2018
# first build an empty model, no training
# Set index
## the reason we subset [0,1] value is that np.corrcoef returns correlation matrix between variables
# group by order_num, then sum up number of tubes per order # lets peek at the result
#Removing one negative value in registred via
#Glance at utc_offset
#renaming second column 
# add results to our data frame
# specify a column as index
# Create second-level index for df based on begining and end or data range
# Correlation coefficient for each field
# ...and it does not matter whether indexing is position-based  # (iloc) or label-based (loc, see below for a more detailed expo)
#Simpan data ke CSV
# Read Files_osha
# Sorting is necessary after parallel execution.
# Remove the missing values
# Save the data, overwriting the old 2018 file
# show last 2 rows of data frame
# Read the data
# Delete newline characters
#Houston': '1c69a67ad480e1b1'
# Baseline score:
#7.(Optional) What was the median trading volume during this year.  #(Note: you may need to implement your own function for calculating the median.)
#Now I need to index the dataframes by their datetime columns
# use BIC to confirm best number of AR components # plot information criteria for different orders
#session query
# used the requests.json() method to convert the JSON object into a Python dictionary # I printed the column_names here for reference
# creating ad df
# there are 63,378 tweets from these influencers
# Calcular rendimientos diarios y graficarlos
#Example1:
# Create a Word / Index dictionary, mapping each vocabulary word to a cluster number
# even tho aldaar is below average it has the most retweets as outlayers 
# find historical data for 2008
# new crossed columns are added in model.py # see first 20 line on codes starting from "INPUT_COLUMNS
#split the dataset 
# Finally, export this file 
#ACF and PACF plots:
# Suburban cities ride totals
# SpaCy pipeline
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
#Merging the second train file with train dataframe in the dictionary
# for test set, we just fill y values with zeros (they won't be used anyway)
#print(highlight(json.dumps(jevent_scores, indent=4, sort_keys=True), JsonLexer(), TerminalTrueColorFormatter()))  # remove the comment from the beginning of the line above to see the full results # Convert to a data frame and show all flagged life events for this client
#Chandler
#pprint(api_json['messages'])
# the cards column contains the cards of each set in json format, so each set of cards can be # converted from a json object into a DataFrame
# 2. Sort free_data by country, educ, and then by age in decending order, modifying the original Data Frame.
# Set the X and y
# we can make structs in numpy via compound data types
# Query all tobs values # Convert tuples into list
# Apply np.cumsum() to each column 
# Let's see a historical view of the closing price
# Train model
# Extract projection information # metadata['projection'] = refl['Metadata']['Coordinate_System']['Proj4'].value
# top beers
# FutureWarning is OK: # https://stackoverflow.com/questions/40659212/futurewarning-elementwise-comparison-failed-returning-scalar-but-in-the-futur
# save model to a file and restore # with open('house_regressor.pkl', 'rb') as f: #    restore = pickle.load(f)
# creates a Series with values (representing temperatures) # for each date in the index
#Review all column names from dataframe once the drop has occured
# Crate an instance of the d2ix post process class: # Post process for a specific scenario: model, scen, version
# instead of fitting a model, let us predict probabilities
# Import the GEM-PRO class
#Load the voltage profiles
# Let's fit& score with Logistic Regression
# write to xlsx
#Convert the returned JSON object into a Python dictionary.
#los df que tienen la info del postulante (sin relacion con el aviso)
# let remove duplicates #checking the presence of Mary again #lets shuffle the data set
# Extract bad band windows
# caution
# find the non-numeric feature columns
# Use the forest's predict method on the test data
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Read tweets from csv file
# remove na values # remove duplicate index values
# Create a vector of random integers, then reshape into a 4x4 array. #
# Get a list of column names and types # columns
# Retrieve the last date entry in the data table
#Write to CSV
# Keep relevant columns only (23 columns of 49)
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# We create a Pandas DataFrame by passing it a dictionary of Pandas Series # We display the DataFrame
# скачиваем патч
# create a column with the result of the analysis: # display the updated dataframe with the new column:
# Create DataFrame
# teacher_info
# Group df by source to get average polarity scores
#Want to check table names -- returns the same as above
# .logical_negation() flips 0 to 1 (Trues => False)
# problem 6 solved.
# check Basin variable meta data in file manager file
# remove URLs and twitter handles
# failing to comprehend this migrating table location # for row in table_rows: #     print(row.text)
# Display data for min and max age
# In single line
################# # Start here    # #################
# Store the API key as a string - according to PEP8, constants are always named in all upper case
## Saving our data # save our pca classifier # save our crosstab_transformed
#Save df to csv
#replace all Nans with 0
#Summarize, using our own percentiles
# creating an explicit/distinct copy of the features
# Requires GenePattern Notebook: pip install genepattern-notebook # Username and password removed for security reasons.
#ADD CODE for using cleanNewUserArtistDF and print
# highest temperature recorded
# largest change in any one day (based on High and Low price)?
#'Tampa': 'dc62519fda13b4ec'
display(HTML('''Account with max following:<br><a href='https://github.com/{}' target="_blank">{}</a>'''.format(max_fwing.login, name)))
#Export all features in a file to check correlations
# Check if the word 'fell' exists in the vocabulary
# check no more missing values
# 分類器のリスト表示
# check to be sure formats match
#test = raw_large_grid_df.query("subject in ['VP1', 'VP3']")
# convert it to pandas data frame
# looks like a bunch of empty strings when a location isn't found, so that's fine.
# The case of "1) variable (time, hru or gru)" #fig = result.get_figure() #fig.savefig('/media/sf_pysumma/pptrate.png')
# import and retrieve portuguese stop words # stop words are not considered as token as usually they carry no meaning (!)
# most missing values in Specialty are now filled
# load credentials
# Drop duplicates from table of latest inspections
# To find  pnew  -  pold  for your simulated values from part (e) and (f).
## convert the variable to numerical values with LabelEncoder() funstion
### Use Sklearn TfidfVectorizer to convert words into vectors
##   Creating and working with Temp Tables
#Construct time features
# plot histogram with KDE curve
#split the strings using the separator "," 
#import stats models to check the R^2 value of the model
# Calculate the sample variance.
## Test code on a reasonably small DF
# find the relative image url
# RT率が高いものを選ぶ
# Creating a 'index' so that plotting is easier
# What are the most active stations?
# Load the necessary data for the module
# change the name for the convenience
# Create time series for the data:
# cost per day
# save data to CSV
# get_data() is a function within Engine that makes a sql call to an API.
# join the output of the previos grouping action to the input data set  # the goal is to optain an additional column which will contain count of publications per author
# Find customerID 
# A:
# The number of different types of vote we are dealing with.
#*--------Merge Command to merge Studies and sponsors--------------------* #studies_c=studies_b.merge(countries,on='nct_id')
# Pick a single tweet to analyze
#df_concat_2["message_likes_dummy"] = pd.get_dummies(df_concat_2.message_likes_rel)
# Backward filling # To do in-place updating # store_items.fillna(method='backfill', axis=0, inplace=True)
# 7. Bar plot visualizing the _overall_ sentiments
# Source Names
#Check duplicated rows by contrator_id
# same data as before
# This is a comment. Anything after the # symbol will not be interpreted as code.
# 9. Print summary statistics for each column for those with an education greater than or equal to 5, grouped by age_cat.
#Target city = Charlotte, NC
# Read the previously-saved Excel file. # List worksheets within the workbook.
# i need to kill 2 days with no data
# set index as datetimeindex
# Strip local time to include date only #itemTable["Date"] = itemTable["Date"].map(parse_full_date) # Categorize energy labels
# creo columnas para agrupar
#Save figure
# Location outside US, ignore
# Prep I-TASSER model folders
# Identify incomplete rows
# Getting some preliminary descriptive statistics for the columns in the df
# put into pandas table
# an attribute is changed within the string-like object
# Save the dataframe in a csv
#p['xx_1'] = p["xx"].shift(1) 
# We replace NaN values with the previous value in the row
# Create BeautifulSoup object; parse with 'html.parser'
# dummying coming_next_reason column
# dataframe is like a powerfull spreadsheet # We print the type of items to see that it is a dictionary
# current.head() # print(data_current) # data_current
#this will take a while
#probability of conversion in cntrol group
# note the index added as a primary key # this is required to make certain calculations -- it will be added automatically
# Using the apply() function, compute the sum and mean for each column in scores.
# the first entry in this Dataframe's shape gives the number of cards in the set, and the second is the number of card attributes (name, mana cost, type, etc)
# create P attribute  # create Plot attribute with open_netcdf method
# default is freq='B' for bdate_range
# Sample five random ratings
#cols = ['sentiment','id','date','query_string','user','text'] #df = pd.read_csv("F:/web mining/webmining project/crawler/annotation_data/Samsung_with date_Merged_final.",header=None) # above line will be different depending on where you saved your data, and your file name
# Transmission 2040 [GWh], late sprint
# also we can specify the intersection of rows
# Display the row's columns and data in dictionary format (first row)
# remove zeros
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# population or sample size of treatment group
# count by event
# Manipulation methods: #Apply
# Density of data # % of seconds with trade
# View the precipitation table
# set SUMMA executable file
# Combine ET for each rootDistExp # add label 
# export # ca_de_xml.to_csv(export_path, index=False)
# convert s1 from US/Eastern to US/Pacific
# headers of 'User-agent' is required if you don't # want a 429 error as it limits your returns.
# check if the DB name exists     cursor.execute('''CREATE TABLE IF NOT EXISTS fib ($                             calculated_value INTEGER)''')$
#Add a query to a dataframe #View data
"""$ Number of unique timepoints$ """$
#get station count, has been checked with measurement station count
# находим центроиды - это среднее значение всех точек в кластере
#total count of individuals
#df1.loc['2017']   'the label [2017] is not in the [index]' #df1.loc['year'==2017]        KeyError: False #df2017.head()
# model.wv.syn0 consists of a feature vector for each work # with a min word count of 30, a vocab of 6,793 words as created # shape of wv.syn0 should be 6793, 200
# Extract title text
# Create RSV only dataframe
#Display each column's data type
#exportamos a un csv
#Design a query to retrieve the last 12 months of temperature observation data (tobs). #Filter by the station with the highest number of observations.
#'Montgomery': '7f061ded71fdc974',
# View the types of data you can download usign wiski SOS
# Scale back
# predic y 
# df.apply(arg) will apply the function arg to each column in df, and return a DataFrame with the result # Recall that lambda x is an anonymous function accepting parameter x; in this case, x will be a pandas Series object
# Add labels to the x and y axes
# Your code here # df = 
# favorite table # for row in table_rows: #     print(row.text)
# specify resume path.
# drop NaN values for purposes of analysis 
# Merge speeches and metadata
#split zipcode columns into two columns by '-'
# just trying different slices of the data
# Loading new datasets # List for storing the group stage games
### unhide the strings in jupyter notebook #new_df.clean_text[new_df['clean_text']==997055707471003653]
#'New Orleans': 'dd3b100831dd1763'
# define the length of period #print 'Accumulated number of tweets: ' + str(accumulated_num) #print 'Frequency of tweets every perid: ' + str(frequency)
# Save dataframe to csv 
#Dot produt to get weights #The user profile
# There is one user_id repeated in df2 # Locating duplicate id
# .dt.strftime("%m-%d-%Y") # .dt.strftime("%m-%d-%Y")
#Add list of nodes and edges
# Count tweets per time zone for the top 10 time zones
# Manually train a logistic regression classifier with the training set (supervised)
# Transmission 2050 [GWh], late sprint
##Plot histrogram of dates of birth
# changing the name of a series:
#called effectiveness function 
#group by blocks #drop extra columns
# Print paragraph texts
# URL of page to be scraped
# Verify accuracy score through cross-validation of X and y data
#aggdf = loctweetdf[['lat','lng','text','lga']].groupby(['lat','lng', 'lga']).agg('count').reset_index() #output
#count of converted users in control group
# 'itos': 'int-to-string'
# this way, numpy can get around the "fixed" type of objects
# Set the index to the date column
# Let's find the 'random' buttom
#BuyingData = BData.parse('Set4-User Buying Behavior')
#using value counts in descending order. 
# we'll read the user file into a Pandas dataframe.
#url for mars image
# correlation between score and num_comments
# Group and drop columns no longer meaningful
# normalizing values
# see an example for a given author
# Create the boxplot # Display the plot
# search for vendor number by name
# combine all product_type
# tokenizer = lambda key_phrases: key_phrases - должен взять список и отдать список  #(вместо взять сплошной текст и отдать список слов) # preprocessor=lambda x: x - должен взять список и отдать список
# msft calls expiring on 2015-01-05
# We change the row label from store 3 to last store # we display the modified DataFrame
# Convert notebook to HTML # Upload the HTML # Upload the Actual Notebook
# Instantiate a Materials object # Export to "materials.xml"
# rename "value" field to "mmol/L" # convert mmol/L to mg/dL and create a new field # view the cgm mg/dL data
# create an HDFS directory for this assignment
# Obtain the hour of the day and the day of the week for each observation and add them as features to the dataframe
# Merge csvs (3)
#Using the statisics library#Using t 
#Twitter connector
##### now apply to rows
#Double-click on the file that is saved at the filepath shown below -  #This will launch your interactive pyldavis chart in a browser window!!
# Read Measurements file with pandas and inspect the data
# freq='M' uses last day of month by default
query = ''' SELECT tweets_info.created_at, tweets_info.id, tweets_info.user_id, tweets_users.name$             FROM tweets_info INNER JOIN tweets_users $             ON (tweets_info.user_id = tweets_users.id); '''$
# Retrieve data from nasa # Create BeautifulSoup object; parse with 'html.parser'
# to copy the next value in the row, we use bfill
#2. Running averages - First counting events/flags of interest in bwd and fwd diretions
# Use the forest's predict method on the test data
# Delete records that duplicate brand / outdoor unit / indoor unit model.
# Save df_TempJams to csv
# the file is a two-column csv: date, tweet
#timeconsuming
# President Obama's 8th state of the union address
# some numerical values do not show as such, clean missing records
# We display the first 10 elements of the dataframe:
# create df copy for use w/ predictions # tail.tail(2)
# re-assign variable value
#change dtype to int64 #weird hack from H2o notebook
# since this took forever, better save the model to disk
# % of pos/neu/neg
# Total number of stations
# Filter outliers in Input Power
#merge in the data
#mistake here to just use priors_reordered for reorder =1 which makes all data reorder =1 only
# Calculate median number of comments 
# Pivot airquality_melt: airquality_pivot # Print the head of airquality_pivot
##Most Liked Posts 
#festivals.rename(columns = header)
# Convert all oz_stop ids to string format
#wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'
# encodedlist is train_X
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#**Which Tasker has been shown the most?**
# accediendo a una columna por índice y obteniendo una serie
#Create a folder called network.  #Within this folder, we create a folder'recurrent_network' where the nodes and edges in our example will be saved
# Most of dogs' names are missing # There are some mispelled names in the name column, like 'a', 'the', 'an'
#Reassign name to remove the unrecognized symbol '?';
# df_2003
#filled in nulls for LinkedAccountId, may need to do entire file later, not sure why payer doesn't need it
# set appoinmtemnt duration column to hours
# save to excel file in worksheet sheet1
# take a look at your results
# Take a peak of the data
# Run seasonal decomposition and plot
# or: # sales_df.groupby(['Product_Id', 'Country']).sum().loc[(5,), 'Quantity']
# iterrows() 함수를 사용하는 경우가 좀 더 좋다. # 받는 인자로 인덱스와 나머지 row를 받는다는 것에 주의하자
#converting size in KB to size in MB
# get only the index of the Series
#types of data
#Create predictions and evaluate
#index objects are immutable
# generate LDA model
# check y
# URL of NASA
# Downcast binary target column 
# popCon = popCon.set_index('contact')
#Save figure
# Apply lambda function to each column.  This example yilelds scalar value from function. # As seen below, NaN is ignored in calculation. 
# Time series comparing Likes vs Retweets 
# 3.2.C OUTPUT/ANSWER #bottom 10
#import dataset
# Checking the columns of our submission sample
# Step 4: Investigate the data # This shows that there are now only 12 unique times b/c the AM and PM are no longer there
#Dictionary of Outliers for Time
#Overall statistics on the entire dataset
# Solo Analizar el periodo 201701
# concatenate dataframes with different columns
# Make columns with country average
#get rid of some unnecessary columns for the purpose of our investigation
#for prophet to work, columns should be in teh format ds and y
#Data from SERI report, table 3 #Difference between the SERI report and our values .
#Plot using Pandas
# Analyse dataset
# For inputting the password
# create our kmeans model & predict clusters
#sum() #Returns the sum of the values for the requested axis. By default, axis is index (axis=0).
# read data and convert to date-time
### read in WCPFC csv file created by using Scrapy crawler spider (for Chinese vessels only)
# this yields the row labels
#This is what the table is going to look like.  #There are nine categories. Above is a short description of each column. 
# plot the fixations as a heatmap # TODO annotation how many fixations from how many pictures are used for each eyetracker
#Works with Pix4D capture october 2017
# считаем количество вхождений
#Most recent date by descending.first
"""Start Logging"""$ # Some of the code below takes a long time to run, the logging helps knwo that it's still working
# plot de predictions.grade si predictions.prediction 
# Calculate the date 1 year ago from today # one year ago is 2016-08-24
# replace vendor id with an equivalent hash function
# extract data from tweets that match search terms 
# check option and selected method of (27) choice of thermal conductivity representation for soil in Decision file
# 1 quarter changes regression: later half of the sample
# Display shape of Projects, Participants and the merged DataFrame fp7 # The number of rows in the merged DataFrame FP7 must be the same as the # number of rows in participants
# shift the signal, so trade at end of month and avoid lookahead bias
#computing the f1 score
# split str of content into a list of words and insert as new column
# block_geoids_2010 = [row[0] for row in query_psql("SELECT geoid2010 FROM sf1_2010_block_p001 order by blockidx2010")]
# Training the model 
#Let's try using groupby to see what different language users were doing
#count vectorizer has extracted 38058 words out of the corpus.
# forward fill
# Drop new column
# Converting your dataframe to a matrix # Displaying rows 1-10 and all columns. Note: rows indices start at 0.
#Create custom stop word list
# Import the data from the station table into a dataframe
# `hawaii.sqlite` database file created in database_engineering steps
#challenge 2 #Create a new column that stores the date and time as a single pandas datetime object
# adding prefix VIP_ to column names
# OAuth process, using the keys and tokens
# user=userid # userid=2605
# Remove rows for games that occur in 1999 season and in 2017 season
# find historical data for 2013
# list of workspace ids # workspace_ids
# Mapping id -> slug # Mapping id -> slug (will be used for readability)
# get multiple sections with the term fees # use SpaCy to determine what type of fees
#dfall.tweet_time = dfall.tweet_time.dt.date
# Which features play the biggest role in predicting campaign success:
#look at the support vectors
# which simply looks up the dunder on the class i.e.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Load package # Instantiate and fit the classifier model
# Sort by various ways
#Looks like all are active, but just in case you want to load in a new dataset lets be sure we get it right... #The status column is now redundant
#  set the index to the date column.
# total number of datapoints
#The semantic vector for ‘fell’ looks like this #model['fell']
# Load the mazda datasets
# Remove rows with missing values in column 'driver_id'. The order was not accepted. there was no trip
# Add a "minute of day" column as a plotting index
# Check for null values to confirm drop
#High on We and collective phrases
# Examine visits here
# drop rows with missing specialty
#'Mesa': '44d207663001f00b'
# check the confusion matrix # it seems that many rates are predicted as 5 point, which generates the most of errors.
# feature transformation
# Convert df to html
# create target vector(s) | lookforward window
# Example 2:
# check option and selected method of (11) choice of groundwater parameterization in Decision file
# How many learners had more than 100 interactions (>100) ?
#how many rows do we have with nan in a given column?
# cinema meta-data
## Print top 10 tweets and notice Sentiment field added at the end of each record
# Use Pandas to calcualte the summary statistics for the precipitation data
#creating test and train dependent and independent variables #Split the data into test and train (30-70: random sampling) #will be using the scaled dataset to split 
# save the file to the output
# Query to retrieve the last 12 months of Temperature Data # Use the start date and end date calculated before.
## Python will convert \n to os.linesep
#appending data of 1st computer to 2nd computer 
# We remove the watches and shoes columns # we display the modified DataFrame
# Create engine using the `hawaii.sqlite` database file created in database_engineering steps
# Change some of the parameters to see how the model fits
# Get coordinates of FSRQs
# Create an array of 1s
# integridad del DF
# drop rows with missing value in specialty column
# Apply the percent function # Seeing what DummyDataframe look like
#creating url for year 2017 by changing the start+_date and end_date parameter of query string. #Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 
# Convert completed time from UTC to EST
# read in the data from a file # display the table of data
#== Selective overwite 
# Sort the dataframe by date
# First, import the relevant modules
# A:
#data[pd.Timestamp('2012-01-01 09:00'):pd.Timestamp('2012-01-01 19:00')],这样也可以达到一样的效果 #data[('2012-01-01 09:00'):('2012-01-01 19:00')]，这样也可以达到一样的效果
# Euclidean Distance Caculator
# group by SN and find average of each group
# grab the last date entry in the data table
# extracts newest and oldest tweet dates from dataframe for display in local timezone
# mean deaths per day:
# type(transaction_dates['Created at'][0])
# Printing the content of git_log_excerpt.csv
#construct and display bbc sentiments dataframe
# Apply smoothing function to each line, identifying transactions between Coinbase and GDAX
# plot a scatter plot of all errors > 1.0e-4
#Show the first 5 rows of that array
# zählen der emoji vorkommen
# Show all characters belonging to the amount
# beginning & end of the data collection in Pacific time
#Saves the final dataframe to a CSV to be compiled in another notebook
# load the endometrium vs. uterus tumor data
#Summarize, **using our own percentiles**
# Load the President Trump's tweets
# Loading in the pandas module # Reading in the log file # Printing out the first 5 rows
# посмотрим на значения в этом столбце
# favorite table # for row in table_rows: #     print(row.text)
# calculating number of commits # calculating number of authors # printing out the results
# Concatinate each Data Frames 
#Import modules
# and mode
# i band
# We replace NaN values with the previous value in the column
# class look up and as it is a frikin descriptor # then call it
# If you don't plan to train the model any further, calling init_sims will make the model much more memory-efficient.
# This creates a list of BeautifulSoup objects.
# Lenghts along time:
#Birmingham
# id any missing specialties
# read csv directly from Yahoo! Finance from a URL
# double check that the bus data doesn't have any weird spaces in it. # Any extra spaces in this string will hinder operations of retrieval by key
# Create a list from 0 to 40 with each step being 0.1 higher than the last
# Array with Interval index of the weeks
# confirm that this is a new array
# Extract the scale factor
# Load model.
#Check via customer sample
# It appears that this stop comes up twice in oz's file. However, stop 7270 is not a real stop when looked up online.
#df.loc[df.age < 0]
# Count hashtags only
# generate historgrams of the # returns of our portfolio
# check Forcing list data in file manager file
# Accuracy: 48.86% for doc similarity # Accuracy: 39.39% for skills
# Import CSV file
# try without extra squre bracket
#res3.text
# Fit model (will take some time)
# Saving to csv to open again with chunks
#Use the REST API for a static search #Our example finds recent tweets using the hashtag #datascience #tweet_list = api.search(q='#%23datascience') #%23 is used to specify '#'
# Make columns for seasons and terms
# remove the unwanted columns # what does our data look like
#firebase.patch("Exhibitions/-LFlR_PhbP2eWNCGPZeu",new_data)
#Tweepy Cursor handles pagination .. 
# find each of the inconsistent rows in this horrid table, which is now in a new place AGAIN # for row in table_rows: #     print(row.text)
### Station Analysis #Design a query to calculate the total number of stations. #I'm doing a group_by just in case there are duplicate stations
#save preprocessed df #psy_prepro = psy_prepro.set_index('subjectkey', verify_integrity=True)
#creating 2 arrays: features and response #features will have all independent variables #response has the target variable
# check if any values are present
#Fetching of data from yahoo finace
# Convert JSON object into Python dict
# probability of recieving new page # probability of recieving old page
q = """SELECT education, occupation, relationship FROM adultData """$
# The header keywords are stored as a dict # table.meta
# TASK E ANSWER CHECK
#_source contains plenty of other fields we'd like to examine on their own- so can do the same thing to expand that json, this time into index
#determine which factors have the gratest impact on trip duration by creating a coefficient matrix and calling the #tripduration column and put it in descending order
#Use Pandas to plot an area plot (`stacked=False`) for the daily normals.
# it has a name
#Initialize the server 
# Export to "tallies.xml"
# joined_hist.info()
# Read in csv file as a Pandas DataFrame
#women_in_medicine_save = 'wikipedia_women_in_medicine.html'
# filter on term for 2017,2018,2019
# Doing some basic descriptives
# создаем словарь # https://blog.mafr.de/2012/04/15/scikit-learn-feature-extractio/
#get station count, has been checked with measurement station count#get sta 
#Normalize the json into a DataFrame format
# simple word algebra example:
##### ignore
## Loading the data created
# Naives Bayes with Kfolds on our Test Data
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#Converting date time to numeric for calculation 
# prototype only: save the light curve to disk
# Drop created and time fetched columns - age takes their place 
# index has freq None
# Create index
# and what is the minimum life expectancy for each year
# This analysis shows that there are more nulls in last four years
# how does our result look?  Should get something like this.
# Add the figure to a layout # Show the plot. This will create a warning as the figure contains no data.
# check option and selected method of (16) type of lower boundary condition for soil hydrology in Decision file
# RTs AND SENTIMENT
# The base dataframe's end.
#Third Data Set:
#  Let's document the numerical values of (mean, sigma) for the record:
# inspect the first 10 rows # the printSchema() method tells you the data type of each column
# Import the pandas package, then use the "read_csv" function to read # the labeled training data
# Sort the morning traffic and observe a cutoff point to classify the extreme value as erroneous
#Alternate method for reading sql with pandas
# use BIC to confirm best number of AR components # plot information criteria for different orders
# Let's see how many people were on each deck
# mean_encoding_test(val, train,'DOW',"any_spot" )  # tbd #mean_encoding_test(val, train,'hour',"any_spot" ) # tbd # mean_encoding_test(val, train,'Block',"Real.Spots" ) # tbd
# Model is very overfit since the test score is much lower than the train score 
# Check whether the zipcode and city difference are correct.
# of unique users = len
# this is the default, raise when unparseable -- convert cell type to Code to see error
# get features names
#Testing tvec
# Use pandas to write the comma-separated output file
# let's visualize our boxplot
# Train test split 
#Limpiamos la base de datos para que no se repliquen los datos o interfieran con los de otras páginas
## make an array of midnight datetimes for slicing dataframe by days
#you can get this from https://www.kaggle.com/egrinstein/20-years-of-games #contains reviews about witcher 3
# Get list of messages
# important: please modify the path as your local spark location 
# Print records in test.csv
# Optional: save model for future comparison
# to force type of columns, use the dtypes parameter # following forces the column to be float64
#works! returns shit for set add #all plans
# Save the DataFrame as a csv
#select only that day
# 6. What was the average daily trading volume during this year?
#try using dendrogram by parsing texts into words
#searching for href
# Grabbing the ticker close from the end of last year
# Making sure we have the right number of predictions
# Delete the item from the current version
# Perform prediction on the Kaggle test set previously imported
# convert date columns to datetime 
# scatter plot: x-cordinate = index, and y-cordinate = Age
#df.dropna(inplace=True)
# Can also visulize entitie detection given a ticket
# Train our classifier by giving it 1796 scanned digits!
# create a column with the result of the analysis: # We display the updated dataframe with the new column:
# replace vendor id with an equivalent hash function
# sort values by date
# Filter outliers in hspf
#Have a look at a few random records
# Time format x-axis to 12-o'clock time
# Create a staging bucket required to write staging files # When creating a model from local files
# Declare the constants. # Type I error, alpha level, significance level. #bootstrap_number_samples = 10000
# let's generate a series with a hierarchical index: 
# Display first five lines of combined DataFrame
# Simulate conversion rates under null hypothesis
#tweetdf.to_csv("tweets_w_lga.csv")
# Create the 'type' column
# to stop TensorBoard
#only want tech groups #our category column is now redundant
# this_tweet.head()
# pipeline / fit / score for lr & tvec
#获取2014年第3季度的营运能力数据
# Convert JSON tree to a Python dict #data = json.loads(geocode_result) # print to stderr so we can verfiy that the tree is correct.
# The case of "1) variable (time, hru or gru)"
# We can even slice easily using the first MultiIndex (year in our case) # health_data_row.loc[1]  # doesn't work
# get dr number of unique providers for each dataframe
#nltk.download('tagsets')
# столбец salary заполнен на четверть
'''Remove duplicates'''$
# how significant z score is
# 6. What was the average daily trading volume during this year?
#Print number of deaths per year sorted by deaths
# find historical data for 2003
# print percentages
#Convert datetime to an actual datetime object
# 깔끔하게 분리
# CHECKING TOTALS # bands.sum(axis=0).sum()
# Double Checking if all of the correct rows were removed - this should come out to be 0
# lets find the number of rows we have now. We want to  # have a reasonable number to rows to train our deep learning model
# How many stations are available in this dataset?
#### note the resulting index
# ANSWER TASK G CODE FINAL
# Get the keys to understand the data structure and how data were nested.
#Show the first 4 rows. 
##### Your Code Here #### ###########################
# Initializing an LSI transformation
# same reason as above to save the data...
# your code here
# aprovechamos y salvamos este nuevo set # y recargamos la informacion nuevamente
#setting up date
# Find unique users in the dataset.
#hyperparam =[0.01, 0.015, 0.020, 0.025]
# RE.COMPILE
# take a look at your results
#dfFull['OverallQualNorm'] = dfFull.OverallQual/dfFull.OverallQual.max()
# Visualizing the distributions
# I want to use woba as a feature in this model. let's make sure these values are close to reality # looks good https://baseballsavant.mlb.com/expected_statistics
# custom grids for every degree # vectors # mesh grid array
#Read in data from source 
#Create a new dataframe from the Flow and Confidence columns
# Use Pandas Plotting with Matplotlib to plot the data # Rotate the xticks for the dates
# The eldery person in our dataset is 92
# and average temperature most active station
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# distribution plot of temp_celcius
# Word frequency for terms only (no hashtags, no mentions)
# Information extraction from dataloader and model # vcf_to_region will generate a variant-centered regions when presented a VCF record.
# Demonstrating that the rows with NaN have been removed and the last row as seen is the row just above the last NaN value # Ths would work is we did df.dropna in the previous step, but thats not what we are going to do now
# Get marketdata from database...
# Например, вот столько общех друзей у меня с юзером 10592581
# Ensure to Convert df_TempIrregular.detectionTimeStamp to datetime
#count of words in messages (posts)    
### Create the necessary dummy variables for landing page
# Package the request, send the request and catch the response: r
# n_old = old page count
# our main index is the 'Mail' column
#Trading volume occurs at [6] #Using the statistics library
#info_final.drop('idpostulante', axis = 1, inplace = True)
#Complete the inner join of the school and students dataframes to reutrn a combined (merged) dataframe. #Drop the School ID and Student ID columns to reduce the columns in the merged dataframe for readability
# Task 1
# Calculate difference
txt = """$ The sparks from the seemingly ordinary July 23 incident on a road near Redding set off one of the most destructive wildfires in state history and killed eight people, including three fire personnel. It also destroyed more than 1,000 homes and consumed 229,651 acres, according to California fire officials said.$ """$
# mapping from integer feature name to original token string
# shape of coarse grid
# aprovechamos y salvamos este nuevo set # y recargamos la informacion nuevamente
# Convert Y from 0/1/2 to one-hot format
# drow rows
# Print out single tweet
#Which Tasker has been shown the least?
### Create the necessary dummy variables
# read, but skip rows 0,2 and 3
# A scratch cell used to pull up details of a table column as needed  #voters.E1_110816.value_counts(dropna=False)
##### sort by label on index
# Reassign data frame without null value rows in name column
# Get the file name
#Autoscaling can increase the number of instances.
# If you prefer sweets... setting "axis=1" tells the function to operate on columns. # "axis=0" is rows.
#读取存储的txt文本文件
# roughly 7.5% of users are classified as active under the current definition
"""$ Count token frequency and print$ """$
# set the index to date
# Hint: Use the same : notation, but use the state names listed above # Your code here:
#Complete a groupby to determine the average math score of each school by grade level.
# 2. Convert the returned JSON object into a Python dictionary
#Larger time period 
# Export to csv
# get mongodb params (using configparser)
#show result
#sn.distplot(a.A.flatten()[:],norm_hist= True,bins=60 ) #sn.kdeplot(a.A.flatten()[:]) #sn.distplot(a.A.flatten()[:], hist=False);
# read data in from JSON
# Location outside US, ignore
# reset the index to the date
# sort grid_pr fires by lat/lon to get projection right
# save the tweets to disk
'''Performing Polynomial Features on the Pending Ratio Column'''$
# Without effect modification # drop1(adj.glm,test="Chisq")
# Create logit_countries object # Fit
#preprocess 
# This shows, that two columns are the same, so take only one column
# Creaate a new column that is product of both dimensions # print a few rows & columns
# Loading in the pandas module. # Reading in the log file # Printing out the first 5 rows
# We create a DataFrame  and provide the row index # We display the DataFrame
# Since we do not need the 'count' lets drop it from the dataframe
# What are the different species of Iris and how many rows are associated with each
# Crear la nueva columna usando funciones de la librería datetime
# how many unique authors do we have?
# Misc utility functions
# read back binary arrow file from disk
# The highest rated dog is called Atticus, it has a rating of 177.6
# Join Dataframes
# fill values from backward
# Instantiate an empty Tallies object
# write your code here
# frequency count
# Y si quiero actualizar el valor de una columna en base a su índice en el DataFrame
# calculate all Wednesdays between 2014-06-01 # and 2014-08-31
sql_query = """$ SHOW COLUMNS from sakila.address;$ """$
# API Information
# A:
# Counting the no. commits per year # Listing the first rows
# 圖形 API 測試工具 > 開發人員工具 > network > 找到 preview = {name: "柯文哲", id: "136845026417486", __debug__: {}}的資源 # 從preview切換至 headers，可以看到get方法能得到的網址 # 關於network可參考
# X will be a pandas dataframe of all columns except meantempm, feutures # y will be a pandas series of the meantempm, target
#abc = abc.reset_index(level=[0,1]) #abc = abc.reset_index()
# add features to test # check wether the join operations where right
#Using cursor to connect and query the newly created table
# adding prefix BOUGHT_ to column names
# Export to "geometry.xml"
# Setting up plotting in Jupyter notebooks # plot the data
# Sure enough, the read_csv method sucessfully converted our data to use a DatetimeIndex. # Unlike with plotting numpy arrays, where we had to provide a formatter for diplaying dates, # pandas is smart enough to do it for us
# text cleaning imports
#Fort Worth': '42e46bc3663a4b5f'
# set code, name, release date and # of cards for the 5 latest expansions
# Read the records from test data
# Create a scatter plot, assuming data are in time order.
# Print the value_counts for 'State'
# Further increase confidence of accuracy score through cross-validation of X and y data
# Return descriptive statistics on each column of the DF
# Save twitter data to CSV file
#yearago_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first() - dt.timedelta(days=7)
#  matplotlib可用風格
# Plot all "Categories" with an occurrence value of under 500.
# sparse_categorical_crossentropy is like categorical crossentropy but without converting targets to one hot
# The beginning of the first 100 comment items...
# create weekday vs weekend column for icu_intime 
## I need to create some files to use in the globing exercises
# Edges files
# Show GMT to EDT
# accediendo a varias filas por etiqueta
#Pull data into a data frame
#get highest interday change
# We can also apply functions to each column or row of a DataFrame
# Create the Dataframe for storing the SQL query results for last 12 months of preceipitation data
### Sample of clean descriptions
# movie link
# Must set filter to subset for len(boolean) to work when calling get_filtered_data, if no filters are set, boolean is None
# Create an engine using the 'hawaii.sqlite' database
# mean US precipitation grid time series # mean plot
# Pickle the 'data' dictionary using the highest protocol available.
# Open Fermi 3FGL from the repo # Alternatively, one can grab it from the server. #table = Table.read("http://fermi.gsfc.nasa.gov/ssc/data/access/lat/4yr_catalog/gll_psc_v16.fit")
#Drop empty columns(axis=1 for columns and axis=0 for rows;  #how='all' to drop if all values are nan and how='any' to drop if any value is nan)
################################################## # Load transaction  ##################################################
# Load dataset
# Call the 'LangStats' class from DOTCE.
# plot number of comments distribution
# set timeseries data
# setting index to date
# A:
##Create an engine to connect to sqlite database created in previous step
# The mean squared error
# Create data frame #Force column order
# get rid of the attributes column # peek at the new columns
# Re-fit the model with fewer features
# Create the Dataframe for storing the SQL query results for last 12 months of preceipitation data
### Fit Your Linear Model And Obtain the Results # Instantiate the model # Fit the model
# Read the last 25 Million records from training data
# read the parameters of the character n-gram extraction module
# We create a pandas dataframe as follows:# We cr  # We display the first 10 elements of the dataframe:
# Apply new function to the column entries
##### transpose the dataframe
# check Basin Parameter info data in file manager file
# Naive Bayes
#churns
#check if projectId in schedules is also in projects and budgets
#total=out3
# are all values equal to 6?
# teacher_behavior # 机构那一列全部改成一个统一的字符串就行：例如：community
# Make columns with country sums
#select all rows for a specific column
#Import the NetworkBuilder module from modelingsdk. This module allows us to create a network object #to which we can add nodes and edges respectively.  #Create network object
# let's find the median and mode
# New dataframe for urban data
#dfg = dfg.reset_index(drop=True)
# For finding the first paragraph tag
# Use DataFrame.plot() and sub in the new dataframe containing predictions made by the model you used  # to generate a graphical comparison of the predicted prices. # YOUR CODE HERE
# we display the description of the features
#Remove unnecessary columns
# Groupby the new year and month columns
# remove retweets (highly probable repetition on dataset) # TODO: remove really short tweets!!
# number of tweets
# ['table_name', 'trip', 'stay_days', 'start_date', 'company', 'dep_time_local', 'stop_info', 'duration'] # groupby(flight2.trip, flight2.stay_days, flight.dep_time_group).agg(func.mean('price')).show() # flight6.groupBy(col('trip'), col('stay_days'), col('lead_time')).agg(F.mean('price_will_drop_num')).show()
# Create sentiments data frame
# Group by # Group data to get different types of information
# Amount invested in BTC in GDAX
# percentage of ratings in lower half (of possible scores, not lower half of distribution)
# concat df and coming_next_reason
# merge with main df
# Cleaning up our column names using string functions:
# pd.DataFrame(cursor.fetchall(), columns=['user_id','Tweet Content','Retweets'])
# Dataframe for alpha values (transparency)
# Checking the columns data-types:
# using numpy.random.choice(a, size=None, replace=True, p=None) we take output 
# Stations with the highest number of observations
#Get the top 10 timezones of the tweet authors
# From the docs: "Max id returns results with an ID less than (older than) or equal to the # specified ID. 
# The "doesnt_match" function will try to deduce which word in a set is most dissimilar from the others:
# Decision Tree Model
#viz_1=sb.countplot(x=studies_b.enrollment, data=studies_b)
# for better logging information
# Create new Data Frame
# Create an array of zeros of length n
#1.Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017  #(keep in mind that the date format is YYYY-MM-DD)
# Create the 'str_split' column
# Repeating words like hurrrryyyyyy
# Get Guido's text: guido_text # Print Guido's text to the shell
#sort by CNN in order to create different color scatter plot
# Using the station id from the previous query, calculate the lowest temperature recorded, 
# data munging # split language and region
# Save references to each table
# Merge and simplify the mini lines to the rest of the lines one final time. sql = """CREATE TABLE mthrice as $ SELECT ST_Simplify((ST_Dump(ST_LineMerge(ST_Union(linestring)))).geom,.00001) as linestring FROM mtwice;"""$
# do the two isochrones intersect?
# append the heading row to price_data and set column names
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# saving model
# make a calculation directly # or create a function and apply it accross rows of data # save data to file
# determine the order of the AR(p) model w/ partial autocorrelation function, alpha=width of CI
# Graficamos
# Importar el modulo data del paquete pandas_datareader. La comunidad lo importa con el nombre de web
# logging.basicConfig(filename=log_file_name, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
#!pip install pymysql
# get the general model # predict with the model
# Merge rows to summing along rows of multidex
# foreach through all tweets pulled # printing the text stored inside the tweet object
#We cannot use rating, review and source hmm
# dataframe with just the most retweeted tweet for each week # dataframe with the number of tweets for each # dataframe with averages
#print(cv2)
# looking at tags of some randomly chosen queries # notice that most cities after 'TO' are incorrectly tagged as VB
# Gradient Treee Boosting
# plot KDE 
# Tag parts of speech (PoS)
# A:
# Initialize environment # Initialize experiment
# find historical data for 2004
# Establish a connection to your GIS.
# bulk convert all non-numeric columns to equivalent hash values # X['ITEM_NO'] = X['ITEM_NO'].apply(lambda x: hash(x))
# Plot all "Categories" and their occurrence count.
# Count mentions only
# accediendo a una fila por etiqueta y obteniendo un dataframe
#in descending order
# fit the classifier to the data
# Create a 'tweets' DataFrame for each row of the 'news organizations' DataFrame # Would it be ok if the funtion get_df_for_user_tweets required a second argument, which was defined here?
#4 convert all characters to lower case
# 34 ST-PENN STA -> 34 ST-HERALD SQ @ 11:30 am = 10 min # Print total crepe sales lost due to transit
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#del df2['building_use']
# Your code here
#exploring the categories csv file
#ua is a dataframe containing all the united airline tweets
# A:
# Doctors Decomposition
# Display a list of all the models
# We extract the mean of lenghts:
def lookup_project_git(org, project):$     """Returns the project github for a specific project. Assumes project is git hosted"""$
# groupby by defaut creates a (hierarchical) index:
# Width # Color # Round values
# 1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD). # https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-05-30&end_date=2018-05-30&api_key=sKXGJG7ybc76fKLMSfxc
# Extracting NYT news sentiment from the dataframe
#Show last row
# Apply the NoData value  # Convert raw reflectance into type float (was integer, see above)
# don't convert anything and return the original input when unparseable
#joined_test.reset_index(inplace=True)
# View unique causes of death
# checked, all tw ID has 18 digits
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# 混同行列
#total3.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2016_1/objs_miss_fall2016.csv',index=False)
# Create an engine for the hawaii.sqlite database
#Reformat variables
# the version iterates over the data
# reflect an existing database into a new model # reflect the tables
# Reduce words to their root form
#Save the table directly to a file.
# Perform a linear regression.
#Change the index to be values in the datetime column and display them
# Aggregate for a Series. #
# Alternative way to get the date only 
# load the model from disk
# Final clean csv.  Other csv's below are for separating categories
# Export to csv
#Create year values from the ActivityStartDate column
# we'll filter out the non-represented classes, sort them, and plot it!
# look for '&amp;'
#  Compute "geovolatility" per our definition.
# how many columns and rows do we have in data?
# check the simulation start and finish times
# Initialize the timing of computation
# sentiment_df[sentiment_df['name'] == "CBS"]
# находим центроиды - это среднее значение всех точек в кластере
# Imported chromedriver, had to redownload the latest chromedriver and insert the file into project file path # because the current chrome browser wouldnt work with the earlier version of chromedriver.
## Sort by Date
# The mean squared error
# First, create new columns with the year of each date columns
# pie chart showing percent of total drivers by city type
# Initialize a 2-group MGXS Library for OpenMOC
# perform count on dataframe composed only of non-crime criteria # but append that information to original dataset
# Sort the dataframe by date
# show overall statistics of the dataframe
# Let's combine the original comments with the model predictions so we can more easily make sense of the results.
#gives you the class average
# We Create a DataFrame that only has Bob's data # We display bob_shopping_cart
# Find the div that will allow you to retrieve the latest mars images
# How many stations are available in this dataset?
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# Convert the pd df to HTML table and clean up. 
# Use start_date and end_date parameters to retrun data for 2017 only 
# without resample
# normalize original tweeters for 
#convert dictionary to a dataframe.  #print(precip_data_df.count())
#Compute 1000 lbs/day from MGD and TotalN
# 15. Create a Data Frame, called demographics, using only the columns sex, age, and educ from the free1 Data Frame.  # Also create a Data Frame called scores, using only the columns v1, v2, v3, v4, v5, v6 from the free1 Data Frame
# summarizing the corrected timestamp column
# Load the President Trump's tweets
# Create a new dataframe that retains only songs that are in the number one position (for each region)
# non unique userid
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# A:
# data munging # satisfaction
# getting rid of columns and rows: drop
#sns.lmplot(data=aa,x='weekofyear',y='message',aspect=2,scatter=True,fit_reg=False,markers='x')
cur.execute("""UPDATE empvw_20$                 SET salary = 1000""")$
# drop rows with missing value in specialty column
# Simulate conversion rates under null hypothesis
#compare summaries for Matthew 1
# Are the ids unique?
# Total dates
# Retrieving the last 12 months of precipitation data.
# drop NaNs created from lagged feature generation
# display unique values with counts for street_address
# Random Forest
# Test function
##Create temperature bins. #bins=[]
# split data into training set and a temporary set using sklearn.model_selection.traing_test_split
# building a dataframe with columns 'features' and 'relevance scores' # Since, the relevance score is compute over the embedding vector, we aggregate it by computing 'mean' # over the embedding to get scalar coefficient for the features
#Make the df look nice.  
# Access the first row by positional index:
#.get_group((2015,871))
# validate inputs to the RNN
# Import studies text file
# Our start period was March 2016 and our period frequency is months.
# Sampling the Recommendations provide using the item:desc dictionary we had created earlier
# plot
# sort from the youngest to the oldest
# merge with council data #df7 = pd.merge(df6,df3,how='left',left_on='Date Closed',right_on='MEETING_DATE',suffixes=('_created','_closed'))
# pipeline / fit / score for lr & hvec
# Colors # Define x-axis and xlabels
#how many wells that were good at time of measurement do we predict will fail within one year from today?
# the connection/session
# Print scores from various models
# to compare score between train / test set.  
# Netstat
# for i,r in tweets.query("text.str.contains('chile',False)").iterrows(): #     print(r.text) #     print('')
#This puts data in order the frames list  #All nucleated data is shown then partial nuc then outside-out...as continues inside brackets
# Set environment variable + check # export instagram_client_secret=91664a8e599e42d2a6a824de6ea456ec # echo $instagram_client_secret
# in order to completely get rid of the original index # and have a fresh one, e.g. in a subsample:
#since we expect ~ 57% precision (true positive rate) how many wells do we expect to ACTUALLY fail this year?
# Save references to each table
# Making df_test_index
# Total tobs
# Are the results only positive? # ANSWER: NO
#tweak the 'minimum probability' argument to expand/contract the list
# Reflect Database into ORM classes
#Use Inspector to find the table names 
# Let's bring class in too:
# Fitting a model for Country with US as baseline
#The return here is the training set of features, testing set of features, #training set of labels, and testing set of labels
# Coin DB # COIN_DB.to_csv('coin_list.csv')
# assemble features
# visualize the importance of features
#verify if connection is established #part 1- create session to connect to DB engine
# Save file to csv
# sanity check out environment is working
#####list the datatypes 
# calculating or setting the year with the most commits to Linux
# show you the table after clean work 
# favorite table # for row in table_rows: #     print(row.text)
# Get feature names and class labels
# Reformatting date/time column
# missing value check
# convert text to datetime object
# Export to csv file as backup file 
# get column names
# What class passenger was on each deck?
# create the training, test sets # we don't need the date columns anymore
#face detection attributes
# Add data to Pandas DataFrame
#merge datasets
#  numpy.setdiff1d() returns the sorted, unique values in ar1 that are not in ar2. #genco ids post-GEN from cameras paired post-genesisco which do not correspond to post-genesisco shopify orders
# kfold cross validation
# CREATED A NEW COLUMN WITH THE RESULT OF ANALYSIS:
# Let's find the highest weighted words
# Print a tweet
#Test to make sure connection is working.
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# The above query returns a list of tuples from the measurement 'table' object.  We want to import these tuples into a pandas # dataframe which extracts the values from the tuples for input to the dataframe. # Add column labels to the df.
#change the case of the text in tweets to consistently search for key terms
# a date without time can be represented # by creating a date using a datetime object
# This gives a list BS ResultSets for all articles. #h2_content = [item.find_all('h2') for item in article_divs]
# Change this to 'jobs_raw.tsv' for subsequent readings # data-ca-180423.tsv
# How many stations are available in this dataset?
# Iterate over all cells and cross section types
# tokenize words 
# check shape
# Checking if quandl is working
# All we are doing is importing libraries that have already been created by the community. Note! That we # have defined np here. np could have been defined under any name. Industry standard is np. Same with plt.
# Copy data frame and apply function
# A dataframe is an RDD of rows plus information on the schema. # performing **collect()* on either the RDD or the DataFrame gives the same result.
#df.loc[20]
# Create a df from list of tweets  # Each row is a separate tweet
#Shuffle the dataset for using  the dataset  #shuffled = scratch #shuffled = shuffled.sample(frac=1).reset_index(drop=True)
# create new column 'prosecution_period' as difference between grant date and filing date # convert timedelta to integer number of days # show summary statistics for prosecution period
# Identify game state # Call it a win for away if away has same or higher win percentage
# { "nameLast" : "Williams", "nameFirst" : "Ted" }
#we need to add another feature and lable
#for prophet to work, columns should be in the format ds and y
# Earliest Date
# tweet_id is int type instead of object type. # Capitalize the first letter of p1,p2,p3 columns to make it consistent # Remove underscores '_' between words in p1,p2,p3 columns 
#Reading all necessary CSV files from Scrapy in the DIR
# read sets into a DataFrame; index is chosen as the orient so the set names are the indices
# later on the first row will be an issue; 1.0 isn't a valid number
# Display confusion matrix
#we create the trainig and testing data from the data we get
#splinter
# load the df_events file into a dataframe
# 重新载入一下测试集训练集 # idpath = "data/most_id.csv"
# Shuffle the training and validation data
# Display the graph for performance comparison
### Create the necessary dummy variables for landing page
# read in the file from matlab # read in the file from our edftocsv
# .table() gives you the frequency count of each unique level
# adding prefix RATE_ to column names
#test the model using lm.predict and the x_test set of data
# common observations in history and train ?
#New dataframe #They might be values that might still be sharing dates and department ID, #re-run similar loop to the one above with new created dataframe to make sure that that no values repeat.
# View the polarities
# process the 'object'type data
# Define random seed for reproducability # Define sample size # Generate sample with replacement
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# resample to minute intervals
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# скачиваем код пакета
# are there any values less than zero?
# read the data and tell pandas the date column should be a date # in the resulting DataFrame
#true == churned, false == active #bug- will return user who 'canceled' and 'retrialed' same day which is latest in scn timeline
#== Transpose (like numpy)
# first, let's check the 5 styles that have the most checkins
#Checking the values of the df
# checking the length of dataframe with number of entries # using len() and unique() to count the number of of unique rows
# 追加したrate (RT / fav)が入っているか確認する
############# Lest's Collect  US cities information ######################
# create Series from dictionaries
# import modules:
# We need to save y values as trained too even though they are not scaled to avoid confusion with previous split
# Create logit_countries object # Fit
# your code here
# calculate weeks between apply and submit
# Random forest score:
# Specify a "cell" domain type for the cross section tally filters # Specify the cell domains over which to compute multi-group cross sections
#Saving the output to SampleRatings.csv in an Output folder
# game_winners
SQL = """$     SELECT category_id FROM film_category WHERE film_id = 2;$ """$
# Be a scatter plot of sentiments of the last **100** tweets sent out by each news organization, ranging from -1.0 to 1.0, where a score of 0 expresses a neutral sentiment, -1 the most negative sentiment possible, and +1 the most positive sentiment possible. # Each plot point will reflect the _compound_ sentiment of a tweet. # Sort each plot point by its relative timestamp.
# Display name and importance
# Mean of the two probabilities
# cleaning the vote column into two columns 'vote' and the 'ballot_type' used
## Initially, think of this as similar to a pivot in excel - but more flexible
# summary results
# copy Snapshot data
# Save DF to csv file
#resolve entity names to deduplicate
#overallYearRemodAdd = pd.get_dummies(dfFull.YearRemodAdd)
#Store original primary sensor data for later comparrison #Identify if primary sensor is passive or aspirated
#Create a barplot for sentiment analysis and approval
# Note: text is already a clean-cleaned description stored in series
# printing the most likely IOB tags for each POS tag # extract the list of pos tags # for each tag, assign the most likely IOB label
# Create a box plot.
# does not recover exactly due to insufficient floating point precision
# Create datetime Series #  number of created items per month over the timeline of the data
# the randomly generated data from the normal distribution with a mean of 1 # should have a mean that's almost equal to 0, hence no error occurs
# Run openmc in plotting mode
# save the file to the output
#convert json to dict with .json()
#Mars News #print(soup.find_all('a', class_='content_title'))
# Most active stations
# Compute cross sections on a nuclide-by-nuclide basis
# lag two days
#fraq_fund_volume_m.plot() #plt.show()
# search vs. booking
# dummy all the subreddits
# Retrieve weekday and weekday name
#take a peek at playlist variable
# Create number purchases per product feature
# evaluate predictions
# get the top level key for the nested dictionary containing max. daily change
#This will give the total number of words in the vocabolary created from this dataset
bulk_data = [(row['url'],row['title'],row['author'],row['tstamp']) for row in res]$ cur.executemany("""INSERT INTO coindesk VALUES (%s, %s, %s, %s)""", bulk_data)$
# df_search_cate_dummies.iloc/[1]['user_id']
#TODO
#Now let's get the genres of every movie in our original dataframe #And drop the unnecessary information
# removed string word comment
# .index.values,'created_at']='2010-11-11 03:59:09'
# Get a list of column names and types
# Caption is having issues in TF-IDF  # it is bc of the numbers...
# 删除列
#res4.text
# invoices creation distributed date
#hierarchical Indexing
# creating bands csv
# Name
# First drop the maxtempm and mintempm from the dataframe # X will be a pandas dataframe of all columns except meantempm # y will be a pandas series of the meantempm
# use average accuracy as an estimate of out-of-sample accuracy
# set c to the minimum
# Set operator as the index.
# Display min & max wavelengths
# view vegetation for June 6th 2016
#working with fakes
# Now combine them, dropping the original compound column that we are expanding.
# sorting values
#helper function
# calculating number of commits # calculating number of authors # printing out the results
# Print the first 5 external nodes
# models trained using gensim implementation of word2vec
# Convert sentiments to DataFrame
# Get data
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
#if '409' in str(d): #   print(properties)
# The index of our data frame is the first column (how the data is organized)
# your code here
# correlation matrix
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# drop tweets which startswith 'RT' --> get only original tweets, without Retweets
#the largest change between any two days (based on Closing Price)?
# What are the most active stations? # List the stations and the counts in descending order. #tobs temperature observations
#let us find out the significance of z-score
# For perspective, guessing the training mean return in the val set results in the following MAE:
# the number of features has been reduced significantly, now only 86 features
"""$ Check number of news per day$ """$
# %load ../solutions/lobsters_dropped.py #stories['github_username'].isna().sum()
# Query for most recent date in Measures # Get string value of recent date from tuple return by query # Get string value of 1 year prior to recent date
# population is a string because of the commas. humm
# A:
#make a copy of dataframe #converting normal values to log values using numpy's log function to remove anamolies
#URL's of pages to be scraped
## Request page ## See the html for the whole page # https://www.purdueexponent.org/
# Average polarity by News Source # View Polarities
# Sort the dataframe by date
# Open/create a file to append data
# Imports the methods needed to abstract classes into tables # Allow us to declare column types
#Plot sites with an area > 25 sq mi
# Pandas series are mutable, i.e. we can change the elements of the series after it's been created
# Veamos el tamaño del dataset completo
# creating dataframe of matrix  and adding the confusion matrix
#We can have a look at the different labels which were created in the VCF
#dealing with json and wrangling
#Trading volume occurs at [6] #Using the statistics library
# Non-numeric Columns # bulk convert all non-numeric columns to equivalent hash values
#Store the final contents into a DataFrame:
#As we can say that rating 1 ,2,3,4,5 are skewed towards right now we will se about outliers using box plots #as we can say there are outliers so text length as may not be as a best feature for predicting
#Assign records collected before 1980 a status of "Before FL" #Assign records collection after 1984 a status of "After FL" #Show a random sample of the dataframe
#get frequesncies of sms #get size of groups
# Perform a query to retrieve the data and precipitation scores
## display the highest-traffic stations for this week (all entries+ all exits)
# the accuracy hasn't changed at all. # then, we are going to decide the best combination of the model
#'Sacramento': 'b71fac2ee9792cbe'
# store the first paragraph
# Inside gamers
# Writing to .csv file
#Printing the schema of the dataframe
# In the next code lines we will define some variables, what are they types?
#print(tweet.created_at,tweet.text)
#Adding a new column to the dataframes that contain the polarity value.
# the most popular words which might contain useful information in the reviews
# SORT AND THEN DROP non numric values
# prediction on test set
# run this cell
# group by Symbol
# This here is what we want!
# List the stations and the counts in descending order.
# calculate mean on compound scores
# Now mash them together into a DataFrame
#get highest within-day change
#can put two datasets back together, and remove "_source" column (which was expanded into expanded_data)
# Clean the dataframe using clean_dataset() function
# convert crfa_f to numeric value
# another example from Tesla with more data points
#join elo table with main table
# Gather benford's distribution percentages
# Create a new dataframe for the Airport ID (It does so by testing true/false on date equaling 2018-05-31)
# Here are the topics that Gensim found ** BASED ON THE TRAIN DATA**
#How many words have more than one translation?
# Facebook
#Specify filename to save to #Save edge parameters #Ext_input.save_edge_types(filename=input_edge_types_file, opt_columns=['weight', 'delay', 'nsyns', 'params_file'])
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# загружаем данные
# Second option - the IndexSlice object
# 查看Requests试用了什么编码
#model = load_model('FC_weights')
# drop low because it is really common #df7.loc[df7['avg_health_index Created'] == 'low', 'avg_health_index Created'] = np.nan
# We can view all of the classes that automap found #Base.classes.keys()
# Author: Warren Weckesser
# Visualize results
#Try indexing the other way -- still only 950 matches
#plt.xlabel('') #plt.ylabel('')
# 3. 
# score on test set
# Count the total number of tweets
# Delete the collection
# Save references to the table
q = """SELECT avg(hours_per_week) Average FROM adultData where sex='Male' and workclass='Private'"""$
#Lincoln': '3af2a75dbeb10500'
# review existing columns | points on yield curve
# Print a sample of it:
## repeated user_id
# convert KOSDAQ ids & names lists to Pandas DF 
#file = open(today + '-' + hour_minute + ' ' + site_name + ' 1 title_page.txt','w')
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Get rid of none values for the timezone
#import Twitter data
# Transmission 2020 [GWh], late sprint
# Set the index to 'time' 
#### we have already seen iloc(); here is another example
# set SUMMA executable file
#Put all data into dataframe sentiments_df
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# Distribution of tweets per year
# view the first five lines of data/msft.csv # !type ..\..\data\msft.csv     # on windows
# We extract the mean of lengths:
##### need this "magic" statement to display plots inside jupyter lab
#dates_list = [datetime.strptime(date, '%Y-%m-%d').date() for date in month] #eth['close'] = eth['close'].astype('float64') 
#array with 23 dataframes (one for each week) with the videos, tags and other information
#Put all bus names in lower case
#Joined(train,test)-googletrend for states(checking if there was any unmatched value in right_t) ##Note: Level of googletrend table for states is "State-Year-Week"
# Taking home team to be the one we are interested in 
# Take a look at the words in the vocabulary
# check if dataset is imbalanced
# get max probability column name, this is final prediction for test set
# Use catplot() to combine a countplot() and a FaceGrid()
# How many usrs in df_log
# Identify incomplete rows
# are all values less than 10?
# first, value counts can show us in text form what we're looking at
#Write out data to csv
# creating purchase csv
#Repeat customers
# MultinomialNB with same three features above
# Convert categorical variable into dummy/indicator variables
# Referencing the adj_close dataframe from above
## Get the No of Followers and retweets 
# check out state abbreviations
#The new contractor_bus_name is correctly created.
# Create engine using the `hawaii.sqlite` database file created in database_engineering steps
# Create logger
# # Create a DataFrame containing the average compound score of each media sources's tweeets #df2.head()
#Show entire description column
# You also can use the index to display any given time range.
# scatter_matrix
# We replace NaN values with the next value in the row
# Using the station id from the previous query, calculate the lowest temperature recorded, 
# Or it could be access by the position of the column (as usual starting from 0)
#pulling in data from synapse table (df_table) needed to merge with validation data google sheet
# create a trial parameter object to check root distribution exponents
# df.groupby([df.created_at.dt.month,'product_version']).count()['Id'].reset_index(1)
# stops_heatmap.save("heatmap.html") # stops_heatmap.render()
# 3.Calculate what the lowest opening prices were for the stock in this period
#== Series by passing list: Index is auto-created ==
#Show rows 100 thru 105
# Calculate basic statistics for y by group within x.
# Create a 1-group structure # Create a new MGXS Library on the coarse 1-group structure
#Convert the returned JSON object into a Python dictionary.
#print head of type
#                                   .size())
# check shape of empty grid
# add local python functions # add the 'src' directory as one where we can import modules
# find indiviual probabilty recievd new_page
# Delete rows where 'Reason for rejection' is X1 or X8.
# Load the weights from disk for our model object
# read the 2017 data to see the original data structure
# plot autocorrelation function for therapist data
# Download the data and remove the header
#  - 申万一级 H股
# Create the grid widget
# Open <country>_users.csv and print last 5 rows
#initial data 
# There is missing data again, so we drop them as well:
#Apply this mask to create a view of just records from Jan thru Sept
# different than find_one
# change data type into readable "datatime" format.  # datetime64[s] shows date/time data by seconds. 
#use_data = use_data[(use_data['win_differential'] <= 0.2) | (use_data['win_differential'] >= 0.9)]
# years since joinging
# Show the nodes file
# responses
# Look at some 1 word ticket
# get the trip_duration column using trip_end_date - trip_start_date
# Generate version number for built
#cisuabn14
# pickle data
# Cretae the DateTime column from the TimeDelta column that we have in the energy generation data
#Add a new columns that calculats the money per student ratio.
#Create a box and whiskers plot of our MeanFlow_cms values, broken by status
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
# We display the total amount of money spent in salaries each year #We display the average salary per year
# Let's display what is contained in the node_types file.
# Notebook customizations
# show the oversampled dataset. Now, we have balanced sample.
# kill line breaks # coerce license date col to datetime and sort descending
##### Can referencce column name without quotes
# Volume    5923 non-null float64,  not 8885
# Creating a Series is easy - just pass in a list of values
# Adding ab_page column #The inv column is unnecessary and can be dropped
# To check that indeed 500 tweets were examined, count them
#using the count method to get the number of elements.
# save to new csv
# Convert columns to numeric values
#inspect station data.
# also useful: isin
# How would we do the same, but groupby both primary and sedondary description?
# check Basin Parameter info data in file manager file
# exist Net_Value_item_level = 0, delete them
#the count for new and old
# This means that I can use series-methods like .head():
#shape of the data
# Let's train popularity model
# Assign the stations class to a variable called `stations`
# Import census tool data pull
# Cargamos hoja de calculo en un dataframe
# dropping values that match conditions in the query function
#save data to a csv file
# check shape
# Create number purchases per user feature
# get the current date
# lets find the most retweeted tweet
##### ignore
# Use the Base class to reflect the database tables\n",
# Find and click the full image button
# INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS
#Takes like 60 seconds
# Load the tallies from the statepoint into each MGXS object
# You can use the axis argument to get the maximum per row instead. #
# Random forest score:
# Round
#quandl database query through API
# Set an option so we can display the full tweet text # have a look at a few of the tweets (remember your subsetting)
# drop ceil_15min from data
# Rename index levels
#Making a copy of the dictionary and showing all the possible campaign stage names options
# We can set a MultiIndex while reading a csv by referencing columns to be used in the index by number
# content of the merge of 3 dataframes
## no.of unique cats given out by the shelter 
# Check out the structure of the resulting DataFrame
#gbm_predictions.loc[ada_predictions['Predictions'] == 1]
# Reading packets from pre-captured file
#Removing rows with null values
# Baseline score:
#Calculate the date one year from today
# use pandas to view the data # We create a pandas dataframe as follow: # We display the first 10 elements of the dataframe
# Flatten out the 'sentiment' dictionary column of the 'tweets' DataFrames
#check the loaded file
# find all sounds of footwork' #for tracks in tracks.collection: #    print(track.title)
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# your code here #Let's briefly look at your result
# Search all the data from twitter related to given tag
# Rejoin this set of ids into the pax_raw dataframe
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# Reset the index of airquality_pivot: airquality_pivot # Print the new index of airquality_pivot
#Clean the df
#nyc open data API, you need apptoken to run this cell
# tweets by location
# drop all null values
#### Create a column that has a number that symbolizes whether a row is close or not to the 'real' pick #### We'll do this first for Top McMurray and then top Paleozoic, which is basically base McMurray #### Top paleozoic version
# you can convert date time index back to period index
SQL = """$ SELECT rating, title FROM film WHERE rating = "PG" OR rating = "G";$ """$
# from lists
# Inspect dataframe
# Writing the file as csv
# these are the important words in the title
# We get the trend for closed prs by month
## Libraries 
# Let's try with our homes median list price data, which we will want to be time-indexed
################################ # create law file and fix law account number
# Let's get some data; top stories from lobste.rs; populate a DataFrame with the JSON
# general information about the file
# Replace NaN data with a high and invalid number which is -99999 # We're saying we want to forecast out 1% of the entire length of the dataset into the future
# from the non-active users, how many still visit the site (LastAccessDate)?
# Display coefficients for different features
#(Train,test)-store (checking if there was any unmatched value in right_t) ##Note: Level of store table is "Store"
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
#iii. How much influence to my posts have? #Ans. Get the sum of all the retweeted tweets and group by users   #     The users with most counts is most influencial
# how many unique authors do we have?
from datetime import date, datetime$ def json_serial(obj):$     """JSON serializer for objects not serializable by default json code"""$
#https://stackoverflow.com/questions/546321/how-do-i-calculate-the-date-six-months-from-the-current-date-using-the-datetime
#Random Forest Result 
# Use your model to make prediction on our test set. 
# load the
## Check if there is 'None' in 'tweetSource' column ## Check how many tweets are we left with now
# check index have correct freq "<MonthEnd>"
# Capture shape of raw data
# analyze validtation between BallBerry simulation and observation data.
# freq='BM' stands for Businees end day of month # the last BM of each month in 2017
# Random forest score:
# migrating table info # for row in table_rows: #     print(row.text)
# Change the plot size.
# Likes vs retweets visualization
#print df2.loc[['2016-09-18'], df2.columns.get_indexer_for('GrossIn')]
#silence some warnings
#You should also convert month from object to datatime!
# Crear la nueva columna usando funciones de la librería datetime
# Merge does not care about the index # As in this example even though the  # index were different, it created only 4 rows
#abc = detailed_confirm_write_to_file_ALL_DRGs(50030,2014,6) # 50030
# It would be nicer if those states were capitalized. We can easily do that
#'Miami': '04cb31bae3b3af93'
# Use a colour-blind friendly colormap, "Paired".
# View all the classes mapped to the Base
# Drop three columns we don't use
#2D correlation matrix plots scope vs type vs site
# Show best score
#  setting a Target variable
# mydata['new'] = dc2015['created_at'].dt.hour
# check added features
cur_b.execute('''$ ''')$ conn_b.commit()
# This is perhaps not 100% intuitive - the value given will determine the # number of ticks appearing between each major tick; it has no relation to  # the scale of the actual values being plotted.
# General plotting settings
#segments
# Checking Correlations # Cost and Sessions highly correlated r = .89 # Margin & Bookings approaching multicolinearity colinear r = .92
#df_sentiments_means.plot(kind='bar', legend=False)
# When using a Datetime Indexed data set, you get some pretty cool  # new methods for calculating different 'rolling' statistics
# The Dataframe's text.
# Inspect station data
# Example
# URL of page to be scraped
#input_nodes_DF = nodes_table(nodes_file, 'FridayHarborBiophysics')
# Concatenate column names from different levels in the hierarchy. #
# Divide each number by each countries annual maximum
## Find the stock with the minimum dividend yield
# Ensure latitude is within bounds (-90 to 90) # Have to do this because grid file has 90.000001
# Merge the overall dataframe with the adj close start of year dataframe for YTD tracking of tickers. # Should not need to do the outer join; # , how='outer'
# reflect an existing database into a new model # reflect the tables
#create dummy variables for the categories
# Load part of the dataset to have a look
#import into python
# Apply the function
#mktime(current_time.timetuple())
# Identifying the top 10 authors # value_counts returns object containing counts of unique values # Listing contents of 'top_10_authors'
## Count the amount of sources, and display  the first 5
# 6/11/2017 ~ 6/18/2017
# no output below means the conversion was successful
#Calculate the highest opening prices for the stock during 2017
# load ADNIMERGE.csv, the main dataset (we will add our own variables to it). 
# Model is very overfit since the test score is much lower than the train score 
# pairings < 0m / all pairings, since 3-01-2018
### final list of chinese_vessels
#Looks like we're good on missing values. Now let's get a summary of the data to see if there's any more issues with it
#(p_diffs<p_diff_abdata).mean()
# use a vectorized string operation over the email addresses
#ignore_index means the indices will not be reset after each append
#final_df['mean'].values
# Print the value counts for 'Borough'
# Check to see if the above worked.
# plot basic histogram 
# Lets set a style
# labels dict contains IOB (inside-out-beginning) labelled entities # printing some randomg k:v pairs 
# get the third and fourth indexes of an Index object
# percentiles look right; most data should be fairly close to the value at the 50% mark
#no aporta nada, solo tiene ids
#register a data frame as table and run SQL statements against it
# remove colums with all na #psy_hx.columns
# save data to CSV
# what is the duration of author's publication career?
#print df
# show available waterbodies
# Make distance_list into a series # Set to var 'distance2' which will become a new col 'distance' added to df
#Count unique topic_names within dataframe
# Rename the column Region to Country
#Upper and lower bollinger bands
# Save file to csv
#dang, there are ~80k nulls in may
#Check the number of tables in the database
# For Displaying last six Data points # For Displaying first six Data points #pd.DataFrame(dummy_var["_Source"][Company_Name]['Close']['Forecast'])[:6]
# How many stations are available in this dataset?
# generate a list of all customerIDs
#Convert from cfs to cms (1 CFS = 0.028316847 CMS)
# showing plottings via terminal rather than window popup
# make sure ES is up and running
# group by user
# a simple logistic benchmark. only knows how often defense shifts and how often batter is shifted against. 
# check option and selected method of (16) type of lower boundary condition for soil hydrology in Decision file
# get the top level key for the nested dictionary containing max. daily change
# Set DateTime as index
# Both Pandas and Numpy are sufficiently clever to figure out that the mean of a  # series shall be the mean of its contents provided they are numerical
#data['new_claps'] = data.claps.rank(method='dense').astype(int)
# to get the last 12 months of data, last date - 365
#renaming columns of the derived table
#create BeautifulSoup object and parse with html
# reg_target_encoding(train,'Block',"Real.Spots")  # tbd # reg_target_encoding(train,'DOW',"any_spot")   #tbd # reg_target_encoding(train,'hour',"any_spot")  # tbd
# Loading a pickle file
# Filter outliers in Min Power
#df_en['text']
# make forecast for future dates created (pd df object)
#Normalize nested data structure to a data frame format.
# train.summary()
# Count the number of stations in the Measurement table
# Add to Pandas Dataframe
#Print the raw comment and output of get_text to compare: #It's supposed to remove tags and markup
# Creating the authentication object # Setting your access token and secret # Creating the API object while passing in auth information
#Review column names in dataframe
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# Only needed so this notebook can be run repeatedly
# Create a new dataframe for the (It does so by testing true/false on date equaling 2018-05-31)
# check Initial condition data in file manager file
# Reading csv file and specifying the index as column 0 of the file
#平均长度
# RE.SPLIT
# Sampling the Recommendations provide using the item:desc dictionary we had created earlier
# Get laste record entry date for temp
# 4. 
# !pip install scrapy
# Query to find the count of stations 
#### sort by value
# dropping test rows
# Creating a y_train with our click information # From the Kaggle site, we know 'id' is just a record indicator, so we can drop it # along with the "click" column, which is our target
#eigenvector_dict = nx.eigenvector_centrality(G) # Run eigenvector centrality, failed to converge # Assign each to an attribute in your network #nx.set_node_attributes(G, eigenvector_dict, 'eigenvector')
#start random forest
# read in pickled emoji dictionary I created from emojis in the dataset. I want to use # each emoji as an individual feature.
# Authorの定義　個別に行って下さい
# Check that new_stops table contains stops which were in stops but not in oz_stops
#  透過plt.style.use直接套用風格檔
# listing all of the collections in our database:
# That looks better # Let's inspect the index to make sure the dates are of the right type
# Tokenize the text 
# reflect an existing database into a new model # reflect the tables
# df_2016
# Fit the model
# Get the raw data values for the reflectance data
# df_data=pd.DataFrame({'time':(times+utcoffset).value,'chips':target.values()})
# Generate some samples from the state space 
#### Test ####
# split into training and test sets using dedicated wrapper
# If the condition is True then the row will be selected
# calculating number of commits # calculating number of authors # printing out the results
#== Check if NaN or not  #print(pd.isna(dfx)) <== depreted?? 
#we seem only to be interested in 'first_name'
#Read the last 5 records
# 无缺失值
"""$ Print any single news title$ """$
#Tokenize words in articles
#Use tweepy.OAuthHandler to create an authentication using the given key and secret #Connect to the Twitter API using the authentication
# Convert minutes_list into a series # set to 'tripduration_minutes' which will be new column 'tripduration_minutes' added to df
# From a dictionary so keys are the index and get sorded by keys
# subset ndvi to us
### Fit Your Linear Model And Obtain the Results
# Get Facebook data and store in variable.
#Import Random Forest libraries 
# Display of first 10 elements from dataframe:
# save the hashtags to disk
# Define a dummy model for a benchmark that predicts the majority class every time
# let's define a list # we will append a new number in the list
# means by month:
# Get a list of column names and types for Measurements Table # columns
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Where are the "outliers"?
### Plotting
#Mars Weather
# loading data into pandas for inspection
#get count by month of each term
###last hour of values
# Functions to count the number of files per folder
# Crear la nueva columna usando funciones de la librería datetime
#re - upload complete file (processed in excel)
# Gets historical data from the subscribed assets, between two dates with daily resolution
# Setting up plotting in Jupyter notebooks # plot the data
# games without events
# Get best partition
##Extract year from Date
# regression model #fit the model
#2. Convert the returned JSON object into a Python dictionary.
# Test our trained SVM classifier with our test data
# Backward filling
# Frequency specifies the length of the periods # — the default, "D," being daily. We understand that BM is bimonthly.
# Load the query results into a DataFrame and set the date column as an index
# Create subset dataframes by city
#Show the last 5 rows 
# How many elements in each, how many sessions, how many elements per session
# calculate daily percentage change
# Optional: Write dataframe out to CSV
# Plot ACF of first difference of each series
# Vectorize the articles into a matrix # predict whether or not economically relevant based on previously fit model
# Continue clicking throught the comics
# try on the test data, however, the result not very good # reason might be overfitting
# drop rows with missing value in specialty column
# Merge multiple rows
# join all messages by the same candidate
# Lets load the Concatenated DataFrame and the engine specification will help to delimit the outliers
# Spark Streaming #Create streaming context with latency of 1
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
# Find z-score and p-value
# dropping columns '[', ']', and ','
#Checking to ensure I pulled all outlets
#Determine number of students with a passing math score (assume that a passing math score is >69)
# loading iris file
''''$ user access to matplotlib functionality, which is another Python package that can allow us to visualize our results.'''' $ import pandas as pd
#2
#Check that you are using the correct version of Python (should be 3.5 for these tutorials)
# Select columns of interest
#by default, the row index is used for abscissa values, which comes in handy for  # time series data
# resetting index to eliminate user id's
# Summary statistics for the ratings data set
# Insert new column at column index 5
# Visualización de likes vs retweets:
# money flow index (14 day) # mean-centered money flow index
# Define sentiment bins and groups # Add column "Sentiment group" to tweets data in sentiment_df
# Calculate % change from quarter to quarter, find those with change greater than 100%
# how many users
# cisuabd4
# Positive Reaction ratio - Negative Reaction ratio  # Bi-variate relationship
#We use tfidf vectorizer with german stop word list (modified) from below. #df.message_vec.head()
# find historical data for 2016
# highest temperature recorded
# Use len() and unique() to calculate unique user_ids in df2
#Example 6: Load a csv while specifying column names
# histogram of score
# pytz
#KNN classification
#### Assessing the final dataset
#Find the largest change in any one day (based on High and Low price) #Verify new column is correct
# URL of page with Mars facts to be scraped
# normalization to put everything in float and convert "N" to nan
# Retrieve last 12 months of data, last date - 365
# Use IFrame to display pyLDAvis results to prevent pyLDAvis from overriding # other formatting of the Jupyter notebook
#1: Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD) #2: Convert the returned JSON object into a Python dictionary
# Push the sentiments DataFrame to a new CSV file for each vader and Watson
#set variables to allow automatic X-axis on plot
#  Large "repeat" values obviously increases computing time... #  Repeatly generate daily prices for one-year, record gmr each time.
# 이렇게 배열의 순서를 찾는 것이 안될 것은 없지만?
# creating month csv
#overallCond = pd.get_dummies(dfFull.OverallCond)
#Obtain all the column names for later calculations
# Get column and types for Measurement Table # columns
# get countries and show the 3 digit code and name # show a subset of the country data
# Pull in a brand new ticket # prepare(new_ticket)
# Getting chromedriver to retrieve html from reddit, not sure what the timer is for exactly need to get info # Giving it 2 seconds to completely load the reddit page before scraping
#plt.ylim(200,400); #plt.xlim(datetime.datetime(2018,2,7),datetime.datetime(2018,3,24))
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Subset to Enrollment Level only # remove colums with all na
# What is the maximum value?
# Cleaning up columns so it's easier to reference them.
# Assign the demographics class to a variable called `Demographics` ### BEGIN SOLUTION ### END SOLUTION
#Task 1: Import 2017 daily data for AFX_X 
#import required modules from the scikit-learn metrics package
#remove columns with identifing info
# Fill NaNs with zeros
# Summary statistics for the movies data set
# set seed for reproducibility
# Función para calcular los rendimientos diarios a partir de los precios de cierre...
# Set "training='probaility'" to train on the labels # Display head of input data of stack layer : probability
# Adjust the two date columns.
# And it's tail:
# extract precip array # check shape
# 10-fold cross-validation with two features (excluding Newspaper)
# Bayesian 0.71106382978723404 # Logistic 0.74416413373860191
#beautifulsoup boilerplate code
#converting to datetime format of pandas for age calculation 
#will create percentile buckets for the goal amount in a category
# Resample
# Transmission 2030 [GWh], marathon
#x = df_stock1.filter(['Date'], axis =1 ) #y = df_stock1.filter(['Close'], axis = 1)
# REMOVE THIS LINE IF YOU ARE GOING TO PUT IN PRODUCTION
# df_plot.sort()
# interestingly, we can use either or both levels of a  # hierarchical index to pick out specific entries
# midToto is 13, therefore user can change each layer value from 0 to 12
# read from the appl worksheet
# Copy dataframe # Remove incriminating rows
# Remove outliers if needed, replot and see if the moments changed significantly. # Checking how many values I have above 100% tipPC
#tlen.plot(figsize=(16,4),label="Length", color='r');
# terminate the spark session
# last word category to emoji
# same logic here:
# Convert sentiments to DataFrame
# Retrieve the first [0] record and store it in the latest date variable # Use strptime function to store the formatted date in the format_latest_date variable. We will use this in our code later
#split the dataset by gender
#total=total.dropna()
# Agrupar por periodo y contar la cantidad de páginas a las que les dí likes
# check group size
# histogram in pandas
# Step 9: Convert pivoted into numpy array ## Use fillna(0) to fill in missing data
# 10. 
# 詳細状況の取得
# Create BeautifulSoup object; parse with 'html.parser'
#**Which Tasker has been hired the most?**
# Column names on the raw data file
# provides info related to the dataframe df_CLEAN1A as follow: 1k observations, each column has datatype of numeric(int64) and also how much memory is using in our machine. 
# Get categorical codes for date columns
# drop ceil_15min from data
# Import the required libraries.
# check Forcing list data in file manager file
# View dataset metadata (in different way)
# Display a list of buckets.
### Test ####
# get the local "now" (date and time) # can take a time zone, but thats not demonstrated here
# Getting probs from logprobs #probs_test = softmax(log_preds_test); # trying this custom softmax. SAME RESULT.
# Print the tail of df
# group data frame by time and number of tweets per each hour
# Sammelt die Beiträge für alle angegebenen Seiten
# List comprehension for reddit total comments of post
#show the band widths between the first 2 bands and last 2 bands 
## We can use the /agences resource of the Index API to get more information. ## Below, we combine the previous steps into one line and ask for the agency associated with the first feed:
#41% of those who pair at <3m, subscribe <3
# Get the average Compound sentiment
# add a notebook to the resource of summa output # check the resource id on HS that created.
# Here it can be seen that less popular people have liked Paula's profile and more polar people doesn't.
#models trained using gensim implementation of word2vec
# df_2002
"""$ Use this code to test your classifier after each round of confidence sampling and plot your results$ """$
# This step can take 5 to 20 minutes # NOTE: Currently, recreate=False is not supported for deploying to ACS clusters
#faamg
#just_coordinates = wash_parkdf.coordinates
# how many records will we drop?
# integer index: a series
#split the data #train_b.summary()
# check the first 5 cards in the Dataframe containing cards from the Ixalan set
#Check count of missing state value 
# read in data only in the Date and close columns, # use Date as the inde
# Merge the two datasets # The lengths of all 3 datasets should be equal
# fit the model to trainng data
#################################################### # LOADING THE DATA ####################################################
# import Quick Inventory of Depressive Symptomatology-Self Rating
#delete duplicate receipts 80874 duplicate receipts
# I am giving our data a name and instantiating a SQL table
# dataframe of negative examples
# Design a query to find the most active stations.
# A: 全A股
#*--------Merge Command to merge Studies and sponsors--------------------* #studies_b=studies_a.join(sponsors,rsuffix='_other')
# get indices for rows where the L2 headers are present # these will indicate the beginning of data 
# convert the string date fields into a date type so that we can make calculations on it
## Initial Apple Negs
# check some of those features
# old allocations
# select orders related to our selected users
# file path, mode, and encoding can be seen when file object is called
# Pulls the last 5 columns from the data
#df['money_csum_cent'] = 100*df.money_csum/df.groupby(['program_id'])['money_collected'].sum()
# Test
#Store locations of long NAN gaps in primary sensor record to be filled
# drop any potential rows w/o valid Reorder_Interval_Group
# request data from the API
# URL of page with Mars hemisphere high resolution images to be scraped
##Check the last 2 tweets in the dataset
# Count the number of tweets in each time zone and get the first 10
# created_at is likely to be parsed as text # We convert dates to a date/time format so we can do 'maths' on them # If ok, dtype should be datetime64[ns] or <M8[ns]
# Converting data type
# first column [0] is target (outcome variable) # second column onward [1:] are features  # we keep the feature name in X_header
#take a peek at each individual track
# Here it should have merged 2001 to 2001 index and so on because all the other valeus are the same but it did not.
#switching date of tweet dtype from object to date-time #setting handle as index
#Data Frame Summary (its a class based on 'pandas_summary' library)
# Will go ahead and fill in null values with empty strings so that we can aggregate cleanly
# Get the columns (movieIds) for the current user
# Joining ab test table with countries table
#importing the libraries
# drop un-needed columns
# Grab a single record to get a sense of schema
# Append results to 'results_list'
# write your code here
# くそツイなしのデータをCSVでエクスポートする # pandasでやるのだるいので、リツイート比率(RT/いいね)をエクセルで出してから再度noshit.csvを呼び出す
# Choose the station with the highest number of temperature observations. # Query the last 12 months of temperature observation data for this station and plot the results as a histogram
#add delay_time to dataframe since it was acting funny when  #we tried directly adding it to the dictionary earlier
# pandas way
#reorganize data set
# look for seasonal patterns using window=52 - therapists
# transform all of the data using the ktext processor
# df.at['Row', 'Column'] = 10 #note the funky name is still present as Index
# Open the data file and read the contents into a dataframe #df=pd.read_csv("SFData5Users.csv")
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Array of 4 features # Corresponding array of 4 labels
#Seccond Data Set:
# an empty model, no training yet
# before_sherpa # after_sherpa
# convert 'C' to null
# UN01.DE is Uniper SE, a recent spin-off
# Requires MSMS installation # See the ssbio wiki for more information: https://github.com/SBRG/ssbio/wiki/Software-Installations
# subset to included participants  ## there are duplicates, weird
#Every genre is separated by a | so we simply have to call the split function on |
# Initialise with a Numpy array. #
# filtrar información aberrante tmin mayor
# get the overall sentiment per media and store it in a dataframe
# City
# Show datatypes
# details from https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words # lower case all text
# Another way to see relevant information is to use the `describe` feature of DataFrames
#connecting to the sqlite table hawaii_hw that was created in data_engineering notebook, then connecting to it
#estatisticas basicas # print(feedbacks_stress.loc[7, ['incomodo', 'interesse1', 'interesse2', 'num_video_preferido', 'justificativa']])
# count the size of the class
# Retrieve Latest Date
#killfile = killfile[:20000]
# calculate the temperature difference between the two cities
# let's check the best estimator
# instantitate an empty list called data to store the result proxy
# To test the slack technologies we now add a demand that we do not have yet added a technology
## 1. Sort traded_volumes
#**the 'tasker_id' column in the following dataframe represents the number of times the tasker_id has been shown**
# bind parameters to the stmt value, specifying the date_filter to be 10 days \ # before today
#`refit` raises an error, so modified
# Use Pandas to print the summary statistics for the precipitation data.
# Calculate the average chart lower control limit.
# Count how many files in training folder
# RESET the TF defaults # Start a fresh graph with no configs -- TODO: get some config info for the graph
# create CustomBusinessDay object based on the federal calendar # now calc next business day from 2014-8-29
# Stats
#use requests builtin method .json() to parse JSON into a dictionary
# these are the card layouts for "typical" Magic cards - the rest are the layouts we need to remove # the outer lambda defines an indexing by location to apply to each element in the cards column, and the # inner map/lambda defines that indexing as one that removes the given layouts/types
# Stripping From StarClinch from the names. # Now, we only have the names.
# View certain rows of the test dataset
# count the gene names
# Create a one-dimensional NumPy array from a range # arange returns evenly spaced values within a given interval, given a starting point (included) # and stopping point (excluded), and an optional step size and data type.
### Step 16: Reproduce the scatter plot with divider of labels ## Provides clear separation of clusters
#babies uid == BABY_UID
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Now make a masked array to find movies to recommend # values are the movie ids, mask is the movies the most # similar user liked.
# https://developers.google.com/maps/documentation/geocoding/intro
# Convert to Spark Dataframe # Create a Spark DataFrame from Pandas
# The result of loading, paring down, and renaming columns should # look something like this:
# let's get every tweet with hashtag "#digitalgc" in for the past week # twitter api only allows searches for the past week # api.search
# Min and Max
# Use Pandas to print the summary statistics for the precipitation data.
# one time only
#import the prepped dataframe that contains weather data and start date and age and distance
## create a list of top 25 users from 'users' DF
#Convert to seconds
# seriously, what's up with this table # for row in table_rows: #     print(row.text)
#nt_price = pd.crosstab(nt.index.to_period("W"),nt.catfathername, values=nt["sq_price_value"], aggfunc='mean', margins=False).fillna(0.0)
# n_new transcations conversion sample values
# the set names form an index, and the columns are the attributes of each set (set name, release date, cards, etc)
# Define the DataFrames with only a single "Category".
# Probability of control group converting
# Split the data into features and target label # Visualize skewed continuous features of original data
# 复权数据
# We still need to rename the columns
# calculating or setting the year with the most commits to Linux
# df.ix['600848']
# convert date from string to datetime
# Create hourly data frame
# Notes:  #      - x-axis is the index. #      - Must find a better way to represent these figures.
# Using agency 'METRA' from first feed
#Set up tweepy authentication
#marvel_comics_save = 'wikipedia_marvel_comics.html'
# value of the accumulator - the number of even numbers in data
# Normalize creates normal daily times and will set the default time for each day as midnight.
# List stores
#원하는 속성값만 남기고 나머지 버린다.
# looking up 'new york' in the airports dataframe # fs refers to 'flightstats code' of the airport
## expand full path to token ## create env variable TWITTER_PAT (with path to saved token) ## save as .Renviron file (or append if the file already exists)
# Converting the index as date
# Note that this file contains 7 duplicates that are removed.
'''$ u.data     -- user id | item id | rating | timestamp. $               The time stamps are unix seconds since 1/1/1970 UTC  '''$
# merging zipincome with zip_depart
# reset the index to the date
# Mars Facts URL to Scrape
# Delete rows where 'Ov.transport status' is 5, 6, or 7.
# remove PAs with no event. these are probably PAs with pinch hitters.  (< 1% of observations)
#print(data[:5])
# Find all 'a' tags (which define hyperlinks): a_tags # Print the URLs to the shell
#Design a query to find the most active stations. #USC00519281 had the highest number of obs, at 2772
# Design a query to calculate the total number of stations.
#Hialeah': '629f4a26fed69cd3'
## Count the number of tweets per userID and get the first 10
# Clean tweet_df, keep only useful columns
# Examine fitness_tests here
# one time if for converison of list
#Show rows which address is null
# sentiment_df.head(100)
# Choose the station with the highest number of temperature observations. # Query the last 12 months of temperature observation data for this station and plot the results as a histogram
#https://stackoverflow.com/questions/19851005/rename-pandas-dataframe-index
# Extract specific values - 2
# and visually?
#drop rows with null values in dirty columns: 27662565 records #drop rows with values in dirty columns: 26290342
#probability that user gonna convert
#  Sanity check on array shape, expect: (16042,) #  from which we can see the population size:
# read in dataframe
# fig, ax = plt.subplots(2)
#  toar() is for conversion to numpy array:
# Calculate total stock price change within the date specified
# get the top 1000
# first attempt, try to include all of the punctuation on the title as the valid datasets # but beforehand, we need to remove the None and NaN val
# For finding all the paragraph tags
# When convinced Raw and Processed data is good, upload to bucket. # Upload raw heat pump spreadsheet to the bucket.
# Use a session query to find the first row in the database
# The protocol version used is detected automatically, so we do not # have to specify it.
# Now, we are interested in looking closer at "Likes". We can use Pandas describe function (for DataFrames and Series): # (normalize=True) gives percentages of total volume. 
# Apply to all movie reviews in dataframe
# Median trading volume
# Create BeautifulSoup object; parse with 'html.parser'
#active
# run the model giving the output the suffix "rootDistExp"
# Store the wavelength info into a key of the same name
# Call method 'score_all_on_classifier_by_key' on our classifier object. # It calls method "score_all" from class 'Classify' # 'score on all the test and validation sets' 
#That's our result! Now let's export this dataframe into a csv in case we want to work with it later. 
# We create a column with the result of the analysis:
# converting the timestamp column # summarizing the converted timestamp column
#df_hi_temps.describe()
#y_train.head()
# split X and y into training and testing sets
# Pull the data from remote location: here is my github "Data" folder # Note: if you read the http files than read the file content and not the html because you'll get issue and not knowing where comes from. # docs source: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html
# define the vector w
# Observed contingency table of test_id against test results # p-value of the null hypothesis
# Select only the `date` and `prcp` values.
# extract full monthly grid
#Example 7: Import File from URL
# create pySUMMA Plotting Object #Val_eddyFlux = Plotting(hs_path + '/summaTestCases_2.x/testCases_data/validationData/ReynoldsCreek_eddyFlux.nc')
# post simulation results of simpleResistance back to HS
#using value counts in descending order. 
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# finds tobs for most active station
# test to see if this is right
# columns in table x
# Print the first Tweet as JSON:
#Example1:
# Train model
#Number of rows in training set  
#users whose most recent scn-status is canceled followed by trialing and were mis-categorized as churn
# Returned as a python dictionary
#Drops the unneeded part of the URL in the feature "Date" 
# write your code here
# evaluate the chunker
# Get all unique artistId, and broadcast them
# The contractor table has been created locally before running this step #Using if_exists='append' can prevent the constraint of columns being changed 
#creating the model 
#'Fremont': '30344aecffe6a491',
#load most up to date data
# retrieve stored API_KEY
## one way or another, by this time, we can merge in the list version, cpdi, to get ## a complete 'eppd' dataframe.
# Add one column named "Total"
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
## filtering out the cats
# Percent of true gains over all samples: compare to precision -- precision should be signficantly higher if this is a useful # prediction # (cnf_matrix[1,0] + cnf_matrix[1,1]) / cnf_matrix.sum() 
#TEST #results = Geocoder.reverse_geocode(df['latitude'][0], df['longitude'][0]) #31.3372728, -109.5609559
# Frequency analysis for words of interest # Number of unique and total words in the text
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
# First option - the slice object # arr[slice(None), 1] is the same as arr[:, 1]
# merge the labels with there games 
# shift forward one day
#== Sorting by values of a column 
# take only elements that belong to groups with a group sum greater than 100
# now I want to find the first and third quartiles
# current.head()
# Use `engine.execute` to select and display the first 10 rows from the table
# Crear 2 nuevas columnas llamadas longitude y latitude, basados en el campo place
# RE.SEARCH
# Use Pandas Plotting with Matplotlib to plot the data # Rotate the xticks for the dates #df.plot(kind="bar", x="date", y="prcp", alpha=1.0)
# Apply standardizing function to "Cause of death" column
# analyze validtation between BallBerry simulation and observation data.
# сохраним значения по искомому ключу в отдельный столбец
# View dataset dimensions
# range of potential gammas using logspace
# Set up Twitter workspace
# Naive Bayes with LOOCV on our Test Data
#remove links
# We extract the mean of lenghts:
# convert date
# Drop these cases
#df_tte_all['UsageType'].fillna('not applicable',inplace=True)
# and then reload it...
# Put in dataframe and view selected rows/columns 
# take a look at the dataset
#Oh you pesky little one
#Grouping by outlet then creating a new DF for the bar chart
# creates a subset of the original DF w/ only certain times represented
# The ApproximateMedialAxis lines created point in different directions,$ # this step simplifies them.$ sql = '''CREATE TABLE merged_approx_medial as SELECT id, ST_LineMerge(linestring) AS linestring FROM approx_medial;'''$
# Export to CSV
# Let's add the classified english data together with the non-english (unclassified)   
# Remove columns
# Visualization of log odds - this needs some work as you cannot really visualize 10,000 words
# Need to change to API request to call for Franfurt Stock Exchange, FSE, AFX_X # Change response to readable JSON
#festivals.index = festivals['IndexT']
# num_comments along the time
# List comprehension for subreddits of reddit posts
# Organize dataframe by date
# write the scaled and unskewed data back to postgresql
# Slice using both "outer" and "inner" index. #
# get now, and now localized to UTC
# factors = ["SIZE", 'VALUE', 'BETA', 'VOL', 'ROE', 'MOM', 'LIQ'] # 为空表示更新所有存在的factors
# inspect Meta Data
#Get the text of the tweet
# break down specialty category by provider ID number
#ignore all 2* tweets #positive sentiment = 4* tweets
# Create the ticks for our bar chart's x axis
# Attach Measurements table to the database
# Calculate number of unique users in dataset
#show how to get the other comments of a user #print(submission.title)
# prediction on validation set
# Count number of lines in ratings
# drop the 1st row (which is the column names): 
# to get the last 12 months of data, last date - 365
#read it into a dataframe
# Getting a list of column names and types # columns
# Which learner had the most interactions and how many interactions did they have?
# For getting all the links found in the nav bar
# Simplify the dataset into question answering pairs # TODO: Filter out some long conversations that appear to be open discussions instead question answering.
# figure out what was the spike at 5-6 am on Small Business Saturday # more than 90% of the collected tweets were created between 5:30 am to 6 am Eastern time # So let's focus on those
# results are returned as an iterable list
# converting the timestamp column # summarizing the converted timestamp column
# we will sort the information in the last array
# calculating number of commits # calculating number of authors # printing out the results
# Append topic codes to dataframe 
# array to dataframe # add name
# remove emojis #print(strip_emoji('baba black sheep 🙄🤔💗'))
# Give the chart a title, x label, and y label
# How many usres in df_users
#The result is correct; 
# Take all vets younger than 38 
# To find value of n_old
#mydf2.datetime = pd.to_datetime(mydf2.datetime)
# Create an object to transform the data to fit minmax processor
# Predicting the sentiment values for test data and saving the results in a csv file 
# Drop non-normalized scores of Brokerage and Betweenness
## Accuracy is the proportion of true positives and true negatives vs false positives and false negatives. # In your confusin matrix, it's top-left + bottom-right / total.
#from collections import Counter
# -r stands for 'recursive'
# Split Data # To avoid Data leakage split the data
# convert crfa_r to numeric value
# create range of indexes (by default dayly timestamps):
# instantiate the algorithm, take the default settings # Convert indexed labels back to original labels.
# Which tweets received zero likes?
# R-Blogger news #print(type(soup)) #print (soup.prettify)
# Put the data into HDFS - adjust path or filename if needed
# Inspect the hierarchy of the json dictionary 
#create a dataframe of a random samplesize of the master dataframe to determine which variables impact the target variable most
# メインスクリプトである`model.py`の使用方法を表示
#Dictionary of Outliers
#Do the same for male
# Transmission 2040 [GWh], marathon
# Index persistence
# Create engine using the `demographics.sqlite` database file ### BEGIN SOLUTION ### END SOLUTION
# 6.
# standardize column names
# cv_score = cross_val_score(rfc, features_class_norm, paid_status_transf, scoring='roc_auc', cv=5)
# Run the model with training set 
#check values substituted
### Validate Pickle ###
#Cleveland': '0eb9676d24b211f1
#df = pd.read_pickle('/home/jm/Documents/testingStuff/my_file.pkl')
# your code here # a DatetimeIndex was created
# build dataframe # view column information
# change name of oz_stop id to stopid to allow merge
# Most retweeted
# You can build a DataFrame from a dictionary:
# The index values are DatetimeIndex objects. # The names of columns in the data frame
# Remove stop words
#info df
# To check missing value and any rows in csv file 
# This is its own cell because it takes a while to load this thing # takes a little bit. increase limit at own risk. # model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=500000)  
#Filter events which topic is in top10 #Compare shapes
# Retrieve latest full mars images
# Keep only unique play text, keep the first timestamp
# Dummy subreddit values 
# examine rolling volatility
# 15 topics seems like too many for donuts so let's go with 10 # train LDA using 6 cores (1 master + 5 workers)
# I did this for the entire set of lead managers before losing all of my work
#     df_tweets['polarity'] = TextBlob(tweet).sentiment.polarity #     df_tweets['subjectivity'] = TextBlob(tweet).sentiment.subjectivity
#replace outlier with nan for correct mean calculation #replace with mean for now. Todo find a better way to replace them. 
# login_url = 'http://www.prisontalk.com/forums/login.php?do=login' # login_response = s.post(login_url, data=login_form)
# Check for missing values?
# open all files
# Display a list of created buckets.
# Tests submitted over time
#Concat two original contractor_bus_name and contractor_number column  #into a unique contractor_bus_name per record
#del festivals.index.name
# it is a string
#code to work with a polygon input
# logistic regression without tfidf #%%time
# take subset to develop algorithm with it 
cur.execute("""SELECT created_at, text, location, time_zone, geo, lang from tweet_dump$                order by id DESC limit 10;""")$
# first we get some 
# validate this is what we expect, missing values may throw off pandas types
#resample transit and cab data to a lower frequency (1 Month) #select by day
"""$ load data$ """$
# start_date = input('Vacation Start Date: Year-Date-Month format ') # end_date = input('Vacation End Date: Year-Date-Month format ')
# Cycling distance # Create a tuple of data
# dateutil
#### Test ####
# Extract unkown users' log
#Let's pick one date and show the first rows #These are cleaned up data and therefore need no further cleaning (I had to do some cleaning of strings, fill NaN, #and eliminate unnecessary columns)
# save the model to disk
# train score
#All formatting is the same so just cut the wordings #RUN ONCE ONLY
# All columns except for 'label' will be considered as features
# y_pred = knn_reg.predict(x_test)
#Create data frame with stations and counts
#!rm ../../output/prealignment/raw/{srx}/{srr}/{srr}*bam*
# We extract the mean of likes:
#print(dir(np))
# Print all of the classes mapped to the Base ### BEGIN SOLUTION ### END SOLUTION
# Find and click the full image button
# create a trip duration column where the duration is in hours using pandas datetime package
# concat df and coming_next_reason
# Create an intance of the class
# Set just like the index for a DataFrame... # ...except we give a list of column names instead of a single string column name
# get the final train dataset
# Remove stop words from 'words'
# Para el caso de zip code y location y dado que son pocos (en virtud de lo observado anteriormente) vamos a desprendernos de aquellos casos en los cuales # no tengamos ni uno ni otro. Puesto que si tenemos uno podriamos llegar a tener el otro
#see data distribution of active/inactive stations
# Now we are picking up the random code that is used for every youtube video
# Extract specific values - 1
# plot histogram
# calculate the average RMSE
# load panda dataframes into sqlite databases
#Let's get the average number of retweets and favorties per location #looks like even though we got rid of None values in userLocation, there are still empty Strings
#define colors and legend values
#Starting by looking at up votes, going to set it up by looking at the differences between #top fifteen percent and those that received 0 up votes (4888 vs. 6910 observations).  #creating 'viral' dummy
# option 2 (finds unique rows, duplicates columns)
# Remove emojis #df.text = df.text.apply(removeEmoj)
# create a dataframe
# Few tests: This will print the odd word among them 
# Make the output Dataframe
#combine the two months of data
# Use the "short_id' field as the index # Show the first few rows
#白噪聲檢查 #返回統計量和p值
# analyze validtation between BallBerry simulation and observation data.
# 字典里值为None的不会被添加到URL的查询字符串里
#load data into pandas dataframe
## Solare Flare 2012-03-07   highest sigma event 
#2.Convert the returned JSON object into a Python dictionary.
#info_final = pd.merge(info_final, avisos_detalles, on = 'idpostulante', how = 'inner')
# Import curren stops table into dataframe
# use the model to make predictions on a new value
#Tally the number of unique sites
# your code
# Load results for
# fire report  #fire.head()
# Logistic Regression model
# 136 tweet ids are not exist in the tweet_archive_table
#df_geo_insta.to_csv('instagram.csv')
# Cleansing the Search Term for the records with SearchCategory of Plant, to only include plant id and removing everything else
#check
#dfg = df_n.groupby('drg3').agg({'2014':[np.size, np.mean]})
#task 3: Calculate what the highest and lowest opening prices were for the stock in this period #create a list to store the Opening prices for the stock #There are some None type values which we need to take care of.
# day_map = {0:'Mon', 1:'Tue', 2:'Wed', 3:'Thu', 4:'Fri', 5:'Sat', 6:'Sun'}
#model.most_similar('yea.r.01',  topn=15)
# create a very simple time-series with two index labels # and random values
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON stru+cture that will be returned
#all_lum.loc[:,'plot_grouping'] = all_lum.block.map(str) + all_lum.lum.map(str)
#JSON을 파이썬 dict로 파싱
#this is a little on-off switch for my various long run-time operations that are behind a check. #go_no_go = 0  #GO #runtime items will be necessary once each time after the kernel is reset.
# Reading back in with chunks
# Plot time (x-axis) against length of tweets (y-axis)
#Add the MGD column to the dfHaw table
# export the df_events as a csv to data output folder
# get the one_way column to show if the search is one way 
#So it looks like 1014-885=129 of the original images were deleted in the deduplication, I think...
#Bakersfield
# Checking our shapes. We see that "test" has an extra column, because it still has "id". We'll drop this column next  # after using it to prepare a submission dataframe, where we'll put our predictions
#new_style_url='https://raw.githubusercontent.com/dunovank/jupyter-themes/master/jupyterthemes/styles/compiled/monokai.css'
###Make sure you have run get_weather.ipynb to pull this csv below
# Find all the plays mentioning Santos
# Compute neutrons produced per absorption (eta) using tally arithmetic
#Tells us the probablity of a Reddit will be correctly identified in the class its assigned
# continue here to extract Median Listing Price and to split the data using train_test_split()
# If there are Nan values we can use inputer  
#iterator returned over all documents
# 10-fold cross-validation with logistic regression
# As the values for converted are one of the two {0, 1} mean can be taken # to determine the probability
# check available info
# Verifying that our new submission has the correct columns
#Create predictions and evaluate
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Colorado Springs': 'adc95f2911133646
#get rid of all the PPK data with no photos and the empty fields
# 预览数据
# 'y' for 1 and 'n' for 0
# The youngest person in our dataset is 92
#list of the 100 major U.S. cities taken from this Wikipedia page:   https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population
# Import csv file from Oz
# convert dictionary to dataframe
# eliminate all zip code outside NYC and change to integer
#Dictionary of value averaged by half-year
#More Manual Calls
## 'engine' is a connection to a database ## Here, we're using postgres, but sqlalchemy can connect to other things too.
# replacing spaces with underscore
# LOAD FROM HARD DRIVE
#create the url using database_code=FSE, dataset_code=AFX_X, start_date=2017-01-01 and end_date=2017-12-31 #use string formatter to format the API key
# This will print the most similar words present in the model
# the number of reports from the most active station
# Convert Timestamp series into datetime format
# p_new under null and p_old under null and p_mean
# Create the 'country' column
#Select the flow data (4th column) for the 100th record
#열화별 날짜 변화에 따른 평점변화 확인
# Initialize the dataframe to hold all the data # Initialize an empty list
# number of iterations in simulation:  reccomended 20000
#Complete a merge of the grouped dataframe and the schools to bring in the type of school (District or Charter) that #each school belongs to. #Presents the final dataframe.
# have a peak at data that occurred before 2018-04-01 23:00
# read in data to dataframe
#train test split, standardize data
#Any funny minumum values left?
#Make 5 recommendations to 2093760 # construct set of recommended artists
# Data type of each column. #
## The QTradableStocksUS universe generally contains a greater number of assets than previous iterations of the tradable universe. ## The resulting summary table displays the mean, std, min-max of daily median in addition to number of assets in this universe:
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# experiment runのモニター
# add a notebook to the resource of summa output # check the resource id on HS that created.
#df.index.names = ['timestamp_ix', 'rank_ix']
# Display the MongoDB records created above
# read in msft.csv into a DataFrame
# get 5000 random ids
# filtrar información
# Instantiate a 2-group EnergyGroups object
# Build Pie Chart
#Extract trading volume from the list of lists, index 6
# This is the fastest way to swap a dictionary key / value order.   # This is the format gensim LDA expects it's vocabulary.
# LOADING LIBRARIES WE MIGHT NEED... # # statistics
# Concat All Events and Validation Events
# -> Being more anxious leads to worse outcomes
# find historical data for 2007
# load the model from disk
# view the start of the file just saved
#Rename the last two fields
# URL of page to be scraped # Retrieve page with the requests module # Create BeautifulSoup object; parse with 'lxml'
# your code here
# Education KOL pred
# unfortunately the index is numeric, which makes # accessing data by date more complicated
# Number of tweets in a conversation on average
# ntree_limits the number of trees in the prediction; defaults to 0 (use all trees)
# Now the Processed data
#Distribution of data across state
# backup our model # show summary of model
# Create time series for data:
# The proportion of p_diffs that are greater than the actual observed difference. # This value should be ≈ 0.9
# Turn team_slug_df into key:value pairs
#Qn 4 # What was the largest change in any one day (based on High and Low price)?
#face detection attributes
# To install package which finds arima order  # !pip install pyramid-arima
# end = datetime.datetime(2013, 1, 27)
# Option 2 just switches two index levels (a more common need than you'd think) # Note: This time we're doing an inplace change, but there's no parameter for this method either.
### we are intersted only in official posts
# Print the street number
# look for seasonal patterns using window=52
# send data to PostgreSQL 
# However, non-numerical columns are simply omitted:
# Restart runtime to allow Jupyter to know the changes above
# list all vendors with receipts
# delta represents the temporal difference between two datetime objects
# data we'll be adding to a test DB: # single shop example:
# Convert sentiment means dataset into DataFrame
# plug in to df_tick
# Print the files on one line each.
# a useful variant: dropna
# 데이터 읽기 # ====== Reading files ======
## Uncomment and complete #trump = trump[trump['source'][:7] == 'Twitter']
# q_all_pathdep = c.submit_query(ptoclient.PTOQuerySpec() #                                .time("2014-01-01","2018-01-01") #                                .condition("ecn.multipoint.connectivity.path_dependent"))
# Which were the 3 most popular trainings? (Trainings with the most learners)
#need df for on demand to creat price list
# Find Correlations
#Or just show the rows, i.e., the first item in the shape result
# Call the method 'generate_chart' from the Class 'report'  
# We load fake Company data in a DataFrame
# sort value by grade column 
# Converting my station_temp_tobs to a dataframe.
# input
# most top-N informative features
# Print some info from the first tweet:
#number of unique projectIDs in projects
#voters.loc[voters.LastVoted.notnull(),:]   # uncomment to see these 6 columns
# The mean absolute percentage error
# Here's where I apply the train test split
# look for seasonal patterns using window=52 - RN/PAs
#import FB data
# We replace NaN values by using linear interpolation using column values
# remove selected workspace #ekos.delete_workspace(user_id = user_id, unique_id = workspace_uuid, permanently=True)
# We create a pandas dataframe as follows:
# The Frequency_score column should have been inferred as a numeric, so it may contain some unwanted non-numeric data
# Or count all the entries (not including nulls or empty) in OBJECT_TYPE:
# it's important to realize that this is a series:
#Display shape of df_students
# from numpy arrays
# All of the steps above a needed to build a combined dataframe # Notice also that the date is a column
# Saves an image of our chart so that we can view it in a folder
# Querying by Team by Game (shows both teams' stats)
# Mapping using BLAST
# after comparing other models with the previous ones, # it turns out that reducing the number of features did not improve the result
# Let's check the baseline accuracy for the model. 
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# tempsApr and tempsMay are valid Series objects
# Check SFL stuff
# Reflect Database into ORM class
# Prepare Results Dataframe to output to CSV file and for the scatter plot ######media_user_results_df = pd.DataFrame.from_dict(results_list)
# Sample user
#Create df with EC2 ProductName, BoxUsage, which eliminates 1 time RI charges, and then filter on TTE_DEV #df_ec2_instance_ri = df_ec2_instance[df_ec2_instance['ReservedInstance']=='Y']
# In order of most delays: # Blue (199), Red (149), Green (110), Yellow (80), Orange (66)
# 4. Which Tasker has been shown the most?
# Run the normalizer on the dataframe
# Get Sobeys for walmart tweet #Read Sobeys tweets csv file
# Fit the model
# Make predictions
# start the Chrome driver
# split into training and test
#Importing movies dataset from Data folder
# The Matcher identifies text from rules we specify
# Count the number of tweets in each time zone and get the first 10 #pretty crazy that Central America (where Costa Rica is) is in 10th place
# emptyDF = spark.createDataFrame(aggDF, struct_v1_0)
#Describe dates
# This compares the stocks as if they were all starting from the same price 100
# Find the max and min of the 'Total Number of Rides' to plot on x axis
# 이 시점부터는 f가 이미 종료되었기 떄문에 f를 다시 사용하지말자.
#Full graph #fullgraph = nx.read_gpickle('all_tweets_pickle') #Bigraph
# output2 = output.select('label', 'features').rdd
cur.execute("""CREATE OR REPLACE VIEW empvw_20$                 AS SELECT * FROM employees WHERE department_id=20""")$
# saving cleaned files to CSV
#add create date in date format
#Drop removes a specified row or column from a dataframe
# Prefixing the my_tag dict with ** passes all its items as separate arguments # which are then bound to the named parameters,with the remaining caught by # **attrs
# Print out AUC, the percentage of the ROC plot that is underneath the curve
# Merge with library_df
#table.div(table.sum(1), axis=0).plot(kind='bar', stacked=True)
# basenames = [os.path.basename(path) for path in cris.SpeechCollector.audio_collection['audio_path']] # basenames[0] in basenames
#Run this command to debug if Service failed
# Usar csv Writer
# Data info
#Number of tweets vs length
# convert ticket_closed_date_time to a pd.datetime field
# The method .get_dtype_counts() will help me to see the number of # columns of each type in the DataFrame
## We have to break this up into ~3000 size inserts, otherwise Neo4J will crash
#### Test ####
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# builtins.uclresearch_topic = 'GIVENCHY' # builtins.uclresearch_topic = 'NYC' # builtins.uclresearch_topic = 'FLORIDA'
# scale data
# Print best error rates achieved
# in order to aggregte data from non-unique index:
# how many columns and rows do we have in data?
# Convert sentiments to DataFrame
#We are sampling the movies into 5 bins based on ratings
# Build orders based on this threshold
#use prettify to analyze
# Print all of the classes mapped to the Base\n",
#how many rows do we have with nan in a given column?
# load logistic model
# Read in csv file containing latitude and longitude information for each of the towns # Spliting each line so each town is a different element of a list
# adjust the request format:
# Importing data
# URL of page to be scraped # Retrieve page with the requests module # Create BeautifulSoup object; parse with 'lxml'
# defind simulation data
#for i in stopword_list: #   print (i)
# join information about first answer into the frame # RUN ONLY ONCE or results in duplicate rows
# Remove rows with lingering null values
# Create a cumulative balance column in BTC for gdax
# create pyspark-dataFrame
# Lets scale everyone's score down by 10% because there was cheating # You have to call collect gather the results back into the driver to view it
# missing value check
#show head of INQ2016 same for 2017,2018
# Export all tallies to a "tallies.xml" file
# Anything less than 0 means that the stock close was prior to acquisition.
# fit network
# Split our data into random train and test subsets with specific proportion
#Return a json list of stations from the dataset.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Creating another lamba function to find spaces and split the each value
#Last 5 Rows
#That cut our number of tweets nearly in half!
# 以两小时为单位显示采集速度
#Return the list of movies with the lowest score:
# summary() function failed to work. Used summary2 instead
# withlocation[1]
#Print accuracy
# show first 5 rows of data frame
# This pivot table will index on the Ticker and Acquisition Date, and find the max adjusted close.
# Number of reports from the most active station
#arima11= ARIMA(dta,[1,1,0],freq='Q').fit() #arima11.summary()
# 10. Which of the vignette has the largest mean score for each education level? What about the median?
# roi['roi_14'] =  roi_on_day(14, users_df, orders_df) # roi['roi_30'] =  roi_on_day(30, users_df, orders_df) # roi
# create a data frame and reset the index
# Extraemos el promedio:
#weight_is_relevant = 1
# The protocol version used is detected automatically, so we do not # have to specify it.
# take a look at your results
# 見えてきたのでブログにまとめるためにCSVを保存する
# read dataframe
# fig.savefig('toma.png', dpi=300)
#make dictionary
# BTC-EUR price from May 2015
# identify single column to target
#convert to date objects
# Task F answers
# write out new master json file
#How about average number of retweets and favorites per timezone?
#### Define #### # Rename id column to keep consistency # #### Code ####
#Pivot table
# a timestamp with both date and time
#Create data frame with stations, dates, and temperature
# comparison to newer files (2017-10-15)
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Use the forest's predict method on the test data
# Come back the next day skip last 2 blocks and resume here
#extract title text
#3a Conversion of columns to proper types. ‘titles’ to string and ‘timestamp’ to an an actual timestamp object.
# Delete the first column.
# Remove columns using drop ... axis=1 for column
#Save figure
#read it into a dataframe
# New dataframe for suburban data
# Show duplicate data entries
#inspect df
#Drop the redundant/useless columns
#split the data as described above #Prepare predictors and response columns
# View the last five rows of data.
# And in a similar manner, we get the trend by quarter
#https://plot.ly/python/linear-fits/
# np.where returns all the indices for which a condition is true
# Will convert to a numpy array
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Predict the values, using the test features for both vectorized data.
#las columnas ciudad y mapacalle tienen demasiados NaN como para aportar alguna infromacion relevante
# Save submission
# cisnwe5
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
#import json back into jupyter  #print(op_ed_articles['full_text'][2])
# Show all output
#7.)
# data.head()
#Delete that row of course
# count how many values you have per column # We print the number of non-NaN values in our DataFrame
#Find the lowest count of movies for a specific rating to get a downsampling size
# first, plot the observed data # then, plot the least squares line
# Add week of the year
#Remove local version of date
# "Slice" the nu-fission data into a new derived Tally
# Import Naives Bayes and 10-fold cross validation # Prepare the kfold model # Leave one out cross validation model
# parse url request to json # pretty print json; suppressed print because it's a lot of lines #print(json.dumps(j, indent=2, sort_keys=True))
#Select top 100 values using 'nlargest'
#veamos los dtypes
# Show the node_types file. Note the common column is node_type_id
# Reflect Database into ORM class
#lsi_out[list(range(50))], reduced #result = clustering.k_means(reduced, n_clusters=5)
# TODO: Create a new agent with a different state space grid
# month time period # freq M :: month
# apple_data.info
# Perform a query to retrieve the data and precipitation scores
# for rows, the same rules apply: integer indexing returns # a series, slice or list indexing returns a dataframe
# Assign the measurements class to a variable called `measurements`
# pickle.dump(ab_groups, open("ab_groups.p", "wb"))
# drop NaNs
# listings.dtypes # listings.head(2)
# Calculate n_new and n_old
# pw_attempt_check(data)
## read and print token
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# We print percentages:
# Checking shapes to make sure our matrices are congruent
# create subjid column by removing videoname # create video name column by removing subjid # change column name to specify video name
# dropping front end of curve, dampens predictive power
# Or: sales_df['Country'].value_counts()
# Transform injury_df Date field to match the games_df date field:
# calculating or setting the year with the most commits to Linux
# Drop any obervations before 2005
# output to .csv file
# Replace NaN with blank string
# %load shared_elements/system_info.py
# find all sounds of footwork' #for tracks in tracks.collection: #    print(track.title)
#Example2:
# Calculates the column-wise standard deviation, ignoring NaNs
# Call the play text the same thing as the other dataframe # Convert date to datetime
# read parquet file created with arrow with dask for compatibility check
# didn't have much luck removing columns # nor renaming the headers but we take care of these problems later
# Use the Inspector to explore the database and print the table names
# defind simulation data
# Get the tweets stored in MongoDB
#Let's get the tweets from Youngstown, Ohio
# save our crosstab
#Split the Data in training and testing dataset
#Take 'full_text' when available, otherwise take 'text'
# Max RTs:
# Make Predictions and output the results
# set datetime
# Compute the 10-day mean difference between the daily high and low prices
# 2. Each recommendation set shows from 1 to 15 Taskers, what is: # - average number of Taskers shown
# Show data range
# merge data
#gives all the row as a ndarray
# Assigns the close column from the dataframe to the series variable, close_series # prints the first five rows in the series
# missing values check
# Save the figure #plt.savefig("Sentiment_Analysis_of_Media_Tweets.png") # Show plot
#Remove bad sensor data, where a value is >4 degrees different from the 2-hour median temp. 
#Bin (categorize) the data in the dataframe and return the head.
# Numeric Columns # scale the columns back to eliminate skew using log function exception - identity column
#'San Antonio': '3df4f427b5a60fea'
#Histogram of tweet lengths
# ANSWER CODE CELL FOR TASK B INITIAL
#Save figure
# Scoring
#modelD2V.wv.wmdistance(currentData[0].words,currentData[1].words)
# I DON'T KNOW WHY THIS ERROR IS HAPPENING
# hyperparameter combinations test
# TASK C ANSWER CHECK
# keep the empty lines for now # if len(line) == 0: # continue
#make ints to simplify
#creating a column = to the index to be used as function argument
# df_2010
#Keep only [2.5, 97.5] quantiles (word count)
#s = "01/12/2011"
# 
# filter SwissProt proteins to remove species for which a proteome was downloaded
# cannot validate when a values type doesn't match the provided type_map
#Show second row
# 5.
# Info method shows the number of NaN records - more than 1400 # PD.read_csv command - parse_dates was used to convert date raw date into datetime format
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
#all plan combinations in this set of users # plans,counts = np.unique(BID_PLANS_df['subs_array'],return_counts=True) #add 'Free-Month-Trial' to start of trial timeline 
#check how many values are missing
# instead of going through output piecemeal, let's look at a summary: # print a summary of the fitted model
# slice price DataFrame to show only Germany
#Most commented posts
# OSX Users can run this to open the file in a browser,  # or you can manually find the file and open it in the browser
#读取数据
# how many rows, columns do we have now?
# Load needed Python libraries
# Add two columns A and B.
# adding prefix MONTH_ to column names
    """$     Solar Incident Radiation Based on yearly guess & month.$     """$
# r squared value
#dropped the Sec filings column using .drop function
# faster way with pandas 'to_datetime' method:
#Create Twitter basemap specifying map center, zoom level, and using the CartoDB Positron tiles
# convert your "Date" objects into datetime objects.
# Scrape the 99bitcoins.com website
# how many 0 and 1 are there in the affair column.
#Save compound data in csv file 
# Requires SCRATCH installation, replace path_to_scratch with own path to script # See the ssbio wiki for more information: https://github.com/SBRG/ssbio/wiki/Software-Installations
# calculating or setting the year with the most commits to Linux
# Plot in Bar Chart the total number of issues closed every day for every Category
#Get info on the tweet's author
#find all mentions of man/men within the posts
#prenatal pairings, all time
# write out our formatted sentence using an f-string
#read real-time data output, change column names, and convert to GeoDataFrame for spatial analysis
#Load Data
#df = pd.read_csv(reviews_file_name, sep=',', quotechar='"', encoding = "ISO-8859-1")
# Check column names # Change column name
# anchor random seed (numpy)
# Create Database Connection
# Import the model we are using # Instantiate model with 1000 decision trees # Train the model on training data
# Find all the json zips that have been scraped.
# it seems the overal quality of estimate is not that high?
#applying cleaner (now without reforming each title into a string)
#Corpus Christi': 'a3d770a00f15bcb1'
# combination of education and purpose
#  each month end of 2017
# create some noise
#Henderson': '0e2242eb8691df96'
#highest # retweets
# Connect to the database hawaii.sqlite # Import the data from the measurement table into a dataframe
#Stem words
# Insert 1 or 0 if the topics appear in the campaign
# Load the list of daily normals into a Pandas DataFrame and set the index equal to the date
# Use the session to query measurments table and display the first 5 locations
#Save a references for the ff tables:
#Let's get rid of None values for the timezone just like in the tutorial
#Initialize Sentiment Analyzer:
# The object type in the date column is a bunch of strings  # — or at least the first one is.
# get today's date
# data science retweets
# count occurences of uppercase words, which may indicate rage or anger
# Calculate h_theta -- Predictionof a row
#These orders do not exist in shopify
# favorite table # for row in table_rows: #     print(row.text)
# Add year/week
#Use unique to show what the 4 unique values are
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#train_df # all_train_df = pd.read_csv(slowdata + 'train.csv', index_col='id', parse_dates=['date'], dtype=dtypes) # train_df=all_train_df[-chunksize:]
#converts a list of JSON into Dataframe
# We insert a new column with label shoes right before the column with numerical index 4 # we display the modified DataFrame
# Dependencies # URL of page to be scraped
#Converting values to float
# A string variable. Strings are funny in NetCDF
# Double Check all of the correct rows were removed - this should be 0
# count = merge_table['S&P500 Open'].value_counts() # len(count)
# get 8-16 week forecast new patients # keep only date in index, drop time
#print(highlight(json.dumps(jscores, indent=4, sort_keys=True), JsonLexer(), TerminalTrueColorFormatter()))  # remove the comment from the beginning of the line above to see the full results # Convert to a data frame and show all products owned by this client
#create a dataframe with a random sample of the dataframe and run a test linear regression to determine accuracy
# load data
#Load engine created in database_engineering
# print a single tweet json for reference
# delete the 'to be deleted' columns # no way to do this, so create a duplicate table containing the columns we do want # now we have a time series with blood glucose
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# we can get the first value in the date column
# Read the data.
# import utils.py to download TestCases from HS, unzip and installation
# Multinomial NB model on age and subreddit 
# Glance at r_close
# Reshape the array into a 4x5 matrix
# use multi-index by specifying keys
# p_new under null 
# Displaying all data from table "measures"
#train on 20m data
#code below used to deal with special characters on the file path during read_csv()
## Make csv file
# Double Check all of the correct rows were removed - this should be 0
#Complete a groupby to determine the average reading score of each school by grade level.
# convert from string (dtype='O') to float64
# Start of time estimation for the notebook.
# sample queries 
# check the number of unique locations # should be 1 less than the calls DataFrame
# convert crfa_f to numeric value
# a query to get the Twitter place_id (code) for each city
# Load data into dataframe
# Call set_up_images
# Print the tuned parameters and score
# Job seems a good feature but needs to be converted into numbers, check how many distinct jobs DF has
# mean US temperature grid time series # mean plot
#standardizing the CIK column by padding 0's using zfill function to make it a 10 digit string which is unique for every company
    """$         create the target directory$     """$
# If we plot all 3 graphs together, we get: # Note that the length appears to be a horizontal line because it's scale (max length = 144) in comparison to  
# Which campaigns ended with positive results? 
# Create the design matrix and target vector
# Resource: https://stackoverflow.com/questions/38421170/cant-set-index-of-a-pandas-data-frame-getting-keyerror
#UTC is 5 hours earlier than EST. So in the graph below, tweets were peaked around 15:30pm
# Number of unique movies in the ratings data file
#plot weekly actual and forecast
### END SOLUTION
# Z-Scores mentioned in the article.
#Create a list of metadata rows to skip; rows 1-29 and 31  #Append '30' to the list
# last day is -2
# run multiple times to see samples # randomly chosen sample IOB tagged queries from training data
# print(address_df.shape)
#### Test ####
#normaliza os dados da versao 1 para range de 5
# Renaming the columns to date, tobs
# Only using data with labels # Splitting that subset into train and val
#Remove small tokens with less then 3 characters.
# Inspect column names 
# convert META_b when REF alleles are different
#features
# Extract the 8-group transport cross section for the fuel # Condense to the 2-group structure
# Count the number of unique values in column <landing_page>
# num_regex = re.compile('[-+]?[.]?[\d]+(?:,\d\d\d)*[\.]?\d*(?:[eE][-+]?\d+)?')
#provided dataset:
# separating new fans and returning fans
# The case of "1) variable (time, hru or gru) and more than 2 hru
#read the data (cleaned and merged)
#
# start_date = input('Vacation Start Date: Year-Date-Month format ') # end_date = input('Vacation End Date: Year-Date-Month format ')
# covert the query results to a pandas dataframe
    """Take in a vector of input values and return the design matrix associated $     with the basis functions."""$
# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})
#Get last year of precipitation data
###YOUR CODE HERE###
#flatfile generation
# (i.e., if the memory is contiguous)
#try two sample t test to see if the means of these distributions differ
# A lot of these variables are redundant, especially squared dummy variables # All of these variables listed next are 'binary' but only some are meaningful
#inspect station data
#dataSeries = pd.Series(df_Q123['Outside Temperature'])
# Notice how the data did not contain headers - let's try again, this time passing  # in some keyword arguments
# The next line causes Pandas to display all the characters # from each tweet when the table is printed, for more # convenient reading.  Comment it out if you don't want it.
# assign the cluster predictions to our customers #cust_clust.head()
# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# div의 sammy에 우리가 원하는 정보가 있다.
# Create our data frame with features. This can take a while to run.
#creating target
# Print the value counts for 'Site Fill'
### Deal with the outliers of the data convert it to the statistical mean of the data check how to do that using other attributes  ### for the time being fill it with mean 
# Plot tweet lengths against time:
"""$ Check stop-words$ """$
#get large image
# Import file
#uniformly format dates #recognize as dates for ordering/comparing #     ORDER_BPAIR_SCN_SHOPIFY[date_col] = pd.to_datetime(ORDER_BPAIR_SCN_SHOPIFY[date_col])
# convert to dict
# Creating a new field called Sepal which is the product of sepal length and sepal width # Getting the Sepal values at the 25th, 50th, and 75th percentiles
#== Using isin() method 
# Note that a single FITS file might contain different tables in different HDUs # You can load a `fits.HDUList` and check the extension names # Then you can load by name or integer index via the `hdu` option
#preprocess('DirectRunner',BUCKET, bigquery_dataset)
# Dicts can contain other dicts, like in the case of the plan:
# zona waktu as index
# NASA Mars News - paragraph
# Create dataframe
#'Tucson': '013379ee5729a5e6'
#删除其中的'系统默认好评', '此用户未填写评价内容'
#Great. Again, let's do a sanity check one more time. Let's show all rows with missing data:
# Check that there are 500 tweets (100 per news source)
# Set the X and y
#### Define #### # Convert id column from a int to a string # #### Code ####
# Merge the portfolio dataframe with the adj close dataframe; they are being joined by their indexes.
# Save all Valid Weather Data in CSV file
# Dados de localização dos postes
# Evaluate the model and print results (optional, only useful when there's a metric to compare against)
# With effect modification # drop1(adj.glm,test="Chisq")
# Split the Data to avoid Leakage #splitting into training and test sets
# name属性
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#Quandl_DF.drop(['Date_series'], axis = 1,  inplace = True)
# Topic 6 is a strong contender for 'eating'
# We print percentages:
# Train our SVM classifier                  
#To get the average per student then you need group the data by student
# Design a query to calculate the total number of stations.
'''Standardizing the Test Set with the Standard Scalar'''$ active_list_pending_ratio_test = x_test[['Active Listing Count ', 'Pending Ratio']].values$ '''Performing the fit and the transform method'''$
# Convert df to csv
# 检查请求是否成功 # r.raise_for_status()
#this is the plot for the full data range which is the default value so have not given the start and end 
# get sum of touchdowns by game day
#Let's remove all rows containing null values. 
#for i in range(5): #    print (i, X_vect[i])
# Als .xlsx für Excel speichern
# To check the size of the dataset
# get 8-16 week forecast existing patients # keep only date in index, drop time
# Predicting the  y values on the testing  Random Forest Classifier #fit on the training, predict on the testing data
# instantitate an empty list called data to store the result proxy
# Distinct Number of Users
# flight_pd.to_csv('C:\\s3\\20170503_jsonl\\flight_pd.csv', sep='\t')
# give tweepy access
# Create a pandas dataframe # Display first 10 elements of the dataframe (ie. 10 most recent tweets)
# find all sounds of footwork' #for tracks in tracks.collection: #    print(track.title)
#seaborn settings
# Validate the zip codes # Manually fix the zip codes for the 7 stations with 4 digit zip codes by checking gmaps.geocode(station 'station, New York, NY')
#gract.to_csv('gract.csv')
#save cleaned data out to csv
# In this previous case the positional index is the same as the name of the index # To see the difference, let's create a new dataframe with just the last 5 rows:
## combine data frame
# Creamos la columna nueva SA y guardamos los valores # Mostramos el dataframe actualizado
#Cuts time and keeps date.
# REPLACE PATH TO THE FILE
# use the same HTTP request... AntiNex will do the rest
# pie chart aesthetics 
#Mean and standar deviation returns
#### Test ####
# Need to determine a way to keep track of the session_id along with words... # This quick and dirty will just go through all the sessions....
# we can devise hierarchical indexes (which by itself just pulls # several columns out as indexes but does not sort or in any other  # way rearrange the rows)
# commit changes
#Wait that's so cool! Emojis show up in jupyter notebook! #I'm noticing a lot of these tweets are retweets
#now its time for forcasting the data
# join the output of the previos grouping action to the input data set  # the goal is to optain an additional column which will contain all authors of that particulair publication
# note we just counted the length of the "legs" output, it contains the details of the actual route # here is what is included in a "leg"
# Create colors for the clusters.
# simple pandas plot
# Display the data table for preview
#Save this model to a pickle file
# until ttk is installed, add parent dir to path
# check our features
# visualisations
# import modules and set up logging
# set up interp spline, coordinates need to be 1D arrays, and match the shape of the 3d array.
# Reflect Database into ORM classes #do some reflection and export our schema
#check the loaded file
# create dataframe of matrix and add to variable conmat
#Activation cohort x cohort
### read in IATTC csv file created by using Scrapy crawler spider (for Chinese vessels only)
# months: use the builtin calendar module
#wikipedia_marvel_comics = 'https://en.wikipedia.org/wiki/Marvel_Comics'
#quandl.get("NASDAQOMX/COMP", authtoken="yEFb5f6a7oQL91qzEsvg", start_date="2003-01-20")
# Log to Database
# build new df from 'df_amznnews' so as to leave that available for other play.
#We create and authenticate an instance of our new ```PrintingStreamListener``` class
# change to out_path directory
# change default figure and font size # visualize the feature importance of every variable
#### Results for 09-05-2015
# Setup Tweepy API Authentication
# Create corpus matrix
#Oh only one database :/ well too bad about database then #But I lazy to drop
# Look at 4 features: title, subreddit, num_comments, and created_utc
#Importing URLs from PC_World_Urls.pickle created by PC_World_Scraper_FindProductURLs.ipynb
# Clean up rct
# Mars facts # url of page to be scraped
# How's it looking?
# favorite table # for row in table_rows: #     print(row.text)
# add columns with automatic alignment
#### Test #### 
"""Merge LDA output and DF"""$ #Make LDA corpus of our Data #make dense numpy array of the topic proportions for each document
# What are the most active stations? # List the stations and the counts in descending order.
# - median  number of Taskers shown
# create a new DataFrame using the time frame above to select rows from the whole DataFrame
# Convert'created_at' time data rounding to nearest minute 
# Count number of stations in Measurement table
##1.4. Creating a (pandas) DataFrame # We create a pandas dataframe as follows:
#autheticate
# get columns by name
#What was the largest change between any two days (based on Closing Price)? # Note, this is interpreted to mean for two consecutive days # Why does it list in reverse order?
# calculating number of commits # calculating number of authors # printing out the results
# convert column into int type 
# check inserted records
# OPTION_CONTEXT???
# Merge csvs (2)
# for parsing bad formats # caution: recognizes some strs as dates that you might not prefer # e.g. '42' will be parsed as the year 2042 with today’s calendar date.
# нужна восьмая CUDA, потому что в Collaboratory более старый драйвер видеокарт (версии 384, который несовместим с CUDA 9)
#dates_by_tweet_count
# HC01_VC36 is mean travel time for workers (census tract level) in minutes
# fit the model
#Put all data into dataframe sentiments_df
# посмотрим на названия должностей # точечно попадаются подозрительные вакансии # результат значительно лучше, чем в анализе выгрузки без фильтрации по сфере (Data Analysis Moscow Vacs)
# Convert this cell to markdown and write your answer here.
# array to dataframe # add name
# Plot time (x-axis) against number of favorites for each tweet
# set index as the date.  #precip_data_df.reset_index(inplace=True,drop=True)
# logistic regression model # fit model
# Interpolate from the current state to the forecasts
# import calendar # calendar.day_name['2016-01-13']
# Load and peek at your data. Change the file name as needed. 
#identifying unique instances where old_page does not line up with control
# another messy  file with mess at the end
# in terminal, this will create a csv from the video
# need to get rid of that empty value in score; subbing in an average of two bracketing scores # 0.187218571
#Aggregate Compound sentiments
# perform fit
#charts.plot(df, stock=True, show='inline')
# We can view all of the classes (i.e. tables) that automap found
# set SUMMA executable file
#range of cohorts
# 1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD).
# Show Figure
# filter data yang berbahasa Indonesia
# Check is Data is imbalanced
# Create corpus # A corpus is a list of bags of words
# Check the result with built in function as cross-validation - passed
# check score.py and main.py
#Display all the columns
#Import all data available at the hackathon
# x is a date in string format e.g. 2/7/14, 12:08 PM
# converting the timestamp column # summarizing the converted timestamp column
# check Initial condition data in file manager file
#equal footing
# We can also select which data from a dictionary we want to put into the data frame # use the keyword column
# pick the top n games based on either leverage_ratio or confidence
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Rural cities average fare
# testing if my functions work #they do!
## total.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_1/target_winter2017.csv',index=False)
# Data frame is ready
sql_lagou = """$     select * from lagou_recruit_day$ """$
#most referers are the mobile app
# Train, test split data_woe
### Checking the number of dummy variables needed
# The proportion of p_diffs greater than the actual difference observed in ab_data.csv is: 89%
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Saving data
# Delete it from the original series
#show info/length of collected data
#Assign the column names to dataframe
#import libraries to display equations within the notebook
# analysis_df.count() # analysis_df.count # analysis_df['compound'].mean()
# Count hashtags only
# save the model to disk for reloading later
# your code here # use a hyphen to remove zero-padding
# Search for '#Australia'
# Nonsmoker descriptive statistics
# A:
# how many are `Literacy & Language`?
# Provides useful statistcal data on the data set
# Raw table showing all the tasks completed each week this year # gDate_vProject
# droping those rows where all the rows have na
#Peek in the dataframe
#Shuffling the data
#Here are the single tweets
# testing = pd.read_excel('SHARE-UCSD-export_reformatted.xlsx')
# We can look at the available keys:
# dropping columns '[', ']', and ','
# Lets check the tokenizing function
# test the resample dataset
# Create x, where x the 'scores' column's values as floats
# Change to text value to category 
#Storing the movie information into a pandas dataframe #Storing the user information into a pandas dataframe #Head is a function that gets the first N rows of a dataframe. N's default is 5.
# calculate the next month end by # rolling forward from a specific date
# analysis.iloc[0]['project_url']
#persistência
# Examine applications here
#we execute the query statement and we get a DataFrame as a result.
# Process a single status
# Example
#9.6 Modify Object Properties # You can modify properties on objects like this:
#YH_df["date"] = YH_df["created_at"].str.encode('utf-8').apply(parser.parse) #YH_df["date"] = pd.to_datetime(YH_df["date"])
# import matplotlib # matplotlib.pyplot.plot_date(df['newDT'], df['ED'])
# create PCA # fit oru data
# Create a string variable which will be used to get all of the different years data
#todo 1a. Display the first and last names of all actors from the table actor
# Make sure cells are full width of the screen
#Check shapes of array
#Count and plot actor 1 Name
#Create list of top10 topic_id from dataframe
#Step 2 in filtering: Applying the masks using logical combinations
# Convert Start/End Coordinates into lists
# Author: Evgeni Burovski
#high res image
# Locais no dataset de atendimentos
#Test the return values
### Step 21: Plot the graph with day of week as coloration
# Design a query to find the most active stations.  # Which station has the highest number of observations? # List the stations and observation counts in descending order
# TODO High-Cardinality Categorical  Cluster # plot(df_train.iloc[:, 35])
# use reindex
#Add a query to a dataframe #View data
# calculating single correlation between two columns is also available, however quite difficult to viz by then
# Make system a 2x2x2 supercell of itself
# print bottom three (least consistent)
# check it
# Creamos series de tiempo para datos:
#Drop the water_year2 column, now that the demonstration is over...
# Insert distance2 into df as a col # loc=11 sets the location at index11 after end station longitude # value=distance2 sets values of the column to the pd series list created above
#Then I resampled by the hour, taking the mean duration and distance traveled for every hour,  #as well as the sum of all the trips and members.
#comp.head()
# Get a random sample of 200 entries from the dataset. # Simple scatter plot
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned # Update link with start data filter, api key and data for the Frankfort stock exchange (FSE) 
#check value by print(pdiff)
# Dataframe of dates (contains 21 values)
# Station Analysis # - Design a query to calculate the total number of stations.
# Need to do this
# Which learner had the most interactions and how many interactions did they have?
# nuisance columns after groupby
# Print records in train.csv
# for testing: # hp = houseprint.Houseprint(spreadsheet='unit and integration test houseprint')
#('J:\Financial Forecasting\Development\Electricity\Load Revisions\Q2_2017', index=False, encoding='utf-8')
#Return a list of all of the columns in the dataframe as a list
# Gráfico
# For finding the first nav
# Get our Mongo on
# check option and selected method of (16) type of lower boundary condition for soil hydrology in Decision file
# convert location to numeric value
# Take a look at our new dataframe
### Fit Your Linear Model And Obtain the Results
# really, answer come so quick
# From the object 'report', list the most important features.
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# Utilize JSON dumps to generate a pretty-printed json
### Fitting the Linear Model And Obtain the Results
# Arithmetic operations between Pandas series and single numbers
# create the API
# find historical data for 2014
# String formatting, to be visited later
# Specify an end point and how many periods should come before.
# Create a database session object
# What are the most active stations? # List the stations and the counts in descending order.
# find the non-numeric feature columns
# Example
#usage_400hz = add_plane_data
# Calculate the exponential of all elements in the input array.
#Plot Histogram of residuals
#df2 = df2.drop(["Symbol"], axis = 1) #df2.set_index("Date", inplace = True)
# TO BE CONTINUED...
#making dataframe from python data dictionary 
# Simulating for old page
# In the following we are just picking up the first link that shows up. 
#Read in data from source 
# Just in case: close the reading of the LSST catalog file.
# Write hdf5 to current directory
#doc source https://seaborn.pydata.org/generated/seaborn.countplot.html
# check shape of DataFrame
# Create a dummy column that is 1 if the row represents an injury  # or a 0 if the row represents a player reactivated.
# drop rows if any col has NA
# Insert tripduration_minutes into df as a column # loc=1 will set location at index1 # value=tripduration_minutes - sets values of the column to pd series list made above
#Convert 'M'/'F' to 1/0 # Converting binary variables from strings to numbers
# Combine ET for model representation of the lateral flux of liquid water # add label 
# 使用chromedriver才可以用开发者权限 # browser = webdriver.Chrome(chrome_driver_path, chrome_options=options)
# instantiate the algorithm, take the default settings # Convert indexed labels back to original labels. #pipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,labelIndexer,OH1,OH2,OH3,OH4,assembler,rf])
# Fill in missing topic values with 0
#inspect measurement table
# Creating a list of items 'tobs_data' from our initial mulitdimensional array 'temperature_data' # tobs_data # Again, commented the tobs_data print as it was just needed to understand the layout
#Check duplcated rows by all columns at first.
# we store current working directory in cwd1, later we change working directory and use it to change it back
# Split PKG/A column into to and drop
## I am just interested in the cats that were adopted
# summary of information about the data frame 
# 显示抓取速度的频率
# Create the Dataframe for storing the SQL query results for last 12 months of preceipitation data
#Create the Pandas GroupBy object
# subset predictor X and y
# RE.FINDITER
# see an example for a given author (e.g. Zhou Shisheng)
#install MXNet
# Load the last statepoint file
# define "db" and "coll" variables for convenience
#Qn 2 #Convert the returned JSON object into a Python dictionary.
#e6e6c9791516c2c3 -- original hash #e6e6c9791516c2e2 -- new hash, off by 1
# get max probability column name, this is final prediction for validation set
    """$     create a new column: 'location'$     """$
# Retrieve page with the requests module # Create BeautifulSoup object; parse with 'html.parser'
# read in the dataset
# run the model giving the output the suffix "rootDistExp" # if user doesn't have executable file or executable file don't work on your local computer, use run_option ='docker_develop' #results_sim_rootDistExp, output_sim_rootDistExp = S.execute(run_suffix="sim_rootDistExp", run_option = 'docker_develop')
#print lxml.html.tostring(item)
#final_member_pivot
# write to db
# make sure each game as only one label # if there is drop the last the was given
# Scalar ranges can be set with a simple tuple: # Or more complex ranges can be set using one the of the ranges objects:
#identifying the number of times new_page and treatment line up
# Transform the list of series into one series # Merge the series to free_data dataframe
# find each of the inconsistent rows in this horrid table
# Stats about Tone
# make predictions for those x values and store them
# City and address columns are correctly filled
#Example 5: Specify dot (.) values as missing values
# extract temp grid as a 843 by n matrix # check shape
# set SUMMA executable file
#cut out only low redshift objects for bad seeing
# Check for null values
# your code here
# Create fuel assembly Lattice
# convert crfa_f to numeric value
# Create an index transformer that calculates similarity based on  # our space # Return the sorted list of cosine similarities to the first document
# To get you started, here is how to test the 5th scanned value.
# from data frames expects index 0,1, ...
# dummy all the subreddits
# Extract data for a specific point using the coordinates 
# For getting all the text (paragraphs)
# Pandas provides a simple way to create data sets using DatetimeIndex. # An easy way to create one is using the pd.date_range method
#Cuts instances with more than 10000.
#Using the statisics library
#converts to dataframe
#overlay the points of delay with a map of Chicago
# let's build a model with GPU
# %load ../solutions/sol_2312.py
# This will actually build the network and determine which nodes are connected to which.  # Until now, only the rules were given and stored.
#setting up 10 commandments chapter to see how the summaries compare
# get terms most similar to cantonese
# create train and test
# a Series can be added as a column to a DataFrame
### read in IOTC csv file scraped from iotc site (for Chinese vessels only)
#Example 2:
#Display the row's columns and data in dictionary format
#preview output of first element
# Verifying that we have compatible shapes of correct dimensionality
#Transforming the files3 dataset
# test_preds_df.head()
# read csv
# see data in tabular form!
# rename the column names  # MAKE SURE THE RENAMING IS OK AS THE ORDER OR COLUMNS MIGHT CHANGE
# wide to long # view head of final row-wise dataset
# Mars Facts
#plt.plot('date', 'tweets1', label='tweets')
# find historical data for 2010
#Prettify indents the parsed page in a 'pretty' and readable format
# reflect an existing database into a new model # reflect the tables
# df_2014
#Estoy levantando 100 posts
# Categorize Gender Column by Sex # use map method
# Use Pandas to calcualte the summary statistics for the precipitation data
# Checking the numbers of polarity, subjectivity and Sentiments of the first 5 tweet.
# Counting the number of null values
# Convert the returned JSON object to a Python Dict
# df_lib_con.title = df_lib_con.title.str.replace('\d+', '')
# convert to categorical the AGE_groups collumn.
# dev4['rank'] = dev4['avg_rank'].rank()
# Take a peak at the first couple of rows in the dataframe
# Load the query results into a Pandas DataFrame and set the index to the date column.
#Example 2: If no header (title) in raw data file
#  Illustrate the KEY IDEA by our helper function: #  which outputs an array of three random values from poparr. #  Rerun this cell, to see how resampling would work.
#Joined(train,test)-weather (checking if there was any unmatched value in right_t) ##Note: Level of weather data is "State-Date"
# read from google
# Get metadata documents
### list of column names
"""$ Run preliminary test of classifier to get baseline accuracy$ """$
#url for mars table
# count by date in datetimes
# find campaigns with under 50 users # remove users with those campaigns, and those with 'unknown'
# Encontrar los registros que se encuentran entre las posiciones [10mo - 20vo] de mi DataFrame
# Counting the no. commits per year / without using the index # Listing the first rows
#read in the schedule file
#Get data from Quandl
# train again with the best rank 
# Initialize map
#print(dataframe.shape)
# Add cleaned tokens as an attribute to by_tweeter:
#Compute counts and plot them as a bar chart
# define user ratings
# 136 records removed
#CHURNED SLICE, activation dates
# Adding a column for sentiment    
# Convert the dataframe into proper format which fbprophet library can understand
# # CREATE DUMMIES # # Month
#==============================================================================
# Divide each number of each countries columns by it's annual maximum 
#check how many values are missing from the data
# And the iloc method for selection by position
# Create a list of filenames for all the years data 
# In jupyter notebooks you can prefice commands with a ! to run shell commands # here we remove any files with the name of the file we are going to download # then download the file
# Extract Projection Information
# copy the datasets for cleaning
#We extract the mean of lengths:
#4c. Oh, no! The actor HARPO WILLIAMS was accidentally entered in the actor table as GROUCHO WILLIAMS,  #the name of Harpo's second cousin's husband's yoga teacher. Write a query to fix the record.
# Select only the date and prcp values.
# Create a new DataFrame with an entry per sentence for the training set
# view the leaderboard
# Instances have a .__class__ attribute that points to their class.
#query tables to get count of daily report, all temp data is complete for each record, so the count #reflects a count of a station giving temp data, prcp data may or may not have been reported on that date
#Summarize the MeanFlow_cfs data, using default outputs
# Global Variables for eacch file # Empty List to store the cumulated jams and irregualrities data
#The train stations names need to be cleaned up
# import data # show top rows
# Retrieve the NuFissionXS object for the fuel cell from the library
#group ride data by city # average fare per city # number of rides per city
#select by day
# Give it a second to load the page
# Replace the negative values with average values
## Transaction: Title
# read the oracle 10k sections
# your code here
# OVERALL SENTIMENT ANALYSIS BY NEWS SOURCE # Create dataframe that calculate the overall compounded score to place on the y axis
# creating dummy variables for DEVICES
# Predict probability estimates instead of 0/1 class labels
# Initialize dictionary # Create dictionary, with dates as the keys for the data in each column.
# Gets historical data from the subscribed assets, the last 360 datapoints with daily resolution
# Get sentiment for Costco tweet #Read costco tweets csv file
"""$ Take a small sample for plotting$ """$
# Same for values in 'E' column 
# Calculate the date 1 year ago from today
# Calculate what the highest and lowest opening prices were for the stock in this period.
# Fix incorrectly geocoded coordinates
# Split into test and train
'''based on what's seen in this column, there are about 862 null values for 'Footnote' but the rest are '*' values $ so it would probably be best to ignore this column for our analysis '''
# Loop Media Sources 
# print(pd.Timestamp.min(df1['created_at']), pd.Timestamp.max(df2['created_at']))
#mentions[0].text
# Optional: Create an output folder and write dataframe out to CSV
# Retrieve tone of each ticket
# Read in 2018 data
# what is bins=100?
# check inserted records
# create a new table which encorporates the necessary values for the bubble chart
'''Evaluating the model with the Test (Hold Out) Data'''$
# create pandas DF
# intentionlly set to display the entire data-frame
# return the rows where the temps for Missoula > 82
# Check to confirm distance was properly added to our list
# Creating the list of files in the folder
# builtins.uclresearch_topic = 'HAWKING' # builtins.uclresearch_topic = 'NYC' # builtins.uclresearch_topic = 'FLORIDA'
# word2vec expects a list of lists. # Using punkt tokenizer for better splitting of a paragraph into sentences. #nltk.download('popular')
# 7.What was the median daily trading volume during this year?
#\xa0 is  non-breaking space in Latin1 (ISO 8859-1). replace with a space
# Transform output into a classification task.
#Calculate the average daily trading volume during this year
# The user "Rezeptesammlerin" seems to have many duplicate recipes # How many recipes does she have in total?
# defind simulation data
#filecount
# Data correlation # corr method gives us a correlation between different columns
# Rural cities ride totals
# final sise check
# df_2013
# save our final DataFrame as a CSV file
# Count hashtags only
# plot histogram of the counts
# ciscij10
#etsamples_100hz = etsamples_100hz.reset_index() #etsamples_100hz.loc[:,"eyetracker"] = etsamples_100hz.level_0
#origine incident is not independant from target
#reindex the DataFrame
# Create a copy dataframe of the origin # Collapse the dates # Showcase the new copy
# New Pandas DataFrame with a new name of the field including the Sentiment Analysis results (SA)
# before I split I want to understand my distributions of my predictors
# create a csv of the url list #print(df.head())
# predict on test set
# Descripción estadística
#set up geometry field for delay_geo.. right now, there are only lat/longs
# Your code goes here #optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) # pass the loss function that optimizer should optimize on.
# Give the file a meaningful name
# The 13 most liked tweets # ... Found by extracting a number from the Likes.max() number, providing a list of  # tweets with likes between that number and .max()
#Read json with pandas and inspect
# Grouped by date 
# separately save the meta_data for each order # then we can add it back later
# df_2011
# dataframe of positive examples
# Train the classifier Naive Bayes Classifier
#sort by [0] to agg same plan_permutations
# Authorize the repository client
# Cria um dataframe vazio e adiciona os arquivos de cada candidato
# Create connection to the read-only DB
# pie chart showing percent of total rides by city type
#check that not all the values are 0
# find all message with @someone
#Using a for loop(s), compute the sum and mean for each column in scores.
# Setting up plotting in Jupyter notebooks # plot the data # ... YOUR CODE FOR TASK 8 ...
#print(os.path.abspath(os.curdir)) #print(os.path.abspath(place))
# Different entries for each index
# create start date column from period index
# let's use tab next to the "." and see the string propierties
#df.isnull().sum().sum()
# 1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that  # the date format is YYYY-MM-DD).
# Load in the csv data #headlines_df = pd.read_csv("../data/headlines/labeled_headlines.csv", index_col=0, parse_dates=[0])
# Call sns.set() to change the default styles for matplotlib to use Seaborn styles.
#print all paragraph texts
#2018-04-03T23:53:00.726Z
#Note : The time column should be of unix format
#'Virginia Beach': '84229b03659050aa'
# いいね数とRT数が多いツイートの特徴を分析したいので、いったんいいね数もRT数も多いと判断できないうんこツイートをすべて削除する # いいね数が10未満のものをうんこツイートとして定義する
cur.execute("""DELETE FROM coindesk;""")$
#Converting it to Pandas df for better view
# Drop users who don't have at least 4 days # Keep only seqn for joining purposes
# slice as column index: a dataframe
# Create a cursor to do SQL queries. The cursor is the object that is used to do  # queries to the archive
# tweets
# view the fees by year|
# Save file
# Don't forget to drop nulls! 
# Full_text column was checked carefully # 22699 tweets were the exact same tweet, available at verizonprotests.com to be posted by one click
# Predict
# It seems younger people mostly performed activity.
#Frequency count
# describe num_words field
# convert a sequence of objects to a DatetimeIndex
# determine the order of the AR(p) model w/ partial autocorrelation function of first difference, alpha=width of CI
# flight6 = spark.read.parquet("C:\\s3\\20170503_jsonl\\flight6.parquet")
# Retain only traffic reading at 6AM and 12PM
# Renaming column
#Join list values
# The correlations are the following
# Predict the rating of a known B class article.
## numbers of user_ids
# extract matrix for fire heatmap
# Write to CSV
# compute difference from original dataset ab_data.csv
# Store the cross section data in an "mgxs/mgxs.h5" HDF5 binary file
#url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY + "&start_date=2017-01-01&end_date=2017-12-31"
#'Tulsa': 'cb74aaf709812e0f'
# get the dataset we just uploaded
# We Create a DataFrame that only has selected items for Alice # We display alice_sel_shopping_cart
#creating a spark context file and then reading the log file using textfile() method.
# remove items shipped before our start date # remove items shipped after our end date
# merge the master dataframe and the dataseries dataframe over the key and drop the key. 
#Example 1:
filter_special_characters('Hi//)(&(/%( \n\n\t)))""""""!!!.')
#establish a conenction
#Drop all of the unnecessary columns for thinking about reliance on conspiracy
# Specify where the dataset is saved and the name of the file
# And we come full circle! We encode the list we created in  # a json string. We could then provide that over the internet # in our own API!!
#conn = sqlite3.connect("/tmp/coindesk.sqlite3")
# Total dates
# export
# Cargamos hoja de calculo en un dataframe
# Convert test_age to series; set to new variable 'age' we add to df
#forest_clf = RandomForestClassifier()
# De paso recolectemos aquellas columnas que tengan o indiquen un formato  "date" # Estas las podriamos utilizar de ser necesario en la importacion.
# Access a collection (create it if not exist)
# get just a time from a datet ime
# If Q1 = yes; add 0 years to Q1A answer # assigning each answer to unique Q list
# we now plot on the KaplanMeierFitter itself to plot both the KM estimate and its confidence intervals:
#group["C"] # group['C'].shape # group.keys()
# Nodes files
# imprimir a soma de todos os ganhadores da sena  # imprimir a soma de todos os ganhadores da quina # imprimir a soma de todos os ganhadores da quadra
#tweets = pd.read_csv('tweets_mentioning_candidates.csv')
# Clean up the dataset for viewing purposes
# adding prefix VIP_ to column names
# Tous les évenements du 10 janvier au 11 janvier (exclus)
#Select records for 2017 #Group by site & compute mean TotalN
    '''Function creates a new data frame with date as the index and the sentiment score $        as a column.$     '''$
# cancatenate with duplicate indices
#The train stations names need to be cleaned up
# gpByDate.plot()
#Converting Data Into Numpy
#We need to forecast the stock price - here we need to forecast out the 1 percent of the valu #0.01 says next 1day prediction into the future
# Number of unique rows
#Save figure
# TODO: plot discriminating over position taken
# count number of deaths in 2014:
# Creation of connection to our mlab instance
# import the data and concatenate into a single pandas dataframe
# формируем список стоп слова
# Summary of results
# Make a copy of the game data to work from for EDA
# This should read datetime64 for Date, and float for everything else.
# write the excel data to a JSON file
#query data
# Use Pandas to calcualte the summary statistics for the precipitation data
# wall time to train model
# remove stop and other words, links
# reflect an existing database into a new model --> creates a Base object # reflect the tables --> creates ORM objects for each table in the 'hawaii.sqlite' database
#goal: 
# Categorize time labels
# First, find a tweet with the data-name `Mars Weather`
# Statistical Summary  of Precipitation. 
# top 5 breweries by total beers drank
# Save and show plot
# We display the updated dataframe with the new column:
# Use Pandas to calcualte the summary statistics for the precipitation data
#Cleaner display
#Review the dataframe that has all of the merged data (schools and students)
#Saving features in csv
#check name for consistency
# See first 10 stations that are not shared
#### dumping dict of data frame to pickle file
# Show first 10 records
# make new dataframes with new and old data
# groupBy function usage if you want to not have the columns you are groupBy to become the index.
# A tweet with just two characters? 
# Reactions anzeigen
# index date
'''Validating the Score with the training set'''$
# 6 blocks x 20s
# Instantiate the count vectorizer with an NGram Range from 1 to 3 and english for stop words. # Fit the text and transform it into a vector. This will return a sparse matrix.
# Normalized the data
# compute the number of mislabeled articles
#### Test ####
# Observing the numeric details...
# analyze validtation between BallBerry simulation and observation data.
# Write to CSV
#El Paso': '6a0a3474d8c5113c'
#NASA news site, pull latest article text and date
# Query to calculate the total number of stations # Counting and grouping operations in SQLAlchemy
# coordinates to a 2 dimension array # check dimensions
#Remove header row
# Drop duplicated user
# Calculate the total number of stations 
# outlier points
# Add the topics columns back onto the original dataframe
# You can then convert the np array to a list
# Creating lambda function to map datetime transformation to datetime columns
#print the final parameters
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
# Replicate booking = 1 by 4 times
# 68 original columns # 10 are empty # 
#I did not know that the ".values" is what outputs #the values in a numpy form !
# create a Series with a PeriodIndex
#A different approach using apply() method and a lambda function:
#Stories that in Code Review/Testing or Approval in reln need to be flagged
# todos os concursos de 2017 # soma dos valores da sena distribuidos no ano de 2017
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Last check for NA values
# Show the clean description of the ticket
#  toar() is for conversion to numpy array:
# number of tweets labelled as mortality salience during missile threat crisis period
#The dtypes attribute reveals the data type for each column in our DataFrame.
# what beers have I drank from To Øl? I can't remember
#separate numeric and object variables
#df_insta.dropna()
# We also load avro-transitions.csv
# total time of active projects
# NOOOO OOOLD
#Features #Targets
# Use Pandas to calculate the summary statistics for the precipitation data
#col_labels = ['Date',,'Lang', 'Author','Stars','Title','Review','Reply',]
# Reference # https://radimrehurek.com/gensim/tut2.html
# The article URLs provided for this project. # Note that this script will work with other articles that share the same HTML layout. Just add URLs to this list.
#title_alt = imgs.find('img')['alt'][2]
# Group by twitter account
#setting driver up
# Import Dependencies
# Loading a JSON file
#Best Parameter by GridSearch
# load the corpus
#for prophet to work, columns should be in the format ds and y
# keys should be all matched keys up to date
# Performance of the testing model 
# A:
#Save Data into CSV
#df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list('ABCD')) #df = percipitation_2017_df.cumsum() #percipitation_2017_df.cumsum()
# # data = pd.read_csv('xclara.csv') # print(data.shape)
# Print the shape of df
# read the csv from S3 # display the first 5 records to verify the import
# # This shows a chart of the count of observations by month # 
# We create an extractor object: ## https://developer.twitter.com/en/docs/api-reference-index
# so roughly 25% of data should fall outside either quartile - note rougly, since we have categorical data here # below lower quartile
#'Modesto': 'e41805d7248dbf1e'
# Define endpoint information.
#Sort our recommendations in descending order #Just a peek at the values
#If you input a dictionary, it will parse the key as the column header
#Convert the ActivityStartDate to a datetime object (currently a string)
# All rounded players..
#Convert the date column into a datetime
# print the row at the fifth position
#active
# figuring out how to sort friends_n_followers by number of friends
# let's use the model we created to make a prediction:
##Prediction:
# remove cancelled, suspended and live
#### ###
# Choose the station with the highest number of temperature observations. # Query the last 12 months of temperature observation data for this station and plot the results as a histogram
# some basic plotting
# discretizing data also results in categoricals
# Visualize the learned Q-table
#Since there are two weather data files, I first merged them into 1 file #Check to see if they have the same shape
# examine the huge bar on top, consisting customers who made their the first and last purchase within 80 days
# Add work day to df # # Encode user type # start_df.UserType = start_df.UserType.map({'Member': 1, 'Casual': 2})
# Look in the utils library to see how we implemented the solution.
# drop dry because it is really common #df7.loc[df7['avg_dew_point Created'] == 'dry', 'avg_dew_point Created'] = np.nan
#read_hdf('store_tl.h5', 'table', where = ['index>2'])
#powers = np.linspace(0, -30, 31) # короткий скан
# We replace NaN values by using linear interpolation using row values
# Pring filtered df for station_distance for : # start station name, end station name,  # start coordinates, end coordinates
# создадим отдельный столбец со сферой бизнеса
#doing business as name
# サポートしている項目の一覧表示
# Find one reinsurer-AIG treaty:
# Find unique users # Check for not unique users
# get compensation of employees: Wages and Salaries
#Indianapolis': '018929347840059e'
#specifying the huerestic and the maxlen yourself makes this run faster the utility doesn't have to calculate these for you.
# subset us lon # subset us lat # print dimensions
# Number of populate for old Page
# Print sentiment percentages
# Take a quick look at the data
#Show the rows corresponding to index values 6 thru 10
# skip 100 lines, then only process the next five
# sum the results
# 1965 - количество текстов в которых встречается 1464 ключевых слова в разных комбинациях 
# messy file
# Use `engine.execute` to select and display the first 10 rows from the table
#Retrieve the title for all NASA articles
# Calculate precision and recall scores with sklearn
# おおい順に並べる
# verify that 6 sets have been removed from all_sets
# From the docs: "Max id returns results with an ID less than (older than) or equal to the # specified ID. 
# using seconds as unit for more precision
# Imports the Google Cloud Client Library. # The client must be created with admin=True because it will create a table.
#Display answer as a Series
#how many rows do we have with nan in a given column?
# load workspace configuratio from ./aml_config/config.json file
# List the primitives in a dataframe
# Total hours
# Import and Initialize Sentiment Analyzer
# View the data and think about the transformations needed.
# Drops the specified column from a dataframe # You'll be able to see that the volume column is no longer there
# Get help
# check if any values are present
# Drop NAs
# -a show attributes, -v verbose
# Stops that are in oz_stops but not in stops # Note: these stops do not exist when looked up online!
#Design a query to retrieve the last 12 months of precipitation data.
#save most recent tweets
# Plot toy dataset per feature #sns.pairplot(df, hue='labels');
# Save the workbook.
# Plot of tasks completed each week by energy level # cust = itemTable.groupby(['Energy'], as_index=True) # cust.get_group('Normal-energy')
# print(i1)
# Query for precipitation data based on date range from most recent to a year before # Create data frame from sql query
# From the original merged table, count the values by type of city
#r_clean[::int(len(r_clean)*.1)].sum(axis=1) #hist_alloc[::int(len(r_clean)*.1)].sum(axis=1)
#show all the geometry levels we can use
# if you want to conert period index to date time index # since this is quater so it will take starting date of quater
# 120 date/time series with 1second resolution
# 'None' is treated as null here, so I'll remove all the records having 'None' in their 'userTimezone' column # Let's also check how many records are we left with now
# URL of page to be scraped
# 100 X 100
#inspect measurement table
# We print the index and data of Groceries
# execute the info() function
# check value counts # pd.value_counts(RNPA_new['ReasonForVisitName'])
# sort by column BG
# Locais no dataset de postes
# Dados de BO Roubo de Celular
# Find total number of stations
# we will remove a number in our array
# Difference of simulated values
# Performing a one tail test using statsmodel
# results 
# There is missing data again, so we drop them as well:
# Apply unix_to_datetime function to every row in the coinbase_btc_eur dataframe
# 学生的comment（唯一会暴露教师姓名个人信息的部分，完全可以不要，到时候drop掉这个comment列）
# return the requested page into a json object
# display the unique values with counts for issue_type
#Example 6: Load a txt file while specifying column names
#Reset the index from all the row dropping
#finding the important features
#Converts time stamp to date and time.
# Create capture object for live capture
#Checking the new csv
# Extract title text
# pass the url and the season to the passing method in the GameLog class.
dbquery = '''select * from mobileos$             join mobilebrowser on mobileos.id_os = mobilebrowser.id_brsw'''
# summary statistics
# Re-format dates in date column # Rename columns before final export
# experiment run_uidの取得
# First, import the relevant modules # only need json_normalize from pandas
# Most favorited
# We'll start with LSI and a 100D vector
#calcuate the age of the stories in the last state it is in
# Compute fast fission factor factor using tally arithmetic
# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')
# Lê os arquivos
# see an example for a given author
#Compare shapes of the two dataframes
# # convert date columns to datetime 
#lets look at the first 5
# Add a new column to data file #Make sure to modify addFarm function on adding new column
#We have scaled the data inside -1 to 1 to again -1 to 1. May not make much sense in this example, but for other real data we might want to transform and re-transform.
# how to create a class using type
# Use Pandas to import a csv file into a dataframe # Check the dataframe to see how many rows and columns it contains
# To create a column for the intercept # To create a dummy variable column #df2[['new_page','old_page']] = pd.get_dummies(df2['landing_page'])
# UniProt mapping
# Find all the tweets mentioning "SANTOS"
# determining the last sensible commit timestamp
# Predictions for the first customer across the first 5 items
#Create series with just the closing price
#Fresno': '944c03c1d85ef480'
# Get the dataframe
#== interactive browsing 
#Import Random Forest libraries 
# Perform a linear regression. #model_x.summary() # For non-categorical X.
#installing pandas libraries #There is a bug in the latest version of html5lib so install an earlier version #Restart kernel after installing html5lib
# Concatenate Coinbase and GDAX BTC wallets
# time 有none
# Check every in_response_to_tweet_id only contains one float value 
# merge onto weather features
# due date distributed
#create time series for market difference 
#fetching the first 5 entry 
# Temporarily save the data, to avoid having to run the first bits of code  # again in case this notebook needs to be re-run/debugged...
# Delete rows/columns # pop -> columns # drop -> rows and columns by using axis
# Check the rating of a known FA class article.
# Get sentiment for walmart tweet #Read walmart tweets csv file
# days
# join tables - method 1, but hard to do c
#Sorting by values
# Now, we're ready to start streaming!  We'll look for recent tweets which use the word "data". # You can pause the display of tweets by interrupting the Python kernel (use the menu bar at the top)
# Removing one of the duplicated rows from df2
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Filter and create a new RDD
#def drop_rows(index, df): #    tweet_text = df["Words"][index] #    df.drop(index, axis=0)
#Get a count of all records from the dataframe's shape (rows, columns)
#applying nupy math functions without losing datastructure
# merge the the newly imported "numberOneUnique_df" with the original "spotify_df" df one to id the songs that make it to # 1
# Mapping using BLAST
# Education KOL
# convert column of dataframe to categorical
# Verifying correct tweets from each source
# how many values less than 6 in each row?
# 12. Print rows 100 to 110 of the free1 Data Frame
# 24 + 4984 = 5008 # found 8 dealers listed with both HYB and STD
#sentiment analyzer #pip install vaderSentiment==2.5
# Apply the count function # Seeing what DummyDataframe look like
# Create a minimum and maximum processor object
# Set a starting and ending time. #end_t = '2017-12-31 23:59:59'
#Urban cities driver totals
# Importing data from the processed 'news' DataFrame.
# from sklearn.feature_extraction.text import TfidfVectorizer
# Reflect Database into ORM class
# Cleaning the speech text
# your code here
# check a single (or multiple) posts
# Export the data in the DataFrame into a CSV file.
# drop rows with NaN
# ROC-AUC curve is its own Class. That seems kinda weird.
#Check via customer sample
#this is p tag looking for, but not for tweets with image
#We create a column with the result of the analysis: #We display the updated dataframe with the new column:
# only have partial jan 2017
# make a resource public
# post simulation results of simpleResistance back to HS
# wide to long # view head of final row-wise dataset
# To know the value of country count
# Appending train and test to get full dataset for cross-validation
# Max FAVs
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Find columns with null values #new_df.isnull().any()
# 74 comments or less is encoded as 0, 75 comments or higher is encoded as 1
#reindex the table
# First, replace NaN with NA in status_message
#Max and min time
# We print some information about Groceries # attributes: shape, ndim, size
#if 5668 birthdays recorded, why only 5552 scn_age generated...oh, NaT #count/len(SCN_BDAY) users provide their baby's age
# Information about the independent factor w?
# change date to datetime # sentiment_df.head(100)
#sort by BBC in order to create different color scatter plot
# Add code here
# transformation primitives
# exog = sm.tools.add_constant(exog)
#TODO
# Timestamp conversion 
## 3. print the median of traded_volumes
#perform delete
#If you ran the upper code, the data can be directly converted to a Spark DF #Else, you can just use a pre-parsed .csv file available in the next codeblock
# Summary statistics
# wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'
# 3. How many total unique Taskers are there in this data sample?
#select the latest sprint that the stories are in and then we can filter the ones that sprints that are closed. #only after the above is done, we can filter the stories that have their latest sprints closed
# Now to import an example I made
# get cat sizes
#show result
#pivot table showing the years of married and affair status
#Plot the results using the DataFrame plot method.
# We append store 3 to our store_items DataFrame # We display the modified DataFrame
#df_4_test.dtypes
#print the first five rows of theft data
# group bar chart   分组柱状图 # 同一副图中添加新的柱状图 # 注意，为了不覆盖第一个柱状图，需要对x轴做偏移
# Create copies of original dataframes
#if running demo, uncomment this and change test['id'].vaues -> test['index'].values
#not required
# coefficient of determination R^2 of the prediction.
# top chatter over time
# count the gene id
# population or sample size of treatment group
# Look at #535, to see if the cleaner function got wrong stuff out
# We change the row label from store 2 to last store # we display the modified DataFrame
# flight_pd.head() # help(pd.read_csv)
# 6. What was the average daily trading volume during this year?
# Write an aggregation function that finds the mean price per minute and the total volume per minute - output this to a new DF
# And this is a plot of these:
# variables = arr[0].keys()
# print the R-squared value for the model # R-squared: proportion of variance explained, meaning the proportion of variance in the  # observed data that is explained by the model
# df.loc[df['column_name'] == some_value] # Reset the index numbers
# boxcox
# rename column names for readability
# Check for NaN values in CAND_NAME
#tweetdf = pd.read_csv("tweets_w_lga.csv")
#go to Mars weather's twitter page. 
# Set data for 2018 & 2019
#'Milwaukee': '2a93711775303f90'
#Downloads prehosted dataset from Dropbox
# print(host)
# sentiment analysis
#'Glendale': '389e765d4de59bd2'
## Sleep on rate limit is false because I check in my code
# Pickle Reader For Exported Data Frame File # /r/news, top 200 threads for big sample testing size. Data From November 6, 2017 # change directory depending on user
# print(df_final['created_time'][0].split('T')[0])
# a series
# Upload the raw Excel files exported from AkWarm Library.
#调出星期几(注意这个星期几有出入)
# obtain the value of the variable
# Look at one of the ticket
# Now we use the Base class to reflect the database tables
# merge weather and 311 #df5 = pd.merge(df4,df2,how='left',left_on='Date Closed',right_on='date',suffixes=('_created','_closed'))
# Note 1: This Notebook is an exercise by following Walker Rower tutorial at https://www.bmc.com/blogs/amazon-sagemaker/
# End of time estimation for the notebook.
# Can you tell me the average low for the week?
# re-arrange cols to original order
# Import qgrid library and set some options for optimal display of tables with # big number of columns. Also, do not allow the widget to edit the values of a # dataframe
# favorite table # for row in table_rows: #     print(row.text)
# google
# M = Month end frq and MS = month start freq
# df = pd.read_csv("../2016-17_teamBoxScore.csv")
# Drop rows with duplicate names
## Time series
# Use csv Writer
#compare summaries for Matthew 1-- one of the chapters describing birth of Christ
# Average all polarities by news source # View the polarities
#'Scottsdale': '0a0de7bd49ef942d'
# Verify
# Perform a query to retrieve the data and precipitation scores
#session query
# make sure all column names are the same and in the same order
# import utils.py to download TestCases from HS, unzip and installation
# remove data that do not have place object
# cisuabe5
# Setting up plotting in Jupyter notebooks # plot the data
# Transform Burgers vector to axes
# the most popular words which might contain useful information in the reviews # apperantly we need to update the stopwords list # here I update 'aa' in the stopwords list
# Repeated part from morning track: fill with media or mean example so can match the orginal cleaned data. 
#Prepare our data for plotting 
# mean and standard deviaton of my ratings
#print (c)
# Using the input folder variable specified at the beginning of the code, and the string variable specified above # Create a variable containing the filename 
# Finding unique user_ids # Checking for non-unique users
# Print the head of airquality_melt
# create a period index representing # all monthly boundaries in 2013
# Check proportion of calibrated minutes vs not: 1 id calibrated, else 0
# We display the total amount of money spent in salaries each year
#  and save a reference to those classes called Station and Measurement.
# may I know how to see attribute of every object in notebook? # . + Tab
#TRANSOFRM THE PARAMETERS BEFORE GOING TO SEARCH 
#r= average_daily_return(data,portfolio=True) #print(r)
# your code here
# Plot all columns as subplots
# datetime of the tweet object is in UTC (Universal Time Coordinate) # so I generate a column for the time of the tweet in Pacific timezone
# predict on the last two months
# Access a row
# And add back to the original dataframe
# Sort the data by time, so rows are in chronological order
#get data from the csv directly instead of running it till now
# resample data to weekly
# your code here
#Converts df slice to demographics sql table
#sorting by an axis
#Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017
#create a function to normalize the data #apply that function to each column of the dataframe
# p.242 Processing documents into tokens # split the sentence/corpora into individual elements
# check current directory
# Separate response variables from predictors # Split the training data into training and validation sets for cross-validation
#Example 7: Import File from URL
# merge the datasets to compare gene_count and chromosome length # convert a series to a dataframe # merge datasets
# Leemos el archivo. Veamos si tenemos algun inconveniente...
# with start- or end-date * periods:
#convert all columns
# Groupby average compound sentiments by Symbol
# read in the file 
#Import the data set into dataframe
# to get the last 12 months of data, last date - 365
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# leadsdf = leadsdf[['_id','firstName','lastName','phone1','ingestionStatus','RVOwner','address','city','email','entryPerson', #                                    'submissionFile','filename', 'participant','program']] # leadsdf
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#create and display cnn sentiments datafrmae
#New Col added
# fitting our reducer to the full dataset # NOTE: takes 18 minutes on my local 8GB machine
#Display the columns and their data types
# path for writing to csv the merged dataset: /home/sb0709/github_repos/bootcamp_ksu/
# print("Accuracy = %g" % accuracy) # print("Test Error = %g" % (1.0 - accuracy))
#df_times.describe()
# get repeats gff
# Display of first 20 elements from dataframe:
#merge of ordered timeline-shards no longer ordered for these users (iloc USER_PLANS)
# df_sum_total 的 sum_total 指标 merge  df_detail
#bow_test=pd.concat([bow_test,data_test_model],axis=1) #bow_test.sort_index(axis = 1, ascending = False) #bow.sort_index(axis = 1, ascending = True)
#one can ignore this cell if ANN is not required
#merging the particpants column into the base table
#turn weekly series into dataframe and add prediction column
# RETRIVE TWIT IN SHORT MODE (as-is)
# Copy original features for use in Pipeline
# get release data
# To access a particular row, we can use the index name:
#Example2: Import File from URL
# a timestamp representing a specific date
# Average weight by position, from heaviest to lighest
#'Pittsburgh': '946ccd22e1c9cda1'
# 9. 
# Get all of the SamplingFeatures from the ODM2 database that are Sites # Read Sites records into a Pandas DataFrame # ()"if sf.Latitude" is used only to instantiate/read Site attributes)
# 2. Convert the returned JSON object into a Python dictionary #this seems to easy... 
#Use a lambda function to assign values to water_year2 based on datetime values
# Divide each number of each countries columns by it's annual maximum 
# Create optimizer
# Import top50 datasets # Remove URL form top50 (optional)
#== Boolean Indexing 
# Examine Consumption Series
# set image path
# delete by axis
#show behaviour during sepcific time window #pdf.loc['2016-1-1':'2016-3-31',top_allocs[:10].columns].plot()
# make the new column the index for bitcoin prices
#randomdata.describe()
# backward fill
# Calculate a set of basic statistics.
# Reflect Database into ORM class
# Load Data Set # Display several random 'size' features
# We can also use the "backfill" method to fill in values to the back
# This will load the cached model
# Take a look at the words in the vocabulary #print(vocab) # Sum up the counts of each vocabulary word
#builtins.uclresearch_topic = 'HAWKING' # builtins.uclresearch_topic = 'NYC' # builtins.uclresearch_topic = 'FLORIDA'
# Creating reference to CSV file # Importing the CSV into a pandas DataFrame # Looking at the top of the df to get a feel for the data
# Loading avro-issues.csv
#### Test ####
#Create a box and whiskers plot of our MeanFlow_cms values
# function to show the yearly intervals
#either way to find unique recSets (using groupby or unique count)
#split validation into 50% each of 10%
# Need to factor in that some positions were purchased much more recently than others. # Join adj_close dataframe with portfolio in order to have acquisition date.
# these columns tend to be set-dependent - for instance, the same card can have different borders in different # reprintings; all_cards is just a list of cards, so we remove these columns # casting the columns as sets makes writing the loc much easier, there could be a computational cost though? not sure
# null value check
# merge with main df
# define the feature set and response variable # drop the response variable from the dataset # drop the identify index column from progresql
# Mentioning the dat range also
# same if we enter a list as column index # (which can easily happen inadvertently)...
# remember from above that beers below 2.0 are very rare, so let's use that as a cutoff point
#cek print
# It seems people recently joined have performed most of the activity in rating or not rating Paula's profile 
# let's use 'StatusDate' to estimate time to request resolution # however, first need to convert to appropriate format to compare with 'CreatedDate' # compute resolution time in days for each request
# 少ない順に並べる
# ☃☃☃�I like snowmen!� # â˜ƒâ˜ƒâ˜ƒ“I like snowmen!” # ☃☃☃�I like snowmen!�
# find a positive 606 value in original dataset
# maybe a pie chart showing the different users and the magnitude of their contributions is the total number of lines changed/removed/added??
#X.head() #abc = df.reset_index().values.tolist()
#coefficient between delay instance and crime instance
# fit (train) the vectorizer with the corpus from video text contents
# investigate on the position and properties of detected fixations
#1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD). # Use requests package
# Grouping interests by user
# save model as...
# check the simulation start and finish times
#### dumping dict of data frame to pickle file
# Import two methods from the DOTCE Class `report`.
#estatisticas basicas
# Save the updated spreadsheet
# make new column for month. 
# query to pull the last year of precipitation data
# timestamp with just a time # which adds in the current local date
#Combining all three datasets(tweets1,tweets2,tweets3) together into a single dataframe named tweets.
# dish it out in snappy parquet for comparison
# Requirement #2: Add your code here
# Split text into sentences
#No Clue
# 10-fold cross-validation with the best KNN model
#Pull 2017 data
# Look at some of the tickets along with labeled tone
# Information about the independent factor x.
# monthly data #assert (fin_coins_r.index == fin_r_monthly[start_date:end_date].index).all()
# take a quick look at the DataFrame that returned
# put the data in the following form # | moveId | title |  # how many unique authors do we have?
# approx cost for a year
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# Create symbolic link in ~/data/libs to use site-packages SystemML jar as opposed to DSX platform SystemML jar
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# How many stations are available in this dataset?
# elegant way of computing the sum columnwise (in case all are numeric): 
# Step 11: Try fitting the two-dimensional X value
# Retrieve page with the requests module # Create BeautifulSoup object; parse with 'html.parser'
#Honolulu': 'c47c0bc571bf5427'
# Upper-case the home.dest field
# Remove rows where df['Injury']==0
# Add sentiment as new column in dataframe # Display the first 10
# defines our session and launches graph # runs result
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#coefficient between delay time and crime instance
# check tail
# n_old transactions conversion sample
# Mean Value of pnew and pold
# foreach through all tweets pulled # printing the text stored inside the tweet object
## Now try creating a dataframe that has the subset of crimes with the primary description CRIMINAL DAMAGE  # and the secondary description TO PROPERTY # Will need to google how to use & operator with pandas
# user_df
# drop 10 empty columns #drop empty last row
# In the tweet_data dataset, Setphan and Bo are the top two dogs that have most favorite and retweets
#transit_df = transit_df.reset_index().set_index('DATETIME') #transit_df_rsmpld = transit_df_rsmpld.reset_index().set_index(['DATETIME','STATION'])
# Inspect data types
cur.execute("""select text, geo, coordinates, location $             from tweet_dump where location != ''$             order by id DESC limit 5;""")$
# determine the order of the AR(p) model w/ partial autocorrelation function of first difference, alpha=width of CI
# BigQueryクエリ結果をDataFrameに読み込む # データの先頭５行を表示
# and distance.
# reduce the dimension of the feature for the test data
# path to saved figures
# logistic regression with tfidf #%%time
# NA imputation
# df_2012
# test datetime object
# Number of Stations
#del festivals['lat_long']
# Attach tokens to the dataframe 
#create a min and a max date
"""$ Print complete news titles$ """$
## Import Raw TTN Data from Google SpreadSheet
#Boston
#Create Dataframe
# Requirement #2: Add your code here
# - Design a query to retrieve the last 12 months of temperature observation data (tobs). #   - Filter by the station with the highest number of observations.
# get 8-16 week forecast existing patients # keep only date in index, drop time
# Add author information for model
# only US, CAN and MEX are returned by default
#check the loaded file
# analyze validtation between BallBerry simulation and observation data.
# Use tally arithmetic to compute the difference between the total, absorption and scattering # The difference is a derived tally which can generate Pandas DataFrames for inspection
# Search for '#Aussie'
# can give a Series a single data value,  # and it will be repeated # to be as long as the index
# Plot all the positiv and negativ of the dummy data # This is an example of not choosing to save the image
# Frequency counts using bracket notation 
# lowest 
# We extract the mean of lenghts:
# Extract url from text column
# create a new DataFrame from calls where the location below removed
# 古い分類器の削除 # (必要な場合このセルを実行)
#Access state lookup table
#check if any null entries exist
#%% Do Blink Analysis
# we only need to "instantiate" once.  Then we can call mine_user_tweets as much as we want.
# Heatmap
res = sqlCtx.sql("""SELECT host, stddev(result)$             FROM tempTable$             GROUP BY host""")$
#remove columsn with identifing info
# Concatenate multiple data frames into a single frame
# %load solutions/query.py
# Split data into train and test datasets
### Genrate mapping of numerical values to Grape names
# Random Forest Model 
# Hint
# Print out the number of movies we will recommend.
# Predict the rating of known stub article.
# Column >> result #display head again
# to find columns for normalizing # looks like diff, trans, volatil, transpm
# Custom Python Library Imports
# how many rows and columns?
# find the min of the IMDB column
# play with FileDataStream
# split features and target
##Reduce DataFrame as the transform function is very expensive on the whole dataset
# set new column names
#We can now list the counts of records by confidence code
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# To talk to a database, pandas needs a connection object passed into its # methods. We'll call this conn. # Now conn is connected to a database file called geo.db
#print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']][-30:])
# running a sqlalchemy query to get the date and tobs from  the measurement table.
# close connection to SalesForce
#Garland': '7c01d867b8e8c494'
# Busqueda de valores nulos
# df is the dataframe where the content of the csv file is stored # note that with dataframes I can refer to variables as dictionary keys,  # i.e. df['starttime'] or as attributes: df.starttime. 
# Setting up plotting in Jupyter notebooks # plot the data
# build the vectors that will be the pradicating features
#............................................................................ ##   Advanced Spark : Partitions #............................................................................
#query data 
#We should remove the rows where treatment is not aligned with new_page, or control is not aligned with old_page #Append both the dfs to get the complete one
# Spot check example # Fliter data where name is one of the repeate values
# how many unique authors do we have?
# store the first title
# Dispersion plot
# Read the JSON file into a list of dictionaries
# Issues open
# number of unique users in the dataset
# Load sklearn and LDA (Latent Dirichlet Allocation): # Load word tokenisers and vectorisers:
# Latest Date
# Read the data from the vtc-cab repos
### when you are done, call conn.close()
# 200 X 200
# We print percentages:
#print head of capa
##choose a 15 day vacation trip
#dfs[25].head()
    '''$     Load in pickle for news data over selected period.$     '''$
#feature importance
#run for March users
# filename2 = 'expr_3_qi_nmax_32_nth_1.0_g_101_04-17_opt_etgl_L-BFGS-B.csv' # df2 = pd.read_csv('../output/data/expr_3/' + filename2, comment='#')
# entity visualization # after you run this code, open another browser and go to http://localhost:5000 # when you are done (before you run the next cell in the notebook) stop this cell 
# 定義ファイルの一覧
# calculate mean average deviation with window of 5 intervals
#Imports
# Load the data of the scraped results 
# The base Dataframe's beginning.
# Design a NEW query to retrieve the last 12 months of AVERAGE precipitation data for each day.
# Make one big dataframe
#look at top 50 days with most complaints. what was the top complaint for each of these days?
# Crear la nueva columna usando funciones de la librería datetime
# cannot pass on non-date-like strings
# Dataframe of user information, here we can determine the last month when they were seen 
#Let's check it out
# day with max return 
# helpers contain helper functions # main_functions contain the feature engineering code # mod_class contains the prediction class
#clean the data
#调出小时
#payload = "water,id="+str(metric[0])+" value="+str(metric[2])+"\n"
# We can use the same objects on test now that they have been fitted on train
# Configure how graphs will show up in this notebook
# Grabbing the files from the download 
# see the histogram of the possible fake review.
# Golf KOL
# let's boolean for cat/funny is in the title
#Excludes the "Cover_Type" column from the features provided
#### Define #### # Convert tweet_id column from int type to object type # #### Code ####
# Summary statistics of Precipitation Date
# last 5 rows
#Uno los dataframes para hacer clustering de TODO #all_df.info()
# Export to CSV
# Control the default size of figures
# For getting the table
#To get an insight on the length of each review, we can create a new column in yelp called text length
# SVC with tfidf
# Set representative structures
#using groupby function to check values
#create time series for data. Not usefule so far:
#read in old csv files
# data munging # screen
# Converting my list to a dataframe.
# Table info to check NULL values
#hours.columns, hours.index
# perform a search # print the number of items returned
# convert to datetime objects
# Reset index
#Do the same for male to get a male only dataset #split the dataset by gender
# getting rid of null values and then providing description to show success in that endeavor
# df_2008
# isMax (bigger value is better than small value)
# find historical data for 2001
# m['created_date'] = m['created_at'].dt.strftime('%Y-%m-%d')
# get defects - had to read in excel as NCR tables not working for moment.
#Size of RC_2018-02 is 56 GB
#Switch to dict
# Dimensions:
# check the area of each hru to calculate areal average ET # read the area of each hru
#Any funny minumum values left?
#downloading dataset
#Calculating median trading volumme for the year
# What are the most active stations? # List the stations and the counts in descending order.
#Drop two columns in the dataframe that shows the number of students that pass either math or reading for readability
#That's even better! #ANSWER TO THE QUESTION take three: #Worse than before but a bit fairer.
# Check on the duplicates
# Identifying the top 10 authors # top_10_authors = git_log.groupby('author').count().apply(lambda x: x.sort_values(ascending=False)).head(10) # # Listing contents of 'top_10_authors'
# Train-test split for accuracy metrics
# create pandas dataframe for conversion to arrow
# accediendo a varias columnas por el nombre (label)
# identify wells with two measurements
## Read in the data set
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# creat a new column in our DataFrame with the sentiment
# Create a column that describes the type of injury based on the notes column using # the function I created: extract_injury, df['Injury_Type']
# load back in the file # get data from loaded in array
# Create DataFrame copy grouped by Categories
# And then you can customize these new coulmns using the same method as above. 
q="""select workclass,avg(age) Avg,min(age) Min,max(age) max from adultData group by workclass"""$ #q="""select count(*) from adultData where workclass='?'"""$
#Stack method rotates the data frame so that columns are represented in rows #To complement this, unstack pivots from rows back to columns.
# Formatting the dataframe in decending order
# For the rows where treatment is not aligned with new_page or control is not aligned with old_page # we cannot be sure if this row truly received the new or old page. # adding rows where <group> equals 'control' and <landing_page> does not equal 'old_page'
# Author: Steve Tjoa
# Save the query results as a Pandas DataFrame and set the index to the date column # Sort the dataframe by date
#using groupby function to check values
# calculating number of commits # calculating number of authors # printing out the results
# 文件太大，需要分解
# total days
#added layer of granularity in event customer churns within month...count the scn
# Print all of the classes mapped to the Base
# We are embedding the video we searched into the following webpage using HTML
# Función DataReader
# Choose 1 column and save that image
# Make predictions on test data using the Transformer.transform() method.
# read in a dataset to print columns
# drop known bots from analysis 
# seeing sections of the table in turn
# read a few columns of the tweet data
## drop a collection via pymongo #client['test_database'].drop_collection('posts')
#Export dataframe to csv:
# np.mean() can be used to calculate the proportion of converted users
# watch out with label-based indexing when indices are scrambled: # we will get all values with indexes 1 and 7 and all that happen to be in between
#Return a list of all of the columns in the new dataframe.
# make a time series of mean score per day
# tweets.apply(lambda row: print(row['tokens']), axis=1) # sorted_x
#  GIT_LOCATION is the path of the git executable
# show topics related to a word
#Create predictions and evaluate #print("Number of features used: {}".format(np.sum(clf.coef_ != 0)))
# We can use a factorplot to count categorical data
# Run in test mode and analyze scores obtained
#inspect station data
# df_2009
#What was the largest change between any two days (based on Closing Price)
#Examine the relationship between score and number of comments 
# do note the freq
# Getting logs of the fake user
#Baltimore
#Select rows where the Mean flow was less than 50 cfs
# rearrange column order to group releveant columns together
#proportion of users who converted to other page
#DF
# option 3 (uses only index from df1)
# deployment
# Set representative sequences
# builtins.uclresearch_topic = 'HAWKING' # builtins.uclresearch_topic = 'NYC' # builtins.uclresearch_topic = 'FLORIDA'
#change the type of columns to integer
#set up only display 10 rows c # can do this in environment
## EXAMPLE # Plotting DataFrames
# Print all of the classes mapped to the Base
## Dump Results
#finds date of most recent and least recent tweet
#We check a column to see the auto renewal status of users license
# Baseline confusion matrix
#grouped['C'].agg([np.sum, np.mean, np.std]) Quantile_95_disc_times_pay.agg([np.min,np.max,np.sum,quantile[0.1]])
#Number of rows in testing set 
# Define database and collection
# test data predictions # make predictions on validation data
# pythonize column names
## So only retrive the tweet text, id, user info (id and screen name), and hashtags. Not sure if we're going to use them ## all but ... # Expand the cursor and construct the DataFrame
# See https://www.mediawiki.org/wiki/Manual:Namespace#Built-in_namespaces
# Merge libraries_df with metadata
#Which categories are most popular?
## ------ 500 tweets ------
# A simple function for us to use in pandas' .apply() method.$ def stem_func(word):$     '''Stems a word'''$
# Create average purchase per user feature
# Normal
# remove duplicate ID entries
# Merge free_sub and ed_level together. Which column should the merge be performed on?  # Do this using both the concat() and merge() functions.
#“%d/%m/%Y”, some problem for date format transformation, using dayfirst=True to fix them.
#let's look at 1 timezone.. washington, dc
#  Importing another file for the coming events
# Find State according to Address1 and City and fill in 
# 上位100件を見てみるよ！
# And so, it can be used with .loc to get the index by its name:
# converting into datetime-format
# Actual Test
#на основе векторизатоа # берем все тексты и ищем уникальные слова # В итоге, vctr1 - обученный векторизатор, X_train1 - это тексты (элементы) в виде точек в пространстве
#Jersey City': 'b046074b1030a44d'
# print min and max date
# Cargamos hoja de calculo en un dataframe
# current fire predictions for grid cells
# Load in your own data
#alias the datacube to something more wieldy and pass a string for reporting purposes
# calculating number of commits # calculating number of authors # printing out the results
# The correlations are the following
# preprend to corpus
# create some noise with higher variance and add bias.
# default value of min_count=5
# Export the new CSV as "NewsMedia.csv"
# because pandas is built on numpy, series have numpy functions
# load the query results into a dataframe and pivot on station
# Load the results into a pandas dataframe.  #df.set_index('prcp', inplace=True, )
#El numero de oficina cual es ?
# We create time series for data:
# If you want to drop the redundant column:
#check the dataset 
# do the rest here (potentially the next day when quotas are refreshed) #print('title:{} artist:{} url:{}'.format(title, artist, youtubeurl))
# We can export that to Excel
#Utility function for joining dfs more conveniently
# from h2o.estimators.gbm import H2OGradientBoostingEstimator # hf = h2o.H2OFrame(flight_pd)
#2D correlation matrix plots scope vs type vs nusers
#Use Pandas to print the summary statistics for the precipitation data.
# periods will use the frequency as the increment
# Clean up the 'brand' column.  Strip spaces and proper case strings.
#format date into dateformat 
#term_freq_df.head()
#%%timeit ### %timeit seems to be having issues. no time to figure out why.
# plot autocorrelation function for first difference of doctors data
# fig.savefig('toma.png', dpi=300)
# the cleanest way, in words: take all rows and the column named "A"
#group data  #total fares for pie slice
# select a column
# perform update 
# What is the minimum value?
# Load the results into a pandas dataframe. Set the index to the `date`
# Now apply the transformations to the data:
# Dict of all co_occurences
# can also use
# Concatenating the two pd.series with a pd.DataFrame
# What are the most active stations? # List the stations and the counts in descending order.
# df2 = merged_pd.drop('air_store_id',1) # df3 = df2.drop('day_of_week',1)
#Mars Weather
# Amount invested in BTC in Coinbase
# run a file to set creds as environment variables
# load the source data
# calculating number of commits # calculating number of authors # printing out the results
# Print the external node types
# Counting the no. commits per year # Listing the first rows
# Analyze the data in one tweet to see what we require #The key names beginning with an '_' are hidden ones and usually not required, so we'll skip themif not param.startswith("_"):print  "%s : %s\n" % 
# generate a Gaussian distribution
# Instantiate a lgb classifier: lgb
# Salva cada item do dicionário em um csv separado
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# largest decrease in gamma / best to stop the logspace around 300, can do 10 steps, accuracy vs. speed
# convert bytes to string!
#merge average fare data into city dataframe
# 181 retweet removed
# Applying function to "Stocks" 
# 对于学术制图，可在标题中包含latex语法
# can only fit model once
# словарь всех текстов по кластерам
#globalCity_pdf = 'http://journals.sagepub.com/doi/pdf/10.1177/0042098007085099'
# analyze validtation between BallBerry simulation and observation data.
# Check if there are any NaN values in the Train DF
#df.drop_duplicates(subset=['last_name'])
#x=dataframe['Date']
# your code here
# 6. 分析api调⽤用次数情况
# Structures data library # Columnar data library
#extract the time
# add intercept column # add dummy variables
#Now the crime data only has M and F
# accediendo a una columna por el nombre (label) y obteniendo una serie
# See if you can select the high temperature for January 18
#create and display nytimes sentiments into dataframe
#drop rows where all columns are NaN
# Use the Base class to reflect the database tables ### BEGIN SOLUTION ### END SOLUTION
# append new records to historical data
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
# Load the data from the query into a dataframe
# Set the X and y
# df.loc[df['column_name'].isin(some_values)]
# read geojson neighborhood data
# why tweets contain "felicidades"?
#Filtering out the movies from the input
# media de tmax anual
# store CV results in a DF
# Getting the values and plotting it # plt.scatter(f1, f2, c='black', s=7)
# Total Number of rows
# Test & extract results 
# Extracting BBC news sentiment from the dataframe # Use datetime library to convert Data stored in string to datetime format
# 自己紹介文はない場合もあるので、ない場合を除外する
# create a set of users # randomly sample 20,000 users
#ARIMA model
# Make columns for seasons and terms
# group by 'A' column for the same value, and take sum to each group  # Column 'B' is ignored for sum() operation 
# is our date column datetime?
# what is conversion rate for Pold under the null
# Veamos permit type y permit_type_definition
#number early subscribers
#First convert mapInfo to a string #see what the split method does
# Assign the Measurement class to a variable called measurement
# feats_dict
# rebuild and display empty database
#Male and crime only dataset
# cutting on too short questions because metrics are not working for those (yet)
#ignore any warnings
    """for every sample, calculate probability for every possible label$     you need to supply your RNN model and maxlen - the length of sequences it can handle$     """$
# print top three (most consistent)
## Fit/train the model (x,y need to be matrices)
# получаем мешок слов и переводим его в массив
## Fit/train the model (x,y need to be matrices)
# change table formatting         #qt_convrates_toClosedWon.columns = convrates_to_closedWon
#Inspect distribution of subjects by prior tweet history volumes
#== Describe : Show quick status summary (interactive tool)
# make predictions 
#the url for one dream from the user 'fataverde' posted five days ago (today's date: 1.9.18)
#try to drop duplicates on the projects dataframe
# Load Data Set # Add column on which to build pivot table # Sample data
#average number of completed tasks
# Read the variation of electricity demand from Data.xlsm
# To print the count of our vocab # For each, print the vocabulary word and the number of times it  # appears in the training set
# Copying dataframe in df2 # Removing increasing rows
# take a peak at the data
# check option and selected method of (12) choice of hydraulic conductivity profile in Decision file
# characterize the data #  number of words, don't remove punctuations and other non-alphanumeric characters
# Wide display
# Perform date transformations
# SAVE A CSV OF THE TABLE ("number_one_chart.csv") TO COMBINE WITH "song_tracker.csv" LATER
# Теперь отнимем от этого момента времени 24 часа и переводем резуьтат в unix time формат # Заметим, что time.mktime переводит с учетом GMT часового пояса, который на 3 часа раньше нашего # Поэтому вычтем 21 час, а не 24 часа, чтобы полученная при конвертации дата была действительна на 24 меньше искомой
# intentionally not skipping rows here since we need the metadata too
# groupby in conjunction with aggregation functions like mean: # fast way to summarize data; a hierarchical index is by default  # generated
# output and submit to kaggle
#Checking for the null count
# add a notebook to the resource of summa output # check the resource id on HS that created.
#Compute accuracy on our training set
## So setup our graph so that we have unique users, tweet id's, and hashtags
#  Quantile-Quantile plot of geovolatility
# this data is now sent out to the executors.
# Convert sentiments to DataFrame
# check build estimator model
# Assign reponse object to a variable r and check the data by calling method .json()
# Check null value exists or not
# Train it over a desired number of episodes and analyze scores # Note: This cell can be run multiple times, and scores will get accumulated
# end_time :: end time of 2016
# Turn the dummified df into two columns with PCA
# ...and as in the case of position-based indexing, using # slices or lists as col indexes results in dataframes
# Save png
# Check is Data is imbalanced
# todo compute rolling corr matrix and plot
#Check to see the change in Index.
#Test #The date format is correct
# Only fetchs historical data from a desired symbol # or qb.History("SPY", 360, Resolution.Daily)
#白噪聲檢查 #返回統計量和p值
#additional features from goal, pledge and backers columns #The above field will be used to compute another metric # In backers column, impute 0 with 1 to prevent undefined division.
# accediendo a varias columnas por índice
# Links anzeigen
# elms_all are last two weeks' elms data
# Resource: https://stackoverflow.com/questions/33899369/ranking-order-per-group-in-pandas # Create 'Inspection_number' columns, populate with groupby function on brkey field and rank ascending # Convert Inspection number field to integer
# Linux / OSX # Windows     #! "c:/Program Files (x86)/GnuWin32/bin/wget.exe" --no-check-certificate https://pjreddie.com/media/files/yolov3.weights
# Filter Twitter Streams to capture data by the keywords:
# Fill in the values from this weeks pace
# Your Code Goes Here
# Instantiate a Materials collection # Export to "materials.xml"
# Show best number of trees
# here, name-based index created above will be kicked out altogether and replaced by the  # default numerical index
# convert id to long instead of string
#Simply use the Dataframe Object to create the table:
#df['age'] = 
# total unique ids of the new dataframe
# return the original input when unparseable
#cur_laurel = conn_laurel.cursor()
# Split in features and target
# Data key value is a dictionary with a children key -- where all the post information is 
# Task 4
# analyze validtation between BallBerry simulation and observation data.
# where's my favorite beer at?!
# Arrow positions per country combinations
# Number of votes
# Group the data by media outlet
# Different way to plot
# Preview the Data in the Measurement table
# Percent of true gains over all samples: compare to precision -- precision should be signficantly higher if this is a useful # prediction # (cnf_matrix[1,0] + cnf_matrix[1,1]) / cnf_matrix.sum() 
# Define the authorization endpoint.
#plt.ylim(200,400);
# Reduce words to their stems
# create with column names
# Extract mean of tweet length
#Creating slice from the df for the second table
# newdf.get_group("2018-02-27 05:06:00")
# Merging event_list and df_test_user whrere event_start_at > created_on
# number of labels
# retrieve life expectancy at birth for all countries # from 1980 to 2014
# Use the base url to create an absolute url
# fixing all the teams name
# get a list of all data fields
"""$ Use this cell to run the preliminary test and calculate the baseline accuracy of the newly created classifer$ """$
# predictions = grid.predict(X) # print(classification_report(y, predictions))
# anchor random seed
# query for prcp data based on date range from most recent to a year before #make data from from sql query
# data_air_visit_data[:3]
# normalize the data attributes
# Dataframe of all events created by a user within a 30 day period
# Dropping the id column from test # Now we can verify that "train" and "test" have the same number of columns, as expected # We can also verify that the "submit" dataframe has the same number of rows as "test"
# Next, compute the duration of the campain, in weeks
# choose the columns we need 
# By default the max() method will calculate the maximum value in each column. #
# Plot causes of death by year
# df_2007
#Load the query results into a Pandas DataFrame and set the index to the date column.
# alternatives:
# Make columns for seasons and terms
# wall time to train model
#  利用參數kind來變換圖形模式 #  barh就是橫軸
#split the dataset 
# Removing the MTU and MTU2 columns
#write the above dataframe, with cleaned up lists for contributors, affiliations, publishers, and tags, to a new excel doc # testing.to_excel('SHARE_cleaned_lists.xlsx', index=False)
# Set the index to date and sort by date # See what it looks like
# Graph of the Polarity by hour for a specific day
# cisrol12 # this subject had mp4s for each activity
# #read windfield geotiff
# Load gh_api_token & meetup_key & genderize_key
# Gets rid of 'u/' so that it is easier to use the api
# index in the resultant contatinated df
#reduce dim (240000,1) -> (240000,) to match y_train's dim
# query values in set:
# We remove the store 2 and store 1 rows # we display the modified DataFrame
#lda_tfidf.print_topics(10)
# Requirement #1: Add your code here
# Gather the data collected into a dataframe and print it.
## 2. Visualization and basic statistics
#### Read the updated dataframes for analysis
# First let's turn each title into a list of words # Then we'll use the lemmatizer to change plurals to singulars to group them # And rejoin the lists of words to one string for the count vectorizer
# Save figure
# Transmission 2050 [GWh], marathon
# Y si quiero actualizar el valor de una columna de los registros que cumplan una condición
# z score
# compound dataframe
# accediendo a una columna por índice y obteniendo un dataframe
# Remove the ".csv" from the file name # Set the index as the 'mail ID'
# Plot the results as a histogram with bins=12.
#store tables
# Read in potential data model during initialization
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
