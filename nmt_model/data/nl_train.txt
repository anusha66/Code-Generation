merged_311_data = pd.merge(pgh_311_data, $          issue_category_mapping, $          left_on="REQUEST_TYPE", $          right_on="Issue") $ merged_311_data.head()
print(intervention_history.shape) $ print(intervention_train.shape) $ print(intervention_test.shape)
twitter_df_merged.head(5)
model.wv.similarity('man', 'women')
merged1 = merged1.set_index('AppointmentDate')
result.head(20)
df_master.dog_name = df_master.dog_name.astype('category') $ df_master.stages = df_master.stages.astype('category') $ df_master ['p1'] = df_master.p1.astype('category') $ df_master ['p2'] = df_master.p2.astype('category') $ df_master ['p3'] = df_master.p3.astype('category')
first_cluster.transform(docs).shape
train.sample(5)
X_test.shape
from sklearn.ensemble import VotingClassifier $ clf_vot_tf = VotingClassifier(estimators=[('rf', clf_RF_tf), $                                           ('lr', clf_LR_tf), $                                           ('rgf', clf_RGF_tf)], voting='soft').fit(X_traincv_tf, y_traincv_tf)
display(data.head(10))
names1 = ['Date','South West Pacific','South East Pacific','PNG/Solomon Islands','Vanuatu','Vanuatu - Gaua','Vanuatu - Ambrym','New Caledonia','Samoa','Raoul/Curtis','Society Islands','New Zealand']
inputNetwork.build()
log_mod = sm.Logit(df3['converted'], df3[['intercept', 'ab_page']])
p_old_std = (df2['converted']==1).std()
7052145 $ 7052146
act_diff = df2.query("group == 'treatment'")['converted'].mean() - df2.query("group == 'control'")['converted'].mean()
train_features_tokenized = vectorizer2.fit_transform(articles_train)
data_tickers = data_tickers.resample(sampling, how='last') $ data_tickers.head()
from sklearn.feature_extraction.text import CountVectorizer $ tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=10000, $                                 stop_words='english') $ tf = tf_vectorizer.fit_transform(text[mask][INDEX])
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ z_score, p_value
aTL.loc[(aTL.BRANCH_STATE.isnull())].shape
group=merged.groupby(['date','store_nbr','class','family'],as_index=False) $ class_onpromotion=pd.DataFrame(group['onpromotion'].agg('sum')) $ pd.DataFrame.head(class_onpromotion)
engine.execute("SELECT  * FROM contractor").fetchall()
obj3 + obj4
import teaching_plots as plot
df['Updated Shipped diff'].max()
n_old = sum(df2['landing_page'] == 'old_page') $ n_old
df_usa['PD2'] = ((df_usa['Population']*1000)/df_usa['Area']).round(2) $ df_usa
wrd_clean['rating_denominator'].describe()
data.values
np.exp(1.6)
plt.scatter(X,y2) $ plt.plot(X, np.dot(X_17, linear.coef_) + linear.intercept_, c='red') $ plt.xlabel('Hour of Day') $ plt.ylabel('Count')
list(df_clean)
learner.get_layer_groups()
with open(os.path.join("../datasets/CSVs/", csvfile), "r") as f: $     data2 = csv.DictReader(f) $     for row in data2: $         print(row)
Y_tweet = dft.groupby(['stamp'])['log_followers_count'].sum()
kochdf.head(10)
data['SA'] = np.array(x)
%matplotlib inline $ fb['2012-04-13':].resample('W').count()['message'].plot(figsize=(18,6))
temp_df.info()
%timeit articles['tokens'] = articles['content'].map(nltk.tokenize.RegexpTokenizer('[A-Za-z]+').tokenize)
df_all.head()
dat[out_columns].to_csv(save_pth, float_format='%g', date_format=date_format) #save data
num_rows = df.shape[0] $ print('Number of rows in dataset: ',num_rows)
returns.tail()
print('Shape of the dataset after transformation:', str(filtered_file.shape), 'Out of:', str(master_file.shape), '\n') $ print(filtered_file.iloc[:, 28:-1].axes[1], '\n') $ print(filtered_file['TECTONIC SETTING'].value_counts())
grouped_sentiments_pd = sentiments_pd.groupby('Media Source') $ sentiment_avg_pd = grouped_sentiments_pd.mean() $ sentiment_avg_pd.sort_values(by=['Compound'], ascending=False, inplace=True) $ sentiment_avg_pd = sentiment_avg_pd.reset_index() $ sentiment_avg_pd
df_twitter_extract_copy.info()
X_train.head()
results = lm.fit() $ results.summary()
test_bkk2.reset_index(inplace=True) $ test_kyo2.reset_index(inplace=True)
unsorted_df=pd.DataFrame(np.random.randn(10,2),index=[1,4,6,2,3,5,9,8,0,7],columns=['col2','col1']) $ unsorted_df
twitter_df_clean.groupby('source').source.count()
catFeatures = ['quarter','month','day','dayofweek', 'pickup_cluster', 'dropoff_cluster','ride_cluster','Weather']
airbnb_od.structure()
df3 = df2.merge(c, on ='user_id', how='left') $ df3.head()
chinadata.tail()
iowa_fc_item.layers
condos.info()
by_weekday = data.groupby(data.index.dayofweek).mean() $ by_weekday.index = ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun'] $ by_weekday.plot(style=[':', '--', '-'])
grouped_by_day_df.to_csv('grouped_by_day_df.csv',index=False)
edu_columns[0]='Category'
tweet_archive_df.rating_numerator.value_counts().sort_index()
output= "SELECT * from user where user_id='@Pratik'" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['User_id','User Name','Following','Followers','TweetCount'])
gsmodel.best_score_
n_new_page = len(df2[df2['landing_page']=='new_page']) $ probabiility_newpage = n_new_page / n_unique_users $ print ("The probability that an individual received the new page is: {:.4f}".format(probabiility_newpage))
sns.regplot(x=dfz.retweet_count, y=dfz.favorite_count)
plt.show()
import pickle $ with open("rows.pkl", 'wb') as f: $     pickle.dump(rows, f)
groceries.drop('apples')
autos['make'].cat.categories
df.ix[0].A = 1 $ store['df'] = df $ pd.HDFStore("../../data/store.h5")['df'].head(2)
aml = H2OAutoML(max_runtime_secs = 300, # 5 minutes $                 seed=2055) $ aml.train(x = predictors, y = Y, $           training_frame = train)
users.dtypes
data.head()
pmean = np.mean([p_new,p_old]) $ round (pmean, 4)
%writefile /tmp/test.json $ {"dayofweek": "Sun", "hourofday": 17, "pickuplon": -73.885262, "pickuplat": 40.773008, "dropofflon": -73.987232, "dropofflat": 40.732403, "passengers": 2}
ghana = ghana.rename(columns={'WindDirDegrees<br />' : 'WindDirDegrees'})
autos.rename({'odometer':'odometer_km'},axis=1,inplace=True)
pd_train.sample(10)
%%time $ with tb.open_file(filename='data/NYC-yellow-taxis-2017-12.h5', mode='a') as f: $     table = f.get_node(where='/yellow_taxis_2017_12') $     table.cols.trip_distance.remove_index() $     table.cols.passenger_count.remove_index()
prob_old = df2[df2.group == 'control'].converted.mean() $ prob_old
price_df = price_df[price_df['date_tuple']>(7,45)] $ price_df = price_df[price_df['date_tuple']<(11,0)]
session_v2.head(127)
describe_precipitation = df['precipitation'].describe() $ describe_precipitation
df2.head(3)
merged.groupby("committee_name_x")
print(df['score'].mean())
join_e.select('party_Id_orig').distinct().count()
monthly_gain_summary.head(10)
cars= cars[(cars.yearOfRegistration <= 2016)& (cars.yearOfRegistration >= 1950) & (cars.price>=100) & (cars.price<=160000)] 
segments.seg_length.apply(np.log).hist(bins=100)
to_be_predicted_Day3 = 81.92415603 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
paired_df = pd.DataFrame(list(pairwise_count.items()), columns=['paired_datasets', 'co_occurence']) $ paired_df.sort_values('co_occurence', ascending=False, inplace=True) $ paired_df.head()
import statsmodels.api as sm $ convert_old = df2.query('group =="control"')['converted'].sum() $ convert_new = df2.query('group =="treatment"')['converted'].sum() $ n_old = len(df2.query('group == "control"')) $ n_new = len(df2.query('group =="treatment"'))
ab_new[['CA','US','UK']] = pd.get_dummies(ab_new['country'])[['CA','US','UK']] #making dummy values for CA and US $ ab_new
plt.plot(x, y, 'r.', markersize=10) # plot data as red dots $ plt.xlim([-3, 3]) $ mlai.write_figure(filename="../slides/diagrams/ml/regression.svg", transparent=True)
df[df['Descriptor'] == 'Loud Music/Party'].groupby(by=df[df['Descriptor'] == 'Loud Music/Party'].index.dayofweek).count().plot(y='Agency')
geocode_endpoint = 'https://maps.googleapis.com/maps/api/geocode/json?'
wordfreq = FreqDist(words_sk) $ print('The 100 most frequent terms, including special terms: ', wordfreq.most_common(100))
station_count.iloc[:,0].idxmax() $
len(v)
np.std(new_means)
old_page_converted = np.random.binomial(1, p_equal_old_new, length_of_old) $ old_page_converted.mean()
df.isnull().values.any()
avails = ['number_of_reviews','availability_30','availability_60','availability_90','availability_365'] $ filtered_df[avails].cov()
PANDAS_ODF = os.path.expanduser('~/src/odf_pandas') $ if PANDAS_ODF not in sys.path: $     sys.path.append(PANDAS_ODF) $     from pandasodf import ODFReader
located_data['country'] = None
df2_control = df2.query('group == "control"') $ df2_control.query('converted == 1').user_id.nunique() / len(df2_control)
df_columns[ ~df_columns['Descriptor'].isnull() & df_columns['Descriptor'].str.contains('Pothole') ].groupby(["Hour of day","AM|PM"]).size().reset_index().sort_values([0], ascending=[False]) $
'Mean ROC_AUC score: {:.3f}, std: {:.3f}'.format(np.mean(cv_score), np.std(cv_score))
sensor = hp.find_sensor('d5a747b86224834f745f4c9775d70241')
yc200902_short['Trip_Pickup_DateTime'] = pd.to_datetime(yc200902_short['Trip_Pickup_DateTime'])
df_tot = df_stock.merge(df_index, left_on='price_date', right_on='price_date') $ df_tot.rename(columns={'variable':'ticker'},inplace=True) $ df_tot.set_index(['price_date', 'ticker'],inplace=True) $ df_tot.rename(columns={'Return': 'Returns'},inplace=True)
n_new = df2.query("landing_page=='new_page'").shape[0] $ n_new
new_page_converted = np.random.choice([0,1] , size = n_new, p=[p_new, (1-p_new)]) $ len(new_page_converted)
import pandas as pd $ import numpy as np
get_adjacency_matrix(1253).head()
df_cs['sentiment'] = df_cs['cleaned_text'].apply(sentiment_calc) $ df_cs['Polarity'] = df_cs['cleaned_text'].apply(polarity_calc) $ df_cs['Subjectivity'] = df_cs['cleaned_text'].apply(subjectivity_calc)
frame.reindex(columns=['Texas', 'Utah', 'California'])
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/adult.data.csv" $ df = pd.read_csv(path, sep =',', na_values=['.']) $ df.head(5)
request_token, request_token_secret = oauth.get_request_token(params={"oauth_callback": "oob"})
error_filtered = error_as_pandas_df[error_as_pandas_df['Reconstruction.MSE'] > 0.0025]
tb = TextBlob(tweets[2]) $ tb.noun_phrases
print 'original rounds: %s, new count of rounds: %s' %(rounds.shape[0], rounds_df.shape[0])
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
df.head(10)
new_page = df.query('landing_page == "new_page"')['user_id'].count() $ total = df.shape[0] $ new_page_prob = new_page / total $ new_page_prob 
from sklearn.model_selection import GridSearchCV
df.loc[dates[0]]
forest.score(X_train_matrix, y_train)
keto = pmol.df[pmol.df['atom_type'] == 'O.2'] $ print('number of keto groups: %d' % keto.shape[0]) $ keto
test_vecs = proc.transform(test_docs)
df2=df $ df2.head(10)
logreg_words.fit(X_words, y).coef_[0].round(3)[-4:]
dedup.shape
!ls -ltha $data_dir
base_retweet_df.head()
df_questionable = pd.merge(left= df_merged, left_on= 'link.domain_resolved', $                            right= df_os, right_on= 'domain', how= 'left') $ df_questionable['tweet.created_at'] = pd.to_datetime(df_questionable['tweet.created_at'])
shirt_1.price
output.shape
store.info()
cachedf = dir2df(cachedir, fnpat='\.ifc$', addcols=['dirname', 'barename']) $ cachedf
latest_timelog = pd.DataFrame.from_dict(latest_time_entries)
columns = ['day_period', 'weekday', 'category', 'is_self', 'is_video'] $ le = LabelEncoder() $ model_df[columns] = model_df[columns].apply(lambda x: le.fit_transform(x))
twitter_df.in_reply_to_status_id.isnull().sum()
grid_id_flat = grid_id_array.flatten()
df1 = pd.read_feather('new_sensor_data2') $ df1.head(10)
infered_gender_for_authors_pq_saved = non_blocking_df_save_or_load( $     infered_gender_for_authors, $     "{0}/infered_gender_for_authors_pq_3".format(fs_prefix))
type(fish.stack())
import numpy as np $ print(np.info(np.fromiter))
df_train=dfd.sample(frac=0.8,random_state=200) $ df_test=dfd.drop(df_train.index)
n_new, n_old = df2['landing_page'].value_counts() $ print("new:", n_new, ",old:", n_old) $ new_p_c = np.random.choice([1, 0], size=n_new, p=[p_mean, (1-p_mean)]) $ new_p_c.mean()
vlc[pd.isnull(vlc.bio.str.strip())].id.size
data_type = df.dtypes
pca_df = pd.DataFrame(pca_data, index = pivoted.T.index.values,columns=labels)
df.groupby(['drg3','year']).quantile([0,.25,.5,.75,1]) $ abc = df.groupby(['drg3','year']).describe()
import matplotlib.pyplot as plt $ mat_plt_plt=plt.figure() $ plt.plot(predict_actual_df['prediction'][0:200]) $ plt.plot(predict_actual_df['actual'][0:200])
train_data_features_tfidf = vectorizer_tfidf.fit_transform(clean_train_reviews_sw) $ train_data_features_tfidf = train_data_features_tfidf.toarray() # Numpy arrays are easy to work with $ print(train_data_features_tfidf.shape)
with open(os.path.join(url.split('/')[-1]), mode='wb') as file: $     file.write(response.content)
dti1 = pd.to_datetime(['8/1/2014']) $ dti2 = pd.to_datetime(['1/8/2014'], dayfirst=True) $ dti1[0], dti2[0]
twitter_archive_clean = twitter_archive.copy() $ image_predictions_clean = image_predictions.copy() $ tweet_json_clean = tweet_json[['id','retweet_count','favorite_count']].copy()
test_target_monthly = test_target['255_elec_use'].resample('M').sum() $ test_preds_monthly = test_preds_df['kwh_pred'].resample('M').sum()
for v in data.values(): $     if 'Q3' not in v['answers']: $         v['answers']['Q3'] = ['Other']
p_new=df2.query('converted == 1').user_id.nunique()/df2['converted'].count() $ p_new
df[df.index.month.isin([6,7,8])]['Complaint Type'].value_counts().head()
body = client_cb054ee4c5d549b6b6a7b15cd3866ac7.get_object(Bucket='tklivedatademo6629b7d8d0bc4dfe8b1070173649a76f',Key='SCORING_INPUT.CSV')['Body'] $ if not hasattr(body, "__iter__"): body.__iter__ = types.MethodType( __iter__, body ) $ scoring_input_data = pd.read_csv(body) $ scoring_input_data = scoring_input_data.sort_values(['ID','DATE'],ascending = ['True','True']) $ scoring_input_data.head()
def predict(classifier, x_test): $     pred_y = [] $     for prediction in classifier.predict(x_test, as_iterable=True): $         pred_y.append(prediction['class']) $     return pred_y
type(dataset)
itos[-10:]
join_e = join_d.join(join_b,'party_id_orig')['party_id_orig',"aggregated_prediction",'highest_match','predicted']
temps_df[temps_df.Missoula > 82]
suburban_driver_total = suburban_type_df.groupby(["city"]).mean()["driver_count"] $ suburban_driver_total.head()
df.converted.mean()*100
con = sqlite3.connect('db.sqlite') $ df=pd.read_sql_query("SELECT * from sqlite_master", con) $ con.close() $ df
inner_list = data['dataset']['data']
data_month = data_nonan_temp.groupby(['Month'], sort=True) $ data_month.describe()
def evening_mapper(x): $     if x.hour>17  and x.hour<23 : $         return 1 $     else: $         return 0 $
station_distance.tail(2) $ station_distance.shape
instance.updateParameters(cannull,ranges,tests) $ instance.testassumptions()
features = ['title_words', 'days_passed', 'ct_image', 'ct_tags', 'ct_words', $        'title_emot_quotient', 'featured_in_tds', 'read_time', $        'img/word',  'text_sentiment', 'keyword_proportion', 'day', 'day_of_week', 'month', $        'bullets_present', 'followedBy', 'following', 'ct_links', 'is_month_end', 'is_quarter_end', 'is_month_start', 'is_quarter_start', $        'week_of_year', 'quarter']
!hdfs dfs -cat {HDFS_DIR}/p32cfr-output/part-00001
def extract_content(tweet): $     content = [tweet['text'],               $                tweet['senti_val']] $     return content
count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english') $ vects = count_vectorizer.fit_transform(data[' Tweet Text']) $ vocabulary = count_vectorizer.get_feature_names() $ print('Vocabulary size: '+str(len(vocabulary)))
non_rle_pscs = active_psc_records[active_psc_records.non_rle_country == True] $ len(non_rle_pscs.company_number.unique())
pd.crosstab(aqi['AQI Category_eug'], aqi['AQI Category_cg'], normalize='index', margins=True).plot(kind='bar', stacked=True);
pres_df['time_from_creation_tmp'] = pres_df['start_time'] - pres_df['date_created'] $ pres_df['time_from_creation_tmp'].head(10)
merged_df.fillna('unknown', inplace=True) $ merged_df.rename(columns={'GROUP':'Group'}, inplace=True) $ merged_df.head()
np.exp(res.params)
df[df["newsOutlet"]=="@BBC"]
logreg = LogisticRegression(penalty='l2', C=1.0) $ logreg.fit(X, y) $ new_pred_prob = logreg.predict_proba(X_new)[:, 1] $ new_pred_prob
pd.read_csv('../data/ebola/guinea_data/2014-09-02.csv').head()
df['word_count'].value_counts().sort_index().plot(color='b', alpha=.5);
contribs.info()
stockdftest = stockdftest.sort(['Date'])
df_episodes.info()
df.first_affiliate_tracked.value_counts()
Annual = Exchange_df.resample('BA', how=('sum'))   $ Annual
def find_prediction(savm_id,data_id): $     return 1 if savm_id==data_id else 0 $ find_prediction_udf = F.udf(find_prediction,types.IntegerType()) $ join_b = join_b.withColumn("predicted", find_prediction_udf(join_b["sales_acct_id_orig"],join_b["sales_acct_id"]))#.select(['endcustomerlinefixed_data','predicted'])
print("Row names for berlin_weather_oldest.csv:") $ df1.index.values
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
display(data[sets_columns_names].head())
from netCDF4 import num2date, date2num, date2index $ timedim = sfctmp.dimensions[0] # time dim name $ print('name of time dimension = %s' % timedim) $ times = gfs.variables[timedim] # time coord var $ print(times)
blame.head()
import autosklearn.regression $ import sklearn.model_selection $ import sklearn.metrics $ X_train, X_test, y_train, y_test = \ $     sklearn.model_selection.train_test_split(features.as_matrix(), labels.as_matrix(), random_state=1) $
n_net3.score(x_test,y_test)
user_df = my_df[my_df["user"] == 0] $ print(user_df.shape) $ print(user_df.head())
from scipy.stats import norm $ norm.ppf(0.025)
with open('start.pkl', 'rb') as f: $     start_df = pickle.load(f)
shows.to_csv('scraped_data4.csv', encoding='utf-8', index=False)
twitter_df_clean = pd.merge(twitter_df_clean, tweet_json_df, $                             on=['tweet_id'], how='inner')
tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True) $ raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1] $ raw_tfidf 
forecast_df.loc[forecast_df['pop'] == 0, 'pop'] = 1 $ forecast_df['white_pct'] = forecast_df['white_pop'] / forecast_df['pop'] * 100 $ forecast_df.drop('white_pop', axis=1, inplace=True) $ forecast_df.loc[forecast_df['pop'] == 0, 'pop'] = 0 $ forecast_df.head()
data["Zip Code"] = data["Zip Code"].astype("int")
from functools import partial  $ new_min_max_pop = partial(min_max_pop, cols = cols_select)
plt.hist(p_diffs); $ plt.title('Plot of 10000 simulation for p_diffs'); $ plt.xlabel('p_diffs'); $ plt.ylabel('Frequency');
df['SSL'].isin(aru_df['SSL']).value_counts()
df.shape $ df.columns.values
corpora.MmCorpus.serialize('experiment/textCorpus.mm', corpus)
kickstarters_2017[cont_vars].corr()
counts_df_clean.info()
sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')
number_of_commits = len(git_log) $ number_of_authors = git_log.loc[:, 'author'].dropna().nunique() $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
def engineer(x): $     if 'Engineer' in x: $         return 1 $     return 0 $ df_more['Engineer'] = df_more['Title'].apply(engineer)
sox.to_csv('../../../../../CS 171/cs171-gameday/data/sox.csv', index=False) $ bruins.to_csv('../../../../../CS 171/cs171-gameday/data/bruins.csv', index=False) $ celtics.to_csv('../../../../../CS 171/cs171-gameday/data/celtics.csv', index=False)
archive_clean.loc[archive_clean['rating_denominator'] != 10, 'rating_denominator'] = 10
len(workspaces_list)
im = pd.read_csv("image-predictions.tsv", sep='\t') $ im.info()
df_json_tweets.duplicated()
tweet_archive_enhanced_clean['rating_denominator'].sum()
len(youthUser2014)
print( "\nSize of the dataset - " + str(len(autodf))) $ autodf = autodf[(autodf.yearOfRegistration >= 1990) & (autodf.yearOfRegistration < 2017)] $ print( "\nSize of the dataset - " + str(len(autodf)))
print(np.min(prec), np.mean(prec), np.max(prec))
date_of_birth = date_of_birth[date_of_birth>'1930-01-01']
dt_obj = datetime.datetime.strptime(df.index[0], '%Y-%m-%dT%H:%M:%S.%f') $ print(dt_obj) $ print(type(dt_obj))
twitter_df_clean['stage_name']="None" $ twitter_df_clean.iloc[191,13]="puppo" $ twitter_df_clean.iloc[200,13]="floofer" $ twitter_df_clean.iloc[191,7]="None" $ twitter_df_clean.iloc[200,7]="None"
LARGE_GRID.plot_accuracy(raw_large_grid_df)
X_test = X_test.loc[:,[col for col in X_test.columns if col not in ['Footnote','CountyName','Month']]]
ip_clean[['p1', 'p2', 'p3']]
import sqlite3 $ conn = sqlite3.connect(os.path.join(outputs,'example.db'))
new['country'].unique()
pprint(q_pathdep_obs.metadata(reload=True))
z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new],[n_old, n_new]) $ (z_score, p_value)
vectorizer?
multi_var = sm.OLS(df2['converted'], df2[['intercept', 'ab_page', 'CA', 'UK']]) $ multi_var_results = multi_var.fit() $ multi_var_results.summary()
raw = pd.read_json('data/raw_data.json') #not needed $ object_check = raw['object_id'][0]
tweet_df["date"] = pd.to_datetime(tweet_df["date"]) $
jimcramer = miner.mine_user_tweets(user='jimcramer', max_pages=10)
r = requests.post(url=url, json={'query': query}, headers=headers)
print("quantidade de user_id: " + str(df2.user_id.nunique()))
df.query('group != "treatment" & landing_page == "new_page"').count() $
df[df['public']=='offline'].count()[0]
fairshare = pd.read_csv('data/fairshare.txt', delim_whitespace=True)
plt.rcParams['axes.unicode_minus'] = False $ dta_6204.plot(figsize=(15,5)) $ plt.show()
df2['is_duplicated'] = df2.duplicated('user_id') $ df2.query('is_duplicated==True')['user_id']
ins2016 = ins[ins['year']==2016]
bnbAx.language.value_counts()
model.score(X_test, y_test)
p_conv_ctrl = df2.query('converted == "1" and group == "control"').user_id.nunique() / df2.query('group == "control"').user_id.nunique() $ p_conv_ctrl
ab_df.info() $
data['affair'].value_counts()
nconvert = len(df2[(df2.converted==1) & (df2.group=='control')]) $ ntot = len(df2[(df2.group=='control')]) $ prob_1 = nconvert/ntot $ print(prob_1)
display('df5', 'df6', $        "pd.concat([df5, df6], join_axes=[df5.columns])")
import os $ os.environ['BIGML_USERNAME'] = "EFETOROS" $ os.environ['BIGML_API_KEY'] = "7e5fc6a649fd0f8517fc8ecf2ebd30151c5d4fb4"
yhat_prob = LR.predict_proba(X_test) $ yhat_prob
print "# %s"%OGLE_ra_dec_data[0] $ print "# %s"%OGLE_ra_dec_data[-1] $
metadata_df_all['type'] = [t.lower() for t in metadata_df_all['type']] $ metadata_df_all['bill_id'] = metadata_df_all['type'] + metadata_df_all['number'].map(int).map(str) $ metadata_df_all = metadata_df_all[['accessids', 'bill_id']]
(events.groupby('client_uuid') $      .map(lambda x: x.total_seconds()) $      .hist(bins=50)) # KABOOM !
top_20_breed_stats[['favorite_count','retweet_count']].sort_values(by='favorite_count', ascending=False).plot(kind='bar', subplots=True)
sentiments_pd['Media Source'].unique()
users['invited'] = [True if i else False for i in users.invited_by_user_id]
Z = np.zeros((8,8),dtype=int) $ Z[1::2,::2] = 1 $ Z[::2,1::2] = 1 $ print(Z)
frame2.year
girls_by_name.loc['FATIMA', :]
fig, ax = plt.subplots(nrows = 1, ncols = 1) $ chinadata.plot(ax = ax,) # So this ploted gdp growth using the axes generated above.... $ ax.set_title("Key Economic Indicator Growth", fontweight = "bold") $ plt.show()
pd.read_pickle("baseball_pickle")
facts_metrics.head()
season_team_groups = nba_df.groupby(["Season", "Team"], as_index = False)
inspector = inspect(engine)
X_train[X_train.isna().any(axis=1)]
%matplotlib notebook $ import matplotlib.pyplot as plt $ plt.plot_date(df_meme['date'],df_meme['occur'],linestyle='solid') $ plt.xticks(rotation=-40) $ plt.show()
contractor_clean[contractor_clean['contractor_id'].isin([382,383,384,385,386,387])]['contractor_bus_name']
df2.query('converted == 1').user_id.nunique()  / df2.user_id.count()
users[users.id == 28]
df[1:3]
SANDAG_age_df[SANDAG_age_df['SEX'] == 'Male'].loc[(2012, '1')]
sub_dataset['SectionName'].value_counts(sort=True, normalize=True)
hm = pd.read_csv('Resources/hawaii_measurements.csv') $ hs = pd.read_csv('Resources/hawaii_stations.csv')
pivot
baseball_h.loc[(2007, 'ATL', 'francju01')]
import pandas as pd
contractor_clean['address1'].value_counts()
sns.heatmap(X.corr()) $ plt.show()
drivers = data_merge.groupby("type") $ drivers_total = drivers['driver_count'].sum() $ drivers_totals = [drivers_total['Urban'], drivers_total['Suburban'], drivers_total['Rural']]
df[df['Descriptor'] == 'Loud Music/Party'].groupby(by=df[df['Descriptor'] == 'Loud Music/Party'].index.hour).count().plot(y="Borough")
metrics.completeness_score(labels, km.labels_)
dict(list(r_close.items())[0:10])
large_df.head()
out_df['high'] = out_df['high']*.077788/.042513 $ out_df['medium'] = out_df['medium']*.227529/.195824 $ out_df['high'] = out_df['high']*.694683/.761663
z_score, p_value = sm.stats.proportions_ztest(count=[convert_old, convert_new], nobs=[n_old, n_new], alternative='smaller' ) $ z_score, p_value
newfile = pd.read_excel('export new.xlsx') $ oldfile = pd.read_excel('export old.xlsx')
churn_df = pd.read_csv("ChurnData.csv") $ churn_df.head()
df.source.value_counts()
df6.unstack()
y_pred_df = pd.DataFrame(y_pred)
df_code_activations.head(3)
df2 = pd.DataFrame(data)
np.random.random((3,4))
sampleDF.full_text[0]
plt.savefig(str(output_folder)+'NB01_6_NDVI_change_'+str(cyclone_name)+'_'+str(location_name))
opportunities.head(3)
fraud_df.head(3)
disposition_df.columns
rng
from sklearn.datasets import load_iris $ import matplotlib.pyplot as plt $ iris_dataset = load_iris()
consol_px = clean_nas(load_consol_px(ticker_map, universe)) $ if(active_etf != None): companies = companies[companies['ETF']==active_etf] # filter by selected ETF $ tickers = companies.index.tolist() $ tickers=[i for i in tickers if i not in ['BF.b','BRK.b']] ## esta es una correcion extra para no incluir 'BF.b','BRK.b' $ consol_px = consol_px[list(set(tickers))]
df['WHO Region'].values
from sklearn.feature_extraction.text import CountVectorizer $ countvec = CountVectorizer() $ sklearn_dtm = CountVectorizer().fit_transform(df.body) $ print(sklearn_dtm)
acc.find(description='12', amount=1500)
y_pred = lgb_model.predict(X_test)
no_of_stations = engine.execute('SELECT count(*) FROM station').fetchall() $ print("No_of_stations", no_of_stations)
season07["InorOff"] = "In-Season" # Again, this assigns an identifier as to whether this is inseason or offseason.
pd.read_html(table.get_attribute('innerHTML').strip(), header=0)[2]
X = master_df[col_to_keep] $ y1 = master_df.response $ y2 = master_df.len_convo
pageviews.head(2)
print(2 ** 3)  # Exponentiation $ print(~3)  # Bitwise inverse $ print(2**120)  # Python ints are arbitrary precision, not 64-bit
students.weight.value_counts(ascending = True)
s = time.time() $ make(-1) $ e = time.time() $ print (e-s) $
np.sum(PyReverseDims(jArr), 0)
! pip install pymongo
pd.DataFrame(features['LAST(loans.MEAN(payments.payment_amount))'].head(10))
import matplotlib.pyplot as plt $ df3_month.columns
mean_sea_level = pd.DataFrame({"northern_hem": northern_sea_level["msl_ib(mm)"], $                                "southern_hem": southern_sea_level["msl_ib(mm)"]}, $                                index = northern_sea_level.year) $ mean_sea_level
import urllib3, requests, json $ headers = urllib3.util.make_headers(basic_auth='{username}:{password}'.format(username=wml_credentials['username'], password=wml_credentials['password'])) $ url = '{}/v3/identity/token'.format(wml_credentials['url']) $ response = requests.get(url, headers=headers) $ mltoken = json.loads(response.text).get('token')
plt.scatter(X,y2) $ plt.plot(X, np.dot(X_20, linear.coef_) + linear.intercept_, c='yellow') $ plt.xlabel('Hour of Day') $ plt.ylabel('Count')
df.iloc[[1,2,5],[0,3]]
data.loc[data.expenses.isnull(), 'expenses'] = data['expenses/price']*data['price_aprox_usd'] $ del data['expenses/price']
time_string = "2017-09-01 00:00:00" $ delta_hours = 1 $ start_time = pd.to_datetime(time_string)
df2[df2.user_id == 773192]
bacteria[0]
df['score_differential'] = df['fld_score'] - df['bat_score']
our_nb_classifier.last_probability
baseball.reindex(id_range, method='ffill', columns=['player','year']).head()
grafBoedo = boedoEnMesesGrouped['price_usd_per_m2']['mean'].plot(kind='line', color='c', title='Precio promedio mensual del m2 en Boedo') $ grafBoedo.set_xlabel("Periodo") $ grafBoedo.set_ylabel("Precio promedio m2")
text_classifier.set_step_params_by_name("text1_char_ngrams", ngram_range =(3,4), use_idf = False) $ text_classifier.get_step_params_by_name("text1_char_ngrams")
data.plot(x='batch_start_time', y='error', kind='line')
prob_new_conv = x_new_conv/x_ $ prob_new_conv $
tweet_counts_by_month['tweet_count'] = tweet_archive_master[['timestamp', 'tweet_id']].groupby(pd.Grouper(key='timestamp', freq='M')).count()
s = pd.Series(['EUR', 'DKK', 'EUR', 'GBP'], index=['NL', 'DK', 'DE', 'UK'], name='Currency') $ data.join(s)  #Add a new column from a series or dataframe.
df_2001
Lab7_Equifax.to_csv('Lab7_Equifax.csv',index=False)
data_sets = {} $ data_sets['15min'] = pd.read_pickle('final_15.pickle') $ data_sets['30min'] = pd.read_pickle('final_30.pickle') $ data_sets['60min'] = pd.read_pickle('final_60.pickle')
df = pd.read_sql_query(sql_query,con)
df_copy.info()
n_new = df2.query('group == "treatment"').shape[0] $ print(n_new) 
df_new = df_countries.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
noloc_df = df[(df.city == '') & (df.state == '') & (df.zipcode_initial == '')].copy() $ df = df[~((df.city == '') & (df.state == '') & (df.zipcode_initial == ''))].copy()
lr = LogisticRegression(C=0.01,solver='liblinear')
mean = np.mean(data['len']) $ print("The lenght's average in tweets: {}".format(mean))
c.execute('SELECT city FROM weather where warm_month = "July" and cold_month != "January" ') $ print(c.fetchall())
np.histogram(noaa_data[(noaa_data.index >= '2018-05-29') & (noaa_data.index < '2018-05-30')].loc[:,'AIR_TEMPERATURE'])
tweet_length.plot(figsize=(20,5),color='r')
import pandas as pd $ student_info = [(100, 62, 'F'), (120, 66, 'M'), (140, 68, 'M'), (110, 62, 'F'), (160, 70, 'M'), (140, 63, 'F'), (140, 66, 'F'), (110, 63, 'F'), (180, 72, 'M'), (190, 72, 'M'), (200, 73, 'M')] $ names = ['Mary', 'Mike', 'Joe', 'Janet', 'Steve', 'Alissa', 'Alison', 'Maya', 'Ryan', 'Paul', 'Michael'] $ students = pd.DataFrame(student_info, columns = ['weight', 'height', 'gender'], index = names) $ students
def isapple(string): $     return string == 'iPhone' $ data.phone = data.phone.apply(isapple)
v_id = df_link_yt['video_id'].value_counts().index[1] $ df_link_yt[df_link_yt['video_id'] == v_id]['twitter_id'].unique()
df['Gender'] = df['Gender'].apply(lambda x: 0 if (x == "male")  else 1) $ df.head()
size_test[:10]
week1_df.rename(columns={'24 - 28 July 2017' : 'title'}, inplace=True) $ week1_df['date'] = '24 - 28 July 2017' $ week1_df
most_retweeted = tweets.loc[tweets.snspostid.isin(grouped.iloc[:100,:].parentPost.astype(np.str).values)] $ most_retweeted.head(2)
pred_test=m.predict(True)
test['visitors'] = 0.2*preds1+0.2*preds2+0.3*preds3+0.1*preds4+0.2*preds5 $ test['visitors'] = np.expm1(test['visitors']).clip(lower=0.) $ sub1 = test[['id','visitors']].copy()
cleaned_asf_people_human_df = asf_people_human_df.select("*", $                            clean_gender_field_udf("Answer_gender").alias("cleaned_gender"))
import pandas as pd $ df = pd.read_csv("../Data/teaminfo.csv")
dfp.groupby('chamber').bill_type.count().plot(kind='bar', color='r') $ plt.xlabel("Chamber the Bill was Introduced") $ plt.title("") $ plt.show()
post_number = len( niners[niners['Jimmy'] == 'yes']['GameID'].unique() ) $ print post_number
tfidf_vocab = np.array(tfidf_vectorizer.get_feature_names())
champ[(champ['priority']=='A') | (champ['priority']=='B')].sort_values(by='best_time').head()
pd.merge(test, session, how=left)
import numpy as np $ date = np.array('2015-07-04', dtype=np.datetime64) $ date
daily_returns.plot(kind='scatter',x='SPY',y='XOM') $ plt.show()
df_likes.head()
df.drop(df[['prospectid', 'ordernumber', 'ordercreatedate', 'dnatestactivationdayid', 'xsell_gsa', 'xsell_day_exact' ]], axis=1, inplace=True)
df2[df2['group']=="treatment"].count()[0]
collection_reference.count_documents({'geo' : { '$ne' : None}})
df3 = df2.join(pd.get_dummies(df2['group'])) $ df3.rename(columns={"treatment":"ab_page"}, inplace=True) $ df3['intercept']=1 $ df3.head(1)
active_mailing = clean_users[clean_users['active']==1]['opted_in_to_mailing_list'].sum()/active_count
data.sample(5)
pd.Timestamp(datetime(2018, 1, 1))
fundret.loc['2017-04':].cumsum()
score_l50.shape[0] / score.shape[0]
plt.hist(p_diffs) $ plt.title("Simulated Results")
import sys $ if not sys.warnoptions: $     import warnings $     warnings.simplefilter("ignore")
import collections
!scrapy crawl myAppName
df_predictions_clean.head(15)
p_score
new_seen_and_click =  seen_and_click[~(seen_and_click['seen_at'] > seen_and_click['clicked_at']) ] $ print(new_seen_and_click.shape) $ new_seen_and_click.head()
df2.info()
dem_in = dem.apply(within_area,axis=1) $ dem.loc[dem_in,["date","type"]].groupby("type").agg(["min","max","count"])
df.info()
pickup_kmeans = KMeans(n_clusters=num_pickup_clusters, random_state=1).fit(pickup_coords)
contribs=pd.read_csv("http://www.firstpythonnotebook.org/_static/contributions.csv")
df['cleaned_text'] = cleaned_tweets
iplot(data.groupby('merged_by.login').size().sort_values(ascending=False)[:20].iplot(asFigure=True, kind='bar', dimensions=(750, 500)))
predictions_train.show()
festivals['lat_long'] = festivals[['latitude', 'longitude']].apply(tuple, axis=1)
print(label((predicted_image)>180).max())
import sys $ import logging
print("Probability of control group:", $       df2[df2['group']=='control']['converted'].mean())
check_rhum = rhum_fine[1] $ sns.heatmap(check_rhum)
aapl.plot()  # plots ALL Series at one $ plt.show()
origin = pd.read_csv('5to5.csv')
df["2016-04-28":"2016-05-27"].mean()
Reset_ind = Exchange_df.reset_index() $ Fields_df = Reset_ind[['Date_of_Order', 'Field', 'New_or_Returning', 'Sales_in_CAD']] $ Fields_df.head()
ma_8.plot() $ ma_34.plot()
segmented_rfm['RFMScore'] = segmented_rfm.r_quartile.map(str) + segmented_rfm.f_quartile.map(str) + segmented_rfm.m_quartile.map(str) $ segmented_rfm.head()
!ls ../src
d_utc.tz_localize('US/Eastern')
van15_fin['FirstMeta'] = van_final.groupby('userid')['pagetitle'].first().str.contains('/')
df = pd.read_csv('./Data/AAPL.csv', index_col=0) $ df.head()
df.drop("water_year2",axis='columns',inplace=True)
top_actions = ['show','index','search_results','personalize','search','similar_listings','reviews']
plt.hist(p_diffs); #Was expecting a normal distribution, this is fairly close to one
set_themes.drop(['set_num', 'num_parts', 'theme_id', 'id_x', 'id_y', 'is_spare'], axis = 1, inplace = True)
df = pd.read_csv('mydata.csv')
countries = pd.read_csv('countries.csv') $ new = df2.set_index('user_id').join(countries.set_index('user_id'))
committees = pd.read_csv("http://www.firstpythonnotebook.org/_static/committees.csv")
predicted_classes = model.predict_classes(X_test) $ y_true = data_test.iloc[:, 0] $ correct = np.nonzero(predicted_classes==y_true)[0] $ incorrect = np.nonzero(predicted_classes!=y_true)[0]
data = pd.read_csv("ml-100k/u.data", sep="\t", header=None, index_col=0) $ data.columns = ["item id" , "rating" , "timestamp"]
df2.head()
d1 = dt.datetime(yr, mo, dd, hr, mm, ss, ms) $ d2 = dt.datetime(yr + 1, mo, dd, hr, mm, ss, ms) $ print(d2-d1)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_partial_autocorrelation(therapist_duration, params=params, lags=30, alpha=0.05, \ $     title='Weekly Therapists Hours Partial Autocorrelation')
n_old = df2.query('landing_page == "old_page"').user_id.count() $ print(n_old)
twitter_archive_df['name'].value_counts(ascending = False)[:20]
gen3 = dta.t[(dta.b==1) & (dta.c==3)]
people["pets"] = pd.Series({"bob": 0, "charles": 5, "eugene":1})  # alice is missing, eugene is ignored $ people
geocoded_df = read_geocoded_df('geocoded_evictions.csv')
df2['user_id'].nunique()
S.decision_obj.stomResist.value = 'Jarvis' $ S.decision_obj.stomResist.value
nb_class = nb_classifier.train(v_train_full) $ test_predict = [nb_class.classify(t) for (t,s) in v_test]
from sklearn.cross_validation import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) $ model2 = LogisticRegression() $ model2.fit(X_train, y_train)
X_train.isnull().sum()
year_prcp_df = pd.DataFrame(year_prcp, columns=['date', 'precipitation']) $ year_prcp_df.head()
from sqlalchemy.orm import Session $ session = Session(bind=engine)
df[df.location_id>0].raw_location_text.value_counts().tail()
autos.price.value_counts().sort_index(ascending=True).head(20)
def localize_time(api_time, myTimeZone): $     api_time = datetime.datetime.strptime(api_time, "%a %d %b %Y %H:%M:%S %z") $     return api_time.astimezone(myTimeZone)
counts = pd.Series([632, 1638, 569, 115]) $ counts
c.execute(create_watch_table) $ con.commit()
experience.to_csv('../data/experience.csv')
feature_layer = major_cities_layers[0] $ feature_layer
weather_df["weather_main"] = weather_df.weather_main.str.replace("drizzle", "rain") $ weather_df["weather_description"] = weather_df.weather_description.str.replace("drizzle", "light rain") $ weather_df["weather_description"] = weather_df.weather_description.str.replace("heavy intensity drizzle", "moderate rain") $ weather_df["weather_description"] = weather_df.weather_description.str.replace("light intensity drizzle", "light rain")
import datetime $ treaty = {"reinsurer": "AIG", $         "treaty": "XOL layer", $         "tags": ["reinsurer", "treaty", "year"], $         "date": datetime.datetime.utcnow()}
to_be_predicted_Day1 = 24.61 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
import pandas as pd $ file_name = "data/survey_Sorsogon_Electricity_Water_wbias_projected_dynamic_resampled_1000_{}.csv" $ sample_survey = pd.read_csv(file_name.format(2010), index_col=0) $ sample_survey.head()
data[['returns', 'strategy']].std() * 252 ** 0.5
df['lead_mgr'].head()
from scipy.stats import norm $ norm.ppf(1-0.05) $
mileage = {} $ for brand in top_brand: $     top = autos[autos['brand']==brand] $     mileage[brand] = top['odometer_km'].mean() $
print("Converted users proportion is {}%".format((df['converted'].mean())*100))
print('KNN F1_score: ', f1_score(y_final, knn_predicted)) $ print('KNN Jaccard: ', jaccard_similarity_score(y_final, knn_predicted))
df3 = df2.copy() $ df3['intercept'] = 1 $ df3[['control','treatment']] = pd.get_dummies(df2['group']) $ df3.head()
np.shape(prec_us_full)
AAPL_array=df["log_AAPL"].dropna().as_matrix() 
matched_df = df.loc[df['Match']=='Match'].copy() $ matched_df[:5]
tweet_df.count()
for u in range(len(U_B_df)): $     ixs = np.unique([a.strip() for a in U_B_df.loc[u,'cameras']]) $     df_slice = USER_PLANS_df.loc[ixs] $     USER_PLANS_df.loc[str(ixs)] = (pd.Series([list(chain(*df_slice[c])) for c in df_slice.columns],index=USER_PLANS_df.columns)) $     USER_PLANS_df.drop(ixs,inplace=True)
to_be_predicted_Day1 = 21.34 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
city_fare=table_grouped_type["fare"].sum() $ city_fare
from nltk import pos_tag, ne_chunk $ from nltk.tokenize import word_tokenize $ tree = ne_chunk(pos_tag(word_tokenize("Antonio joined Udacity Inc. in California."))) $ IPython.display.display(tree)
model.fit(X,y) #fit the regression model using OLS
autos['num_cylinders'].unique()
df.loc[row,'source_url']
ls_cprc_columns = ['deidcrpc2date_index_dd','deidcrpc3date_index_dd','deidmetastaticdate_index_dd']
fillna_with_zeros(joined, 'city_hol')
df.groupby('Park Borough').count().sort(desc("count")).show(10)
temp_df = proportion_and_count(active_psc_records,'kind',len(active_psc_records)) $ temp_df.to_csv('data/viz/types_of_psc.csv') $ temp_df
gram_collection = db.gram_posts $ print(gram_collection.count())
all_data.head()
df_TempIrregular.set_index('timeStamp',inplace=True)
graf_counts.head(2)
import requests $ from collections import defaultdict $ from statistics import mean,median $
fig, axarr = plt.subplots( 1, 1, figsize=(20,10) ) $ plt.hist(editors.control["reverted_content_revisions"], log = True, alpha = 0.5, range = (0, 20), normed = 1, bins = 20, label = "Source editor only") $ plt.hist(editors.treatment["reverted_content_revisions"], log = True, alpha = 0.5, range = (0, 20), normed = 1, bins = 20, label = "Both editors") $ plt.legend(loc="upper right") $ plt.show()
from gensim import corpora, models, similarities $ from gensim.models import Word2Vec $ from sklearn.feature_extraction.text import TfidfVectorizer $ from sklearn.metrics.pairwise import cosine_similarity $ from sklearn.cluster import KMeans
json_data = r.json() $ for k in json_data.keys(): $     print(k + ': ', json_data[k]) $
twitter_df_clean['date'] = twitter_df_clean['timestamp'].apply(lambda time: time.strftime('%d-%m-%Y')) $ twitter_df_clean['time'] = twitter_df_clean['timestamp'].apply(lambda time: time.strftime('%H:%M:%S'))
df.year.unique()
hits_df.loc[outliers] = np.NaN
url='https://dzone.com/articles/top-5-data-science-and-machine-learning-course-for'
flight6.cache()
lst_date_to_set
new_doc = "Human computer interaction" $ new_vec = dictionary.doc2bow(new_doc.lower().split()) $ print(new_vec)  # the word "interaction" does not appear in the dictionary and is ignored
support_NNN.amount.sum()
sum(image_clean['jpg_url'].duplicated())
pd.crosstab(index=data["Survived"],columns=data["Sex"])
df['user_type'].nunique()
df_merged = pd.merge(df2, df_countries, how='inner', left_index=True, right_index=True) $ df_merged.head()
svc = search.best_estimator_
run txt2pdf.py -o"2018-06-12-0815 MAYO CLINIC HOSPITAL ROCHESTER - 2014 Percentiles.pdf" "2018-06-12-0815 MAYO CLINIC HOSPITAL ROCHESTER - 2014 Percentiles.txt"
grouped = tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['dog_stage']!='None'].groupby(['dog_stage'])['favorite_count']
avg_num_pets = round(len(pets_pet) / num_users_w_pets, 3) $ print("The average number of added pets is: ", avg_num_pets)
queryForSpecies = 'https://gc2.datadistillery.org/api/v1/elasticsearch/search/bcb/tir/tir?q={"query": {"bool": {"must": {"match": {"properties.source": "SGCN"}},"must": {"match": {"properties.scientificname": "Anodontoides ferussacianus"}}}}}' $ rSpecies = requests.get(queryForSpecies).json() $ display (rSpecies) $
%matplotlib inline $ new_df.describe() $ new_df[['created_time', 'total_likes']].plot(type='') $ new_df[['created_time', 'total_comments']].plot()
sdof_resp() # arguments aren't necessary to use the defaults. 
x_train, x_test, y_train, y_test = train_test_split(xarr, yarr, $                                                     test_size=0.3, random_state=1)
pp.barplot(df=df, filters={'technology': tecs, 'variable': ['ACT']}, $            title='ACT - light')
IDX_train, X_train, Y_train = shuffle(IDX_train, X_train, Y_train) $ IDX_test, X_test, Y_test = shuffle(IDX_test, X_test, Y_test)
print cbg.crs $ print g_geo.crs
"this is %s number %i" % ('string', 1)
for k in ['linear', 'poly', 'rbf', 'sigmoid']: $     clf = svm.SVR(kernel = k) $     clf.fit(X_train, y_train) $     confidence = clf.score(X_test, y_test) $     print("Confidence for kernel - ", k, "is ", confidence)
data = np.zeros((2,), dtype=[('A', 'i4'),('B', 'f4'),('C', 'a10')]) $ data[:] = [(1,2.,'Hello'), (2,3.,"World")] $ pd.DataFrame(data, index = ['first','second'], columns=['C','B','A'])
df[df.country == "es"].id.size
test = pd.Series(test)
final_data = final_data[jobs_data['clean_titles'].isin(freq_titles['clean_titles'])] $ final_data.nunique()
ben_fin['FirstMeta'] = ben_final.groupby('userid')['pagetitle'].first().str.contains('/')
us.loc[us['cityOrState'] == 'Puerto Rico', 'country'] = 'Puerto Rico' $ us[us['cityOrState'].isin(states) == False]['cityOrState'].value_counts(dropna=False)
twitter_ks = df[(~df.twitter.isnull()) & (df.state == 'KS')] $ twitter_ks.twitter.unique()
import pandas_profiling as pp $ pp.ProfileReport(train)
indexed_activity
p_value = (p_diff < ab_data_diff).mean() $ p_value $
Name_transformer = NameTransformer(inputCol='Name', outputCol='Name') $ DateTime_YearExtractor = YearExtractor(inputCol='DateTime', outputCol='Year') $ DateTime_MonthExtractor = MonthExtractor(inputCol='DateTime', outputCol='Month') $ DateTime_DayExtractor = DayExtractor(inputCol='DateTime', outputCol='Day') $ SexuponOutcome_transformer = FillMissingValues(inputCol='SexuponOutcome', outputCol='SexuponOutcome') $
p_diff = p_new - p_old $ p_diff
token_sendReceiveAvg_month = token_sendReceiveAvg_month.fillna(0)
autos[['date_crawled','last_seen','ad_created']].head()
titanic.pivot_table('survived', index='sex', columns='class')
ad_group_performance['Conversions'].median()
stories.submitter_user.iloc[3]
samples_query.print_records(40)
p_new=df2['converted'].sum()/len(df2) $ p_new
obser_eve # difference between the convert rate for the new page and $
topWords(M_NB_model, ftrs, 50)
df_r.melt(id_vars=['price_date'], value_vars=[i])
df2.query('landing_page == "new_page"')['user_id'].count() / df2['user_id'].count()
trips, pickup_neighborhoods, dropoff_neighborhoods = load_nyc_taxi_data() $ preview(trips, 10)
train_features = bind_features(train_reduced, train_test="train").cache() $ train_features.count()
for name, fullname in session.query(User.name, User.fullname): $ ...     print(name, fullname)
autos = autos[autos['registration_year'].between(1900,2018)] $ autos['registration_year'].value_counts(normalize=True).head(15)
len(new[new['converted']==1])
loss_history = [np.mean(lh) for lh in loss_history]
definition_2_details = client.repository.store_definition(filename_mnist, model_definition_2_metadata) $ definition_2_url = client.repository.get_definition_url(definition_2_details) $ definition_2_uid = client.repository.get_definition_uid(definition_2_details) $ print(definition_2_url)
series + pandas.Series({'a': 2, 'c': 2})
results2.summary()
y_class_baseline = demo.get_class(y_pred_baseline) $ cm(y_test,y_class_baseline,['0','1'])
df2['intercept'] = 1 $ df2.head() $
r.json()
print("Service URL: {}".format(web_service_full._service_url)) $ print("Service Key: {}".format(web_service_full._api_key))
grouped_by_year_DRG_max.tail()
cars = pd.read_csv("/Users/ankurjain/Desktop/autos.csv", encoding='latin1') $ cars.head()
run txt2pdf.py -o '870 - SEPTICEMIA OR SEVERE SEPSIS W MV 96+ HOURS.pdf'  '870 - SEPTICEMIA OR SEVERE SEPSIS W MV 96+ HOURS.txt'
df.query("converted==True").shape[0]/df.shape[0]
len(b_list)
data
classifiers_comparison[['name', 'best_params', 'F1_score']].sort_values('F1_score', ascending=False)
from sklearn.naive_bayes import MultinomialNB
hr2007.add(hr2006, fill_value=0)
AAPL.describe()
tweet_archive_clean.head()
grad_days_mean = records3[records3['Graduated'] == 'Yes']['Days_missed'].mean() $ grad_days_mean
df2 = df2.join(countries.set_index('user_id'), on='user_id') $ df2.head()
close_series = stock_data['close'] $ display(close_series.head(5)) 
df3['G'] = ['yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', ] $ df3
df
gscv.best_estimator_
df.info()
X_train = pd.DataFrame(im.transform(X_train),columns = X_train.columns)
taxi_hourly_df.loc[taxi_hourly_df.isnull().all(1) == True, "missing_dt"] = True $ taxi_hourly_df.loc[~(taxi_hourly_df.missing_dt == True), "missing_dt"] = False
girls_by_name = girls.set_index('Name') $ girls_by_name.head()
log_lm = sm.Logit(df2['converted'], df2[['ab_page', 'intercept']]) $ results = log_lm.fit() $ results.summary()
sharpelist = [] $ for coin in coins_top10: $     print(coin, sharpe(r[coin])) $ sharpe(rr.Fund)
year11 = driver.find_elements_by_class_name('yr-button')[10] $ year11.click()
X = pd.merge(X, at_menu_id_granularity, left_on='master_menu_id', right_on='menu_id', how='left') $ del X['menu_id']
glm_multi_v2
df2['intercept']=1 $ ab_page = ['treatment', 'control'] $ df2['ab_page'] = pd.get_dummies(df2.group)['treatment']
pd.value_counts(appointments[appointments['Specialty'] == 'RN/PA']['Provider'])
calls = pd.read_csv('./data/CallsRingCentral.csv')
ins.groupby("type").size()
from pyspark.sql import SQLContext $ sqlContext = SQLContext(sc) $ data = sqlContext.read.json("samples/sample.json")
p_old = df2.converted.mean() $ print(p_old)
df.head()  # let's see some samples
subwaydf['4HR_Entries'] = subwaydf['4HR_Entries'].where(subwaydf['SCP'] == subwaydf['SCP'].shift(1)) $ subwaydf['4HR_Exits'] = subwaydf['4HR_Exits'].where(subwaydf['SCP'] == subwaydf['SCP'].shift(1))
predictions = knn.predict(test[['property_type', 'lat', 'lon','surface_covered_in_m2','surface_total_in_m2','floor']])
import statsmodels.api as sm $ convert_old = df2.query(" landing_page == 'old_page' and converted == 1").shape[0] $ convert_new = df2.query(" landing_page == 'new_page' and converted == 1").shape[0] $ n_old = len(df[df['group']=='control']) $ n_new = len(df[df['group']=='treatment'])
donors_c['Donor Zip'] = donors_c['Donor Zip'].astype(str)
y_age_valid = valid[:,-1]
cities.notnull()
plt.style.use('default') $ %pprint
len_bins = pd.groupby(df['max_len'], by=[df.index.year, df.index.month]) $ print(len(len_bins)) $
px
nufission = xs_library[fuel_cell.id]['nu-fission'] $ nufission.print_xs(xs_type='macro', nuclides='sum')
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response = requests.get(url) $ with open('image-predictions.tsv', mode = 'wb') as file: $     file.write(response.content)
import calendar $ may_2016 = calendar.month(2016, 5) $ print(may_2016.encode('latin1').decode('cp1251'))
df_countries = pd.read_csv('countries.csv') $ df_countries.sort_values(['user_id'], inplace = True) $ df_countries.reset_index(drop = True, inplace = True)
dr_new_hours = weekly_resample(dr_new['AppointmentDuration']) $ dr_existing_hours = weekly_resample(dr_existing['AppointmentDuration'])
tubes
submit = test[['id', 'price_usd']]
df2.query('converted == 1').user_id.nunique() / len(df2)
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])
df_comment= pd.read_sql_table('comments',con=disk_engine) $ df_authors = pd.read_sql_table('authors',con=disk_engine) $ df_comment = pd.merge(df_comment,df_authors,left_on='author_id',right_on='authorId') $
df_users.describe()
pd.DataFrame(pdata)
result.tostring()
analyze_set[analyze_set['tweet_id']==744234799360020481].jpg_url
dictionary,posts_words_vectors = create_dictionary(posts_df["processed_text"])
import pandas as pd $ import numpy as np $ import requests
!ls | grep 'sample_data.json'  # this is the file
df = pd.read_hdf('thermal.h5', key='df')
df_2011 = pd.DataFrame(rows)
autos.columns = ['date_crawled', 'name', 'seller', 'offer_type', 'price', 'abtest', $        'vehicle_type', 'registration_year', 'gearbox', 'power_ps', 'model', $        'odometer', 'registration_month', 'fuel_type', 'brand', $        'unrepaired_damage', 'ad_created', 'nr_of_pictures', 'postal_code', $        'last_seen'] $
autos['registration_year'].describe()
returned_orders_data.to_csv("returned_orders_data.csv")
0.5
res_dict['search_metadata']
pd.date_range('2015-07-03', periods=8, freq='H')
data.head(-2)
Lab7.insert(loc = 6,column = 'Ticker_Symbol', value = ticker)
%timeit df_census.ward.astype(int)
response = requests.post(OAUTH_SERVER_URL, $                          data={'username': USER_NAME, 'password': PASSWORD, $                               'scope': 'cloud_api', 'grant_type': 'password'}, $                          auth=(CLIENT_ID, CLIENT_SECRET)) $ print_status(response)
"In the last {0} tweets, I've been most subjective {1} percent and neutral {2} percent".format(subjectivity.sum().counts, percentConvert(subjectivity.loc["positive"].percent), percentConvert(subjectivity.loc["neutral"].percent))
grouped = df_providers.groupby(['year']) $ (grouped.aggregate(np.sum)['disc_times_pay'])
max(tweet_json['id_str'].value_counts())
df_clean.rating_denominator = df_clean.rating_denominator.astype(int)
train['title_len'] = train.title.apply(len) $ train.boxplot('title_len', by='popular')
counts_compare = df1.join(df2)
train_4_reduced.shape, y_train.shape
train = train.sort_values(by='date_created', ascending=True) $ train = train.loc[train['date_created'] >= pd.to_datetime('2016-01-01')]
from keras.callbacks import EarlyStopping, ModelCheckpoint $ early_stopping = EarlyStopping(monitor='val_loss', patience=5)
autos['odometer_km'].value_counts().sort_index(ascending = False)
df.T
df_countries = pd.read_csv("countries.csv") $ df3 = df_countries.set_index('user_id').join(df2.set_index('user_id'), how = 'inner') $ df3.head()
sample_df = df[df['encrypted_customer_id'].isin(toy['encrypted_customer_id'].unique())]
df['reviw'] = df['review'].apply(preprocessor)
f_counts_week_device.show(1)
users[(users['LastAccessDate'] < (pd.Timestamp.today() - pd.DateOffset(years=1))) & $      (users['DaysActive'] < 10)]['DaysActive'].plot(kind = 'hist') $ plt.title('Histogram of Number of Active Days \n for Users that Left Permanently') $ plt.xlim((0,9)) $ plt.xlabel('Days Active')
df_group=df2.groupby(['group','landing_page']) $ df_group.describe() $ prob=(145310/(145274+145310)) $ print("Probability that an individual received the new page is "+str(prob))
xirrd[split]
bufferdf = bufferdf.groupby([bufferdf.RateCodeID])
from sklearn.neural_network import MLPClassifier $ n_net = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(1000, 500, 250, 100, 50), $                       random_state=1, verbose = True) $ n_net.fit(x_train,y_train > 0)
labels = ['AG_0_30','AG_30_50','AG_50_70','AG_70_UP'] $ df_CLEAN1A['AGE_groups'] = pd.cut(df_CLEAN1A['AGE'], bins=[0, 30, 50, 70, 100], labels=labels) $
news_df['topic'].value_counts()
url="https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?&limit=1&api_key=" $ r=requests.get(url) $ json_data=r.json() $ print(json_data) $
standalone_series = df['emails'] >= 120 $ standalone_series
austin.isnull().sum() $
actual_holidays=holidays_events[(holidays_events.transferred==False)] $ print("Rows and columns:",actual_holidays.shape) $ pd.DataFrame.head(actual_holidays)
!wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-GB,en-US;q=0.8,en;q=0.6" "https://storage.googleapis.com/kaggle-competitions-data/kaggle/6322/train-jpg.tar.7z?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1532362578&Signature=bEVBoqLUCBiiz7J%2F%2FjIley%2F6k4eQ273wCeqvGZ3l40AMu03p3yJEFy%2FoWM6Xp5sLLZm5jrxNEW0lZbY7qEf3Fkj1KqnhYt7FsjRUcngewQ6vpGH79Oq0A4%2B9y6J%2F8gcL%2BtMZKTHkeh7HpLUGegji8T3l7lCv4LoVSsdpnT4aWpuvVp2UtNOLvGxW%2FSm1GKWIu6rqmxvixzH6S3zeSmTkBvxwZyDdCFT38MYqvn6Djr9VuIbMM6FvX0ZLL47o0VW%2BW3shEcumO8wdC1Srl2fayyhflUT%2BLkTqsXdQMCYV4KGRAH5xWJbd4cLvixpOymjduZGsbwEJb94V6tr1H6AL1w%3D%3D" -O "train-jpg.tar.7z" -c
df2.query('user_id == ' + str(repeated_id[0])) $
twitter_data.name.value_counts()
plt.hist(p_diffs);
print('Reply tweets: ' + str(len(tweets_clean.query('in_reply_to_status_id == in_reply_to_status_id')))) $ print('Total tweets: ' + str(len(tweets_clean))) $ tweets_clean = tweets_clean[tweets_clean.in_reply_to_status_id != tweets_clean.in_reply_to_status_id]
pd.Timestamp('2015.11.30')
df['TMAX'] = df['TMAX'] * 9/5 + 32 $ df['TMIN'] = df['TMIN'] * 9/5 + 32 $ df['TOBS'] = df['TOBS'] * 9/5 + 32 $ df['PRCP'] = df['PRCP'] * 0.0393701
ride_kmeans = KMeans(n_clusters=num_ride_clusters, random_state=1).fit(ride_coords)
df[["first_rental", "new_customer", "repeat_customer"]] = pd.get_dummies(df["reservation_frequency"])
doctors.head()
nba_df.index
ws = Workspace(workspace_name = workspace_name, $                subscription_id = subscription_id, $                resource_group = resource_group) $ ws.write_config()
store_items['new watches'] = store_items['watches'][1:] $ store_items
df_q = pd.read_sql(query, conn, index_col=None) $ df_q.head(10)
actual.head()
print(age.min()) $ print(age.max())
nntest = pd.DataFrame(labeled_features.loc[labeled_features['datetime'] >= pd.to_datetime('2015-10-01 00:00:00')]) $ pred = model.predict(test_x) $ pred = np.argmax(pred, axis=1) $ nntest['predicted_failure'] = enc.inverse_transform(pred)
ridgereg = Ridge(alpha=.1) $ ridgereg.fit(X_train, y_train) $ y_pred2 = ridgereg.predict(X_test) $ print np.sqrt(metrics.mean_squared_error(y_test, y_pred1))
plt.hist(drt_avg17, bins=20, align='mid'); $ plt.xlabel('Defensive Rating') $ plt.ylabel('Count') $ plt.title("Histogram of Teams' Defensive Rating, 2017-2018 Season\n");
with open(home_dir + '/general_warehouse_key.json', 'rb') as a: $     Config = json.load(a) $ Historical_Raw_Data = Connect_Sql_Warehouse( $     Config, Historical_Demand_AU, "Historical Demand")
holidays
categorical_features = ['payment_method_id','city','registered_via']
blame.timestamp = blame.timestamp.astype('int64') $ blame.head()
import feather $ df = feather.read_dataframe('meterdata_with_datetime.feather') $ df.head()
df_nona['create_date'] = df_nona.created.map(pd.to_datetime) $ app_usage = df_nona.groupby('create_date').count().app_id $ app_usage.resample('w', sum).plot(title='Adoption through time') $
df2.query('landing_page == "new_page"').count()[0] / df2.shape[0]
plotly_df.groupby('month').sum()
user_summary_df[user_summary_df.gender == 'F'][['followers_count', 'following_count', 'tweet_count', 'original', 'quote', 'reply', 'retweet', 'tweets_in_dataset']].describe()
classification_data.groupby('primary_role').size()
len(zero_labels)+len(final_labels)
P.plot_1d_layer('iLayerHeight')
len(df)==df.timestamp.nunique()
weekdays_avg.reset_index(inplace=True) $ weekdays_count.reset_index(inplace=True) $ weekends_avg.reset_index(inplace=True) $ weekends_count.reset_index(inplace=True)
val=Series([-1.2,-1.5,-1.7],index=['two','four','five']) $ frame2.debt=val $ frame2
df_archive_clean.text = df_archive_clean.text.apply(lambda row: row[:-24])
filename = 'expr_3_qi_nmax_32_nth_1.0_g_101_04-13_opt_etgl.csv' $ df1 = pd.read_csv('../output/data/final/' + filename, comment='#')
token_receiveavg = token_receivecnt.groupby("receiver").agg({"receivecount":mean_except_outlier}).reset_index()
import pandas as pd $ dict(pandas=pd.__version__)
d.visualize()
aux = aux.join(pd.get_dummies(aux['country'])) $ aux.drop(['user_id', 'country', 'CA'], axis=1, inplace=True) $ aux.head()
S_lumpedTopmodel.decision_obj.bcLowrSoiH.options, S_lumpedTopmodel.decision_obj.bcLowrSoiH.value
baseball.describe()
df2[df2.duplicated(['user_id'])]
tmp_cov_1 = tmp_cov.reset_index(level=0) $ tmp_cov_1
df_elect.head()
dfb_test.describe()
countries = pd.get_dummies(df_new['country']) $ df_new = df_new.join(countries) $ df_new.drop(['CA'],axis=1,inplace=True) $ df_new.head()
train.groupby(by = 'Page').mean().head()
!curl -O https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-population.csv $ !curl -O https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-areas.csv $ !curl -O https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-abbrevs.csv
print(int(2.5)) # Convert to int with truncation $ print(round(2.5))  # Convert to int with rounding (oddly, round() with 0 $ print(round(3.5))  #   decimal places rounds to even number, not up). $ print(round(2.5001))  # Convert to int with rounding
number_of_commits = git_log.timestamp.count() $ number_of_authors = git_log.author.value_counts(dropna=True).count() $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
from functions import et_helper $ et_helper.plot_around_event(etsamples_grid,etmsgs_grid,etevents_grid,raw_large_grid_df.query("eyetracker=='pl'&subject=='VP4'&block==1").iloc[5],plusminus=(-2,5))
df2[['group_receive_1','ab_page']]=pd.get_dummies(df2['group']) $ df2.head()
%matplotlib inline $ from matplotlib import pyplot as plt $ import numpy $ x=numpy.linspace(0,1,1000)**1.5
pd.Timestamp('January 1st 2018')
liquor['Sale (Dollars)'] = [s.replace("$","") for s in liquor['Sale (Dollars)']] $ liquor['Sale (Dollars)'] = [float(x) for x in liquor['Sale (Dollars)']] $ liquor_state_dollars = liquor['Sale (Dollars)']
concerns = ['sexual','abuse','kill','murder','suicide', 'rape'] $ medical = wk_output[wk_output.explain.str.contains('|'.join(concerns))] $ medical.shape $ medical.to_csv('medical_feedback.csv')
bd.reset_index(drop=True)
twitter_archive_enhanced_clean.timestamp.max
learner.save_encoder('lm1_enc')
yc = np.array(y)
import calendar
tweet_archive_enhanced_clean =  pd.merge(tweet_archive_enhanced_clean,tweet_json[['retweet_count','favorite_count','id']], how ='left', left_on ='tweet_id',right_on ='id')
df.info()
scr_retention_sum = pd.Series([sum(scr_retention_df[col]) for col in scr_retention_df.columns],index=scr_retention_df.columns,name='Total')
print(teamAttr.shape) $ teamAttr.head(3)
df_data['COM_LUZ_SOLAR'] = (((df_data.PERIDOOCORRENCIA_TIPO==1) | (df_data.PERIDOOCORRENCIA_TIPO==2))) $ df_data['SEM_LUZ_SOLAR'] = (((df_data.PERIDOOCORRENCIA_TIPO==3) | (df_data.PERIDOOCORRENCIA_TIPO==4)))
train_visitor_map.head()
All_tweet_data_v2.rating_numerator.value_counts().sort_index()
S = Simulation(hs_path+'/summaTestCases_2.x/settings/wrrPaperTestCases/figure07/summa_fileManager_riparianAspenSimpleResistance.txt')
df2.drop(1899, inplace=True)
print('Density distribution of the error') $ sb.distplot(errors) $ plt.xlabel('error')
np.std(p_diffs)
data_config = dict(path='tests/data/nhtsa_as_xml.zip', $                    databasetype="zipxml", # define that the file is a zip f $                    echo=False)
frames_liberia = [first_values_liberia, last_values_liberia, count_liberia] $ result_liberia = pd.concat(frames_liberia, axis=1) $
hamburg.info()
%%time $ dask_df['new_col'] = dask_df.body.apply(clean_func) $ dask_df['token_body'] = dask_df.new_col.apply(tokenizer)
pgn2value = dict(pd.read_csv('../data/resultlist.csv').values[:,1:])
fires = pd.read_csv('../../datasets/san_francisco/san_francisco/fire_data/fire_incidents.csv') $ crimes = pd.read_csv('../../datasets/san_francisco/san_francisco/sf_crime/sfpd_incidents_from2003.csv') $
%%time $ prepared_test = create_feature(test_df) $
df_r2.loc[df_r2["CustID"].isin([customer])]
archive_copy['name'].loc[archive_copy['name'].str.islower()]
!pttree --use-si-units --sort-by 'size' 'data/NYC-yellow-taxis-100k.h5'
df2['new_page_uk'] = df2['new_page'] * df2['country_uk'] $ df2['new_page_us'] = df2['new_page'] * df2['country_us'] $ df2.head()
tweet_image.head()
corrplot(rr.loc[start_date:].corr().round(1), annot=True) $ plt.title('Correlation matrix - monthly data \n from ' + start_date + ' to ' + end_date) $ plt.savefig('output/corr-fund_top10-monthly_largesize.png', dpi=750) $ plt.savefig('output/corr-fund_top10-monthly_smallsize.png') $ plt.show()
logit_mod = sm.Logit(df_new["converted"], df_new[["intercept","CA", "UK"]]) $ results = logit_mod.fit() $ results.summary()
stores["GrandTotalSales"] = stores["TotalSales"] * stores["Total_Customers"] $
datacounts = pd.read_sql_query(query,engine)
sns.lmplot(x="income", y="satisfied", data=training, x_estimator=np.mean, order=1)
acs_df.dropna(inplace=True) $ acs_df = acs_df[['pop', 'age', 'pct_male', 'pct_white', 'income', 'unemployment', 'pct_bachelors', $                   'crime_count', 'permit_count', 'homeval']]
pd.concat([city_loc.set_index("city"), city_pop.set_index("city")], axis=1)
likes[likes['actor'] != 'Adrian Fernandez Jauregui']
data
from xml.dom import minidom $ from lxml import etree as ET #Supports xpath syntax $ from collections import defaultdict $ from pprint import pprint
dfMeanFlow.head()
twitter_ar['source_url'] = twitter_ar['source_url'].map(lambda x: x.lstrip('<a href=').rstrip('" ')) $ twitter_ar['source_type'] = twitter_ar['source_type'].map(lambda x: x.lstrip(' ="nofollow">').rstrip('</a>'))
unsorted_Google_data = pd.read_csv(Google_data_file) $ unsorted_Google_data.head()
def fix_runtime(runtime): $     return runtime.split(' ')[0]
x
total_rows2 = df2.shape[0] $ users_converted2 = df2.converted.sum() $ prob_user_conversion = users_converted2 / total_rows2 $ print(prob_user_conversion)
raw_authors_by_project_and_commit_df = git_project_data_df.select("project_name", "data.Author", "data.CommitDate") $
%matplotlib inline $ %reload_ext autoreload $ %autoreload 2
df['A']
data["Improvements_split"] = data["Improvements"].apply(splitcol)
F_ms = m_ms[score_variable].var() / a_ms[score_variable].var() $ degrees1ms = len(m_ms[score_variable]) -1 $ degrees2ms = len(a_ms[score_variable]) -1
auto = auto.drop("DatePosted", axis=1) $ auto = auto.drop("Difference_Date", axis=1)
usersDf.hist(column=['friends_count'],bins=50) $
conn.commit()
modifications_over_time['modifications_norm'] = modifications_over_time['modifications'].clip_upper( $     modifications_over_time['modifications'].quantile(0.99)) $ modifications_over_time[['modifications', 'modifications_norm']].max()
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs) $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('Simulated Difference of new_page & old_page converted under the Null');
pilot_timestamps = timestamps_between(str_2017, end_2017, 100) $ df_pilots["created"] = pilot_timestamps
twitter_archive_df.columns
nan_sets = {} $ for res_key, df in data_sets.items(): $     data_sets[res_key], nan_table = find_nan(df, headers, res_key, patch=True) $     nan_sets[res_key] = nan_table
contribs = pd.read_csv("http://www.firstpythonnotebook.org/_static/contributions.csv")
pred7 = nba_pred_modelv1.predict(g7) $ prob7 = nba_pred_modelv1.predict_proba(g7) $ print(pred7) $ print(prob7)
print("Probability of control group converting:", $       df2[df2['group']=='control']['converted'].mean())
df = train[columns]
df.tail(5)
print(X_resampled.shape, y_resampled.shape)
df.sort_values("Year",ascending=True)
dff-DataFrame(df_true['GOOG'])
print(input_hash_tag.value)
preci_df = pd.DataFrame(preci_data) $ preci_df.head()
metrics.precision_score(y_valid, y_pred)
try: $     cur_a.execute('UPDATE hotel set hotel_id=99 WHERE hotel_id=1') $ except Exception as e: $     print('Exception: ', e)
dfbrexits = dfstationmeanbreak[['exits','STATION']] $ dfbrexits.nlargest(10, ['exits'])
data['intercept'] = 1.0
dataurl1 = urlopen('https://s3.amazonaws.com/tripdata/JC-201707-citibike-tripdata.csv.zip') $ dataurl2 = urlopen('https://s3.amazonaws.com/tripdata/JC-201701-citibike-tripdata.csv.zip') $ data1 = pd.read_csv(io.BytesIO(dataurl1.read()), compression='zip', sep=',') $ data2 = pd.read_csv(io.BytesIO(dataurl2.read()), compression='zip', sep=',')
print(len(df_concat)) #Prints number of rows. $ df_concat.head()
fig, ax = plt.subplots() $ typesub2017['Wind Onshore'].plot(ax=ax, title="Onshore Wind Energy Generation 2017 (DE-AT-LUX)", lw=0.7) $ ax.set_ylabel("Onshore Wind Energy") $ ax.set_xlabel("Time")
asdf[:5]
oppose_NNN = merged_NNN[merged_NNN.committee_position == "OPPOSE"]
lmscore.resid.head(4)
tweets_master_df.ix[774, 'text']
notus['country'] = notus['country'].str.replace('United States of America', 'USA') $ notus['country'] = notus['country'].str.replace('Maryland', 'USA')
open('test_data//open_close_test.txt').read()
mpiv=pd.pivot_table(movie,index=['Date'],columns=['Name'],values=['Point']) $ mpiv.head()
deltat= t1 - t2
graf_counts.head()
imagelist = $ [ 'BARNES JEWISH HOSPITAL title_page.pdf', $  'BARNES JEWISH HOSPITAL ALL DRGs.pdf', $  'BARNES JEWISH HOSPITAL  Top 10 DRGs -    2015  Pay vs. Discharges .pdf', $  'BARNES JEWISH HOSPITAL 260032.pdf']
length = [len(h.tweets) for h in heap]
daily_feature = sentiment_normal_avg_byday(data_demo, "vader")
df[['id','price_doc']][:5]
twitter_archive_clean[twitter_archive_clean['in_reply_to_status_id'].isnull()==False]
s = pd.Series([2, 3, 1, 5, 3], index=['a', 'b', 'c', 'd', 'e']) $ s.plot()
df_capsule_name = df_capsule[['Capsule id','capsule name']]
with open('tweet_json.txt', 'a') as outfile: $     json.dump(tweets_list, outfile)
stream_context.stop()
grab_state_data()  # Calling the function to save the quandl data
X_tr[0:len(X_train)-40-1].shape
df_train.head()
pred.info() $
autos['lastSeen'] = autos['lastSeen'].str[:10] $ (autos['lastSeen'] $ .value_counts(normalize=True,dropna=False) $ .sort_index(ascending=True))
print("All Tweets: {0} | Users: {1}".format(len(df), df.user.nunique())) $ print("Tweets in the Harvey 160 Collection: ", len(df.query('harvey160'))) $ print("Users in the Harvey 160 Collection: ", df.query('harvey160').user.nunique())
print(pd.date_range('2018-01-01',periods=4,freq='BQ'))
lr.fit(X_train, y_train)
grades > 'C'
autos["date_crawled_10"]= autos["date_crawled"].str[:10] $ autos["ad_created_10"]=autos["ad_created"].str[:10] $ autos["last_seen_10"]=autos["last_seen"].str[:10]
tweet_counts_stock_df.head()
dfc = pd.read_csv('countries.csv') $ dfc.head()
for dataset in combine: $     dataset['Embarked'] = dataset['Embarked'].fillna(freq_port) $ train_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY $ request = requests.get(url)
rnd_reg_2 = RandomForestRegressor(n_estimators=500, max_leaf_nodes=16, $                                 n_jobs=-1, oob_score=True)
temp_df2['titles'] = temp_df2['titles'].str.lower()
kickstarter.describe()
python.detect_language()
cond = df_full['index'].isin(data_l2_end + data_other) $ df_full.loc[cond,'school_name'] = df_full[cond]['school_code'].map(DATA_SUM_DICT) $ df_full.loc[cond,'school_code'] = INVALID_SCHOOL_CODE $ cond = df_full['index'].isin(data_other) $ df_full.loc[cond,'school_type'] = ALL_SCHOOL_TYPE
ax = mains.plot() $ co.steady_states['active average'].plot(style='o', ax = ax); $ plt.ylabel("Power (W)") $ plt.xlabel("Time");
df_new['current_date'] = '2018-06-24' $ df_new['current_date']= df_new['current_date'].apply(pd.to_datetime) $ df_new['current_date']
import seaborn as sns $ import matplotlib.pyplot as plt $ from matplotlib.colors import ListedColormap $ %matplotlib inline
e = abs(y_test_pred - y_test) $ for th in [1,2,3]: $     e_ = e[e <= th] $     print('Accuracy for error less or equal to {} year: {}'.format(th,np.round(len(e_)/len(e),2)))
key = os.environ.get('YT_API')
from scipy import misc $ image = misc.imread('data/image.bmp')
tickerdata.head(5)
print() $ print('Number of non-NaN values in the columns of our DataFrame:\n', store_items.count())
res = requests.get('http://togows.org/search/kegg-pathway/cancer/1,50.json') $ pp(res.json())
print(len(list_others)) $ print(len(list_tweets)) $ print(len(list_tweets_je))
data = [{'a': i, 'b': 2 * i} $         for i in range(3)] $ pd.DataFrame(data)
for doc in corpus_lsi: $     print(doc)
a = pick_a_site()
posts = got_data.head(10)[['id', 'message']] $ posts['link'] = 'https://facebook.com/'+posts['id'] $ for post in posts['link']: $     print(post)
logit_multi_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_test','UK','US']]) $ results = logit_multi_mod.fit() $ results.summary()
import pandas as pd $ import numpy as np $ from datetime import datetime, timedelta $ import calendar
festivals.head(5)
to_be_predicted_Day3 = 21.29160417 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
closing = [] $ for day in dict_17: $     closing.append(dict_17[day]['Close']) $ largest_change = max(closing) - min(closing) $ largest_change
df[df['converted']==1].count()[0]/df.count()[0]
df_protest.info()
autos.price.unique().shape
titanic_df.head()
archive.info()
prcp_12monthsDf.head()
if 1 == 1: $     news_titles_sr = pd.read_pickle(news_period_title_docs_pkl)
s = pd.Series(['Tom ', ' William Rick', 'John', 'Alber@t']) $ s
fig = sns.countplot(x = 'weekday', hue = 'subscription_type', data = trip_data) $ plt.xticks(np.arange(0,7),['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])
images_df.head()
print('Some examples of companies with very old PSCs:') $ active_companies[active_companies.CompanyNumber.isin(very_old_pscs.company_number.unique())][['CompanyNumber','CompanyName','URI']].head(5)
gs_rfc_under = GridSearchCV(pipe_rfc, param_grid=params_rfc, cv=ts_split_under, scoring='roc_auc') $ gs_rfc_under.fit(X_train, y_train_under)
X_train.info()
a[a.find(':'):]
print userItemInt.shape $ userItemInt.head()
df_new = df2[df2['landing_page'] == 'new_page'] $ converted = df_new['converted'] $ new_page_converted = np.random.choice(converted,n_new)
output= " select tag_id, sum(retweets) as crt from tweet_details as td inner join tweet_tags as tt where td.tweet_id=tt.tweet_id group by tag_id order by crt desc limit 8; " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['tag_id','Sum'])
data.to_csv('time_series_arsenal.csv')
page_html = load_or_get(10,1)
commons_site = pb.Site("commons", "commons")
print("Probability of treatment group converting:", $       df2[df2['group']=='treatment']['converted'].mean())
(violations08Acounts / boro_counts).plot(kind='bar')
df.tail(3)
air_reserve.head()
ben_out = ben_fin.drop(['diffs','pagetitle','stiki_mean','cluebot_mode'],axis=1)
sampled_contirbutors_human_agg_by_gender_and_proj_df = non_blocking_df_save_or_load_csv( $     group_by_project_count_gender(sampled).repartition(1), $     "{0}/sampled_contirbutors_human_agg_by_gender_and_proj_3c".format(fs_prefix)).alias("sampled")
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_ipb_brace_crown[0][0])) $ perr = np.sqrt(np.diag(pcov_ipb_brace_crown[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
api_key = '' $ api_secret = '' $ access_token = '' $ access_token_secret = ''
sandwich_test.iloc[2]['reviews_text']
customer_visitors_new = customer_visitors_new.unstack(1) $ customer_visitors_new.columns = customer_visitors_new.columns.droplevel(0) $ customer_visitors_new = customer_visitors_new.reindex_axis(sorted(customer_visitors_new.columns, key= lambda x: list(calendar.day_name).index(x)), axis=1) $ customer_visitors_new
Correct the rating of denominators that are > 10
squares = list(map(lambda x: x * x, [0, 1, 2, 3, 4])) $ squares
data_2018 = data_2018.reset_index()
1/np.exp(-0.0408) $
client.experiments.list_runs()
for post in snapshotted_posts: $     post['later.score'] = youngest_score[post['id']]['score'] $     post['later.score.interval.minutes'] = youngest_score[post['id']]['later.score.interval.minutes']
cols = ['Respiratory Rate', 'Mean Airway Pressure', 'Inspired Tidal Volume', 'SpO2', 'Heart Rate', 'Extrinsic PEEP', 'Pulse Rate'] $ data1, features = create_features(data, t_before='300s', t_moving='180s', n_before = 6, cols = cols) $ data1 = data1[data1['SpO2'].isnull() == False] $ print(data1.groupby(['y_flag']).size())
y = df['comments'] $ X = df['subreddit']
df = pd.concat(dfs[4:12]) $ df
ben_final['FirstMeta'] = ben_final.groupby('userid')['pagetitle'].first().str.contains('/')
trips_completed = stats.binom.rvs(n=1, p=0.95, size=len(df_trips)) $ df_trips["trip_completed"] = trips_completed
net_loans_exclude_US_outstanding_user.head()
firstWeekUserMerged[firstWeekUserMerged.objecttype.isnull()].head(5)
df_estimates_false = df_estimates_false.dropna(axis=0, subset=['points']) $ print(df_estimates_false) $
df3.dtypes
breed_ratings = df_twitter.groupby('p1')['score_rating'].mean()
df2.query('converted==1')['user_id'].count()/df2['user_id'].count()
merged2.index
df2['intercept'] = 1 $ df2[['ab_page','B']] = pd.get_dummies(df2['landing_page']) $ df2 = df2.drop('B',axis = 1) $ df2.head()
df.groupby("cancelled")["pickup_hour"].mean()
cfModel.compile(loss='mse', optimizer='adam')
for i in ['Updated Shipped diff']: $     t = df[i].hist(bins=500) $     t.set_xlim((0,500)) $     t.set_ylim((0,2000))
import statsmodels.api as sm $ logit = sm.Logit(df['converted'], df[['intercept', 'treatment']])
tlen = pd.Series(data=data['len'].values, index=data['Date']) $ tfav = pd.Series(data=data['Likes'].values, index=data['Date']) $ tret = pd.Series(data=data['RTs'].values, index=data['Date']) $ tlen.plot(figsize=(16,4), color='r');
LT906474 = pd.read_table("GCA_900186905.1_49923_G01_feature_table.txt.gz", compression="infer") $ CP020543 = pd.read_table("GCA_002079225.1_ASM207922v1_feature_table.txt.gz", compression="infer")
merged_df.head()
store_items.fillna(0)
df['2016-12':]
net.save_edges(edges_file_name='edges.h5', edge_types_file_name='edge_types.csv', output_dir=directory_name)
p_value, z_score
bnbB= bnbx[(bnbx['age']<80) & (bnbx['age']>=18)]
max_activity = indexed_activity.station_count.max()
nbar_clean = xr.concat(sensor_clean.values(), 'time') $ nbar_clean = nbar_clean.sortby('time') $ nbar_clean.attrs['crs'] = crs $ nbar_clean.attrs['affin|e'] = affine
100.0*(1-df_outcomes).sum()/len(df_outcomes)
print("no. of converted users = ",(df["converted"].mean()*100),'%')
tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_stem, $                             stop_words=stopwords.words('english'), $                             max_df=0.5, $                             min_df=0.1, $                             lowercase=True)
r.json()
df2.nunique()
series1 = df2['DepTime'].apply(deptime_to_string) $ series1.head(10)
pd.options.display.max_rows = 100 $ pd.options.display.max_colwidth = 300 $ all_tweets.iloc[:30,7:8].style.set_properties(**{'text-align': 'left'})
pop={'Nevada':{2001:2.4,2002:2.9},'Ohio':{2000:1.5,2001:1.7,2002:3.6}} $ frame3=DataFrame(pop) $ frame3
new_reps.tail(10)
log_mod1 = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $
prob_convert = df2.converted.mean() $ prob_convert
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ WHERE twitter_day = 6 \ $ GROUP BY tag_text ORDER BY count DESC LIMIT 1;
train_df.head()
alice_sel_shopping_cart = pd.DataFrame(items, index=['glasses', 'bike'], columns=['Alice']) $ alice_sel_shopping_cart
loan_stats["revol_util"].head(rows=2)
p_converted_treatment_user2 = df2.query('converted==1 and group=="treatment"').user_id.nunique()/df2.query('group=="treatment"').user_id.nunique() $ p_converted_treatment_user2
xgb_learner.best_model.best_iteration
swPos.shape
poly17= PolynomialFeatures(degree=17) $ X_17 = poly17.fit_transform(X) $ linear = linear_model.LinearRegression() $ linear.fit(X_17, y2) $ (linear.coef_, linear.intercept_)
afl_data = prepare_afl_data()
logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit_mod.fit() $ results.summary()
[np.exp(0.0507),np.exp(0.0408)]
pd.read_csv("ign.csv",nrows=4)
print(np.unravel_index(100,(6,7,8)))
doctype_by_day.columns.isin(doctype_by_day.min().sort_values(ascending=False)[:10].index)
als_ref = pd.read_excel("in/ref.xlsx")
p_old = df2.converted.sum()/df2.converted.count() $ old_page_converted = np.random.choice([0,1],old_page.landing_page.count(), p=(p_old,1-p_old))
o = np.exp(-0.0124) $ print('Odds Ratio: {:.2f}'.format(o))
uber_15 = pd.read_csv('../data/uber/uber-raw-data-janjune-15.csv')
ttTimeEntry = deepcopy(ttAll)
bnb[bnb['age']>1000].age.head(10)
df.idxmax()
archive_df_clean.rating_denominator.value_counts()
df8 = pd.read_csv('2008.csv')
final_df_columns=["Date" , "Day", "Month", "Country" , "Number of new cases" , "Total number of deaths"] $ final_df=pd.DataFrame(columns=final_df_columns)
q_mine = c.submit_query(PTOQuerySpec().time("2018-06-10", "2018-06-12").set_id(0x6) $                                       .condition("ecn.negotiation.*") $                                       .group_by_condition())
giss_temp = giss_temp.set_index("Year") $ giss_temp.head()
p_diffs=np.array(p_diffs) $ plt.hist(p_diffs) $ plt.title('Graph of p_diffs')#title of graphs $ plt.xlabel('Page difference') # x-label of graphs $ plt.ylabel('Count') # y-label of graphs
new_page_converted = df2.sample(n_new, replace = True) $ p_new = new_page_converted.converted.mean() $ p_new
df_goog
states.set_index(['day'])
query = 'CREATE TABLE new_york_new_york_points_int_ct10 AS (SELECT geoid, osm_id, latitude, longitude FROM beh_nyc_walkability, new_york_new_york_points WHERE ST_INTERSECTS(beh_nyc_walkability.the_geom, new_york_new_york_points.the_geom))' $
from collections import Counter
[i for i in b1.index if i not in b2.index]
cat_outcomes.head()
pd.to_datetime(pgh_311_data['CREATED_ON']).head()
s2 = pd.Series(np.random.randint(0, 7, size=10)) $ s2
dat.status[dat.completed.isnull()].unique()
df = pd.read_csv('/Users/jledoux/Documents/projects/Saber/baseball-data/statcast_with_shifts.csv')
df_questionable = pd.merge(left= df_links, left_on= 'link.domain', $                            right= df_os, right_on= 'domain', how= 'inner')
props = pd.read_csv("http://www.firstpythonnotebook.org/_static/committees.csv")
gps_df = full_df[['latitude','longitude']].copy() $ gps_df.head(5)
top20_brands = autos.brand.value_counts().head(20).index
logit_mod = sm.Logit(df2['converted'], df2[['ab_page', 'intercept']]) $ results = logit_mod.fit()
exec("def simon(x): return x*10") $ print(simon("Ha "))
us_uk=['CA','US']
def parsear_zona(row): $     mylist = row.loc['place_with_parent_names'].split("|") $     return mylist[3] $ prop_caba_gba.loc[:,'neighborhood'] = prop_caba_gba.apply(parsear_zona, axis=1)   
fix_zip('890')
complete_df = df2.merge(country_df, on = 'user_id', how = 'inner') $ complete_df.head()
tickers = ['AAPL', 'FB', 'GOOGL', 'NFLX'] $ ticker_okays = [True, False, True, True] $ names = ['apple', 'facebook', 'google', 'netflix']
data.columns
groupby_example.groupby('key').sum()
from pyspark.sql.functions  import to_date $ df = df.withColumn("post_creation_date", to_date(df["post_creation_date"],'yyyy-MM-dd'))
new_page_converted = np.random.binomial(1,0.1196,145310) $ new_page_converted
print("Don't worry MisoMunje, there are") $ print("only {} authoritarian notable items".format(len(asdf))) $ print("in {} weeks.".format(max(asdf['wk'])))
params_file_path = os.path.join(data_dir, "params.tsv") $ text_classifier.export_params(params_file_path) $ params_file_path
image_predictions.head()
stock['forecast'] = results.predict(dynamic=False)
data['age']=data['Week Ending Date']-data['temp'] $ data['age'].head(10) $
train.drop('Hours', axis = 1, inplace = True) $ test.drop('Hours', axis = 1, inplace = True)
s.index[[2,3]]
forecast = m.predict(future)
calc_mean_auc(product_train, product_users_altered, $               [sparse.csr_matrix(item_vecs), sparse.csr_matrix(user_vecs.T)], product_test) $
wd = 1e-7 $ wd = 0 $ learn.load_encoder('lm5_enc')
to_be_predicted_Day1 = 36.61 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
ratings.loc[ratings.rating_denominator > 10]
tweet_archive_clean.text.head()
obj4.name = 'population'
dum.columns
row = df2.loc[df2['user_id']==773192].index $ print("The row information for the repeat user_id is : {} ".format(row[1]))
df
tweets_clean.drop(columns = ['pupper', 'doggo', 'puppo', 'source'], inplace=True) $ tweets_clean.info()
for col in var_cat: $     cats = taxi_sample[col].unique()[1:]  # drop first $     taxi_sample = taxi_sample.one_hot_encoding(col, prefix=col, cats=cats) $     del taxi_sample[col]
import pandas as pd
payments_all_yrs= [] $ file_name = input('Which file to read for PAYMENTS? ') $ if len(file_name)==0: $     file_name = 'Payments including all years with Percentiles - This ONE.csv' $ payments_all_yrs = pd.read_csv(file_name)
sum(df.duplicated())
df.groupby("cancelled")["pickup_end_of_month"].mean()
percipitation_2017 = hawaii_measurement_df[['Date','Percipitation']] $
len(orig_android_tweets[orig_android_tweets['date'].dt.year == 2016])
text_classifier.get_step_param_names_by_name("text_word_ngrams")
coinbase_btc_eur['Timestamp'] = coinbase_btc_eur["Time"].apply(lambda row: unix_to_datetime(row))
df2.query('landing_page == "new_page"').user_id.nunique() / df2.shape[0]
df
weather_dt = c.fetchall() $ print 'fetched'
filled.dropna(axis=1)  # each column containing NaN is dropped
print(rhum_nc) $ for v in rhum_nc.variables: $     print(rhum_nc.variables[v])
subject_id_to_group['MA-OTS BHC0100-1 repeat'] = subject_id_to_group['MA-OTS BHC0100-1'] $ subject_id_to_group['KT-OTS BHC0227-2'] = 2 $ subject_id_to_group['M-M OTS BHC0303-1'] = 2 $ subject_id_to_group['AC-OTS BHC0111-1'] = 1
X_train_pca = pca.fit_transform(X_train_sc) $ X_test_pca = pca.transform(X_test_sc)
df_twitter_archive_master.groupby(['p1']).mean()['retweet_count'].sort_values(ascending=False).head(5)
df_by_donor = donations[['Donor ID','Donation ID', 'Donation Amount', 'Donation Received Date']].groupby('Donor ID', as_index=False).agg({'Donation ID': 'count', 'Donation Received Date': 'max', 'Donation Amount': ['min', 'max', 'mean', 'sum']})
dfs = [] $ for time, month_range in times.items(): $     dfs.append(wiki_table_to_df(time, month_range)) $ df = pd.concat(dfs) $ print('We have {} registered attacks from January 1st, 2011 up to today (November 28th, 2017)'.format(df.shape[0]))
wb.search('cell').head()
ab_file=pd.read_csv('ab_data.csv') $ ab_file.head(10)
tw.head()
prob_convert = (df2['converted'] == 1).sum()/unique_users_2 $ prob_convert
results = pd.read_csv("analysis_csv_251560641854558.csv", index_col=None) $ number_of_posts = results["ID"].count() $ print("Total number of posts: {}".format(number_of_posts))
df['production_companies'] = df['production_companies'].apply(lambda x: x.split('|')[0])
df.shape
df2['user_id'][df2.duplicated(['user_id'], keep=False)]
df_t.head()
df2.duplicated('user_id').sum()
start = df['timestamp'].min() $ end = df['timestamp'].max() $ print('Our experiment started on the {} and has been ended on {}'.format(start,end))
autos = pd.read_csv('autos.csv',encoding='Latin-1') $ autos.info() $ autos.head() $
data_list = list(data)
unicode_tokens = word_tokenize(demoji.lower()) $ print(unicode_tokens)
%matplotlib notebook
tweet_df.tweet_created_at.max()
liquor['State Bottle Cost'] = [s.replace("$","") for s in liquor['State Bottle Cost']] $ liquor['State Bottle Cost'] = [float(x) for x in liquor['State Bottle Cost']] $ liquor_state_cost = liquor['State Bottle Cost']
pd.pivot_table(more_grades, index="name", values=["grade","bonus"], aggfunc=np.max)
df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit = 's') $ df.head()
import numpy as np $ import pandas as pd $ import matplotlib as mpl $ import matplotlib.pyplot as plt $ %matplotlib inline
glm_perf = glm_model.model_performance(valid = True) $ glm_perf.plot()
clean_df = pd.DataFrame(clean_tweet_texts,columns=['text']) $ clean_df['target'] = df.sentiment $ clean_df.head()
print(list(cos.buckets.all()))
treatment = df2.query('group == "treatment"') $ treatment_converted = treatment.query('converted == 1') $ p_treatment_convert = treatment_converted.count()[0]/treatment.count()[0] $ p_treatment_convert
intervals = [30] $ def mean(numbers): $     return float(sum(numbers)) / max(len(numbers), 1)
dfExport = df[['tweet_id','airline_sentiment', 'airline', 'lat', 'lon', 'tweet_location', 'tt_lat', 'tt_lng','user_timezone', 'usr_lat', 'usr_lng','hour','date','dow']]
print("Annualized mean:", fundmean*100, "%", $       "Annualized vol:", fundvol*100, "%", $      "(Based on monthly data.)")
subs_and_comments['op_comment'] = (subs_and_comments['author_x'] == subs_and_comments['author_y'])
from test_environment import consumer_key, consumer_secret, access_token, access_token_secret
len(pax_raw)
tables = pd.read_html('https://en.wikipedia.org/wiki/List_of_accidents_and_disasters_by_death_toll', header=0)
p_mean = (pnew+pold)/2 $ p_mean
inactive_count_with_na = len(clean_users[clean_users['active']==0]) $ inactive_mailing = clean_users[clean_users['active']==0]['opted_in_to_mailing_list'].sum()/inactive_count_with_na
p_diffs=np.array(p_diffs) $ plt.hist(p_diffs)
cv = CountVectorizer(stop_words='english', max_features=100, binary=True)
dfdaycounts = dfdaycounts.sort_values(['created_date'])
knn_yhat = kNN_model.predict(test_X) $ print("KNN Jaccard index: %.2f" % jaccard_similarity_score(test_y, knn_yhat)) $ print("KNN F1-score: %.2f" % f1_score(test_y, knn_yhat, average='weighted') )
stocks_happiness=pd.merge(happiness_df,stocks_df,on='dates',how='outer')
tweet.text
from sklearn.linear_model import LogisticRegression $ log_reg = LogisticRegression(C = 0.0001) $ log_reg.fit(train, train_labels)
df.dtypes
intervention_history.sort_values(['INSTANCE_ID', 'CRE_DATE_GZL', 'INCIDENT_NUMBER'], inplace=True)
search['trip_end_date'] = search['message'].apply(trip_end_date)
df.head()
store_items = store_items.drop(['store 1', 'store 2'], axis=0) $ store_items
answer26_pattern = 
df2_control = df2.query("group == 'control'") $ convereted_rate_old = round(df2_control.converted.mean(),4) $ print(convereted_rate_old)
print 'Total amount for non-USA location: ', df[((df.state == '') & (df.city != ''))].amount.sum()
url.urlretrieve('https://data.cityofnewyork.us/download/i8iw-xf4u/application%2Fzip', 'zipcode.zip') $ os.system('mv zipcode.zip $PUIDATA') $ os.system('unzip -o $PUIDATA/zipcode.zip -d $PUIDATA/zipcodes')
colnames = ['log***','gfhdfxh'] $ df.columns = colnames
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])
df.resample('H').count().head(10)
autos[['date_crawled', 'ad_created', 'last_seen']][0:5]
pd.value_counts(ac['Non-Compliance Found']).head()
df_link_yt['channel_title'].value_counts().head(30)
new_page_converted = pd.DataFrame(np.random.choice([0,1],n_new,p=[1-CRnew,CRnew])) $
pd.DataFrame(df['price'].describe())
shmuel = relevant_data[relevant_data['User Name'] == 'Shmuel Naaman'] $ df = shmuel['Event Type Name'].value_counts() $ df_shmuel = pd.Series.to_frame(df) $ df_shmuel.columns = ['Count_Shmuel'] $ df_shmuel
df_loan2.set_index('fk_loan',inplace=True)
new_page_converted=np.random.binomial(n_new, p_null)
new_cases['Date'] = new_cases['Date'].apply(lambda d: datetime.strptime(d, '%Y-%m-%d')) $ grouped_months_new = new_cases.groupby(new_cases['Date'].apply(lambda x: x.month)) $ new_cases['Total_new_cases_guinea'] = new_cases['Total_new_cases_guinea'].astype(int)
df_new_true = df2.query('group == "control"')['converted'].mean() $ df_new_true
p_new_page = df2.query('landing_page == "new_page"').shape[0] / df2.shape[0] $ p_new_page
B = lv_workspace.get_subset_object('B') $ print(B.get_step_list())
total_users = len(clean_users) $ total_users_mailing = clean_users['opted_in_to_mailing_list'].sum()
grouped = data[['processing_time','Borough']].groupby('Borough') $ grouped.describe()
%%timeit -n 10 $ for group, frame in df.groupby('STNAME'): $     avg = np.average(frame['CENSUS2010POP']) $     print('Counties in state ' + group + ' have an average population of ' + str(avg))
indices = [1050619, 1375586, 1050589, 1050598, 538048, 538018, 538036, 538034, 538029, 538031] $ samples = pd.DataFrame(data.loc[indices], columns = data.keys()).reset_index(drop = True) $ print "Chosen samples of wholesale customers dataset:" $ display(samples) $
female_journalists_mention_summary_df = journalists_mention_summary_df[journalists_mention_summary_df.gender == 'F'] $ female_journalists_mention_summary_df.to_csv('output/female_journalists_mentioned_by_journalists.csv') $ female_journalists_mention_summary_df[journalist_mention_summary_fields].head(25)
from sklearn.model_selection import train_test_split $ from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, roc_auc_score
conn_b.commit()
data["e1_len"] = data["project_essay_1"].str.len() $ data["e2_len"] = data["project_essay_2"].str.len() $ data["ttl_len"] = data["project_title"].str.len() $ data["summ_len"] = data["project_resource_summary"].str.len()
train.shape
plt.figure(figsize=(12, 8)) $ plt.plot(df['Close'], label='Close') $ plt.plot(df['Close'].rolling(5).mean(), label='3 Day MA') $ plt.plot(df['Close'].rolling(14).mean(), label='14 Day MA') $ plt.legend()
sns.regplot(x="totqlesq", y="y", data=psy_native).set_title("Quality of Life Enjoyment and Satisfaction") $
gender.value_counts()
model = sm.Logit(reg_df['converted'], reg_df[['intercept', 'ab_page', 'is_US', 'is_UK']])
driver.get("http://www.reddit.com") $ time.sleep(1)
twelve_months_prcp.describe()
committees_NNN = committees[committees.prop_name == "PROPOSITION 064- MARIJUANA LEGALIZATION. INITIATIVE STATUTE."]
users['DaysActive'].describe()
data = r.json() $ print(data) $
matrix=rate_change.drop(['numerator','rating_denominator','date'],axis=1)
summary.loc['missing'] = len(records) - summary.loc['count'] $ summary
tidy_test = dat.groupby(by=['COMMITTEE_ID','CAND_ID','TRANSACTION_DATE','TRANSACTION_AMOUNT']).size().sort_values(ascending=False) $ tidy_test.head(20)
df2['intercept'] = 1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('control', axis=1) $ df2.head()
from IPython.core.display import display, HTML $ htmlFile = 'Data/titanic.html' $ display(HTML(htmlFile)) $
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt
to_be_predicted_Day3 = 38.62438598 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
days[:10]
x = pd.DataFrame(autodf.notRepairedDamage.value_counts()) $ print "Percentage of cars not repaired = " + str(round( x.ix['ja'] * 100 /x.ix['nein'],2)) + " %"
quandl.ApiConfig.api_key = input('Please enter your quandl Key: ') $ exchange = 'WIKI'
plt.figure(figsize=(20,8)) $ sns.swarmplot(x='Type 1', y='Total', data=pokemon, hue='Legendary') $ plt.axhline(pokemon['Total'].mean(), color='red', linestyle='dashed') $ plt.show()
rf = RandomForestClassifier(n_estimators = 10) $ rf = rf.fit(train_data_features, y_train)
data_sets['3min'][data_sets['3min']['interpolated_values'].notnull()].tail()
list(c.elements())
rf = RandomForestClassifier() $ rf_grid = GridSearchCV(rf,params,scoring='precision')#'f1')
closed_pr = PullRequests(github_index).is_closed().get_cardinality("id_in_repo").by_period(period="quarter") $ print("Trend for quarter: ", get_trend(get_timeseries(closed_pr)))
import pandas as pd $ df = pd.DataFrame(complaints) $ df.columns = ['unique_key', 'complaint', 'created_date', 'closed_date'] $ df_original = df.copy()
abc = payments_by_year[2015].sort_values( ascending=[False]) $ plot_top_DRGs(abc,5,2015,'payment',False)
autos['offer_type'].unique()
ccl["Date"] = pd.to_datetime(ccl.Date, format="%Y/%m/%d", errors='ignore')
stock_list = df_release['ticker'].unique() #plan to loop for each ticker, fiscal_year and fiscal_quarter $ print(stock_list) $ print(len(stock_list)) $ df_release_pivot = df_release.set_index(['ticker', 'fiscal_year', 'fiscal_quarter']).sort_index() $ df_release_pivot
def normalize(row): $     return row / row.sum()
video_ids = yt.get_video_urls_from_playlist_id(playlist_id, key, $                                                cutoff_date=datetime.datetime(2017,1,1))
ch_year
df_final.head()
(autos["date_crawled"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index() $ )
plt.hist(p_diffs); $ plt.axvline(x=diff, color = 'red');
temperature.describe()
first_row = session.query(Measurement).first() $ first_row.__dict__
from sklearn.metrics import roc_auc_score
deaths_by_decade = pd.DataFrame(explotions.groupby('decade')['Deaths'].sum())
print('CV Accuracy: %.3f' $      % gs_lr_tfidf.best_score_) $ clf = gs_lr_tfidf.best_estimator_ $ print('Test Accuracy: %.3f' % clf.score(X_test, y_test)) $
sorted(first_status.keys())
old_page_converted = np.random.choice([0,1], size = (145274), p = [0.88, 0.12]) $ p_old = (old_page_converted == 1).mean() $
gp = df[['placeId', 'hashtags']].groupby('placeId')
all_data_merge.groupby('brand').size()
from sklearn.feature_extraction.text import CountVectorizer
df2.groupby([df2['group']=='control',df2['converted']==1]).size().reset_index()[0].iloc[3]/df2.query('landing_page == "old_page"').user_id.nunique()
df.index.month.value_counts().sort_index()
reddit_data = pd.DataFrame $ reddit_data = pd.DataFrame.from_dict(topics_dict) $
df[df.reviewerName.isnull()]
new_df['index'] = new_df['index'].apply(conv)
y = df['loan_status'].values $ y[0:5] $ y[y=="PAIDOFF"] = 1 $ y[y=="COLLECTION"] = 0 $ y = y.astype("int")
titanic.survived == 1
my_house_sqft = 2650 $ estimated_price = get_regression_predictions(my_house_sqft, sqft_intercept, sqft_slope) $ print('The estimated price for a house with {} squarefeet is {:.2f}'.format(my_house_sqft, estimated_price))
vect = CountVectorizer(stop_words = 'english',ngram_range = (1,3),min_df = 0.2, binary = True,max_features = 100)
liberiaFileList = glob.glob("Data/ebola/liberia_data/*.csv") $ liberiaFrameList = [pd.read_csv(file,usecols=['Variable','Date','National'],index_col=['Variable','Date']) for file in liberiaFileList] $ len(liberiaFileList)
y_dist = list(np.zeros(394)) + list(np.ones(394)) $ y_dist = np.asarray(y_dist) $ print(y_dist)
!head -n 20 ../data/microbiome/microbiome_missing.csv
lm_us = sm.OLS(df_new['converted'], df_new[['intercept_us', 'ab-page']]) $ results_us = lm_us.fit() $ results_us.summary()
df.nunique().user_id
scoresdf = {} $ for key in scores.keys(): $     scoresdf[key] = pd.DataFrame(scores[key]) $     scoresdf[key].columns = ['min_df', 'time_taken', 'score']
df['tweet']= tweetCleaner(df['text'])
!wget https://www.ssa.gov/oact/babynames/names.zip
print('{} false positives and {} false negatives'.format(confusion_matrix(y_test,knn_pred)[0,1],confusion_matrix(y_test,knn_pred)[1,0])) $
count_polarity=pd.concat([count_polarity_2012,count_polarity_2013,count_polarity_2014 $                            ,count_polarity_2015,count_polarity_2016], axis=1)
import google.datalab.storage as storage
data3.head()
price_vs_km["mean_odometer_km"].describe()
%pylab inline $ stackTaskCompleted = gDate_vProject.plot.bar(stacked=True, figsize=(15, 15)) $ stackTaskCompleted.set_ylabel("Tasks Completed") $ stackTaskCompleted.set_title("Weekly Task Completion")
X_train = ss.fit_transform(X_train) $ X_test = ss.fit_transform(X_test)
df.set_value(index=index_to_change, col='ab_page', value=1)
git_log.timestamp.head()
%matplotlib inline $ import datetime $ import bqapi as bp
ben_final.shape
twitter_dataset.info()
print("Proportion of converted users is {}%".format((ab_file['converted'].mean())*100))
np.mean(df2.ltv)
import datetime $ datetime.datetime.now()
googletrend.loc[googletrend.State=='HB,NI', "State"].head()
s.str.lower()
states
df3 = df[(df['group'] == 'treatment') & (df['landing_page'] != 'new_page')] $ df4 = df[(df['group'] != 'treatment') & (df['landing_page'] == 'new_page')] $ df3.shape[0] + df4.shape[0]
s.index[0]
df.user_id.nunique()
s = pd.Series(np.random.randn(10000)) $ s.plot(kind='kde', color='b') 
analytics = initialize_analyticsreporting()
after.head()
1/np.exp(results_2.params)
df
no_atmention = re.sub(r'@[\w_]+','', no_urls) $ print(no_atmention)
active_countries = users.pivot_table(index = 'Country', $                                      columns = 'Active', $                                      values = 'CreationDate', $                                      aggfunc = 'count', margins = True) $ active_countries['% of Total'] = (active_countries[True] / active_countries['All'] * 100)
df_madrid['clean_text'] = [tweet_cleaner(t) for t in df_madrid.text]
s = issues_df.to_json(orient='records')
import regex as re $ from nltk.corpus import stopwords
data.head()
wb.search('income').head()
gene_df[gene_df['length'] > 2e6].sort_values('length').iloc[::-1]
stationActive_df = pd.read_sql("SELECT station, count(*) as `Station Count` FROM measurement group by station order by `Station Count` DESC", conn) $ stationActive_df
similarity_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)
df[['text', 'retweet_count', 'date']][df.retweet_count == np.max(df.retweet_count)]
temp_df.shape
result = pd.merge(df, leaderboard, on='ref') $ result.head()
sub_df.groupby('Rating').size().plot(kind='bar') $ plt.title("Frequency of Review Rating") $ plt.xlabel("Rating") $ plt.ylabel("Number of reviews") $ plt.xticks(rotation=0)
red_4 = red[['title', 'subreddit', 'num_comments', 'created_utc', 'id', 'time fetched']].copy(deep = True) $ red_4.head()
df.columns
vessels.type.value_counts()
weather_df.to_csv("Weather data.csv", encoding = "utf-8-sig", index = False)
pd.set_option('display.max_colwidth', 100)
gen1 = dta.t[(dta.b==1) & (dta.c==1)]
dataFile = "wasb://sampledata@pysparksampledata.blob.core.windows.net/sampledata.csv" $ dataFileSep = ',' $ df = sqlContext.read.csv(dataFile, header=True, sep=dataFileSep, inferSchema=True, nanValue="", mode='PERMISSIVE') $
OldConverted = df2.query('converted == 1')['user_id'].count() $ HNull = (OldConverted/NewTotalUser) $ print("Null Convert Rate: ",HNull)
unstackedCloses = stackedCloses.unstack() $ unstackedCloses
df2.loc[df2.user_id.duplicated(keep = False),:]
ctr.head(5)
statezips = csvData['statezip'].value_counts().reset_index() $ statezips.columns = ['statezip', 'count'] $ statezips[statezips['count'] < 5]
slFileList = glob.glob("Data/ebola/sl_data/*.csv") $ slFrameList = [pd.read_csv(file,usecols=['variable','date','National'],index_col=['variable','date']) for file in slFileList] $ len(slFileList)
url = 'https://raw.githubusercontent.com/yinleon/LocalNewsDataset/master/data/local_news_dataset_2018.csv' $ df = pd.read_csv(url)
import pods $ from ipywidgets import IntSlider
X[col_with_na].describe()
df.head(10) $ df[['Indicator_ID','Country','Year','WHO Region','Publication STATUS']].sort_index().head(3)
soup = BeautifulSoup(html, 'html.parser') $ pet = soup.select("#block-system-main > div > div > div.view-content > div.views-row.views-row-1.views-row-odd.views-row-first.Adopt.Me") $ print(pet)
merge_event.loc[(merge_event.user_type == 1)].groupby('nweek')['user_id'].nunique().plot(label="user 1") $ merge_event.loc[(merge_event.user_type == 2)].groupby('nweek')['user_id'].nunique().plot(label="user 2") $ merge_event.loc[(merge_event.user_type == 3)].groupby('nweek')['user_id'].nunique().plot(label="user 3") $ plt.legend() $ plt.show()
svc = SVC(random_state=20, C=10, decision_function_shape='ovo', kernel= 'rbf') $ svc.fit(X_tfidf, y_tfidf) $ svc.score(X_tfidf_test, y_tfidf_test)
data = data[data.area == "San Francisco"] $ data = data[data.int_bed == 1] $ data = data[data.price < 10000] $ data = data.reset_index()
logit_mod_3 = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'UK','new_page_UK','US','new_page_US']]) $ results_3 = logit_mod_3.fit() $ results_3.summary()
df2 = pd.DataFrame(np.arange(20.).reshape((4, 5)), columns=list('abcde'))
f_yt_raw = '/scratch/olympus/projects/ideology_scaling/congress/youtube_links_raw.csv' $ df_yt.to_csv(f_yt_raw, index=False) $ shutil.chown(f_yt_raw, group='smapp')
tweets = tweets.loc[tweets['created_at'] > startDay]
lasso = Lasso() $ lasso.fit(X_train, y_train) $ predicted_lasso = lasso.predict(X_test) $ print("Lasso Mean Squared error:",mean_squared_error(y_test, predicted_lasso)) $ print("Lasso R2 Score:",lasso.score(X_test, y_test))
twitter_data.info() $
daily = count_15.resample('1d').sum()
df_clean3.loc[1202, 'rating_numerator'] = 11 $ df_clean3.loc[1202, 'rating_denominator'] = 10
new_page_converted = np.random.binomial(1, p_new, size = n_new)
talks['text'][0]
expiry
cust_demo.memory_usage()
training.head(10)
df = pd.read_csv("FuelConsumption.csv") $ df.head() $
df['rating'] = df.rating_numerator/df.rating_denominator $ df['rating_category'] = pd.cut(df.rating, bins = [0.0, np.percentile(df.rating,25), np.percentile(df.rating,50), np.percentile(df.rating,75), np.max(df.rating)],labels=['Low','Below_average','Above_average','High']) $ df.drop(['rating_numerator','rating_denominator'], axis=1, inplace=True)
Test=IonTechGFC1000('MFC1')
plt.figure(figsize=(20, 5)) $ distplot = sns.distplot(df['question_answer'], bins=101, color='darkblue', kde=False)
df_airports = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2011_february_us_airport_traffic.csv') $ df_airports.head()
new_page_converted = np.random.normal(p_new,np.std(p_new), n_new)
print('Element in row 0, column 0:', array[0,0]) $ print('Element in row 0, column 4:', array[1,4]) $ print('Elements in row 0:', array[0,:])
df_merge['interaction_UK'] = df_merge['ab_page'] * df_merge['UK'] $ df_merge['interaction_US'] = df_merge['ab_page'] * df_merge['US'] $ df_merge.head()
df2['rx_requested'] = df2['rx_requested'].map(lambda x: x.lower())
facts_metrics.groupby('dimensions_item_id').size().describe()
sentiment_df = pd.DataFrame.from_dict(sentiment_freq,  orient='index') $ sentiment_df = sentiment_df.reset_index() $ sentiment_df
top_10 = scores.loc[168].argsort()[::-1][:11] $ trunc_df.loc[list(top_10)]
nold= df2[df2["landing_page"] == "old_page"].count() $ nold = nold[0] $ nold
rt_count_1.to_csv('retweets_by_keywords.csv')
df.loc[(df["date"].dt.hour > 6) & (df["date"].dt.hour < 19), 'day'] = 1 #this is day $ df.loc[(df["date"].dt.hour <= 6) | (df["date"].dt.hour >= 19), 'day'] = 0 #this is night
threeoneone_geo['fix_time']=threeoneone_geo['closed_date']-threeoneone_geo['created_date'] $ threeoneone_geo['fix_time_sec']= [x / np.timedelta64(1, 's') for x in threeoneone_geo['fix_time']]
grades - grades.values.mean() # substracts the global mean (8.00) from all grades
test_df.field.value_counts()
supertrimmed = prune_below_degree(tweetnet, 500) $ nx.info(supertrimmed) $ supertrimmed_degrees = supertrimmed.degree() $ plt.hist(supertrimmed_degrees.values(), 100) #Display histogram of node degrees in 100 bins
ex2 = pd.Series(["b","a","c"])
a = np.array([[1., 2.], [3., 4.]]); a
weathertweet = soup.find('div', class_="js-tweet-test-container") $ first_para = soup.find('p', class_="TweetTextSize TweetTextSize--normal js-tweet-text tweet-text") $ weathertext = soup.find('p', class_="TweetTextSize TweetTextSize--normal js-tweet-text tweet-text").text $ print("mars_weather = " + weathertext)
x_train = scaler.transform(x_train)
p_old = df2.converted.mean() # the same!!! $ p_old
print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $ imagelist = [i for i in os.listdir() if i.endswith(".pdf")  ] $ imagelist
training, test = train_test_split(review_df, test_size=0.2, random_state=233) $ print(len(training), "train +", len(test), "test")
df['label'] = df[forecast_column].shift(-forecast_out)
stock_return.plot(grid = True).axhline(y = 1, color = "black", lw = 2)
tweets_clean.head()
import datetime as dt $ import decimal $ import warnings $ warnings.filterwarnings('ignore')
datafiles
autos = autos[autos["registration_year"].between(1900, 2016)]
final_df_test = dt_features_test.join(tree_c_features_test)
dta.select_dtypes(['category']).describe()
cm = our_nb_classifier.conf_matrix(our_nb_classifier.data_results['out_y'], $                             our_nb_classifier.data_results['out_n']) $ print(ascii_confusion_matrix(cm))
print("Probability of user converting:", df2['converted'].mean()) $
params = {'sender_address':'all'} $ response = requests.get( $   'https://panel.sendcloud.sc/api/v2/shipping_methods', $     auth=('key', 'secret_key'),params=params)
engine = create_engine('mysql+pymysql://root:AQib.21Talib@localhost/sakila') 
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], $                                               nobs=[n_new, n_old],alternative='larger') $ print("z-score:", z_score,"\np-value:", p_value)
startups_USA.head()
df.to_csv('ab.csv',index=False) #creating a new dataset file
display_topics(nmf2, features2, no_top_words)
data_t = data.copy() $ data_t['Time'] = pd.to_datetime(data['Time'], unit='s') $ data_t = data_t.set_index('Time') $ data_t.head()
fin_r.isnull().sum()
print("PASS: ", pass_students.ATAR.std()) $ print("FAIL: ", fail_students.ATAR.std())
print('The In/Out counter indicates the order in which cells have been executed.')
n_comp = 600
loan_stats["loan_status"].table()
price_vs_km
senateAll.to_csv("../data/senateCrosswalk.csv") $ houseAll.to_csv("../data/houseCrosswalk.csv")
distance_from_sun = [149.6, 1433.5, 227.9, 108.2, 778.6] $ planets = ['Earth','Saturn', 'Mars','Venus', 'Jupiter'] $ dist_planets = pd.Series(data = distance_from_sun, index = planets) $ time_light = dist_planets / 18 $ close_planets = time_light[time_light < 40] $
cand_date_df['sponsor_class'] = cand_date_df['sponsors'].map(mapping_dict) $ cand_date_df.head()
autos['offer_type'].value_counts()
proj_df['Project Subject Category Tree'].unique()
try: $     close_month.nlargest(2) $ except TypeError as err: $     print('Error: {}'.format(err))
voted_classifier = VoteClassifier(naive_bayes_classifier, MNB, logistic, SGD, SVC)
r = requests.get(query_url)
plot = events_top10_df.boxplot(column=['yes_rsvp_count']\ $                            , by='topic_name', showfliers = False, showmeans = False, figsize =(17,8.2)\ $                            ,whis=[20, 80])
sns.distplot(autodf.kilometer,rug=True)
exportID['avg_distance'] = exportID['event.longSum_sum_distance']/exportID['event.longSum_total_records']
data_set.to_csv('csv_submission.csv', index = False) $
pd.get_dummies(grades_cat)
start = dt.datetime(1982,1,1) $ end = dt.date.today() $ compTicker = 'AAPL' $ dataFrame = readData(start,end,compTicker) $ dataFrame.head() $
!head -n 2 Consumer_Complaints.csv
[endpoint_deployments] = [x.get('entity').get('deployments').get('url') for x in json.loads(response_get.text).get('resources') if x.get('metadata').get('guid') == saved_model.uid] $ print(endpoint_deployments)
mod2 = sm.Logit(final_df['converted'], final_df[['ab_page','country_CA','country_UK','intercept']]) $ fit2 = mod2.fit() $ fit2.summary()
%matplotlib inline
station_count = session.query(Station.station).count() $ station_count
import pandas as pd $ weather = pd.DataFrame(weather_json['data']) $ weather['date']  = pd.to_datetime(weather['date'])
df.drop_duplicates(subset = ['last_name'], keep = 'last', inplace = False)
S.decision_obj.stomResist.value = 'simpleResistance' $ S.decision_obj.stomResist.value
excess_returns=returns.sub(rf, axis='index')  #Subtract series rf from all columns.
carprice={} $ for brand in top20.index: $     price=autos[autos["brand"]==brand]['price'].mean() $     carprice[brand]=price $ print(carprice)   
total = scores.sum() $ scores[:2.75].sum()/total
train.reset_index(inplace=True) $ test.reset_index(inplace=True) ##Note reset_index add another column called 'index with the old index cols $ train.head(5)
conn.getsessopt('caslib')
daily_feature.head()
dates
df_new['intercept']=1 $ logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','US','UK']]) $ results=logit_mod.fit() $ results.summary()
temp_df2.shape
y = df['comments'] $ X = df[['subreddit', 'title', 'age']].copy(deep=True)
seqsize = subs.user_id.value_counts(normalize=False)[0] $ print ("the longuest sequence is %d accepted submissions long" % seqsize )
oppose=merge[merge.committee_position=="OPPOSE"]
sample_survey.loc[:, 'i_Education'].unique()
df
results
df = pd.DataFrame(web_stats) # Converted dictionary to a dataframe
extracted_rating=twitter_archive_clean.text.str.extract('([0-9]+\.?[0-9]*/[0-9]+)', expand=False) $ extracted_numerator=extracted_rating.apply(lambda x: x.split('/')[0]) $ twitter_archive_clean['rating_numerator']=extracted_numerator
prices = df[::2].transpose().stack()
Precipitation,=plt.plot(df_d['date'], df_d['Prcp'],label='precipitation') $ plt.tight_layout() $ plt.show()
results = scrape_tweets('Caixabank', 'es', consumer_key, consumer_secret, access_token, access_secret, 500, salida_dir, output_tweets, pickle_name, proxy = proxy)
print(precision) $ print(recall)
movies.info()
cg_counts = aqi.groupby(['AQI Category_cg']).size() $ (eug_cg_counts / cg_counts).unstack(fill_value=0)
archive_clean.drop(['in_reply_to_user_id', $                     'in_reply_to_status_id', $                     ], $                   axis=1, inplace=True)
logs['date'] = logs.date_time.apply(date_only)
autos
preds.max()
s5 = pd.Series([1000,1000,1000,1000]) $ print("s2 =", s2.values) $ print("s5 =", s5.values) $ s2 + s5
def from_pickle(filename): $     with open(filename, 'rb') as f: $         return pickle.load(f)
autos[['date_crawled','last_seen','ad_created','registration_month','registration_year']].dtypes
import math $ df1[forcast_col][:10]
from sqlalchemy import create_engine $ engine = create_engine('sqlite:///sample', echo=True)
y_test_log_pred = XGB_model.predict_proba(X_test)
lm=sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results=lm.fit() $ results.summary()
dogs= pd.DataFrame(index=['Rex', 'Rover', 'Polly'], columns=['breed', 'gender', 'weight']) $ dogs
ps = pd.Series(np.random.randn(len(prng)), prng)
pd.date_range(start, periods=5, freq='B')
trump_time.to_csv("trump_time.csv") $ schumer_time.to_csv("schumer_time.csv")
week = df2.loc[idx.get_indexer(df1.By), 'Week'] $ week[0:10]
test_post
consumerKey = 'XXXXXXXXXXXXXXXXXXXXXXX' $ consumerSecret = 'XXXXXXXXXXXXXXXXXXXXXXX' $ auth = tweepy.OAuthHandler(consumer_key=consumerKey, $     consumer_secret=consumerSecret) $ api = tweepy.API(auth)
session.query(func.count(Mea.date)).all()
print(new_page_converted.mean() - old_page_converted.mean())
rt.head()
X_train, X_test, y_train, y_test = train_test_split(X_resampled,y_resampled,test_size=0.2)
from sklearn.manifold import TSNE $ tsne = TSNE(n_components=2, random_state=0) $ smallest_author = 0  # Ignore authors with documents less than this. $ authors = [model.author2id[a] for a in model.author2id.keys() if len(model.author2doc[a]) >= smallest_author] $ _ = tsne.fit_transform(model.state.gamma[authors, :])  # Result stored in tsne.embedding_
mydata.iloc[0] 
import pandas $ df2 = pandas.DataFrame(v2) $ df2.columns = [id2key[x] for x in range(len(keyarr) )] $ df2
recommendation_df['hacker_count'] = recommendation_df.groupby(['hacker_id'])['challenge_id'].transform('count')
basePricePerDay = dfEPEXbase.groupby(dfEPEXbase.index.date).aggregate('mean')['Price'] $ basePricePerDay.head() # verify calculation
df2=df2.drop(1899) $ df2.query("user_id==773192") #Checkinng if it ha been succesfully removed
trump[trump['is_retweet']==True].count()
pickle.dump(tfidf_df,open('../proxy/dataset3_texttf','wb'))
df_clean[df_clean['in_reply_to_status_id'].notnull()]
df.describe()
Raw_Forecast.Qty = Raw_Forecast.Qty.apply(lambda x: pd.to_numeric(x))
ins.head(5)
import datetime 
D2 = [(str(v), str(t).replace("|","")) for v,t in D2] $ D2[0:5]
!rm output.json $ !aws sagemaker-runtime invoke-endpoint --endpoint-name tensorboard-names-2018-03-20-22-40-47-154 --body '{"name":"barnam"}' --content-type "application/json" output.json $ ! cat output.json
test_orders=orders[orders['eval_set']=='test'] $ test_orders.head()
training = shelter_df $ testing_pd = pd.read_csv('test.csv') $ testing_pd = testing_pd[['ID','Name','DateTime','AnimalType','SexuponOutcome','AgeuponOutcome','Breed','Color']].astype(str) $ testing = sqlContext.createDataFrame(testing_pd)
datAll['blk_rng'] = datAll['Block_range'].map(str)+' '+datAll['Street_name'].map(str)
conn.fetch(table=dict(name='data.iris', caslib='casuser'), to=5)
df = pd.read_csv('Crimes_-_2001_to_present.csv')
data.loc[7, 'year'] = np.nan $ data
train_df.dropna(subset=['text','Q0_RELEVANT'], inplace=True)
df = pd.DataFrame(stationtotals)
%time df_columns['closed_at'] = pd.to_datetime(df_columns['Closed Date'], format="%m/%d/%Y %H:%M:%S %p") $
rentals = _get_DF(Q)
replies = pd.read_csv('LaManada_new/tblreplies.csv',sep=SEP,quotechar='"') $ replies.shape
reddit_comments_data.select('link_id').distinct().count()
for action in list_top_actions: $     action_feat = "action_" + action $     sessions_summary[action_feat] = sessions_summary.apply(lambda r: action in r["action"], 1)
df_users_6_mvp.to_csv('/Users/nikhil.mogare/Desktop/DSA_Reporting/Week3_Sep19/CSVs/historical_mvp_users.csv') $ df_users_6_after_mvp.to_csv('/Users/nikhil.mogare/Desktop/DSA_Reporting/Week3_Sep19/CSVs/after_july_2017_mvp_users.csv')
df2[df2.converted == 1].shape[0]/df2.shape[0]
df1
print("Avg Number of Comments/User: ") $ d/q
re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', cfd_index['rt'][10])
telemetry['datetime'] = pd.to_datetime(telemetry['datetime'], format="%Y-%m-%d %H:%M:%S")
st.pearsonr(df_final.rt_count, df_final.img_num)
max_words = 1000 $ tokenize = text.Tokenizer(num_words=max_words, char_level=False)
df_new['intercept'] = 1 $ logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'CA']]) $ results = logit_mod.fit() $ results.summary()
product_searches_response = get_product_searches_report(analytics) $ product_searches_df = create_dataframe_from_response(product_searches_response) $ product_searches_df
active_stations = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).\ $                order_by(func.count(Measurement.tobs).desc()).all() $ busiest_station = active_stations[0][0]    $ print("Most active station: ",busiest_station) $ active_stations
au.find_some_docs(uso17_qual_coll,limit=3)
temp.head()
googClose = goog.reset_index(level=0, drop=True) $ googClose = googClose['Close'] $ googClose
train_data["totals.transactionRevenue"] = train_data["totals.transactionRevenue"].fillna(0.0) $ train_data["totals.transactionRevenue"] = np.log1p(train_data["totals.transactionRevenue"])
sub_df.head()
df['source'].value_counts() $ df
print('web service hosted in ACI:', aci_service.scoring_uri)
stocks = stocks.rename(columns = {'id' : 'stock_code', 'name' : 'company'})
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.country.unique()
twitter_data_v2=twitter_data.copy() $ Imagenes_data_v2=Imagenes_data.copy() $ tweet_data_v2=tweet_data.copy()
session = Session(engine)
df_reindexed = df.reindex(index=[0,2,5], columns=['A', 'C', 'B']) $ df_reindexed
from config import consumer_key $ from config import consumer_secret $ from config import access_token $ from config import access_secret
country = pd.get_dummies(auto_new.Country) $ country.head()
data.tail(10)
model = sm.Logit(reg_df['converted'], reg_df[['intercept', 'ab_page', 'is_US', 'is_UK', 'page_UK', 'page_US']])
new = auto_new.drop(["CarModel", "CarYear", "Country", "Custom", "Payment_Option", "Hand_Drive", "Body_Type", "Purchased"],axis=1)
p_new = df2[df2['landing_page'] == "new_page"]['converted'].mean() $ p_new
first_movie.h3.a
df3['intercept'] = pd.Series(np.zeros(len(df3)), index=df3.index) $ df3['ab_page'] = pd.Series(np.zeros(len(df3)), index=df3.index) $ df3.head()
print('The probability of conversion:', df2['converted'].mean())
df2 = cgc2.as_dataframe() $ df2[['Match','Exact']].describe()
print df['text'][823984] $ sentiment_analysis(df['text'][823984])
fig,axes = plt.subplots(1, 2, figsize = (16,4), sharey= True) $ axes[0].plot_date(x=obama.created_at, y = obama.n_chars,linestyle = '-',marker='None') $ axes[1].plot_date(x=trump.created_at, y = trump.n_chars,linestyle='solid',marker='None') $ plt.savefig("fig/n_char_comparison.png")
import pandas as pd $ link = 'https://raw.githubusercontent.com/yinleon/usnpl/master/data/usnpl_newspapers_twitter_ids.csv' $ df = pd.read_csv(link, dtype={'Twitter_ID' : str}) $ df.head()
to_be_predicted_Day2 = 55.13755314 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
df_arch_clean[df_arch_clean['doggo'] == 'None'] $
def finish_check(name): $     b = phimage(name) $     l1 = b.time_steps() $     print(f"{name}: last = {l1[-1]}, btime = {b.BTime()}")
transformed2.shape
df.iloc[1:5, 2:4]
print('Unique number of users notified: {}'.format(len(atdist_opp_dist[~atdist_opp_dist['infoIncluded']]['vendorId'].unique())))
user_text = input('Give me some text>') $ lower_text  = user_text.lower() $ text_length = len(user_text) $ print('Your text is {}, its length is {}'.format(user_text, text_length)) $
temp_df.shape
import os $ import urllib.request $ import zipfile
import pandas as pd $ from sqlalchemy.engine import create_engine $ engine = create_engine('presto://localhost:8080/system/runtime') $ df = pd.read_sql('select * from queries limit 1', engine) $ df.head()
r_close = {i[0]:i[idx_close] for i in r_lol}
len(youthUser2016)
temps_df.ix[1].index
for table_info in crsr.tables(tableType='TABLE'): $     print(table_info.table_name);print(table_info.table_info.)
print(trump.favorite_count.mean()) $ print(trump.retweet_count.mean())
import statsmodels.api as sm $ convert_old = control_df.converted.sum() $ convert_new = treatment_df.converted.sum() $ n_old = control_df.shape[0] $ n_new = treatment_df.shape[0]
chart = top_supports.head(5).amount.plot.barh() $ chart.set_yticklabels(top_supports.contributor_fullname)
two_digits = lambda x: '%.2f' % x $ perc_df.applymap(two_digits).head()
new_page_converted = np.random.choice([0,1],n_new, p=(p_new,1-p_new))
date.strftime('%A')
rule_one_above = df[(df['X'] > x_chart_ucl)] $ for i in range(0, rule_one_above.shape[0], 10): $     display_html(rule_one_above.iloc[i:i+10].T)
merged.head()
%matplotlib inline $ taxa_count.plot(kind='bar') $
plate_appearances.loc[plate_appearances.batter_name=='javier baez',].tail(10)
from scipy.stats import entropy $ sp.stats.entropy(df['prob'])
all_simband_data = concat_simband_files(files, fields_of_interest)
pd.DataFrame(rows, columns=np.transpose(table_info)[1])
new_page_converted = np.random.choice([0,1], size = n_new, p = [1-p_new, p_new])
df.groupby(('C/A', 'UNIT', 'STATION', 'SCP', 'DATE')).sum()
retweets.columns
df_proj = pd.read_csv(projFile, usecols = projCols, $                  dtype = proj_dtypes)
active_stations = session.query(func.count(Measurement.tobs),Measurement.station).\ $     group_by(Measurement.station).\ $     filter(func.strftime("%Y-%m-%d", Measurement.date) > "2016-08-23").\ $     order_by(func.count(Measurement.tobs).desc()).all() $ print(active_stations)
train_texts = load_texts.process_file(lee_train_file, keep_all=False) $ print(train_texts[5])
train_df.describe() $ train_df['transaction_month'] = train_df['transactiondate'].dt.month $ train_df
df2.head() $
df = pd.DataFrame(d) $ df.head()
%%timeit $ for i in range(10000): $     b = pattern.sub('BB', the_str)
data_folder = '../input/export_Feb_8_2018' $ ods = SlackLoader(data_folder)
from sklearn.ensemble import RandomForestClassifier
print('if we spend 50k on TV ads, we predict to sell:', round(lm.predict(X_new)[0],2), ' thousand units')
with open("demo.log", "r") as log_file: $     for line in log_file: $         print(line)
url = 'http://www.straitstimes.com/asia/east-asia/now-its-japans-turn-to-brace-for-a-monster-storm-as-typhoon-lan-nears' $ article = Article(url)
round(len(df_twitter[df_twitter.dog_label == 'doggo']) / len(df_twitter.dog_label), 2)
df.head()
train.columns
num_pickup_clusters = 7 $ num_dropoff_clusters = 7 $ num_ride_clusters = 7
title_tokens.head()
print (archive_copy['new_rating_denominator'].value_counts()) $ archive_copy[archive_copy.new_rating_denominator == 'Error']['tweet_id'] $
tweets_df = pd.DataFrame(status)
g = df_A.groupby(df_A.index.str.len()) $ g.filter(lambda x: len(x) > 1)
(p_diffs>diff).mean()
smart_authors = authors[(authors['count'] >= 10) & (authors['mean'] >= 0.5)] $ smart_authors.shape[0]
twitter_df_clean.loc[twitter_df_clean.rating_denominator != 10, 'rating_denominator'] = 10
n_new = (df2['landing_page'] == 'new_page').sum()
print('Unique number of users notified: {}'.format(len(atloc_4x['vendorId'].unique())))
writer.close()
officers = pd.read_csv('data/outputs/active_officers.csv') $ officers.company_number = officers.company_number.astype(str)
display(observations_ext_node['number'][DATA].dropna().head(9))
building_pa_prc_shrink.to_csv("buildding_02.csv",index=False) $ building_pa_prc_shrink=pd.read_csv('buildding_02.csv',parse_dates=['permit_creation_date'])
import  statsmodels.api as sm $ lm= sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = lm.fit() $
model.save('/tmp/model.atmodel') $
gen2 = dta.t[(dta.b==1) & (dta.c==2)]
df3['timestamp'].max(),df3['timestamp'].min()
print(type(y)) $ print(type(X))
df['is_application'] = ['Application' if date is not None else "No Application" for date in df['application_date']] $ df.head(3)
sp500.loc[['MMM', 'MSFT']]
client.experiments.delete(experiment_run_id)
my_set | your_set
model[model['NODE'].isin(set(model_id))]
to_be_predicted_Day3 = 21.38157719 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
twitter_archive.name.value_counts()
countries_df = pd.read_csv('./countries.csv')
df2 = df2.drop_duplicates(subset='user_id') $ df2.shape[0]
import pandas as pd $ filepath = 'ISSN_D_tot.csv' $ sunspots = pd.read_csv(filepath, sep=';')
df_merge.head()
df2.sample(5)
df2 = df.copy()
festivals_clean['End_Date'] =  pd.to_datetime(festivals_clean['End_Date'])
sorted_requests.foreachRDD((rdd, time) => { $     println("Top users @ " + time) $     rdd.take(5).foreach( $     pair => printf("User: %s (%s)\n", pair._2, pair._1)) $ }
item.metadata
print('Scores') $ print("Mean is {:0.2f} and the Median is {:0.2f}".format(np.mean(df.score),np.median(df.score))) $ print('Number of coments') $ print("Mean is {:0.2f} and the Median is {:0.2f}".format(np.mean(df.comms_num),np.median(df.comms_num))) $
sc = SparkContext(appName="Spark Streaming mini project") $ stc = StreamingContext(sc, 10)
import statsmodels.api as sm $ convert_old = df2_control.converted.sum() $ convert_new = df2_treatment.converted.sum() $ n_old = len(df2_control) $ n_new = len(df2_treatment)
import scipy $ corr_coeff = scipy.stats.pearsonr(data_compare['SA_textblob_de'], data_compare['SA_google_translate']) $ print('Der Korrelationskoeffizient zwischen TextBlob_DE und Google Translation ist:') $ print('----------------------------------------------------------------------------') $ print(corr_coeff[0])
data_words
result.to_csv("gaurav4.csv", index=False)
inspector = inspect(engine) $ inspector.get_table_names()
merged1.to_csv('./data/appointments_full.csv')
actual_diff
gs.grid_scores_
result = session.query(Measurements.date, Measurements.prcp).\ $            filter(Measurements.date >= date1).\ $            filter(Measurements.date <= date2).all()
df = pd.read_csv("entries.csv") $ topics = pd.read_csv("topics.csv")
significant_features
new_page_converted = np.random.choice([0, 1], size=n_new, p=[1-p_new_null, p_new_null])
plot_time(read_monthly(orig_tweets[orig_tweets['contains_url'] == False]))
plt.hist(x);
df_2014 = pd.DataFrame(rows)
avg_price= df2.pivot_table(values=["price"], index=['vehicleType'], aggfunc=np.mean) $ print(avg_price)
Results_ZeroFill = Results_ZeroFill[['ID', 'Approved']] $ Results_ZeroFill.head()
query = ['data science','data scientist','#datascience']
joined.describe() $
p_new_diff = (new_page_converted.sum() / len(new_page_converted)) $ p_new_diff
df_3_test = pd.read_csv('Medicare_Hospital_Spending_by_Claim.csv') $ df_4_test = df_3_test.drop(df_3_test.columns[[11, 12]], 1) $
df.Genre.value_counts()
df2['intercept'] = 1
prcp_12monthsDf.plot() $ plt.ylabel('Precp %') $ plt.title('Precp for 12 Months') $ plt.xticks(rotation=45) $ plt.show()
ks_goal_success = ks_goal_success.sort_values(by = ['counts'], ascending = False) $ ks_goal_success.head(10)
_ = df_trips.trip_requested.plot(kind="hist", bins=11)
data.loc[1:3]
merged_data.drop(['customer_creation_date', 'invoices_creation_date', 'last_payment_date'], axis=1, inplace=True)
t2 = StringIndexer(inputCol="user_id", outputCol="user_idn").fit(df_city_reviews) $ df_city_reviews = t2.transform(df_city_reviews)
tt_final.drop(['id'], axis = 1, inplace= True) $
menu_course_counts_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/menu_course_counts.csv')['Body'].read().decode('utf-8') $ menu_course_counts = pd.read_csv(StringIO(menu_course_counts_csv_string), header=0, delimiter='|')
more_recs.to_sql('samples', con, if_exists='append', index=False)
BroncosBillsTweets = tweets_gametitle.loc[tweets_gametitle['Game Title Date'] == 'Broncos vs. Bills  2017-09-24']
s1 = pd.Series(np.random.randn(3).round(2), index=list('abc'), name='S1') $ s2 = pd.Series(np.random.randn(5).round(2), index=list('cdefg'), name='S2') $ s3 = pd.Series(np.random.randn(4).round(2), index=list('fghi'), name='S3') $ print s1, '\n\n S2:\n', s2, '\n\n S3:\n', s3
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=10000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
cars.dtypes
autos["odometer_km"].value_counts
cachedf = dir2df(cachedir, fnpat='\.ifc$', addcols=['dirname', 'barename'])
tabulation.index
df1 = pd.DataFrame(np.random.randn(6,3),columns=['col1','col2','col3']) $ df2 = pd.DataFrame(np.random.randn(2,3),columns=['col1','col2','col3']) $ print(df2.reindex_like(df1))
topic_dist_df = pandas.DataFrame(topic_dist) $ df_w_topics = topic_dist_df.join(df_lit) $ df_w_topics
random.sample(labels.items(), 25)
spearmanr(merge_['tweet.created_at_x'], $           merge_['tweet.created_at_y'])
soup = BeautifulSoup(page, 'html.parser')
df = pd.read_pickle(train_data_dir+'/LSTM_train_data.pkl') $ df.shape
len(non_rle_pscs[non_rle_pscs.secret_base == True]) / len(non_rle_pscs)
match = len(df[(df['group']=='control') & (df['landing_page'] == 'old_page')]) + len(df[(df['group']=='treatment') & (df['landing_page'] == 'new_page')]) $ print(df.shape[0] - match)
win_paths = glob.glob(raw_windows_path+'*/*.csv') $ print("Found {} windows".format(len(win_paths)))
median_absolute_error(overdue_duration, [median_overdue_dur] * len(overdue_duration))
model_artifact = MLRepositoryArtifact(model, name="test", meta_props=props)
ml_repository_client = MLRepositoryClient(wml_credentials['url']) $ ml_repository_client.authorize(wml_credentials['username'], wml_credentials['password'])
xgb_learner.params['nthread'] = 4
df_measures_users.head()
getScoreGraph(sentiment_by_week_df, 'LiverpoolFC', [])
crimes.head()
imgp.info()
test_X.head()
import statsmodels.api as sm $ convert_old = len(df2.query("landing_page=='old_page' and converted=='1'")) $ convert_new = len(df2.query("landing_page=='new_page' and converted=='1'")) $ n_old = len(df2.query("landing_page=='old_page'")) $ n_new = len(df2.query("landing_page=='new_page'"))
nfl = pd.read_csv('https://raw.githubusercontent.com/ryurko/nflscrapR-data/master/data/season_play_by_play/pbp_2017.csv', low_memory=False)
txns.describe()
df.info()
vocab = vectorizer.get_feature_names() $ print(len(vocab)) $ print(vocab[:10])
samples_query.columns
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=False)
sns.set(style="whitegrid") $ f, ax = plt.subplots(figsize=(10, 10)) $ sns.set_color_codes("pastel") $ sns.barplot(x="Total_Num_Comments", y="Subreddit", data=subred_num_tot[:10], $             label="Comments", color="b")
model = joblib.load(path+"/xgb_model_GIVENCHY (2).dat") $ unique_users = load_pickle(path+"/80000users_3.pkl") $ network = load_pickle(path+"/80000users_3.pkl") $ users=load_pickle(path+'/Stanford_features.pkl') $ seeds=load_pickle(path+'/Stanford_seeds.pkl')
pro_con = df['converted'].mean()*100 $ print("Proportion of users converted are: {}%".format(round(pro_con,2)))
region_cat_return_amt = returned_orders_data.groupby(['Region', 'Category'])['Sales'].sum() $ print(region_cat_return_amt)
data_volumn
tm_2030 /= 1000 $ tm_2030_norm = tm_2030 ** (10/11) $ tm_2030_norm = tm_2030_norm.round(1) $ tm_2030_alpha = tm_2030 ** (1/3) $ tm_2030_alpha = tm_2030_alpha / tm_2030_alpha.max().max()
sentiment_df["Date"]=dates $ sentiment_df.head()
df1_clean.drop('test', axis=1, inplace=True)
(df[df["converted"] == 1]['user_id'].count())/df.shape[0]
dfg = dfg.set_index(['drg3', 'discharges']) $
def f(x): $     return pd.Series([x.min(), x.max()], index=['min', 'max'])
df2['ab_CA'] = df2['ab_page']*df2['CA'] $ df2['ab_UK'] = df2['ab_page']*df2['UK'] $ df2['ab_US'] = df2['ab_page']*df2['US'] $ df2.head()
pred.head()
treatment_cnv = df2.converted.mean() $ treatment_cnv
linkNYC.shape
    X_train = X.loc[X.num > test_size ].drop(columns='num') $     X_test =  X.loc[X.num <= test_size ].drop(columns='num') $     y_train = y.loc[X.num > test_size ] $     y_test =  y.loc[X.num <= test_size ] $     X_test.head(15)
nitrogen['ActivityMediaSubdivisionName'].unique()
df = df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False) $ df.shape
print("Identify Injured by Keyword") $ print(df.cdescr.str.contains('INJUR|HURT').sum()) $ print(len(df))
reviews.groupby('price').points.max().sort_index()
data.groupby('category').position.agg(pd.Series.mode)
con.commit()
vals2.sum(), vals2.min(), vals2.max()
print(LSTM_testinput[0]) $ print(LSTM_testoutput[0])
import os $ from geopy.geocoders import Nominatim $ geolocator = Nominatim() $ from geopy.exc import GeocoderTimedOut $ import time $
season_type_groups.aggregate(np.mean)
baseball1_df.loc[baseball1_df['ageAtDebut'].idxmin()]
brands
cpi2 = cpi[start_date:end_date] $ ann_ret_cpi = [annualized_returns(series=cpi2, years=i+1) $                for i in range(num_years)]
observCt_results = session.query(func.count(Measurements.date)) $ observCt_results
noise = pd.read_csv('ny_cb_shp_w_noise_counts')
gs_k150_under.score(X_train, y_train_under)
model_NB = MultinomialNB() $ model_NB.fit(count_vectorized, df_train.author)
red_flag_columns = active_companies.columns[-8:] $ temp_df = pd.DataFrame(active_companies[red_flag_columns].sum(axis=0)) $ temp_df.columns = ['number_of_companies_with_red_flag'] $ temp_df.index.name = 'red_flag' $ temp_df
pd.Series([2, 4, 6], index=['a','b','c'])
df_clean = pd.read_csv(tweets_clean_file) $ print(df_clean.shape) $ df_clean.head(5)
tag_df = stories.tags.apply(pd.Series) $ tag_df.head()
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine
fig, axs = plt.subplots(1, 3, sharey=True) $ data.plot(kind='scatter', x='TV', y='sales', ax=axs[0], figsize=(16, 8)) $ data.plot(kind='scatter', x='radio', y='sales', ax=axs[1]) $ data.plot(kind='scatter', x='newspaper', y='sales', ax=axs[2])
autos["number_of_photos"].value_counts()
df = df[(df.powerPS > 50) & (df.powerPS < 1000) ]
autos['unrepaired_damage'].unique()
from pyspark.sql import SQLContext $ sqlContext = SQLContext(sc)
df_ml_690_01.tail(5)
merged1 = pd.merge(left=merged1, right=offices, how='left', left_on='OfficeId', right_on='id')
start_date = pd.Timestamp("20150101") $ end_date = pd.Timestamp("20151231") $ mask2015= (df['Date'] >= start_date) & (df['Date'] <= end_date) $ Data2015 = df[mask2015].sort_values(by = 'Date') $
df_gt = pd.read_excel('../../data/essentials/Report By Category.xls', index_col=[0])
tweet_full_df = df_merge.merge(tweet_clean,how='left',on = 'tweet_id')
ppm_title.token_count_pandas().head()
df.tail(2)
print('Unique number of users notified: {}'.format(len(atloc_opp_dist['vendorId'].unique())))
def strip_singlequote(text): $     try: $         return text.strip('\'') $     except AttributeError: $         return text
full.groupby(['PastPCPVisits'])['<=30Days'].agg({'sum':'sum','count':'count','mean':'mean'}).sort_values(by='mean',ascending=False) $
dataFrame.head(5)
label_set = set(tree_labels) $ len(label_set)
dfChile = pd.read_pickle("dfAllGPSTweetsFilterChile.p")
sim_diff = new_page_converted.mean() -  old_page_converted.mean() $ sim_diff
df2 = df.query("group == 'control' and landing_page == 'old_page'")
print("Number of unique users: {}".format(df.user_id.nunique()))
df.iloc[:4]
df1.resample('D', how='ohlc')
df_raw = pd.read_csv('autos.csv', encoding='latin-1') $ df = df_raw.copy() #just to keep raw data in memory to later uses $ print 'list of columns available in our dataset: ' $ df.columns
twitter_ar.head(2)
months = pd.Categorical(['Jun', 'Jun', 'Mar', 'Apr'], $                         categories=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', $                                     'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], $                         ordered=True) $ months
print min(train_data['created'].apply(lambda d: d[:10])) $ print max(train_data['created'].apply(lambda d: d[:10]))
DataSet = DataSet[DataSet.userTimezone.notnull()] $ len(DataSet) $
test_data["Age no Nans"] = test_data["AgeuponOutcome"].fillna(value="2 years") $
cols=["brand", "price", "odometer_km", "registration_year"] $ brand_stats = autos[cols].groupby("brand").mean() $ brand_stats.sort_values(by=["price", "odometer_km"],ascending=False) $
B2 = B.get_step_object('step_2') $ B2.load_indicator_settings_filters()
merged_df = pd.concat([taxi_weather_df, seats_per_hour], axis=1)
tweets.head()
stocks.isnull().sum() # Checking for missing data
def calculation(column): $     df3 = df.groupby(column) $     df4=df3.describe() $     return df4 $ calculation('group')
wrd['source'][0]
output = spark.read.parquet("/home/ubuntu/parquet/output.parquet/output.parquet")
out = conn.addtable(table='crops', caslib='casuser', $                     **exceldmh.args.addtable) $ out
df_count_clean["tweet_id"] = df_count_clean["tweet_id"].astype(str)
rng = pd.date_range('3/6/2017 00:00', periods=5, freq='M') $ ts1 = pd.Series(np.random.randn(len(rng)), rng) $ ts1
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df3.set_index('user_id'), how='inner') $ df_new.head()
goog = goog['Close']
round(1/np.exp(-0.0099), 2), round(1/np.exp(-0.0507), 2)
iplot(data.groupby('assignee.login').size().sort_values(ascending=False)[:20].iplot(asFigure=True, kind='bar', dimensions=(750, 500)))
features = afl_data[['date', 'game', 'team', 'opponent', 'venue', 'home_game']].copy()
df_combined.info()
cnx = create_engine('postgres://jessica:123@localhost:5432/rpred', isolation_level='AUTOCOMMIT') $ conn = cnx.connect()
df_all_loans.to_clipboard()
df.to_json("json_data_format_split.json", orient="split") $ !cat json_data_format_split.json
gr1_success = pd.merge(user_group1,success_order, how='inner', left_on=['CUSTOMER_ID'], right_on =['CUSTOMER_ID']) $ len(gr1_success)
print(df.count()) $ print('There are no missing values, because the number of rows and the numbers from the count function are the same.')
fig = sns.FacetGrid(trip_data, row = "subscription_type", aspect = 4) $ fig.map(sns.countplot, 'start_hour')
c5 = c4.get_cayley_table()[('r','g')] $ c5
new_page_converted = df2.sample(n_new, replace = True).converted $ new_page_converted.mean()
%matplotlib inline
print(r.json())
dat.head(3)
df1=df[df.Trip_distance>=100] $ df1.loc[:,['Trip_distance','Trip_duration']]
ddp.head()
c.find_one({'born': datetime(1958, 8, 29)})
series.index $
for name, in session.query(User.name).\ $ ...             filter_by(fullname='Ed Jones'): $ ...    print(name)
triadic_closure = nx.transitivity(G) $ print("Triadic closure:", triadic_closure)
newPage_df = df2.query('landing_page == "new_page"') $ n_new = newPage_df.shape[0] $ n_new
pred4 = nba_pred_modelv1.predict(g4) $ prob4 = nba_pred_modelv1.predict_proba(g4) $ print(pred4) $ print(prob4)
criteria = so['viewcount'] > 100000 $ type(criteria)
df= pd.read_csv('../PANDAS/data/DataForTutorial3.csv', parse_dates=['Date'], usecols=list(range(1,8)))
baseball[['r','h','hr']].rank(ascending=False).head()
[ print(c, " - ", str(len(california_house_dataframe[c].unique()))) for c in california_house_dataframe.columns]
StockNames = [StockName for _, StockName in Stocks] $ print(StockNames) $ StockData['MeanPrice'] = StockData[StockNames].mean(axis=1)  # axis=1 require to generate means per row $ StockData['AMZN/AAPL'] = StockData['Amazon'] / StockData['Apple'] $ StockData[StockNames + ['MeanPrice', 'AMZN/AAPL']].head()
sns.countplot(calls_df["call_type"])
malebydate = male.groupby(['Date','Sex']).count().reset_index() $ malebydate.head(3)
diffs = np.array(diffs)
bnb[bnb['age']>80].head() $
autos.loc[autos.price>350000.0,'price']=np.nan
df_countries = pd.read_csv('./countries.csv') $ print(df_countries.country.value_counts()) $ df_countries.head()
pos_tweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] > 0] $ neu_tweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] == 0] $ neg_tweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] < 0]
df3.head()
df = pd.read_sql('SELECT a.address_id, a.city_id, c.city FROM address a LEFT JOIN city c ON a.city_id = c.city_id ORDER BY a.address_id DESC;', con=conn) $ df
prop57.info()
image_df.head(5)
len(mydf) #number of tweets
final_df = dt_features.join(tree_c_features)
X = sales_2015[['store_number','sales_jan_mar', 'volume_sold_lt', 'net_profit', $                 'state_bottle_cost', 'state_bottle_retail', 'bottles_sold']] $ X.head()
df2.drop(2893,inplace=True)
def f(func): $
df2[df2['user_id'] == 773192]
now.weekday(), now.isoformat()
name = '@realDonaldTrump' $ nbr_tweets = 200 $ results = myapi.user_timeline(id=name, count=nbr_tweets)
df_subset.describe()
X = dfData['property_description'].values $ y5 = dfData['rqual_score5'].values $ y10 = dfData['rqual_score10'].values $ X_train, X_test, y5_train, y5_test = train_test_split(X, y5, test_size=0.33, random_state=42) $ X_train, X_test, y10_train, y10_test = train_test_split(X, y10, test_size=0.33, random_state=42)
merged.sort_values('amount', ascending=False)
close_open_diff = TenDayMeanDifference()
Desc_active_stations = session.query(Measurement.station, func.count(Measurement.prcp)).\ $                                      group_by(Measurement.station).order_by(func.count(Measurement.prcp).desc()).all() $ Desc_active_stations
business_df.shape, address_df.shape
df = df.rename(index = str, columns = {'new.recap':'newRecap'})
for df in (joined, joined_test): $     df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32) $     df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32) $     df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32) $     df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)
just_team_year_and_wins = df[['teamid', 'year', 'wins']] $ just_team_year_and_wins.head(10)
from google.datalab.ml import TensorBoard $ TensorBoard().start('{}/notebooks/outputdir'.format(REPO))
test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True) $ test_df.head()
tlen = pd.Series(data = data['len'].values, index = data['Date']) $ tfav = pd.Series(data = data['Likes'].values, index = data['Date']) $ tret = pd.Series(data = data['RTs'].values, index = data['Date']) $
finaldf = df.drop(['index', 'irlsp'], axis=1) $ finaldf.head()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
!dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb
full['PastPCPVisits'].describe()
list(c.find({}, {'name.first': 1, $                  'born': 1}))
itos2 = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb')) $ stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})
stacked.pivot(index='name', columns='info', values='value')
params = dict(limit = 100, offset = 0) $ response = requests.get(url_speaker,params = params)
df9 = pd.read_csv('2009.csv')
df_goog['Closed_Higher_v2'] = (df_goog.Open < df_goog.Close) * 1 $ df_goog.head(2)
tweetering.polarity.plot(kind='hist', normed=True) $ range = np.arange(-1.5, 1.5, 0.001) $ plt.plot(range, norm.pdf(range,0,1))
convo4
pd.concat((df1, df2), axis=1).mean(axis=1)
new_data.head(1)
train_preprocessed = pd.merge(df_users, df_sessions, on='id') $ train_preprocessed.shape
data.resample('M').mean().head()  $
import pickle $ pkl_file = open('votes_by_party.pkl', 'rb') $ votes_by_party = pickle.load(pkl_file) $ pkl_file = open('speeches_features.pkl', 'rb') $ speeches_cleaned = pickle.load(pkl_file)
aapl.loc["2008-03-28"]
autos.info()
n_old=df2.query('landing_page=="old_page"').count()[0] $ n_old
final_log_mod = sm.Logit(df3['converted'], df3[['intercept', 'new_page', 'UK_new_page', 'US_new_page', 'UK', 'US']]) $ final_results = final_log_mod.fit() $ final_results.summary()
tweets = tweets[['id','text']]
df_clean.info()
assert len(combined_df) == len(session_df)
sorted_degree = sorted_degree_map(tag_degrees) $ sorted_degree[:50]
df.query('group=="treatment" and landing_page=="old_page"').count() $
df_times['Time of daily highs'].value_counts().idxmax()
ax.set_xlabel('Time') $ ax.set_ylabel('Price') $ ax.legend(loc='upper left') $ plt.show()
all_data_df = pd.read_csv('github_issues.csv') $ all_data_bodies = all_data_df['body'].tolist()
investors_df.head()
print(a) $ print(a.index('dog')) # Get index of first matching entry; throws exception if not found $ print(a.count('cat'))  # Count the number of instances of an element
pd.to_datetime(['2009/07/31', 'asd'], errors='coerce')
logmodel.fit(X_train, y_train) $ logmodel_predictions = logmodel.predict(X_test) $ num_of_logmodel_pred = collections.Counter(logmodel_predictions) $ num_of_logmodel_pred
def named_drug(row): $     for d in drugs: $         if d.lower() in row.lower(): $             return(d)        
for tweet in lista[:5]: $     print(tweet.text) $     print()
from quantopian.pipeline import CustomFactor, Pipeline $ import numpy as np
tweet.lang $ tweet.text $ tweet.retweet_count $ tweet.place $ tweet.geo
df=pd.DataFrame({'fault_description':df_launchpad.fault_description,'req':df_launchpad.req}) $ df_st=pd.DataFrame({'longtext':df_stackoverflow.longtext,'title':df_stackoverflow.title,'tags':df_stackoverflow.tags})
df_bkk = open_to_pandas("\BKK_th-en_tweets.txt") $ df_kyt = open_to_pandas("\KYT_ja-en_tweets.txt") $ df_ham = open_to_pandas("\HAM_de-en_tweets.txt") $ df_spl = open_to_pandas("\SPL_pt-en_tweets.txt") $ df_sfc = open_to_pandas("\SFC_en_tweets.txt")
df2['ab_page_US'] = df2['ab_page']*df2['US'] $ df2['ab_page_UK'] = df2['ab_page']*df2['UK'] $ df2.head()
data["time_up_clean"] = data["time_up"] $
raw_data.describe()
autos["date_crawled"].value_counts(normalize=True, dropna=False) $
HOU = pd.read_excel(url_HOU, $                     skiprows = 8)
print("Combing the bag of words and the w2v vectors...\n") $ train_bwv = hstack([train_bow, train_w2v]) $ test_bwv = hstack([test_bow, test_w2v])
df_goog.Open > df_goog.Close
results_baseline = smf.ols('favorite_count ~ dow + time_group + has_media + has_hashtags + has_urls', data=df_clusters[(df_clusters.favorite_count>0)]).fit() $ results_clusters = smf.ols('favorite_count ~ dow + time_group + has_media + has_hashtags + has_urls+cluster_cat', data=df_clusters[(df_clusters.favorite_count>0)]).fit()
atloc_4x_count_prop_overall = compute_count_prop_overall(atloc_4x, 'remappedResponses') $ atloc_4x_count_prop_overall
learner.save_encoder('lm5_enc')
pivoted.plot(legend=False, alpha=0.1)
np.save(local_pickel_root + 'train/train_data.npy', train_data, allow_pickle=True) $ np.save(local_pickel_root + 'train/train_label.npy', train_label, allow_pickle=True) $ np.save(local_pickel_root + 'test/val_data.npy', val_data, allow_pickle=True) $ np.save(local_pickel_root + 'test/val_label.npy', val_label, allow_pickle=True) $ !ls -R data/processed/visa-kaggle/data/ $
test = adjmats $ print adjmats.
df8 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'], $                    'rank': [1, 2, 3, 4]}) $ df9 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'], $                    'rank': [3, 1, 4, 2]}) $ display('df8', 'df9', 'pd.merge(df8, df9, on="name")')
obs_diff_conversion = prob_treatment_conversion - prob_control_conversion $ print(obs_diff_conversion)
df.to_csv("ccl_clean.csv")
co = CombinatorialOptimisation() $ co.train(elec,cols=[('power','active')]) $
 type(model.wv.syn0)
country_data.head(3)
df_parsed['Open'].plot();
[x for t in zip(list_a,list_b) for x in t]
"{0:.4f}%".format(float(sum(df['converted'] == 1)) / (sum(df['converted']==0) + sum(df['converted'] == 1))*100)
df.head(2)
movie_df
dates = pd.date_range('2009-01-01', '2011-12-31') $ df = get_data(symbols=['XOM','GOOG','GLD'],dates=dates) $ df.head()
survey = resdf.iloc[:,:113] $ survey.insert(2,'LangCd',resdf.iloc[:,120]) $ survey.to_sql('surveytabl',conn)
(d < p_diffs).mean()
countries = pd.read_csv(r"D:\courses\Nanodegree\term1\statistics\AnalyzeABTestResults\countries.csv") $ df4 = pd.merge(df3,countries, how= 'inner',left_on="user_id" , right_on= "user_id" ) $
prcp_df = pd.DataFrame(prcp_data, columns=['Precipitation Date', 'Precipitation']) $ prcp_df.set_index('Precipitation Date', inplace=True) # Set the index by date
X = pd.get_dummies(df['subreddit']) $ y = df['HIGH_LOW'] $ from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
df_con.shape
mySql = 'SELECT dateTime, endpoint FROM AccessLog WHERE responseCode = 403' $ access_logs_df.createOrReplaceTempView('AccessLog') $ query_results = spark.sql(mySql) $ query_results.show(truncate=False)
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['DEPT'] - df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['TopWellDept'] $ df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['BotWellDept'] - df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['DEPT'] $ df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['WellThickness'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['BotWellDept'] - df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['TopWellDept'] $
train_data.groupby(['device.browser']).agg({'totals.transactionRevenue': 'mean'}).reset_index().set_index("device.browser",drop=True).plot.bar()
p_diffs = [] $ for _ in range(10000): $     old_converted = np.random.binomial(1, size = n_old, p= p_old) $     new_converted = np.random.binomial(1, size = n_new, p=p_new) $     p_diffs.append(new_converted.mean() - old_converted.mean())
A = np.zeros(3, dtype=[('A', 'i8'), ('B', 'f8')]) $ A
to_be_predicted_Day3 = 31.29300657 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
print(dd.feature_metadata.columns)
new_page_converted = np.random.choice([1,0], size=n_new_page, p=[p_mean, (1-p_mean)]) $ print(new_page_converted.mean())
print (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())
n_new = df2.query(('landing_page == "new_page"')).count()[0] $ n_new
len(df)
workspace_data = w.data_handler.get_all_column_data_df() $ lst = workspace_data.WATER_BODY_NAME.unique() $ print('WATER_BODY_NAME in workspace:\n{}'.format('\n'.join(lst)))
df.head()
RunSQL(sql_query)
temp_df1 = df[(df["landing_page"] == "new_page") & (df["group"] != "treatment")] $ temp_df2 = df[(df["landing_page"] != "new_page") & (df["group"] == "treatment")] $ n = temp_df2['user_id'].count() + temp_df1['user_id'].count() $ print("The number of times the new_page and treatment don't line up =", n)
for the_year in range(initial_year,final_year + 1): $     abc = payments_by_year[the_year].sort_values( ascending=[False]) $     abc.head() $     plot_top_DRGs(abc,5,the_year,'payment',False)
for row in range(100): #doing range(100) because it is a 1.9million row dataframe so iterating and populating this $     df['dist_km'] = haversine(df.start_long.iloc[row], $                               df.start_lat.iloc[row], $                               df.end_long.iloc[row], $                               df.end_long.iloc[row]) $
census['GEOID_tract']=census.apply(lambda x: x['GEOID'][:11],axis=1)
knn = KNeighborsClassifier(n_neighbors=5) $ scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy') $ print(scores)
selectedComponents = [x for x,y in zip(range(len(pca.explained_variance_)),pca.explained_variance_) if y>=0.1]
import csv $ import urllib2 $ url_1 = 'http://www.nasdaq.com/screening/companies-by-industry.aspx?exchange=NASDAQ&render=download' $ url_2 = "http://www.nasdaq.com/screening/companies-by-industry.aspx?exchange=NYSE&render=download"
lr_grid.fit(X_train,y_train)
visual_df['team'] = visual_df.index
df.head()
southern_sea_level.year == northern_sea_level.year
train_target_vecs = proc.transform(target_docs) $ hidden_states = embedding_model.predict(train_target_vecs[:, 1:])
twitter_archive.head()
(df2.query('group == "control"')['converted']==1).mean()
FREEVIEW.plot_histogram(raw_fix_count_df)
plt.figure(figsize = (5,5)) $ plt.scatter(X[:,0],X[:,1], label='True Position') 
twitter_ar['date'] = twitter_ar['timestamp'].apply(lambda row: row.split(' ')[0]) $ twitter_ar['time'] = twitter_ar['timestamp'].apply(lambda row: row.split(' ')[1])
errors['errorID'].value_counts().plot(kind='bar') $ plt.ylabel('Number of Errors') $ plt.show()
tweet_ids=[x.id for x in results] $ orig_tweet_ids=set(tweet_ids) $ len(tweet_ids) == len(orig_tweet_ids) $ len(tweet_ids)
x.shape
lgbm_train.to_csv('stacking_input/lgbm_tscv_train.csv',index=False)
day_ny.head()
list(df17)
appleinbounds.to_csv('../data/allAppleNeg.csv')
r.html.links
import datetime $ def time_to_date(row): $     return datetime.datetime.fromtimestamp(row['deadline']).strftime('%Y-%m-%d %H:%M:%S') $ df_us_['deadline_dt'] = df_us_.apply(time_to_date, axis =1)
loc = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', sep=',').load('treas_parking_meters_loc_datasd.csv') $ loc.show(3)
corpus = [dictionary.doc2bow(interests_token) for interests_token in interests_tokens] $ print(interests_tokens[1]) $ print(corpus[1])
week45 = week44.rename(columns={315:'315'}) $ stocks = stocks.rename(columns={'Week 44':'Week 45','308':'315'}) $ week45 = pd.merge(stocks,week45,on=['315','Tickers']) $ week45.drop_duplicates(subset='Link',inplace=True)
for i in we_rate_dogs[temp].text.values.tolist(): $     print('- ' + i + '\n')
df2.head() $
celgene_df.info()
Data['train']
education_2011_2015.head()
from sklearn.metrics import confusion_matrix $ %matplotlib inline
list_of_game_ids = list(set(re.findall(r'0021600\d{3}', str(soup))))
print(len(df_enhanced[df_enhanced['stages'] != 'nan'])) $ df_enhanced[df_enhanced['stages'] != 'nan']
model.most_similar('queen')
labels.describe()
model_country = Logit(df2['converted'], $                            df2[['intercept', 'CA', 'UK']]) $ results_country = model_country.fit() $ results_country.summary()
isinstance(elm, datetime)
df.drop(['id', 'id_region', 'country', 'name_region'],inplace=True, axis = 1)
Base.classes.keys()
data['popular'] = [1 if x >= percentile_50 else 0 for x in data['comments']] $
testy = pd.concat([test_kyo2, test_bkk2], axis=1)
ex4.dropna(axis = 1)
dfTemp=transactions.merge(users, how='outer',left_on='UserID',right_on='UserID') $ dfTemp
data.info()
df2[df2.duplicated('user_id')]
user=tweet.author. $ for param in dir(user): $     if not param.startswith("_"): $         print "%s : %s" % (param, eval("user." + param))
df_full["Field4"] = df_full.Field4.fillna("None")
dfFull.MasVnrArea = dfFull.MasVnrArea.fillna(dfFull.MasVnrArea.mean())
key_words=['roseanne' 'cancelled' 'abc' 'scraps' 'seaso']
cur = con.cursor() $ data_ls = cur.fetchall()
df2.user_id.value_counts()
walmart.asfreq('M', how='start') # We are changing the freq from quartely to monthly using the start date
extension
pd.value_counts(ac['Filing Date']).head()
x.loc[:,["B","A"]]
df_notnew = df.query('landing_page != "new_page"') $ df_3 = df_notnew.query('group == "treatment"') $ df_3.nunique() $
logistic_mod_time = sm.Logit(df3['converted'], df3[['intercept', 'wk2','wk3']]) $ results_time = logistic_mod_time.fit() $ results_time.summary()
not_so_recent = oldest_jobs[(oldest_jobs['Days Elapsed'] < 400) & (oldest_jobs['Days Elapsed'] > 200)] $ kinda_old_sorted = not_so_recent.sort_values(by='# Of Positions', ascending=False) $ kinda_old_sorted.head(30)
grouped_months_liberia = deaths_liberia.groupby(deaths_liberia['Date'].apply(lambda x: x.month)) $ deaths_liberia['National'] = deaths_liberia['National'].astype(int) $
chk = joined.loc[joined['nat_event']==1].head() $ holidays_df.loc[holidays_df['date']==chk.head()['date'].iloc[0]].head()
y = df.values $ y.size
gDateEnergy = itemTable.groupby([pd.Grouper(freq='1W', key='Date'),'Energy'], as_index=True) $ gDateEnergy_content = gDateEnergy['Content'] $
longitudinal = activeDF.groupBy("cid")\ $                        .agg({"ssd": "collect_list", $                              "num_ssd": "collect_list"})\ $                        .withColumnRenamed("collect_list(ssd)","ssd")\ $                        .withColumnRenamed("collect_list(num_ssd)", "num_ssd")
import statsmodels.api as sm $ convert_old = df2.query('group=="control"')['converted'].sum() $ convert_new = df2.query('group=="treatment"')['converted'].sum() $ n_old = len(df2.query('group=="control"')) $ n_new = len(df2.query('group=="treatment"'))
body = pd.get_dummies(auto_new.Body_Type) $ body.head()
archive_clean.info()
subcols = dropcol(subcols, ['hotel_cluster']) $ print(len(subcols)) $ print(subcols)
df_archive[df_archive.text.duplicated()]
X=df3[['FLOW']] $ y=df3[['REVENUE']]
model = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK', 'interaction_ab_CA', 'interaction_ab_UK']]).fit()
h2020.columns == fp7.columns
x_train, y_train = x[0:int(0.6*len(x))], y[0:int(0.6*y.size)] $ x_train = np.float32(x_train) $ y_train = np.float32(y_train)
cust_data.sort_values(by='MonthlyIncome', ascending=False).head(10) $
actual_payments.columns
vio.head(5)
prop.plot(kind='bar') $ plt.title('Anonymous location, Is the Donor a Teacher?') $ plt.show()
df_adjusted=df.merge(df_grouped_mean, how= 'left', on='year_month', validate= 'm:1')
import statsmodels.api as sm
df['hold_time'] = df['time_elapsed'] / df['totalTakeovers'] $ df['total_value'] = (df['hold_time'] * df['pointsPerHour']) + df['takeoverPoints']
merged_df.shape
df_modeling_categorized.head()
young = users.filter(users.age<21)
users.dtypes
len(pin2)
treatment_conv = conv_ind.query('group == "treatment"').shape[0] $ treatment_group = df2.query('group == "treatment"').shape[0] $ print('Probability of TREATMENT page converted individual: {:.4f}'.format(treatment_conv/treatment_group))
the_drg_number = 66 $ idx = df_providers[ (df_providers['year']==2011) & \ $                   (df_providers['drg3']==the_drg_number)].index.tolist() $ print('There are',len(idx),'sites for DRG',the_drg_number) $ print('Max payment:',np.max( df_providers.loc[idx,'disc_times_pay'] ))
h2o.save_model(aml.leader)
print(df_subset.dtypes)
dataPath = os.path.join("..", "example-data") $ data = pd.read_csv(os.path.join(dataPath, "example-from-j-jellyfish.csv")) $ data.head()
import numpy as np $ import pandas as pd $ from sklearn.preprocessing import MultiLabelBinarizer $ from itertools import product $ from sklearn.model_selection import StratifiedKFold
autos.head()
fld = 'StateHoliday' $ df = df.sort_values(['Store', 'Date']) $ get_elapsed(fld, 'After') $ df = df.sort_values(['Store', 'Date'], ascending=[True, False]) $ get_elapsed(fld, 'Before')
s[[1,2]]
result_2 = pd.concat([df1, df3], axis = 1, join_axes=[df1.index]) # concatenate one dataframe on another along columns $ result_2
cityID = 'a6c257c61f294ec1' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Greensboro.append(tweet) 
val_idx=[0]
df2.loc[df2.user_id.duplicated(keep = False),:]
s.describe()
df_twitter_copy[df_twitter_copy.expanded_urls.isnull()]
len(train_data[train_data.abtest == 'control'])
ab.converted.sum()/ab.user_id.nunique()
df.drop(34867,inplace=True)
n_new
train_df.shape
top10.plot.bar()
from shapely.geometry import Polygon $
dtc = DecisionTreeClassifier(max_leaf_nodes=1002) $ dtc.fit(X_train, y_train) $ predictions = dtc.predict(X_test) $ accuracy_score(y_true = y_test, y_pred = predictions)
mask = df["cancelled"] == 1 $ df["booking_application"][mask].value_counts() / df["booking_application"][mask].count()
s_t = np.sqrt(((n2-1)*n2*sd2+(n3-1)*n3*sd3)/(n2+n3-2)) $ t = (m3-m2)/(s_t*np.sqrt(1/n2+1/n3)) $ tscore = stats.t.ppf(.95,n2+n3-2) $ print("t stats is {0}; 95% t score is {1}".format(t,tscore))
X_train = houses_train.loc[:, houses_train.columns != "SalePrice"].values # convert to np.array $ y_train = houses_train.loc[:, houses_train.columns == "SalePrice"].values.reshape(-1, ) $ X_test = houses_test.loc[:, houses_train.columns != "SalePrice"].values # convert to np.array
plt.scatter(df['B'], df['C']) $ plt.title('Scatterplot of X and Y')
price_data = json_normalize(data, [['dataset', 'data']]) $ heading = json_normalize(data_test, [['dataset', 'column_names']]).transpose()
data['Date'] = pd.to_datetime(data['Date'], format="%d/%m/%Y")
train.columns
new.describe()
images_clean.info()
def plot_scores(scores, rolling_window=100): $
releases.columns
n_old = len(df2_control.index) $ n_old
df_clean.info()
autos["model_brand"] = autos["model"] + " " + autos["brand"] $ autos["model_brand"].head()
stringlike_instance_2.content = 'changed content'
data['area']
noise.head(2)
doc = r.text
lst_tickers = list(lst_tickers)
priors_product_reordered= priors_reordered.groupby(["product_id"]).size().reset_index(name ='reordered_count') $ priors_product_reordered.head()
df_link_meta[df_link_meta['link.domain'] == 'bit.ly']['link.domain_resolved'].value_counts().head(25)
stores = pd.read_csv("DataSets/stores.csv") $ stores
df.sort_values('date', inplace=True) $ df.head()
df_new['afternoon']= pd.get_dummies(df_new['day_part'])['afternoon']
gen = dta[dta.b==1]
sentiment_df.sort_values(by="Date",inplace=True,ascending=True)
_ = ok.grade('q04a') $ _ = ok.backup()
p_old = (df2['converted']==1).mean() $ p_old
pd.set_option('display.max_colwidth', -1) $ jimcramer_df = pd.DataFrame(jimcramer) $ jimcramer_df
sox.sort('date', ascending=True, inplace=True) $ sox.reset_index(drop=False, inplace=True) $ sox.rename(columns={'index':'game_id'}, inplace=True)
annual_returns
def get_list_of_Media_Body_URL(the_posts): $     list_of_Media_Body_URL = [] $     for i in list_Media_ID: $         list_of_Media_Body_URL.append(the_posts[i]["object_url"]) $     return list_of_Media_Body_URL
df['sentiment'].value_counts()
MergeMonth = Merge.copy(deep=True)
weights = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)
get_freq(series_obj=raw.sex_age)
import pandas as pd
store_items['suits'] = store_items['shirts'] + store_items['pants'] $ store_items
snow.select ("select * from ST_ADPKD_diag limit 5")
pd.concat([msftA[:5], aaplA[:3]], axis=1, join='inner', keys=['MSFT', 'AAPL'])
jail_census.loc['2017-02-01'].groupby('Gender')['Age at Booking'].mean()
male_journalists_mention_summary_df = journalists_mention_summary_df[journalists_mention_summary_df.gender == 'M'] $ male_journalists_mention_summary_df.to_csv('output/male_journalists_mentioned_by_journalists.csv') $ male_journalists_mention_summary_df[journalist_mention_summary_fields].head(25)
def download_recent_tweets_by_user(user_account_name, keys): $     import tweepy $     ...
treatment_convert = df2.query('group == "treatment"').converted.mean() $ treatment_convert
df_tweet_clean2 = df_tweet_clean[df_tweet_clean.tweet_id.isin(id_list)] $ df_image_clean2 = df_image_clean[df_image_clean.tweet_id.isin(id_list)]
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"').converted.sum() $ convert_new = df2.query('landing_page == "new_page"').converted.sum() $ n_old = df2.query('landing_page == "old_page"').landing_page.count() $ n_new = df2.query('landing_page == "new_page"').landing_page.count() $
with pd.option_context("use_inf_as_null", True, $                        "max.rows", 15): $     print(ratio.dropna())
Population = np.array(Population)
141/645
commits = EQCC(git_index) $ commits.get_cardinality("hash") $ total_commits = commits.get_aggs() $ print("total commits: ", total_commits) $ all_commits = commits.fetch_results_from_source("hash", "commit_date")
df_new.to_csv(path_or_buf='data/df_fe_dummy_comma.csv', sep=',', $               header=True, index=False, $               encoding='utf-8')
unique_countries = df_new.country.unique() $ unique_countries
test = pd.read_json('./data/test.json')
df_a.join(df_b, how = "outer") # outer join (see above for definition)
df_pivot1 = df_pivot[df_pivot[10102].notnull() & df_pivot[10120].notnull()] $ df_pivot1.shape
print(data.dtypes)
result_concat.loc[3]
idx = pd.IndexSlice $ df.loc[idx['a', 'ii', 'z'], :]
import sys $ src_dir = os.path.join('..', 'src') $ sys.path.append(src_dir)
!gunzip -c GCA_900186905.1_49923_G01_feature_table.txt.gz | head -n 4
df = pd.read_csv("contact.csv", index_col=None) 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=4) $ print('Train set: ',X_train.shape,y_train.shape) $ print('Test set: ', X_test.shape, y_test.shape)
sns.barplot(x='user',y='retweets',data=highly_retweeted)
cc.high.describe()
len(coins_infund)
import numpy as np $ np.where(lv_workspace.index_handler.subset_filter)
svm_model = SVC(C=0.01, class_weight='balanced', kernel='linear')
from splinter import Browser $ import time
reddit[['Titles', 'Subreddits']].duplicated().sum() 
weather_mean.iloc[5:15:3, :]
contribs.amount
from sklearn.linear_model import LogisticRegression
from scipy.stats import norm $ print(norm.ppf(1-(0.05)))
grid_df = pd.DataFrame(data = us_grid) $ grid_df.columns = ['glon', 'glat'] $ grid_df = pd.concat([grid_id, grid_df], axis=1) $ grid_df.head()
countries.user_id.nunique(), countries.user_id.shape[0], countries.user_id.nunique() == countries.user_id.shape[0], countries.user_id.shape[0] == df2.shape[0]
import statsmodels.api as sm $ df_new['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['intercept','UK','US']]) $ res = log_mod.fit() $ res.summary2()
knn.fit(train[['property_type', 'lat', 'lon','surface_covered_in_m2','surface_total_in_m2','floor']], train['rooms'])
data_sets['15min'].to_pickle('final_15.pickle') $ data_sets['30min'].to_pickle('final_30.pickle') $ data_sets['60min'].to_pickle('final_60.pickle')
store_items.isnull().sum()
pres_df['hour_aired'].dtypes
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page' and group == 'control' and converted == '1'").count()[0] $ convert_new = df2.query("landing_page == 'new_page' and group == 'treatment' and converted == '1'").count()[0] $ n_old = n_old $ n_new = n_new
unwanted= df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False] $ unwanted.shape[0]
d = docx.Document(downloadIfNeeded(example_docx, example_docx_save, mode = 'rb')) $ for paragraph in d.paragraphs[:7]: $     print(paragraph.text)
q = [ [scores.iloc[score].idxmax(),scores.iloc[score].max()] for score in range(len(scores)) ]
gram_collection.find_one({"account": "deluxetattoochicago"})['link_to_post'] $ gram_collection.find_one({"account": "deluxetattoochicago"})
x_train = scaler.transform(x_train) $ x_test = scaler.transform(x_test)
df.to_csv('clean_df.csv')
n_old=len(df2_control) $ n_old
old_page_converted = np.random.choice([1,0], size=n_old, p=[p_old,(1-p_old)]) $ old_page_converted.mean()
df2 = df2.drop([2893])
conditions.value_counts()
df_archive_clean[["timestamp","retweeted_status_timestamp"]].info()
lsi.save('trump.lsi') $ lsi = models.LsiModel.load('trump.lsi')
temp_cat.get_values()  #array
date_df.head()
from DeepText.preprocess import preProcessor $ pp = preProcessor() $ clean_func = pp.default_cleaner $ tokenizer = pp.default_tokenizer
scores.info()
df.apply(lambda a: sum(a.isnull()),axis=0) 
measurement_df.describe() 
transactions.merge(users, how='inner', on=['UserID'])
moving_average_for_closed_issues
print(json.dumps(experiment_details, indent=2))
p_new = df2[(df2.group == 'treatment')&(df2.converted == 1)].shape[0]/df2[df2.group == 'treatment'].shape[0] $ p_new
df_nasdaq = spark.read.csv(bucket.object('nasdaq.csv').uri, header=True, schema=csv_schema).withColumn('IDX', func.lit('NASDAQ')) $ df_nasdaq.show() $ df_russell_2k = spark.read.csv(bucket.object('russell_2000.csv').uri, header=True, schema=csv_schema).withColumn('IDX', func.lit('RUSSELL_2000')) $ df_russell_2k.show() $
NewPage = df2.query('landing_page=="new_page"')['user_id'].count() $ NewPage
CONSUMER_KEY    = 'D7PQj2xuuw2CQunHiLFCUyy8v' $ CONSUMER_SECRET = '0u9PwH9JhLsvndnsRv5tF2NyoH7KJim2CruJ4zVE7TsMZetHFi' $ ACCESS_TOKEN  = '2163444086-30rJ2fINs8CNQiCslKpIMcSGC3eeqMMaQRLWesH' $ ACCESS_SECRET = 'aQOkHU0pH5q5vEto6VnlNZYlUOm6EdrI5lPMDatFtk1dn'
df.tail(2)
url_CHI = "https://bears.seasonticketrights.com/Images/Teams/ChicagoBears/SalesData/Chicago-Bears-Sales-Data.xls"
89337
resdf=resdf.drop(['Unnamed: 0'], axis=1) $ resdf.head(3) $
df_twitter_archive.tweet_id.nunique()
model = denseModel1() $ model.summary()
MNB = SklearnClassifier(MultinomialNB()) $ MNB.train(train_set)
data_dummies.head()
df2.head()
new_page_converted = np.random.binomial(n_new , p_new)
station_distance.insert(loc=11, column='Distance(Miles)', value=distance)
os.mkdir('AV')
dfg.sort_values(['clusterlabel']).style.set_table_styles([dict(selector="th",props=[('max-width', '150px')])])
grinter1.head()
telemetry = pd.read_csv('telemetry.csv', encoding='utf-8') $ telemetry.head()
hdf = df.set_index('AgeBins') $ hdf.loc['child', :].head()
len(significant_features)
%%time $ dfRegMet["sentences"] = dfRegMet["sentences"].str.split()
count1df = pd.DataFrame(kochbar02) $ count1df = count1df.drop_duplicates(subset=['name', 'user']) $ count1df.info()
fb.founded_on.year
ts.shift(1)
for zc in weather.zip_code.unique(): $     print weather[weather.zip_code == zc].isnull().sum() $     print
print('Feature engineering started at : ', datetime.datetime.now())
X_train = pd.get_dummies(columns=['term', 'home_ownership', 'verification_status', 'purpose'], data=X_train)
len(bitcoin_df)
df_t = df.groupby(['user_id','nd_key_formatted'])['course_key'].apply(lambda x: ', '.join(x)).reset_index() $ df_t
type(r.json())
new_read = pd.merge(is_read, articles[['id','read_time']], how='inner', left_on=['article_id'], right_on = ['id']) $ print(new_read.shape) $ new_read.head() 
bagofwords = countvec.fit_transform(train_df['desc']) $ tfidfdata = tfidfvec.fit_transform(train_df['desc'])
text = np.array(text)
df_326 = pd.read_sql(sql_326,conn_laurel) $ df_326.groupby(['accepted','paid','preview_clicked','preview_watched','preview_finished'])['applicant_id'].count().unstack().fillna(0)
for i in range(len(weather_all.columns)): $     column_name= weather_all.columns[i] $     type_= weather_all.dtypes[i] $     if (type_== int) or (type_== float): $         weather_all[column_name]= weather_all[column_name].fillna(int(np.mean(weather_all[column_name])))
groceries # apples still in groceries
df.info() $ df.isnull().values.any()
delay_only_time_dfs = perspective_time_dfs.copy()
t
data_vi.describe()
import lightgbm as lgb $ mdl =lgb.LGBMClassifier(boosting_type ='gbdt',objective ='binary', num_leaves =30) $ mdl.fit(X_train,y_train)
test = pd.DataFrame(raw_df.groupby('Date')['DA-price'].mean()) $ test['DA-price'].values
tweet_ids_twitter_archive.shape[0]
import statsmodels.api as sm $ convert_old = df2.query('group == "control"').converted.sum() $ convert_new = df2.query('group == "treatment"').converted.sum() $ n_old = len(df2.query("group == 'control'")) $ n_new = len(df2.query("group == 'treatment'"))
prop = props[props.prop_name == "PROPOSITION 064- MARIJUANA LEGALIZATION. INITIATIVE STATUTE."]
df2['intercept'] = 1 $ df2[['control', 'ab_page']] = pd.get_dummies(df2['group'])
logit_mod3 = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'CA', 'ab_CA']]) $ results3 = logit_mod3.fit() $ results3.summary()
cX_test = X_test.copy()
df2.drop_duplicates(subset='user_id', inplace = True)
df['seqid'].unique()
difference = treatment_group.converted.mean() - control_group.converted.mean() $ plt.hist(p_diffs); $ plt.axvline(x=difference, color='orange'); $ plt.title('Distribution of mean differences'); $ print(difference)
df2.plot.bar(stacked=True, rot=0)
prng.to_timestamp(how='e')
(ggplot(raw_large_grid_df.groupby(['eyetracker','posx','posy'],as_index=False).mean(),aes(x="posx",y="posy",size="accuracy"))+geom_point()+facet_wrap("~eyetracker"))+coord_fixed()
Base.metadata.create_all(engine)
import statsmodels.formula.api as smf $ model = smf.ols('VIX ~ SP500', data=df) $ result = model.fit(cov_type="HAC", cov_kwds={'maxlags':5}) $ print(result.summary2())
df_schoo11 = df_schools.rename(columns={'name':'school_name'}) $ df_schoo11.head()
import pandas as pd $ bild = pd.io.json.json_normalize(data=bild) $ spon = pd.io.json.json_normalize(data=spon)
df_rand = unclassified(df).sample(1000) $ df_rand = df_rand[df_rand.postcodes.notnull()] $ num_pc_entries = len(df_rand) $ print("This run has {} random entries containing postcode information.".format(num_pc_entries))
def get_ab_page(row): $     if row['group'] == "treatment": $         return int(1) $     else: $         return int(0)
plt.hist(ort_avg17, bins=20, align='mid'); $ plt.xlabel('Offensive Rating') $ plt.ylabel('Count') $ plt.title("Histogram of Teams' Offensive Rating, 2017-2018 Season\n");
print('Max time:', data.created_at.max()) $ print('Min time:', data.created_at.min())
noLatLon = datatest[datatest.lat.isnull()].groupby('place_name').agg(np.size) $ noLatLon
containers[0]
(combined[['location','country','date','zika_cases']] $  .groupby('country') $  .sum() $   .sort_values('zika_cases',ascending=False)) $
tweet_archive_clean[tweet_archive_clean.tweet_id == 855862651834028034]
mars_table = table[0] $ mars_table.columns = ["Parameter", "Values"] $ mars_table.set_index(["Parameter"]) $
parse_dict['creator'].head(5) $
os.getcwd()
df_log.head()
names=my_zip.namelist() $ size= [f.file_size for f in my_zip.filelist] $
spp['season'] = spp.index.str.split('.').str[0] $ spp['term'] = spp.index.str.split('.').str[1]
etsamples_100hz.iloc[0:1]
nold=df2[df2['group']=='control'].count()[0] $ nold
tsla_neg = mapped.filter(lambda row: row[3] < 0) $
pt=pd.pivot_table(data=cust_demo, index=['Location','Martial_Status'], columns=['Gender', 'Own_House'], values='age',aggfunc='mean' )
groupby_regiment = df['score'].groupby([df['author'],df['com_id']]) $ q=groupby_regiment.max() $ q.head()
df.head(5)
np.bincount(y_train)
linear_predictor = linear.deploy(initial_instance_count=1, $                                  instance_type='ml.m4.xlarge')
tbl.head()
filter_df.shape[0] - all_df.shape[0], ((filter_df.shape[0] - all_df.shape[0]) / all_df.shape[0]) * 100
df.shape
autos['kilometer'].describe()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth)
result[0]
dst_cap[]
department_df_sub.sum(axis = 0) # non-apply equivalent 
week_num=[] $ for i in range(len(one_station)): $     week_num.append("week"+str(math.ceil((i+1)/7))) $ week_num[:14]
target_column = 'DGS30'
stations_total = (stations.groupby(['STATION', 'WEEK']).DAILY_ENTRIES.sum().reset_index()) $ stations_total.head(5)
df['misaligned']=((df.group=='treatment') & (df.landing_page=='old_page')) | ((df.group=='control') & (df.landing_page=='new_page'))
train.describe(include='all')
urls["domain"] = urls["url"].str.replace("http.*://([^/]+).*",r"\1") $ urls.head()
reddit['Late Night Hours'] = reddit['Hours'].apply(lambda x: 1 if x<3 and x>22 else 0)
plt.savefig('aapl.png') $ plt.savefig('aapl.jpg') $ plt.savefig('aapl.pdf') $ plt.show()
data = json_normalize(r.json(), [['fantasy_content', 'leagues', '0', 'league']]) $ data
sns.regplot(x=df['score'], y=df["comms_num"], $             line_kws={"color":"r","alpha":0.7,"lw":5});
df_members.head()
c.execute('SELECT * FROM iris')
df["booking_user_agent"][mask].value_counts() / df["booking_user_agent"][mask].count()
combined_df4 = combined_df3[['rv','vo_propdescrip','empty_prop', $                    'llpg_usage', 'Ward', 'paymeth_code','bill_no', 'account_bal', 'bill_bal']]
data = pd.read_csv('pop_hot_raw.csv') $ data.head()
pickle.dump(sample_weight, open(slowdata + 'sample_weight.pkl', 'wb'))
df.median() + 1.57 * (df.quantile(.75) - df.quantile(.25))/np.sqrt(df.count())
cm = metrics.confusion_matrix(y_test, y_pred) $ print(cm)
trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy')) $ val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))
import numpy as np $ import pandas as pd $ from bs4 import BeautifulSoup $ from selenium import webdriver $ import time
usecols = ['ip', 'app', 'device', 'os', 'channel', $ 'click_timeDay', 'click_timeHour','random_mean_encode_ip', $ 'random_mean_encode_app', 'random_mean_encode_device', $ 'random_mean_encode_os', 'random_mean_encode_channel', $ 'random_mean_encode_click_timeHour']
crimes.columns = crimes.columns.str.strip() $ crimes.columns
genre_vectors = cv.fit_transform(shows['genres'].dropna()).todense() $ genre_vectors = pd.DataFrame(genre_vectors, columns=cv.get_feature_names())
display(data[['error', 'relative_error']].describe(include='all'))
p_diffs = np.random.binomial(n_new, p_new, 10000)/n_new - np.random.binomial(n_old, p_old, 10000)/n_old $ p_diffs
allVars = read.getVariables()
INQ2016.head(1)
import pandas as pd $ import numpy as np $ pd.options.mode.chained_assignment = None  # default='warn' $ import glob as glob
db.domains
recipes.shape
m.fit(train_p)
region = pd.DataFrame(df.pop("region").tolist()) $ df = df.join(region, rsuffix="_region")
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','Category']).closed_at.count() $ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'purple', 'red'], grid=False)
df[df.sentiment == 4].index
data3 = pd.get_dummies(data2)
technique_name
df = pd.read_csv("msft.csv", $                 skip_footer = 2, $                 engine = 'python') $ df
mb = pd.read_csv("data/microbiome.csv") $ mb
tf.logging.set_verbosity(tf.logging.INFO) $ shutil.rmtree('taxi_trained', ignore_errors=True) # start fresh each time $ model = tf.contrib.learn.LinearRegressor( $       feature_columns=feature_cols, model_dir='taxi_trained') $ model.fit(input_fn=get_train());
!ls Data/*.csv
dr = pd.date_range('1/1/2010', periods=3, freq='3B')
cursor.execute(sq83) $ cursor.execute(sq84) $ results = cursor.fetchall() $ results
s = pd.Series(np.random.randint(0,10,size=10)) $ s
properati[properati['zone'] == "Bs.As. G.B.A. Zona Norte"]
autos['price'].head()
menu["components"].apply(add_side)
df_final['cluster'] = y_kmeans
import seaborn as sns $ %matplotlib inline $ from matplotlib import pyplot as plt $ plt.figure(figsize=(10,10)) $ sns.distplot(df_nd101[(df_nd101['ud120']>=0)&(df_nd101['ud120']<=1)].ud120)
loan_stats['loan_status'].table()
mb.xs(1, level='Patient')
for post in posts.find(): $     print(post)
plt.scatter(range(len(train_df)), train_df.price)
X, y = plot_profit(test_df, 'model_predict', early_stop=None)
consumerKey = "Wnb1NHXFj8dJockenbOQtHRs1" $ consumerSecret ="B9R7aJwPxK3v5jDWprjBLf4Q6q4WWTI8TXHLjtc2MzHMq48Qas"
df_new[['UK', 'US']] = pd.get_dummies(df_new['country'])[['UK', 'US']] $ df_new.head()
scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='neg_log_loss') $ scores
old_page_converted = pd.DataFrame(np.random.choice([0,1],n_old,p=[1-CRold,CRold])) $
df.user_id.unique().shape
data.describe()
train_small_data.to_feather("../../../data/talking/train_small_data.csv.feather") $ val_small_data.to_feather("../../../data/talking/val_small_data.feather")
plt.scatter(reddit['Comments'], reddit['Upvotes']) $ plt.xlabel('Upvotes',fontsize='large') $ plt.ylabel('Comments',fontsize='large') $ plt.title('Upvotes and Comments have a positive relationship!', fontsize='large') $ plt.show()
newdf.head(12)
SCN_BDAY = pd.merge(BID_PLANS_df,pd.to_datetime(BDAY_PAIR_df['birthdate']).dt.strftime('%Y-%m-%d').to_frame(),how='left',left_index=True,right_index=True)
treaty_id  ##output is an object
Google_stock.isnull()
ts_mean = ts.resample(sampling, how='mean') $ ts_mean = pd.concat([nposts, ts_count, ts_mean], axis=1)
data = pd.read_json("http://dq-dataset.herokuapp.com/data")
timezones = DataSet['userTimezone'].value_counts()[:20] $ print(timezones)
YS1517.Close.plot(label='Yahoo') $ moving_average.plot(label='Moving average') $ plt.legend() $ plt.gcf().set_size_inches(15,5)
predict_prob = model.predict(padded_docs) $ predictions = model.predict(padded_docs)
for iter_x in np.arange(lookforward_window)+1: $     df['y+{0}'.format(str(iter_x))] = df[base_col].shift(-iter_x)
n_new = df2[df2['landing_page']=='new_page'].count()[0] $ n_new
%time pax_raw = pd.read_hdf(os.path.join(data_dir, hdf_path), 'pax_raw_sample')
import json $ with open("credentials.json", "r") as file: $     credentials = json.load(file) $     toggl_cr = credentials['toggl'] $     APIKEY = toggl_cr['APIKEY']
convert_size(olympus_btyes)
logit = sm.Logit(df2['converted'], df2[['intercept','ab_page']])
df_imputed = pd.DataFrame(df_imput) $
old_page_converted = np.random.choice([0, 1], size=n_old, p=[(1-p_old), p_old]) $ (old_page_converted == 1).sum()
merge.shape
users.country_destination.value_counts()
df2['converted'].sum()/len(df2)
autos['date_crawled'].str[:10].value_counts(normalize = True,dropna = False).sort_index()
df2.query('converted == "1"').count()[0]/df2.shape[0]
target_pf['is_rebalance'] = target_pf['date'].map(lambda x: True if x in lst_date_to_set else False)
npath = out_file2 $ resource_id = hs.addResourceFile('1df83d07805042ce91d806db9fed1eeb', npath)
_ = df.isnull().sum(axis=0) $ print ("quantidade de missing values: "+str(_.sum()))
ben_fin.describe()
segmentData['opportunity_won_amount'] = segmentData.apply(stageWonMap, axis=1)
INT.loc[:,'YEAR'] =INT['Create_Date'].apply(lambda x: "%d" % (x.year))
from IPython.display import IFrame $ IFrame('http://www.aflcio.org/Legislation-and-Politics/Legislative-Alerts', 800, 600)
autos['price'].describe()
slope, intercept
for a in tqdm(hol_dur): $     df[a]=df[a].fillna(pd.Timedelta(0)).dt.days
major_cities_l1 = major_cities_layers[0] $ major_cities_l1_fset = major_cities_l1.query(where= 'FID < 11') $ type(major_cities_l1_fset)
train.head(1)
month['PERIOD ENTRIES'] = month['ENTRIES'].diff() $ month['PERIOD ENTRIES'] = month['PERIOD ENTRIES'].shift(periods = -1) $ month
data.loc[data.rooms > 12, 'rooms'] = np.NaN
labels_for_menu_dishes_about_latent_features = ['menu_dishes_about_related_to_salad_ingredients', 'menu_dishes_about_related_to_curry_ingredients', 'menu_dishes_about_related_to_sushi_making', 'menu_dishes_about_related_to_bread_drinks', 'menu_dishes_about_related_to_beefy_potato', 'menu_dishes_about_related_to_stir_fry', 'menu_dishes_related_to_dessert', 'menu_dishes_about_related_to_cake_noodles', 'menu_dishes_about_related_to_organic_ingredients', 'menu_dishes_about_related_to_marinated_stir_fry']
(df2['landing_page'] == 'new_page').mean() $
df_loc = df_location[['country','name','state']] $ df_loc = df_loc.rename(columns={'country':'loc_country','name':'loc_name','state':'loc_state'}) $
week50 = week49.rename(columns={350:'350'}) $ stocks = stocks.rename(columns={'Week 49':'Week 50','343':'350'}) $ week50 = pd.merge(stocks,week50,on=['350','Tickers']) $ week50.drop_duplicates(subset='Link',inplace=True)
total_number_of_values_in_array = (arr_size[0]*arr_size[1]*arr_size[2])
tag_df.stack().head()
yhat = SVM_model.predict(X_test) $ yhat
reg = sm.Logit(df3['converted'], df3[['intercept', 'US', 'CA']])
train.shape, test.shape
X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)
EBOLA_FOLDER = DATA_FOLDER + "/ebola" $ GUINEA_FOLDER = EBOLA_FOLDER + "/guinea_data" $ LIBERIA_FOLDER = EBOLA_FOLDER + "/liberia_data" $ SIERRALEONE_FOLDER = EBOLA_FOLDER + "/sl_data"
df.describe()
raw_df.head()
cleaned_sampled_contirbutors_human_df = sampled_contirbutors_human_df.select( $     "*", $     clean_gender_field_udf("Answer_gender").alias("cleaned_gender"))
df_twitter.describe()
my_columns = list(df_.columns) $ print(my_columns)
prcp_analysis_df.set_index(['Date'], inplace=True) $ prcp_analysis_df.head()
my_query = "SELECT * FROM ephys_roi_results LIMIT 1" $ my_result = limsquery(my_query) $ first_element = my_result[0] $ print first_element.keys() $
API.InitializeRateLimit()
print('Best Score: {}'.format(XGBClassifier.best_ntree_limit))
df_new['interaction1'] = df_new['CA']*df_new['new_page'] $ df_new['interaction2'] = df_new['UK']*df_new['new_page'] $ loginter = sm.Logit(df_new['converted'],df_new[['intercept','interaction1','interaction2','new_page','CA','UK']]) $ results2=loginter.fit() $ results2.summary()
plt.hist(p_diffs) $ plt.axvline(x = diff_obs,color = "r");
df_geo.sample(10)
dfChile = df[df["latitude"] > -56.000000]
pop = pd.read_csv('data/state-population.csv') $ areas = pd.read_csv('data/state-areas.csv') $ abbrevs = pd.read_csv('data/state-abbrevs.csv') $ display('pop.head()', 'areas.head()', 'abbrevs.head()')
npcontrol = df[(df.landing_page == "new_page") & (df.group == "control")] $ optreatment = df[(df.landing_page == "old_page") & (df.group == "treatment")] $ inaccurate = pd.concat([npcontrol, optreatment]) $ inaccurate_index = inaccurate.index $ df2 = df.drop(inaccurate_index)
train.bedrooms.value_counts()
calls_df[calls_df["phone number"]==5713155441]
exportID['join_col'] = exportID['event.origin_subzone_name'] + exportID['timestamp']
df.head() $ len(df)
bacteria_data
from sklearn.svm import SVC
pt = jdfs.pushed_at.apply(lambda x: time.mktime(x.timetuple())) $ npt = pt - pt.min()
autos['price'].describe()
customer_funnel
gMapAddrDat.set_ServerTimeout(9)
auto_new = auto_new[auto_new.Price > 1000]
for row in session.query(Measurements).limit(5).all(): $     print(row)
cleaned = facilities.drop_duplicates(subset=['facname','address','idagency']) $ cleaned.info()
print('Most negative tweets:') $ for t in senti.sort_values('polarity').head()['text']: $     print('  ', t)
table_1c[u'Collected from port'] < store_time
train["date_time"] = pd.to_datetime(train["date_time"], errors="coerce") $ train["srch_ci"] = pd.to_datetime(train["srch_ci"], errors="coerce") $ train["srch_co"] = pd.to_datetime(train["srch_co"], errors="coerce") $
idx = test_df[ (test_df['Provider Id']==260096)  ].index.tolist() $ idx
agent = TradingAgent(data)
temp_df2['timestamp'] = pd.to_datetime(temp_df2['timestamp'],infer_datetime_format=True)
from sklearn.mixture import GaussianMixture $ gmm = GaussianMixture(2) $ gmm.fit(X) $ labels = gmm.predict(X) $ labels
inspector = inspect(engine) $ columns = inspector.get_columns('measurement') $ for c in columns: $     print(c['name'], c["type"])
test = np.load('./pt1sz2.npy') $ print test.shape
plt.hist(p_diffs) $ plt.xlabel('p_new - p_old') $ plt.ylabel('Frequency') $ plt.show();
not_lined_up = df_ab[(df_ab['group']=='treatment') & (df_ab['landing_page']!= 'new_page')|(df_ab['group']=='control') & (df_ab['landing_page']!= 'old_page')] $ not_lined_up.count()[0] $
jobs_data['clean_titles'].head(5)
!wget ftp://ftp.solgenomics.net/tomato_genome/annotation/ITAG3.2_release/ITAG3.2_RepeatModeler_repeats_light.gff -P $DATA_PATH
print(X_train.iloc[0,]) $ print(cvec.get_feature_names()[64], cvec.get_feature_names()[94])
len(active_psc_statements[active_psc_statements.statement.str.contains('no-individual-or-entity-with-signficant-control')].company_number.unique())
import csv $ import os $ import pandas as pd $ from sklearn.feature_extraction.text import TfidfVectorizer $ from sklearn.metrics.pairwise import linear_kernel
simple = TextBlob("Simple is better than complex. Complex is better than complicated.") $ simple.translate(to='it')
festivals_clean.info()
google_stock
df.loc[  100:103  ]  # by row label range
v_invoice_hub.columns.tolist()
le.fit(df2['E'])
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, inspect, func
price_df.drop(['date'], axis = 1, inplace = True)
I = np.array([0, 1, 2, 3, 15, 16, 32, 64, 128]) $ B = ((I.reshape(-1,1) & (2**np.arange(8))) != 0).astype(int) $ print(B[:,::-1]) $ I = np.array([0, 1, 2, 3, 15, 16, 32, 64, 128], dtype=np.uint8) $ print(np.unpackbits(I[:, np.newaxis], axis=1))
import re
exiftool -csv -createdate -modifydate cisuabf6/cisuabf6_cycle1.MP4 cisuabf6/cisuabf6_cycle2.MP4 cisuabf6/cisuabf6_cycle3.MP4 cisuabf6/cisuabf6_cycle4.MP4  > cisuabf6.csv
s_t = np.sqrt(((n1-1)*n1*sd1+(n3-1)*n3*sd3)/(n1+n3-2)) $ t = (m3-m1)/(s_t*np.sqrt(1/n1+1/n3)) $ tscore = stats.t.ppf(.95,n1+n3-2) $ print("t stats is {0}; 95% t score is {1}".format(t,tscore))
QUIDS_wide.dropna(subset =["qstot_0"], axis =0, inplace=True)
rt_extended.head()
countries_df=countries_df.set_index('user_id')
INC.head()
big_append = df1.append([df2,df3,df4], sort=True) $ tabulation = pd.crosstab(big_append['Temperature (F)'], columns='count', margins=True) $ tabulation.columns.name = 'Freq Tabulation' $ tabulation
pd.concat([s1, s2], ignore_index=True)
commas = xmlData.loc[89, 'address'].count(',') $ print commas
print(df.query('group=="treatment" & landing_page!="new_page"').head()) $ print(df.query('group=="treatment" & landing_page!="new_page"').shape) $ print(df.query('group=="control" & landing_page!="old_page"').shape) $ 1965+1928
customers = pd.read_table(url_customers,sep=',') $ transactions = pd.read_table(url_transactions,sep=',')
infinity.head()
print('Total number of telemetry records: {}'.format(len(telemetry.index))) $ print(telemetry.head()) $ telemetry.describe()
df_times.head() $
df1.dropna(inplace=True)
df_city_restaurants = df_restaurants.filter(df_restaurants.city == 'Edinburgh')
dfs_loan[0].columns
source = dataset['Tweet_Source'] $ chart_title = 'Proportion of tweets from devices on different platforms' $ source.value_counts().plot.pie(label='', fontsize=11, autopct='%.2f', figsize=(6, 6), title=chart_title, legend=True);
import matplotlib.pyplot as plt $ from matplotlib import style $ style.use('ggplot')
tweet_archive_master_name = 'twitter_archive_master.csv' $ tweet_archive_master_path = os.path.join(folder_name, tweet_archive_master_name) $ tweet_archive_master.to_csv(tweet_archive_master_path)
volmeans = cc.groupby(['name'])['volume'].mean() $ volmeans.sort_values(ascending = False)
df['Shipping Method name'] = df['Shipping Method name'].fillna(df['Shipping Method Id'])
df_hour = df.copy()
import pandas as pd $ import matplotlib.pyplot as plt
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print(z_score) $ print(p_value)
interactions1Data = pd.read_pickle(folderData + 'interactions1Data.pkl')
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=", auth=('', '')) $
plt.hist(p_diffs) $ plt.xlabel('probability of difference') $ plt.ylabel('Frequency') $ plt.title('Plot of 10K simulated p_diffs');
P_new = df2.converted.mean() $ print("The convert rate for p-new under the null is {}.".format(P_new))
import os $ imagelist = [i for i in os.listdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $              if i.startswith('BARNE') and  i.endswith('pdf') ]
nb = MultinomialNB() $ nb.fit(X_train, y_train) $ nb.score(X_test, y_test)
sum(trumpint.num_likes),sum(cliint.num_likes)
bacteria_data.index
RF.fit(X_train,Y_train)
counts = Counter(word_list) $ pprint(counts.most_common(10))
for row in nps.itertuples(): $     if row.GNIS_ID == '1309762': $         print (row.PARKNAME + '\n') $         print (str(row.geometry))  
frac_volume_m = pd.concat([fraq_volume_m_coins, $                            fraq_fund_volume_m], axis=1)
df.info()
df.drop('min', axis=1, inplace=True) $ df.drop('max', axis=1, inplace=True) $ df
dtc = DecisionTreeClassifier() $ dtc.fit(X_train, y_train) $ predictions = dtc.predict(X_test) $ accuracy_score(y_true = y_test, y_pred = predictions)
fda_drugs.head()
indeed['category'] = indeed['title'].apply(is_category) $ tia['category'] = tia['title'].apply(is_category)
Lab7_RevenueEPS0.head()
max_activity = activity[0][0:2] $ max_activity
finals[finals.pts_l!=1].shape
tx = pd.read_csv("../data/inputs/TEC_CF_CSV/filers.csv", dtype = {"filerIdent": int}, $                  parse_dates = ['filerEffStartDt','filerEffStopDt', 'treasEffStartDt', 'treasEffStopDt'])
print(optimal_weights[30])
df_archive_clean.info()
df_new1=df.query('landing_page=="new_page" & group=="control" ') $ df_new1.tail(10) $ df_new1.nunique()
ab_data.describe()
obs_diff = p_treatment- p_control $ (p_diffs > obs_diff).mean()
df_int.plot() $ plt.show()
model.most_similar("man") $
df.pivot_table(values='(kW)', index='YEAR', columns='Make', aggfunc=np.mean)
pizza_poor_reviews = pizza_poor_reviews.assign(bad_reviews = reviews_bad ) $ lemm_bad_text = [ process( ii['bad_reviews']) for ind, ii in pizza_poor_reviews.iterrows() ] $ pizza_poor_reviews = pizza_poor_reviews.assign(bad_reviews_token =lemm_bad_text) $ pizza_poor_reviews = pizza_poor_reviews[pizza_poor_reviews['bad_reviews'] != '' ] $ pizza_poor_reviews.dropna( how = 'any' , inplace = True) $
pd.crosstab(index=mydf.comp, columns='counter')
scores = pd.read_csv('../../data/fandango_score_comparison.csv')
df_temp_by_station = engine.execute("select min(tobs), max(tobs), avg(tobs) from measurement where station = 'USC00519281';").fetchall() $ df_temp_by_station
tb = pd.read_csv('https://assets.datacamp.com/production/course_2023/datasets/tb.csv') $ print(tb.head())
df.groupby('status')['MeanFlow_cms'].describe(percentiles=[0.1,0.25,0.75,0.9])
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new],[n_old,n_new], alternative = "smaller") $ print(z_score, p_value)
df_elect.shape
r.json()
cars.loc[cars.price > 160000].count()['name'] $
tips.set_index(["sex","day"]).head(5)
test = spark.read.csv(os.path.join(datapath,"test.csv"), header=True) $ print('Found %d observations in test set.' %test.count())
daily.asfreq("H")
afx_2017_url = base_url+"?start_date=2016-12-31&end_date=2018-01-01"+api_url $ r = requests.get(afx_2017_url)
oppDF.head()
help(datasets[0]) $ print(datasets[0].__dict__)
df["message_tokens"] = df.message.apply(word_tokenize) #Tokenize and remove stop words first. $ df.loc[:,"message_tokens"]= df['message_tokens'].apply(lambda x: [item for item in x if item not in stopword_list])
honeypot_df['Time stamp'][1:5]
cX_test.drop(['Thai', 'Italian', 'Indian', 'Chinese', 'Mexican', 'Text_length'], axis=1, inplace=True)
All_tweet_data_v2.info()
temp_df2.sort_values(by='timestamp', ascending=True).head()
archive_df.info()
all_dates = df['Created Date'] # Get the date when each incident was created (reported) $ all_dates[0:5]
df_new[['US','UK','CA']] = pd.get_dummies(df_new['country']) $ df_new.drop('CA', axis = 1, inplace = True) $ df_new.head()
output= "Insert into user values('@Pratik','Demo Add User', 100000,10 , 9999)" $ cursor.execute(output)
yc.head()
n_new = df2[df2.landing_page == 'new_page']['user_id'].count() $ n_new
sns.barplot(k.index,k.percent_of_total)
hashed_train1.unpersist() $ hashed_dev1.unpersist()
aphid_columns = [ $     f'fields__oSets__oPoints__observations__a{n}__number' for n in range(1, 4)] $ aphid_names = ["English Grain", "Bird Cherry Oat", "Greenbug"] $ aphid_dict = dict(zip(aphid_columns, aphid_names))
fn_ss = 'neep_ashp_data.xlsx' $ dfh = pd.read_excel(f'data/heat-pump/raw/{fn_ss}', skiprows=6) $ dfh.head()
df.describe(include='all')
predictions = keras_entity_recognizer.predict(df_test) $ predictions[['text', 'label', 'prediction', 'confidences']][:5]
inspector = inspect(engine) $ inspector.get_table_names()
dfjoined.sort_values(['count_complaints_day', 'count_type_day'], ascending = [0,0], inplace = True)
autos.describe(include='all')
for col in var_cat: $     taxi_sample[col] = taxi_sample[col].astype(np.int64)
ldamodel_Tesla.print_topics(num_topics=3, num_words=7)
f, ax = plt.subplots(figsize=(15, 5)) $ sns.stripplot(data=samp311, x="complaint_type",y="agency", hue='borough',size = 10);
import pandas as pd $ import numpy as np $ rng = np.random.RandomState(42) $ ser = pd.Series(rng.randint(0, 10, 4)) $ ser
print('Last run:', datetime.datetime.utcnow(), 'UTC')  # timezone can't be detected from browser
usage_plot(trip_data, 'weekday', n_bins = 7)
lq.isnull().sum()
df_goog
df_archive_clean["source"].unique()
store.head()
df.head(2)
np.save(LM_PATH / 'tmp'/ 'tok_trn.npy', tok_trn) $ np.save(LM_PATH / 'tmp'/ 'tok_val.npy', tok_val)
df_geo_segments.head(3)
X = df3[["ab_page", "c2", "c3"]] $ df3['intercept'] = 1 $ logit_mod = sm.Logit(df3['converted'], X) $ results = logit_mod.fit() $ results.summary()
tip_sample.to_csv('text_prepartion/yelp_tips_prepared.csv')
cols_to_drop = ['in_reply_to_status_id','in_reply_to_user_id','retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'] $ twitter_archive_clean.drop(labels=cols_to_drop, axis=1, inplace=True) $ twitter_archive_clean.reset_index(inplace=True, drop=True)
data['len'].hist(bins=14)
m2 = m[:, 1:3]  #slicing the m2 to extract the second and third column $ print("m2: ", m2)
X=dataset.iloc[:,1].values
pp = pd.DataFrame(pp, columns=['dril_pp', 'laziestcanine_pp', 'ch000ch_pp'])
agg_post_sentiment.show()
training, test = train_test_split(review_df, test_size=0.2, random_state=233) $ print(len(training), "train +", len(test), "test")
ax.set_xlabel('Time') $ ax.set_ylabel('Price') $ ax.legend(loc='upper left') $ plt.show()
(df2.landing_page == "new_page").mean()
grades - grades.mean()  # equivalent to: grades - [7.75, 8.75, 7.50]
sns.countplot(x="deck", data=titanic, palette="Greens_d");
df['month'] = df.date.dt.month
df_sb.isDuplicated.value_counts() $ df_sb.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
%load_ext autoreload $ %autoreload 2 $ %matplotlib ipympl
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=10000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
del df['Timestamp']
X = data_sub["ratio"] $ y = data_sub["retweets"] $ model = sm.OLS(y, X).fit() $ predictions = model.predict(X) # make the predictions by the model $ model.summary() $
df_concat.head()
url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars' $ browser.visit(url)
data.plot(x='batch_start_time', y=['downstream_usage', 'expected_downstream_usage'], kind='line')
print(stock.head()) $ print(maxi.head())
df_small = df_small.drop(columns='variable')
backup = df2.copy()
answer = pd.read_csv('C:\\Users\\Parth\\Desktop\\Sem 3\\Project\\tweets1.csv',error_bad_lines= False)
obj = pd.Series(range(4), index=['d', 'a', 'b', 'c'])
data['patient']
print data_df.clean_desc[20] $ print 'becomes this ---' $ print text_list[20]
from datetime import datetime $ from datetime import timedelta $ lesson_date = datetime(2016, 3, 5, 23, 31, 1, 844089)
nold=df2[df2.landing_page=="old_page"].count()[0] $ nold
with open('train.csv') as f: $     size=len([0 for _ in f]) $     print("Records in train.csv => {}".format(size))
r = result_df.set_index(['metric_date', 'ticker'])
ndf = ndf.assign(Norm_summary = normalize_corpus(ndf.summary))
crime_data = crime_data.iloc[:, [5, 6, 8, 9, 10, 11, 15, 16]] $ crime_data.columns = ['code', 'description', 'disposition', 'location', 'patrol_zone', 'landmark', 'lat', 'long'] $ def set_semester(day_of_year): $
df2['user_id'].duplicated().sum()
target_google = target_google.sort_values('date').copy() $ target_google.head(2)
talks_train.reset_index(drop = True, inplace = True)
projects.head(3)
airbnb_od.strong_cutoff
df['Unique Key'] = df['OpenData Unique Key'] $ df['ComplaintID'] = df['Master SR #'] $ df['ProblemID'] = df['SR #'] $ df = df[['Unique Key', 'ComplaintID', 'ProblemID', 'Created Month']] $ df.head(2)
data_df.groupby('nwords')['ticket_id'].nunique()
logit_mod = sm.Logit(df_new.converted, df_new[['intercept', 'UK', 'US']]) $ results = logit_mod.fit() $ results.summary()
df['log_EBAY']=np.log(df['NASDAQ.EBAY']) $ Ebay_array=df["log_EBAY"].dropna().as_matrix() $ df['diff_log_EBAY']= df["log_EBAY"]-df["log_EBAY"].shift(periods=-1) $ model_Ebay = ARIMA(Ebay_array, (2,2,1)).fit() $ predEbay=model_Ebay.predict() $
full.groupby(['Age'])['<=30Days'].agg({'sum':'sum','count':'count','mean':'mean'})['mean'].plot(kind='bar',figsize=(12,4)) $ plt.title('Readmittance rate by Age',size=15) $ plt.ylabel('readmitted rate')
ridge = linear_model.Ridge(alpha=0.5) $ ridge.fit(X_17, y2) $ (ridge.coef_, ridge.intercept_)
trip = calc_temps('2017-08-23', '2017-08-27') $ trip
reddit2 = reddit_body["body"]
df.groupby("pickup_hour")["cancelled"].mean()
system = am.load('poscar', 'Al-fcc.poscar') $ print(system)
data_final = ben_out.append(van_out)
actual_diff = p_treatment-p_control $ plt.hist(p_diffs) $ plt.axvline(x=actual_diff, color='red') $ (p_diffs>actual_diff).mean()
%time train_4 = ss.fit_transform(train_4)
df.to_csv(save_dir)
pipe = pc.PipelineControl(data_path='examples/simple/data/varying_data.csv', $                           prediction_path='examples/simple/data/predictions.csv', $                           retraining_flag=False) $ pipe.runPipeline()
df[df.num_comments == max(trumpint.num_comments)]
testmx2 = testmx.toarray()
HOU_analysis = team_analysis.get_group("HOU").groupby("Category") # Pulls only team transactions from sample, then groups $
df1.io_state[2]
%%time $ intersections_irr = gpd.sjoin(geo_segments_all, geo_TempIrrs, how="inner", op='intersects')
full['Age'].plot(kind='box',figsize=(12,4),vert=False) $ plt.xlabel('yrs old') $ plt.title('Distribution of Age',size=15)
autos['price'] = autos.price.str.replace('$','').str.replace(',','').astype(float)
from sklearn.feature_extraction.text import CountVectorizer $ from sklearn.decomposition import LatentDirichletAllocation
(df_new.ab_page == df_new.new_page).mean()
sentences = MySentences('./data/') # a memory-friendly iterator $ print(list(sentences))
mta_avg = dfs_morning.groupby(by = 'STATION').ENTRIES_MORNING.mean().reset_index() $ mta_census = mta_avg.merge(census_zip, left_on=['STATION'], right_on=['station'], how='left') $ mta_census.drop('station', axis = 1, inplace = True)
lobbyFrame['Client'].value_counts()
against.sort_values('amount', ascending= False).head(10)
import pandas as pd $ import numpy as np
[(v.standard_name) for v in dsg.data_vars()]
len(df_licenses)
merged_NNN.info()
autos["date_crawled"].str[:10].value_counts(normalize=True, dropna=False).sort_index()        
tsne_vectors.head()
df = pd.read_html(str(table[1]), header=0, encoding='utf8')[0] $ df.head()
pickle.dump(lsa_tfidf, open('iteration1_files/epoch3/lsa_tfidf.pkl', 'wb'))
s.head()  # default to length 5
users_visits = pd.merge(users_visits, Relations, how='outer', on=['name', 'id_partner']) $ users_visits = users_visits[['visits', 'chanel']] $ users_visits.head()
y.head()
data["bd"].describe()
df_train.head(887)
train_texts[0]
df2_treatment = df2.query('group == "treatment"') $ df2_treatment['converted'].mean()
for res_key, df in data_sets.items(): $     df.to_pickle('fixed_'+res_key+'.pickle')
session = Session(engine) $ inspector = inspect(engine)
leadPerMonth.tail()
data.info()
df['timestamp'] = pd.Timestamp('20180101') $ df
rng = pd.date_range(end='2018-01-19', periods=200, freq='BM') $ type(rng)
df['price_log'] =  np.log10(df['price']) $ df['powerPS_log'] =  np.log10(df['powerPS'])                           
source = pd.crosstab(tweet_table.source_cleaned_2,tweet_table.lang) $ source
print df_t_best['Shipped At'].apply(lambda x: x.date()).min() $ print df_t_best['Shipped At'].apply(lambda x: x.date()).max()
df_numpy = df.value.as_matrix().reshape(-1,1) $ print(df_numpy[:10])
fig, axarr = plt.subplots(1,1, figsize =(12,8)) $ df_prod_wide.Production[['AG', 'AI,AR', 'BE,JU', 'BL,BS']].plot(ax=axarr, colormap=customcolors.deloitte_colormap()) $ axarr.set_ylabel('Energy produced per day [kWh/day]') $ axarr.set_xlabel('Time');
credentials_file = 'oauth.json' $ yfs2 = yfs_test(credentials_file)
cityID = '27485069891a7938' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         New_York.append(tweet) 
start = pd.datetime(2011, 1, 1)
sorted_precip = index_date_df.sort_values(by=['date']) $ sorted_precip.head()
df_master = df_master.dropna() $ df_master.shape
visual_df = pd.DataFrame.from_dict(win_dict, orient='index')
built_df = built_df.loc[built_df['category'] != ""][:20].reset_index() $ built_df
fld = 'Promo' $ df = df.sort_values(['Store', 'Date']) $ get_elapsed(fld, 'After') $ df = df.sort_values(['Store', 'Date'], ascending=[True, False]) $ get_elapsed(fld, 'Before')
data = pd.Series([0.25, 0.5, 0.75, 1.0]) $ data
print len(table1[table1['class']==0]) $ print len(table1[table1['class']==1])
def get_regression_predictions(input_feature, intercept, slope): $     predicted_values = input_feature * slope + intercept $     return predicted_values
musk = pd.read_csv('elonmusk_tweets.csv')
df.dtypes
net.build()
subcols = dropcol(subcols, dnas.keys()) $ print(len(subcols)) $ print(subcols)
run txt2pdf.py -o '2018-06-22 2011 FLORIDA HOSPITAL Sorted by payments.pdf'  '2018-06-22 2011 FLORIDA HOSPITAL Sorted by payments.txt' $
twitter_Archive.head()
%%time $ reg = xgb.XGBRegressor().fit(X_train, y_train)
df2[df2.duplicated(['user_id'], keep=False)].info()
with rio.open(local_edit) as src: $     data = src.read(indexes=1) $     pyplot.imshow(data)
df_goog.index.min(), df_goog.index.max()
df3['intercept'] = pd.Series(np.zeros(len(df3)), index=df3.index) $ df3['ab_page'] = pd.Series(np.zeros(len(df3)), index=df3.index)
dupli = df2[df2['user_id'].duplicated()==True] $
baseball1_df['ageAtDebut'] = baseball1_df['debut'] - baseball1_df['birthDayDate'] $ baseball1_df['ageAtFinal'] = baseball1_df['finalGame'] - baseball1_df['birthDayDate'] $ baseball1_df.head()
X=review_to_words(X)
stm = "select * from daily_price where ticker in %s and instrument_type='factor' and price_date >= '2016-01-02' and  price_date <='2017-01-05'" % str(tuple(_factors)) $ index_ts = pd.read_sql(stm, engine) $
import carto $ help(carto)
print(autos.info()) $ print(autos.head())
users_query = "SELECT id, login FROM users" $ users_df = get_lims_dataframe(users_query)
shows['release_month'] = shows['release_date'].dropna().apply(lambda x: x.strftime('%m')) $ shows['release_month'] = shows['release_month'].dropna().apply(lambda x: str(x)) $ shows['release_month'] = shows['release_month'].dropna().apply(lambda x: int(x))
print(df.keys())
team_attributes = pd.read_sql_query('select * from Team_Attributes', conn)  # don't forget to specify the connection $ print(team_attributes.shape) $ team_attributes.head()
states.index
gm_df.tail()
meanLat = datatest[['place_name','lat']].groupby('place_name').agg(np.mean) $ meanLon = datatest[['place_name','lon']].groupby('place_name').agg(np.mean) $
np.shape(rhum_us_full)
train['discussion_other'] = ((train.url.isnull()) & (~train.title.str.contains(' HN: '))).astype(int) $ train.groupby('discussion_other').popular.mean()
autos['registration_year'].describe()
baseball1_df = baseball_df[['playerID','birthYear','birthMonth','birthDay','debut','finalGame']].dropna() $ baseball1_df.reset_index(inplace = True) $ baseball1_df.tail() $
group_2 = df.groupby(['STATION', pd.Grouper(freq='D',key='DATETIME')]).sum().reset_index() $ print (len(group_2)) $ group_2.head()
df_test_2 = df2.merge(countries_df, left_on = 'user_id', right_on = 'user_id', how='inner') $ df_test_2.head()
dup_labels.intersection(dup_labels2)
raw_data.matched_targeting[:5].tolist()
s.dt.hour  # extract hour as integer
dule_files = [f for f in os.listdir(folder) if re.match('Dul.*.elec.*.csv', f)] $ dule_files
max_IMDB = scores.IMDB.max()
sns.factorplot('fuelType',data=autodf,kind='count')
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1 - p_new)])
nov.head(1)
Base = automap_base() $ Base.prepare(engine, reflect=True) $ Base.classes.keys()
df_countries[['CA', 'UK', 'US']] = pd.get_dummies(df_countries['country'])
twitter_archive_with_json = pd.merge(twitter_archive_clean, tweet_json_clean, how='left', on=['tweet_id']) $
df.max()
sns.residplot(x='ratio', y='retweets', data=data_sub, color='green') $ plt.title('Residuals of regression model')
X = df[['gdp_per_capita', 'life_expectancy', 'unemployment']] $ y = df['big_mac_price'] $ X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)
monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam', $                   'Eric Idle', 'Terry Jones', 'Michael Palin'])
normal,malicous = plots.proto_proportions() $ print(normal) $ print(malicous)
ideas['Time'] = pd.to_datetime(ideas['Time'],errors='coerce') $ stocks['Date'] = pd.to_datetime(stocks['Date'],errors='coerce')
mainstream_facts_metrics.to_csv('DATA/mainstream_content.csv.gz', compression='gzip')
ice = gcsfs.GCSFileSystem(project='inpt-forecasting') $ with ice.open('inpt-forecasting/Inpatient Census extract - PHSEDW 71118.csv') as ice_f: $   ice_df = pd.read_csv(ice_f)
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import seaborn as sns $ import datetime as dt
grades = pd.read_csv("files/grades.csv") $ grades.head()
df3.query('group == "treatment"').head()
ps.to_timestamp()
a=DataAPI.write.update_secs_industry_sw(industry="H_SWL1", trading_days=['2017-12-29'], override=False)
fcst_weekly =fcst_trips.resample('W').sum() #resample weekly $ fcst_trips['date']=fcst_trips.index #add date column for saving $ fcst_weekly['date']=fcst_weekly.index #add date column for saving
pclass1_survived = df_titanic.loc[df_titanic['pclass'] == 1, 'survived'] $ print(pclass1_survived.value_counts()) $ pclass1_survived.value_counts().plot(kind='pie', autopct='%.2f', fontsize=20, figsize=(6, 6))
complaints2016_geodf.crs = nyc_census_tracts.crs
read_table("adj_close_stock_data_yahoo_2005_2010.txt", sep="\s+", parse_dates=[[0,1,2]], na_values=["-"])
soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]['href'][9:]
new_page_converted=np.random.binomial(n_new,p_new)
pd.to_datetime(['04-01-2012 10:00'], dayfirst=True)
df.drop(df.query("group== 'control' and landing_page=='new_page'").index,inplace=True) $ df.drop(df.query("group== 'treatment' and landing_page=='old_page'").index,inplace=True) $ df.info()
%matplotlib inline $ import pandas as pd
df.groupby(['Gender'])['loan_status'].value_counts(normalize=True)
trump_df
num_of_converted = len(df2[df2.converted == 1]) $ float(num_of_converted)/len(df2)
species_count.to_excel('species_output.xlsx') $ print("Done Writing!")
plt.figure(figsize=(12,6)) $ sns.set(style='white') $ ax = sns.barplot(x='country', y='class',data=Fraud_Data)
numPurchP = train.groupby(by='Product_ID')['Purchase'].count().reset_index().rename(columns={'Purchase': 'NumPurchasesP'}) $ train = train.merge(numPurchP, on='Product_ID', how='left') $ test = test.merge(numPurchP, on= 'Product_ID', how='left')
plantlist.iloc[:,6:-1].head()
print(len(df_final))
meeting_status.head()
df['speaker'].value_counts()
print wikipedia.summary("Lost in translation (film)")
users.tail()
yxe_tweets['source'] = yxe_tweets.source.str.replace("\<a href\=\".+\"\s*rel\=\"nofollow\"\>", '') $ yxe_tweets['source'] = yxe_tweets.source.str.replace("\<\/a\>", '') $
df_archive_clean.sample(5)
dataframes = [b_cal, b_list, b_rev, s_cal, s_list, s_rev] $ x = 0 $ for frame in dataframes: $     print(frame.shape)
df['Margin'] = (df['State Bottle Retail'] - df['State Bottle Cost'])*df['Bottles Sold'] $ df['Price per Liter'] = df['Sale (Dollars)'] / df['Volume Sold (Liters)']
a.append([3,4]) $ a
clients['join_month'] = clients['joined'].dt.month $ clients['log_income'] = np.log(clients['income']) $ clients.head()
nba_df.info()
%timeit df1 + df2 + df3 +df4
model = smf.ols(formula='revenue ~ day_of_week', data=tmdb_movies) $ results = model.fit() $ print (results.summary())
col = pd.read_csv('NYPD_Motor_Vehicle_Collisions.csv', index_col = 0, low_memory = False)
my_gempro.get_dssp_annotations()
combined_df3=combined_df2[combined_df2['fin_year']==201718.0]
data['moving_avg'] = pd.rolling_mean(data['sentiment'], 100)
arima12= ARIMA(dta_713,[2,2,0],freq='Q').fit() $ arima12.summary()
from sklearn.model_selection import train_test_split $ from sklearn.linear_model import LogisticRegression $ from sklearn import linear_model $ from sklearn.metrics import accuracy_score
wsdlurl, location, variable, variable_name
print(pd.DataFrame(test_bow).head())
tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['text'].str.extract('(([0-9]*[.\\]*[0-9]+)\/(10))',expand = True)[1]=='...10']
df_ta_caps_user = pd.merge(left=df_test_attempt_wth_caps_name,right=df_user,on="user id",how ='inner') $ df_ta_caps_user = df_ta_caps_user.sort_values(["Capsule id","user id","date created"])
from IPython.core.magic import (register_line_magic, register_cell_magic, $                                 register_line_cell_magic) $ @register_cell_magic $ def sql(line, cell): $     return presto2df(cell)
new_page_converted = np.random.choice([0,1],size=n_new,p=[(1-p_new_null),p_new_null])
store_items = store_items.rename(index={'store 3': 'fun'}) $ store_items
df.columns # get the list of columns names
df = pd.read_csv('basic_statistics_single_column_data.csv')
df_restaurants.select('name','categories').limit(10).toPandas()
refinedData.query()
p_new = df2['converted'].mean(); $ p_new
load2017.columns = ['time', 'day-ahead', 'actual'] $ load2017.head()
df.xs(key='x', level=2)
plt.pie(total_fare, explode=explode, autopct="%1.1f%%", labels=labels, colors=colors, shadow=True, startangle=140) $ plt.show() $
tweet_data_df=pd.DataFrame(tweet_data) $ tweet_data_df.head()
plt.style.use('ggplot')
age1 = records4[(records4['Graduated'] == 'Yes') & (records4['Gender'] == 'Male')] $ age2 = records4[(records4['Graduated'] == 'Yes') & (records4['Gender'] == 'Female')] $ age3 = records4[(records4['Graduated'] == 'No') & (records4['Gender'] == 'Male')] $ age4 = records4[(records4['Graduated'] == 'No') & (records4['Gender'] == 'Female')]
def combine_text_columns(df): $
print("Converted users proportion are {}%".format((df['converted'].mean())*100))
meal_is_interactive_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/meal_is_interactive.csv')['Body'].read().decode('utf-8') $ meal_is_interactive = pd.read_csv(StringIO(meal_is_interactive_csv_string), header=0, delimiter='|')
relevant_data = chartio[[u'User Name', u'Invitee Name', u'Invitee Email', u'Event Type Name', $        u'Start Date & Time', u'End Date & Time', $        u'Event Created Date & Time', u'Canceled']] $ relevant_data
df.loc['20180101':'20180103', ['A','B']]
response.headers["content-type"]
reduced_trips_data['duration'].hist() # still this does not help in obserbving the distribution of duration
prediction.plot(figsize=(15,5), title='Probabilities')
df_everything_about_DRGs.head()
pca = PCA() $ a = pca.fit_transform(full_orig[['PastPCPVisits','Age','LOS']])
r = requests.get(url1) $ json_data = r.json()
autos['date_created'].str[:20].value_counts(normalize=True, dropna=False).sort_index() * 100
top10_topics_2 = top10_topics_1.sort_values(by='count_event_per_topic', ascending=False) $ top10_topics_2.head(10)
tweets_df = pd.DataFrame(formatted_retweets, columns=columns)
autos["registration_year"].value_counts(normalize=True).sort_index(ascending=True).head(10)
sns.set_style('whitegrid') $ sns.distplot(data_final['countCollaborators'], kde=False,color="red")#, bins=20) $
from sklearn.ensemble import RandomForestRegressor $ from rfpimp import *
df_likes_grouped = df_likes.groupby(['authorName_x','authorName_y']).count().reset_index()#.unstack('authorName_y') $ df_likes_grouped = df_likes_grouped.fillna(0) $ df_likes_grouped.head()
rsv = df_temp_diff_redL $ rsv = rsv.append(df_temp_diff_redM) $ rsv = rsv.append(df_temp_diff_redR) $ rsv.sort_index(inplace=True) $ rsv.sample(5)
import csv $ import pandas as pd
df_group_by.head(20)
autos["price"].head()
def hash_tag(text): $     return re.findall(r'(#[^\s]+)', text) $ def at_tag(text): $     return re.findall(r'(@[A-Za-z_]+)[^s]', text)
pn = df2.query("converted==1").count()[0]/len(df2) $ pn
userByCountry_dfMax =userByCountry_df.loc[userByCountry_df.groupby(["channel_title"])["views"].idxmax()] $ userByCountry_dfMax
new_reps.newDate[new_reps.Kasich.isnull()]
odds = (pd.read_csv('data/weekly_epl_odds.csv') $            .replace({ $                 'Man Utd': 'Man United', $                 'C Palace': 'Crystal Palace'}))
images_df.to_csv("images_aranjuez.csv", header=True, sep="|", index=False)
final_topbikes = final_topbikes.drop_duplicates()
data['rooms'] = data['rooms'].apply(lambda x: (float)((int)(x)))
pd.set_option('display.max_columns', 1844278)
gmap.draw("my_map.html")
volume_m.shape
from ssbio.pipeline.gempro import GEMPRO
print "Baseline probability of a show being cancelling in season 1 is: ", 901.0/2590
noaa_data.loc["2018-06-10 22:00:00":"2018-06-10",'PRECIPITATION']
print "Columns: ", df3.columns.values $ df3
df=actual_payments $ df.values.nbytes + df.index.nbytes + df.columns.nbytes
df_birth.shape
import spacy $ nlp = spacy.load("en")
saved_tweets=pd.read_csv("../output/clean_data_4_18_14_40.csv",sep=",",encoding="utf-8",engine="python")
subwaydf.iloc[28630:28636] #this high number seems to be because entries and exits messes up
df_msg[df_msg["subtype"].isnull()].to_csv("messages.csv") $
store_items = store_items.rename(columns = {'bikes': 'hats'}) $ store_items
snow.select("select count(distinct claim_number) from st_pharma_cohort1")
daily_returns.hist() $ plt.show()
a2.head()
criteria = s > 10 $ criteria.head()
googletrend.head()
tcga_target_gtex_expression.head()
logit_mod_new = sm.Logit(df_new['converted'], df_new[['intercept','US','UK']]) $ results_new = logit_mod_new.fit() $ results_new.summary()
my_gempro = GEMPRO(gem_name=PROJECT, root_dir=ROOT_DIR, gem_file_path=GEM_FILE, gem_file_type=GEM_FILE_TYPE, pdb_file_type=PDB_FILE_TYPE)
result = pd.concat([df1, df2]) # concatenate one dataframe on another along rows $ result
url = form_url(f'organizations/{org_id}/members', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
tweets_df.columns
gbm = GradientBoostingClassifier(max_depth = 6, n_estimators= 400, max_features = 0.3) $ gbm.fit(X, y) $ y_score = gbm.predict_proba(X)[:,1] $ metrics.roc_auc_score(y, y_score)   # 0.77
import os $ os.listdir(path_stop_word)
df2.any(axis=1)
joined_df.head()
browser.click_link_by_partial_href('photojournal.jpl.nasa.gov/jpeg')
new_page_converted = np.random.choice(a=[1,0], size=n_new, replace=True, p=[p_new, 1-p_new]) $
issue_types_num[:6]
lst = tab.find({'_id':{'$exists':True}}) $
idx = payments_total_yrs[ payments_total_yrs['disc_times_pay']<1000000].index.tolist() $ print('There are',len(idx),'sites with < $1,000,000 in Total Payments') $
sns.heatmap(data_numeric.corr().abs().round(2),annot=True)
adopted_cats.info()
%sql \ $ SELECT avg(heats) \ $ FROM twitter \ $ WHERE url IS null;
plt = sns.boxplot(data=df, x="race_desc", y="log_time_detained", hue="race_desc", dodge=False) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) $ plt.set_xticklabels(plt.get_xticklabels(), rotation=45);
obs_diff = treatment_converted - control_converted $ obs_diff
cur = conn.cursor() $ cur.execute('ALTER TABLE actor DROP COLUMN IF EXISTS middle_name;') $ df = pd.read_sql('SELECT * FROM actor', con=conn) $ df.head()
twitter_final.groupby('time_cat')['tweet_id'].count().reset_index(name="Count").sort_values(by='Count', ascending=False)
forest_model
pattern = re.compile(r'<a.*>([\w\s]+)</a>')
pokemon['Legendary'] = np.where(pokemon['Legendary'] == True, 1, 0) $
print('alpha=0.1:\n%s\n\nalpha=0.25\n%s' % (dd, dd2))
import MySQLdb
con = sqlite3.connect('db.sqlite') # open connection $ con.close() # close connection, important to prevent data losses
df2.head()
mean_of_threshold_precision = np.array(mean_of_threshold_precision) $ mean_of_threshold_precision $ for p in mean_of_threshold_precision: $     print(p) $
selectfromcols = X_train.columns[gs_from_model_under.best_estimator_.named_steps['frommodel'].get_support()] $ selectfrom_coef = gs_from_model_under.best_estimator_.named_steps['logreg'].coef_ $ pd.DataFrame(selectfrom_coef, columns=selectfromcols).T.sort_values(0, ascending=False).head(10)
cvec = CountVectorizer(stop_words='english') $ X  = pd.DataFrame(cvec.fit_transform(new_df.joined_text).todense(), $              columns=cvec.get_feature_names())
dfs.head()
run txt2pdf.py -o"2018-06-14 2148 UNIVERSITY HOSPITALS OF CLEVELAND Sorted by Discharges.pdf"  "2018-06-14 2148 UNIVERSITY HOSPITALS OF CLEVELAND Sorted by Discharges.txt"
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page' and converted == 1").shape[0] $ convert_new = df2.query("landing_page == 'new_page' and converted == 1").shape[0] $ n_old = df2.shape[0] - df2.query("landing_page == 'new_page'").shape[0] $ n_new = df2.query("landing_page == 'new_page'").shape[0] $
df2.converted.mean()
max_sharp = minimize(sharp_, W, args=(ER, COV, rf), method='SLSQP', bounds=b_, constraints=c_) $ if not max_sharp.success: $     print 'sharp_ not optimized: ', max_sharp.message $ Results = Results.join(pd.Series(max_sharp.x, name='max_sharp').round(4)) $
df4 = pd.DataFrame(q4_results,columns=['tobs']) $ df4.columns $
week23 = week22.rename(columns={161:'161'}) $ stocks = stocks.rename(columns={'Week 22':'Week 23','154':'161'}) $ week23 = pd.merge(stocks,week23,on=['161','Tickers']) $ week23.drop_duplicates(subset='Link',inplace=True)
df.info()
del df['fullgender2']
def developer(x): $     if 'Developer' in x: $         return 1 $     return 0 $ df_more['Developer'] = df_more['Title'].apply(developer)
train_df.columns[train_df.isnull().any()].tolist()
prec_us_full = prec_nc.variables['pr_wtr'][:, lat_li:lat_ui, lon_li:lon_ui]
tok_trn, trn_labels = get_all(df_trn, 1) $ tok_val, val_labels = get_all(df_val, 1)
pred.p3_dog.value_counts()
p_new =df2[df2['landing_page'] == "new_page"].converted.mean() $ p_new
import text_clean as tc $ test_clean_string = tc.clean_corpus(test_corpus) $ train_clean_string = tc.clean_corpus(train_corpus) $ print('>>> The first few words from cleaned test_clean_string is: {}'.format(test_clean_string[0][:100])) $ print('>>> The first few words from cleaned train_clean_string is: {}'.format(train_clean_string[0][:100]))
pold = pnew $ print(pold)
liberia_df_new_deaths = liberia_df[liberia_df['Variable'] == liberia_wanted_row_list[3]] $ liberia_df_new_deaths['Variable'] = DEFAULT_NAME_ROW_DESCRIPTION_NEW_DEATHS
with tb.open_file(filename='data/my_pytables_file.h5', mode='r') as f: $     print(f.root.dangling_link[...]) $     print(f.root.dangling_link.is_dangling())
tweets_clean.dog_class.value_counts()
geopy.distance.vincenty(start, end).miles
def ls_dataset(name,node): $     if isinstance(node, h5py.Dataset): $         print(node)
df_new.head() $ df_new['interaction_us_ab_page'] = df_new.US *df_new.ab_page $ df_new['interaction_ca_ab_page'] = df_new.CA *df_new.ab_page $ df_new.head() $
r,q,p = sm.tsa.acf(resid_713.values.squeeze(), qstat=True) $ data = np.c_[range(1,41), r[1:], q, p] $ table = pd.DataFrame(data, columns=['lag', "AC", "Q", "Prob(>Q)"]) $ print(table.set_index('lag'))
d_submission_data
today_UTC
sumvalues = youthUser3.cityName.value_counts() $ sumvalues.to_csv('lrng/sumvalues.csv') $ type(sumvalues)
from elasticsearch import Elasticsearch
suspects_with_25_1 = suspects_with_25_1.reset_index()
ac['Date Closed'].describe()
sentiment_freq = nltk.FreqDist(sentiment_count) $ sentiment_freq
df.head()
busiest_stn=stations_des[0][0] $ busiest_stn
yuniq = df['hotel_cluster'].unique() $ if list(range(len(yuniq)))==sorted(list(yuniq)): $     print('Hotel cluster classes span range of {0} to {1}'.format(min(yuniq), max(yuniq)))
sampled_contirbutors_human_df = non_blocking_df_save_or_load( $     rewrite_human_data(sampled_contirbutors_human_raw_df), $     "{0}/human_data_cleaned/sampled_contributors".format(fs_prefix)) 
df_protest.loc[2]
s1.tail(2)
column_created = given_fundamental_dict_create_columns(temp_dict)
df = pd.DataFrame(fb_vec.todense(), columns=cv.get_feature_names())
from scipy.stats import norm
liquor['State Bottle Retail'] = [s.replace("$","") for s in liquor['State Bottle Retail']] $ liquor['State Bottle Retail'] = [float(x) for x in liquor['State Bottle Retail']] $ liquor_state_retail = liquor['State Bottle Retail']
c_rate_null = (df2["converted"] == 1).mean() ### assumed that is  pnew  and  pold  are equal
df['converted'].mean()
malebydatenew  = malebydate[['Sex','Offense']].copy() $ malebydatenew.head(3)
pd.Series({'a' : 0., 'c' : 1., 'b' : 2.},index = ['a','b','c','d'])  # from Python Dict, index specified, no auto sort
data.complex_features
groupby_imputation = taxi_hourly_df.groupby([taxi_hourly_df.index.month, taxi_hourly_df.index.dayofweek, taxi_hourly_df.index.hour]).mean()
ttarc = pd.read_csv('twitter-archive-enhanced.csv') $ ttarc.shape $
legHouse = legHouse.set_index("TLO_id")
seaborn.countplot(company_vacancies.hour)
query_result1.spatial_reference
df_clean['tweet_id'].dtype
s_lyc_transcript_assemblies_dir = OUT_PATH + "/S_lyc_transcriptome_assembly" $ s_lyc_all_transcripts_concat = transcript_assemblies_dir + "/all_transcripts_concat.fasta"
def twitter_setup(): $     auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) #authenticate giving our consumer keys $     auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET) #give access token, can get data but has limit(7500?) $     api = tweepy.API(auth) $     return api
from sklearn import linear_model $ lm = linear_model.LogisticRegression() $ lm.fit(x_train, y_train > 0)
sagemaker.Session().delete_endpoint(gender_predictor.endpoint)
class MyOpener(FancyURLopener): $     version = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/601.2.7 (KHTML, like Gecko) Version/9.0.1 Safari/601.2.7' $ MyOpener.version
people_person.head()
file_name = todaysdate_for_file_name + 'Unique Providers.csv' $ print('Writing to:',file_name) $ df_unique_providers.to_csv(file_name)
predictions = client.deployments.score(scoring_url, scoring_data) $ print("Scoring result: " + str(predictions))
cig_data.dtypes
X = stock.iloc[925:-1].drop(['volatility', 'volume', 'high', 'low', 'close', 'open'], 1) $ y = stock.iloc[925:-1].volatility
stats = baseball[['h','X2b', 'X3b', 'hr']] $ diff = stats - stats.loc[89521] $ diff[:10]
stops_per_crime_per_month=stops_month_window["sum_stops"].Count/stops_month_window["sum_window_crimes"].Count
print("The largest change in one day is %3.3f."%(abs(df.High - df.Low).max()))
last_12_precip = session.query(Measurement.date, Measurement.prcp).\ $ filter(Measurement.date >= '2016-08-24').filter(Measurement.date <= '2017-08-23').order_by(Measurement.date).all()
shifts = pd.DataFrame(df.groupby('atbat_pk')['is_shift'].sum()).reset_index() $ shifts.loc[shifts.is_shift > 0, 'is_shift'] = 1
import json
lin2 = sm.OLS(df_new['converted'], df_new[['intercept','US','UK']]) $ result2 = lin2.fit()
date_cols = ['CRE_DATE', 'UPD_DATE', 'DATE_RESILIATION', 'DATE_DEBUT', 'DATE_FIN'] $ contract_history = pd.read_csv(data_repo + 'contract_history.csv', dtype={'NUM_CAMPAGNE': object}, parse_dates=date_cols, **import_params)
frame.head()
df2 = df2.drop_duplicates(['user_id'], keep='first') $ df2[df2.duplicated(['user_id'], keep=False)]['user_id']
bd.columns.name = "Data" $ bd
p_diffs_alt = new_converted_simulation - old_converted_simulation
kNN500x = KNeighborsClassifier(n_neighbors=500) $ kNN500x.fit(X_extra, y_extra)
user = 'XXX' $ password = 'XXX' $ host = 'localhost' $ dbname = 'mimic' $ schema = 'public, mimiciii_demo'
a.ndim  #Number of dimensions.
df_ab.user_id.nunique()
spice_list = ['salt', 'pepper', 'oregano', 'sage', 'parsley', 'rosemary', 'tarragon', 'thyme', 'paprika', 'cumin']
df.drop(columns=['year2','year3'])  # drop multiple columns
data = pd.Series([1, None, 3]) $ data
voters = pd.read_csv('data_raw_NOGIT/voters_district3.txt', sep='\t') $ households = pd.read_csv('data_raw_NOGIT/households_district3.txt', sep='\t') $ households_with_count = pd.read_csv('data_raw_NOGIT/AK_hhld_withVoterCounts.txt', sep='\t')
n_old = len(df2.query("group == 'control'")) $ print(n_old)
from google.cloud import bigtable $ client     = bigtable.Client.from_service_account_json(JSON_SERVICE_KEY,project=project_id, admin=True) $ instance   = client.instance(instance_id) $
len(train_df4)
train_set.to_csv(PROCESSED_DATA_DIR + '/train_set.csv', index=False) $ test_set.to_csv(PROCESSED_DATA_DIR + '/test_set.csv', index=False)
df.resample(rule='M')['Created Date'].count()
datetime.datetime.strptime(df_TempJams['timeStamp'][0],"%Y-%m-%d %H:%M:%S")
preds_train=preds_train.as_data_frame() $ h2o_train['visitors']=preds_train
c2 = file.to_dataframe(mode='pivot', aggfunc='size') $ c2.head()
rfc = RandomForestClassifier(n_estimators=1000, max_depth=100, max_features=2, n_jobs=-1) $ rfc.fit(X_train, np.ravel(y_train,order='C')) $ predictions = rfc.predict(X_test) $ accuracy_score(y_true = y_test, y_pred = predictions)
fb
table = pd.read_html("https://en.wikipedia.org/wiki/List_of_sandwiches", header=0)[0] $
pold=pnew $ pold
joined = join_df(df1, df2) $ len(joined[joined.Close.isnull()])
len(airlines)
temps_df.Missoula
df_2014.dropna(inplace=True) $ df_2014
df2 = df $ drop_rows = df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].index.values
maxEndDate = df_obs['endDate'].max() $ df_obs = df_obs[df_obs['endDate'] == maxEndDate] $ df_obs = df_obs.sort('barrelID') $ df_obs
columns = ['doggo', 'floofer', 'pupper', 'puppo'] $ archive_copy.drop(columns, axis=1, inplace=True)
ml.ConfusionMatrix.from_csv( $   input_csv='./evalme/predict_results_eval.csv', $   schema_file='./evalme/predict_results_schema.json' $ ).plot()
All_tweet_data_v2[['rating_denominator','rating_numerator']][All_tweet_data_v2.rating_denominator!=10]
display(Markdown(q3c_answer))
w_mat, b_mat = create_wbl(mcap_mat, p=c) $ coins_all = b_mat.sum().index $ coins_infund = coins_all[b_mat.sum() > 1] $ print("Coins that once has been in top 10 \n", coins_infund)
SCC_med_inc = income_T[['Median household income']] $ SCC_med_inc
strategy.df_trades().head()
pd.date_range(end='2012', periods=7*12, freq='M')
newdf.isnull().any()
st = dt.datetime(1982, 1, 1)
spread(y)
new_page_converted= np.random.binomial(1, p=p_new, size=n_new) $ new_page_converted
pd.crosstab(train.TYPE_BI, train.TYPE_UT)
af.length.sum() / 1e9
df.corr()['Order_Qty']
import statsmodels.api as sm $ df2['intercept'] = 1 $ df2[['ab_page', 'old_page']] = pd.get_dummies(df['landing_page']) $ df2.drop('old_page', axis=1, inplace=True) $ df2.head()
null_vals = np.random.normal(0, p_diffs.std(), 10000)
print ("Mean:%f STD:%f Min:%f Max:%f Median:%f" % (df.petal_length.mean(),df.petal_length.std(),df.petal_length.min(),df.petal_length.max(),df.petal_length.median()))
priceData = [] $ for i in range(0, len(page)): $     data = pd.DataFrame.from_dict(page.Contracts[i],  orient='index').transpose() $     priceData.append(data) $ priceData = pd.concat(priceData, axis=0, ignore_index=True)
between_my_posts = time_between(tweets[tweets['retweeted'] == 0]['created_at'])
print(ozzy.buddy.name) $ print(ozzy.buddy.age)
ab.user_id.nunique()
data.describe(include = 'all')
print("Number of Groups in Mobile ATT&CK") $ print(len(all_mobile['groups'])) $ df = all_mobile['groups'] $ df = json_normalize(df) $ df.reindex(['matrix', 'group', 'group_aliases', 'group_id', 'group_description'], axis=1)[0:5]
aqmdata.describe()
cars.query('mpg > 15')             # Simple query $
from IPython.core.display import HTML $ css = open('../static/style-table.css').read() + open('../static/style-notebook.css').read() $ HTML('<style>{}</style>'.format(css))
from numpy import median $ width, height = 18, 6 $ plt.figure(figsize=(width, height)) $ ax = sns.barplot(x="Memory", y="Wait", data=jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0) & (jobs.Memory % 4000 == 0)], estimator = median, ci= None)
!gsutil cp %Zs3_key% %Zgs_key% $ !gsutil cp %Zs3_key2% %Zgs_key2%
df_wm.head(2)
import time
df.groupby("cancelled")["local_rental"].value_counts()
df.loc[df.sex.isin(sex2NA),'sex']=np.nan $ df.loc[df.sex.isin(sex2m),'sex']='m' $ print(df.sex.loc[df.sex.isin(sex2NA)==True].count()) $ print(df.sex.loc[df.sex.isin(sex2m)==True].count())
df2[df2.group == 'control'].converted.mean() $
df['fileType'].value_counts()
n_new = len(df2.query("group == 'treatment'" )) $ n_new
cand_date_df = pd.read_pickle('data/pres_sorted_with_sponsors_and_party.pickle') $ cand_date_df.head()
import datetime
import seaborn as sns $ df['converted'].value_counts() $ sns.countplot(x='converted',data=df2, palette='hls');
grid_lat = np.arange(24, 50.0, 1) $ grid_lon = np.arange(-125.0, -66, 1) $ glons, glats = np.meshgrid(grid_lon, grid_lat)
yhat = DT.predict(X_test) $ yhat
lons,lats = f.variables['X'], f.variables['Y'] $ print(lons, lats)                 
df_cal['start_date'].dtypes
n_old = df2.query('group == "control"')['user_id'].nunique() $ n_old
def computeAge(birthDate): $     timeSinceBirth = pd.Timestamp(2018, 2, 20)-birthDate $     return int(timeSinceBirth.days/365) $ dfClean['age'] = dfClean['datetime.birthDate'].apply(computeAge)
c.execute('SELECT city FROM weather where cold_month = "January" LIMIT 2') $ print(c.fetchall())
rng.asi8.dtype
list(airbnb_10)
p_fnl = (abs_rate + thermal_leak) / (abs_rate + leak) $ p_fnl.get_pandas_dataframe()
sentiment_df.head()
results['GS Index'].plot.bar() $ plt.show()
df
num_of_dtmodel_pred.keys()
total_features = create_features(joined_df) $ churned = joined_df.groupby(['id'])['churned'].mean().to_frame() $ total_features = total_features.join(churned, how='left') $ print(total_features.shape) $ total_features.head()
df_enhanced = df_enhanced.drop(df_enhanced.columns[8:12], axis=1) $ df_enhanced.head()
L = Quantity(1e35, unit='erg/s') $ d = 8 * u.kpc $ flux = L / (4 * np.pi * d ** 2) $ print(flux.to('erg cm-2 s-1')) $ print(flux.to('W/m2')) $
blame.author = pd.Categorical(blame.author) $ blame.author = blame.author.str.replace(",", ";") $ blame.timestamp = pd.Categorical(blame.timestamp) $ blame.timestamp = pd.to_datetime(blame.timestamp) $ blame.info()
logit_countries2 = sm.Logit(df4['converted'], $                            df4[['ab_page', 'US', 'UK', 'intercept']]) $ result3 = logit_countries2.fit() $
head = pd.Timestamp('20151101') $ tail = pd.Timestamp('20151104') $ df = hp.get_data(sensortype='gas', head=head,tail=tail, diff=True, unit='kW') $ charts.plot(df, stock=True, show='inline')
logit_country = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']]) $ results_country = logit_country.fit()
sc = spark.sparkContext $ access_logs_raw = sc.textFile("data/apache.log")
crimes.head()
p_old = df2.converted.mean() $ p_old
df_fuel_type =  pd.get_dummies(df['fuelType']) $ df_gear_box =  pd.get_dummies(df['gearbox']) $ df_vehicleType =  pd.get_dummies(df['vehicleType']) $ df_ = pd.concat([df, df_fuel_type,df_gear_box,df_vehicleType], axis=1)
%load "solutions/sol_2_17.py"
print(train["review"][0]) $ print() $ print(example1.get_text())
hometeams = newdf.home_team.unique() $ awayteams = newdf.away_team.unique() $ allteams = set(hometeams) | set(awayteams) $ print (len(allteams))
new_page_converted = np.random.choice([0, 1], size=n_new, p=[(1 - convert_mean), convert_mean])
df.plot();
all_simband_data.sdid.nunique()
base_dict_by_place['count'] = base_dict_by_place['hashtags'].apply(lambda x: len(x))
grid.best_params_
df[['text', 'favorite_count', 'date']][df.favorite_count == np.min(df.favorite_count)]
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print(z_score, p_value)
tizibika.head()
%store -r keys_0430 $ keys = keys_0430.copy()
min_div_stock=df.iloc[df["Dividend Yield"].idxmin()] $ min_div_stock $ print("The stock with the minimum dividend yield is %s with yield %s" % (min_div_stock['Company Name'],min_div_stock['Dividend Yield']))
remove_dog
test = full_orig.set_index('AdmitDate')['2016'] $ X_test = test.drop(['Patient','Readmitted','DaysSinceAdmission','<=30Days'],axis=1)
prcp_12monthsDf.info()
train = pd.merge(train_data, date_info, how='left', on=['visit_date']) $ test = pd.merge(test_data, date_info, how='left', on=['visit_date']) 
df['weekend']= df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0) $ df.head()
df_master.info()
unique_vals = pd.Series(['c', 'b', 'a'])
df['id'] = df['id'].astype('category') $ df['sentiment'] = df['sentiment'].astype('category') $ df['text'] = df['text'].astype('string')
questions_scores = sorted(posts_df[posts_df["post_label"]=="Question"]["Score"])
goodreads_users_df.head(3)
results.summary()
crimes_by_type = pd.DataFrame(datAll.groupby(datAll['Offense Type']) $                               .agg({'Offense_count':'sum'}))
df.dtypes
search_fc = gis.content.search("title:AVL_Direct_FC", item_type='Feature Collection') $ iowa_fc_item = search_fc[0] $ iowa_fc_item
df_oncstage_dummies = pd.get_dummies(df_with_metac1['ONC_LATEST_STAGE'])
df
tweets['retweeted'].value_counts(dropna=False) $ tweets['favorited'].value_counts(dropna=False)
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=10000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
loans.columns
df_new[['UK', 'US', 'CA']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop('US', axis = 1) $ df_new.head() $
my_df.dropna(inplace=True) $ my_df.reset_index(drop=True,inplace=True) $ my_df.info()
cleanedData = allData $ cleanedData[cleanedData['text'].str.contains("\&amp;")].shape[0]
All_tweet_data_v2.date_time=All_tweet_data_v2.date_time.str.replace(' \+0000','') $ All_tweet_data_v2.date_time=pd.to_datetime(All_tweet_data_v2.date_time)
image_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars' $ image_browser.visit(image_url) $ html = image_browser.html
df_predictions = pd.DataFrame(data) $ for i, row in df_predictions.iterrows(): $     if row["Q0"]=="NO" : $         df_predictions.at[i,"Q1"] = "NULL" $ df_predictions
old_page_converted = np.random.choice([0, 1], size=n_old, p=[(1-mean_of_p), mean_of_p])
for ind, column in enumerate(reddit.columns): $     print(ind, column)
import pandas as pd $ import numpy as np $ import matplotlib as plt $ import seaborn as sns $ %matplotlib inline
print("_____________________________________________") $ media_user_overall_df = pd.DataFrame.from_dict(media_user_overal_results) $ media_user_overall_df.head() 
def strip_date(a_datetime): $     return datetime.strptime(a_datetime.strftime('%Y-%m-%d'), '%Y-%m-%d') $ tweet_counts_by_month.index = tweet_counts_by_month.index.map(strip_date)
test_post = test_post.replace('.','. ') $ print(len(sent_tokenize(test_post)), end='\n\n') # number of sentences $ for sent in sent_tokenize(test_post): $     print(sent, end='\n\n')
autos = autos.drop(["seller","offer_type"], axis=1)
model.wv.similarity('king', 'women')
twitter_archive[twitter_archive['tweet_id'].duplicated()]
df_city_restaurants.select('business_id', 'business_idn').take(5)
df2[['ab_page2', 'ab_page']] = pd.get_dummies(df2['group'])
autos["registration_year"].describe()
df.signup_app.value_counts()
sns.heatmap(prec_us)
testo.detect_language()
(actual_payments.iso_date==EOM_date.date()).sum()
len(pred_hacker_list)
import pandas as pd $ from functools import partial $ import matplotlib $ %matplotlib inline $ import pickle
def chunks(l, n): $
data["Postal Code"]=data["Postal Code"].astype(str) $ data.info()
vip_reason.to_csv('../data/vip_reason.csv')
len(vocab)
df = sql.createDataFrame(df,schema=schema)
LARGE_GRID.make_table_accuracy(raw_large_grid_df)
usersDf.hist(column=['favourites_count'],bins=50) $
df[df['Complaint Type'].str.contains('homeless', case=False)]['Complaint Type'].unique()
total_word_count = 0 $ for text in all_text: $     total_word_count += len(text.split()) $ print('There are {0} words total.'.format(total_word_count))
stories[['created_hour', 'score']].corr()
e_p_b_two.rename(columns = {0:"Count"},inplace=True)
for Quarter, Data in StockData.groupby('Date-Quarter'): $     print("Q{} mean prices".format(Quarter)) $     print("There are {} rows in this group of data".format(len(Data))) $     print(Data[StockNames].mean()) $     print()
s1 = pd.Series(['A1','A2','A3','A4']) $ s2 = pd.Series(['B1','B2','B3','B4']) $ s3 = pd.Series(['C1','C2','C3','C4']) $ df = pd.DataFrame({ 'A': s1, 'B': s2}) $ df
c.loc[c>0.7]
import graphlab $ import numpy as np $ from math import sqrt
to_be_predicted_Day4 = 50.98147755 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
new_data = data*4.3 $ np.savetxt('data/new_data.csv', new_data)
pd.Series(sales, index=pd.date_range(start='2018-01-01', periods=len(sales), freq='5T'))
authors = train.groupby('author').popular.agg(['count', 'mean']) $ authors.shape[0]
from sklearn.model_selection import train_test_split $ from sklearn.metrics import r2_score
x = [5,6,3,2,6] $ x.append(9) $ x.insert(1,200) $ x.remove(2) # remove by value $ del x[3] # remove by index $
df.info()
import pyspark.sql.functions as F $ df_business.createOrReplaceTempView("business") $ df_restaurants = sqlc.sql("select * from business where array_contains(categories, 'Restaurants')")
BTCprice = sns.regplot(x=final["BTC Price"], y=final['Crypto Positive'], fit_reg=False, color = 'g') $ BTCvol = sns.regplot(x=final["BTC Volume"], y=final['Crypto Positive'], fit_reg=False, color = 'g')
data_2017_12_14_iberia = data_2017_12_14.loc[data_2017_12_14.text_2.str.contains("iberia", case = False, na = False)]
nouns.most_common(10)  # most frequent nouns
trips.isnull().any()
df2['intercept'] = 1 $ df2[['ab_page', 'old']] = pd.get_dummies(df2['landing_page']) $ df2.drop('old', axis=1, inplace=True) $ df2.head()
from_sql
collection.list_items(source='Quandl')
pd.DataFrame(df.groupby(['Parcel Status Id','Parcel Status'])['Announced At'].count())
df = pd.read_csv("Z:00_ETL/CustomerBehaviour/rawdata2.txt", sep = "\t", encoding = "ISO-8859-1")
nnew = df2[df2['group'] == 'treatment'].shape[0] $ nnew
prec_df = pd.DataFrame(data = us_prec) $ prec_df.columns = ts.dt.date
loan_requests_indebtedness_web=loan_requests_indebtedness.merge(df2.reset_index(),on='id_loan_request')
my_df.head()
noHandReliableData = reliableData[reliableData['ID'] != 'Hand'] $ noHandReliableData['ID'].unique()
driver.find_element_by_xpath('//*[@id="middleContainer"]/ul[1]/li[3]/a').click()
df_sale_price=df_sale_price.loc[(df_sale_price.RegionName=='Santa Clara'), ] $ df_sale_price
alldata.loc[alldata['bridge_id'] == 'LMY-S-FOSL']
import pandas as pd $ url = "https://space-facts.com/mars/" $ tables = pd.read_html(url) $ tables
month_year_crimes = crimes.groupby(['year', 'month']).size() $ month_year_crimes
m = p_diffs.mean() $ m1 = p_diffs_new.mean() $ m1
c_df = c_df.dropna(axis=1,how='all') $ c_df.size
model_rf_20 = RandomForestClassifier(min_samples_leaf=20, random_state=42) $ model_rf_20.fit(x_train,y_train) $ print("Train: ", model_rf_20.score(x_train,y_train)*100) $ print("Test: ", model_rf_20.score(x_test,y_test)*100) $ print("Differnce between train and test: ", model_rf_20.score(x_train,y_train)*100-model_rf_20.score(x_test,y_test)*100)
cercanasAfuerteApacheEntre25Y50mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 25) & (cercanasAfuerteApache['surface_total_in_m2'] < 50)] $ cercanasAfuerteApacheEntre25Y50mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre25Y50mts.apply(descripcionDistancia2, axis = 1) $ cercanasAfuerteApacheEntre25Y50mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
df.hist(column = "funding_rounds", by = 'founded_year', figsize = (20,16),layout = (5,6))
train_df = effectiveness(train_df) $ print(train_df.head(5))
import re $ haystack = "Folder contains 01.mp3 and 12.wav (as well as 23.mp3)" $ needles = re.findall(r'\d{1,2}\.(?:mp3|wav)', haystack) $ print(needles)
log_mod_countries_2 = sm.Logit(df_new['converted'],df_new[['intercept','US','UK','ab_page']]) $ results_countries_2 = log_mod_countries_2.fit() $ results_countries_2.summary()
df_twitter_copy.loc[1445].text
df3['F'] = s1
ls_sparse_cols
autos["price"].value_counts().sort_index(ascending=False).head(20)
mindex = pd.MultiIndex.from_tuples(coords) $ mindex
logging.disabled = True
com_grp
print(advancedtrain.shape)
ab.shape[0]
S_lumpedTopmodel.basin_par.filename
from sklearn.model_selection import KFold $ cv = KFold(n_splits=100, random_state=None, shuffle=True) $ estimator = Ridge(alpha=7000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
unsegmented_users.groupby(['segment']).user_id.count()
fname = join(DATA_FOLDER, 'sim_index2/sim') $ index = similarities.Similarity(fname, tfidf[corpus], $                                 num_features=len(dictionary), num_best=200, $                                 chunksize=10*256, shardsize=10*32768) $ index.save(fname) $
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=16000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
iris.head()
df.iloc[:2]
conv_giv_trt = prob_conv_and_prob_trt/prob_trt $ conv_giv_trt
from attackcti import attack_client
from sklearn.mixture import GaussianMixture $ gmm = GaussianMixture(2)
points['India']
for model_name in nbsvm_models.keys(): $     valid_probs[model_name] = nbsvm_models[model_name].predict_proba(X_valid_cont_doc)
import requests 
df2['intercept']= 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment']
"unique user-id's in experiment data: ", len(df_experiment.user_id.unique())
df["Complaint Type"].value_counts().head(1)
pd.to_datetime('2010/11/12')
print('{0:55s}:{1:s}'.format('game','rating')) $ print('-'*70) $ for gamename,gameranking in zip(name,rating): $     print('{0:55s}:{1:s}'.format(gamename,gameranking)) $     print('-'*70)
human_genome_length = gdf.length.sum() $ print("The length of the human genome is {} bases long.".format(human_genome_length))
print 'Pecentage of total amount from donations with no location: ', 100*sum(df[(df.city == '') & (df.state == '') & (df.zipcode_initial == '')].amount)/sum(df.amount)
cityID = '01fbe706f872cb32' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Washington.append(tweet) 
results.summary()
!h5ls 'data/file1.h5'
lr_best = LogisticRegression(C = 4.641588, penalty = 'l2' $                                   , random_state = 42, n_jobs = -1) $ lr_best.fit(X_train, y_train) $ test_preds = lr_best.predict(X_test) $ display_f1(y_test, test_preds)
dtypes={'date':np.str,'store_nbr':np.int64,'transactions':np.int64} $ parse_dates=['date'] $ transactions = pd.read_csv('transactions.csv', dtype=dtypes,parse_dates=parse_dates) # opens the csv file $ print("Rows and columns:",transactions.shape) $ pd.DataFrame.head(transactions)
type(until_nye)
df = pd.read_csv("https://raw.githubusercontent.com/YingZhang1028/practicum/master/Data_DataMining/train_score.csv") $ train_df["description_score"] = df["description_score"] $ train_df["description_score"].ix[np.isnan(train_df["description_score"]) == True] = train_df["description_score"].mean()
rf_tfidf = Pipeline([ ('vect', tfidf) , $                       ('clf',  rf_clf)]) $
auditfile.groupby(['companyName', 'softwareDesc'])['amount'].sum().round(-2) # check of de trainsacties in balans zijn $
analyser = SentimentIntensityAnalyzer() $ df_en['polarity_vader'] = df_en['text'].apply(lambda tweet: analyser.polarity_scores(tweet)['compound'])
(df2['landing_page']=='new_page').mean()
df.info()
(df1.head())
top10_topics_list = top10_topics_2.head(10)['topic_id'].values $ top10_topics_list
df.shape, noloc_df.shape
p_new = df2[df2['landing_page']=='new_page']['converted'].mean() $ print("P of conversion of new page(p_new):", p_new)
df_clean['rating_numerator'].value_counts()
df_campaigns['Send Date'][0]
who_purchased.to_csv('../data/purchase.csv')
adf_check(dfs['Seasonal Difference'].dropna())
gender[unidentified]=sex_bin
df
model_data['adopted_user'] = [1 if x in final_dict else 0 for x in model_data.object_id] $ print(model_data)
print(deep_learning_tweet1.text)
df2 = df2.drop(1899)
from sklearn import svm $ clf = svm.SVC(gamma=0.001, C=100.)
frame2.columns
l = [i for i in dummy_features if i not in test.columns.tolist()] $ print('%d dummy_features in train not in test.'%len(l))
race_vars.head()
df = df[df["city"]=="Pittsburgh"] $ df.reset_index(drop=True, inplace=True) $ df.shape
times = pd.DatetimeIndex(df_en['date']) $ grouped = df_en.groupby([times.hour, times.minute])['text'].agg(['count'])
for p in mp2013: $     print("{0} {1}".format(p.start_time,p.end_time))
soup.body.div.a
df_users_test = df_users.iloc[:2, :] $ df_users_test.iloc[1, -1] = '2017-09-20' $ df_users_test
for i, image in enumerate([x_test[0], x_test[1]]): $     plt.subplot(2, 2, i + 1) $     plt.axis('off') $     plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
np.corrcoef(data['int_rate'], data['fico_range_low'])
session.query(func.count(Station.station)).all()
%matplotlib inline $ g=sns.lmplot(x='favorite_count', y='retweet_count', data=matrix, $            palette="muted", size=4,scatter_kws={"s": 10, "alpha": 1}) $ g.set_axis_labels("Favoriate Count", "Retweet Count")
ign.platform.value_counts()[:10].plot.pie(figsize=(10,10))
import pandas as pd $ df = pd.read_csv('/Users/davidswan/Downloads/Iowa_Liquor_sales_sample_10pct.csv') $ df["Date"] = pd.to_datetime(df["Date"], format = "%m/%d/%Y") $ df.head()
pd.read_sql('select * from users limit 5;', cnx)
sns.countplot(data=data, x='OutcomeType',hue='AnimalType', palette="Set3")
en_es.head()
def generate_returns(close): $     return None $ project_tests.test_generate_returns(generate_returns)
nold = df2.query('landing_page == "old_page"').shape[0] $ nold
d ={'A':10,'B':20,'C':5}
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
resultados = cross_val_predict(modelo, freq_tweets, classes, cv = 10)
session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all() $
test = datatest[datatest['rooms'].isnull()] $ train = datatest[datatest['rooms'].notnull()]
weblogs_partitioned = spark.read.load("weblogs-partitioned") $ weblogs_partitioned.printSchema()
new_page_converted=np.random.choice([0,1],n_new,p=[1-p_new,p_new]) $
data_spd['SA'] = np.array([ analize_sentiment(tweet) for tweet in data_spd['tweets'] ])
user_converted = df['converted'].mean() $ print('Proportion of user converted:{}%'.format(user_converted*100))
def convert_time (time): $     new_time = datetime.datetime.strptime(time, "%a %b %d %H:%M:%S %z %Y").timestamp() $     new_time = datetime.datetime.fromtimestamp(new_time).strftime('%Y-%m-%d %H:%M:%S') $     return(new_time)
first_commit_timestamp = git_log.iloc[-1].timestamp $ last_commit_timestamp = pd.to_datetime('now') $ corrected_log = git_log[(first_commit_timestamp <= git_log.timestamp) & (git_log.timestamp <= last_commit_timestamp)] $ corrected_log['timestamp'].describe()
col_names_full = ['Field1', 'Field2', 'Field3', 'Field4', 'Field5', $                   'Field6', 'Field7', 'Field8', 'Field9', 'Field10', $                   'Field11', 'Field12', 'Field13', 'file'] $ df_full.columns = col_names_full
revenue.map(lambda x: '%.2f' % x) # insert 2 decimal places for each element in the Series 
assert trump['text'].loc[884740553040175104] == 'working hard to get the olympics for the united states (l.a.). stay tuned!'
active = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).\ $                order_by(func.count(Measurement.tobs).desc()).all() $ print(active)
train['Age'] = train.apply(lambda row: int(row[6].split("-")[0]) - int(row[1].split("-")[0]), axis = 1) $ test['Age'] = test.apply(lambda row: int(row[6].split("-")[0]) - int(row[1].split("-")[0]), axis = 1)
exiftool -csv -createdate -modifydate MVI_0011.mp4 MVI_0012.mp4 > out.csv
train.age.describe()
len(df[~(df.group_properties == {})])
params_rfc = { $     'rfc__n_estimators': [10, 50, 100], $     'rfc__max_depth': [3, 6, 10], $     'rfc__min_samples_split': [2, 3, 4] $ }
data_2018 =data_2018[['lat', 'lon', 'tmin', 'month', 'day', 'year']]
from spacy import displacy
for res_key, df in data_sets.items(): $     logger.info(res_key + ': %s', df.shape)
p_new = df2['converted'].mean()
from pyspark.ml.feature import StringIndexer $ t1 = StringIndexer(inputCol="business_id", outputCol="business_idn").fit(df_city_restaurants) $ df_city_restaurants = t1.transform(df_city_restaurants)
df = df[df.userTimezone.notnull()] $ len(df)
%matplotlib inline $ import matplotlib.pyplot as plt $ import numpy as np
import pandas as pd $ import re $ import numpy as np $ import tweepy, requests, json, csv, sys
dfa=new_table.groupby("type")
df2['intercept'] = 1 $ df2[['control','ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop(['control',],axis=1) $
volume = 13 $ for issue in [1,2]: $     page_html = load_or_get(volume, issue) $     print  scrape_headlines(page_html)
taxi_hourly_df.loc[(taxi_hourly_df.missing_dt == True) & $                     (taxi_hourly_df.index.hour.isin((0, 1, 2, 3, 4, 5, 6))) & $                     (taxi_hourly_df.num_pickups.isnull()) & $                     (taxi_hourly_df.num_passengers.isnull()), : $                    ]
models = { $           'rf' :RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1, verbose = 2 ,max_depth=17,min_samples_leaf=17 ) , $           'et' :ExtraTreesClassifier(n_estimators=50, random_state=42, n_jobs=-1, verbose = 2 ,max_depth=17,min_samples_leaf=17 , max_features = 0.8) , $          }
lbl = preprocessing.LabelEncoder() $ lbl.fit(list(df_train['manager_id'].values)) $ df_train['manager_id'] = lbl.transform(list(df_train['manager_id'].values))
df[df.client_event_time < datetime.datetime(2018,4,1,23,0)][['client_event_time', 'client_upload_time', 'event_time', 'server_received_time']].sort_values('client_event_time').head()
converted = df2.query('group == "control" and converted == 1').user_id.count() $ total = df2.query('group == "control"').user_id.count() $ converted / total
lm=sm.Logit(df2['converted'], df2[['intercept','ab_page','Friday', 'Monday', 'Saturday', 'Thursday', 'Tuesday', $        'Wednesday']]) $ results=lm.fit() $ results.summary()
df.groupby("newsOutlet")["compound"].min()
df["main_cpv"] = df.cpvs.apply(getMainCPV) $ it_df = df[df.main_cpv.apply(filterIT)]
user_tweet_count_df.head()
intersections_irr["avg_traffic_flow"] = intersections_irr[normalized_columns].apply( $     lambda s: loaded_traffic_flow_prediction_model.predict([s])[0], axis=1)
len(orgs)
df4[['CA','UK','US']] = pd.get_dummies(df4['country']) $ df4 = df4.drop('CA', axis=1) $ df4.head()
cumFrame = pd.DataFrame(changes, columns=('Date', 'Client', 'Change')) $ cumFrame = cumFrame.sort_values('Date') $ print(cumFrame.loc[cumFrame['Client'] == 'AT&T']) $
df_old = df2[(df2['landing_page'] == 'old_page')] $ converted = df_old['converted'] $ old_page_converted = np.random.choice(converted, n_old)
year_prcp_df.plot() $ plt.show()
df2=df2.drop(1899)
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/car_data.txt" $ df = pd.read_table(path, sep ='\s+', na_values=['.']) $ df.head(5)
dul.drop_duplicates(inplace=True) $ dul
norm.ppf(1-(0.05/2))
df_goog.head()
del jobs   # Free up memory
(df.where(df['SUMLEV']==50) $     .dropna() $     .set_index(['STNAME','CTYNAME']) $     .rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'}))
evaluate(y_pred, test_labels_bin)
tfidf = models.TfidfModel(corpus) #just to train the tfidf model $ corpus_tfidf = tfidf[corpus] #now the corpus is represented as a tfidf matrix $ lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)#nitialize an LSI transformation $
unique_users_count = df.user_id.nunique() $ print("Number of unique users: {}".format(unique_users_count))
tipsDF = tipsDF.drop(tipsDF.columns[0], axis = 1)
def print_tweet(tweet): $     print "@%s - %s (%s)" % (tweet.user.screen_name, tweet.user.name, tweet.created_at) $     print tweet.text $ tweet=results[1] $ print_tweet(tweet)
import statsmodels.api as sm $ convert_old = df2.query('group == "control" and converted == 1')['user_id'].count() $ convert_new = df.query('group == "treatment" and converted == 1')['user_id'].count() $ n_old = df2.query('group == "control"')['user_id'].count() $ n_new = df.query('group == "treatment"')['user_id'].count()
twitter_archive_clean.drop(dog_stage_list, axis=1, inplace=True)
pd_aux=pd.DataFrame(building_pa_prc_shrink[['permit_number','permit_creation_date']][filt_M]) $ pd_aux.sort_values(by=['permit_creation_date'],inplace=True) $ pd_aux.head(20)
ma_8 = df['Close'].pct_change(n).rolling(window=8).mean() $ ma_13= df['Close'].pct_change(n).rolling(window=13).mean() $ ma_21= df['Close'].pct_change(n).rolling(window=21).mean() $ ma_34= df['Close'].pct_change(n).rolling(window=34).mean() $ ma_55= df['Close'].pct_change(n).rolling(window=55).mean()
graph.run("CREATE CONSTRAINT ON (u:User) ASSERT u.user_name IS UNIQUE;") $ graph.run("CREATE CONSTRAINT ON (t:Tweet) ASSERT t.tweet_id IS UNIQUE;") $ graph.run("CREATE CONSTRAINT ON (h:Hashtag) ASSERT h.tag IS UNIQUE;")
answers_scores = sorted(posts_df[(posts_df["post_label"]=="Answer")|(posts_df["post_label"]=="Accepted answer")]["Score"])
df_group_by.columns = df_group_by.columns.droplevel(1)
preds1 = model1.predict(test[col]) $ preds2 = model2.predict(test[col]) $ preds3 = model3.predict(test[col]) $ preds4 = model4.predict(test[col]) $ preds5 = lgbm.predict(test[col])
WHO_Region = np.array(df['WHO Region']) $ WHO_Region
userProfile = userGenreTable.transpose().dot(inputMovies['rating']) $ userProfile
db = db_connect.SpectrumDB()
Stockholm_data_final = pd.concat([Stockholm_data, pd.DataFrame(y_predict_bayes)], axis=1)
np.median(df2.ltv)
df2['nrOfPictures'].hist()
df2.query('converted == 1').user_id.nunique() / df2.user_id.nunique()
re_split_raw = re.findall(r'\w+|\S\w*', raw) $ print(re_split_raw[100:150])
s.pct_change()
type(df.date[0])
session.query(func.count('*')).select_from(User).scalar()
f1_score(Y_valid_lab, val_pred_svm, average='weighted', labels=np.unique(val_pred_svm))
newuser = len(df2.query("group == 'treatment'")) $ user=df2.shape[0] $ newuserprobability = newuser/user $ print(newuserprobability)
ac['Dispute Resolution Start'].describe()
image.info()
plt.plot(dataframe.groupby('quarter').daily_worker_count.mean()) $ plt.show()
fld_student = np.array(np.unique(d['If I could be any type of scientist when I grow up, I would want to study:'])) $ fld_student_all = d['If I could be any type of scientist when I grow up, I would want to study:']
condition = (us['cityOrState'].str.contains(r'[A-Z]{2}')) & (us['cityOrState'].str.len() != 2) & (us['cityOrState'].isna() == False) $ us.loc[condition]
np.exp(-0.0149), np.exp(-0.0099), np.exp(-0.0506)
xmlData['bedrooms'] = pd.to_numeric(xmlData['bedrooms'], errors = 'raise') $ xmlData['bathrooms'] = pd.to_numeric(xmlData['bathrooms'], errors = 'raise') $ xmlData['floors'] = pd.to_numeric(xmlData['floors'], errors = 'raise')
33**2*df.h0/1e3/df.Da
k
answers_df = pd.io.json.json_normalize(raw_annotations_df.answers.tolist()) $ answers_df.tail(5)
stations = session.query(Stations.name, Stations.station).all()
act_diff = prop_treat - prop_cntrl $ (p_diffs > act_diff).mean()
df["cancelled"] = ((df["current_state"] != "finished") & (df["current_state"] != "started")).astype(int)
from scipy import stats $ stats.describe(Profit)
cleanedData[cleanedData['text'].str.contains("&amp;")].text
aaa = pd.DataFrame(A) $ bbb = pd.DataFrame(X) $ aaa['labels'] = pd.Series(b) $ bbb['labels'] = pd.Series(y)
gs_lr_tfidf = GridSearchCV( lr_tfidf, param_grid, #sends each subset to a different core $                           scoring = 'accuracy', $                           cv = 5, verbose = 1, $                           n_jobs = -1) # n_jobs -1 uses all computer cores
np.exp(results_new.params)
df['yob'].idxmax(axis=1)
test_scores = run(q_agent_new, env, num_episodes=100, mode='test') $ print("[TEST] Completed {} episodes with avg. score = {}".format(len(test_scores), np.mean(test_scores))) $ _ = plot_scores(test_scores)
import os, sys
orders.head()
df3 = df2.copy()
df2 = df $ df2['Language'] = df2['comments'].apply(ensure_unicode) $ df2['Language'] = df2['comments'].apply(run_detect) $ df2.tail(100)
result.head() $
df2=pd.merge(df2,country, on=['user_id']) $ df2.head()
df2.drop_duplicates('user_id',inplace=True) $ df2.info()
abc = Grouping_Year_DRG_discharges_payments.groupby(['year','drg3']).get_group((2015,871)) $ abc.head()
col = ['msno','plan_list_price','payment_plan_days','actual_amount_paid','payment_method_id','transaction_date','membership_expire_date'] $ transactions = utils.read_multiple_csv('../../input/preprocessed_data/transactions', col) # 20,000,000 $
ac['Filing Date'].describe()
hm_clean.info()
predictions.select("label", "prediction", "probability").show() $
result.head()
lr_cv.best_params_
mainkey = ['C/A','UNIT','SCP','STATION'] $ ttTimeEntry.sort_values(mainkey + ['DT'], inplace = True, ascending = False)
df.sample(5)
submit = pd.read_csv('mloutput20160428_0512.csv') $ print(submit.shape) $ submit.tail()
new_df[features[12:24]]
train_holiday_oil_store_transaction_item_test_004 = train_holiday_oil_store_transaction_item_test_004.drop('family', 'class') $ train_holiday_oil_store_transaction_item_test_004.show()
sns.regplot(x = np.arange(-len(my_tweet_df[my_tweet_df["tweet_source"] == "CBS News"]), 0, 1),y=my_tweet_df[my_tweet_df["tweet_source"] == "CBS News"]["tweet_vader_score"],fit_reg=False,marker = "+",scatter_kws={"color":"skyblue","alpha":0.8,"s":100}) $ ax = plt.gca() $ ax.set_title("Sentiment Analysis (CBS News)",fontsize = 12) $ plt.savefig('Sentiment_CBS.png')
print(np.min(temp), np.mean(temp), np.max(temp))
def get_child_column_data(node: dict) -> list or None: $     child_data_nodes = get_child_data_nodes(node) $     if child_data_nodes: $         return [column[DATA] for column in child_data_nodes.values()] $     return []
df = df.loc[df['game_type'] == 'R',]
company_vacancies.count()
conversion_rate_all_pages = df2.query('converted == 1').shape[0] / df2.shape[0] $ print('The probability of an individual converting regardless of the page is {}'.format(conversion_rate_all_pages))
from IPython.display import display_html $ display_html(html_string, raw=True)
country_dummies = pd.get_dummies(df_new.country) $ df_new = df_new.join(country_dummies) $ df_new['intercept'] = 1 $ df_new.head()
hdb = pd.read_json(json.dumps(res_json['result']['records']))
cols_add = [ 'share', 'net', 'capital']
inCSV = dataDir+'input/new-york_new-york_points.csv' $ df = pd.read_csv(inCSV) $ df = df[['X','Y','osm_id']] $ df.columns = ['longitude','latitude','osm_id'] #just delcare col names here $ df.to_csv(dataDir+'processing/new-york_new-york_points.csv', index=False)
def split_by_idx(idxs, *a): $     mask = np.zeros(len(a[0]),dtype=bool) $     mask[np.array(idxs)] = True $     return [(o[mask],o[~mask]) for o in a]
df.loc[df['user_id'] == 63]
sns.set_style("darkgrid") $ sns.set_context(rc={"figure.figsize": (8,4)}) $ sns.barplot(weekday.index, weekday.values, palette="Blues").set_title("Rides per Weekday")
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old],alternative='larger') $ print(z_score) $ print(p_value)
index_to_remove = not_linedup.index
np.info(np.arange)
test_orders_prodfill=pd.merge(test_orders_prod3,prods, on='product_id') $ test_orders_prodfill.head()
file_name = 'NationalFile' $ downloadUrl ='https://geonames.usgs.gov/docs/stategaz/NationalFile.zip' $ ur.urlretrieve(downloadUrl, file_name+'.zip') $ subprocess.call(r'"C:\Program Files\7-Zip\7z.exe" x ' + 'NationalFile.zip' )
def format_sample(x): $     data = json.loads(x) $     data['timestamp'] = datetime.fromtimestamp(data['timestamp']).strftime('%Y/%m/%d %H:%M:%S') $     data['doc_id'] = data.pop('count') $     return (data['doc_id'], json.dumps(data))
df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df_downsampled, df_thermal, $                                                                 test_size=0.2, random_state=42)
dr_new = doctors[doctors['ReasonForVisitDescription'].str.contains('New')] $ dr_existing = doctors[~doctors['ReasonForVisitDescription'].str.contains('New')]
s3_key_builtup_merge
output = pd.DataFrame(data={"id":test["id"], "sentiment":result,})# "probs":result_prob[:,1]}) $ output.to_csv(os.path.join(outputs,'Word2Vec_AverageVectors.csv'), index=False, quoting=3)
store_items['new watches'] = store_items['watches'][1:] $ store_items
gender = records[records['Gender'].isnull()]['First_name'].apply(get_gender)
all_docs = get_all_docs(DATA_FOLDER)
n_old = len(df2[df2['landing_page'] == 'old_page']) $ print ("n_old is: {}".format(n_old))
df_carto = df.head(0) $ for elem in final_date_list: $     df_new = df[df['Timestamp +2'] == elem] $     df_carto = pd.concat([df_carto, df_new])
df2.info()
import statsmodels.api as sm $ z_score,p_value = sm.stats.proportions_ztest([convert_old,convert_new],[n_old,n_new],alternative='smaller') $ print('z_score= '+str(z_score)) $ print('p_value= '+str(p_value))
session = Session(engine)
tfidf_matrix = tfidf.fit_transform(df_videos['video_desc']) $ tfidf_matrix.toarray()
pings = ( $     pings_dataset $     .records(sc, sample=0.01) $ )
country_dummies = pd.get_dummies(df_new['country']) $ df_new = df_new.join(country_dummies) $ df_new.head()
np.exp(-0.0674), np.exp(0.0118), np.exp(0.0783), np.exp(0.0175), np.exp(0.0469)
cats_df.describe()
twitter_data_v2.tweet_id.unique().shape
with tb.open_file(filename='data/my_pytables_file.h5', mode='r') as f: $     print(f.root.dangling_link[...])
print(array.max()) $ print(array.min()) $ print(array.mean(axis=0))
from statsmodels.tsa.stattools import adfuller as ADF $ print() $ print(ADF(resid_701.values)[0:4]) $ print() $ print(ADF(resid_701.values)[4:7])
df_vow['Low'].unique()
pgh_311_data_merged['Category'].value_counts()
complete_techniques[0]
df = pd.read_csv('ab_data.csv') $ countries = pd.read_csv('countries.csv')
def on_base_percentage(df): $     return ( df.h + df.bb + df.hbp )/( df.ab + df.bb + df.hbp + df.sf + 1e-6) $ obp = on_base_percentage(baseball) # equivalent to obp = baseball.apply(on_base_percentage, axis=1) $ obp.sort_values(ascending=False).round(4)
autos["brand"] = autos["brand"].str.replace("sonstige_autos","other")
X_test.columns.values
smith.head(4)
week4 = week3.rename(columns={28:'28'}) $ stocks = stocks.rename(columns={'Week 3':'Week 4','21':'28'}) $ week4 = pd.merge(stocks,week4,on=['28','Tickers']) $ week4.drop_duplicates(subset='Link',inplace=True)
lyra_lightcurve_fullday = lc.LYRALightCurve.create('2011-06-07') $ lyra_lightcurve = lyra_lightcurve_fullday.truncate('2011-06-07 06:00', '2011-06-07 08:00')
df.airline.unique()
logistic_model3=sm.Logit(df4['converted'],df4[['intercept','UK','CA','ab_page','UK_page','CA_page']])
asf_agg_by_gender_df = non_blocking_df_save_or_load_csv( $     group_by_gender(cleaned_asf_people_human_df).repartition(1), $     "{0}/asf_people_cleaned_agg_by_gender_3c".format(fs_prefix))
search['prefetch'] = search['message'].apply(prefetch)
ax = mains.plot() $ h.steady_states['active average'].plot(style='o', ax = ax); $ plt.ylabel("Power (W)") $ plt.xlabel("Time"); $
df_clean3.nlargest(10, 'rating_denominator')[['text', 'rating_numerator', 'rating_denominator']]
df.to_pickle(pretrain_data_dir+'/pretrain_data_02.pkl')
extract_deduped.APPLICATION_DATE_short.max()
pd.set_option('display.max_colwidth', -1)
!python ~/neuroscience/utils/create_dataset.py ~/neuroscience/data/processed/dataset.csv
ad_group_performance.loc[:5]
import lxml.html $ import requests $ r = requests.get("http://music.naver.com/search/search.nhn?query="+keyword+"&x=0&y=0") $ _html = lxml.html.fromstring(r.text)
df.head()
customer_visitors.DateCol.dt.dayofweek
after['count'] = after.groupby('hashtags')['hashtags'].transform(pd.Series.value_counts) $ after.sort('count', ascending=False) $ after.hashtags.dropna().head()
df['converted'].mean()
contract_history.STS_CODE.value_counts()
USER_PLANS_df = BID_PLANS_df.copy(deep=False) $ USER_PLANS_df.head()
loans_act_arrears_xirr=cashflows_act_arrears_investor.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
import os $ os.chdir(dir) $ files = [f for f in os.listdir('.') if os.path.isfile(f)] $ files = [f for f in files if '.xls' in f]
(autos["ad_created"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         )
spacy_tok(review[0])
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], value=0, alternative='larger') $ z_score, p_value
result['goal_usd'] = result.goal * result.rate $ result['pledged_usd'] = result.pledged * result.rate
print (user.index.values.tolist()[:5])
df[df.index.month.isin([5,6,7])]['Complaint Type'].value_counts().head()
properati[['state_name','price_per_m2','price_usd_per_m2','price_aprox_usd']].groupby('state_name').agg(['mean','sum'])\ $         .sort_values(('price_usd_per_m2','sum'),ascending=False)
prices.info() $ prices[0:100] $ prices.loc[prices['symbol'] == 'XRP']
jobs_data4 = json_normalize(json_data4['page']) $ jobs_data4.head(5)
weather_all = pd.read_csv('data/weather_airports_24hr_snapshot.csv')
obs_diff = df2[df2.landing_page == 'new_page'].converted.mean() - df2[df2.landing_page == 'old_page'].converted.mean() $ obs_diff
(autos["last_seen"] $  .str[:10] $  .value_counts(normalize=True,dropna=False) $  .sort_index() $ )
blob.noun_phrases
plt.barh(bottom = range(len(customer_purchase_timing['Earliest Transaction'])), $          width = pd.to_datetime(customer_purchase_timing['Paid at'] - customer_purchase_timing['Earliest Transaction'].timestamp(), $         left = customer_purchase_timing['Earliest Transaction'].timestamp())
df['AgeVideo'][df['AgeVideo']<700].count()
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=24000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
archive_df = archive_df[archive_df.retweeted_status_id.isna() & $                         ~archive_df.expanded_urls.isna()] $ len(archive_df)
df_c = df.query('landing_page!= "old_page"')  $ df_cb = df_c.query('group == "control"') $ df_cb.nunique() $
len(zero_labels)
precip_data_null= precip_data_df[precip_data_df.isnull().any(axis=1)] $ precip_data_null.tail(3) $ precip_data_df = precip_data_df.dropna(how="any") $ precip_data_df = precip_data_df.sort_values("date") $ precip_data_df.info() $
for row_no, row in master_file.iloc[:, 28:171].iterrows(): $     for col_no, data in zip(count(), row): $         data = str(data).strip() $         if data[0] == '-':  # Data under limit of detection has format *-data*. *<data* is not used $             master_file.iloc[row_no, col_no + 28] = -float(data) / 2 $
mars_html = mars_html.replace('\n', '') $ mars_html
loan_requests1=extract_indebtedness(loan_requests)
client = MongoClient(config.get('Mongo', 'host'), int(config.get('Mongo', 'port')))
nba_df.describe()
consumer_token = "FSdktl3u8Q4xhmEHeC7tt4Q6P" $ consumer_secret = "pErt27djL4HCcBt0z9PU6w7zVFygfVsEP1Aqx2gQ70uYibqP7p" $ access_key = "1902120404-I1QEwvP7uLEBRJefMMPdPSf4uChDnsNvzO8hFcj" $ access_secret = "RZAp4AlaXoszBXqp8gkMJM1ed6TJzTBFCDS99248hV9i4"
testObjDocs.outDF.tail(10)  ## new records on the end ... still need to delete the bad ones
tmp_df.reset_index().to_file('geocoded_evictions_deidentified.shp')
dedups['notRepairedDamage'].fillna(value='not-declared', inplace=True) $ dedups['fuelType'].fillna(value='not-declared', inplace=True) $ dedups['gearbox'].fillna(value='not-declared', inplace=True) $ dedups['vehicleType'].fillna(value='not-declared', inplace=True) $ dedups['model'].fillna(value='not-declared', inplace=True)
simple_features = ['bathrooms','bedrooms','price', $                    'day_created','month_created','hour_created', $                    'number_of_features', 'desc_length','NTACode', 'num_photo', 'manager_id'] $ target = 'interest_level'
new_page_converted = np.random.choice([1,0],size = len(new),p = [convert,1-convert])
df2_trt = df2.query('group == "treatment"'); $ df2_trt['converted'].mean()
print("Number of Techniques in PRE-ATT&CK") $ print(len(all_pre['techniques'])) $ df = all_pre['techniques'] $ df = json_normalize(df) $ df.reindex(['matrix', 'tactic', 'technique', 'technique_id', 'detectable_by_common_defenses'], axis=1)[0:5]
n_new = df2[df2['group'] == 'treatment'].shape[0] $ n_new
df['new_date']=df['DATE'].apply(lambda x: datetime.fromtimestamp(x).strftime("%Y-%m-%d %H:%M:%S"))
components2
df.shape
stem_freq_dict = grouped.set_index('Word_stem')['Frequency'].to_dict()
userItems.registerTempTable("predictions") $ query = "select * from predictions order by prediction desc limit 5" $ sqlContext.sql(query).toPandas()
AAPL.iloc[:,0:4].plot.hist(bins=25)
podcast_descriptions
predictor = 'is_troll' $ pol_tweets[predictor] = 0 $ pol_users[predictor] = 0 $ troll_tweets[predictor] = 1 $ troll_users[predictor] = 1
def updateDict(name): $     nameAmtDict[name] = nameAmt(name)
a = np.random.randn(300, 3)  #Create a 300x3 matrix of standard normal variates. $ a.std(axis=0)  #or np.std(a, axis=0)
piotroski_univ_sets = soup.select('td[class^="number"]') $ piotroski_univ_sets
tweets_df.userLocation.value_counts()
merged.loc[merged['state/region'] == 'PR', 'state'] = 'Puerto Rico' $ merged.loc[merged['state/region'] == 'USA', 'state'] = 'United States' $ merged.isnull().any()
ts.tshift(-1,freq="H")
stock['volatility'] = stock.high - stock.low
tweets['hashtags'] = tweets['hashtags'].apply(lambda x: x.strip('[]').lower().replace("'",'').split(', ')) $ tweets['user_mentions'] = tweets['user_mentions'].apply(lambda x: x.strip('[]').lower().replace("'",'').split(', '))
CSV_FILE = "issues.csv"
american_ratings.head(3)
df2[df2['user_id'].duplicated(keep = False)] $
df1 = df.copy()
urls = get_list(URL.format(1))
stats = loans.groupby('client_id')['loan_amount'].agg(['mean', 'max', 'min']) $ stats.columns = ['mean_loan_amount', 'max_loan_amount', 'min_loan_amount'] $ stats.head()
p_diffs = [] $ for _ in range(10000): $     bs_new_page_c = np.random.binomial(1, p_new, n_new) $     bs_old_page_c = np.random.binomial(1, p_old, n_old) $     p_diffs.append(bs_new_page_c.mean()-bs_old_page_c.mean())
tweet_df = df_tweet.filter(['id_str','text','created_at','in_reply_to_status_id_str','in_reply_to_user_id_str','in_reply_to_screen_name','lang'],axis=1)
train_embedding=train_embedding.rename({'DETAILS3':"WordVec"}, axis=1)
filepath = 'data/Iris_Data.csv' $ data = pd.read_csv(filepath) $ print(data.iloc[:5])
run txt2pdf.py -o"2018-06-12-0815 MAYO CLINIC HOSPITAL ROCHESTER - 2013 Percentiles.pdf" "2018-06-12-0815 MAYO CLINIC HOSPITAL ROCHESTER - 2013 Percentiles.txt"
donors_c = donors.copy()
bad_iv = options_frame[np.isnan(options_frame['ImpliedVolatilityMid'])]
X.info()
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ WHERE twitter_day = 3 \ $ GROUP BY tag_text ORDER BY count DESC LIMIT 1;
autos.loc[autos["registration_year"]<1900, "registration_year" ]
c.execute('SELECT Count(*) FROM weather') $ print(c.fetchall())
sns.lmplot(x="Income", y="Age", data=training, x_estimator=np.mean, order=1)
df_tweet.head()
mean = mc_estimates.expanding().mean()
scores_df = pd.DataFrame(post_scores) $ scores_filename = "r_worldnews_scores_01.20.2017.a.csv" $ scores_df.to_csv("outputs/" + scores_filename)
engine = engine.connect() $ engine
movie2000rating.to_csv('..\\Output\\WordCloud2.csv')
base_url = 'http://www.americansocceranalysis.com/' $ response = requests.get(base_url)
df2[(df2.group == 'control')&(df2.converted == 1)].shape[0]/df2[df2.group == 'control'].shape[0] $
import pandas as pd
my_model_q8 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='label', useEntireData='True') $ my_model_q8.fit(X_train, y_train) $ y_pred = my_model_q8.predict(X_valid) $ accuracy = metrics.accuracy_score(y_valid, y_pred) $ print("Accuracy: " +  str(accuracy))
len(reason_for_visit['Name'].unique())
le_data_all = wb.download(indicator="SP.DYN.LE00.IN", $                          country = countries['iso2c'], $                          start='1980', $                          end='2012') $ le_data_all
hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=1, validation_data=(X_val, y_val), $                  callbacks=[RocAuc], verbose=1)
df_else=df[~df.BCLASS.isin(condoclass)] $ df = df_condo.append(df_else) $ df.shape
df_h1b_mv[df_h1b_mv.pw_1>700000][['pw_1','lca_case_wage_rate_from','lca_case_wage_rate_to']]
MICROSACC.plot_default(microsaccades,subtype="count/(6*20)")+ylab("microsaccaderate [1/s]")
image_array = to_array_variabel(image, (153,1)) $ format_array = to_array_variabel(review_format, (153,1)) $ fake_array = to_array_variabel(fake, (153,1)) $ X_tfidf_test = np.hstack((image_array, format_array, body.toarray(), title.toarray(), fake_array))
training_set = sentim_analyzer.apply_features(train['splitText'].tolist()) $ test_set = sentim_analyzer.apply_features(tweets['splitText'].tolist())
X_testfinal = X_testfinal.rename(columns={'fit': 'fit_feat'})
sp['day_ago_open'] = sp.Open.shift(periods = 1) $ sp['week_ago_open'] = sp.Open.shift(periods = 7)
predicted_probs_first_measure.hist(bins=50)
information_ratio[information_ratio > 45]
print 'Using strings instead of datetime objects for Date Series creation:' $ dates = ['2014-08-01', '2014-08-02'] $ ts = pd.Series(np.random.randn(2), dates)
train.dropna().shape
import datetime $ from dateutil.relativedelta import relativedelta $ from dateutil.parser import parse
print(train)
result['timestampCorrected'] = result['timestamp'] $ result.loc[result['timestamp']> 6e+17, 'timestampCorrected' ] = np.NaN
missing_info = list(data.columns[data.isnull().any()]) $ missing_info
gs_pca_over.score(X_train_pca, y_train_over)
mydata.plot(figsize =(15 ,6)) $ plt.show()
daily_cases.unstack().T.head()
def get_logout_hash(href): $     return href and re.compile('logouthash').search(href) $
twitter_archive_df_clean = twitter_archive_df.copy() $ tweets_df_clean = tweets_df.copy()
from __future__ import print_function $ import statsmodels.api as sm $ from statsmodels.tsa.arima_process import arma_generate_sample $ np.random.seed(12345) $
gatecount_station_line[gatecount_station_line.name == 'South Station'].head()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
predictions = pipeline.predict(fb_test.message)
results
users[users['friends_count'].str.contains(r'[A-Za-z]') == True]
print(autos[~autos["registration_year"].between(1900,2018)]["registration_year"].value_counts()) $
AFX_X_2017 = r.json()
gains = [0.3, 0.32, 0.25, 0.24, 0.19] $ phases = [-0.28, -0.38, -0.58, -0.88, -1.28]
segments=pd.read_csv("transit_segments.csv") $ segments.head()
Project.head()
num_songs = len(playlist['tracks']['items']) $ num_songs
information_ratio[['Manager_A', 'Manager_B']] * 1.5
gas_df = gas_df.set_index('MONTH') $ gas_df.head()
relationships[0]
cat_american = reviews_without_rare_words(cat_american,'reviews_token',rare_wrds)
df.to_csv('trump_lies.csv', index=False, encoding='utf-8')  
lr.fit(X_train, y_train)
before_filter = kickstarter.shape $ kickstarter = kickstarter[(kickstarter["state"] == "successful") | (kickstarter["state"] == "failed")] $ assert list(kickstarter["state"].unique()) == ["failed", "successful"] $ after_filter = kickstarter.shape $ print("Shape before: {0}; Shape After: {1}".format(before_filter, after_filter))
n_trees = 10 $ y_pred = model.named_steps['XGBClassifier'].predict(mapper.fit_transform(X_test), ntree_limit= n_trees)
p_conv = df2['converted'].mean() $ p_conv
print(donald[0])
print(list(sentences))
WholeDfNonNULL =WholeDf[WholeDf.Rating == WholeDf.Rating]
df2.query("group == 'control'")["converted"].mean()
df[df.Target == 5]
loss = 10 / np.linspace(1, 100, a.size) $ loss.shape
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old],alternative='larger') $ print(z_score, p_value)
likes.groupby(['year']).size().plot(kind='bar')
if (kaggle|sim): test = test.reset_index(drop=True)
data = pd.read_csv("datosSinNan.csv") $ test = pd.read_csv("datatestSinNan.csv")
needcol = ["e1_len","school_state_n","e2_len","price","teacher_number_of_previously_posted_projects", $           "hour","teacher_prefix_n","quantity","count_items","day","project_grade_category_n", $            "quantity_min","quantity_max","price_min","price_max","year","month"] $ corref = data[needcol].corr() $ corref[corref>0.5]
Xs = df_sample.values
e_p_b_one.TimeCreate = e_p_b_one.TimeCreate.apply(lambda x:x.date())
ks_particpants=kick_projects.groupby(['category','launched_year','launched_quarter','goal_cat_perc']).count() $ ks_particpants=ks_particpants[['name']] $ ks_particpants.reset_index(inplace=True)
pred.info()
dfSF.head()
df.num_comments = df.num_comments.astype(int)
history = model.fit(x_train, y_train, $                     batch_size=batch_size, $                     epochs=epochs, $                     verbose=1, $                     validation_split=0.1)
Z = np.dot(np.ones((5,3)), np.ones((3,2))) $ print(Z) $
df_twitter_archive_master.drop(['Unnamed: 0'],axis=1,inplace=True)
contribs.info()
year7 = driver.find_elements_by_class_name('yr-button')[6] $ year7.click()
team_groups.first()
top_brand_prices = dict() $ for brand in top_brands: $     top_brand_prices[brand] = autos.loc[autos['brand']==brand, 'price'].mean() $ top_brand_prices
actual_p_new = df2[df2['landing_page']=='new_page']['converted'].mean() $ actual_p_old = df2[df2['landing_page']=='old_page']['converted'].mean() $ p_diff = actual_p_new - actual_p_old $ greater_than_diff = [i for i in p_diffs if i > p_diff] $ print('Proportion greater than actual difference:', len(greater_than_diff)/len(p_diffs))
def get_contract_address(tx): $     receipt = eth_getTransactionReceipt(tx) $     return receipt['contractAddress']
autos["odometer_km"].shape $ autos["odometer_km"].describe() $ autos["odometer_km"].value_counts(sort=True).head(20) $ autos["odometer_km"].value_counts().sort_index(ascending=False).head(20) $
df_measurement.describe()
LARGE_GRID.display_fixations(raw_large_grid_df, option='offset',input_subject="VP1",input_block=1)
reg_mod_uk = sm.OLS(df_all['converted'], df_all[['intercept', 'UK']]) $ analysis_uk = reg_mod_uk.fit() $ analysis_uk.summary()
tweets['id_str'] = tweets['id_str'].str.decode('utf-8') $ tweets['full_text'] = tweets['full_text'].str.decode('utf-8') $ tweets['created_at'] = tweets['created_at'].str.decode('utf-8')
data = aapl.get_call_data(expiry=aapl.expiry_dates[4]) $ data.iloc[0:5:, 0:5]
X = acs_df.drop('homeval', axis=1).values $ y = acs_df['homeval'].values
for item in g: $     print(item)
countries_df = pd.read_csv('./countries.csv') $ df3 = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df3.head()
df.groupby('key')
autos.describe(include='all')
fps = DirFileMgr(dr_mod_id_str) $ fps.create_all_modeling_fps(dr_mod_id_str) $ fps.add_fp('pyLDAvis')
maxitems = 10 $ print "Edinburgh tweets retrieve testing" $ print '---------------------------------' $ for tweet in tweepy.Cursor(api.search, q="place:%s" % place_id_E).items(maxitems): $     print tweet.text
print('There are',len(list_of_py),'files that are','py','files') $ for k in range(len(list_of_py)): $     print(list_of_py[k]) $     shutil.copy(os.path.abspath(list_of_py[k]), holding_file_name) # target filename is /dst/dir/file.ext $
d_dates.head()
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline
df = pd.read_excel("../../data/stocks.xlsx") $ df.head()
test_df.head(1000)
pres_df['ad_length_tmp'] = pres_df['end_time'] - pres_df['start_time'] $ pres_df['ad_length_tmp'].head(10)
historicalPriceC = pdr.DataReader('C', 'yahoo', "2005-01-01", "2015-10-06")
vc.plot(kind='bar', x='day', y='count', figsize=(20,5), fontsize=25)
data["Visit Type"].value_counts()
DataSet.loc[(DataSet['userLocation']=="Melbourne")&(DataSet['userTimezone']=="Melbourne"),:]
payments_all_yrs.tail() $
df3 = df2.copy() $ df3['intercept'] = 1 $ df3[['remove','ab_page']] = pd.get_dummies(df['group']) $ df3.drop('remove', axis=1, inplace=True) $ df3.head()
movies.show(5)
df.converted.mean()*100
lm=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','country_UK','country_US']]) $ r=lm.fit()
df.doggo.value_counts()
from arcgis.features import FeatureLayer
!cat crossref-by-doi/*.json | jq -r .message.DOI | grep ',' | wc -l
archive_df[archive_df.name.str.islower()]
joblib.dump(tfidf_matrix, 'pickles/tfidf_matrix.pkl') $ joblib.dump(tfidf_vectorizer, 'pickles/tfidf_vectorizer.pkl') $ joblib.dump(km, 'pickles/km.pkl')
grouped_publications_by_author.tail(2)
dates_with_nulls=len(stores_tran_nulls['date'].unique()) $ all_dates=len(class_merged['date'].unique()) $ dates_with_nulls/all_dates
df.groupby(['landing_page', 'group']).size()
grouped_publications_by_author['countCollaborators'] = grouped_publications_by_author['authorCollaboratorIds'].apply(len)
print(df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False]) $ df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
len(reddit['title'].unique())
tstart = dt.datetime.now() $ ','.join([str(d) for d in duse]) + \ $
cityID = '55b4f9e5c516e0b6' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Orlando.append(tweet) 
df_predictions_clean['p1'] = df_predictions_clean['p1'].str.replace('_', ' ') $ df_predictions_clean['p2'] = df_predictions_clean['p2'].str.replace('_', ' ') $ df_predictions_clean['p3'] = df_predictions_clean['p3'].str.replace('_', ' ') $
ideas.dtypes  # Inspecting datatypes
csv_df['timestamp'].min()
from io import StringIO $ Z = np.genfromtxt(s, delimiter=",", dtype=np.int) $ print(Z)
len(df2.query('group == "control" and converted == 1')) / len(df2.query('group == "control"'))
smap = folium.Map(location=[site_dct['Latitude'], site_dct['Longitude']], tiles='CartoDB positron', zoom_start=16)
n_old = len(df2[df2['group'] == 'control']) $ print (n_old)
horror_readings['date'] = horror_readings.tracking_time.dt.date
sentiments['created_at'] =  pd.to_datetime(sentiments['created_at'])
autos["odometer_km"].describe()
w_change = w - w.shift(1) $ w_change[coins_infund].plot(legend=False) $ plt.show()
df_total = pd.merge(df2,df_countries, how="left", on="user_id") $ df_total.head()
np.exp(results.params)
%matplotlib inline $ import matplotlib.pyplot as plt, numpy as np
merged_data['Rain'] = merged_data['Precipitation_In '].apply(lambda x:1 if x != "0" else 0)
x.loc[:,"B":"C"]
import pandas as pd $ import numpy as np
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" & converted == 1').count()['user_id'] $ convert_new = df2.query('landing_page == "new_page" & converted == 1').count()['user_id'] $ n_old = df2[df2['landing_page'] == 'old_page'].count()['user_id'] $ n_new = df2[df2['landing_page'] == 'new_page'].count()['user_id']
train_bf.summary()
obs_diff = new_page_converted.mean() - old_page_converted.mean() $ print(obs_diff)
import pandas as pd $ pd_cat = pd.get_dummies(ibm_hr_cat.select("*").toPandas()) $ pd_cat.head(3)
train.drop(['Age','Flight Date', 'Booking Date', 'Name'], axis = 1, inplace = True) $ test.drop(['Age', 'Flight Date', 'Booking Date', 'Name'], axis = 1, inplace = True)
result_control_1.summary()
params = {'figure.figsize': [8, 8],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_decomposition(doc_duration, params=params, freq=31, title='Doctors Decomposition')
df_ct.to_csv("can_tire_senti_score.csv", encoding='utf-8', index=False)
df.info()
KKK = str(time.strftime("%m-%d-%y")) + "-Output.csv" $ df.to_csv("output/" + KKK, encoding="utf-8")
del df_t['Matching Shipping Method id'] $ del df_t['Matching Parcel status'] $ del df_t['Updated Created diff'] $ del df_t['Shipped Created diff']
logrecnos
df.head()
df_clean3['dog_name'] = df_clean3['name'] $ df_clean3 = df_clean3.drop('name', axis=1) $ df_clean3.head()
df = pd.read_sql_query('show tables', engine)
assert len(combined_df) == len(session_df) == len(classifier_df)
train.isnull().sum()
df_RSV = df_full[df_full.Field4 == "RSV"]
import sklearn.ensemble $ bdt = sklearn.ensemble.GradientBoostingRegressor(n_estimators=200, max_depth=5) $ bdt.fit(train[features], train[target].values.ravel()) $ p = bdt.predict(test[features]) $ print("Loss", np.mean((p - test[target])**2) / baseline_loss)
df_age = pd.concat([X_age, y_age], axis=1)
total = df2[df2['group']=='treatment']['user_id'].unique().shape[0] $ conv = df2[((df2['converted']==1) & (df2['group']=='treatment'))]['user_id'].unique().shape[0] $ probab = conv/total $ print(probab)
import pandas as pd $ from datetime import datetime, timedelta
displaySentimentPieChart(data)
len(twitter_archive_df_clean[pd.isnull(twitter_archive_df_clean['expanded_urls'])])
archive_clean=pd.merge(archive_clean, tweet_json_clean, left_on='tweet_id', right_on='id')
asf_agg_by_gender_df.count()
from scipy.spatial.distance import pdist, cdist, squareform
org_counts = org_name_counter(df) $ org_counts[(org_counts >= 2) & (org_counts < 3)]
injuries_hour.head()
data.head(10)
X_test_tfidf = vectorizer.transform(X_test) $ y_pred_train = svr.predict(X_train_tfidf) $ y_pred_test = svr.predict(X_test_tfidf)
print('twitter_goodreads_users_df: ', len(twitter_goodreads_users_df))
import urllib $ from bs4 import BeautifulSoup as bs $ import requests $ import lxml.html $ import getpass
es_rdd.take(5)
yhat_knn = neigh.predict(X_test_knn) $ yhat_knn[0:5]
all_df.dtypes
df1=pd.DataFrame(index=pddata.index) $ df1=df1.join(pddata)
ind * (BATCH_SIZE // len(GPU_CORE)),(ind + 1) * (BATCH_SIZE // len(GPU_CORE))
dfTemp = getDataFiltering(df_train) $ dfTemp.shape[0]
rain_score = session.query(Measurement.prcp, Measurement.date)\ $                .filter(Measurement.date > past_year).\ $                 order_by(Measurement.date).all()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ z_score, p_value
df = pd.read_csv(r'C:\Users\Patrik\Downloads\webrobots.iokickstarter-datasets\Kickstarter_2015-11-01T14_09_04_557Z\Kickstarter007.csv') $ df.head()
print(automl.leader)
data[0:2]
data3.to_csv('heating.csv')
dry_ndvi = dry_vals(all_ndvi_sorted)
df_numbers['Number**2'] = df_numbers['Number'].map(lambda inset: inset**2) $ df_numbers
ratio_df
df2=df $ print(id(df),sep='\n') $ print(id(df2),sep='\n')
data = pd.read_csv('FremontBridge.csv', index_col='Date', parse_dates=True) $ data.head()
autos = autos[(autos['registration_year'] >= 1900) & (autos['registration_year'] <= 2017)]
data.drop(['ceil_15min_x','ceil_15min_y'], axis=1,inplace=True)
(autos['date_crawled'] $ .value_counts(normalize=True,dropna=False) $ .sort_index(ascending=True))
forest = RandomForestClassifier(max_depth=10, n_estimators=5) $ lr=LogisticRegression()
eval_sa_and_plot('entertainment',1,'body')
countries_df = pd.read_csv('countries.csv') $ countries_df.head()
df.head(5)
learner.metrics = [accuracy] $ learner.unfreeze()
sf_small_grouped = sf_small.groupby('start_date').agg({'duration':'size','Rain':'mean'})
df_enhanced.groupby('name')['name'].size()
test_results = lrModel.evaluate(test_data) $ test_results.rootMeanSquaredError
scipy.stats.kruskal(df2["tripduration"], df3["tripduration"])
os.chdir(root_dir + "data/") $ filtered_df_fda_drugs.to_csv("filtered_fda_drug_reports.csv", index=False)
pgh_311_data = pd.read_csv("311.csv", $                            index_col="CREATED_ON", $                            parse_dates=True) $ pgh_311_data.head()
(df2.query('group == "control"')['converted'] == 1).mean()
!wc -l < /home/denton/Downloads/2013_311_Service_Requests.csv # Number of lines in dataset
squares.head(2)
extract_all.loc[(extract_all.application_month=='2018-03') $                 &(extract_all.app_branch=='NV0848') $                 &(extract_all.APP_PRODUCT_TYPE=='PL') $                &(extract_all.approved_ind==1),['APPLICATION_DATE_short','DEC_LOAN_AMOUNT1']]
pd.read_csv("Data/microbiome.csv", nrows=4)
for v in contin_vars: $     joined[v] = joined[v].astype('float32') $     joined_test[v] = joined_test[v].astype('float32')
print(train["comment_text"][0]) $ print(example1.get_text()) $
cv_auc_rf = cross_val_score(model_rf, features, labels, cv=10, scoring='roc_auc') $ cv_auc_rf $
df_vow = pd.read_csv('./datasets/vow.csv')
CLIENT_ID = 'f5ee75104c694db8b49b46424b04e205' $ CLIENT_SECRET = 'F1DDE71D081844F799D83A12D8BC3C33' $ USER_NAME = 'org_data_analyst@blastmotion.com' $ PASSWORD = '#47sT>@Aa9'
coeffdf.tail(10)
pca.components_
data['new_claps'] = buckets $ data.head()
f_yt_resolved = '/scratch/olympus/projects/ideology_scaling/congress/youtube_links_resolved.tsv' $ df_yt_resolved.to_csv(f_yt_resolved, index=False, sep='\t') $ shutil.chown(f_yt_resolved, group='smapp')
sites_in_region= (df_providers.groupby(['drg','year'])[['disc_times_pay']].sum()) $     payments_by_DRG_by_YEAR = payments_by_DRG_by_YEAR.reset_index() $     payments_by_DRG_by_YEAR = payments_by_DRG_by_YEAR.drop(payments_by_DRG_by_YEAR.index[0]) $     payments_by_DRG_by_YEAR.to_csv(str(today) + ' Payments by DRG by YEAR.csv',index=False) $
combined.isnull().sum()
highRate = movies['Your Rating'] >= 4.0
encoder_model_loaded = load_model('encoder_model_inference.h5')
no3Mask = nitrogen['DetectionQuantitationLimitMeasure/MeasureUnitCode'] == 'mg/l NO3' $ nitrogen.loc[no3Mask,'TotalN'] = nitrogen['TotalN'] * 0.2259
d = docx.Document(downloadIfNeeded(example_docx, example_docx_save, mode = 'rb')) $ for paragraph in d.paragraphs[:7]: $     print(paragraph.text)
yhat = loanlr.predict(X_test) $ yhat
df['time'].hist(bins=100) $ plt.xlabel('Time (hours)') $ plt.ylabel('# of complaints')
geo_collect = collect.set_custom_filter(is_getlocated)
train.info()
import os $ import sys $ module_path = os.path.abspath(os.path.join('..')) $ if module_path not in sys.path: $     sys.path.append(module_path) $
grinter_day0 = inter_day0.groupby(['user_id','item_id']).size().reset_index() $ grinter_day0.columns = ['user_id','item_id','number_int'] $ grinter_day0.shape
tweets.isnull().sum()
bitcoin = df[df['tweet'].str.contains("bitcoin") | df['tweet'].str.contains("btc") | df['tweet'].str.contains("BTC")] $ bitcoin = bitcoin.reset_index(drop=True) $ bitcoin.info()
s_mean_df["date"] = pd.to_datetime(s_mean_df["date"], format='%Y-%m-%d') $ s_mean_df.info()
page.exists()
df = pd.read_csv(io.StringIO(response_clean), $                  skiprows=rowsToSkip,     #Skip metadta and data spec lines $                  delimiter='\t',          #Set to tab delimited $                  dtype={'site_no':'str'}) #Set site_no to a string datatype
sns.set() $ sns.distplot(np.log(df_input_clean.toPandas().Resp_time), kde=True, color='b')
df2[df2.duplicated(subset='user_id')]
from src.pipeline import pipeline_json $ pj = pipeline_json(response.content) $ X = pj.convert_to_df(scaling=True, filtered=True)
clicking_conditions.loc[clicking_conditions.user_id == '379881d5-32d7-49f4-bf5b-81fefbc5fcce']
df, y, nas = proc_df(df_raw, dep_col)
details.drop(['budget', 'original_language', 'overview', 'production_companies', 'production_countries', 'spoken_languages', 'status', 'tagline', $               'production_companies_number', 'production_countries_number', 'revenue', 'spoken_languages_number', 'imdb_id', $               'original_title' $              ], axis=1, inplace=True)
cachedf = dir2df(cachedir, fnpat='\.ifc$', addcols=['barename']) $ cachedf
print(samples_query.private_attributes)
interestlevel_df = train_df.groupby('interest_level')['listing_id'].count().reset_index().copy() $ interestlevel_df.columns = ['interest_level','count_of_listings'] $ interestlevel_df = interestlevel_df.sort_values(by='count_of_listings', ascending=False) $ interestlevel_df.head()
data.groupby('Agency').size().sort_values(ascending=False).plot(kind='bar', figsize=(20,4))
yhat = neigh.predict(X_test) $ yhat[0:5]
y_pred = m.predict(X_valid) $ cnf_matrix = metrics.confusion_matrix(y_valid, y_pred) $ cnf_matrix
df['text_no_urls_names'][55]
SeriesPandas1 = pd.Series([1,'a',2,3,np.nan,5,6,7]) $ SeriesPandas1
twitter_df_clean.groupby('rating_numerator')['rating_numerator'].count().plot()
for df in (joined, joined_test): $     for c in df.columns: $         if c.endswith('_y'): $             if c in df.columns: df.drop(c, inplace=True, axis=1)
s_t = np.sqrt(((n1-1)*n1*sd1+(n3-1)*n3*sd3)/(n1+n3-2)) $ t = (m3-m1)/(s_t*np.sqrt(1/n1+1/n3)) $ tscore = stats.t.ppf(.95,n1+n3-2) $ print("t stats is {0}; 95% t score is {1}".format(t,tscore))
df2 = df.drop(['year'], axis=1) $ df2.corr()
type(df_concat_2.date_series) $ df_concat_2["time"] = df_concat_2.date_series.dt.time.astype(str) $ df_concat_2["time"].head()
data.shape
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin' $ S.executable = excutable +'/summa.exe'
ylabel = 4 $ ylabel
df2['account_created'][0].month
lst_year_arrive = trip_arrive - last_year $ lst_year_leave = trip_leave - last_year $ print(lst_year_arrive) $ print(lst_year_leave) $
np_diff = np.diff(close) $ max(np_diff) $
test_data = load_df('data/test.csv')
word = 'supercalifragilisticexpialidocious' $ print(re.findall(r'[aeiou]', word))
df.iloc[:4]
pd.Series(index = feats_used, data = rf.feature_importances_).sort_values().plot(kind = 'bar')
from statsmodels.stats.diagnostic import acorr_ljungbox $
df['Precipitation'].unique()
display(data.head(10))
df.head()
data.drop(outliers.index, inplace=True)
np.var(p_diffs)
if 0 == 1: $     news_titles_sr.to_pickle(news_period_title_docs_pkl)
demoji = emoji.demojize(text, delimiters=('__','__')) $ print(demoji)
fed_reg_dataframe['token_text'] = fed_reg_dataframe['str_text'].apply(lambda x: word_tokenize(x.lower()))
a = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=yyWt4QfGTATLc5Ryjvnd&start_date=2017-01-01&end_date=2017-12-31')
train_size = int(train_.shape[0]*0.8) $ sentiment_train_tweets = [(tweet, sentiment) for tweet, sentiment in train_[['tweetText', 'polarity_value']].values[:train_size]] $ sentiment_train_tweets_full = [(tweet, sentiment) for tweet, sentiment in train_[['tweetText', 'polarity_value']].values] $ sentiment_validation_tweets = [(tweet, sentiment) for tweet, sentiment in train_[['tweetText', 'polarity_value']].values[train_size:]] $ sentiment_test_tweets  = [(tweet, sentiment) for tweet, sentiment in test_[['tweetText', 'polarity_value']].values]
t0.shape, t1.shape
probability_true[0:10]
def var(scenarios, level=99, neutral_scenario=0): $     pnls = scenarios - neutral_scenario $     return - np.percentile(pnls, 100-level, interpolation='linear')
mean_price_serie = pd.Series(mean_price_by_brand) $ mean_mileage_serie = pd.Series(mean_mileage_by_brand) $ mean_df = pd.DataFrame(mean_price_serie, columns=['mean_price']) $ mean_df["mean_mileage"] = mean_mileage_serie $ mean_df
%time sf_json = odm2rest_request('samplingfeatures', sf_params)
df2.head(2)
str(df.columns)
sum(contractor.state_id.isnull()) #the count of missing state_id value is 0 $ contractor.state_id.value_counts() #The state_id columns do not have missing data
print(norm.cdf(z_score))
df = pd.DataFrame(np.random.randn(5, 3), index=['a', 'c', 'e', 'f', 'h'], $                   columns=['one', 'two', 'three']) $ df
df.describe()
archive.name.value_counts()
%%writefile 1st_flask_app_2/static/style.css $ body { $     font-size: 2em; $ }
talks.sort(inplace = True, columns = 'id')
table1.dtypes
words = lc_review.split(" ") $ words_no_stop = [w for w in words if w not in stopwords.words("english")]
data_dir = os.path.join(os.getcwd(), 'data') $ print(data_dir)
old_page_converted = df2.sample(n_old, replace = True) $ p_old = old_page_converted.converted.mean() $ p_old
d1 = pd.DataFrame([['A1', 'B1'],['A2', 'B2']], columns=['A', 'B']) $ d2 = pd.DataFrame([['C3', 'D3'],['C4', 'D4']], columns=['A', 'B']) $ d3 = pd.DataFrame([['B1', 'C1'],['B2', 'C2']], columns=['B', 'C']) $ pd.concat([d1, d2])
youthUser4['year'].groupby(youthUser4['cityName']).describe()
tem=list(mars_facts[0].values()) $ tem
stars.to_pickle('data/pickled/new_subset_starred.pkl')
log_with_day.select('dateTime','dayOfWeek').show(10) $
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True) $ toma.iloc[::20].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.', '.', '.', '.']) $ ax.set_ylabel('Relative error') $
plt.scatter(cdf.FUELCONSUMPTION_COMB, cdf.CO2EMISSIONS,  color='blue') $ plt.xlabel("FUELCONSUMPTION_COMB") $ plt.ylabel("Emission") $ plt.show()
control_cr = df2.query('group == "control"').converted.mean() $ control_cr
df['quantity'] = df['quantity'].astype(int) $ df
with tf.variable_scope('Optimizer', reuse=tf.AUTO_REUSE): $     train_op_model = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=model_vars) $     train_op_elmo = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=elmo_vars) $     train_op_elmo_coef = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=elmo_vars_coef) $     train_op_elmo_cell_weights = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=elmo_vars_cell_weights)
_ = ok.grade('q12')
len(out.index.unique()), out.shape
np.exp(-0.0150) $ 1/np.exp(-0.0150)
All_tweet_data_v2=All_tweet_data.copy()
corn_vege.agg(["sum","mean"])
df_new.head()
aviation = disaster_tables[2] # reading from html $ aviation.head()
import scipy $ corr_coeff = scipy.stats.pearsonr(data_compare['SA_mix'], data_compare['SA_google_translate']) $ print('Der Korrelationskoeffizient zwischen dem Mix und Google Translation ist:') $ print('----------------------------------------------------------------------------') $ print(corr_coeff[0]) $
temp_df['reorder_interval_group'].replace('', np.nan, inplace=True) $ temp_df.dropna(subset=['reorder_interval_group'], inplace=True) $ temp_df.reorder_interval_group.apply(int)
findNumbers = r'\d+' $ regexResults = re.search(findNumbers, 'not a number, not a number, numbers 2134567890, not a number') $ print(regexResults.group(0))
pitches
records2.loc[records2['Age'].isnull(), 'Age'] = records2['Age'].mean() $ records2.loc[records2['GPA'].isnull(), 'GPA'] = records2['GPA'].mean() $ records2.loc[records2['Days_missed'].isnull(), 'Days_missed'] = records2['Days_missed'].mean()
preds = lm.predict(X_new) $ preds
tweets_per_day_df.plot.line(x=tweets_per_day_df['date'],y='number_of_tweets',figsize=(15,8),lw=1)
pickle.dump(bow_df,open('../proxy/dataset3_textb','wb'))
y_test_under[rfc_bet_under].mean()
df[:10][['penalties', 'overall_rating']]
fwd = df[['Store']+columns].sort_index(ascending=False $                                       ).groupby("Store").rolling(7, min_periods=1).sum()
Meter1.Data
df.dealowner.unique()
new = df.groupby(['Year','Month','hashtags'])['hashtags'].agg({'no':'count'}) # do a groupby and count each hashtag $ new.reset_index(level=[0,1,2], inplace=True) # reset the indexes $ new['datetime'] = pd.to_datetime((new.Year*10000+new.Month*100+1).apply(str),format='%Y%m%d') # convert back to a datetime with a default day of 1 $ idx = new.groupby(['datetime'])['no'].transform(max) == new['no'] # find the indexes of those max hashtags per month $ new[idx] # return the result
word_freq_df[word_freq_df.Word=='table']
encode_categorical_cols()
ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng) $ ts.head()
weekly_gtrends_data = pd.read_csv("data/multiTimeline_2014-12-01_to_2018-06-27_weekly.csv")
ayush_sean = df_ayush.join(df_sean, how = 'outer').fillna(0) $ ayush_sean
df_usa = df_usa.rename(index = str, columns = {"Gross Domestic Product (GDP)":"GDP", "National Rainfall Index (NRI)":"NRI", "Population density":"PD", "Total area of the country":"Area", "Total Population":"Population"})
df_link_meta.head(3)
df_sched[['Initiation','Design','Procurement','Construction', 'Project Closeout']].describe()
RNPA_new_8_to_16wk_arima = RNPA_new_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']] $ RNPA_new_8_to_16wk_arima.index = RNPA_new_8_to_16wk_arima.index.date
ddf = dd.read_csv('test-data/output/sample-xls-case-badlayout1.xlsx-*.csv') $ ddf.compute().head() # dask breaks! use d6tstack.combine_csv $
train_df = pd.read_json("train.json") $ test_df = pd.read_json("test.json")
print(true_file.shape) $ print(test_file.shape) $ display(true_file.head()) $ display(test_file.head())
import tweepy $ import pandas as pd $ import matplotlib.pyplot as plt
df.shape
dr_2018 = df_2018[df_2018['Specialty'] == 'doctor'] $ RNPA_2018 = df_2018[df_2018['Specialty'] == 'RN/PA'] $ ther_2018 = df_2018[df_2018['Specialty'] == 'therapist']
tweet_archive_clean.loc[tweet_archive_clean['new'].apply(find_dot) == True]
def hex_to_dec(x): $     return int(x, 16)
dr_id_str = "dim_red" $ mod_id_str = "gpu_mod_run_1"
%%time $ theano_coverage_err_func(validation_labels, val_prediction)
wd = 1e-7 $ bptt = 70 $ bs = 52 $ opt_fn = partial(optim.Adam, betas=(0.8, 0.99))
df_timeseries['cryptdex100'] = df_timeseries.groupby('date')['cryptdex_index'].transform('sum')
data_with_dates = pd.DataFrame(df_final[['created_time', 'total_likes', 'total_comments']]) $ splits = data_with_dates['created_time'].str.split('T') $ data_with_dates['created_time'] = splits.str[0] $ print(data_with_dates.head()) $ d_dates =  data_with_dates.groupby('created_time').sum()
df.loc[~df["ORIGINE_INCIDENT"].isin(origines), "ORIGINE_INCIDENT"] = "NAN"
sns.distplot(posts[posts['Score'] <= 30]['Score'], kde = False, bins = 25)
df_cal = pd.read_csv('calendar.csv')
n_high = total-a $ n_med = total-b $ n_low = total-c
from bs4 import BeautifulSoup $ soup=BeautifulSoup(_html, "lxml")
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, (1-p_new)]) $ new_page_converted
unique_users2 = df2.user_id.nunique() $ print('The unique number of users_id in the df2 is {}'.format(unique_users2))
from __future__ import print_function $ import seaborn as sns, matplotlib.pyplot as plt $ sns.set() $ %matplotlib inline
data['created'] = pd.to_datetime(data['created'], infer_datetime_format=True)
autos['price'].value_counts().sort_index(ascending=True).head(10)
(df.query('converted == "1"')['user_id'].nunique()/df['user_id'].nunique())*100 $
rate_pnew_null = sum(df2['converted'])/df2.shape[0] $ rate_pnew_null
df3.sort_values(by='DepDelay', ascending=False).head(5)
aapl.plot() $ plt.yscale('log')  # logarithmic scale on vertical axis $ plt.show()
knn_yhat = knn.predict(test_X) $ print("KNN Jaccard index: %.2f" % jaccard_similarity_score(test_y, knn_yhat)) $ print("KNN F1-score: %.2f" % f1_score(test_y, knn_yhat, average='weighted') )
fundret.idxmax(), fundret[fundret.idxmax()]
samples_query.result_set
r.xs('AMD', level=1)[['weight']].plot()
tweets_rt['full_text'] = tweets_rt['retweeted_status'].map(lambda value: value[0].decode('utf-8')) $ tweets_rt['created_at'] = tweets_rt['created_at'].str.decode('utf-8')
df = pd.DataFrame([[1, np.nan, 2], $                   [2, 3, 5], $                   [np.nan, 4, 6]]) $ df
LARGE_GRID.display_fixations(raw_large_grid_df, option='fixations')
X = pd.get_dummies(X, columns=['subreddit'], drop_first=True)
y_2_pred = rnd_reg_2.predict(X_future)
s = so['commentcount'] $ s.head()
sorted(users.age.unique())
text[::-1]
dfs = [df10, df11, df12, df13, df14, df15, df16, df17] $ for i,d in enumerate(dfs): $     year = 2010 $     d.columns = ['zipcode', 'month', 'heating_complaint'] $     d['year'] = year+i
y_train.value_counts()
autos["odometer_km"].describe()
stores_pd = stores.toPandas() $ pd.value_counts(stores_pd['store_level'].values, sort=True).plot(kind="bar")
archive_clean.info()
own_star['created_at_star'] = own_star['created_at_star'].fillna(own_star['created_at_own'])
loan_fundings.state.unique()
df.sample(3)
import sqlite3 $ import numpy as np $ import pandas as pd $ import sqlalchemy $ from sqlalchemy import create_engine
injury_df['Date'] = injury_df['Date'].map(lambda x: x.replace('-','')) $ injury_df['Date'].head()
dt_features['deadline'] = pd.to_datetime(dt_features['deadline'],unit='s')
df['word_count'] = df['body'].apply(lambda x: len(str(x).split(" "))) $ df[['body','word_count']].head()
posts.groupby('from').aggregate(sum)
ign.head()
autos[autos["registration_year"].\ $ between(1900,2016)]["registration_year"].\ $ value_counts(normalize=True).sort_index(ascending=True)
import sqlite3 as lite $ import pandas as pd $ import os $
from pandas.plotting import scatter_matrix $ axes = scatter_matrix(data.loc[:, "TMAX":"TMIN"])
shopping_carts = pd.DataFrame(items) $ shopping_carts
ins['new_date'] = ins['date'].apply(convert) $ ins['year']     = ins['new_date'].apply(lambda x: x.year) $
tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)
mention_df = base_mention_df.join(user_summary_df['gender'], on='user_id') $ mention_df.count()
[doc['id'] for doc in resp.json()]
temp = pd.DatetimeIndex(df_clean2['timestamp'])
len(remove_dog)
import pandas as pd $
df15stats=df15.groupby(['county','store_number']).agg({'state_retail':'mean','profit':'mean','ppb':'mean','volume_sold_l':'mean','bottles_sold':'mean','bottle_ml':'mean','sale':'sum','sale':'mean',}) $ df15stats  
included_features = cs_ret + cs_vol + cs_drsd + ts_ret + ts_vol + ts_drsd + spy_cols $ train_data = dflong[dflong['Date'].isin(train_dates)][included_features].as_matrix() $ validation_data = dflong[dflong['Date'].isin(validation_dates)][included_features].as_matrix() $ test_data = dflong[dflong['Date'].isin(test_dates)][included_features].as_matrix()
import numpy as np $ from gmplot import GoogleMapPlotter
stories.head()
dumpfile='legion_2013_08_16.sql'
df[df['Agency Name'].isin(['Department of Transportation', 'DOT'])].index.month.value_counts().sort_index().plot() $ df[df['Agency Name'].isin(['New York City Police Department', 'NYPD'])].index.month.value_counts().sort_index().plot()
unrepaired_damage_dict={'nein':'no', $                         'ja':'yes' $ } $ autos['unrepaired_damage'].replace(unrepaired_damage_dict,inplace=True) $ autos['unrepaired_damage'].fillna('unspecified',inplace=True)
with tb.open_file('data/my_pytables_file.h5', 'r') as f: $     for node in f: $         print(node) $
popularity.head()
?JsonCollection()
for category in unique_categories: $     print (category, '\n') $     print (all_df[category].unique()) $     print ('\n')
df['created_at'] = pd.to_datetime(df['created_at'])
own_star = pd.merge(owns, stars, how='outer', on=['user_id', 'repo_id'], suffixes=('_own', '_star')) $ own_star.info() $ own_star.head()
df4
new_page_converted - old_page_converted
predict.predict_score('Stuart_Bithell')
autos.head()
np.dot(a, b)
a = pd.DataFrame(np.random.random((5, 2)), columns=['one', 'two']) $ a.iloc[1, 1] = np.nan $ a
all_df.head(2)
new_page_converted = np.random.binomial(n=n_new, p=p_null) / n_new $ new_page_converted
dul_ecolls_to_exclude = pd.read_csv('IDs to exclude\elec colls to exclude\Duluth electronic colls to exclude.txt', $                                    sep='\t', dtype=str) $ dul_ecolls_to_exclude
df.groupby('label').count().show() $
C_resource_id = '1df83d07805042ce91d806db9fed1eeb'
df2.head()
with open('tweet_json.txt') as file: $     status = [] $     for line in file: $         status.append(json.loads(line))
negative = '/Users/EddieArenas/desktop/Capstone/negative-words.txt' $ negative = pd.read_table(negative, encoding = "ISO-8859-1")
dftemp = df1[df1['Area'].isin(['Iceland'])]    #storing the rows where area is iceland $ dftemp
reddit_comments_data.select(max("score")).show(truncate=False)
classes = labels_dedupe.pivot('funding_round_uuid','investor_uuid','invested')
image_predictions.info()
import psycopg2 $ import pandas as pd $
URL = "http://www.reddit.com"
df2.query("group == 'control'").converted.mean()
timestamps = [ $     datetime(2018, 1, 1), datetime(2018, 1, 2), datetime(2018, 1, 3), $     datetime(2018, 2, 1), datetime(2018, 2, 2), datetime(2018, 2, 3), $     datetime(2019, 1, 1), datetime(2019, 1, 2), datetime(2019, 1, 3), $ ]
new_page_converted.sum()/len(new_page_converted)-old_page_converted.sum()/len(old_page_converted)
def my_scaler(col): $   return (col - np.min(col))/(np.max(col)-np.min(col)) $ data_scaled = data_numeric.apply(my_scaler)
dr_new_data_plus_forecast = get_ARIMAX_output_df(dr_new_data_plus_forecast, num_dr_new) $ dr_existing_data_plus_forecast = get_ARIMAX_output_df(dr_existing_data_plus_forecast, num_dr_existing)
autos = autos[autos["registration_year"].between(1900,2018)]
xx=list(building_pa_prc_zip_loc['permit_type'].unique()) $ aux_list=[] $ for x in xx: $     aux_list.append((x,set(building_pa_prc_zip_loc.permit_type_definition[building_pa_prc_zip_loc['permit_type']==x]))) $ aux_list
with tb.open_file(filename='data/file2.h5', mode='w') as f: $     f.create_array(where='/',  name='array2', obj=[4, 5, 6, 7])
options_frame[abs(options_frame['ModelError']) >= 1.0e-4].plot(kind='scatter', x='BidAskSpread', y='ModelError')
naimp.get_overlapping_matrix()
df.tail()
newgeo = pd.concat([us, notus]) $ len(geo) == len(newgeo)
null_vals = np.mean(p_diffs) $ null_vals
log_file = "hdfs://localhost/user/logs/*" $ filtered_data = sc.textFile(log_file)\ $     .filter(lambda line: any(item in line for item in broadcast_list_sc.value)) $ filtered_data.take(10)
print("Feature importances") $ for f, i in sorted(zip(features, bdt.feature_importances_), key=lambda x: x[1]): $     print(f.ljust(20), i)
import os $ import pandas as pd $ import matplotlib.pyplot as plt
sum(new_page_converted).astype('float32')/145310 - sum(old_page_converted).astype('float32')/145274
test_data_features_tfidf.shape
img_df.info()
from h2o.estimators.gbm import H2OGradientBoostingEstimator
print(polynomial_sframe(graphlab.SArray([1.0, 2.0, 3.0]), 3))
new_page_converted = np.random.choice([0,1], size=treatment_group.shape[0], p=[1-p_new, p_new]) $ print(new_page_converted.mean())
df_clean.info()
tweets_kyoto.head()
auto_new.Hand_Drive.unique()
apple.Date = pd.to_datetime(apple.Date, infer_datetime_format=True) $ apple.dtypes
access_token = 'YOUR_ACCESS_TOKEN' $
forecast_df['under_20'] = 0 $ for ind, row in SANDAG_age_df[SANDAG_age_df['AGE_RANGE'] == '10 to 19'].iterrows(): $     forecast_df.loc[ind, 'under_20'] = row['POPULATION'] $ forecast_df.head()
df2_group = df.groupby('group') $ df2_group.describe()
X_final_test_2 = X_reduced.iloc[indices_test] $ X_training_2 = X_reduced.iloc[indices_train]
store=join_df(store,store_states,"Store") $ weather=join_df(weather,state_names,'file','StateName') $ sum(store['State'].isnull()),sum(weather['State'].isnull())
def norm_by_data2(x): $     x['data1'] /= x['data2'].sum() $     return x $ display('df', "df.groupby('key').apply(norm_by_data2)")
len(calls_nocontact_2017)
df.shape
df['Complaint Type'] [df['Agency'] =='DOT'].value_counts().head()
tweet_scores_clean.rename(columns={"id": "tweet_id"},inplace=True)
data.patient
cig_data[['nicot','tar']][:2]
df2.iloc[5,3]=np.inf
import statsmodels.api as sm $ convert_old = df2.query('converted == 1 and group == "control"').shape[0] $ convert_new = df2.query('converted == 1 and group == "treatment"').shape[0] $ n_old = df2.query('group == "control"').shape[0] $ n_new = df2.query('group == "treatment"').shape[0]
from ast import literal_eval $ import pandas as pd $ import numpy as np
old_page_converted = np.random.choice([0,1], size=n_old, p=[1-p_old, p_old]) $ print(old_page_converted.mean())
Salesdata = load_SalesData ('../data/SalesDataA.xlsm', sheetname = 'Sales') $ data.head()
Base = automap_base() $
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
for res_key, df in entso_e.items(): $     logger.info(res_key + ': %s', df.shape)
gs = GridSearchCV(KNeighborsRegressor(), parameters, cv = 5, scoring = 'neg_mean_squared_error') $ gs.fit(X_train, y_train)
df_converted = df.query('converted == 1') $ df_converted['user_id'].nunique()/df['user_id'].nunique()
subs_and_comments = sub_df.merge(sub_comments, on='id', how='outer')
for col in ('building_id', 'display_address', 'manager_id', 'street_address'): $     X_train, X_test = factorize(X_train, X_test, col)
tweet1.author
treatment = df2[df2['group']=='treatment'] $ treatment.head(5)
permits_df.dropna(inplace=True)
p_diffs = [] $ for i in range(10000): $     new_page_converted = np.random.binomial(1, p_new, n_new) $     old_page_converted = np.random.binomial(1, p_old, n_old) $     p_diffs.append(new_page_converted.mean()-old_page_converted.mean())
my_string = 'my text' $ my_string
mnnb.feature_log_prob_.shape
stocks = pdread.DataReader('AAPL', 'yahoo', st, ed)
df.cumsum().plot();
reddit_comments_data.createOrReplaceTempView('tmp_reddit_df')
n_old = df2.query('landing_page == "old_page"').count()[0] $ n_old
control_group = df2.query('group == "control"') $ pconversion_control = control_group['converted'].mean() $ pconversion_control
df_vow['High'].unique()
data['comments'].describe()
df = df4[df4.day == days[0]] $ place = total_base_dict_by_place.index[20] $ total_base_dict_by_place.loc[place]
summed.interpolate()  # notice all the details in the interpolation of the three columns
parse_dict['creator'].head(5)
horror_readings=horror_readings[['visitor_id','id','visit_id','tracking_time']]
print(pd.merge(df1,df2))
priors_product_reordered_spec= priors_reordered.groupby(["user_id","product_id"]).size().reset_index(name ='reordered_count_spec') $ priors_product_reordered_spec['userprod_id']=priors_product_reordered_spec['product_id'] + priors_product_reordered_spec['user_id'] *100000 $ priors_product_reordered_spec.head(10)
from sklearn.preprocessing import MinMaxScaler $ from sklearn.model_selection import train_test_split
symbol='^NBI' $ benchmark1 = web.DataReader(symbol, 'yahoo' , start_date ,end_date)
soup.find_all('div', class_='schedule-container')[0].select('.disabled')
data.loc[3]
users_usage_summaries = pd.pivot_table(df_usage[['id', 'feature_name']], index=['id'], columns=['feature_name'], aggfunc=len, fill_value=0) $ accepted_rate = df_usage.groupby(['id'])['accepted'].mean().to_frame() $ churned = joined_df.groupby(['id'])['churned'].mean().to_frame() $ users_usage_summaries = users_usage_summaries.join(accepted_rate, how='left').join(churned, how='left') $ users_usage_summaries.head(10) $
y=df['loan_status'].values $ y[0:5]
p.asfreq('M', how='start')
%matplotlib inline $ import warnings $ warnings.filterwarnings('ignore')
new_page_converted = np.random.choice([0,1], n_new, p=[1-p_new, p_new]) $ new_page_converted
y = tf.placeholder(shape=[None], dtype=tf.int64, name="y")
df_first_published_at = df_events.copy()
df4.describe() # basic stats all from one method $
sn.distplot(train['age'])
dr_2018.shape
my_query = "SELECT * FROM specimens LIMIT 10" $ my_result = limsquery(my_query) $ first_element = my_result[0] $ print sorted(first_element.keys())
sr = nltk.corpus.stopwords.words('english') $ sr[:10]
df2.user_id.drop_duplicates(inplace = True, keep = 'first')
rolled_returns['2017-06-07':].mean()
df_1 = pd.read_csv('bitcoin_data_one_minute_2016_2017.csv', parse_dates = ['timestamp']).set_index('timestamp')
np.exp(results_4.params)
y = df.groupby('Journal')['Title'].count() $ df_s = df[df['Journal'].apply(lambda x: y[x]>15)] $ avg_pd = df_s.groupby('Journal').agg({'PubDays':'mean', 'Publisher':'first'}) $ avg_pd = avg_pd.reset_index() $ avg_pd['Journal']=avg_pd['Journal'].str.lower()
industries.head()
pf.cost.sum()/100
subreddits.keys()
treatg = df2.query('group == "treatment" & converted ==1').user_id.count() $ greatttl = df2.query('group == "treatment"').user_id.count() $ treatment_convert = treatg/greatttl $ treatment_convert
print('max value: ' + str(df_usa['NRI'].max()) + ' occurs in ' + str(df_usa['NRI'].idxmax())) $ print('min value: ' + str(df_usa['NRI'].min()) + ' occurs in ' + str(df_usa['NRI'].idxmin()))
twtter_count1 = df1.groupby('placeId').count()[['id']] $ twtter_count2 = df2.groupby('placeId').count()[['id']] $ twtter_count3 = df3.groupby('placeId').count()[['id']]
z_score, p_value = sm.stats.proportions_ztest([17489, 17264], [145274, 145310], alternative='smaller') $ z_score, p_value $
maturities
train
first_week_day_num = pd.Series(np.arange(1,8), index = first_week) $ first_week_day_num
import pandas as pd $ print pd
group[['inspection_date', 'results']]
from pyspark.ml.feature import HashingTF $ num_hash_buckets = 2 ** 18 $ hashingTF = HashingTF(inputCol="rawFeatures", outputCol="features", numFeatures=num_hash_buckets)
df_ad_airings_5 = pd.read_pickle('./TV_AD_AIRINGS_FILTER_DATASET_3.pkl')
test_start='2018-03-05' $ test_end= '2018-04-30' $ train_start = '2015-01-26' $ train_end = '2018-02-26'
item = collection.item('AAPL') $ item
es = ft.EntitySet(id = 'clients')
diff_real = 0.118808 - 0.120386  $ p_diffs = np.array(p_diffs) $ (p_diffs > diff_real).mean()
date = pd.to_datetime(logins.login_time.iloc[0]) $ print(type(date), date.weekday)
f_oneway(week2, week3)
collection.write('AAPL', aapl[['Close', 'Volume']], $                  metadata={'source': 'Quandl'}, $                  overwrite=True) $ df = collection.item('AAPL').to_pandas() $ df.tail()
query = ' SELECT count(id) FROM nodes_tags WHERE key = "tourism" AND value = "hotel"' $ c.execute(query) $ results = c.fetchall() $ print "Hotels in Leiden database:", results[0][0]
wages = web.DataReader("A576RC1A027NBEA", $               "fred", $               datetime.date(1929, 1, 1), $               datetime.date(2013, 1, 1)) $ wages
ser4 = pd.Series(dic_a,index=["p","q","r","a"]) $ ser4
frame.loc['a', 'Ohio']
import pandas as pd $ import numpy as np $ autos = pd.read_csv('autos.csv',encoding='Latin-1') $ autos[:10]
display(no_outliers)
sqlc = sqlite3.connect('iris.db')
daily_returns.hist(bins=20) $ plt.show()
tweet_data = tweet_data[['tweet_id', 'timestamp', 'name', 'rating_numerator', $        'rating_denominator', 'stage', 'prediction', 'confidence', $         'favorite_count','favorited','retweet_count','text']]
y_hat = map_estimate['alpha_interval__'] + map_estimate['betas_race_interval__'].reshape((-1,1)).T.dot(X[:,1:5].T) +\ $ map_estimate['beta_income_interval__'].reshape((-1,1)).T.dot(X[:,5:6].T)+ \ $ map_estimate['beta_complaint_interval__'].reshape((-1,1)).T.dot(X[:,6:7].T) $ plt.scatter(y_hat.T,y) $ plt.show()
data['retained'] = pd.to_datetime('2014-07-01') - pd.to_datetime(data.last_trip_date) <= pd.Timedelta('30 days')
print(liberia_df)
import pandas as pd $ df = pd.DataFrame(dataset)
cats = ['ip', 'app', 'device', 'os', 'channel', $ 'click_timeDay', 'click_timeHour']
tf_is = 3 $ n_docs = 3 $ idf_is = np.log((n_docs+1) / (3+1)) $ tfidf_is = tf_is * (idf_is + 1) $ print('tf-idf of term "is" = %.2f' % tfidf_is)
ggplot(aes(x="comp"), maint) + geom_bar(fill="blue", color="black")
smpl.count()
print("Pairs\n" + str(zip(wordlist, wordfreq)))
X= preprocessing.StandardScaler().fit(X).transform(X) $ X[0:5]
df_goog.asfreq('D', method='bfill')
donald_tweets_df = transform_tweets( donald_tweets, $                                      "Sentiment Score" ) $ donald_tweets_df.head()
full_monte = pd.DataFrame({'name': monte, $                           'info': ['B|C|D', 'B|D', 'A|C', $                                   'B|D', 'B|C', 'B|C|D']}) $ full_monte
ftr_imp_rf=zip(features,trained_model_RF.feature_importances_) $ for values in ftr_imp_rf: $     print(values)
df2 = df.copy() $ df2[df2 > 0] = -df2 $ df2
giss_temp.columns
x_test = np.float32(x_test) $ y_test = np.float32(y_test)
td /= 1000
rr.tail(3)
table_grouped_type=original_merged_table.groupby("type") $
binary_sensors_df.head()
strike = pd.DataFrame(K*np.ones(ndays*nscen).reshape((ndays,nscen)), index=dates) $ call_hist = pd.DataFrame({'Prima':np.exp(-r*ndays)*np.fmax(sim_closes_hist-strike,np.zeros(ndays*nscen).reshape((ndays,nscen))).T.mean()}, index=dates) $ call_hist.plot();
CRnew = df2.query('landing_page == "new_page"')['converted'].mean() $ CRold = df2.query('landing_page == "old_page"')['converted'].mean()
max_date=session.query(func.max(Measurement.date)).all() $ max_date
len(acorn)
playlists = yt.get_playlists(channel_id, key) $ df_playlists = pd.DataFrame(playlists) $ df_playlists.head(2)
final_data = pd.read_pickle('D:/CAPSTONE_NEW/jobs_data_clean.pkl')
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer $ analyzer = SentimentIntensityAnalyzer()
df_pd.dtypes
autos["price"].value_counts().sort_index(ascending=True)
word_count = np.array([len(d.split(' ')) for d in docs]) $ print('Done.')
tweets_clean.set_index(tweets_clean.columns[0], inplace = True) $ tweets_clean.head()
df_old = df2[df2['landing_page'] == 'old_page'] $ len(df_old)
list(range(1,9))
import emoji
n_new = df2[df2['group']=='treatment']['user_id'].count() $ n_new
df_dd.shape $ df_dd.head()
bnb = pd.concat([bnb,pd.get_dummies(bnb['first_affiliate_tracked'],prefix='channel_')],axis=1) $ bnb.head()
(autos["last_seen"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         )
import plotly.plotly as py $ import plotly.graph_objs as go
test_features = ['goal','Cat-Nums','City-Nums','launched_atYM','Length_of_kick','Days_spent_making_campign','City_Pop','staff_pick'] $ X = df_select[test_features].copy() $ y = df_select[['status']].copy() $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=324)
birth_dates["BirthDate_dt"] = pd.to_datetime(birth_dates["BirthDate"], format="%d.%m.%Y") $ birth_dates.head()
dataset = os.path.join('..\\','Data', 'ml-20m') $ ml_20m_path = os.path.join(dataset, 'ml-20m.zip') $ ml_1m_path = os.path.join(dataset, 'ml-latest-small.zip')
msft = pd.read_csv("../../data/msft.csv", index_col=0) $ msft.head()
try: $     r=table.find(text='Registration').find_next('td').text $ except: $     reg='No data' $ reg
grid_search.fit(train, train['Visits'])
events.to_feather("./data/swimming-records.feather")
data.shape
wd = "/Users/zy/Documents/2017Spring/DataMinning/Project/data" $ train_df = pd.read_json(wd+"/train.json") $ train_df = train_df.reset_index(drop=True) $ test_df = pd.read_json(wd+"/test.json") $ test_df = test_df.reset_index(drop=True)
oppstage.loc[0].plot(kind='bar', stacked=True, figsize=(12,6));
clfgtb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(x, y) $ print(clfgtb.score(x_new, y_new)) $ feature_importances = np.stack([new_feature_cols, clfgtb.feature_importances_], axis=1) $ print(np.sort(feature_importances, axis=0)[::-1])
df1.head()
sentiment_df.plot(x="News_Source", y="Compound", title = "News Mood Analysis_Channel", kind = "bar") $ plt.show()
submit = perf_test[['ID_CPTE']] $ submit['Default'] = log_reg_pred $ submit.head()
glm_multi_v2.confusion_matrix(valid)
df.groupby(df.date.dt.year)['first_day_pctchg'].mean()
df = pd.DataFrame(results, columns=['Station', 'Temperature']) $ df.head()
billstargs.drop(['congress', 'number', 'chamber', 'committee_ids', 'introduced_on', 'last_action_at', 'last_vote_at', 'official_title', 'urls'], axis=1, inplace=True)
type(close_arr)
table.head() # refined into the table we would like to store
analyze_set.loc[analyze_set['favorites']==80]
p_diffs = [] $ new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation $ p_diffs = np.array(p_diffs)   
dsg.axes('Z')
logregmodeltf = 'logregmodeltf.sav' $ pickle.dump(pipeline, open(logregmodeltf, 'wb'))
print (df.shape) $ df2 = df.drop(df.index[[5, 12, 23, 56]]) $ print (df2.shape) $
offseason07["Category"] = "2007 Offseason" # This assigns a season identifier to the transactions.
url_api = ('https://www.quandl.com/api/v3/datatables/WIKI/PRICES.json?' + $            '&date.gte=2017-05-01&date.lte=2017-05-31&api_key=' + config.api_key)
individuals_metadata_df.shape
twitter_coll_reference.count()
base_dict_by_place.head()
df.head()
X_extra.describe(include=['object'])
plt.hist(p_diffs) $ plt.xlabel('p_diff') $ plt.ylabel('Frequency') $ plt.title('10,000 simulated p_diffs');
autos.describe(include='all')
np.c_[np.random.random(5),np.random.random(5)]
ticks.tshift(1, 'min').head()
repos.created_at = pd.to_datetime(repos.created_at)
train = train[train.price < 100000]
f_close_clicks_train.show(3)
print(df.index[0])
pt_after=pd.DataFrame.pivot_table(df_users_6_after,index=['cohort'],values=['uid'],aggfunc='count',fill_value=0)
%load_ext dotenv $ %dotenv
classification_df = encoded_df.query('(id == 5 or id == 14) and best != 0') $ classification_df.best.describe()
tweets_rt['retweeted'].value_counts()
train.StateHoliday.head()
x_train, x_test, y_train, y_test, msk = demo.prep_for_model(snowshoe_df)
scaler = preprocessing.StandardScaler() $ x_train_scaled = scaler.fit_transform(x_train) $ x_test_scaled = scaler.transform(x_test)
df.head(10)
print(soup.prettify())
movie_rating=pd.merge(top100ratings,movies, on='movieId', how='inner')
df_NYPD = df[df['Agency Name'].isin(['New York City Police Department', 'NYPD'])]
print(team.shape) $ team.head()
df3 = df2 $ df3.head()
import datetime $ multiple_party_votes_all['date'] = pd.to_datetime(multiple_party_votes_all['date'])
print(df[0:2]) $ print(df[-3:])
op_ed_articles.head() $ op_ed_articles.to_csv('for_paper.csv')
df_copy=df.copy() $ image_copy=image.copy() $ tweet_data_copy=tweet_data.copy()
data.describe()
sdf = sdf.assign(Norm_reviewText = normalize_corpus(sdf.reviewText))
y_pred
dfd['brand'] = [b.strip().title() for b in dfd.brand] $ dfd.brand.unique()
autos_p['registration_year'].value_counts().sort_index(ascending = False).head(10)
twitter_archive_master.describe()
fpr, tpr, threshold = metrics.roc_curve(y_test, probability [:,1])
backers_rate['n_proj'].plot.bar(title='Number of projects vs backers')
finals['type'].value_counts()
en_tweets_df = tweets_wo_dublicates[tweets_wo_dublicates['lang'] == 'en']
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif $ df2_vif = pd.DataFrame() $ df2_vif["VIF Factor"] = [vif(team.values, i) for i in range(team.shape[1])] $ df2_vif["features"] = team.columns $ df2_vif
plt.rcParams['axes.unicode_minus'] = False $ dta_6203.plot(figsize=(15,5)) $ plt.show()
b.find_by_xpath('//*[@id="day-section"]/div/div[3]/div[1]/div/ul/li[5]/button').click()
obs_diff = (df2.query("landing_page=='new_page'").converted.mean() - df2.query("landing_page=='old_page'").converted.mean()) $ obs_diff
df2[['control', 'treatment']] = pd.get_dummies(df['group']) $ df2['intercept'] = 1
%store extract_all
v_to_c = pd.merge(visits, checkouts)
df.index
clean_users[clean_users['active']==0].dropna(how='any')['account_life'].mean()
dframe_team.drop(dframe_team.columns[[2,3,4,5]],inplace=True,axis=1)
df['Updated Shipped ranges'] = pd.cut(df['Updated Shipped diff'], $                                      bins=[0,500,1000,2000,2500,3000,10000], $                                      right=False)
latest_df['annotations'] = latest_df.annotations.apply(load_json)
sc.defaultParallelism
test['air_genre_name']=test['air_genre_name'].fillna('Other')
dates=pd.date_range(start='1/Oct/2020', periods=5, freq='3M') $ print(dates)
patterns = {'rent':rent_patterns, $             "cash": cash_patterns, $             'deposit': deposit_patterns} $ PATTERNS = {k: "|".join(["({})".format(el)for el in v]) for k, v in patterns.items()}
df10 = pd.read_csv('2010.csv')
print(str(autos["odometer_km"].unique().shape[0]) + " unique values") $ print(autos["odometer_km"].describe())
compressed_data = compress_data("loans_2007.csv", 5, dropcols=[], encodeval="ISO-8859-1", $                                 parsedatecols=["issue_d","earliest_cr_line","last_pymnt_d","last_credit_pull_d"])
people.groupby(lambda x:GroupColFunc(people,x,'a')).mean()
data_issues['changelog'][0]['total']
with open('twitterkeys.txt') as f: $     consumerKey = f.readline().strip() $     consumerSecret = f.readline().strip() $
print(f"{urls[1]} returns:") $ ux.is_short(urls[1])
df = pd.DataFrame({'one':[10,20,30,40,50,2000], $ 'two':[1000,0,30,40,50,60]}) $ df
class A(ChattyObject, metaclass=type): $     pass $ a = A() $ print(f'a.__class__ = {a.__class__}') $ print(f'A.__class__ = {A.__class__}')
dataset['User_Created_At'] = dataset['User_Created_At'].dt.strftime('%m/%Y') $ acc_created_date = pd.DataFrame({ $     'Date': pd.to_datetime(dataset['User_Created_At'].value_counts().index.tolist()), $     'Freq': dataset['User_Created_At'].value_counts() $ }) $
reddit_info.titles.drop_duplicates(inplace = True)
int_columns = int_columns=['AcceptedAnswerId','AnswerCount','CommentCount','FavoriteCount','Id','LastEditorUserId', $              'OwnerUserId','ParentId','PostTypeId','Score','ViewCount'] $ datetime_columns = ['ClosedDate','CommunityOwnedDate','CreationDate','LastActivityDate','LastEditDate'] $ text_columns=['Body'] $ posts_df = clean_dataframe(posts_df,int_columns,datetime_columns,text_columns)
df['Trip_duration'].dt.components
print(ad_source.sum(axis=0).sum()) $ print() $ print(ad_source.sum(axis=0))
c.execute(query) $
from scipy.stats import norm $ norm.ppf(1-(0.05/2))  # z score for 95% level of confidence
adopted_cats.loc[adopted_cats['Color']=='Orange Tabby/Orange','Color'] = 'Orange Tabby' $ adopted_cats.loc[adopted_cats['Color']=='Calico/Orange Tabby','Color'] = 'Calico Orange Tabby' $ adopted_cats.loc[adopted_cats['Color']=='Tortie/Orange','Color'] = 'Tortie Orange'
df_select.head()
new_user_project.to_csv('data_table/user_project.csv', index = False)
station = session.query(Station.station, Station.name, func.count(Measurement.tobs)).\ $ group_by(Measurement.station).filter(Station.station ==  Measurement.station).group_by(Measurement.station).\ $ order_by(func.count(Measurement.tobs).desc()).all() $ station
sns.tsplot(time=cum_cont.date, data=cum_cont.cum_contr)
df[(df.full_sq>10)&(df.full_sq<1500)] $ df.query('full_sq>10 and full_sq<1500') $
fwd.ix['gbp curncy','fwd curve']
data.loc[3]
del train_data['price']
pold=df2['converted'].mean() $ print(pold)
X = pivoted.fillna(0).T.values
hour_coef
fav_max = data['Likes'].max() $ fav_tweet = data[data['Likes'] == fav_max] $ print("The tweet with more likes is: \n{}".format(np.array(fav_tweet['Tweets'])[0])) $ print("Number of likes: {}".format(fav_max)) $ print("{} characters.\n".format(np.array(fav_tweet['len'])[0]))
df1, df2, df3, df4, df5 = (pd.DataFrame(rng.randint(0, 1000, (100, 3))) $                           for i in range(5))
problem_combos = pings.flatMap(get_problem_combos)
[(s.user.name, s.text) for s in retweets_of_me[:5]]
tSVD = TruncatedSVD(n_components=1000)
key = 'recordio-pb-data' $ boto3.resource('s3').Bucket(bucket).Object(os.path.join(dist, key)).upload_fileobj(buf) $ s3_train_data = 's3://{}/{}{}'.format(bucket, dist, key) $ print('uploaded training data location: {}'.format(s3_train_data))
df.head()
van_final['diffs'] = van_final.groupby(['userid'])['revtime'].transform(lambda x: x.diff()) / np.timedelta64(1, 'm')
empInfo = empInfo[["ID","level"]]
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).filter(Measurement.station == "USC00519281").all()
environment.gym.observation_space
print("\nClassification Report:\n",classification_report(y_test, y_hat)) $
top_songs['Year'] = top_songs['Date'].dt.year
for topic_idx, topic in enumerate(lda.components_): $     print "Topic %d:" % (topic_idx) $     print " ".join([tf_feats[i] $                     for i in topic.argsort()[:-10 - 1:-1]])
df1.columns
from quantopian.pipeline.data import Fundamentals $ from quantopian.pipeline.filters.fundamentals import IsPrimaryShare
df.head()
df2[df2.duplicated(['user_id'], keep=False)] #identifying the repeated user_id $
ml.confusion(dep_test.reshape(dep_test.shape[0]), $              predicted, ['No', 'Yes'], 2, 'Smoker Classification [Numeric & Categoric]')
all_tables_df.iloc[2:4, 1:]
sum(tw.retweeted_status_id.notnull())
clean_stations
logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','c1','c2']]) $ res = logit_mod.fit() $ res.summary()
soup.body
plt.plot(sample[:, 2], sample[:, 3], ',k', alpha=0.1) $ plt.xlabel('$g_1$') $ plt.ylabel('$g_2$') $ print("g1 mean: {0:.2f}".format(sample[:, 2].mean())) $ print("g2 mean: {0:.2f}".format(sample[:, 3].mean()))
act_irr
conn = pymysql.connect(host='mysql.guaminsects.net',user='readonlyguest',passwd='readonlypassword',db='oryctes') $ sql = 'SELECT * FROM YigoBarrelLocations;' $ pd.io.sql.read_sql(sql, conn)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old,n_new], alternative='smaller') $ print('z_score : {}, p_value : {}'.format(str(z_score),str(p_value)))
mask = df2.user_id.duplicated(keep=False) $ df2[mask] #found this technique on Stack Overflow - nice!? $
games_to_bet.head()
trading.hedge_ratio_ols(window=30)
cov_df.loc[lst_dates[0]].copy()
twitter_archive_clean.doggo=twitter_archive_clean.doggo.astype('category') $ twitter_archive_clean.floofer=twitter_archive_clean.floofer.astype('category') $ twitter_archive_clean.pupper=twitter_archive_clean.pupper.astype('category') $ twitter_archive_clean.puppo=twitter_archive_clean.puppo.astype('category')
tmax_day_2018.dims
precipitation_data = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >= "2016-08-23")
X = np.array(df_ohc.iloc[:,4:]) $ Y = np.array(df_ohc[['trip_num','trip_dur_avg']])
chart.set_yticklabels(top_supporters.contributor_lastname)
claps = pd.Series(data['claps']) $ plt.boxplot(claps,0,'rs') $ plt.hlines(y = claps.quantile(0.90), label = '99.5 percentile', xmin = 0,xmax = 100, linestyles='dashed' ) $
test_ind.shape
bb_df.isnull().any()
error_set = df.query('group == "treatment" & landing_page !="new_page" | group != "treatment" & landing_page =="new_page"' ) $ error_set.head(2)
df = pd.read_csv('ab_data.csv') $ df.head(10)
wells_df_new_cleaned_plus_nn_wNoNulls.head()
talks = pd.merge(talks_train[['description', $                              'id', 'keywords', 'name']],labels,on = 'id')
last_hours = quarterly_revenue.to_timestamp(how="end", freq="H") $ last_hours
model_1 = graphlab.linear_regression.create(train_data, target = 'price', $                                                   features = model_1_features, $                                                   validation_set = None)
data.year.fillna(2013, inplace=True) $ data
init() $ pred=run(df) $ print(pred)
test_labels = [reuters.categories(doc_id)for doc_id in doc_id_list if 'test' in doc_id]
loans_df.home_ownership.value_counts()
import mglearn $ pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker='o', $                            hist_kwds={'bins': 20}, s=60, alpha=.8, cmap=mglearn.cm3)
schumer_time = df[["time","count_schumer"]][df["count_schumer"]>0][1:] $ schumer_time.head()
df.rename(columns={'Indicator': 'Indicator_id'}, inplace=False)  # Since the change is only temporary, it is not getting reflected in the df output $ df.head(2)
vip_crosstab_percentage = vip_crosstab.apply(lambda x: x/x.sum(), axis=1) $ vip_crosstab_percentage
ingreso_rng_dict = {v: k for k, v in enumerate(df["RNG_INGRESO_BRUTO"].unique().tolist())} $ ingreso_rng_dict
topicmax = list() $ for topic in doctopic: $     topicmax.append(argmax(topic))
date_ini = str(date(2013, 1, 1)) $ date_end = str(date.today()) $ df = get311NoiseComplaints(date_ini, date_end)
Test.SetFlowValues(8.5, 10.5, 0.25)
display(kayla.head(5))
s519397_df["prcp"].max()
validation.analysis(observation_data, lumped_simulation)
list.sort() $ print(list)
type(dataset[['customer_id','order_id']].values)
set(train_data['offerType'])
mask = percent_quarter.abs().apply(lambda x: x > 1) $ percent_quarter[mask].nlargest(4)
action.head()
df.dropna(inplace=True)
df.stemmed[:10]
print(calc_temps('2017-07-15', '2017-07-22'))
df2['landing_page'].value_counts()[0]/len(df2)
df_csv.visualize()
ltccomment.to_pickle('Litecoin2016-17.pkl') $ ethcomment.to_pickle('Ethereum2016-17.pkl') $ xrpcomment.to_pickle('Ripple2016-17.pkl')
store = join_df(store, store_states, "Store") $ len(store[store.State.isnull()])
null_vals = np.random.normal(0, np.array(p_diffs).std(), len(p_diffs))
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA']]) $ results = logit_mod.fit() $ results.summary()
import pandas as pd $ import numpy as np $ import datetime $ import matplotlib.pyplot as plt $ %matplotlib inline
%%time $ lr2 = LogisticRegression(random_state=20, max_iter=10000, C= 1, multi_class='ovr', solver='saga') $ lr2.fit(X_tfidf, y_tfidf) $ lr2.score(X_tfidf_test, y_tfidf_test)
df2.head()
convers_con['total_number_conversations'] = convers_con['added'] $ convers_con.head(2)
print(station_availability_df.info()) $
df_old = df.drop_duplicates(subset=None, keep='first', inplace=False)
from pytesseract import image_to_string $ import cv2 $ from PIL import Image
tweets_num = len(data) $ users_num = len(set(data["user_id_str"])) $ print("Number of tweets at this period:", tweets_num) $ print("Number of users that tweets at this period:", users_num) $ print("Average tweets per user", tweets_num / users_num)
df_user_info['activated'].value_counts()
df2[df2.group == 'control'].converted.mean()
index = pd.date_range(end='%s-23:00'%date, periods=7*24, freq='h') $ prices = df[::2].transpose().stack() $ prices.index = index
data = read_acs_5year_data(2009, 'ak', 1)
stories['created_hour'] = stories.created_at.map(lambda x: x.hour)
fairshare.drop(columns = 'GROUP', inplace = True) $ fairshare.rename(columns = {'ACCOUNT': 'Group'}, inplace = True)
matrix_product = tf.matmul(tf_mat1, tf_mat2) $ matrix_sum = tf.add(tf_mat1, tf_mat2)
map_label = test['dataset']['column_names']
from sklearn.linear_model import LogisticRegression $ from sklearn.model_selection import train_test_split $ from sklearn.metrics import accuracy_score, roc_auc_score,f1_score $ from sklearn.metrics import confusion_matrix $ from sklearn.grid_search import GridSearchCV $
s.find_all('a')
hr[hr["icustay_id"]==14882].plot(x="new charttime", $                                  y=["chart delta"])
MATTHEWKEYWORD = get_tweets(EVENTS['matthew'], None)
sel=[Measurement.date, $      func.sum(Measurement.prcp)] $ day_prcp=session.query(*sel).filter((Measurement.date>=year_ago)).filter((Measurement.date<'2017-08-04')).group_by(Measurement.date).all() $
snow.select("select count(distinct patient_id) from nk_albatross_ftd where left(patient_id, 5) != 'XXX -' and patient_id in (select distinct patient_id from nk_albatross_psp)")
print_words_for_tag(classifier_tfidf, 'c', mlb.classes, tfidf_reversed_vocab, ALL_WORDS) $ print_words_for_tag(classifier_tfidf, 'c++', mlb.classes, tfidf_reversed_vocab, ALL_WORDS) $ print_words_for_tag(classifier_tfidf, 'linux', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)
newdf.index = newdf.yearmonth
imgp.shape
df = pd.read_csv('http://ix.cs.uoregon.edu/~lowd/aqi-lanecounty-2012-2017.csv') $ df.head()
df['time_elapsed'] = current_date - df['dateCreated'] $ display(df['time_elapsed'].head())
data2.head()
dir(y) # Shows all the properties of this time period objecu
df = pd.read_excel('PPB_gang_records_UPDATED_100516.xlsx')
ad_groups_zero_impr.groupby( $     ['CampaignName', 'Date'] $ ).groups
n = len(joined); n
long_microseconds = datetime.datetime(2015, 12, 31, 23, 59, 12, 999999).strftime("%f") $ print("long", long_microseconds) $ short_microseconds = datetime.datetime(2015, 12, 31, 23, 59, 12, 999999).strftime("%f")[:-3] $ print("short", short_microseconds)
df3 = df2[df2['user_id'].duplicated()] $ print('The one user_id that is repeated is:  ' + str(df3.iloc[0][0])) $
new_page_converted.mean() - old_page_converted.mean()
df.mean() #mean 
dft['stamp_round'] = dft.stamp.dt.round('1Min')
breed_predict_df_clean.head(1)
%%sql mysql://user1:logger@172.20.101.81/pidata $ select * from temps3;
testheadlines = test["text"].values $ basictest = basicvectorizer.transform(testheadlines) $ predictions = basicmodel.predict(basictest)
browser.click_link_by_partial_text('more info')
from matplotlib.patches import Rectangle
autos.groupby(['odometer_group'])['price'].mean()
comment_rate = ncptable[["author", "num_comments"]] $ comment_rate = pd.merge(comment_rate, posts, how="left", on="author") $ comment_rate["rate"] = comment_rate["num_comments"] / comment_rate["links"] $ comment_rate.sort_values(inplace=True, by="links", ascending=False) $ comment_rate[0:9].sort_values(by="rate", ascending=False)
number_of_commits = git_log['timestamp'].count() $ number_of_authors = len(git_log['author'].dropna().unique().tolist()) $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
Class_frame.head()
all_features = train['feature_list'].str.cat(sep=',')
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='larger') $ print(z_score) $ print(p_value)
tweets.retweeted.value_counts()
import statsmodels.api as sm $ convert_old = df2.query('landing_page=="old_page" and converted==1').shape[0] $ convert_new = df2.query('landing_page=="new_page" and converted==1').shape[0] $ n_old = df2.query('landing_page=="old_page"').shape[0] $ n_new = df2.query('landing_page=="new_page"').shape[0] $
twitter_archive_clean=twitter_archive_clean[twitter_archive_clean.rating_numerator<15]
tweet_json_clean.info()
pold = df2.converted.mean() $ pold
from datetime import datetime $ def convert_date_to_datetime(string_time): $     return datetime.strptime(string_time,'%Y-%m-%d') $ df['datetime'] = df.date.map(convert_date_to_datetime)
image_copy['p1']=image_copy['p1'].str.replace('_',' ') $ image_copy['p2']=image_copy['p1'].str.replace('_',' ') $ image_copy['p3']=image_copy['p1'].str.replace('_',' ')
vectors = pd.concat([pos_vectors, neg_vectors]) $ targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index]) $ labels = list(pos_vectors.index) + list(neg_vectors.index)
df['TOTAL_PAYMENT'].describe()
grader = Grader()
%%time $ df = pd.read_csv("data/311_Service_Requests_from_2010_to_Present.csv",nrows=50000)
season13["InorOff"] = "In-Season"
tw_clean.drop_duplicates(subset='expanded_urls', inplace=True)
fig, ax = plt.subplots(1, figsize=(12,4)) $ plot_with_moving_average(ax, 'Seasonal AVG RN/PAs', RN_PA_duration, window=52)
brand_mean_prices = {} $ for brand in brands: $     mean_price = autos.loc[autos["brand"]==brand,"price"].mean() $     brand_mean_prices[brand] = mean_price $ brand_mean_prices
df_cal.is_all_day.value_counts()
from sklearn.naive_bayes import GaussianNB $ gnb = GaussianNB() $ gnb.fit(X_train, Y_train)
df = pd.DataFrame.from_csv('tweetdata', header = 0, sep='~,~', index_col=None).dropna() $ df['date']=pd.to_datetime(df['date'])
url = "http://www.hotashtanga.com/p/letoltesek-downloads.html" $ html_txt = urllib.request.urlopen(url).read() $ dom =  lxml.html.fromstring(html_txt) $ for link in dom.xpath('//a/@href'): # select the url in href for all a tags(links) $     print(link)
print(mean_squared_error(df['log_AAPL'][2:],AAPL_pred))
cityID = '4fd63188b772fc62' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Laredo.append(tweet) 
pos_data_train = data_train.query("rating >= 4") $ pos_data_test = data_test.query("rating >= 4")
learner.fit(lrs, 1, wds=wd, use_clr=(20, 10), cycle_len=15)
eda(billtargs)
rush_hours = [5,6,7,8,15,16,17,18] $ train['is_rushhour'] = train['start_timestamp'].map(lambda x: 1 if x.hour in rush_hours else 0)                      $ test['is_rushhour'] = test['start_timestamp'].map(lambda x: 1 if x.hour in rush_hours else 0)
omdb_df.info()
nba_df.head(10)
pd.merge(d1, d3, left_on='city', right_on='place').drop('place', axis=1)
df.values
highc = closep.max() $ lowc = closep.min() $ print('Largest change bewteen any two days (not consecutive) is:',highc-lowc)
items.shape
pd.Series([1, True, 's'])
week37 = week36.rename(columns={259:'259'}) $ stocks = stocks.rename(columns={'Week 36':'Week 37','252':'259'}) $ week37 = pd.merge(stocks,week37,on=['259','Tickers']) $ week37.drop_duplicates(subset='Link',inplace=True)
plt.scatter(rets.RGSE,rets.TAN)
df = df_providers[['drg3','disc_times_pay','year']].sort_values(['drg3','year'],ascending=[True,True]) $ df.head() $
fullData = pd.concat([tweets, sentiment_df], axis=1) $ fullData.head()
df.groupby('Single Name').sum()
test_collection.find({"account": "deluxetattoochicago"}).count()
df['Descriptor'][df['Complaint Type'] == 'Street Condition'].value_counts().head()
age_up70 = data[data['AGE'] > 70]
results = crowdtruth.run(data, config)
cityID = '7d62cffe6f98f349' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Jose.append(tweet) 
sns.jointplot(x='number_of_claims', y='prosecution_period', data=utility_patents_subset_df, kind="kde") $ plt.show()
result = model.model(q, T, n_iterations, top_bc, bot_bc)
from datetime import datetime, timedelta
float(trips_sorted_passenger["overlap"].sum()) / len(trips_sorted_passenger)
mapping = pd.read_excel('store_url_mapping.xlsx') $ mapping['rpc'] = mapping['rpc'].astype(str) $ all_data['rpc'] = all_data['rpc'].astype(str) $ mapping.rename(columns={'Store':'store'}, inplace=True) $
authors_by_project_and_commit_df = raw_authors_by_project_and_commit_df.select( $     "project_name", "Author", extract_email("Author").alias("email"), extract_name("Author").alias("name"), $     F.to_date(strip_junk("CommitDate"), format="EEE MMM d H:mm:ss YYYY ").alias("CommitDate"))
cd ~/kmer-hashing/sourmash/lung_cancer/
os.makedirs('data/tmp', exist_ok=True) $ scaled.to_feather(file_name)
control = df2.query("group == 'control'") $ prop_conv_ctrl = len(control.query("converted == '1'")) / control.shape[0] $ print('Given that an individual was in the control group, the probability they converted is {}'.format(prop_conv_ctrl))
autos["price"].unique().shape
train_df.info()
twitter_data.head()
train=pd.get_dummies(train,columns=['air_genre_name'])
split_and_stem2(shows['keywords'][5])
data2 = pd.DataFrame(data=[tweet.text for tweet in lista], columns=['Tweets'])
print('Spell checking') $ en_df['spelling_errors'] = en_df.combined.apply(lambda x: count_mistakes(x))
df[df['Descriptor'] == 'Loud Music/Party']['Unique Key'].groupby(df[df['Descriptor'] == 'Loud Music/Party'].index.hour).count().plot()
BroncosBillsTweets.loc[4365]['text30']
df.set_index('datetime',inplace=True) $ df.index
wrd_full['favorite'].describe()
import pdfplumber $ import pandas as pd
print(len(props.prop_name.unique()))
restaurants.head(10)
clf = RandomForestClassifier(max_features=1) $ pred = cross_validate_clf(design_matrix=X_iris, classifier=clf, cv_folds=folds_iris, labels=y_iris) $ print(metrics.accuracy_score(y_iris, np.where(pred > 0.5, 1, 0)))
odometer_IQR = autos["odometer_km"].quantile(.75) - autos["odometer_km"].quantile(.25) $ odometer_LF = autos["odometer_km"].quantile(.25) - (1.5 * odometer_IQR) $ print(odometer_LF)
len(user_tweet_error)
webmap.zoom = 4
%%time $ df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', nrows=900000) $
outdir = './output' $ if not os.path.exists(outdir): $     os.mkdir(outdir)
gdax_trans_btc = gdax_trans.loc[gdax_trans['Account_name'] == 'BTC',:]
dat_missing_zip.shape
with open(home_dir + '/general_warehouse_key.json', 'rb') as a: $     Config = json.load(a) $ Historical_Raw_Data = Connect_Sql_Warehouse( $     Config, Historical_Demand_Canada, "Historical Demand")
from datetime import datetime $ datetime(year=2016, month=6, day=6, $          hour=12, minute=3, second=5, microsecond=1) $
df.shape[0]
l = LogisticRegression() $ l.fit(x_train,y_train) $ y_pred = l.predict_proba(x_test)
z_score, p_value = sm.stats.proportions_ztest( [convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print(z_score, p_value)
noise.columns
df.info()
popC15 = pd.DataFrame(yr15.groupby(by=['contact','content']).size()) $ popC15.columns = ['counts'] $ popC15 = popC15.reset_index('content') $ popC15.sort_values(by='counts', ascending=False).head(10)
autos['registration_year'].sort_values(ascending=False).head(15)
today = pd.datetime.today().date() $ begin = today - pd.offsets.Day(365.25) $ date_range = pd.date_range(begin, today) $
failures['datetime'] = pd.to_datetime(failures['datetime'], format="%Y-%m-%d %H:%M:%S") $ failures.count()
API_KEY = 'z-6V67L2Lei8_zy742pd'
Sandbox = Historical_Raw_Data.copy()
query = session.query(Station).all() $ print(len(query))
plt.figure(figsize=(7,7)) $ plt.plot(x,y,'.') $ plt.savefig('res/img/Eric_trajectory.png') # save the plot (optional)
control_group = df2.query('group == "control"') $ converted_control_group = converted_group.query('group == "control"') $ print(converted_control_group.user_id.nunique() / control_group.user_id.nunique())
def getStartDate(collectionDate): $   startDate = max(df_epi.index[df_epi.index<collectionDate]) $   return startDate.strftime('%Y-%m-%d')
crimes[(crimes['PRIMARY_DESCRIPTION']=='CRIMINAL DAMAGE')&(crimes['SECONDARY_DESCRIPTION']=='TO PROPERTY')].head() $
print(color)
run txt2pdf.py -o"2018-06-14 2148 CLEVELAND CLINIC Sorted by Discharges.pdf"  "2018-06-14 2148 CLEVELAND CLINIC Sorted by Discharges.txt"
deployment_details = client.deployments.create(model_guid, name="MNIST keras deployment")
X = dfX[fNames].values $ y = dfY.values
df.plot(x='homeWinPercentage', y='startyardsToEndzone', kind='scatter')
import pandas as pd $ import numpy as np $ import warnings $ from df2gspread import df2gspread as d2g $ warnings.simplefilter('ignore') 
inter = mr * vol $ inter = inter.between_time('9:30', '16:00') $ lagged_inter = inter.tshift(1, 'min').between_time('9:30', '16:00') $ pd.ols(y=mr, x=lagged_inter)
df2.query('group == "control" and converted == 1').count()[0]/df2[df2['group'] == 'control'].count()[0]
print(calls_df[["length_in_sec"]].hist(bins=50,figsize=(20,10)))
df = pd.DataFrame(directory) $ df
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ z_score, p_value 
df_more = df_more.join(df_more['Location'].str.split(',', 1, expand=True).rename(columns={0:'City', 1:'State'}))
data
import statsmodels.api as sm $ logit_model = sm.Logit(y, X) $ result = logit_model.fit()
'Remember when we learned about split?'.split('we')
for column in ls_cprc_columns: $     df_cprc[column].plot(kind='hist') $ plt.show()
model = gensim.models.Word2Vec(sentences, workers=4)
oil_spark = spark.createDataFrame(oil_pandas) $ oil_spark.show()
OAUTH_KEYS = {'consumer_key':CONSUMER_KEY, 'consumer_secret':CONSUMER_SECRET, $  'access_token_key':ACCESS_KEY, 'access_token_secret':ACCESS_SECRET} $ auth = tw.OAuthHandler(OAUTH_KEYS['consumer_key'], OAUTH_KEYS['consumer_secret']) $ api = tw.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)
list = [column for column in l.index.values if 0.5<abs(l[column])<1]
df.columns.values
df.head()
df_columns['Day of the week'].value_counts().plot(kind='barh') $
x
Ralston.mean() $
from pyspark.sql.functions import udf $ from pyspark.sql.types import DoubleType, StructType $ def get_pred(probability): $     return(round(probability.toArray().tolist()[1],6)) $ get_pred_udf = udf(get_pred, DoubleType())
df_twitter_copy['score_rating'] = df_twitter_copy.rating_numerator / df_twitter_copy.rating_denominator
df2.converted.mean()
sorted_degree[0:20]
user_list = list(submissions['hacker_id'].unique())
locale_name_local_hols=local_holidays['locale_name'].unique() $ print(locale_name_local_hols)
walk = (np.random.randint(0, 200, size=N) - 100) * 0.25
g = sns.FacetGrid(train, col ='loan_status') $ g.map(sns.distplot, "loan_amnt");
all_cards_columns = ['names', 'layout', 'manaCost', 'cmc', 'colors', 'colorIdentity', $                     'supertypes', 'types', 'subtypes', 'text', 'power', 'toughness', $                     'loyalty', 'rulings', 'foreignNames', 'printings', 'legalities']
df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])
text[2]
crimes.head(2)
ip[ip.duplicated()]
df[(df['outcome']=='Exposed to Potential Harm') | $    (df['outcome']=='No Negative Outcome')].count()[0]/df[df['public']=='offline'].count()[0]*100
example_features = ['sqft_living', 'bedrooms', 'bathrooms']
definition_url = client.repository.get_definition_url(definition_details) $ definition_uid = client.repository.get_definition_uid(definition_details) $ print(definition_url)
len(heap)
a_list.append(another_list) $ a_list
pd.read_csv("msft.csv", nrows=3)
noaa_data.loc[:,'PRECIPITATION'].groupby(pd.Grouper(freq='W')).sum()
tweet_df_clean.dtypes
train_view.sort_values(by=7, ascending=False)[0:10]
df2.nunique() #checking the number of unqiue ids
options_frame.head()
accounts[' Total BRR '] = accounts[' Total BRR '].map(lambda tbrr: float(tbrr.split('$')[1] $                                                                          .replace(',','') $                                                                          .replace('-','0.0'))) $ accounts[' Total BRR '] = accounts[' Total BRR '].astype(float)
data_compare['SA_mix'].plot() $ data_compare['SA_google_translate'].plot() $ data_compare['SA_textblob_de'].plot()
df2_new['intercept']=1 $ logit4=sm.Logit(df2_new['converted'],df2_new[['intercept','ab_page','CA','UK','ab_page_UK','ab_page_CA']]) $ result4=logit4.fit() $ result4.summary()
young.registerTempTable("young") $ context.sql("SELECT count(*) FROM young")
merge.committee_position.value_counts()
peakPricePerDay = dfEPEXpeak.groupby(dfEPEXpeak.index.date).aggregate('mean')['Price'] $ peakPricePerDay.head() # verify calculation
text_features_test = test_df[['name', 'desc','keywords']]
pumaPop['public use microdata area'] = pumaPop['public use microdata area'].astype('str')
X.stay.describe()
maxitems = 10 $ print "London tweets retrieve testing" $ print '----------------------------------' $ for tweet in tweepy.Cursor(api.search, q="place:%s" % place_id_L).items(maxitems): $     print tweet.text
y = analyze_set['date'] $ count = Counter(y) $ count.most_common(1)
print('Aggregated Providers - grouped by name') $ print(aggregated_providers.head()) $ agg_providers = pd.DataFrame({'name':aggregated_providers.index, 'value':aggregated_providers.values}) $ agg_providers['name'] = agg_providers['name'].str.replace(',', ' ') $
my_instance.special_print = print("x") $ my_instance.special_print
old_page_converted = np.random.binomial(1, prop_users_converted, n_old)
pd.Series(d)
mask = ((multiple_party_votes_all['vote_position'] == 'Yes') | $         (multiple_party_votes_all['vote_position'] == 'No') | $         (multiple_party_votes_all['vote_position'] == 'Not Voting')) $ multiple_party_votes_all = multiple_party_votes_all[mask]
eia_facility_fuel = eia_facility.copy() $ for key in facility_fuel_cats.keys(): $     eia_facility_fuel.loc[eia_facility_fuel['fuel'].isin(facility_fuel_cats[key]),'type'] = key $ eia_facility_fuel = eia_facility_fuel.groupby(['type', 'year', 'month']).sum() $ eia_facility_fuel.head()
cats_in = intake.loc[intake['Animal Type']=='Cat'] $ cats_in.shape
df_byzone = df_byzone.loc[:,['time_stamp_local', 'price']]
dup_labels
tweets_df.entities.value_counts()
country_df = pd.read_csv('/Users/Wei.Zhao/Documents/2017_INSIGHT/Interview code prepare/data challenges/Fraud/timezonedb/country.csv',names = ['country_code','country']) $ timezone_df = pd.read_csv('/Users/Wei.Zhao/Documents/2017_INSIGHT/Interview code prepare/data challenges/Fraud/timezonedb/timezone.csv',names = ["zone_id","abbreviation","time_start","gmt_offset","dst"]) $ zone_df = pd.read_csv('/Users/Wei.Zhao/Documents/2017_INSIGHT/Interview code prepare/data challenges/Fraud/timezonedb/zone.csv',names = ["zone_id","country_code","zone_name"])
merged2 = merged1.copy()
iowa_csv = 'file:///Users/Andrew/Downloads/iowa_liquor_sales_sample_10pct.csv'
import numpy as np $ data = np.genfromtxt('data/data.csv') $ print(data)
ks_projects['name_length'] = ks_projects['name'].apply(lambda x: len(str(x))) $ ks_projects.head(5)
df.head(3)
from statsmodels.stats.diagnostic import acorr_ljungbox $
df2[['control','treatment']]=pd.get_dummies(df2['group']) $ df2[['new_page','old_page']]=pd.get_dummies(df2['landing_page']) $ df2['intercept']=1 $ df2.head()
people_with_one_or_zero_collab = grouped_publications_by_author[grouped_publications_by_author['countCollaborators']<2]
df.groupby('Year').agg(np.size)
csvDF = pd.read_csv("C://customer.csv",header=None,names=['custid','fname','lname','age','job'])
print(ha.accounts.credit_subaccount.__init__.__doc__)
weather = weather.drop(['events','zip_code'],1)
dftop['temp'].mean()
symbols=[] $ dates=pd.date_range('2010-01-01', '2011-12-31') $ df = get_data(symbols=symbols,dates=dates)
speeches_similarity_df = pd.DataFrame(speeches_similarity) $ speeches_similarity_df['index'] = speeches_similarity_df.index
pd.read_csv(r'./CRNS0101-05-2018-NC_Durham_11_W.txt').head()
subscription_key = '269b18656960431c9cf7a2f2a943e15c' # active only for 7 days, fella
subset=data.iloc[0:10] $ subset
reddit.multireddit('bitcoin', 'programming')
net_loans=match_id(merkmale, merkmale.Merkmalcode.isin(['KR','ML'])) $ net_loans_exclude_US=drop_id(net_loans,net_loans.Merkmalcode=='US') $ net_loans_exclude_US_outstanding=drop_paid_off(net_loans_exclude_US) $
category = [] $ for i in range(avg_preds.shape[0]): $     category.append(np.argmax(avg_preds[i])) $ type(category), len(category), category[0:10]
df_arch_clean["source"].value_counts() $
df_vow.columns.values $
s1 = pd.Series(range(5)) $ s1
m3.freeze_to(-1) $ m3.fit(lrs/2, 1, metrics=[accuracy]) $ m3.unfreeze() $ m3.fit(lrs, 1, metrics=[accuracy], cycle_len=1)
train = pd.read_csv('train.csv') $ test = pd.read_csv('test.csv')
stfvect.get_stop_words()
cols = df_os.columns.tolist() + df_usnpl_one_hot_state.columns.tolist() $ media_classes = [c for c in cols if c not in ['domain', 'notes']] $ breakdown = df_questionable_3[media_classes].sum(axis=0) $ breakdown.sort_values(ascending=False).head(20)
Base.metadata.create_all(engine)
shots_df
autos['price'].unique().shape[0]
df_ec2[df_ec2['AvailabilityZone'].isnull()]['UsageType'].unique()
run txt2pdf.py -o"2018-06-18  2015 853 disc_times_pay.pdf"  "2018-06-18  2015 853 disc_times_pay.txt"
range(0, af.length.max(), 100000)
archive_df_clean.info()
label_encoder = preprocessing.LabelEncoder() $ df1['loan_status'] = label_encoder.fit_transform(df1['loan_status']) $ y_loadtest = df1['loan_status'].values
import pandas as pd
df2 = pd.read_csv("../../data/msft.csv",usecols=['Date','Close'],index_col=['Date']) $ df2.head()
df_developer_blog=pd.ExcelFile('/Users/nikhil.mogare/Desktop/DSA_Reporting/Week3_Sep19/developer_blog.xlsx')
result.summary2()
ufos_df2.show(3)
import requests $ from collections import namedtuple, OrderedDict, Counter $ import pprint $ pp = pprint.PrettyPrinter(indent=2)
df2['intercept'] = 1 $ df2[['control','ab_page']] = pd.get_dummies(df2['group']) $ df2.head()
pax_raw.paxstep.max() <= step_threshold
print(hr["realtime"][0]) $
df_variables = pd.merge(df_variables,df_features,how="left",on="CustID",indicator=True)
cols = recent.columns.tolist() $ cols = [(cols[0])] + [cols[5]] + [cols[7]] + cols[1:5] + [cols[6]] $ recent = recent[cols] $ recent.head()
from sklearn.linear_model import LogisticRegression $ LR = LogisticRegression(C=0.01).fit(X_train,y_train) $ LR
print('R^2 train: %.3f' % (r2_score(y, y_predit)))
twitter_df.to_csv('../Data_sources/twitter_df.csv')
df_pilots.head(5)
df1 = pd.DataFrame(np.random.randn(6,3),columns=['col1','col2','col3']) $ df1
converted = ts.asfreq('45Min', method='pad')
print 'number of records with 0 conversions: ', np.size(zeroConvs), '\n' $ print 'number of records with 0 conversions by datestamp: ', '\n', df_zeroConvs.groupby('datestamp').size(), '\n' $ print 'number of records with 0 conversions by country: ', '\n', df_zeroConvs.groupby('country_code').size(), '\n' $ print 'number of records with 0 conversions by marketing_channel: ', '\n', df_zeroConvs.groupby('marketing_channel').size(), '\n'
prev_year = dt.date.today() - dt.timedelta(days=365) $
lr.fit(X_train, y_train.iloc[:,2])
artists_info = [sp.artist(artist_id[i]) for i in range(0,num_songs)]
engine = create_engine('postgresql://maasr:UWCSE414@dssg18.cofuftxjqsbc.us-east-1.rds.amazonaws.com/dssg18')
df.columns
pd.DataFrame(rows)
eug_cg_counts = aqi.groupby(['AQI Category_eug', 'AQI Category_cg']).size() $ eug_cg_counts.unstack(fill_value=0)
data['battery_level'].value_counts().sort_index().plot(kind='bar', figsize=[25, 5])
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])
daily_averages['2014-10'].head()
print('Rows: ', df.shape[0], '\nColumns: ', df.shape[1])
for i in ['pts', 'oreb', 'dreb', 'reb', 'ast', 'stl', 'blk', 'to']: $     main["{}_ta_d".format(i)] = main['{}_ta'.format(i)] - main['{}_ta_opp'.format(i)] $ for i in ['pts', 'oreb', 'dreb', 'reb', 'ast', 'stl', 'blk', 'to']: $     main["{}_tb_d".format(i)] = main['{}_tb'.format(i)] - main['{}_tb_opp'.format(i)]
def get_residual_sum_of_squares(input_feature, output, intercept, slope): $     predictions = input_feature * slope + intercept $     residuals = predictions - output $     RSS = (residuals ** 2).sum() $     return(RSS)
len(pax_raw)
Recent_Measurements.describe()
import pandas as pd $ import numpy as np
data = df.drop_duplicates(subset='hash_id')
merged = pd.merge(pop, abbrevs, how='outer', $                  left_on='state/region', right_on='abbreviation') $ merged = merged.drop('abbreviation', 1) # drop duplicate info $ merged.head()
np.all(x == 6)
stock_return = stocks.apply(lambda x: x / x[0]) $ stock_return.head()
master_list.sort_values(by='Count', ascending=False).head(10)
tvec.fit(X_train) $ tvec_df  = pd.DataFrame(tvec.transform(X_train).todense(), $                    columns=tvec.get_feature_names(), $                    index=[X_train])
packages = (('Pymongo', pymongo), ('NLTK', nltk), ('Gensim', gensim), $            ('Regex', re),('Plotly',py)) $ for package in packages: $     print('{0} version: {1}'.format(package[0],package[1].__version__)) $ !Python -V
pd.read_pickle('data/wx/tmy3/proc/tmy3_meta.pkl', compression='bz2').head()
dummies_df = pd.get_dummies(combined_df4, columns=['Ward', 'paymeth_code']) $ dummies_df = dummies_df.drop(['vo_propdescrip','llpg_usage'],axis=1)
words_only_sp_freq = FreqDist(words_only_sp) $ print('The 100 most frequent terms (terms only): ', words_only_sp_freq.most_common(20))
print("The probability of an individual converting is: {}".format(df['converted'].mean()))
s = Series(randn(len(rng)), rng) $ s.head()
base_model_relation
data['processing_time'] =  data['Closed Date'].subtract(data['Created Date'])
from cfg import rebalance_month $ importlib.reload(util) $ tagged_df = util.tag_rebalance_date(target_pf, period = rebalance_month.rebalance_period)
X = train.title_author $ y = train.popular $ cross_val_score(pipe, X, y, cv=5, scoring='roc_auc').mean()
np.exp(0.0043)
cur = conn.cursor() $ cur.execute('ALTER TABLE actor ALTER COLUMN middle_name TYPE text;') $ df = pd.read_sql('SELECT * FROM actor', con=conn) $ df.head()
autos["registration_year"].value_counts(normalize=True).sort_index(ascending=False).head(20)
output_variables
os.getcwd()
os.listdir("./")
df_spend
rt.head()
Year3_df = Distribution_df.drop(Distribution_df.index[:24]) $ Year1_df = Distribution_df.drop(Distribution_df.index[12:]) $ Year2_df = Distribution_df.iloc[12:24]
db.show(load_buf_array)[:]['schema'][0]
noaa_data.loc[:,'AIR_TEMPERATURE'].groupby(pd.Grouper(freq='w')).mean()
timezones = DataSet['userTimezone'].value_counts()[:10] $ print(timezones)
len(repos.id.unique())
df_meta = pd.read_table('data/meta_data.tsv', encoding='latin-1') $ df_meta.head()
obj.unique()
r1.json()
load2017['hour'] = load2017['time'].str.slice(11,16) $ load2017['hour'] = pd.to_datetime(load2017['hour'], format = '%H:%M').dt.time $ load2017['hour'].head()
%pwd $ os.chdir(os.getcwd() + os.sep + "STARD")
X.head()
stocks.dtypes # Inspecting datatypes
Measurement = Base.classes.Measurement $ Station = Base.classes.Station
varx=x.var() $ vary=y.var() $ print(varx) $ print(vary)
from sklearn.multiclass import OneVsRestClassifier $ from sklearn.linear_model import LogisticRegression, RidgeClassifier
df=df[["tripduration","date"]] $ df.head()
writers
learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)
df.iloc[[11,24, 37]]
elon['nlp_text'] = elon.text.apply(lambda x: tokenizer.tokenize(x.lower())) $ elon.nlp_text = elon.nlp_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x]) $ elon.nlp_text = elon.nlp_text.apply(lambda x: ' '.join(x))
tweets.head()
(mydata / mydata.iloc[0] * 100).plot(figsize = (15, 8)); # 15 deals with the width and 8 deals with the price. $ plt.show() # this function always plots the price using matplotlib functions. $
grid_size = 15 $ diff = 0.1/grid_size
d = corpora.Dictionary.load(fps.dictionary_fp) $ c = CorpStreamer(d, fps.corp_lst_fp, inc_title='Y') $ bow_c = BOWCorpStreamer(d, fps.corp_lst_fp, inc_title='Y')
new_action = action.drop_duplicates(subset=['article_id', 'project_id', 'user_id', 'user_type'], keep='first') $ print(new_action.shape) $ new_action.head() 
df1 = pd.read_csv('https://raw.githubusercontent.com/kjam/data-wrangling-pycon/master/data/berlin_weather_oldest.csv') $ df1.head(2)
new_samp = df2.sample(df2.shape[0], replace=True) $ new_page_converted = new_samp.query('landing_page == "new_page"').converted.mean() $
diffs_evening = [] $ for _ in range(10000): $     new_evn_con = np.random.choice(a=[1,0], size=eve_new, replace=True, p=[eve_prob, 1-eve_prob]).mean() $     old_evn_con = np.random.choice(a=[1,0], size=eve_old, replace=True, p=[eve_prob, 1-eve_prob]).mean() $     diffs_evening.append(new_evn_con - old_evn_con) $
writer = pd.ExcelWriter('my_dataframe.xlsx') $ merged_df.to_excel(writer, 'Sheet1') $ writer.save()
corrplot(rr.loc[start_date:end_date].corr(), annot=True) $ plt.title('Correlation matrix - monthly data \n from ' + start_date + ' to ' + end_date) $ plt.show()
y = my_data["Drug"] $ y[0:5]
speakers = pd.DataFrame(speakers)
ts.index
all_lum.iloc[0:1]
len(data_2017_subset[data_2017_subset.borough.str.match('(Unspecified)', na=False)])
low_temps[3].head()
from nltk.corpus import stopwords $ print(stopwords.words('english'))
twitter_df_clean['source'] = twitter_df_clean['source'].str.extract('((?<=>).*(?=<))', expand=True)
logins.info()
StationCount = session.query(Station.id).count() $ print(f'There are {StationCount} stations.')
pd.value_counts(ac['IFI Support']).head(10)
df_predictions_clean.head()
providers_schedules.info()
df["pickup"] = get_datetime(df["pickup"]) $ df["dropoff"] = get_datetime(df["dropoff"]) $ df["created_at"] = get_datetime(df["created_at"]) $ df["updated_at"] = get_datetime(df["updated_at"])
abc.to_csv('Completed sepsis pass.csv',index=False)
len(submission.ImageId.values)
X_test_dtm = pd.DataFrame(cvec.transform(X_test.title).todense(), columns=cvec.get_feature_names())
subset = subset.filter(lambda p:\ $                        p["payload/info/subsessionLength"] is not None\ $                        and p["meta/Timestamp"] is not None\ $                        and p["meta/creationTimestamp"] is not None)
newdf.reset_index(inplace=True) $ newdf['Date'] = newdf['Date'].apply(lambda x: str(x)[0:7]) $ newdf.set_index('Date',inplace=True)
gene_df.sort_values('length').head()
p = model.predict(X_kaggle, num_iteration=model.best_iteration)
df_biname.rate.hist() $ plt.title('two-word name users (rate)') $ plt.show()
new_page = df2.query('landing_page == "new_page"')['user_id'].count() $ total = df2.shape[0] $ new_page_prob = new_page / total $ new_page_prob 
ser3[-3:]
complete_wind_df.head()
ss = StandardScaler()
tweet_json = pd.read_json('tweet_json.txt',lines=True) $ tweet_json.to_csv('tweet_json.csv')
data_scraped.head(3)
likes_max = np.max(data['Likes']) $ print(likes_max) $ retweet_max  = np.max(data['RTs']) $ print(retweet_max)
url = "https://www.predictit.org/api/marketdata/ticker/RDTS." + str(endDay.month).rjust(2, '0') + str(endDay.day + 7).rjust(2, '0') + str(endDay.year)[-2:]
html_table = df.to_html() $ html_table
df = df[(((df.Longitude > min_lon) & (df.Longitude < max_lon)) & \ $          ((df.Latitude  > min_lat) & (df.Latitude  < max_lat)))]
z_score , p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='larger') $ z_score , p_value
device.key
df[df['group']=='treatment'].query('landing_page != "new_page"').shape
ordered_by_susceptibility = sleep.groupby('ID').sum()['extra'].sort_values(ascending=False) $ ordered_by_susceptibility
StockData.describe()
pd.Timestamp('2012-05-01')
import copy $ w = copy.copy(sercRefl_md['wavelength']) #make a copy to deal with the mutable data type $ w[((w >= 1340) & (w <= 1445)) | ((w >= 1790) & (w <= 1955))]=np.nan #can also use bbw1[0] or bbw1[1] to avoid hard-coding in $ w[-10:]=np.nan;  # the last 10 bands sometimes have noise - best to eliminate $
df.converted.mean()  #returns average of a value
t2.tweet_id=t2.tweet_id.astype(str)
df.tail() $
df.apply(np.cumsum)
readRenviron("~/.Renviron")
genre_vectors.columns
df.info()
n = pd.read_sql_query(QUERY, conn) $ n
lr_mod = sm.Logit(df2['converted'], df2[['intercept', 'treatment']]) $ lr_fit = lr_mod.fit()
import pybaseball as pyb $ import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
train_x = encodedlist $ print np.asarray(train_x).shape
ideas = categorical.join(ideas)
dfX_hist.head()
xtx, xty = get_covariance(returns, index_weighted_returns) $ xtx = pd.DataFrame(xtx, returns.index, returns.index) $ xty = pd.Series(xty, returns.index) $ helper.plot_covariance(xty, xtx)
%%bash $ gcloud config set project $PROJECT $ gcloud config set compute/region $REGION
cvec = CountVectorizer(stop_words='english', max_features=500) $ tvec = TfidfVectorizer(stop_words='english', max_features=500)
Xs.to_csv('just_subreddit', index=False)
is_08A = (restaurants["VIOLATION CODE"] == "08A") $ violations08A = restaurants[is_08A] $ violations08Acounts = violations08A['BORO'].value_counts() $ violations08Acounts
city_eco["economy"] = city_eco["eco_code"].astype('category') $ city_eco
df_new.groupby(['country', 'group'], as_index =False).count()[['country','group', 'landing_page']]
station_availability_df['dock_id'].plot(kind='hist', logx = True, logy=True) $ plt.show() $ station_availability_df = station_availability_df.groupby('dock_id').filter(lambda x: len(x) >= 1440) $ station_availability_df['dock_id'].plot(kind='hist', logx = True, logy=True) $ plt.show()
import pandas as pd $ import os $ os.chdir('C://Users/dane.arnesen/Documents/Projects/pytutorial/')
train.iloc[[0, 1, 4]]
hdf['Age'].mean(level=0) # simple as that
reddit_comments_data.groupby('controversiality').count().orderBy('count', ascending = False).show(100, truncate = False)
nold = df2.query('group == "control"').user_id.nunique() $ nold
new_page_converted = np.random.binomial(1, p_null, n_new) $ new_page_converted.mean()
noise_graf = noise_graf.rename(columns={"Descriptor":"noise_count"})
to_be_predicted_Day1 = 51.80 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
first_main = main_deduped.filter(lambda p:\ $                                     p.get("payload/info/previousSessionId") == None and\ $                                     p.get("payload/info/reason") == "shutdown")
countvec = CountVectorizer(analyzer='word', ngram_range = (1,1),max_features=500) $ tfidfvec = TfidfVectorizer(analyzer='word', ngram_range = (1,1),max_features=500)
pred_probas_under = log_reg_under.predict_proba(X_test)
churned_bool = pd.Series([USER_PLANS_df.loc[uid]['status'][np.argmax(USER_PLANS_df.loc[uid]['scns_created'])] =='canceled' for uid in USER_PLANS_df.index],index=USER_PLANS_df.index)
treatment_convert = treatment[treatment['converted']==1].user_id.nunique()/treatment['user_id'].nunique() $ treatment_convert
speeches_df3['text'] = [text.replace('\n', '').replace('  ', '') for text in speeches_df3['text']]
df_inner_users_sess = pd.merge(users,sessions,how='left',on='UserID') $ df_inner_users_sess[df_inner_users_sess['Registered']== df_inner_users_sess['SessionDate'] ].reset_index().drop('index',axis=1)
df_gt.columns
print('The In/Out counter indicates the order in which cells have been executed.')
commits = EQCC(git_index) $ commits.since(start=start_date).until(end=end_date) $ commits.get_cardinality("author_uuid").by_period() $ print(pd.DataFrame(commits.get_ts()))
financial_crisis.drop('Spain defaults 7x', inplace = True) $ print(financial_crisis)
station_count = session.query(func.count(distinct(Measurement.station))).all() $ print ("There are" + " " + str(station_count[0][0]) + " " + "unique stations")
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = sm.graphics.tsa.plot_acf(resid_713.values.squeeze(), lags=40, ax=ax1) $ ax2 = fig.add_subplot(212) $ fig = sm.graphics.tsa.plot_pacf(resid_713, lags=40, ax=ax2)
%sql mysql://root@localhost:8889/DB_Assignment3
def calcLogNormalStockReturns(stockPrices = np.empty(0), n_days=1):   $     n_day_lognormal_returns = np.empty(len(stockPrices) - n_days) $     for i in range(len(stockPrices) - n_days):      #n-day training return; use "-n" to omit oldest n days that won't have a pair with which to compare $         n_day_lognormal_returns[i] = np.log(float(stockPrices[i]) / float(stockPrices[i+n_days])) $     return n_day_lognormal_returns
pd.Timestamp(2018, 4, 23)
df_grp = df.groupby('group') $ df_grp.describe()
DataAPI.write.update_indicators(["HIGH", "LOW", "IPO_LISTDAYS"], trading_days, override=False, log=True)
ts.resample('T').mean().dropna()
! pwd $ ! ls -la
autos['price'].value_counts().sort_index()
df2['quarter_opened'] = df2['account_created'].map(lambda x: int((x.month+2)//3))
Y_tweet.max()
tt1['ZIPCODE'] = tt1['ZIPCODE'].astype('int')
new_page_converted = np.random.binomial(1, p_new, n_new); $ new_page_converted
flights2.loc[:,"January"]
from sklearn.feature_extraction.text import TfidfVectorizer $ tf = TfidfVectorizer(max_df=0.8, max_features=200000, $                      stop_words=stop, preprocessor=remove_nums, $                                  use_idf=True) $ X2 = tf.fit_transform(corpus) $
loans = pd.read_csv('lending-club-loan-data/loan.csv') $ loans.head(2)
model_lm2 = LogisticRegression() $ model_lm2.fit(X2_train, y2_train) $ y2_preds = model_lm2.predict(X2_test) $ confusion_matrix(y2_test, y2_preds)
tweet_df.info()
n_new
y.values
df_tte_all.to_csv('tte_may.csv')
print 'Black-Scholes call value %0.2f' % black_scholes_call_value(S, K, r, t, vol) $ print 'Black-Scholes put value %0.2f' % black_scholes_put_value(S, K, r, t, vol)
df2['Change']=df2['Open']-df2['Close']
TEXT.vocab.stoi['the']
caps2_output_norm = safe_norm(caps2_output, axis=-2, keep_dims=True, $                               name="caps2_output_norm")
print(len(w.index_handler.booleans['step_0'][subset_uuid]['step_1']['step_2'].keys())) $ w.index_handler.booleans['step_0'][subset_uuid]['step_1']['step_2'].keys()
Counter(tag_df.values.ravel()).most_common(5)
run txt2pdf.py -o"2018-06-12-0815 MAYO CLINIC - SAINT MARYS HOSPITAL - 2011 Percentiles.pdf" "2018-06-12-0815 MAYO CLINIC - SAINT MARYS HOSPITAL - 2011 Percentiles.txt"
len(df)
df_first_published_at.first_published_at = df_first_published_at.first_published_at.replace({'1970-01-01 00:00:00.000009999': 'NaN'})
pos_tweets = [ tweet for index, tweet in enumerate(twitter_data['OriginalTweets']) if twitter_data['Sentiment'][index] > 0] $ neu_tweets = [ tweet for index, tweet in enumerate(twitter_data['OriginalTweets']) if twitter_data['Sentiment'][index] == 0] $ neg_tweets = [ tweet for index, tweet in enumerate(twitter_data['OriginalTweets']) if twitter_data['Sentiment'][index] < 0]
print(type(df1))
fin_df.isnull().any()
X_test=tok.texts_to_sequences(X_test) $ x_test=sequence.pad_sequences(X_test,maxlen=maxlen)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
df2_treat = df2.query('group == "treatment"').converted.mean() $ df2_treat
del num_features, StandardScaler, SS
regression = np.corrcoef(df_merged['repair_time'], df_merged['2017_homeAverage']) $ regression
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) $ history = model.fit(X, y, epochs=1, batch_size=500, validation_split=0.1)
tweet_df_clean['tweet_id'].isin(tweet_archive_clean['tweet_id']).value_counts()
df_ab_raw = pd.read_csv('ab_data.csv') $ df_ab_raw.head()
debate_evolution=combine(df_a, df_a_com) $ creation=combine(df_b, df_b_com) $ prochoice=combine(df_c, df_c_com) $ prolife=combine(df_d, df_d_com)
df.drop_duplicates('title', inplace = True)
for category in page.categories() : $     if not category.isHiddenCategory() : $         print (category.title())
df = git_log['2007':'2018'] $ df['timestamp'].hist(bins=200, figsize=(10,8)) $ df[df['author'] == 'Linus Torvalds']['timestamp'].hist(bins=200, figsize=(10,8))
df = pd.DataFrame({'Count 1': 100 + np.random.randint(-5, 10, 9).cumsum(), $                   'Count 2': 120 + np.random.randint(-5, 10, 9)}, index=dates) $ df
df.query('(group == "treatment" and landing_page != "new_page") or (group != "treatment" and landing_page == "new_page")').shape[0]
retweet_sum = twitter_df_merged.groupby('date', as_index = False).sum() $ retweet_sum = retweet_sum.sort_values(by = "date", ascending=False) $ retweet_sum['date'] = pd.to_datetime(retweet_sum['date'], format = "%m-%d-%Y") $ retweet_sum = retweet_sum.sort_values(by = "date") $ retweet_sum.set_index('date', inplace=True) $
df['text'] = df['text'].apply(lambda x: x.decode("utf-8") ) $ df['datetime'] = pd.to_datetime(df['datetime'])
!rm tmp.json
aapl.index
old = df2[df2['landing_page']=='old_page'] $ len(old)
ozzy = Dog("Ozzy",2) $ skippy = Dog("Skippy", 12) $ filou = Dog('Filou',8)
plot_BIC_AR_model(data=RN_PA_duration.diff()[1:], max_order_plus_one=8)
load2017['actual'].isnull().sum()
last_12_precip_df.describe()
import pandas as pd $ import numpy as np $ pd.set_option('display.notebook_repr_html', False) $ pd.set_option('display.max_columns', 8) $ pd.set_option('display.max_rows', 8)
df3 = df2.unstack(1) #['var'].plot(subplots=True) $ df3
projects_csv = non_blocking_df_save_or_load_csv(projects, "{0}/projects".format(fs_prefix))
payments_all_yrs_ZERO_discharge_rank = (df_providers.loc[idx_ZERO_discharge_rank,:].groupby(['id_num','year'])[['discharge_rank']].sum()) $ payments_all_yrs_ZERO_discharge_rank = payments_all_yrs_ZERO_discharge_rank.sort_values(['discharge_rank'], ascending=[False]) $ print('payments_all_yrs_ZERO_discharge_rank.shape',payments_all_yrs_ZERO_discharge_rank.shape)
df.drop(df[df.state.isin(non_usa_states)].index, axis=0, inplace=True)
df_total = df_total[ls_columns_reordered]
df_kws.plot()
adds.to_csv(folder + "\\" + TC_adds, sep="\t", index = False) $ deletes.to_csv(folder + "\\" + TC_deletes, sep="\t", index = False)
print d.variables['trajectory'][0] $ print ''.join(d.variables['trajectory'][0])
rvs1 = stats.norm.rvs(loc=92576890.092929989,scale=0.1,size=500) $ rvs2 = stats.norm.rvs(loc=242523522.74525771,scale=0.2,size=500) $ stats.ttest_ind(rvs1,rvs2)
os.getcwd() $ os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $ list_of_files = [i for i in os.listdir(os.getcwd()) $              if i.startswith("Payments with DRG percentiles") ] $ list_of_files
psy = pd.get_dummies(psy, columns = cat_vars, drop_first=False)
stations = session.query(func.count(Station.id)).scalar() $ print(stations)
start = datetime.datetime(2010, 12, 31) $ end = datetime.datetime(2013, 12, 31) $ gspc = web.DataReader("^GSPC", 'yahoo', start, end)[['Adj Close']].pct_change() $ amzn = web.DataReader("AMZN", 'yahoo', start, end)[['Adj Close']].pct_change()
param_test2 = {'max_depth':range(1,16,2), 'min_samples_split':range(200,1001,200)} $ gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=80, max_features='sqrt',subsample=0.8),\ $                         param_grid = param_test2, scoring='log_loss',n_jobs=4,iid=False, cv=5) $ gsearch2.fit(drace_df[feats_used],y)
max(train['CompetitionDaysOpen'])
gas_df = gas_df.sort_index().loc['2016-01-01':'2017-12-31'] $ gas_df.head()
pd.datetime.strptime('2017-03-31', '%Y-%m-%d')
to_be_predicted_Day1 = 34.26 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
salary_df.head(10)
import calour as ca $ ca.set_log_level(11) $ %matplotlib notebook $ import numpy as np $ np.random.seed(2018)
airbnb_df.info(memory_usage='deep')
n_new = len(df2.query("landing_page == 'new_page'")) $ print("new_page count: {}".format(n_new))
df_geo_unique_asn.head()
hawaii_measurement_df = pd.DataFrame(results_measurement[:], columns=['Station', 'Date', 'Percipitation','Temperature']) $ hawaii_measurement_df['Date'] =  pd.to_datetime(hawaii_measurement_df['Date']) $ hawaii_measurement_df.set_index('Station', inplace=True) $ hawaii_measurement_df $
nan_tables['60min']
naive_model = Naive() $ naive_model.train(df_train,df_train['Sales']) $ y_train_pred = naive_model.predict(df_train) $ print('Error on training set ', rmspe(y_train_pred, df_train['Sales'].values))
X_d = pd.get_dummies(X, columns=['domain_d'], drop_first=True)
score_a.shape[0] / score.shape[0]
Session=sessionmaker(bind=engine) $ session=Session() $ result_set=session.query(Adultdb).first() $
df = bq.Query(query).execute().result().to_dataframe() $ df.head() $ df.describe()
data.registerTempTable("my_data")
s.groupby(['group','ID']).size().unstack()
preg.columns[10:30]
df2.loc[[1899]]
ndays = 123 $ nscen = 10 $ dates = pd.date_range('2018-02-13', periods = ndays) $ dates
df2.drop([2893], inplace=True)
negative_topic_dataferame.to_csv("negative_topics.csv")
f"So many fields about {column_name.split(SEPARATOR)[0]}, I tell you!"
from sklearn.metrics import classification_report $ print(classification_report(y_test,y_pred_rf))
new_page_users2 = float(df2.query('landing_page == "new_page"')['user_id'].nunique()) $ Newpage_p2 = new_page_users2/float(df2.shape[0]) $ print("The probability that an individual received the new page is {0:.2%}".format(Newpage_p2)) $
revisions = [revision for revision in page.revisions()] $ revisions[0]
counts_df.tail()
sns.boxplot(california.FIRE_SIZE)
df.loc[:, 'prev_location_id'] = df.groupby('episode_id').location_id.shift().fillna(0).astype('int64')
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\metrics.sas7bdat" $ df = pd.read_sas(path) $ df.head(5)
users[users['LastAccessDate'] < (pd.Timestamp.today() - pd.DateOffset(years=1))]['DaysActive'].plot(kind = 'hist') $ plt.title('Histogram of Number of Active Days \n for Users that Left Permanently') $ plt.xlim((0,2070)) $ plt.xlabel('Days Active')
from sklearn.cluster import KMeans $ num_clusters = 5 $ km = KMeans(n_clusters=num_clusters) $ km.fit(tfidf_doctopic)
mustang.wheels
munich.info()
for col in my_df_small_T.iteritems(): $     print(col)
pd.Series([1, 2, 3, 4, 5])
state_lookup.info()
df.plot()
full_clean_df.groupby('type')['rating_numerator'].mean()
df_questions_read.head(3)
in_forecast = lambda x: np.logical_and(in_domain(x), in_range(x.time, 'time')) $ print('in_forecast: %6d'%in_forecast(adm).sum())
x = Gauss_Option(96, 90, msft_mu, msft_sigma, 20, .05) $ x.payoff_fx = x.call_payoff # set the proper payoff for a call option $ x.simulate_many(ap = 0.01, rp = .01, ns = 100000) 
news_sentiment_df
plotAverageAvailBikes(519)
contrib_state = contribs.groupby('contributor_state')['amount'].sum().reset_index()
df['comments'] = [1 if x >= per_75 else 0 for x in df['num_comments']] $ df['comments'].value_counts()
df.boxplot()
train[train.id.isin(session.id.unique())].country_destination.value_counts()
msftAC[:5]
df
INQ2016.Create_Date.dt.month.value_counts().sort_index()
import geopandas as gpd $ geoLineData = '/green-projects/project-waze_transportation_network/workspace/share/Scripts/data/Borough Boundaries.geojson' $ newYork_gdf = gpd.read_file(geoLineData) $ print(type(newYork_gdf)) $ newYork_gdf.crs = {'init': 'epsg:4326'}
df_master.describe()
future_pd_30360_origpd_all[(future_pd_30360_origpd_all.id_loan==350)& (future_pd_30360_origpd_all.fk_user_investor==88)].to_clipboard()
result_df.shape
url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza' $ r = requests.get(url) $ json_data = r.json() $ pizza_extract = json_data['query']['pages']['24768']['extract'] $ print(pizza_extract) $
not_needed = ['p1_conf', 'p2_conf', 'p3_conf'] $ tweets_clean.drop(columns = not_needed, inplace = True) $ tweets_clean.head()
is_read.to_csv('data_table/read.csv', index = False)
print keywords(data_df.clean_desc[23], ratio=0.5)
plt.style.use('seaborn') $ breed_ax = breed_conf[0:51].plot(kind = "bar",figsize=(20,8)) $ breed_ax.set_title("Breeds with Highest Image Recognition", fontsize = 24) $ breed_ax.set_xlabel("") $ plt.savefig('plots/breed_ax.png', bbox_inches='tight')
df_new['week1'] = 0 $ df_new['week2'] = 0 $ df_new['week3'] = 0
options_data[options_data['EXP_MONTH'] == 7]
btc_price_df['mean'].plot(figsize = (15,7))
session.query(Stations.station,Stations.name,Stations.latitude, Stations.longitude,Stations.elevation).all() $
google_after_april = target_google[window] $ google_after_april.head()
students.T
submit_button = driver.find_element_by_name('submit') $ submit_button.click()
np.exp(results.params)
df2['css_count'] = pd.to_numeric(df.css_count)
final.dropna(inplace=True) $ final.head()
if sim: $     test = make_sim('p081434') $     train = train.drop('p081434',axis=0) $     del make_sim
plt.hist(p_diffs) $ plt.xlabel('Page Difference') $ plt.ylabel('Frequency') $ plt.title('Plot of 10K simulated Page Difference'); $ plt.show()
wrd_clean.query('name == "a"')
fh_2 = FeatureHasher(input_type='string', non_negative=True) $ %time fit2 = fh_2.fit_transform(train.device_id)
for i in TrainData_ForLogistic.columns: $     if i not in TestData_ForLogistic.columns: $         print i
X_test_dtm = stfvect.transform(X_test) $ X_test_dtm.shape
liberiaFullDf.index.get_level_values(0).value_counts()
SandP = ['MMM', 'ABT', 'ABBV', 'ACN', 'ATVI', 'AYI', 'ADBE', 'AAP', 'AES', 'AET', 'AMG', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ALXN', 'ALLE', 'AGN', 'ADS', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK', 'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'APC', 'ADI', 'ANTM', 'AON', 'APA', 'AIV', 'AAPL', 'AMAT', 'ADM', 'ARNC', 'AJG', 'AIZ', 'T', 'ADSK', 'ADP', 'AN', 'AZO', 'AVB', 'AVY', 'BHI', 'BLL', 'BAC', 'BCR', 'BAX', 'BBT', 'BDX', 'BBBY', 'BRK.B', 'BBY', 'BIIB', 'BLK', 'HRB', 'BA', 'BWA', 'BXP', 'BSX', 'BMY', 'AVGO', 'BF.B', 'CHRW', 'CA', 'COG', 'CPB', 'COF', 'CAH', 'KMX', 'CCL', 'CAT', 'CBOE', 'CBG', 'CBS', 'CELG', 'CNC', 'CNP', 'CTL', 'CERN', 'CF', 'SCHW', 'CHTR', 'CHK', 'CVX', 'CMG', 'CB', 'CHD', 'CI', 'XEC', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CTXS', 'CME', 'CMS', 'COH', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'CXO', 'COP', 'ED', 'STZ', 'GLW', 'COST', 'COTY', 'CCI', 'CSRA', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA', 'DE', 'DLPH', 'DAL', 'XRAY', 'DVN', 'DLR', 'DFS', 'DISCA', 'DISCK', 'DG', 'DLTR', 'D', 'DOV', 'DOW', 'DPS', 'DTE', 'DD', 'DUK', 'DNB', 'ETFC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'EMR', 'ETR', 'EVHC', 'EOG', 'EQT', 'EFX', 'EQIX', 'EQR', 'ESS', 'EL', 'ES', 'EXC', 'EXPE', 'EXPD', 'ESRX', 'EXR', 'XOM', 'FFIV', 'FB', 'FAST', 'FRT', 'FDX', 'FIS', 'FITB', 'FSLR', 'FE', 'FISV', 'FLIR', 'FLS', 'FLR', 'FMC', 'FTI', 'FL', 'F', 'FTV', 'FBHS', 'BEN', 'FCX', 'FTR', 'GPS', 'GRMN', 'GD', 'GE', 'GGP', 'GIS', 'GM', 'GPC', 'GILD', 'GPN', 'GS', 'GT', 'GWW', 'HAL', 'HBI', 'HOG', 'HAR', 'HRS', 'HIG', 'HAS', 'HCA', 'HCP', 'HP', 'HSIC', 'HES', 'HPE', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HPQ', 'HUM', 'HBAN', 'IDXX', 'ITW', 'ILMN', 'INCY', 'IR', 'INTC', 'ICE', 'IBM', 'IP', 'IPG', 'IFF', 'INTU', 'ISRG', 'IVZ', 'IRM', 'JBHT', 'JEC', 'SJM', 'JNJ', 'JCI', 'JPM', 'JNPR', 'KSU', 'K', 'KEY', 'KMB', 'KIM', 'KMI', 'KLAC', 'KSS', 'KHC', 'KR', 'LB', 'LLL', 'LH', 'LRCX', 'LEG', 'LEN', 'LUK', 'LVLT', 'LLY', 'LNC', 'LLTC', 'LKQ', 'LMT', 'L', 'LOW', 'LYB', 'MTB', 'MAC', 'M', 'MNK', 'MRO', 'MPC', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MAT', 'MKC', 'MCD', 'MCK', 'MJN', 'MDT', 'MRK', 'MET', 'MTD', 'KORS', 'MCHP', 'MU', 'MSFT', 'MAA', 'MHK', 'TAP', 'MDLZ', 'MON', 'MNST', 'MCO', 'MS', 'MSI', 'MUR', 'MYL', 'NDAQ', 'NOV', 'NAVI', 'NTAP', 'NFLX', 'NWL', 'NFX', 'NEM', 'NWSA', 'NWS', 'NEE', 'NLSN', 'NKE', 'NI', 'NBL', 'JWN', 'NSC', 'NTRS', 'NOC', 'NRG', 'NUE', 'NVDA', 'ORLY', 'OXY', 'OMC', 'OKE', 'ORCL', 'PCAR', 'PH', 'PDCO', 'PAYX', 'PYPL', 'PNR', 'PBCT', 'PEP', 'PKI', 'PRGO', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PXD', 'PNC', 'RL', 'PPG', 'PPL', 'PX', 'PCLN', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PSA', 'PHM', 'PVH', 'QRVO', 'QCOM', 'PWR', 'DGX', 'RRC', 'RTN', 'O', 'RHT', 'REG', 'REGN', 'RF', 'RSG', 'RAI', 'RHI', 'ROK', 'COL', 'ROP', 'ROST', 'RCL', 'R', 'SPGI', 'CRM', 'SCG', 'SLB', 'SNI', 'STX', 'SEE', 'SRE', 'SHW', 'SIG', 'SPG', 'SWKS', 'SLG', 'SNA', 'SO', 'LUV', 'SWN', 'SWK', 'SPLS', 'SBUX', 'STT', 'SRCL', 'SYK', 'STI', 'SYMC', 'SYF', 'SYY', 'TROW', 'TGT', 'TEL', 'TGNA', 'TDC', 'TSO', 'TXN', 'TXT', 'BK', 'CLX', 'COO', 'HSY', 'MOS', 'TRV', 'DIS', 'TMO', 'TIF', 'TWX', 'TJX', 'TMK', 'TSS', 'TSCO', 'TDG', 'RIG', 'TRIP', 'FOXA', 'FOX', 'TSN', 'USB', 'UDR', 'ULTA', 'UA', 'UAA', 'UNP', 'UAL', 'UNH', 'UPS', 'URI', 'UTX', 'UHS', 'UNM', 'URBN', 'VFC', 'VLO', 'VAR', 'VTR', 'VRSN', 'VRSK', 'VZ', 'VRTX', 'VIAB', 'V', 'VNO', 'VMC', 'WMT', 'WBA', 'WM', 'WAT', 'WEC', 'WFC', 'HCN', 'WDC', 'WU', 'WRK', 'WY', 'WHR', 'WFM', 'WMB', 'WLTW', 'WYN', 'WYNN', 'XEL', 'XRX', 'XLNX', 'XL', 'XYL', 'YHOO', 'YUM', 'ZBH', 'ZION', 'ZTS'] $ for company in SandP: $     qb.AddEquity(company)
ridgereg = Ridge(alpha=0.1, normalize=True) $ ridgereg.fit(X_train, y_train) $ y_pred = ridgereg.predict(X_test) $ print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
df_twitter_archive_master.groupby(['p1']).mean()['favorite_count'].sort_values(ascending=False).head(5) $
pd.DataFrame(data, index=['orange', 'red'])
%matplotlib inline $ import seaborn as sns $ sns.set()
station_data = session.query(Stations).first() $ station_data.__dict__
diff_ab_data = (treatment_convert-control_convert)
finalDf.loc[:0:]
import datetime
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.csv" $ df = pd.read_csv(path, sep =',', nrows=1200, skiprows=(1,2,5,10,60), usecols=(1,3,5,8,12)) $ print("df.head(5)\n",df.head(5)) $ print("Length of df = ",len(df))
aa='2016-09-17T09:17:46Z' $ aa.split('T')[1][:-1]
model = sm.Logit(reg_df['converted'], reg_df[['intercept', 'ab_page']])
exportOI['avg_duration'] = exportOI['event.longSum_sum_duration']/exportOI['event.longSum_total_records']
states.set_index(['day', 'location'])
dfcv.shape
trump_month_distri = pd.DataFrame(list(trump_summaried_month.items()), index = ["January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]).drop([0], axis=1).iloc[:,0] $ obama_month_distri = pd.DataFrame(list(obama_summaried_month.items()), index = ["January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]).drop([0], axis=1).iloc[:,0] $ trump_month_distri
tweets['text'].str.lower().str.contains('donald trump|president trump').value_counts()
smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name, $                                                HyperParameterTuningJobConfig = tuning_job_config, $                                                TrainingJobDefinition = training_job_definition)
returns.corr()
all_data_long[:2]
twitter_archive_clean.to_csv('twitter_archive_master.csv') $ image_predictions_clean.to_csv('image_predictions_master.csv')
s.empty
df_master[df_master['retweet_count']==61859]
sc.stop()
from scipy.stats import norm $ norm.cdf(z_score) $ norm.ppf(1-(0.05/2)) $
doc_id_list = np.array(reuters.fileids(category_filter)) $ doc_id_list = doc_id_list[doc_id_list != 'training/3267']
results = logit_stats.fit() $ results.summary()
weather_cols = ['Weather', 'TempF', 'TempC', 'Humidity', 'WindSpeed', 'WindDirection', 'Pressure', 'Precip', ] $ weather_df = weather_df[weather_cols] $ weather_df.head()
text_clf.fit(tweet_train_data, tweet_train_target)  
df4.head()
df.info() $
test = old_test.append(new_test).reset_index() $
pca=decomposition.PCA() $ stocks_pca_t1= pca.fit_transform(stocks_pca_m1)
type(df.loc['Mon'])
len(df2['user_id'])
df2[df2['user_id'].duplicated()]
pd.PeriodIndex(['2011-1', '2011-2', '2011-3'], freq='M')
df['Has_Media_Link'] = np.where(df['Media URL'].isnull(), True, False) $ df.drop('Media URL', axis=1, inplace=True)
pizza_corpus, pizza_train_model, pizza_dictionary = create_LDA_model(pizza_train, 'reviews_without_rare_words', 5, 10)
joined = join_df(joined, weather, ["State","Date"]) $ joined_test = join_df(joined_test, weather, ["State","Date"]) $ len(joined[joined.Mean_TemperatureC.isnull()]),len(joined_test[joined_test.Mean_TemperatureC.isnull()])
samples_query.count_records()
df2[df2.index==1899].index
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['closTopBotDist'] = np.where(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell']<=df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'], df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell'], df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'])
conn.execute(sql) $ conn.execute(sql) $ conn.execute(sql)
last_year = dt.date(2017, 6, 2) - dt.timedelta(days=365) $ print(last_year)
df_age_gender[0:25]
import plotly.plotly as py $ import plotly.graph_objs as go $ py.sign_in('adarsh108', 'zGjLFPqCsMfBaQdwZLnp')
dataset['text length'] = dataset['text'].apply(len)
close_ys = YS1517['Adj Close'] $ close_ys
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'useless'
dfd.zones.value_counts()
len(t.user_id.unique())
archive.rating_denominator.value_counts()
week2 = week1.rename(columns={14:'14'}) $ stocks = stocks.rename(columns={'Week 1':'Week 2','7':'14'}) $ week2 = pd.merge(stocks,week2,on=['14','Tickers']) $ week2.drop_duplicates(subset='Link',inplace=True)
iv = options_frame[options_frame['Expiration'] == '2016-03-18'] $ iv_call = iv[iv['OptionType'] == 'call'] $ iv_call[['Strike', 'ImpliedVolatilityMid']].set_index('Strike').plot(title='Implied volatility skew')
trace = go.Scatter(x=df['Date'], y=df['Likes'])
df_goog['Closed_Higher'] = df_goog.Open > df_goog.Close $ df_goog.head(2)
sc.addFile("lazy_helpers.py")
import pandas as pd $ %matplotlib inline
autos.head()
score_mean = df['score'].mean() $ score_std = df['score'].std() $ score_cutoff = score_mean + 3*score_std $ anomalies = df_subset[df_subset['score'] > score_cutoff] $ anomalies
print(df_aggregate.head())
s.index
import seaborn as sns $ sns.barplot(y="features", x="importance", data=rankImportance, palette= 'hls')
dfMonth = dfMonth.rename(columns = {'Weighted Value': 'Weighted Value (Monthly)', $                                  'Contract Value (Daily)': 'Contract Value (Monthly)'})
from pysumma.Simulation import Simulation $ from pysumma.Plotting import Plotting
plans_counts = [(plans[i],counts[i]) for i in range(len(plans))] $ sorted(plans_counts,key=lambda x:x[1],reverse=True)
print (df2.loc[df2['user_id'] == 773192])
thisWeek = thisWeek[['text', 'created_at','fromStart', 'daysFromStart', 'weekNumber', $        'dayNumber', 'hourNumber']]
words_list=get_each_word(words)
max((maxi - stock).abs())
beijing['Date'] = to_datetime(beijing['Date'])
extraRecordsForLag.dtypes
import pandas as pd
- $
df_mes['DOLocationID'].unique().shape
medals_data = medals_data.assign(log_population=np.log(medals.population))
vio.head(10)
tabulation.columns
dataAnio.to_csv('data/dataPorUbicacion_Anios_tmin.csv')
Q96P20=Graph().parse(format='ttl', $                      data=turtle) $ for row in qres: $     print("%s is a protein with a gene %s that has %s names" % row)
df2[['xx','ab_page']]=pd.get_dummies(df['group']) $ del df2['xx'] $ df2['intercept']=1 $ df2.head()
df_EMR_with_dummies.dtypes.unique()
df_small = raw_data[raw_data.num_tubes < sets_threshold] $ df_sets = raw_data[raw_data.num_tubes >= sets_threshold]
contractor_merge['month_year'].tail()
DataSet = DataSet[DataSet.userTimezone.notnull()] $ len(DataSet)
df_prep9 = df_prep(df9) $ df_prep9_ = pd.DataFrame({'date':df_prep9.index, 'values':df_prep9.values}, index=pd.to_datetime(df_prep9.index))
lagged = mr.shift(1) $ lagged.head()
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller') $ print(z_score, p_value)
n_old = len(df2.query('landing_page=="old_page"'))
the_item = 'mx5' $ data_path = "data/data_photos_mx5/"
autos.head() $ autos.info()
df['post_duration'].plot(kind='hist', alpha=.5);
volt_prof_before=pd.read_csv('../inputs/opendss/{feeder}/voltage_profile.csv'.format(feeder=_feeder)) $ volt_prof_after=pd.read_csv('../outputs/from_opendss/to_opendss/{feeder}/voltage_profile.csv'.format(feeder=_feeder))
import pandas as pd $ import numpy as np $ from sklearn.tree import DecisionTreeRegressor
engine.execute('SELECT * FROM measurement LIMIT 10').fetchall() $ Measurement = Base.classes.measurement $ precipitation_12monthsago = session.query(Measurement.prcp, Measurement.date).\ $ filter(Measurement.date > '2016-10-01').all() $ print(precipitation_12monthsago)
len(candidates)
train.info()
df1, df2, df3, df4, df5 = (pd.DataFrame(rng.randint(0, 1000, (100, 3))) for i in range(5))
df_adjusted[['adjusted_numerator', 'rating_numerator_x']].iloc[np.r_[0:5, -10:0]]
saveToFile = os.path.join(PROCESSED_PATH, 'Opportunities_with_Potential_Accounts.csv') $ zero_rev_acc_opps.to_csv(saveToFile, index = False)
u'facebook\u2019s'
autos['seller'].value_counts()
d5
print dfSF.columns $ print len(dfSF.columns)
ts.index
observations_ext_node = observations_node['|']
result1 = df2.T[0] + df3.iloc[1] $ result2 = pd.eval('df2.T[0] + df3.iloc[1]') $ np.allclose(result1, result2)
autos.loc[max_price, "price"].count()
noise_graf.to_csv('noise_graf_counts_cbg.csv')
sanfran.tables
hm_sub = grid_heatmap[grid_heatmap['pr_fire'] >= 0.25] $ fire_hm = hm_sub[['glat', 'glon', 'pr_fire']].values
mngr = dsdb.ConnectionManager(dsdb.LOCAL, user="jacksonb") $ local = mngr.connect(dsdb.LOCAL) $ local._deep_print()
print(gbm_grid_cart)
res.status_code   # Checking the HTTPS Response
!wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.119 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-GB,en-US;q=0.9,en;q=0.8" "https://storage.googleapis.com/kaggle-competitions-data/kaggle/8076/train.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1519109908&Signature=mwFZu2%2B%2FQ6UMu%2BQP4SZDmhFP%2BSyF0LfJTtKH74a2P0JWhc7K640LR88mcETS2oX7%2B4CVs1PG1U48fIKCWn0fQp%2Bz3t%2BrplrMl%2BMOUmXCiq6akD9pXPa6Vb6zV9aZoaOOSpAKO8dsU%2BNdOjKMlMoLfnfvFLHcIwzmZtN2KtiG6AG1Qjb%2BeD4DlC8plsQFqgLRetZ%2BqZqkQH4FAGhzVQiG5gDbEzsqHTharjIz2TzqhJHvxOeMDv4EBiI84Rx5nA3UokpJU5iB8niywL4%2FeV4r2xi9xynpjr5xbbMtwEyok6mbX6f3OpaYFWSFap%2FZLX3QcWYng7fwELGYKRYQr0NfkQ%3D%3D" -O "train.csv.zip" -c
import re $ import pandas as pd
sns.lmplot(x='hour', y='start', data=hours, aspect=1.5, scatter_kws={'alpha':0.2})
autos["price"].value_counts().sort_index(ascending = False).head(20)
linkNYC.head()
print('Original Grocery List:\n', groceries) $ groceries.drop('apples', inplace = True) $ print() $ print('Grocery List after removing apples in place:\n', groceries)
PRE_PATH = PATH/'models'/'wt103' $ PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'
interaction = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page','US', 'UK']]) $ result_interact = interaction.fit() $ result_interact.summary()
top_20_tweet_days
print(gMapAddrDat)
dfDay = dfDay[(dfDay['Date'].dt.year == 2018) | (dfDay['Date'].dt.year == 2019)]
f_ip_device_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_device_clicks"), header=True) $ print('Found %d observations.' %f_ip_device_clicks.count())
start = "2005-01-01" $ end = "2018-06-31" $ trading_days = fk.get_monthly_last_trading_days(start=start, end=end) $
df_goog['good_month'] = df_goog['Close'] > df_goog['Open'] $ df_goog.head()
df_goog['month'] = df_goog.index.month
for treaty in treaties.find(): $      pprint.pprint(treaty)
autos[autos["price"].between(50,1000000)]["price"].\ $ value_counts()
driver = selenium.webdriver.Safari() # This command opens a window in Safari $ driver.get('https://www.boxofficemojo.com')
user_date_frequency = user_df["date"].value_counts() $ ax = plt.subplot(111) $ ax.bar(user_date_frequency.index, user_date_frequency.data) $ ax.xaxis_date() $ plt.show()
countries_df = pd.read_csv('data/countries.csv') $ countries_df.head()
train = pd.read_excel('./Data/sentimentTrain.xlsx') $ train['splitText'] = train['SentimentText'].apply(lambda t : str(t).strip().split(' ')) $ train['trainTuple'] = train.apply(lambda row : (row['splitText'], row['Sentiment']), axis=1)
countries_df.head()
print ('FCBridge table length: ', len(data_FCBridge)) $ print ('FCInspevnt table length: ', len(data_FCInspevnt))
week25 = week24.rename(columns={175:'175'}) $ stocks = stocks.rename(columns={'Week 24':'Week 25','168':'175'}) $ week25 = pd.merge(stocks,week25,on=['175','Tickers']) $ week25.drop_duplicates(subset='Link',inplace=True)
top_rsvp = df.sort_values("mean_rsvp", ascending=False).reset_index(drop=True) $ top_rsvp.head(5)
help(results.remove)
plt.scatter(X2[:, 0], X2[:, 1], c=labels, cmap='rainbow') $ plt.colorbar();
All_tweet_data_v2.Stage.value_counts()
cbg.isnull().sum()
start2 = datetime.datetime.strptime('2014-05-01', "%Y-%m-%d").timestamp() $ end2 = datetime.datetime.strptime('2015-12-31', "%Y-%m-%d").timestamp() $ stockdftest = getStockDataframe(stocklist, start2, end2, True) 
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']]) $ res = lm.fit() $ res.summary2()
pres_df = pres_df.rename(columns={'subject_count_tmp': 'subject_count'}) $ pres_df.head(2)
collection.list_items()
np.exp(-0.015)
from sklearn.metrics import r2_score, mean_squared_error, explained_variance_score $ print('R^2 test: %.3f' % (r2_score(y_test, results))) $ print(mean_squared_error(y_test, results)) $ print(explained_variance_score(y_test, results))
hist=pd.merge(users, transactions.groupby('UserID').first().reset_index(), how='left', on='UserID') $ hist.head()
pd.Timestamp('2017-03-31')
from scipy.stats import norm $ norm.cdf(z_score), norm.ppf(1-(0.05/2))
def post_to_filtered_word_vector(post): $     return [word for word in word_tokenize(post.lower()) if word not in stopwords.words('english')]
ward_crosstab = pd.crosstab(combined_df4['Ward'],combined_df4['bin_label']) $ ward_crosstab['neg_pctg']=ward_crosstab[1]/ward_crosstab[0]*100 $ ward_crosstab[ward_crosstab[1]>3].sort_values('neg_pctg',ascending=False)
np.random.seed(42) $ new_page_converted=np.random.binomial(n=1,p=P_new,size=n_new) $ (new_page_converted)
com_luz_flagrante = ((com_luz) & (df_data.FLAGRANTE_BOOL==True)) $ sem_luz_flagrante = ((sem_luz) & (df_data.FLAGRANTE_BOOL==True)) $ print('Com Luz Solar Flagrante: ', com_luz_flagrante.sum()/com_luz.sum()) $ print('Sem Luz Solar Flagrante: ', sem_luz_flagrante.sum()/sem_luz.sum())
train_data['brand_int'] = train_data['brand'].apply(get_integer5) $ test_data['brand_int'] = test_data['brand'].apply(get_integer5) $ del train_data['brand'] $ del test_data['brand']
frame2.loc['three']
from urllib.parse import quote $ from pprint import pprint
precipitation = session.query(measurement.date, measurement.prcp).\ $     filter(measurement.date >= '2016-08-23').\ $     order_by(measurement.date).all() $ precipitation
tia['date'][0][14:]
df= pd.read_csv("ab_data.csv") $ df.head()
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == True')['converted'].count() $ convert_new = df2.query('landing_page == "new_page" and converted == True')['converted'].count() $ n_old = df2.query('landing_page == "old_page"')['landing_page'].count() $ n_new = df2.query('landing_page == "new_page"')['landing_page'].count()
bb = [1,2,3,4,1,2,3,4] $ len(bb), set(bb), len(set(bb))
text_classifier.get_step_params_by_name("text_word_ngrams")        
twitter_data[twitter_data.expanded_urls.duplicated()].size
users['created_at'] = pd.to_datetime(users['created_at']) $ users['followers_count'] = pd.to_numeric(users['followers_count'], downcast='integer') $ users['friends_count'] = pd.to_numeric(users['friends_count'], downcast='integer') $ users['favourites_count'] = pd.to_numeric(users['favourites_count'], downcast='integer') $ users['statuses_count'] = pd.to_numeric(users['statuses_count'], downcast='integer')
df1 = df.drop(columns = ['Area Id', 'Variable Id', 'Symbol'])   #droping the columns
df3=df2.join(df2_curr)
df.groupby('converted').count()['user_id']/df.count()['user_id']
scores
hp.find_site(1)
print(len(train_df['ip'].unique()))
score = cross_val_score(rf, Xs, y, cv = cv, verbose = 1)
np.exp(results_model.params)
ifcdf = pd.concat([ifc2df(row) for row in cachedf.itertuples()]) $ ifcdf
print("That means there have been") $ print("%f fatalities due to mass shootings per year"% (fperyr)) $ print("and") $ print("%f people injured due to mass shootings per year" % (iperyr))
url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'
festivals.head(5)
active_num_authors_by_project = active_distinct_authors_latest_commit.groupBy("project_name").agg(F.count("Author").alias("author_count")) $ active_num_authors_by_project.cache() $ active_num_authors_by_project.show()
conversion_mean = df['converted'].mean() $ conversion_mean
tf = TfidfVectorizer(analyzer = "word", $                         ngram_range = (1, 3), $                         min_df = 0, $                         stop_words = "english")
print(cvModel2.bestModel._java_obj.getMaxIter()) $ print(cvModel2.bestModel._java_obj.getMaxDepth()) $ print(cvModel2.bestModel._java_obj.getMaxBins())
% matplotlib nbagg $ import pandas as pd $ import matplotlib.pyplot as plt
autos['date_created'].describe()
m3.load_cycle('imdb2', 4)
df_test_1 = countries_df.join(df2, how='inner') $ df_test_1.head()
last12_df.describe()
print ("Probability that individual converting regardless of the page they receive: %0.4f" % (df2.query('converted == 1').shape[0]/df2.shape[0]))
from azureml.core import Workspace $ subscription_id = "b1395605-1fe9-4af4-b3ff-82a4725a3791" $ resource_group = "meetup_aml_rg" $ workspace_name = "meetup_aml_workspace" $ workspace_region = 'eastus2' # or eastus2euap
normal_dist = np.random.normal(0, np.std(p_diffs), p_diffs.size) $ plt.hist(normal_dist); $ plt.axvline(diff, color='red');
pred.sample(10)
df2_new.country.value_counts()
pd.MultiIndex.from_product([['a','b'], [1,2]])
filename = 'Daily_Stock_Prediction_latest.pk' $ with open('./Models/'+filename, 'rb') as f: $     model_test = pickle.load(f)
store.head()
RNPA_existing_data_plus_forecast.columns
MATTHEW_WORDS_160 = ['storm','flooding','house','waters','surge','evacuate','hurricane','information','rains','country','community','level','leave','island','news','weather','coast','moving','damage','risk','forecast','neighborhood','power','rising','zone','media','decision','preparing','hotel','marsh','drain','river','emergency','mandatory','scary','wind','government','tidal','tornado','worst','raise','coastal','events','fema','highest','scare','service','block','drainage','causeway','bottom','hill','katrina','traffic','urgency','management','neighbor','worried','hospital','television','tropical','voluntary','warning','alert','barrier','broadcasting','danger','generators','landmark','newspaper','ocean','climate','computer','disability','nature','parents','plain','radar','roof','sand','underwater','channel','disaster','lower','meetings','notice','pollution',' telephone','together','afraid','approaching','electricity','guard','meteorologist','reporting','survive','warming',' wave','commissioners','connect','country','dark','deep','engineering','gulf','lake','message','patients','responsibility','roots','animals','citizens','depot','devastating','died','dogs','drove','environment','googling','intertidal','landscape','nursing','panic','police','pump','repaired','resources','respect','sewer','siren','soaked','source','spread','stuck','tributaries','washed','website','announcements','authority','break','buses','cellphone','cuts','debris','disadvantage','downstream','extreme','fire','food','foundation','hazard','marshside','recovery','shaking','swept','terrible','shelter','threat','trouble','uncertainty'] $ MATTHEW_WORDS_95  = ['storm','flooding','house','waters','surge','evacuate','hurricane','information','rains','community','leave','maps','island','news','weather','coast','damage','risk','forecast','power','rising','media','decision','preparing','emergency','mandatory','scary','wind','government','tidal','tornado','coastal','fema','scare',' service','drainage','causway','katrina','traffic','urgency','managment','mother','nieghbor','hospital','television','tropical','volutary','warning','alert','broadcasting','danger','generators','landmark','newspaper','climate',' computer','parents','radar','roof','underwater','airport','disaster','pollution','telephone','afraid','appraoching','electricity','meteorologist','survive',' wave','commissioners','gulf','devastating','died','environmet','googling','intertidal','panic','police','resources','siren','soaked','stuck','website','announcements','authority','buses',' cellphone',' debris','extreme','fire','hazard','recovery','terrible','shelter']
df2 = df[['id_partner', 'name', 'Amount']].groupby(['id_partner', 'name'], as_index=False).sum()
for f in files_txt: $     data = pd.read_csv(f, sep="|", header=None) $     df_list.append(data) $ df_full = pd.concat(df_list, axis=0)
df2['intercept'] =1 $ df2['ab_page'] =pd.get_dummies(df2['landing_page'])['new_page'] $ df2.head()
ls_other_columns = df_uro.loc[:, ls_both].columns
itemTable["Time"] = itemTable["Content"].map(time_effort)
df_reg=df2 $ df_reg.loc[df_reg['group']== 'control','ab_page']= 0 $ df_reg.loc[df_reg['group']== 'treatment','ab_page']= 1
with open('map_data.json', 'w') as the_file: $     for item in coordinates: $         the_file.write("{}".format(item)) $ pd.DataFrame(coordinates).to_json('map_data.json', orient='records')
print('Original Tweet: ' + '%s' %(train['text'][1])) $ print('\nCleaned tweet: ' + '%s' %(process_tweet(train['text'][1])))
motion_sensors_list = [entity[0] for entity in entity_id_list if 'motion' in entity[0]] # Print only the sensors $ motion_sensors_list
p_diffs=[] $ for i in range(10000): $     new=np.random.choice([0,1],n_new,p=[1-p_new,p_new]) $     old=np.random.choice([0,1],n_old,p=[1-p_old,p_old]) $     p_diffs.append(new.sum()/len(new)-old.sum()/len(old)) $
una_tweets = pd.DataFrame(una) $ una_tweets
import warnings $ warnings.filterwarnings('ignore')
appendedFrames = noHandReliableData.append(extraRecordsForLag); $ appendedFrames.shape
from sqlalchemy import create_engine, inspect $ inspector = inspect(engine) $ columns = inspector.get_columns('measurement') $ for column in columns: $     print(column['name'],column['type'])
fig = plt.figure(figsize=(12,8)) $ ax = fig.add_subplot(111) $ fig = qqplot(resid_6201, line='q', ax=ax, fit=True)
ds_train.head()
twitter_data = pd.DataFrame(list(twitter_coll_reference.find())) $ twitter_data.head()
testheadlines = test["text"].values $ basictest = basicvectorizer.transform(testheadlines) $ predictions = basicmodel.predict(basictest)
df['text_no_urls_names_nums'][4]
tsne_vectors[u'word'] = tsne_vectors.index
cur.execute("use mysql;") $ cur.execute("show tables;") $ for r in cur.fetchall(): $    print(r)
from pyspark.sql import SparkSession $ spark = SparkSession.builder.getOrCreate()
scalers, trains_scaled, valids_scaled = scale_tech_indicators(trains, valids)
[x for x in dir(total) if not "_" == x[0]]
df = df.selectExpr("_c0 as text", "_c1 as senti_val") $ df.show()
!ls -lah | grep .dpkl
import pandas as pd $ props = pd.read_csv("http://www.firstpythonnotebook.org/_static/committees.csv")
text = data["name"] $ vect = CountVectorizer(stop_words="english", min_df=3) $ X = vect.fit_transform(text) $ X = pd.DataFrame(X.toarray(), columns = vect.get_feature_names()) $
s = pd.Series([1,2,3], index =['one','two','three']) $ s
df_CLEAN1A.head()
n_conversion = df[df['converted'] == 1] # number of conversions $ unique_users_converted = n_conversion['user_id'].nunique() # number of unique users converted $ proportion_converted = ((unique_users_converted/n_unique_users)*100) $ print ("The proportion of users converted is: {:.2f}%".format(proportion_converted))
p_old = df2[df2['converted'] == 1].user_id.nunique() / df2.user_id.nunique() $ p_old
users_conv = df.converted.mean() $ print('Proportion of users converted: ', users_conv)
print s1.json()
log_mod_results.summary()
mnnb = MultinomialNB() $ mnnb.fit(X_train_dtm, y_train)
tweet_full_df.info()
len(df.index)  # number of total tweets
df_ab_page.drop(['group','landing_page','old','control','UK','US','CA','ab_page','UK_ind_ab_page'], axis=1, inplace=True)
apple_SA = apple.groupby('SA').agg({'SA':'count'}).rename(columns=({'SA': 'Count'})) $ apple_SA ['% of total'] = apple_SA['Count']/apple_SA['Count'].sum() $ apple_SA ['Sentiment'] = ['Negative', 'Neutral', 'Positive'] $ apple_SA = apple_SA.reindex(columns = ['Sentiment', 'Count','% of total']) $ apple_SA
type.__dict__['__call__'].__get__(type)
twitter_archive_clean.head(1)
df_train.head()
df_providers.shape $ idx = df_providers[ (df_providers['id_num']==id_num)].index.tolist() $ print('length of IDX',len(idx)) $ assert len(idx) > 0, 'Length of IDX is NULL' $ print( df_providers.loc[idx[0],'name'] ) $
import RPi.GPIO as GPIO $ import time
acs_df.to_csv('final_df.csv')
rf_yhat = rf.predict(X_test)
df_breed.head(3)
input_data.groupby('year').num_commits.mean()
ts.asfreq('45min',method='pad')
df2[df2['user_id'] == repeated_user_id]
df[df.Miles > 0].Day_of_week.value_counts().plot(kind='bar')
df_y_pred.values
mood=Stockholm_data_final.iloc[:,10] $ pd.value_counts(mood).reset_index()
p_old = df2.converted.mean() $ p_old #  displaying the convert rate for the control
tweets_clean.info()
trainheadlines = train["text"].values $ basicvectorizer = CountVectorizer() $ basictrain = basicvectorizer.fit_transform(trainheadlines) $ print(basictrain.shape)
sales_update = sales.ix[:,'2015 Sales':] $ sales_update.head()
df_archive_clean["tweet_id"] = df_archive_clean["tweet_id"].astype(str)
computed_aggs = map( $     lambda c: F.corr("`nonmale_percentage`", "`{0}`".format(c)), $     filter(lambda c: "sentiment.pos" in c or "sentiment.neg" in c, numeric_df.columns))
autos['price'] = autos['price'].str.replace('$', '').str.replace(',', '') $ autos['price'] = autos['price'].astype(int) $ autos['price'].head()
df_ml_52 = df.copy() $ df_ml_52.index.rename('date', inplace=True) $ df_ml_52_01=df_ml_52.copy()
twitter_final.drop(['expanded_urls','source_url','id','name1'],inplace=True,axis=1) $ twitter_final.head(2)
trn_df = full_data[:train_nrows] $ val_df = full_data[train_nrows:train_nrows+val_nrows] $ test_df = full_data[-test_nrows:]
aux = aux.merge(countries_df, how='inner', on='user_id') $ aux.country.value_counts()
from nltk.book import *
active_df.sort_values(by="station_count", ascending=False, inplace=True)
count[0]
import pandas as pd
import pandas as pd $ from datetime import timedelta $ %matplotlib inline $
m1.df
new_fan.to_csv('../data/new_fan.csv') $ return_fan.to_csv('../data/return_fan.csv')
print("nb strong outliers : {}".format(outliers_age.is_outlier.sum()))
actual_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ actual_diff
trX, trY, tsX, tsY = createDataset(preppeddata)
rdf_clf.fit(X_final[columns], y_final)
texts = df[df['section_text'].str.contains('fees')]['section_text'].values[0:5]
recommendation_df.drop(['hacker_count', 'challenge_count'], axis = 1, inplace = True)
learner = md.get_model(opt_fn, em_sz, nh, nl, $                dropouti=0.05, dropout=0.05, wdrop=0.1, dropoute=0.02, dropouth=0.05) $ learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1) $ learner.clip=0.3
ca_unit_stations = (turnstiles_daily.groupby(['C/A', 'UNIT', 'STATION','DATE_TIME']).DAILY_ENTRIES.sum().reset_index()) $ ca_unit_stations.head(5)
autos['registration_year'].describe()
from helper_functions import load_articles, compute_sentiment, plotly_plotting
taxiData2 = taxiData
corrmat = gain_df.corr() $ f, ax = plt.subplots(figsize=(6, 6)) $ sn.heatmap(corrmat, vmax=.8, square=True, annot=True) $ plt.show()
endpoint_instance = wml_credentials['url'] + "/v3/wml_instances/" + wml_credentials['instance_id'] $ header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken} $ response_get_instance = requests.get(endpoint_instance, headers=header) $ print(response_get_instance) $ print(response_get_instance.text)
df['ab_page'] = pd.Series(np.zeros(len(df)), index=df.index)
index.save('trump.index') $ index = similarities.MatrixSimilarity.load('trump.index')
pd.date_range('2005', periods=3, freq='A-JUN')
score['pred_rf'].sum()
mean1 = pd.DataFrame(df[df['dataset_location'] == 'MICU2-6FL-B1'].groupby(pd.TimeGrouper(freq = '15Min'))['ABPm'].mean().reset_index()) $ max1 = pd.DataFrame(df[df['dataset_location'] == 'MICU2-6FL-B1'].groupby(pd.TimeGrouper(freq = '15Min'))['ABPm'].max().reset_index()) $ min1 = pd.DataFrame(df[df['dataset_location'] == 'MICU2-6FL-B1'].groupby(pd.TimeGrouper(freq = '15Min'))['ABPm'].min().reset_index()) $ mock_data = pd.merge(mean1, max1, how = 'left', on = ['dataset_datetime']) $ mock_data = pd.merge(mock_data, min1, how = 'left', on = ['dataset_datetime'])
pred_clean.drop(['jpg_url','img_num'], axis=1,inplace=True);
data['freq']=np.where(data['online'] == False, 1, 0)
from bs4 import BeautifulSoup $ html_soup = BeautifulSoup(response.text, 'html.parser') $ type(html_soup)
actual_diff = df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean() $ actual_diff
learn.save('multi-label')
%%time $ df['created_at'] = pd.to_datetime(df['Created Date'], format='%m/%d/%Y %I:%M:%S %p')
aqmdata2 =  pd.read_html("https://en.wikipedia.org/wiki/List_of_mobile_telephone_prefixes_by_country", match="Afghanistan") $ data = aqmdata2[0] $ data.head() $
true_file = pd.read_csv(filepath_or_buffer=os.path.join(edfDir, 'pt1sz2_eeg.csv'), header=None) $ test_file = pd.read_csv(filepath_or_buffer=outputData, header=None)
cust_data2.reset_index(inplace=True) #create variable
df_new['morning']= pd.get_dummies(df_new['day_part'])['morning']
find_name(slices[15])
first_year = first_movie.h3.find('span', class_ = 'lister-item-year text-muted unbold') $ first_year
le_data_all = wb.download(indicator="SP.DYN.LE00.IN", start='1980',end='2014') $ le_data_all
old_page_converted = np.random.choice([0, 1], num_old, p=[1 - null_p_old, null_p_old]) $ print(old_page_converted[:10])
ol.uuid
df_clean.head(1)
url_df_full=url_df[url_df['url'].isnull()==False]
df_new['country'].unique()
support.sort_values("amount", ascending=False).head()
temp_cat.categories
from pandas.tseries.offsets import Week $ data['DayOfWeek'] = data['DATE'].dt.dayofweek $ data['SaturdayWeekEndingDate'] = data['DATE'] + Week(weekday=6)
stocks.write.format("json").save("stocks.json")
df.query('user_id == 773192')
df2 = tier1_df.reset_index() $ df2 = df2.rename(columns={'Date':'ds', 'Incidents':'y'}) $ df_orig = df2['y'].to_frame() $ df_orig.index = df2['ds'] $ n = np.int(df_orig.count())
df.subreddit.value_counts()
API_KEY = 'RGYoyz3FAs5xbhtGVAcc'
def get_child_column_names(node): $     return [column.name for column in get_child_column_data(node)]
countdf = pd.DataFrame(kochbar03) $ countdf = countdf.drop_duplicates(subset=['name', 'user']) $ countdf.info()
test_ind["Pred_state_LR"] = best_model_lr.predict(test_ind[features]) $ train_ind["Pred_state_LR"] = best_model_lr.predict(train_ind[features]) $ kick_projects_ip["Pred_state_LR"] = best_model_lr.predict(kick_projects_ip_scaled_ftrs)
a = WholeDf.groupby(by="Rating")
xml_in.dropna(subset = ['venueName', 'publicationKey', 'publicationDate'], inplace = True) $
df2['user_id'].duplicated() $ sum(df2['user_id'].duplicated())
plt.hist(new_page_converted); $ plt.axvline(x=p_new, color="red");
names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'] $ df_new.timestamp=pd.to_datetime(df_new.timestamp) $ df3 = df_new.join(pd.get_dummies(df_new.timestamp.dt.weekday_name) $                 .set_index(df_new.index).reindex(columns=names, fill_value=0)) $ df3.head()
with tf.Session() as sess: $     print(sess.run(tf_tensor)) $     print(sess.run(tf_tensor[0])) $     print(sess.run(tf_tensor[2]))
df.select('a', a_sevenPointSeven).show(5)
dfX = df2.query('group == "treatment"') $ treatment_convert = dfX.converted.sum()/dfX.count()[0] $ treatment_convert
df['dated'] = pd.to_datetime(df['dated'],unit='s') $ df['edited'] = pd.to_datetime(df['edited'],unit='s') $ df=df.rename(columns = {'flag':'Parent?'}) $ b=df.loc[:,['com_id','author','content','dated','upvotes','score','depth','distinguished','removal_reason','gilded','edited','Parent?']] $ b
convert_old = df2.query('landing_page=="old_page"').query('converted==1').shape[0] $ convert_new = df2.query('landing_page=="new_page"').query('converted==1').shape[0] $ n_old = df2.query('landing_page=="old_page"').shape[0] $ n_new = df2.query('landing_page=="new_page"').shape[0]
val_pred_svm = lin_svc_clf.predict(X_valid_cont_doc)
new_page_converted = np.random.binomial(n_new,Pnew) $ new_page_converted
obs_diff = np.array(treatment_cr - control_cr) $ (p_diffs > obs_diff).mean()
from IPython.core.display import HTML $ HTML(filename=DATA_FOLDER+'/titanic.html')
from sklearn import metrics $ print("Accuracy: %.3f" % # TODO $          )
cust_demo.info()
festivals_clean['Start_Date'] =  pd.to_datetime(festivals_clean['Start_Date'])
print(lr_gd.best_params_) $ print(lr_gd.best_score_)
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ results.summary()
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 1) & (jobs.GPU == 1)].groupby(['Group']).JobID.count().sort_values(ascending = False)
import dis $ dis.dis(Widget)
df[df.sentiment == 1].count()
giss_temp = giss_temp.where(giss_temp != "****", np.nan)
from ramutils.utils import extract_report_info_from_path $ metadata = extract_report_info_from_path(fr5_session_summary_locations[0]) $ metadata
help(h2o)
team_groups.groups
data = sh.flatten_event(raw)
reddit_comments_data.groupBy('author').agg({'sentiment':'mean'}).orderBy('avg(sentiment)').show()
pickle.dump(nmf_cv_data, open('iteration1_files/epoch3/nmf_cv_data.pkl', 'wb'))
tweet_archive.rating_denominator.value_counts()
conv_users=df.query('converted==1').user_id.nunique() $ conv_users_prop=conv_users/df['user_id'].nunique() $ conv_users_prop
archive_copy['name'] = archive_copy['name'].replace('None', np.NaN)
url='https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response=requests.get(url) $ with open(url.split('/')[-1],mode='wb')as file: $     file.write(response.content)
search.timestamp = pd.to_datetime(search.timestamp) $ search.trip_start_date = pd.to_datetime(search.trip_start_date) $ search.trip_end_date = pd.to_datetime(search.trip_end_date)
all_noms = pd.read_csv("all_nominations_confirmations_late_june.csv") $ all_noms.head()
pd.set_option('display.max_columns', 65) $ df.head() $
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 2) & (jobs.GPU == 1)].groupby(['Group']).JobID.count().sort_values(ascending = False)
n_new = df2.query('group == "treatment"')['user_id'].count() $ n_new = int(n_new) $ n_new
cfg_fnames = list(glob.glob('test-data/excel_adv_data/sample-xls-case-badlayout1*.xlsx')) $ print(len(cfg_fnames))
XGB_model.fit(X_train, y_train, eval_metric='mlogloss')
cur.copy_from(excelurl_textstream, 'jpstat_excel_urls', null="", sep='\t') # null values become '' $ conn.commit() $ logging.info('Excel URL database table updated.')
df.count()
treatment_rate = df2[df2['group'] == 'treatment']['converted'].value_counts()[1]/df2[df2['group'] == 'treatment']['converted'].count() $ treatment_rate
rtitle = [x.text for x in soup.find_all('a', {'data-event-action':'title'})] $ rtitle.pop(0) $ subreddit = [x.text for x in soup.find_all('a', {'class':'subreddit hover may-blank'})] $ rtime = [x.text for x in soup.find_all('time', {'class':'live-timestamp'})] $ comments = [x.text for x in soup.find_all('a', {'class':'bylink comments may-blank'})]
np.exp(results.params)
def page_to_df(page): $     table = page.extract_table() $     lines = table[1:] $     return pd.DataFrame(lines, columns=cols)
merge.tail() $
print(data.shape) $ data = data.groupby(['cust_id','order_date']).head(1) $ print (data.shape)
usecols = ['ip', 'app', 'device', 'os', 'channel', $ 'click_timeDay', 'click_timeHour','random_mean_encode_ip', $ 'random_mean_encode_app', 'random_mean_encode_device', $ 'random_mean_encode_os', 'random_mean_encode_channel', $ 'random_mean_encode_click_timeHour']
main = pd.read_csv("1986_2016_seasons_shifted_v1.csv") $ main.shape
meaning = pd.Series(42, ["life", "universe", "everything"]) $ meaning
df.dtypes
y4, X4 = patsy.dmatrices('DomesticTotalGross ~ Budget + G + PG + PG13 + R + Runtime', data=df, return_type="dataframe") $ model = sm.OLS(y4, X4, missing='drop') $ fit4 = model.fit() $ fit4.summary()
data = pd.Series([1, "quick", "brown", "fox"], name="Fox") $ data
df[(df.country == "es") & (df.city == "Valencia")].id.size
df_master.drop('Unnamed: 0',axis=1,inplace=True)
normed = df/df.iloc[0,:] $ normed.head()
print(g1800s.info())
noaa_data[(noaa_data.index >= '2018-05-27') & (noaa_data.index < '2018-06-03')].loc[:,'AIR_TEMPERATURE']
all_records
df2[df2['landing_page']=='old_page'].shape[0]
df.select(lowercase(df.hashtag)).show()
churned_df.iloc[18:30]
import nltk $ from nltk.corpus import stopwords
price = autos['price'] $ ax = sns.boxplot(y= price)
len(df2)
a_df.size $ b_df.size
df_input_clean = df_input_clean.withColumn("isLate", (df_input_clean.Resp_time > 1).cast('integer'))
adopted_cats.loc[adopted_cats['Color']=='Tortie/White','Color'] = 'White Tortie' $ adopted_cats.loc[adopted_cats['Color']=='White/White','Color'] = 'White' $ adopted_cats.loc[adopted_cats['Color']=='Calico/White','Color'] = 'White Calico' $ adopted_cats.loc[adopted_cats['Color']=='White/Calico','Color'] = 'White Calico' $ adopted_cats.loc[adopted_cats['Color']=='Calico/Calico','Color'] = 'Calico'
archive_df.name.value_counts()
transactions_items.head(2)
n_new = len(df2.query("group == 'treatment'")); $ n_new
len(package.resources)
from itertools import islice $ list(islice(answer2.iteritems(), 2))
df_goog.Open.asfreq('D', method='backfill').plot()
df.columns
%%time $ def parse_binary_sensor_domain(parsedDF): $
seaborn.countplot(vacancies.weekday)
nba_df['Date'] = pd.to_datetime(nba_df['Date'])
stock['model_predict'] = best_dt.predict(stock.drop(['target', 'true_grow', 'predict_grow'], 1))
discounts_table.Product.unique()
(df[df['group'] == 'control'])
dd_df = pd.concat([dd_df.drop(['gearbox', 'notRepairedDamage', 'fuelType'], axis=1), dd_df_gearbox, $                    dd_df_notRepairedDamage, dd_df_fuelType], axis=1)
snt['pos']
s.loc[['b','c']]
from IPython.display import Image $ from IPython.core.display import HTML $ Image(url= "https://shanelynnwebsite-mid9n9g1q9y8tt.netdna-ssl.com/wp-content/uploads/2017/03/join-types-merge-names.jpg")
net_loans_exclude_US_outstanding.head()
tbl = df.groupby('msno').date.size().to_frame() $ tbl.columns = ['num_log_in'] $ tbl.reset_index(inplace = True) $ tbl['log_in_ratio'] = tbl.num_log_in/ df.groupby('msno').apply(listening_longevity).listening_longevity
titanic = pd.read_excel('Data/titanic.xls')
split='overall' $ act_irr=xirr(gps_actual[split])
holdout_results.head()
new_seen_and_click = new_seen_and_click.drop_duplicates(subset=['article_id', 'project_id', 'user_id', 'user_type'], keep='first') $ print(new_seen_and_click.shape) $ new_seen_and_click.head() 
sum(df2.user_id.duplicated())
tmin = trip_temps[0][0] $ tavg = trip_temps[0][1] $ tmax = trip_temps[0][2]
df_archive_clean["in_reply_to_user_id"] = df_archive_clean["in_reply_to_user_id"].astype(str) $ row_drop = df_archive_clean[df_archive_clean["in_reply_to_user_id"]!="nan"]["tweet_id"].index $ df_archive_clean.drop(row_drop, inplace=True) $ df_archive_clean["in_reply_to_user_id"].replace("nan", np.nan, inplace=True)
url_payouts = grouped['total_payout_value'].agg({'total_payout': 'sum', 'avg_payout': 'mean'})
pd.get_option('display.max_colwidth')
df = pd.read_pickle("dfWords.p")
def rank_performance(stock_price): $     if stock_price > 11.18766746411483: $         return 'Above' $     else: $         return 'Below'
pd.Series([1,2,3,np.nan]).mean(skipna=False)
df6
rfc = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=0) $ cv_score = cross_val_score(rfc, features_class_norm, overdue_transf, scoring='roc_auc', cv=5) $ 'Mean ROC_AUC score: {:.3f}, std: {:.3f}'.format(np.mean(cv_score), np.std(cv_score))
unzipfile("test.csv.zip","data") $ unzipfile("train.csv.zip","data")
df['youtube_url'] = df.apply(lambda x: youtube_urls[str((x['title'], x['artist']))], axis=1) $
sp['day_ago_close'] = sp.Close.shift(periods = 1) $ sp['week_ago_close'] = sp.Close.shift(periods = 7)
import pandas as pd $ import numpy as np $ autos = pd.read_csv('autos.csv', encoding = 'Latin-1') $ autos.info()
plt.rcParams['axes.unicode_minus'] = False $ dta_56.plot(figsize=(15,5)) $ plt.show()
server_response = urlopen(within_sakhalin_request_url) $ sakhalin_data_in_bbox = pd.DataFrame(json.loads(server_response.read().decode('utf-8'))['data'])
api_df.sample(10)
team_groups.ngroups
pipe.set_params(**gs.best_params_).fit(X_train, y_train) $ print('Best features:', pipe.steps[0][1].k_feature_idx_)
without_condition_heatmap = folium.Map([41.90293279, -87.70769386], $                zoom_start=11) $ print(without_condition_heatmap) $ without_condition_heatmap
df = pd.DataFrame(results_list)
building_pa_prc_zip_loc.to_csv("buildding_03.csv",index=False) $ building_pa_prc_zip_loc=pd.read_csv('buildding_03.csv',parse_dates=['permit_creation_date'])
result['uid'].nunique()
page = requests.get('https://api.opencorporates.com/v0.4/companies/search?q=barclays+bank')
df = pd.DataFrame(np.random.randn(8,4), columns=['A','B','C','D']) $ df
plt.hist(inches, 40);
learner.load_encoder('adam3_20_enc')
df.mean(1)
from google.colab import files $ files.upload()
pop_cat = timecat_df.groupby('userLocation')[['tweetRetweetCt', 'tweetFavoriteCt']].mean() $ pop_cat.head()
xml_in_merged['authorId'].nunique()
df.rename(columns={'PUBLISH STATES': 'Publication Status', 'WHO region': 'WHO Region' }, inplace=True) $ df.head(2)
import re $ liquor.columns = [re.sub("[^a-zA-Z]+", "", x) for x in liquor.columns]
table1= pd.read_csv('Fraud/Fraud_Data.csv') $ table2= pd.read_csv('Fraud/IpAddress_to_Country.csv') $ table1.head(3)
tw_clean.stage.value_counts()
print("The lengh of the errors", len(error_list))
word = 'Dad' $ it = iter(word) $ print(next(it)) $ print(next(it)) $ print(next(it))
versiData = irisRDD.filter(lambda x: "versicolor" in x) $ versiData.count()
mention_pairs = mentions[["id","screen_name","mention"]].groupby(["screen_name","mention"]).agg({"id":"count"}).rename(columns={"id":"Weight"}) $ mention_pairs.reset_index(inplace=True) $ mention_pairs.head()
Z = np.random.random((5,5)) $ Zmax, Zmin = Z.max(), Z.min() $ Z = (Z - Zmin)/(Zmax - Zmin) $ print(Z)
print(len(Users.id_partner.value_counts())) $ print(len(Cost.id_partner.value_counts()))
c1 = file.to_dataframe(mode='pivot', aggfunc='count') $ c1.head()
cohort_churned_df.head()
nlp = spacy.en.English() $ df['parsed'] = df.text.apply(nlp)
print df.shape[0] + noloc_df.shape[0]
all_tables_df.OBJECT_NAME
import pint $ ureg = pint.UnitRegistry() $ 3 * ureg.meter + 4 * ureg.cm
jobs.loc[(jobs.GPU == 1) & (jobs.FAIRSHARE == 1)].groupby('ReqCPUS').JobID.count().sort_values(ascending= False)
pattern = re.compile(' +') $ print(pattern.split('AA   bc')) $ print(pattern.split('bcA A'))
result.street_number
df_concensus = pd.read_csv( pwd+'/consensus_shift_history.052317.csv') #consensus?
grid.cv_results_['mean_test_score']
data_df.head()
overlap = list(set.intersection(set(targetarticles_files_list),set(pmcids_list))) $ print(len(overlap)) $ missing = list(set(targetarticles_files_list)-set(pmcids_list)) $ print(missing)
santos_tweets['date'] = pd.to_datetime(santos_tweets['date'])
df1['Year'] = pd.to_datetime(pd.Series(df1['Year']).astype(int),format='%Y').dt.year $ df1.tail()
companies = ['MSFT', 'WMT', 'PG', 'GM'] $ returns = data[companies] / data[companies].shift(1) - 1 $ returns = returns.fillna(method='ffill').dropna() $ returns.head()
autos = autos[autos['price'].between(1,350000)] $ autos['price'].describe()
building_pa_prc_shrink.loc[pd_aux.index, 'permit_number']=pd_aux.permit_number_notM
merged_df = pd.read_excel(data_folder_path + '/final-merged-df.xlsx') $ demand_df = pd.read_excel(data_folder_path + '/demand/demand-forecast.xlsx')
df_subset['Initial Cost'] = pd.to_numeric(df_subset['Initial Cost'].str.replace('$',''), $                                           errors = 'coerce') $ df_subset['Total Est. Fee'] = pd.to_numeric(df_subset['Total Est. Fee'].str.replace('$',''), $                                           errors = 'coerce') 
log_model.coef_
plt.subplot(1,2,1) $ master_df.loc[master_df.rating_numerator_normal < 1776, ['rating_numerator_normal']].boxplot() $ plt.subplot(1,2,2) $ outlier_df[['rating_numerator_normal']].boxplot();
from sklearn.metrics import r2_score $ r2_score(y_test, pred) $
promo_df['after_onpromotion']=promo_df['after_onpromotion'].fillna(pd.Timedelta(0)).dt.days $ promo_df['before_onpromotion']=promo_df['before_onpromotion'].fillna(pd.Timedelta(0)).dt.days
trump.shape
dfW17  = dfW17[['dt','temp','pressure','humidity','wind_speed','weather_description','clouds_all','wind_deg']]
P.plot_1d_layer('mLayerVolFracWat')
new_converted_simulation = np.random.binomial(n_new, .1196,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, .1196,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation $ print(p_diffs)
import requests $ url = 'https://goo.gl/FwemWV' $ page = requests.get(url)
access_logs_df = access_logs_parsed.toDF() $ access_logs_df.cache()
norm.ppf(1-(0.05/2))# Calculating value at 95% 
breakfastlunchdinner
print(data.iloc[[34450]])
data.head()
similarity_weight = cosine_similarity(sample_pivot_table) $ prefiltering_sw = prefiltering_of_neighbors(similarity_weight, 0.1) $ predicted_table = cs_classification_predicted_score_user_based(sample_pivot_table,prefiltering_sw,[0,1])
df_clean = df[df['frauds'] == False] $ df_dirty = df[df['frauds']]
merged_NNN.sort_values("amount", ascending=False)
iris.head().loc[:,:"Sepal.Length"] $
miner = TweetMiner(api, result_limit=200)
twitter_archive.describe()
 df.describe(include=['object'])
status = status.favorite() $ status.favorited, status.favorite_count
history.to_pickle('../data/merged_data/history.pkl')
ex4.drop("corn", axis = 1)
df_final_.state.value_counts() $
p_diffs = np.array(p_diffs) $ null_value = np.random.normal(0, p_diffs.std(), p_diffs.size)
seg_data = [month1cls, month2cls, month3cls, month4cls]
for x,y in zip(predY.split('\n'),trueY.split('\n')): $     print(x.strip(), "\t\t\t",y.strip()) 
about.html
datetime_df.Time.dt.weekday_name.head()
crimes[['PRIMARY_DESCRIPTION', 'SECONDARY_DESCRIPTION']].head()
old_page_converted = [] $ for _ in range(n_old): $     b = np.random.binomial(1, p_old) $     old_page_converted.append(b) $ old_page_converted = np.asarray(old_page_converted)
Lab7_Redesign.to_csv('Lab7_AllTicker.csv',index=False)
train.to_pickle('../data/merged_data/train.pkl')
reg_df['page_US'] = reg_df['is_US'] * reg_df['ab_page'] $ reg_df['page_UK'] = reg_df['is_UK'] * reg_df['ab_page'] $ reg_df.head() $
stats_cols = ['backers', 'usd_pledged_real', 'usd_goal_real'] $ df[stats_cols].describe()
t = tasks $ e = emails $ sl = site_landings $ o = opportunities
import statsmodels.api as sm $ convert_old = df2.query('group == "control" & converted == 1')['user_id'].nunique() $ convert_new = df2.query('group == "treatment" & converted == 1')['user_id'].nunique() $
favourite_max = np.max(data['Likes']) $ favourite = data[data.Likes == favourite_max].index[0] $ print("Tweet with most number of likes is: \n{}".format(data['Tweets'][favourite])) $ print("NUmber of likes: {}".format(favourite_max)) $ print("{} characters.\n".format(data['len'][favourite]))
grouped_dpt_city.aggregate(np.sum) # aggregate sum based on two groups
import csv $ import json $ import os $ import pprint $ import requests
for x in range(5): $     print donald[x]['text'] $     print('--')
g_influxdbconn = InfluxDBManager() $
z = [ii * x for ii in y] $ z 
a =R16df.rename({'Create_Date': 'Count-2016'}, axis = 'columns') $
instance.example() $ print('\nContextid: {}'.format(contextid))
total_fare=pd.DataFrame(city_fare) $ totol_fare=total_fare.reset_index() $ totol_fare $
tb['D2']
learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))
small_train['user_id'].nunique()
Merge = pd.merge(Project, Pipeline, how='outer', on=['Date'])
sns.set(style="white") $ sns.pairplot(matrix) $ plt.show()
print pd.concat([s1, s2, s3], axis=1, keys=['S1', 'S2', 'S3'])
asf_people_human_raw_df = session.read.format("csv").option("header", "true") \ $                 .option("inferSchema", "true").load( $     "{0}/human_data/asf_people".format(fs_prefix))
LR = LogisticRegression(C=0.01).fit(X_train,y_train) $ LR
user_summary_df[user_summary_df.tweets_in_dataset == 0].count()
MetaMetaclass.__new__
df.info()
is_08A_manhattan = (restaurants["VIOLATION CODE"] == "08A") & (restaurants["BORO"] == "MANHATTAN") $ inspections08A_in_manhattan = restaurants[is_08A_manhattan] $ inspections08A_in_manhattan["DBA"].value_counts()[:10].plot(kind='bar')
plt.scatter(low['longitude'],low['latitude'])
g = mixture.GMM(n_components=3) $ g.fit(cluster)
df.resample('D').mean().plot()
lr = LogisticRegression(random_state=20, max_iter=10000) $ param_grid = { 'C': [1, 0.5, 5, 10,100], 'multi_class' : ['ovr', 'multinomial'], 'solver':['saga','newton-cg', 'lbfgs']} $ grid_tfidf = GridSearchCV(lr, param_grid=param_grid, cv=10, n_jobs=-1)
import requests $ from bs4 import BeautifulSoup $ from selenium import webdriver
print("CV test error at %s is %.2f%% or %.2f%% accuracy +/- %0.2f%%" % (num_boost, $                                                                          (cv_xgb.iloc[-1]['test-error-mean'] * 100), $                                                                          (1-cv_xgb.iloc[-1]['test-error-mean']) * 100, $                                                                          cv_xgb.iloc[-1]['test-error-std'] * 100))
tweet_hour = tweet_data[(tweet_data['timestamp'] >= start_time) & $                         (tweet_data['timestamp'] <= start_time + pd.Timedelta(hours=delta_hours))].copy()
for word in words: $     print('current word:', word) $     _ = save_tweet(word, conv, since, until) $
ca_pl_all.loc[(ca_pl_all.FIRST_ACCOUNT!=ca_pl_all.LAST_ACCOUNT)].shape
df1 = pd.read_csv("{}MN_17_3616_MJ_Table2.csv".format(data_dir)) $ df2 = pd.read_csv("{}MN_17_3616_MJ_Table4.csv".format(data_dir))
for i in files_to_manage: $     print i $     account.files_delete(path="/nba games/test_ws_workflow/{}_2017.csv".format(i))
fs_url = 'http://sampleserver3.arcgisonline.com/ArcGIS/rest/services/SanFrancisco/311Incidents/FeatureServer' $ sanfran = FeatureLayerCollection(fs_url)
df_person.set_index("id", inplace=True) $ df_grades.set_index("person_id", inplace=True)
print len(encodedlist) $ print len(encodedlist[0]) $ print len(encodedlist[0][0])
old_page_converted = np.random.choice([1, 0], size = n_old, p=[p_old, (1-p_old)]).mean() $ old_page_converted
autos["price"].value_counts().sort_index(ascending=True).head(20)
tweet_archive_clean.info() $ print() $ image_predictions_clean.info() $ print() $ tweet_json_clean.info()
print("Number of rows in the sheet:"), $ print(sheet.nrows)
revenue.sort_values(by = ["totals.transactionRevenue"],ascending=False,inplace=True) $ revenue = revenue.assign(cum_revenue = revenue["totals.transactionRevenue"].cumsum()) $ revenue = revenue.assign(cum_revenue_pct = revenue["totals.transactionRevenue"].cumsum()/revenue["totals.transactionRevenue"].sum()) $ (revenue[revenue.cum_revenue_pct <= 0.81].fullVisitorId.nunique())
df_unique_providers.tail()
msft_monthly_cum_ret = msft_cum_ret.resample('M') $ msft_monthly_cum_ret
plt.plot(weather.index, weather.power_output)
obs_diff=df2[df2['group']=='treatment']['converted'].mean()-df2[df2['group']=='control']['converted'].mean() $ obs_diff
df.head(2)
from dotce.report import generate_chart
pd.set_option('display.max_columns',100) $ df.tail()
len([earlyPr for earlyPr in BDAY_PAIR_df.pair_age if earlyPr < 3])/BDAY_PAIR_df.pair_age.count()
grouped = url_with_author_data.groupby('url', as_index=False)
test2 = df_main.tweet_id == 670319130621435904 $ df_main[test2]
df2 = pd.read_csv('new_edited.csv')
p_new = df2.converted.mean() $ print("{:.4f}".format(p_new))
token.columns = ["_type","sender","date","receiver"]
trips.shape
import requests $ import json $ from pandas.io.json import json_normalize
es_rdd = es_rdd.map(lambda x: x[1])
new_page_converted = np.random.choice([0,1],size=n_new, p=(cr_new_null,1-cr_new_null)) 
(test.groupby(by='Page').mean() - train.groupby(by='Page').mean()).plot(figsize = (15,5))
TestData_ForLogistic.shape
df2 = df2.drop(labels = 2893, axis=0)
s = df_Providers['year_name'].value_counts() $ print('sum of (value_counts > 1): ',sum(s>1), '    (0 indicates that there are no dups)') $
(train.shape, test.shape, y_train.shape)
import matplotlib.image as mp_image $ filename = "../imgs/keras-logo-small.jpg" $ input_image = mp_image.imread(filename)
df.iloc[:5,[0,-1]]
from plotly import __version__ $ from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot $ init_notebook_mode(connected=True)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
vedanta_data = {'method1' : 589329 , 'method2' : 728822}
q1= pd.Period('2017Q1') $ q1
eclf4 = VotingClassifier(estimators=[('lr', alg), ('calC', alg7), ("ranFor", alg2), ("DT", alg6), ("Ada", alg4)], voting='soft') $ eclf4.fit(X_train, y_train) $ probs = eclf4.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
df[df['Agency']=='DOT']['Complaint Type'].value_counts()
%run process_twitter2tokens.py -i ../data/Training_relate_unrelated.csv -ot ../data/Training_relate_unrelated.txt -oc ../data/Training_relate_unrelated_tokenized.csv -co text
probability_of_winning = ( $     data_for_model[data_for_model['final_status']==1].count())/data_for_model['final_status'].count() $ probability_of_winning
plt.scatter(cc['high'],cc['spread']) $ plt.title("High vs Spread Price") $ plt.xlabel("High Value") #Change x and y axis scale $ plt.ylabel("Spread") $ plt.show()
df.isnull().sum(axis=0)
pd.pivot_table(more_grades, index="name", values="grade", columns="month", margins=True)
text = "The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ?" $ print(text)
uber_15["hour_of_day"].value_counts()
training_features, test_features, \ $ training_target, test_target, = train_test_split(X, y, test_size = 0.33, random_state=12)
store_items.insert(5, 'shoes', [8, 5, 0]) $ store_items
treatment_conv_prob = df2.loc[(df2["group"] == "treatment"), "converted"].mean() $ treatment_conv_prob
copy.head()
nnew = len(df2[(df2.landing_page=='new_page')]) $ print(nnew)
t = time.time() $ N = features.shape[0] * 4 $ ns = NegativeSampler(N,features,rdf,static_feature_names,seed=12345) $ negative_examples = ns.sample().set_index(['timestamp','segment_id','station_id']) $ print((time.time() - t),'seconds')
p = getpass.getpass() $ conn = pymysql.connect(host='localhost', port=3306, user='root', passwd=p,)# db='example') $ cur = conn.cursor()
treatment = df2[df2['group']=='treatment']['converted'].mean() $ treatment
import folium $ map_osm = folium.Map(location=[1.3521, 103.8198],zoom_start=12, tiles='Stamen Toner') $ def adding_to_map(lbs): $     return folium.CircleMarker(location=[lbs[5],lbs[6]], radius=12, color='#3186cc', fill_color='#3186cc').add_to(map_osm)
datatest.loc[datatest.place_name == "Prados del Oeste",'lat'] = -34.594077 $ datatest.loc[datatest.place_name == "Prados del Oeste",'lon'] = -58.829508
threeoneone = pd.read_csv('data/311_Service_DOT_since2016.csv') $
team_analysis = ALLbyseasons.groupby(["Team"]) # Groups our sample by team
df.xs(key=('a', 'ii', 'z'))
discGrouped = discovery.groupby(['lead_source', 'opportunity_stage']).opportunity_stage.count() $ discConvpct = discGrouped.groupby(level=[0]).apply(lambda x: 100* x / float(x.sum())); 
df.to_sql('places', conn)
n_new = new.count() $ n_new
data.treatment
reg_country.summary()
test_dum.shape
t1 = df_clean2['timestamp'].sample() $ t1 = pd.DatetimeIndex(t1)
local.export_to_quilt(3)
df_characters[df_characters.character_id>0].sort_values( $     'num_lines', ascending=False).head(10)
len(df2.user_id.unique()) $
team_id = 'eefcfb58-4d42-490d-9aea-4d4fd97964ce' $ url = form_url(f'teams/{team_id}') $ response = requests.get(url, headers=headers) $ print_body(response, skip_audit_info=True)
df.head()
train_data.head()
X_train.columns[220:240]
df['ts_year']=df['timestamp'].apply(lambda x:x.year)
train_set.tweetText[0]
import pandas as pd
(taxiData2.Fare_amount == 0).any() # This Returns True, meaning there are values that equal to 0
check_int('2016-11-06').number_repeat.max()
p_diffs = [] $ for _ in range(10000): $     new_page_converted_sample = np.random.binomial(1, p_new_sample, n_new) # e $     old_page_converted_sample = np.random.binomial(1, p_old_sample, n_old) # f $     p_diffs.append(new_page_converted_sample.mean() - old_page_converted_sample.mean()) # g $
print 'Python Version: %s' % (sys.version.split('|')[0]) $ hdfs_conf = !hdfs getconf -confKey fs.defaultFS ### UNCOMMENT ON DOCKER $ print 'HDFS filesystem running at: \n\t %s' % (hdfs_conf[0])
cnn_df = constructDF("@CNN") $ display(constructDF("@CNN").head())
Base = automap_base() $ Base.prepare(engine, reflect=True) $ Station = Base.classes.stations
educ_freqs = [0.112, 0.291, 0.1889, 0.096, 0.202, 0.111]
excelDF.head()
label_and_pred = lrmodel.transform(testData).select('label', 'prediction') $ label_and_pred.rdd.zipWithIndex().countByKey()
All_tweet_data_v2.columns
%%time $ df = pd.read_csv("data/311_Service_Requests_from_2010_to_Present.csv") $ pd.to_datetime(df['Created Date'],format="%m/%d/%Y %X %p")
mentions_df_begin.head()
data_full = data_full.dropna()
os.listdir()
meanMonthlyReturns
result.acknowledged
failures = pd.read_csv('failures.csv', encoding='utf-8') $ failures.head()
mlb = MultiLabelBinarizer(classes=sorted(tags_counts.keys())) $ y_train = mlb.fit_transform(y_train) $ y_val = mlb.fit_transform(y_val)
stats.norm.cdf((active_referred-inactive_referred)/SD_referred)
url = "http://www.ismyshowcancelled.com/shows/all/" $ content = requests.get(url).content $ soup = BeautifulSoup(content, "lxml") $ print(soup.prettify())
train['dot_gov'] = train.url.str.contains('.gov', case=False, na=False, regex=False).astype(int) $ train.groupby('dot_gov').popular.mean()
df.text[226]
TrainData[['DOB_clean', 'Lead_Creation_Date_clean']].describe()
full_globe_temp[full_globe_temp == -999.000] = np.nan $ full_globe_temp.tail()
new_array.shape
test1
archive.rating_numerator.value_counts()
train_data.isnull().sum()
pd.value_counts(appointments['Provider']), len(pd.value_counts(appointments['Provider']))
final_word_df = final_word_df.reset_index()
data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0) $ data.head()
test_case = contest_cr.where(F.col('party_id')== test[0][0]) $ test_case.take(1) $
np.exp(information_ratio)
for i in range(len(df_enhanced)): $     df_enhanced['source'][i] = df_enhanced['source'][i].split('<')[1].split('>')[-1]
STEPS = 365*10 $ random_steps = pd.Series(np.random.randn(STEPS), index=pd.date_range('2000-01-01', periods=STEPS))
ctd = gcsfs.GCSFileSystem(project='inpt-forecasting') $ with ctd.open('inpt-forecasting/Census Tool Data Pull CY2017- May 2018CONFIDENTIAL.xls.xlsx') as ctd_f: $   ctd_df = pd.read_excel(ctd_f)
df.mean() # same as df.mean(0)
print(q5b_answer)
xdata =x_data/max(x_data) $ ydata =y_data/max(y_data)
users.body['members']
df_train = df['text'].tolist()
merged_data['overdue'] = merged_data['due'] < merged_data['last_payment_date']
has_text['grade_levels'] = grade_levels
results = logit_mod.fit() $ results.summary()
users_conditions = df_experiment.copy() $ users_conditions = users_conditions[-users_conditions.user_id.isin(have_seen_two_versions)] # drop user who have seen more than one version
n_new = df2.query("landing_page == 'new_page'")['user_id'].count() $ n_new
test_probs = pd.DataFrame(columns=Y_valid_df.columns, index=test_df.index)
obs = df_gp_hr.groupby('level_0').mean() $ observation_data = obs['Observation (aspen)']
new_set = fraud_data_updated[fraud_data_updated['class']==1]
noloc_df['city'] = '' $ noloc_df['state'] = '' $ noloc_df['zipcode'] = ''
series1.cov(series2)
bus.head(3)
churned_ordered.head()
liberia_data1 = liberia_data[liberia_data.Description.isin(['Total new cases registered so far', $                                                          'New deaths registered'])] 
df  # See the entire DataFrame
date_splits = sorted(list(mentions_df["date"].unique())) $
f_channel_hour_clicks.show(1)
df3 = pd.merge(df1, df2, on='employee', how='outer') # default is how='inner' $ df3
exported_pipeline.named_steps['gradientboostingregressor'].feature_importances_
autos.odometer.value_counts()
df = df.iloc[3:] $ df
sns.heatmap(data.isnull().T)
week8 = week7.rename(columns={56:'56'}) $ stocks = stocks.rename(columns={'Week 7':'Week 8','49':'56'}) $ week8 = pd.merge(stocks,week8,on=['56','Tickers']) $ week8.drop_duplicates(subset='Link',inplace=True)
tweet_archive_clean.name = tweet_archive_clean.name.where(~tweet_archive_clean.name.isin(list_of_exclusion_words))
ticket3 = data_df.clean_desc[22] $ parsed3 = nlp(ticket3) $ svg1 = displacy.render(parsed3, style='dep', jupyter=True, options={'distance':140})
null_mean=0 $ null_vals=np.random.normal(null_mean,np.std(p_diffs),10000) $ plt.hist(null_vals);
control_gp = df2.query('group == "control"') $ prob_control_converted = control_gp[control_gp["converted"] == 1].count()  / (control_gp[control_gp["converted"] == 0].count() + control_gp[control_gp["converted"] == 1].count()) $ prob_control_converted = prob_control_converted[0] $ prob_control_converted
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative = 'larger') $ print(z_score) $ print(p_value)
df_tweet = pd.read_json('tweet_json.txt',orient='index') $
part = total[datetime(2017, 6, 30, 11, 0, 0):datetime(2017, 6, 30, 13, 59, 59)] $ plt.plot(part['field4']) $ plt.show()
full_text_classifier = full_text_classifier.fit(df_train)
model = models.TfidfModel(corpus, normalize=True)
katespadeseries.sum()
X = lsa.fit_transform(X)
print("Probability an individual recieved new page:", $       len(df2[df2['landing_page']=="new_page"])/len(df2))
file_path = "data/winemag-data-130k-v2.json" $ with open(file_path) as f: $     data = json.load(f) $ dataset = pd.DataFrame.from_dict(json_normalize(data), orient='columns')
y.start_time # Tells us that the object/period starts on 1st Jan 2016
df2 = df2.drop_duplicates(['user_id'], keep='last')
mar_file = os.path.join(DATA_DIR, "addresses.xlsx")
real_test_df
list(cur.execute('SELECT * FROM experiments'))
datacamp[datacamp["publishdate"]>='2017-01-01']["author"].value_counts(sort=True, ascending=False)[:10].plot(kind='bar')
columns = make_column_tree(data)
df = pd.read_csv("sample50.csv")
week17 = week16.rename(columns={119:'119'}) $ stocks = stocks.rename(columns={'Week 16':'Week 17','112':'119'}) $ week17 = pd.merge(stocks,week17,on=['119','Tickers']) $ week17.drop_duplicates(subset='Link',inplace=True)
samples_query.display_records(50)
df_students.shape
prcp_df.plot() $ plt.xlabel("Date range of 2015 records from 8/24/2016 through 8/23/2017") $ plt.ylabel("Precipitation") $ plt.show()
cpi_sdmx.codes['Dimension'][1]
df.info()
df[df['converted']==1]['user_id'].count() / df.shape[0]
autos['date_crawled'].str[:10] $ autos['date_crawled'].value_counts().sort_index(ascending=False).head(20)
rnd_reg.fit(X_train, y_train)
df[df['Descriptor'] == 'Pothole'].groupby(by=df[df['Descriptor'] == 'Pothole'].index.dayofweek).count().plot(y="Borough")
import pandas as pd $ from datetime import timedelta $ %pylab inline $ goog = pd.read_csv('../../assets/datasets/goog.csv')
df = pd.DataFrame({'Counts': [10, 31, 42, 5, 1, np.nan]}) $ df
df = pd.get_dummies(model_data, columns = ['creation_source','org_id']) $ df.fillna(0, inplace = True) $ df
len(df2[df2.converted==1])/len(df2)
df2.drop(labels=1899, axis=0, inplace=True)
lmdict.tail()
df = pd.merge(df,df_avg_DEP,how='left',on='ORIGIN') $ df = pd.merge(df,df_avg_ARR,how='left',on='DEST')
jobPostDF['date'] = pd.to_datetime(jobPostDF['date'], errors='coerce')
Raw_Forecast.Qty = Raw_Forecast.Qty.apply(lambda x: pd.to_numeric(x))
pop_df.stack()
raw_full_df.bathrooms.value_counts()
tbl[tbl.msno.duplicated()]
train_holiday_oil_store_transaction_item_test_002 = train_holiday_oil_store_transaction_item_test.fillna('False', subset=['transferred', 'type', 'locale', 'locale_name', 'description']) $
len(btc_price)
trump_df.head()
vals1.sum()
df2=df2.join(df_c.set_index('user_id'),on='user_id') $ df2.head()
df.describe()
for el in joined_donald[joined_donald['Proba_Donald']==min(joined_donald['Proba_Donald'])]['text']: $     print el
joined.loc[joined['cluster'] == 3].head()
subred_num_avg.head(1)
os.chdir('..')
twitter_archive_full[(twitter_archive_full.text.str.contains('doggos'))][['tweet_id','stage','text']]
ffr_recent = ffr["1985":]
views = containers[0].find("li", {"class":"views"}).contents[1][0:3] $ views
n_old = df2.query('group == "control"').count()[0] $ print(n_old)
small_train = pd.merge(small_train, dests_small, on='srch_destination_id', how='left') $ small_train.isnull().sum(axis=0) #how many nulls were introduced?
mysom.plot_hit_histogram(data.values) $
max_fwing = df.loc[df.following.idxmax()] $ name = max_fwing['name'] if max_fwing['name'] is not None else max_fwing['login'] $
device = hp.find_device('FL03001552') $ device
year_prcp_df.set_index('date', inplace=True) $ year_prcp_df.head()
df.text[343]
grouped = df_providers.groupby(['year' ]) $ type(grouped.get_group(2011)) $
n_new = df.query('group == "treatment" and landing_page == "new_page"').nunique()[0] $ print(n_new)
joined=load_df('joined_elapsed_events.pkl')
ebay[pd.date_range('2017-01', periods=52, freq='W-FRI')].plot()  
dfBill.head(10)
tokens.head(10)
r.loc[0, 'test-result-id']
ab_data.info()
plt.figure(1, figsize=(10,5)) $ plt.plot(lyra_lightcurve.data.index, lyra_lightcurve.data['CHANNEL3'], color='b') $ plt.plot(lyra_lightcurve.data.index, lyra_lightcurve.data['CHANNEL4'], color='r') $ plt.ylabel('Flux (Wm^2)') $ plt.show()
def stringDate_to_decade(string): $     decade = int(string[-4:-1]+"0") $     return decade $ stringDate_to_decade(explotions['Date'][0])
tweet_archive_enhanced_clean.info()
conn.execute(q); conn.commit()
with open('.CB_key', 'r') as f: $     key = f.read().replace('\n', '')
users.head()
amount = contribs.amount
week18 = week17.rename(columns={126:'126'}) $ stocks = stocks.rename(columns={'Week 17':'Week 18','119':'126'}) $ week18 = pd.merge(stocks,week18,on=['126','Tickers']) $ week18.drop_duplicates(subset='Link',inplace=True)
overallTotRmsAbvGrd = pd.get_dummies(dfFull.TotRmsAbvGrd)
import TogglPy
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4) $ print ('Train set:', X_train.shape,  y_train.shape) $ print ('Test set:', X_test.shape,  y_test.shape)
post_url = 'https://staging.app.wikiwatershed.org/api/watershed/'
df.drop(['ARR_TIME','AIR_TIME','ACTUAL_ELAPSED_TIME','ARR_HOUR'],axis=1,inplace=True)
df_cont = df[['goal','pledged', 'backers', 'date_diff']] $ g = sns.pairplot(df_cont)
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','ca','uk']]) $ results = logit_mod.fit()
ret_aapl = calc_daily_ret(closes_aapl)
cityID = '095534ad3107e0e6' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Louisville.append(tweet) 
p_obs_diff = 0 $ for i in p_diffs: $     if i > obs_diff: $         p_obs_diff += 1 $ (p_obs_diff/len(p_diffs))
def drop_sequence(cur, sequence_name): $     cur.execute(sql)
tickets.describe().T
plt.scatter(X2[:, 0], X2[:, 1], c = day_of_week, cmap='rainbow'); $ plt.colorbar();
df = pd.merge(releases, bookings, on='hash_id', how='left')
len(df)
list(first_result.children)[1]
latest_time_entries[1]
from matplotlib import pyplot as plt $ %matplotlib inline
df2.query('group == "treatment" and converted == 1').count()[0]/df2[df2['group'] == 'treatment'].count()[0]
iowa_fset = iowa_fc.query()
clf = LinearRegression(n_jobs=-1) $ clf.fit(X_train, y_train) $ confidence  = clf.score(X_test, y_test) $ print("Confidence our Linear Regression classifier is: ", confidence)
team_names = pd.read_csv('player_stats/team_names.csv') $ playoff_dict = {0:'DNQ',1:'First Round',2:'Second Round',3:'Conference Finals',4:'NBA Finals',5:'Champion'}  # Decoding $ team_names['playoff_round'] = team_names.Playoffs.apply(lambda x : playoff_dict[x]) $ names = list(team_names['Name'])
twitter_archive_clean[twitter_archive_clean['retweeted_status_timestamp'].isnull()==False].shape[0]
Merge column headers "doggo", "floofer", "pupper", and "puppo" into one column
data['title_len'] = [len(x) for x in data['title']]
dfrecent.head(10) #looks like winter days have the highest counts
p_diffs = [] $ diffs = np.random.binomial(n_new, p_new, 10000)/n_new - np.random.binomial(n_old, p_old, 10000)/n_old $ p_diffs.append(diffs)
local_holidays=actual_holidays[actual_holidays.locale=='Local'] $ print("Rows and columns:",local_holidays.shape) $ pd.DataFrame.head(local_holidays)
bruins = bruins[pd.to_datetime(bruins.date).isin(bad_dates) == False] $ bruins = bruins[pd.to_datetime(bruins.date) <= dt.datetime(2015,2,11)]
difference = new_page_converted.mean() - old_page_converted.mean() $ difference
model = Word2Vec.load(os.path.join(outputs,model_name))
pd.date_range('2018 May 1st', '2018 Jul 3', freq = 'D')
col_drop = ['id', 'athlete_count', 'avg_speed', 'type', 'latlng', 'avg_hr', 'pace'] $ zone_train = zone_train.drop(col_drop, axis=1) $ zone_test = zone_test.drop(col_drop, axis=1) $ zone_train.head()
n_old = df2[df2.group == 'control'].count()[0] $ n_old
df_1=pd.DataFrame(df.scores)
X_features = features.iloc[:, 1:8].as_matrix() $ y_features = features.iloc[:, 8].as_matrix() $ y_features = y_features * 5 $
pickle.dump(nmf_cv, open('iteration1_files/epoch3/nmf_cv.pkl', 'wb'))
details['Year'], details['Month'] = details['Released'].dt.year, details['Released'].dt.month
pd.crosstab(df_protest.loc[:, 'Violent_or_non_violent'], df_protest.loc[:, 'Type'])
page.status_code
data.set_index("Unique Key",inplace=True) $ data.head()
to_be_predicted_Day4 = 31.34447701 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
pd.isnull(reviews).head()
null_p_new = df2.converted.mean() $ print(null_p_new)
data = data.drop(["project_subject_subcategories","project_subject_categories","sub1","sub2"],axis=1)
import os $ os.path.exists("classA.csv")
df2 = pd.read_csv('so2_summary.csv', names=names2, skiprows=89, parse_dates=True, index_col='Date')
autos["registration_year"].describe() $ autos["registration_year"].sort_values(ascending=True) $ autos=autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].describe()
df.info()
d = pd.date_range('20170101', periods = 12) $ d
n_rows = df.shape[0] $ print(n_rows)
def tokenize_text(text): $     word_tokens = nltk.word_tokenize(text) $     tokens = [token.strip() for token in word_tokens] $     return tokens
tweet_archive_clean[tweet_archive_clean.name.isin(list_of_exclusion_words)]
print(r.text[0:500])
data.iloc[:, :3][data.three > 5]
fda_drugs.loc[fda_drugs.DrugName.str.contains('AMANTADINE')]
'TRUMP' in asdf['text'][1].upper()
url = "https://mars.nasa.gov/news/" $ response = requests.get(url)
df.iloc[(len(df)-lookforward_window)-2:(len(df)-lookforward_window),:]
import os $ os.chdir('models/research/')
if 'TRAVIS' in os.environ: $     df.loc[:500].to_csv('movie_data.csv') $     df = pd.read_csv('movie_data.csv', nrows=500) $     print('SMALL DATA SUBSET CREATED FOR TESTING')
bestest = bestseg $ print(bestest)
joined = join_df(joined, googletrend, ["State","Year", "Week"]) $ joined_test = join_df(joined_test, googletrend, ["State","Year", "Week"]) $ len(joined[joined.trend.isnull()]),len(joined_test[joined_test.trend.isnull()])
len(reddit_df)
df_repeat=df2[df2['user_id'].duplicated()]['user_id'].values $ print("Repeated user_id is "+str(df_repeat[0]))
autos['odometer_group'] = autos['odometer_km'] / 10000.0 $ autos['odometer_group'].value_counts()
y_pred_rf = rf.predict_proba(X_test)[:, 1] $ fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)
model_artifact = MLRepositoryArtifact(model, training_data=train.select('ATM_POSI','POST_COD_Region','DAY_OF_WEEK','TIME_OF_DAY_BAND','FRAUD'),\ $                                       name="Predict ATM Fraud") $ model_artifact.meta.add("authorName", "Data Scientist");
tweetsS03 = pd.read_sql_query("SELECT created_at, extracted FROM tweets_info;", connS03, parse_dates=['created_at'] ) $ tweetsS03['created_at'] = tweetsS03['created_at'].dt.tz_localize("UTC").dt.tz_convert("Europe/Berlin") $ print("Number of Tweets: %s" %len(tweetsS03)) $ tweetsS03.head()
table
BroncosBillsPct.head()
for k in range(len(imagelist)): $     send2trash.send2trash(imagelist[k]) $ imagelist = [i for i in os.listdir() if i.endswith(".pdf")  ] $ len(imagelist)
tweet_df[['total']].count()
meanDailyReturns
number_of_users = df.user_id.nunique() $ number_of_users
pair.DATETIME[65000]
nbart_allsensors['NDVI'] = (nbart_allsensors['nir']-nbart_allsensors['red'])/(nbart_allsensors['nir']+nbart_allsensors['red'])
props.head()
import pandas as pd $ import numpy as np $ pd.options.mode.chained_assignment = None  # default='warn' $ baseball = pd.read_csv("Data/baseball.csv", index_col='id')
tweet_archive_enhanced_clean['tweet_id'] =tweet_archive_enhanced_clean['tweet_id'].astype(str) $
last_year = dt.date(2017,8,23)-dt.timedelta(days=365) $ last_12_months = session.query(Measurement.date,Measurement.prcp).\ $     filter(Measurement.date > last_year).all() $ last_12_months
len(final_data.item.unique())
n_old = df2.query('landing_page == "old_page"').count()[0] $ n_old
df[df["stamp"]>= "02-20-2018"].head()
metrics.f1_score(y_valid, y_pred)
result2 = result[result['age']>=18] $ result2.shape
%%time $ df_all_wells_wKNN = combine_DfOfAllWells_with_knnDf(df_all_wells_basic,wells_df_new_cleaned_plus_nn_wNoNulls)
print location $ print location.split(', ')[-6]
e = Example() $ print(e.__dict__) $ print(e.__dict__.__class__)
df['Precipitation'].describe()
autos = autos[(autos["price"]>0) & (autos["price"]<=350000)] $ autos["price"].describe()
data.phone.value_counts()
df[df['bmi']< 18.5] # this is the way to select data, by using filters. $
df_parsed.country.value_counts()
idx = pd.IndexSlice $ health_data_row.loc[idx[:, :, 'Bob'], :]  # very close to the naive implementation
T = 0 $ folder = 'trainW-'+str(T) $ user_logs = utils.read_multiple_csv('../../feature/{}/compressed_user_logs'.format(folder)) $
month1 = oanda.get_history(instrument_1, $                           start = '2017-12-31', $                           end = '2018-2-1', $                           granularity = 'M10', $                           price = 'A')
n_new = df2[df2.landing_page =='new_page'].count()['user_id'] $ n_new
result_df['country'].value_counts()
events_enriched_df['topic_name'].drop_duplicates().count()
test_pd = test.pivot_table(values = 'Visits', index = 'Page', columns = 'date')
new_page_converted = np.random.choice([1,0], size = n_new, p = [0.1196,1-0.1196]) $ new_page_converted
dfEtiquetas["date"] = dfEtiquetas["created_time"].apply(lambda d: datetime.strptime(d, '%Y-%m-%dT%H:%M:%S+%f').strftime('%Y%m%d'))
dates=[tweet.created_at for tweet in all_tweets] $ dates[:5]
df['w'].unique()
test= test.reset_index(drop = True) $ test['rooms'] = pd.Series(predictions) $ datatest = pd.concat([train, test]) $ datatest = datatest.reset_index(drop = True)
df2.drop_duplicates(subset='user_id',keep='first', inplace=True)
data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)
df2['intercept'] = 1
new_page_converted = np.random.choice([0, 1], size=n_new, p=[1-p_new, p_new]) $ p_new_sim = new_page_converted.sum()/len(new_page_converted) $ p_new_sim
model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)
cityID = '488da0de4c92ac8e' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Plano.append(tweet) 
a=[0,1] $ old_page_converted = np.random.choice(a,145274,p=[0.8804,0.1196]) $ print(old_page_converted.mean())
rng.tz_localize('Etc/GMT-1')
pd.read_sql_query("SELECT * FROM person LEFT JOIN grades ON person.id = grades.person_id" , conn)
usage_400hz = hc.table('asm_wspace.usage_400hz_2017_Q4') $
temps_df.loc['2018-05-05']
pd.DataFrame({'Decison Making' : [x[1] for x in HARVEY_92_USERS_DM]}).hist(bins=[1,2,3,4,5,6,7,8,10,100,200,1000])
len(train_data[train_data.fuelType == 'benzin'])
df.dtypes
positives = NB_results.loc[NB_results.sentiment == 'P'] $ sample_size = 10 $ for tweet in positives.tweet.sample(sample_size): $     print(tweet)
groceries['eggs'] = 2 $ groceries
sdMonthlyReturns = dict( { 'Infy': monthly_gain_summary.Infy.std(), 'Glaxo': monthly_gain_summary.Glaxo.std(), $                           'BEML': monthly_gain_summary.BEML.std(),'Unitech': monthly_gain_summary.Unitech.std()  } ) $ sdMonthlyReturns
season_groups = nba_df.groupby("Season") $ team_groups = nba_df.groupby("Team") $ season_type_groups = nba_df.groupby("GameType")
def index_level_dtypes(df): $     return [f"{df.index.names[i]}: {df.index.get_level_values(n).dtype}" $             for i, n in enumerate(df.index.names)] $ index_level_dtypes(read_multi_df)
projects = pd.read_csv('data_old/sc_report_projects.csv') $ print(projects.shape) $ projects
df = pd.read_csv('./reddit_data_2.csv')
!more "files/approved users.txt"
print(df.seller.unique()) $ print(df.offerType.unique()) $ print(df.abtest.unique()) $ print(df.nrOfPictures.unique())
zero_rev_acc_opps.head()
pd.Series([100,100]).rank()
sql = "select * from [Frame_User].[FinFC].[HHdataVarAnalysis]" $ RunID_new = 4463 $ RunID_old = 4308 $ RunIDs = {'NewScenario': RunID_new , 'OldScenario': RunID_old} $ RunIDs
means = pd.DataFrame(means2_table) $ means
for k in range(3,15): $     knn = KNeighborsClassifier(n_neighbors=k).fit(X_train,y_train) $     yhat = knn.predict(X_test) $     print("k = " + str(k) + " -> f1 score = " + str(f1_score(y_test, yhat, average = "weighted")))
pt_unit_top.head(20)
cols = ['rt_count', 'fav_count', 'tweet_id'] $ from_api = pd.DataFrame(df_list, $                         columns = cols)
pd.get_dummies(df)
edgereader = sample_df[['series_or_movie_name','encrypted_customer_id']] $ edgereader.columns = ['source','target']
timestamps = [ $     datetime(2018, 1, 1), datetime(2018, 1, 2), datetime(2018, 1, 3), $     datetime(2018, 1, 4), datetime(2018, 1, 5), datetime(2018, 1, 6), $     datetime(2018, 1, 7), datetime(2018, 1, 8), datetime(2018, 1, 9), $ ]
groupedvalues
doesnt_meet_credit_policy = loan_stats['loan_status'].grep(pattern = "Does not meet the credit policy.  Status:", $                                                      output_logical = True)
g8_groups.agg(['mean', 'std'])
num_topics = 2
import datetime
le.fit(df2['tag']) $ df2["tag_num"] = le.fit_transform(df2['tag'])
from gensim.summarization import summarize $ from gensim.summarization import keywords
all_boroughs_cleaned = all_boroughs[['GEO.id', 'GEO.id2', 'GEO.display-label','HD01_VD01','HD02_VD02','HD01_VD04','HD01_VD06','HD01_VD07','HD01_VD12','HC02_EST_VC02']] $ all_boroughs_cleaned= all_boroughs_cleaned.ix[:,[0,7,14,21,25,26,28,30,32,34]] $ all_boroughs_cleaned=all_boroughs_cleaned.rename(columns={'HD01_VD01':'population','HD02_VD02':'white_alone','HD01_VD04':'black_alone','HD01_VD05':'native_alone','HD01_VD06':'asian_alone','HD01_VD07':'other_race','HD01_VD12':'hispanic','HC02_EST_VC02':'median_income'}) $ all_boroughs_cleaned.head()
model = LogisticRegression()
turnaround_planes_df=turnaround_planes.toPandas() $ turnaround_planes_df
browser = Browser('chrome', headless=False) $ image_url  = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars' $ browser.visit(image_url ) $ time.sleep(1)
engine.score_all_on_classifier_by_key("nhtsa_classifier")
convo1 = companyNeg[companyNeg.group_id == 'group_98805' ]
potentialFeatures = ['acceleration', 'curve', 'free_kick_accuracy', 'ball_control', 'shot_power', 'stamina']
expx=np.mean(x) $ expy=np.mean(y) $ expx, expy
df2['intercept']= 1
pf_mus, pf_sigmas = create_random_portfolios(returns.values, n_portfolios=3000) $ pf_sigmas
processing_test.files
topics_top_doc_wt = docs_by_topic.max()[['TopicWt']] $ topics_top_doc_wt
test_tokens = testset['tokens'].tolist() $ test_corpus = createCorpus(test_tokens)
temp_dict = {ticker: fundamental_dict[ticker] $              for ticker in np.random.choice(symbols, 10) if ticker in fundamental_dict}
new_df $
df2[df2.group == 'treatment'].converted.mean()
x_train_data = training_DF.drop(['IncidentInfo','ATRW','LRW','DRW','NLT','NTT','NDT','cut','text'],axis=1) # $ y_train_data = training_DF['IncidentInfo']
students[students.gender == 'F']
df.groupby('product_type')['price_doc'].median()
act_diff = df2[df2['group']=='treatment']['converted'].mean() - df2[df2['group']=='control']['converted'].mean() #calculating values greater than the actuall diff we calculated before. $ (act_diff < np.array(p_diffs)).mean()
import pandas as pd $
from spacy.matcher import Matcher
new_crs = np.array(new_crs)
appointments.shape
max_value = rankings['total_points'].idxmax() #idxmax() returns the index of the max value from column total_points $ rankings.iloc[max_value] #iloc[] looks up the observation when given an index number
lr.fit(pscore, btc_price)
for i in range(len(df.dtypes)): $     print (df.dtypes.index[i], df.dtypes[i])
pumashplc.to_file(os.getenv('PUIDATA')+'/pumashplc.shp')
posts = pd.read_csv('data/posts.csv', index_col = 0, parse_dates = [5, 6, 7]) $ posts.columns
yc = T.set_subtensor(y_true[T.eq(y_true,0).nonzero()], 100)
corn.count()
p_control = df2.query('group=="control"').converted.mean() $ p_control
data.loc[data["Breed"]== "Mix", "Breed"] = 1 $ data.loc[data["Breed"]!= 1, "Breed"] = 0 $ print(data["Breed"].unique())
print(parquet_file) $ df = sqlContext.read.load(parquet_file) $ df.show()
temps_df.iloc[1].index
model.most_similar("man") $
Helper().why_adaptive_selection()
convert_new = df2[(df2['landing_page']=='new_page') & (df2['converted']==1)].count()[0] $ convert_new
df_arch_clean['rating_denominator'] = df_arch_clean['rating_denominator'].astype('float') $ df_arch_clean['rating_numerator'] = df_arch_clean['rating_numerator'].astype('float') $
map_osm
df_subset['Initial Cost'].head()
try: $     import test_package.print_hello_function_container.print_hello_function $ except: $     print("Functions cannot be imported directly")
df_imputed_median_NOTCLEAN1A.head(5)
import pickle
print(df['converted'].mean()*100)
def remove_characters(text): $     text = text.strip() $     PATTERN = '[^a-zA-Z ]' # only extract alpha characters $     filtered_text = re.sub(PATTERN, '', text) $     return filtered_text
P_new - P_old #under null hypothesis
graf['DETAILS2']=graf['DETAILS'].progress_apply(text_process)
len(interactions_recruiter), len(red_inter_recr)
df2.drop([2893], axis=0, inplace = True)
df2['converted'].mean()
jaja= dataset.loc[dataset['customer_id']=="450e1c2cbd21687780153995f1be0c23"]
l = pd.read_sql_query(QUERY, conn) $ l $
X=Feature $ X[0:5] $
learn.save("dnn40")
results1 = pd.DataFrame(results, columns=['itapudid', 'max1stdetectwssc', 'max1stdetectwssd', 'max1stdetectwsse', 'max1stdetectwssf', 'eventtime', 'correct'])
archive_copy = archive_df.copy()
ser7+2
import pandas as pd $ wine = pd.read_csv('solutions/data/raw/winemag-data-130k-v2.csv', index_col=0) $ wine.head()
df_2001.dropna(inplace=True) $ df_2001
dfs[index_max][dfs[index_max]['Outside Temperature'] == dfs[index_max]['Outside Temperature'].max()]
df.groupby('dog_type')['rating'].describe()
n_new = df2.query('group == "treatment"').user_id.count() $ n_new
census_finaldata.sample(100).plot()
print('Cells can be executed multiple times, edited, copied/pasted, and re-ordered.')
import pathlib $ import os 
march_2016 = pd.Period('2016-03', freq='M')
numPurchU = train.groupby(by='User_ID')['Purchase'].count().reset_index().rename(columns={'Purchase': 'NumPurchasesU'}) $ train = train.merge(numPurchU, on='User_ID', how='left') $ test = test.merge(numPurchU, on= 'User_ID', how='left')
df_agg_notop_rand.head()
confusion_matrix(fashion.index, fashion.predicted)
len([shared_ix for shared_ix in SCN_BDAY.index if shared_ix in BDAY_PAIR_df.index])
rtime = [x.text for x in soup.find_all('time', {'class':'live-timestamp'})]
channel_meta['topic_ids']
run txt2pdf.py -o "MEMORIAL HERMANN HOSPITAL SYSTEM  Sepsis.pdf"   "MEMORIAL HERMANN HOSPITAL SYSTEM  Sepsis.txt"
pickle.dump(combo_models_list, open('iteration1_files/epoch3/combo_models_list.pkl', 'wb'))
df.first_browser.value_counts()
out = conn.upload('/u/username/data/iris.tsv', $                   importoptions=dict(filetype='csv', delimiter='\t'), $                   casout=dict(name='iris_tsv', caslib='casuser')) $ out
predictions
final_train_users = train_nonUS_purchase.copy() $ final_train_users = final_train_users[final_train_users['id'].isin(sessions['user_id'])] $ final_train_users.head() $
executable_path = {'executable_path': '/usr/local/bin/chromedriver'} $ browser = Browser('chrome', **executable_path, headless=False)
plt.figure(figsize=(5,5)) $ sns.countplot(auto_new.Custom)
talks['text'] = talks['text'].apply(lambda x: re.sub("<.*?>", "", x))    
fig,axes = plt.subplots(1, 2, figsize = (16,4), sharey= True) $ axes[0].plot_date(x=obama.created_at, y = obama.n_words,linestyle = '-',marker='None') $ axes[1].plot_date(x=trump.created_at, y = trump.n_words,linestyle='solid',marker='None') $ plt.savefig("fig/n_word_comparison.png")
pivoted_data.plot(figsize=(10,10))
import seaborn as sns $ ax = df['DepDelay'].hist(bins=500) $ ax.set_xlim(-100, 500) $ plt.title('Number of flights per delay bin') $ plt.show()
obs_diff_org=p_new-p_old $ obs_diff_org
order_data.head()
sessions.info(null_counts = True)
df_cust_data['Registration Date'] = df_cust_data['Registration Date'].as_matrix()
df.count()
missing_values = df.isnull().sum() $ missing_values
autos["registration_year"].value_counts(normalize = True, dropna = False).sort_index()
learn.fit(lr, 3, cycle_len=3)
df=df.set_index('date')
FREEVIEW.plot_number_of_fixations(raw_fix_count_df, option=None)
df_new=df2.query("landing_page=='new_page'") $ P=len(df_new)/len(df2) $ P
Train_extra = Train.merge(train_dum_clean, right_on='ID', left_index=True) $ Train_extra.head()
df_by_donor.columns = [' '.join(col).strip() for col in df_by_donor.columns.values]
assoc = db.get_associations(limit=150) $ assoc.head()
df_new.drop('US',axis=1, inplace=True)
columns = inspector.get_columns('station') $ for c in columns: $     print(c['name'], c["type"])
s_n_s_epb_one = s_n_s_df[s_n_s_df.Beat=='5K02'] $ s_n_s_epb_one = s_n_s_epb_one.groupby(s_n_s_epb_one.Date).size().reset_index() $ s_n_s_epb_two = s_n_s_df[s_n_s_df.Beat=='5G02'] $ s_n_s_epb_two = s_n_s_epb_two.groupby(s_n_s_epb_two.Date).size().reset_index()
state_grid_new = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins=(21, 21)) $ q_agent_new = QLearningAgent(env, state_grid_new) $ q_agent_new.scores = []  # initialize a list to store scores for this agent
plt.rc('figure', figsize=(5, 5)) $ mosaic(crosstabkw.stack(),gap=0.03 ) $ plt.title('Mosaic graph: Successful / Contain Keyword') $
%time median_of_column(dest, 'my_col7')
print("Number of docs in 'uso17': %i" % uso17_size) $ print("Number of docs in 'uso17_qual': %i" % uso17_qual_size)
df_users.user_id.nunique()
model.fit(X_tr, y_tr) $ preds = model.predict(X_val) $ np.sqrt(metrics.mean_squared_error(y_val, preds)) $
sqlClient.logon() $ jobId = sqlClient.submit_sql(sql) $ print("SQL query submitted and running in the background. jobId = " + jobId)
df_to_interp.interpolate()  # the index values aren't taken into account
from keras import optimizers $ opt = optimizers.Adam() $ model.compile(optimizer=opt, $                 loss='mse', $                 metrics=['accuracy'])
yc_new4.describe()
print("The simulated difference is: %.4f" %(new_page_converted - old_page_converted))
input_data = pd.merge(mit, weather, on='date')
fullDF = append_sentiment(fullDF)
sns.regplot(x = np.arange(-len(my_tweet_df[my_tweet_df["tweet_source"] == "BBC News (World)"]), 0, 1),y=my_tweet_df[my_tweet_df["tweet_source"] == "BBC News (World)"]["tweet_vader_score"],fit_reg=False,marker = "o",scatter_kws={"color":"darkred","alpha":0.8,"s":100}) $ ax = plt.gca() $ ax.set_title("Sentiment Analysis (BBC)",fontsize = 12) $ plt.savefig('Sentiment_BBC.png')
data.name.duplicated(keep=False).sum()
esp.to_csv('spanish_language_feedback.csv')
df_to_interp.interpolate(method='index')  # notice how the data obtains the "right" values
prec_long_df.describe()
df.to_csv('ab_data_cleaned.csv', index = False)
kNN5.fit(X, y)
to_be_predicted_Day2 = 14.52028076 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
tfav = pd.Series(data=data['Likes'].values, index=data['Date']) $ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
df_pol=pd.read_csv('Pol03-06-2018.csv') $ df_pol.shape
from sqlalchemy import table
data.info()
df_dem_2016 = df_dem[(df_dem['created_at'] > '2015-12-31') & (df_dem['created_at'] < '2017-01-01')]
cohort_activated_df.to_csv('activation_churned_subscribers.csv')
weather_yvr['Relative Humidity (fraction)'] = rh_frac
import os  $ os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars elasticsearch-hadoop-6.1.1/dist/elasticsearch-spark-20_2.11-6.1.1.jar pyspark-shell'  
df_western = df[df['genres'].str.contains("Western")]
cust_data.head(3)
IMDB_dftouse=IMDB_df[movie_mask]
print(sentiments['comments'].iloc[3])
unique = df2.user_id.nunique $ unique()
if page.isRedirectPage(): $     page = pb.Page(commons_site, 'File:Parlament Europeu.jpg').getRedirectTarget() $     print(page.title())
from quilt.data.dsdb import secondary_processing $ secondary_processing
pd.DataFrame.to_csv(pd.DataFrame(file_lines),"Fireeye_output.csv",index = False)
periods = [pd.Period('2012-01'), pd.Period('2012-02'), pd.Period('2012-03')] $ periods
co_occurence_on_top50 = {ds: [(el[0], str(el[1])) for el in datasets_co_occurence[ds]] for ds in top50.dataset_id.tolist()}
print (sanders[0]['text'])
properati['place_name'].value_counts(dropna=False)
df["2015-01":"2015-03"]['Complaint Type'].value_counts().head(5)
df.show()
!wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/002/079/225/GCA_002079225.1_ASM207922v1/GCA_002079225.1_ASM207922v1_feature_table.txt.gz
df.first_device_type.value_counts()
3 / float(2)
tweet_vector = CountVectorizer(analyzer='word',stop_words=stopWords).fit(tweet_hour['tweet_text'])
print(x)
tweet_json[tweet_json.duplicated('id')]
dfNiwot["TMAX"].mean()
print (summarize(text, word_count=200))
z_score,p_value = sm.stats.proportions_ztest([convert_old,convert_new],[n_old,n_new],alternative='smaller') $ print("z-score = ", z_score,"\n", $       "p_value = ", p_value) $
conn, cur = connect()
df2.query('landing_page == "old_page" and group == "treatment"')
test_df[["id", "labels"]].to_csv("submission_8.27.csv", index=False)
ins.head(5)
!ls ../input/ #MacOS command $
model_df.news_text = model_df.news_text.fillna('') $ model_df.tesla_tweet = model_df.tesla_tweet.fillna('') $ model_df.elon_tweet = model_df.elon_tweet.fillna('')
pd.date_range(start = '10/23/2016 17:00:10', periods = 13, freq = '%w', normalize =True )
session.query(Measurements.date).all()
pd.options.display.max_rows = 6 $ all_coins_df = my_cryptory.extract_bitinfocharts("btc") $ all_coins_df
df['rating'].describe()
day_of_week14.to_excel(writer, index=True, sheet_name="2014")
df2.head() $ df2
question_1_dataframe = question_1_dataframe.size().to_frame(name='count') $ question_1_dataframe
len([premieSn for premieSn in SCN_BDAY.scn_age if premieSn < 0])/SCN_BDAY.scn_age.count()
headers = ['region', 'variable', 'attribute', 'source', 'web', 'unit']
max(image_predictions['tweet_id'].value_counts())
df.info()
col.index
for i in range(1, 10): $     print("Features with {} importance: gives {} number of features".format(10**-i, np.sum(rfc_feat_sel.feature_importances_ >= 10**-i)))
sub1 = sub1.drop_duplicates().reset_index(drop = True)
prcp_scores = session.query(measurements.date,func.avg(measurements.prcp)).\ $                     filter(measurements.date>=year_ago).\ $                     group_by(measurements.date).all() $ prcp_scores
less_hackers = recommendation_df[recommendation_df['hacker_count'] < 10]
old_crs = np.array(old_crs) $ old_crs.mean()
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator $ paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]).build() $ cvEvaluator = MulticlassClassificationEvaluator() $
convert_old = df2.query("group == 'control'")['converted'].sum() $ convert_new = df2.query("group == 'treatment'")['converted'].sum() $ n_old = df2.query("group == 'control'")['converted'].count() $ n_new = df2.query("group == 'treatment'")['converted'].count()
%timeit df1 + df2 + df3 + df4
forecast_col = 'Adj. Close' $ df.fillna(value=-99999, inplace=True) $ forecast_out = int(math.ceil(0.01 * len(df)))
null_diff = new_page_converted.mean() - old_page_converted.mean() $ print(null_diff)
end_date = [churned_df.loc[bid,'canceled_at'][-1] if churned_df.loc[bid,'cancel_at_period_end'][-1] == False else churned_df.loc[bid,'current_period_end'][-1] if churned_df.loc[bid,'cancel_at_period_end'][-1]==True else None for bid in churned_df.index]
n_old = df2.query('group == "control"')['user_id'].count() $ n_old = int(n_old) $ n_old
normalized_data = json_normalize(data=df['programs'], record_path='works', $                             meta=['id', 'orchestra','programID', 'season'])
len(kick_data_state)
close_day2 = [] $ for day in data: $     close_day2.append(day[4])   $ close_day2.pop(0)  $
left = pd.DataFrame({'key' : ['foo', 'foo'], 'lval': [1,2]}) $ left
most_recent_investment.head() $ most_recent_investment.columns = ['uuid','most_recent_investment']
articles_by_pub.head()
df['date_int'] = df.created_at.astype(np.int64) $ tweetVolume(df) $ print("looks like there are some spikes in 2015.")
googClose.plot()
qtrclosePrice.head()
cryptos[['name', 'market_cap_usd']]
test_agent_and_comment.head()
S_data = session.query(Station).first() $ S_data.__dict__
df_new=df_new.drop('CA',axis=1) $ df_new.head()
p_new = df2[df2['converted'] == 1]['user_id'].count() / df2['user_id'].count() $ p_new
g.columns = ['customer','validOrders'] $ print(g["validOrders"].sum()) $ print(g.shape) $ g.head()
diff = p_new - p_old $ print(diff)
xml_in_sample = xml_in.sample(500000)
nflx= web.DataReader("NFLX", 'yahoo', start, end)[['Adj Close']].pct_change()['2012-07-20':'2013-07-23']*100 $ gspc_nflx = web.DataReader("^GSPC", 'yahoo', start, end)[['Adj Close']].pct_change()['2012-07-20':'2013-07-23']*100 $ tbl4 = pd.concat([nflx,gspc_nflx], axis = 1, join='outer') $ tbl4.columns=['NFLX','GSPC'] $ tbl4.head()
sim_diff = new_page_conv/n_new - old_page_conv/n_old $ sim_diff
df_twitter_copy = df_twitter_copy[df_twitter_copy.expanded_urls.notnull()]
df_regression.head()
new_df = new_df.set_index('index')
sample_size = compute_sample_size(prop1 = 0.0972, min_diff = 0.02) $ print('sample size required per group:', sample_size)
df_prep16 = df_prep(df16) $ df_prep16_ = pd.DataFrame({'date':df_prep16.index, 'values':df_prep16.values}, index=pd.to_datetime(df_prep16.index))
cooks_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/cooks.csv')['Body'].read().decode('utf-8') $ cooks = pd.read_csv(StringIO(cooks_csv_string), header=0)
compound_wdate_df0 = pd.DataFrame(np.array(save_compound_list_with_date[0]).reshape(100,3), columns = ['Compound', 'Date', 'User']) $ compound_wdate_df1 = pd.DataFrame(np.array(save_compound_list_with_date[1]).reshape(100,3), columns = ['Compound', 'Date', 'User']) $ compound_wdate_df2 = pd.DataFrame(np.array(save_compound_list_with_date[2]).reshape(100,3), columns = ['Compound', 'Date', 'User']) $ compound_wdate_df3 = pd.DataFrame(np.array(save_compound_list_with_date[3]).reshape(100,3), columns = ['Compound', 'Date', 'User']) $ compound_wdate_df4 = pd.DataFrame(np.array(save_compound_list_with_date[4]).reshape(100,3), columns = ['Compound', 'Date', 'User'])
!cp ./stockdemo-model/score.py ./
import scipy.stats $
tech=sess.get_historical_data(['msft us equity','aapl us equity'],'px last', format=bp.data_frame('date', 'Security')) $ tech.head()
import sqlite3 $ conn = sqlite3.connect("database.db") $ cursor = conn.cursor() $
total_users = df2['user_id'].count() $ converted_users = df2[df2['converted'] == True]['user_id'].count() $ converted_users / total_users $
prog = re.compile('\d{3}-\d{3}-\d{4}') $ result = prog.match('123-456-7890') $ print(bool(result)) $ result = prog.match('1123-456-7890') $ print(bool(result)) $
nodes.head()
X = firstWeekUserMerged.groupby('userid')["action"].count().reset_index(name="totalAct") $ print(X.head(5)) $ print(X.shape) $ assert X.shape[0], numUsers
df_twitter_archive_master.sort_values(by=['favorite_count'],ascending=False).head(5)
key = "10c5e2b9f97644c7bead47a4ba1f1b55" $ api = articleAPI(key) $
plt.close()
df_ad_airings_filter_3 = df_ad_airings_filter_3[df_ad_airings_filter_3['start_time'] > '8/1/2016']
fps.model_dir
print('distinct_storeType:', store['StoreType'].unique(), '\ndistinct_assortment: ', store['Assortment'].unique(), '\ndistinct_promoInterval: ', store['PromoInterval'].unique())
from pyspark.sql import SQLContext $ sqlContext = SQLContext(sc)
df.query('converted == "1"').user_id.nunique() / len(df['converted'])
print(soup.title)
train.unique_items
df_clean.columns
df_new['intercept']=1 $ df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])
s2 = s.copy() $ s2.reindex(['a', 'f'], fill_value=0)
from sklearn.svm import SVC $ from sklearn.utils import class_weight
df[:0]   #printing column names
train_1m_ag = train_1m_ag.drop(['ip'],axis=1) $ train_10m_ag = train_10m_ag.drop(['ip'],axis=1) $ train_50m_ag = train_50m_ag.drop(['ip'],axis=1)
user.set_index("screen_name").head(3)
store_info = pd.read_csv('store.csv') $ training = pd.read_csv('train.csv') $ testing = pd.read_csv('test.csv')
violation_counts = restaurants["VIOLATION DESCRIPTION"].value_counts(); $ violation_counts[0:10]
df.Agency.value_counts()
pn_and_qty_no_duplicates = dict([(key, 0) for key in pn_and_qty['PN']]) $ pns = pn_and_qty['PN'] $ qtys = pn_and_qty['QTY'] $ for pn, qty in itertools.izip(pns, qtys): $     pn_and_qty_no_duplicates[pn] += qty
race_cols = race_vars.columns.tolist()
best_k
%load "solutions/sol_2_6.py"
pd.Period('2012-1-1 19:00', freq='5H')
closest_id = sim.get(df.iloc[-100]['skills']) $ print('Intersection:', sim.df_new.iloc[closest_id].sum(), 'tags')
df2['intercept']=1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2.head()
leadDollarsClosedPerMonth.mean().plot.bar(figsize={12,6}) $ plt.title('Average Opportunity Revenue per Month');
print( "The best accuracy was with", mean_acc.max(), "with k=", mean_acc.argmax()+1) 
sales_change2.head()
all_acct_ids = contest_savm.where\ $ ((F.col('postal_code').rlike("^95630$"))  & (F.col('start_date')>=test[0][2]))#.take(50) $ actual_acct_id = all_acct_ids.where(F.col('sales_acct_id')==test[0][1]) $ print actual_acct_id.count(),'/',all_acct_ids.select('sales_acct_id').count() $ print 'Distinct sales_acct_ids: ',all_acct_ids.select('sales_acct_id').distinct().count()
print("Number of Techniques in PRE-ATT&CK") $ techniques = lift.get_all_pre_techniques() $ print(len(techniques)) $ df = json_normalize(techniques) $ df.reindex(['matrix', 'tactic', 'technique', 'technique_id', 'detectable_by_common_defenses', 'contributors'], axis=1)[0:5]
null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size) $ plt.hist(null_vals); $ plt.axvline(p_observed, c='red');
import json $ from config import consumer_key,consumer_secret,access_token,access_token_secret
print('textrank:') $ print(matt1_tr)
dataframe.head(20)
df_prep12 = df_prep(df12) $ df_prep12_ = pd.DataFrame({'date':df_prep12.index, 'values':df_prep12.values}, index=pd.to_datetime(df_prep12.index))
measure.describe()
timezones = DataSet['userTimezone'].value_counts()[:10] $ timezones
imagehash.hex_to_hash('ff6042c59f3870e2') - imagehash.hex_to_hash('fe6046c59f3870e2')
ratings.head()
data['Date']=data['Date'].apply(lambda x : pd.to_datetime(pd.to_datetime(x).strftime('%Y%m%d')))
!hdfs dfs -cat 31results-output/part-0000* > 31results-output.txt $ !head 31results-output.txt
afx = dict(r.json())
w = 'holocaust' $ model.wv.most_similar (positive = w)
%load "solutions/sol_2_34a.py"
tweets_clean.retweet_count.mean()
Magic.__dict__['__repr__'].__get__(None, Magic) $
data.dtypes
!pwd
census_finaldata = census[['GEOID_tract','geometry']].merge(threeoneone_census_complaints.drop('geometry',1),how='outer',left_on='GEOID_tract',right_on = 'GEOID_tract') $ census_finaldata = gpd.GeoDataFrame(census_finaldata.drop('geometry',1),geometry=census_finaldata['geometry'],crs=census.crs)
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline $ import seaborn as sns
m.fit([inputs], targets, epochs=1, batch_size=64, validation_split=0.1)
word_features
auth = OAuth1(CONSUMER_KEY, CONSUMER_SECRET, TOKEN, TOKEN_SECRET)
session.query(Measurement.station, func.count(Measurement.station))\ $     .group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()
feature_freq = pd.DataFrame(sorted(freqs, key = lambda x: -x[1]))
autos['odometer_km'].describe()
df_elect.info()
ax = df[['counter', 'label']].head(20).plot(kind = 'bar', color = '#5566b7', alpha=0.6, figsize=(15, 5)) $ ax.set_xticklabels(df['label'].str.decode('utf8'), rotation=40, fontsize=12, ha='right')
def label_abpage (row): $    if row['group'] == "control" : $       return 0 $    elif row['group'] == "treatment" : $       return 1
print("Probability of user converting:", df2.converted.mean())
saem_base_url = 'https://www.saem.org' $ saem_women = 'https://www.saem.org/docs/default-source/awaem/best-practices-awaem.pdf?sfvrsn=cb373dfd_0' $ saem_women_save = 'saem_women.html'
highest_scores = fcc_nn.loc[fcc_nn['score'] >= 10000].reset_index().drop(['level_0'], axis=1) $ highest_scores
ts
print(bigdf.shape) $ print(bigdf_read.shape)
file_name = f'data/tmp/F-{STAMP}-gain-raw'
time2 = parser.parse(dfnychead["Created Date"][1]) $ print "time2 =", time2 $ print "time2.year =", time2.year
print("Logistic Regression") $ param_grid = {'solver':['newton-cg'], 'n_jobs':[-1], 'max_iter':[100, 300, 400], 'C':[0.01, 0.1, 1.0, 10.0, 100, 1000]} $ train_give_me_a_value(LogisticRegression(solver='newton-cg', n_jobs=-1), param_grid)
full_dataset.to_pickle('rolling_dataset.pkl')
dfz['retweet_count'].plot(color = 'red', label='Retweets') $ dfz['favorite_count'].plot(color = 'blue', label='Favorites')
new_texas_city.columns = ["Measurement_date", "Compaction"] $ new_texas_city.loc[546, "Measurement_date"] = "2015-07-23 00:00:00" $ new_texas_city.tail(10)
text = "Sample file text" $ saveFile = open("exampleFile.txt","w") $ saveFile.write(text) $ saveFile.close()
stocks_pca_m4
n_new = df2[df2['group'] == 'treatment']['user_id'].count() $ print('Elements of treatment group n_new: ',n_new)
from pytrends.request import TrendReq $ import pandas $ import os $ path = os.path.dirname(os.path.realpath('__file__'))
d.head()
df_count_clean.info()
type(counts)
transformed.plot(kind='scatter', $                  x='X', y='Y', $                 figsize=(20,16)) $ matplotlib.pyplot.axis('off')
print('proportion of users converted: {:.5f}'.format(df['converted'].mean()))
import re
conf_matrix = confusion_matrix(y_tfidf_test, lr2.predict(X_tfidf_test), labels=[1,2,3,4,5]) $ conf_matrix
import requests $ from bs4 import BeautifulSoup $ import pandas as pd
for item in top_three: $     print('{} has a std of {}.'.format(item[0], item[1]))
main_dir = "/Volumes/LaCie/Research/NAC/" $ data_dir = "{}data/".format(main_dir) $ out_dir = "{}output/".format(main_dir)
mit.info()
df2[df2['converted']==1].user_id.nunique()/df2.user_id.nunique()
df.shape
tweet_json_clean['followers_count'] = tweet_json_clean.user.map(lambda x: x['followers_count'])
import numpy as np $ import pandas as pd $ pd.set_option('display.notebook_repr_html', False) $ pd.set_option('display.max_columns',10) $ pd.set_option('display.max_rows',10)
print(gridCV.best_params_) $ print(gridCV.best_score_)
df = pd.read_excel('D:/knowledge/college docs/DSP/Project2/FinalVideoListCommentPrevInfo_1.xlsx')
for word in first_words.take(10): $     print word
df_twitter_copy['rating_numerator'] = extract['rating_numerator'] $ df_twitter_copy['rating_denominator'] = extract['rating_denominator']
np.sin(df * np.pi / 4)
s.get(2) # ---> Returns '12'
cityID = 'fef01a8cb0eacb64' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Akron.append(tweet)   
df7 = df[df['hired'] ==1] $ print("Please see the table below for answers") $ display(df7.groupby('category')['position','hourly_rate','num_completed_tasks'].mean().reset_index() \ $     .rename(columns={'category': 'category', 'position': 'average_position','hourly_rate':'average_hourly_rate', \ $                  'num_completed_tasks':'average_num_completed_tasks'}))
autos[['date_crawled','ad_created','last_seen']][0:5]
xgb_pred = gbm.predict(X_test)
txt_count = sc.accumulator(0)
df[:5][['id', 'price_doc']] $
precipitation_df.head()
metadata.create_all(engine)
learn.load('clas_0')
df = file.to_dataframe(mode='fwf') $ df.head()
rh = weather_yvr['Relative Humidity (%)'] $ rh.head()
df2.groupby(['group'],as_index=False).mean()
from src.pipeline import pipeline_json $ pj = pipeline_json('../data/data.json')
my_model_q2 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_rf, training='label') $ cross_validation.cross_val_score(my_model_q2, X_test, y_test, cv=10, scoring='accuracy')
df_h1b_mv_ft.pw_1.describe()
measurement_results = session.query( $     Measurements.station, Measurements.date, Measurements.prcp, Measurements.tobs).all()
cnx = sqlite3.connect('database.sqlite') $ df = pd.read_sql_query("SELECT * FROM Player_Attributes", cnx)
trump = pd.read_json("PresidentTweets.json")
import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
df = pd.DataFrame(np.random.randn(4, 3), index=index, columns=['A','B','C']) $ df
data = pd.read_csv("links.csv") $ data["created_utc"] = pd.to_datetime(data["created_utc"]) $ data.head(10)
df['waiting_days'].describe()
ac['Registration Date'].describe()
print('Data imported at: ', datetime.datetime.now())
ts3 = pd.Series(np.random.randn(100), index = pd.date_range('1/1/2003', periods=100)) $ ts3 = ts3.cumsum() $ ts3.plot() $ plt.show()
MonthlyReturns = np.asarray([meanMonthlyReturns['BEML'],meanMonthlyReturns['Glaxo'], $                              meanMonthlyReturns['Infy'],meanMonthlyReturns['Unitech']]) $ MonthlyReturns
taxiData = taxiData2 $ taxiData["Tip_percentage_of_total_fare"] = Tip_percentage_of_total_fare $
Meta2 = Meta('M2',(),{}) $ print_classes(Meta2)
data['temp'] = pd.to_datetime(data['temp']) $ data['Week Ending Date'] = pd.to_datetime(data['Week Ending Date']) $ data.head(5) $
z3.head()
df_hdf = dd.read_hdf(target, '/data') $ df_hdf.head()
forecast = m.predict(future) $ forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()
df.query('director == "Unknown"').head(20) $ df.query('production_companies == "Unknown"').head(20) $ df.query('genres == "Unknown"').head(20)
merged2.head()
knn= KNeighborsClassifier( n_neighbors=3,  $                            weights='uniform') $ scores = cross_val_score(knn,  X_test, y_test,  cv=5) $ np.mean(scores), np.std(scores)    # scoring on my Testing Data set the same so similar to the training data set 
id_pickup_label=pickup_kmeans.labels_ $ id_dropoff_label=dropoff_kmeans.labels_ $ id_ride_label=ride_kmeans.labels_
try: $     redirect_url = auth.get_authorization_url() $ except tweepy.TweepError: $     print('Error! Failed to get request token.')
flights.air_time = [d.seconds/60 for d in (flights.wheels_on - flights.wheels_off)] $ flights.arrival_delay = [d.seconds/60 for d in (flights.arrival_time - flights.scheduled_arrival)]
b[0] # label (index)-based!!
nodes['is_lab'] = nodes['user'].apply(lambda x: islab(x)) $ ways['is_lab'] = ways['user'].apply(lambda x: islab(x)) $ lab = nodes[nodes['is_lab']==1] $ lab2 = ways[ways['is_lab']==1]
top_songs['Streams'].dtype
tweets_df_lang = tweets_df.groupby('language')[['tweetRetweetCt', 'tweetFavoriteCt']].mean() $ tweets_df_lang $
gdf_gnis = gpd.read_file('gnis_20180601.geojson')
tz_cat['tweetRetweetCt'].max() $ tz_cat.index[tz_cat['tweetRetweetCt'] == tz_cat['tweetRetweetCt'].max()].tolist()
df_new['country'].unique()
bmp_series
control_converted = df2[((df2['group'] == 'control') & (df2['converted'] == 1))].count()[0] $ control_all = df2[((df2['group'] == 'control') )].count()[0] $ print('Given that an individual was in the control group, the probability they converted is {}'.format(round((control_converted/control_all),4))) $ print('\nConverted in control is {} '.format(control_converted)) $ print('Total number of control group {}'.format(control_all))
new .head(5)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)
image_predictions.info()
autos.head()
pvt.info()
df['source'].value_counts()
plt.boxplot(data) $ plt.ylim(0, 20000) $ plt.xlabel('Number of images') $ plt.ylabel('Number of retweets') $ plt.title('Number of Images vs Retweets');
df2 = df $ df2.drop(mismatch_all.index,inplace=True)
X_ls.shape
df2.query('landing_page == "new_page"').shape[0]/df2.shape[0]
tweet_archive = pd.read_csv('twitter-archive-enhanced.csv') $
df = pd.read_csv("data/311_Service_Requests_from_2010_to_Present.csv",nrows=50000)
from sklearn.ensemble import RandomForestRegressor
sub_dataset[['WordCount']].describe()
urban_driver_total = urban_type_df.groupby(["city"]).mean()["driver_count"] $ urban_driver_total.head() $
test_df = pd.DataFrame.from_records(data_spread_list) $ test_df.columns = ['field', 'raw_data', 'relative stdev']
image_pred.head(2)
import pyspark.sql.functions as func $ test.groupBy().agg(func.max(col('id'))).show()
objectFeaturesStore = ['StoreType', 'Assortment', 'PromoInterval']
import statsmodels.api as sm $ convert_old = df[(df.landing_page == 'old_page') & (df.converted == 1)].count()[0] $ convert_new = df[(df.landing_page == 'new_page') & (df.converted == 1)].count()[0] $ n_old = df[df.landing_page == 'old_page'].count()[0] $ n_new = df[df.landing_page == 'new_page'].count()[0]
100 * ab_data[ab_data.converted==1].shape[0] / ab_data.shape[0]
df[ $     (df['datetime'] > datetime(2016, 1, 1)) & $     (df['datetime'] < datetime(2017, 1, 1)) $ ].set_index('datetime').resample('1d').aggregate('count').plot()
df_2015['sales_jan_mar']=[y if ((x.month >=1) & (x.month <=3)) else 0.0 for x, y in zip(df['Date'],df['sale_dollars'])]
len(a) + len(b) $
for topic in topic_words: $     print(topic[:20])
tweets[0]._json['created_at'], tweets[0]._json['id'], tweets[0]._json['user']['id'], tweets[0]._json['user']['screen_name'], tweets[0]._json['user']['followers_count']
plt.figure(figsize=(8, 5)) $ plt.title('Max value of the #favs by property is_company') $ prepared_train.groupby('is_company').favs.max().plot.bar(); $ plt.xticks(rotation='horizontal');
pdiffs = np.array(p_diffs) $ act_diff = df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean() $ (act_diff < pdiffs).mean()
dbl2 = dbl.reset_index() $ dbl2['first_date'] = dbl2.groupby('district_id').create_date.transform(min) $ dbl2['first'] = dbl2.create_date == dbl2.first_date $ dbl2.hist('install_rate', by='first') $
df_new.country.value_counts()
p_new = (df2['converted']==1).mean() $ p_new
df.to_csv("prepped_data.csv", index=False)
prob_conrol_converted = df2[df2['group'] == 'control']['converted'].mean() $ print (prob_conrol_converted)
total =  np.hstack((onehot_encoded, train_cleaned_df[list(range(34))])) $ X_train = total[:int(total.shape[0] * 0.8),:-1] $ y_train = total[:int(total.shape[0] * 0.8),-1] $ X_test = total[int(total.shape[0] * 0.8):,:-1] $ y_test = total[int(total.shape[0] * 0.8):,-1]
data2['date'] = data2['dates'].dt.date $ data2['day'] = data2['dates'].dt.day $ data2['month'] = data2['dates'].dt.month $ data2['year'] = data2['dates'].dt.year
tf.logging.set_verbosity(tf.logging.INFO) $ shutil.rmtree('taxi_trained', ignore_errors=True) # Start fresh each time by clearing the directory $ model = tf.contrib.learn.LinearRegressor( $       feature_columns=make_feature_cols(), model_dir='taxi_trained') $ model.fit(input_fn=make_input_fn(df_train), steps=10);
knn = KNeighborsClassifier(n_neighbors = 100) $ knn.fit(x_train,y_train > 0)
train_data.dtypes
l=test_scores.select('score').rdd.map(lambda x: x.score).collect()
df["grade"]
autos['price'].value_counts().head()
print(gs_rfc_over.best_score_) $ print(gs_rfc_over.best_params_)
df.head()
max(a, key=a.get)
investments.head()
df_users = df_user_count[df_user_count > 5] $ df_users.count()
test = pd.Series(list(set(aggregates['campaign_name']))).apply(lambda x: format_cust_campaign(x))
df = pd.read_pickle(pretrain_data_dir+'/pretrain_data_01.pkl') $ loaded_shape = df.shape $ print ("Data shape: ",loaded_shape) $ df = df.sort_values(by=['serial_number','date'],axis=0,ascending=True)
rspmhigh['state'].value_counts()
print(classification_report(yy_test, logmodel_predictionsX))
oil_pandas.isnull().sum()
All_tweet_data_v2.expanded_urls.fillna("Unknown", inplace=True)
df.shape
tf_idf = tfidf_vectorizer.fit_transform(df['body'])
new_page_converted = np.random.choice([0, 1], p=[(1-p_new), p_new], size=n_new)
mean_absolute_percentage_error(y_test, y_pred)
pm_data = pd.read_csv('../data/predictive_maintenance_dataset.csv') $ print(pm_data.shape) $ pm_data.head()
bookings.head()
spark_df.limit(10).show()
temp_cat.get_values()
import statsmodels.api as sm $ convert_old =  df2.query("landing_page == 'old_page' and converted == 1").shape[0] $ convert_new =  df2.query("landing_page == 'new_page' and converted == 1").shape[0] $ n_old = df2.loc[df2["landing_page"]=="old_page",].shape[0] $ n_new = df2.loc[df2["landing_page"]=="new_page",].shape[0]
new_page_converted = np.random.choice(2, n_new, p=[1-p_new, p_new])
soup = BeautifulSoup(page.text, 'html.parser') $ print(soup.prettify())
import numpy as np $ X_feat = np.zeros((len(text_tw), 300)) $ X_feat[:,:] = lines $ X_tweets = text_tw
station_cluster.head()
for r in k.groupby('Gender'): $     print r
pgh_311_data_merged = pgh_311_data.merge(pgh_311_codes, left_on="REQUEST_TYPE", right_on="Issue") $ pgh_311_data_merged.sample(10)
ffr.resample("7D").max().head(10)
data.name.isnull().sum()
tweet_image_clean.shape
tweets_master_df[tweets_master_df.isnull().any(axis=1)]
import collections $ unique_reacts = collections.Counter() $ for c in react_counts: $     for r in c: $         unique_reacts[r] += 1
!which chromedriver $ executable_path = {'executable_path': '/usr/local/bin/chromedriver'} $ browser = Browser('chrome', **executable_path, headless=False) $ url_hemispehere = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars" $ browser.visit(url_hemispehere) $
list = [BAL, CHI, CIN, HOU, NYG, PHI, PIT, SEA] # List of all the data sets $ for team in list: $     team["Per Seat Price"] = team["Total Price"] / team["# Seats"] # Creates a new column for Per Seat Price
datetime.strptime('1.14.86', '%m.%d.%y')
print(f'Proportion of users converted by total count:  {users_converted/users_total:.4f}') $ print(f'Proportion of users converted by unique count:  {users_conv_uniq/users_unique:.4f}')
modelslist = autodf['model'].value_counts() $ top_models = list(modelslist.index) $ sns.factorplot('model',data=autodf,kind='count',size=50,x_order = top_models) $ p.set_xticklabels(rotation=90) $
contract_history.dtypes
taxi_weather_df = pd.concat([taxi_hourly_df, weather_df], axis=1)
results.summary()
yhat_prob = loanlr.predict_proba(X_test) $ yhat_prob
X = df2[['intercept', 'ab_page', 'CA', 'US']]
import sys $ reload(sys) $ sys.setdefaultencoding('utf8')
print ("Filtered records for query 1 :  ", len(final_location_ll))
m_plus = 0.9 $ m_minus = 0.1 $ lambda_ = 0.5
yc_merged_drop.head()
temp_df.info()
dates = pd.date_range('2018-01-01', '2018-05-23', freq='M') $ dates
count_categories = kickstarter.groupby('main_category').size() $ count_categories
train.tail()
df.index
ppm_title = preProcessor_in_memory(hueristic_pct=.99, append_indicators=True, padding='post', keep_n=4000, maxlen=12) $ vectorized_title = ppm_title.fit_transform(data_to_clean_title)
logit_mod = sm.Logit(df3.converted, df3[['intercept', 'ab_page']]) $ result = logit_mod.fit() $
pd.scatter_matrix(df, color='k', alpha=0.5, figsize=(12, 6)) $ tight_layout()
start_from_user = date(2006, 1, 1) $ end_from_user = date(2017, 5, 31)
z_stat, p_val = stats.ranksums(virginica, versicolor) $ print('MWW RankSum p-value %0.15f' % p_val)
X_train.head()
pdf, r_clean, hist_alloc = get_perf_ts(dwld_key, px, freq, 20, 0.1)
ndExample = df.values $ ndExample
pd.DataFrame(tfidf_vecs.todense(), $              columns=tfidf.get_feature_names() $              ).head()
path='C:\\Users\\Christopher\\Google Drive\\TailDemography\\outputFiles\\' $ filename = path+"data that could not be processed by refineLizardNumber.csv" $ df.loc[df.lizardNumber.isin(unprocessed)].to_csv(filename) $ print(filename)
print(pd.DataFrame(test_bow).head())
total = df.shape[0] $ df[df.converted == 1].shape[0]/total
arthritis_sans_weights[:5] $ trimmed_arth_sans_weights = [x for x in arthritis_sans_weights if x > 2] $ plt.hist(trimmed_arth_sans_weights, bins=100)
df.shape[0]-(df.query("landing_page=='new_page' and group=='treatment'").shape[0])-df.query("landing_page=='old_page' and group=='control'").shape[0]
goal_rate['successful'].plot.bar(title='Rate of success vs goal');
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/adult.data.TAB.txt" $ mydata = pd.read_table(path, sep= '\t') $ mydata.head(5)
import requests $ import json $ from collections import defaultdict $
np.any(x < 0)
closeSeriesQ = closingPrices.resample('Q-sep').mean()
df2[df2['landing_page']=='old_page'].shape[0]/df.shape[0]
giss_temp.fillna(method="ffill").tail()
train_df.head()
def is_sorted(l): $     return all(l[i] <= l[i+1] for i in xrange(len(l)-1)) $ for key, grp in part1_flt.groupby(["campaign_id"]): $     print "{0: <20}{1}".format(key, is_sorted(grp.campaign_spend.values))
df['gender']=df['gender'].replace({'Male': 0, 'Female': 1}) $ for col in ['Partner','Dependents','PhoneService','PaperlessBilling','Churn']: $      df[col].replace({'Yes': 1, 'No': 0}, inplace=True)
en_es = pd.read_csv(FPATH_ENES, sep=" ", header=None) $ en_es.columns = ["en", "es"] $ en_es.describe()
list(df_tsv.columns.values)
df3 = df2.set_index('user_id').join(ct_df.set_index('user_id'))
model_df['score_str'] = "x" $ model_df.score_str[model_df.score <= model_df.score.quantile(.5)] = "below_avg" $ model_df.score_str[model_df.score > model_df.score.quantile(.5)] = "above_avg"
twitter_status = unique_urls[(unique_urls.domain == domain) & (unique_urls.url.str.count('/') == 3)] $ twitter_status.sort_values('num_authors', ascending=False)[0:50][['url', 'num_authors']]
page_soup.body.div
log_mod_2 = sm.Logit(df_new['converted'], df_new[['CA', 'UK', 'intercept']]) $ result_2 = log_mod_2.fit() $ result_2.summary()
criteria = so['answercount'] > so['score'] $ so[criteria].head()
extract_deduped_with_elms_v2.shape, extract_deduped_with_elms.shape
conv_count_each_country = df_new.groupby('country')['converted'].sum() $ conv_rate_each_country = conv_count_each_country / countries_count $ print(conv_rate_each_country) $ conv_rate_each_country.plot.bar()
patient_group = pd.DataFrame.from_csv('/Users/jeff/Desktop/K2_DataScience/DANA_Simband_copy/orthostatic_experiment_results/OrthostaticTestingSt_DATA_2018-06-16_0913.csv') $ patient_group.reset_index(drop=False, inplace=True)
intervention_history.dtypes
df2['landing_page'].value_counts()
data['year'] = 2013 $ data
pd.DataFrame({'Adaptive Capacity' : [x[1] for x in HARVEY_92_USERS_AC]}).hist(bins=[1,2,3,4,5,6,7,8,10,100,200,1000])
import pandas as pd $ names = pd.Series(data) $ names
sns.countplot(x="dayofweek",data=twitter_final)
tweet_scores = tweet_scores[['id', 'retweet_count','favorite_count']] $ tweet_scores.head()
events_df['event_time'] = events_df['event_time'].apply(lambda s: datetime.datetime.strptime(str(s),'%Y-%m-%d %H:%M:%S'))
df_archive.text[0]
plt.clf() $ fig = plt.figure(figsize =(15,10)) $ mean_dry_for_plots = dry_ndvi.mean(dim =('x','y')) $ plt.scatter(mean_dry_for_plots.time.data,mean_dry_for_plots.data) $ plt.show()
final_job = job_results["data"]["job"] $ final_result = job_results["data"]["result"] $ accuracy = final_result["acc_data"]["accuracy"] $ predictions = final_result["predictions_json"].get("predictions", [])
titanic.embarked.value_counts(sort=False).plot(kind='bar')
col['date_time'] = col.index.map(str) + " " + col["TIME"] $ col['date_time'] = pd.to_datetime(col['date_time']) $ col['date_time']=pd.to_datetime(col.date_time.dt.date) + pd.to_timedelta(col.date_time.dt.hour, unit='H')
df["converted"].value_counts()[1]/df.shape[0]
df_weather.to_csv("weather_by_hour.csv")
my_data = sc.textFile("file:/path/*")
%%time $ dask_df.body.apply(lambda x: sleep(.0001)).head()
atdist_opp_dist_info_count_prop_overall = compute_count_prop_overall(atdist_opp_dist[atdist_opp_dist['infoIncluded']], 'emaResponse') $ atdist_opp_dist_info_count_prop_overall
result1 = (df['A'] + df['B'] / df['C'] - 1) $ result2 = pd.eval("(df.A + df.B) / (df.C - 1)") $ np.allclose(result1, result2)
df_wm['blobw'] = df_wm['cleaned_text'].apply(TextBlob)
from awshelpers import rshelp $ import pandas as pd $ pd.set_option('display.max_columns', None) $ pd.set_option('display.max_rows', 100)
df_group=df2.groupby('group') $ df_group.describe() $ a=0.120386 $ b=0.118808 $
df[(df.year==2017) & (df.label==1)]['description']
twitter_archive.timestamp
for tz in pytz.all_timezones: $     print(tz)
url = "https://raw.githubusercontent.com/miga101/course-DSML-101/master/pandas_class/TSLAday.csv" $ tesla_days = pd.read_csv(url, index_col=0, parse_dates=True) $ tesla_days
cars[cars.Model.str.contains('Toyota')]
df_members = pd.read_csv('C:/Users/ajayc/Desktop/ACN/2_Spring2018/ML/Project/WSDM/DATA/members_v3.csv', parse_dates=['registration_init_time'], dtype={'city': np.int8, 'bd': np.int16, 'registered_via': np.int8})
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.999) $ nb = MultinomialNB() $ nb.fit(x_train, y_train) $ nb.score(x_test, y_test)
conn.upload(df, casout=dict(name='iris_df', caslib='casuser'))
header = cylData.first() $ cylHPData= cylData.filter(lambda line: line != header) $ print (cylHPData.collect())
from nltk import tokenize
dta.head(5)
import seaborn.apionly as sns $ sns.set_style('whitegrid') $ sns.jointplot("nb_words_content", "pp_uniq_words", data = train_data, $               kind='reg', size=6, space=0, color='b')
autos['registration_year'].describe()
autodf = rawautodf.drop('name',axis = 1) $ print( "Size of the dataset - " + str(len(autodf)))
del(StockData)
ks_name_lengths = ks_projects.groupby(['name_length', "state"]).size().reset_index(name='counts') $ ks_name_success = ks_name_lengths.drop(ks_name_lengths.index[ks_name_lengths.state != 'successful']) $ ks_name_success.set_index('name_length', inplace=True) $
local_hol=local_holidays[['date','description','locale','locale_name']] $ local_hol.columns=['date','description_local_hol','locale_local','city'] $ pd.DataFrame.head(local_hol)
plt.scatter(X2[:, 0], X2[:,1], c=dayofweek, cmap='rainbow') $ plt.colorbar()
resultids = [str(r['ResultID']) for r in results]
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/car_data.txt" $ df = pd.read_csv(path, sep ='\s+', na_values=['.']) $ df.head(5)
df_t[df_t['Shipping Method name']=='PostNL with signature 0-30kg']['Updated Shipped diff'].hist() $ pd.DataFrame(df_t[df_t['Shipping Method name']=='PostNL with signature 0-30kg']['Updated Shipped diff'].describe())
model3 = linear_model.LinearRegression() $ model3.fit(x3, y) $ (model3.coef_, model3.intercept_)
df['Category']=df['Memo'].apply(returnCategory) $ df['Single Name']=df['Name'].apply(returnName) $ df.head()
print(train_df['Tag'].value_counts()) $ print(train_df['Tag'].value_counts(normalize=True))
df2.types
data = pd.read_csv("output2.csv")
plt.figure(figsize=(10,5)) $ sns.heatmap(data = train_data.isnull(), yticklabels=False, cbar = False)
reddit.Comments.value_counts(ascending=False).head(25) #just seeing the distribution of the number of comments $
no_line_up = first_comb.count() + second_comb.count() $ no_line_up
print(my_matrix[0,2])  # Read '[0,2]'as '1st and 3rd'                                     # $ print(my_matrix[0,:])  # Read as: Select... $ print(my_matrix[:,0])  # Read as: Select... $ print(my_matrix[1:3,2]) # Read as: Select... $
textFilter = tweets["screen_name"].str.contains("Tiffany") $ tweets[textFilter].head()
datetime.now()
temp_cat   # original category object categories not changed
pred=model.predict(nn_X_test)
old_page_converted = np.random.binomial(1, pold, nold)
print('Number of unique users in the dataset is {}.'.format(df.user_id.nunique()))
overallQual = pd.get_dummies(dfFull.OverallQual)
convert_new
census_withdata = census.merge(all_boroughs_cleaned,how='inner',left_on='GEOID_tract',right_on='GEO.id2')
token_sendReceiveAvg_month.columns = ["ID","sendCntMean_mon","receiveCntMean_mon"]
data_compare['SA_google_translate'].mean()
username = "qwadada@gmail.com" # your email here $ password = "fgh654321" # your password here
df_pd = df_pd[["values","timestamp"]] $ df_pd
grouped[grouped['Word_stem']=='table']
help(plt.grid)
result = calculate_avg_order_interval()
jobs_data = pd.read_pickle('D:/CAPSTONE_NEW/jobs_data_full.pkl')
converted = df2[df2.converted==1] $ prob_conveted = converted.shape[0]/df2.shape[0] $ prob_conveted
non_grad_days_mean = records3[records3['Graduated'] == 'No']['Days_missed'].mean() $ non_grad_days_mean
pd.merge(staff_df, student_df, how='inner', left_index=True, right_index=True)
df_goog['change'] = df_goog.Close - df_goog.Open
logit_mod=sm.Logit(df3['converted'],df3[['intercept','ab_page']])
properati.count().plot(kind='bar',figsize=(20,10))
sc = SparkContext("local", "w205_2017_fall_final_project") $ sqlContext = SQLContext(sc) $ hc = HiveContext(sc)
tweets_1.head()
df['time_created'] = pd.to_datetime(df['time_created'])
freq_titles1 = jobs_data.groupby(['record.title']).size().reset_index(name='counts').sort_values('counts', ascending=False).head(50) $ freq_titles1
f.include_list_filter
gs_rfc_over.score(X_train, y_train_over)
df = df[pd.notnull(df['jpg_url'])]
train_df.groupby(['Domain'])['Tag'].nunique().value_counts()
series=np.reshape(data['valuea'].values,(np.shape(data['valuea'].values)[0],1))
twitter_dataset=pd.merge(tweet_data_copy,image_copy,on='tweet_id',how='inner') $ twitter_dataset=pd.merge(twitter_dataset,df_copy,on='tweet_id',how='inner')
df
print(len(chefdf[['name','user']]))
df_tsv.drop_duplicates() $ df_archive_csv.drop_duplicates() $ df_json_tweets.drop_duplicates()
from bs4 import BeautifulSoup $ think_tanks_response = requests.get('http://www.citizensource.com/Opinion&Policy/ThinkTanks.htm') $ think_tanks_text = think_tanks_response.text $ think_tanks_soup = BeautifulSoup(think_tanks_text, "lxml")
shelter_df_idx = model.transform(shelter_df) $ shelter_df_idx.show()
df_centered.limit(5).toPandas()
regional_holidays=actual_holidays[actual_holidays.locale=='Regional'] $ print("Rows and columns:",regional_holidays.shape) $ pd.DataFrame.head(regional_holidays)
Raw_Forecast["ID"] =  Raw_Forecast.Date_Monday.astype(str)[:] + "/" + Raw_Forecast.Product_Motor +"/" + Raw_Forecast.Part_Number.str[:] $ Raw_Forecast.head(10)
result6 = sm.ols(formula="NFLX ~ GSPC", data=tbl4).fit() $ result6.summary()
data = [{'a': 1, 'b': 2}, {'a': 5, 'b': 10, 'c': 20}] $ df = pd.DataFrame(data, columns=['a', 'b']) $ print(df.shape) $ print(df)
sample.head(5)
coming_next_reason.columns = ['NEXT_'+str(col) for col in coming_next_reason.columns]
scores = cross_validate(lr, X, y, cv=10, $                         scoring=['accuracy'], $                         return_train_score=True $                        ) $ scores $
users = mydata[['user.id','user.screen_name','user.location','user.verified','user.followers_count','user.friends_count','user.statuses_count','user.created_at']] $ users = users.drop_duplicates(subset = ['user.id']) $ users['klout_score'] = [0.0] * len(users) $ users
from skmultilearn.problem_transform import ClassifierChain $ from sklearn.naive_bayes import GaussianNB $ classifier = ClassifierChain(GaussianNB()) $ classifier.fit(X_tr[0:len(X_train)-40-1], y_ls) $ classifier.score(X_tr[0:len(X_train)-40-1], y_ls)
rounds.loc[0]
obj
rows = session.query(Adultdb).filter_by(occupation="?").all() $ print("-"*100) $ print("Count of rows having occupation '?' before delete: ",len(rows)) $ print("-"*100)
edge_types_DF = pd.read_csv(edge_types_file, sep = ' ') $ edge_types_DF
third_qtr = df['time_open'].quantile(q=0.75) $ print(third_qtr)
tweets_master_df.iloc[tweets_master_df['favorite_count'].nlargest(10).index, :]
tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['name']== 'a']
diff_1 = df2[df2['group'] == "treatment"]['converted'].mean() - df2[df2['group'] == "control"]['converted'].mean() $ (np.array(p_diffs) > diff_1).mean()
list_acceptable_exts =[] $ for k in range(len(list_of_list_of_file_extensions)): $     list_acceptable_exts.append(list_of_list_of_file_extensions[k][0])
IMDB_df.head()
df_centered.cache()
training_active_listing_dummy.shape
review_length = [] $ for i in range(len(review_body)): $     count = len(re.findall(r'\w+', review_body.loc[i])) $     review_length.append(count)
from bs4 import BeautifulSoup $ import requests
data_year_df.info()
type(data['created'])
features = ['water_source', 'water_tech', 'management', $        'source', 'adm1', 'adm2', 'pay', 'lat_deg', 'lon_deg', $        'new_install_year', 'age_well_years','time_since_meas_years', 'fuzzy_water_source', $         'fuzzy_water_tech','status_binary']
score = score.dropna() # drops null values in the score df and reassigns it to score
groceries['eggs'] = 2 $ groceries
noise['AFFGEOID'].dtypes
from scipy import optimize $ max_a = optimize.minimize(lambda x: -f(x), 37) $ print(max_a) $ f(max_a.x, True)
treaties = db.treaties $ treaty_id = treaties.insert_one(treaty).inserted_id $ treaty_id
API_KEY = ##????
resp = requests.get(stats_url)
xedb.commit() $ data = pd.read_sql("SELECT * FROM emp_seq",xedb) $ print(data)
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first() $ print(last_date)
dbdmh = dmh.DBAPI(sqlite3, c, nrecs=10)
offseason09["InorOff"] = "Offseason"
data.dropna(thresh=4)
feature_imp = pd.read_csv(r"C:\Users\nkieu\Desktop\Python\Loan data\2018-04-02\Feature importance 0420.csv") $ xx = feature_imp.loc[feature_imp.mult_gbm < 6, ['name','mult_gbm']] $ X_reduced = X[xx.name.values] # shape is around 42 $ X_test_reduced = X_test[xx.name.values] $ X_reduced.shape $
quantiles = rfmTable.quantile(q=[0.25,0.5,0.75]) $ quantiles = quantiles.to_dict()
data_archie.loc[data_archie['cur_sell_price'].isnull()].head(5)
first_result.find('strong') $ first_result.strong
learner.save_encoder('adam3_10_enc')
df_new['country'].unique()
sns.barplot(x="tweet_source",y="tweet_vader_score",data= my_tweet_df,estimator=np.mean)
rng_pytz = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz=tz_pytz)
p_new = df2[df2['landing_page']=='new_page']['converted'].mean() $ p_new
elms_all_0604.shape, elms_all_0611.shape
num_vars = list(df.dtypes[df.dtypes=='int64'].index) + list(df.dtypes[df.dtypes=='float64'].index)
reddit['comm_range'] = [1 if x > comm_median else 0 for x in reddit['num_comments']]
pairwise_count.most_common(5)
proj_df['Project Need Statement'].str.startswith('My students').value_counts()
closed_pr = PullRequests(github_index) $ closed_pr.since(start=start_date).until(end=end_date) $ closed_pr.is_closed() $ closed_pr.get_average("time_to_close_days").by_period() $ print(pd.DataFrame(closed_pr.get_ts()))
df_clean[df_clean['tweet_id'] == int(df_tweet_clean['id'].sample().values)]
reverseAAPL = AAPL.sort_index(ascending=False) $ reverseAAPL.head()
sqlContext.sql("select * from pcs where person like 'Abdullah%'").show()
ripc = ripc[(bitc.w_sentiment_score != 0.000)] $ ripc = ripc.reset_index(drop=True)
tsla_neg_cnt = tsla_neg.count()*100 $ print "{:,} users have no activity before {} ({:.2%} of DAU)"\ $       .format(tsla_neg_cnt, D0.isoformat(), tsla_neg_cnt*1./dau)
df['day_of_year'] = df.time_created.dt.date
from matplotlib.dates import MonthLocator, WeekdayLocator, DateFormatter $ months = MonthLocator() $ monthsFmt = DateFormatter("%b-%y")
appointments = appointments.set_index('AppointmentDate')
df['DAY'] = 1 $ df['DATE'] = pd.to_datetime(df[['YEAR','MONTH','DAY']])
print(x)
twitter_archive_clean.text = twitter_archive_clean.text.str.replace('&amp;','&')
fig, axarr = plt.subplots(1, 2, figsize=(12, 6)) $ _ = df_passengers["created"].plot(kind="density", ax=axarr[0]) $ _ = axarr[0].set_title("Passenger Account Creation Density") $ _ = df_pilots["created"].plot(kind="density", ax=axarr[1]) $ _ = axarr[1].set_title("Pilot Account Creation Density")
code, start, end = '1', '2017/08/01', '2017/11/10' $ sql = "select * from hundred_stocks_twoyears_daily_bar where code=%s"% (code) $ df000001 = pd.read_sql(sql, conn_helloDB)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ print(z_score, p_value)
print("{0} eligible first-time posts per day in r/Feminism".format(len([x for x in eligible_posts if x['previous.posts']>0]) / days_in_dataset)) $ print(eligible_posts[0]['created'])
groupby_month = df_master.groupby(['year','month'],sort=False)['tweet_id'].count().plot(kind='bar',figsize=(10, 5),fontsize=12) $ plt.title('Number of original tweets per month',fontsize=15) $ plt.ylabel('Count') $ plt.xlabel('') $ plt.show() $
df.info()
df=pd.read_csv("ab_data.csv") $ df.head()
commits_per_weekday = git_log.timestamp.dt.weekday.value_counts(sort=False) $ commits_per_weekday
daily = data.resample('D').sum() $ daily.rolling(30, center=True).sum().plot(style=[':', '--', '-']) $ plt.ylabel('mean hourly count')
train = pd.read_csv("wikipedia_train3.csv", parse_dates= ['date']) $ test = pd.read_csv("wikipedia_test3.csv",parse_dates= ['date'])
mnist_train = datasets.MNIST("../data", train=True, download=True, $                              transform=transforms.ToTensor()) $ mnist_test = datasets.MNIST("../data", train=False, download=True, $                             transform=transforms.ToTensor())
overallPoolArea = pd.get_dummies(dfFull.PoolArea)
rnd_search_cv.best_score_ $
bitcoin_price = pd.read_csv('/Users/Madhu/Documents/Courses/data_bds/price_BTC_oct29.csv')
n_old = df2[df2['group']== 'control'].shape[0] $ n_old
n_new = df2.query('group =="treatment"').shape[0] $ n_new
ticker = ['AAPL', 'MSFT', 'NVDA', '^GSPC'] $ start = '2015-01-01' $ end = '2017-12-31' $ closes = get_closes(tickers=ticker, start_date=start, end_date=end, freq='d') $ closes
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key='API_KEY $ r = requests.get(url)
webmap.basemap = 'gray'
pd.read_sql(f'select * from {view}', engine)
print(f'dataframe shape: {league.shape}') $ league.head()
features = [col for col in df.columns if not col == 'Close'] $ X = normalized_df[features] $ model.predict(X) $
reddit_info.columns
! ls -lah | grep .npy
type_df = new_df_left.dtypes.reset_index() $
loan_requests1.shape
precipitation_year = session.query(Measurements.date,func.avg(Measurements.prcp)) \ $              .filter(Measurements.date >= '2016-08-23').filter(Measurements.date <= '2017-08-23') \ $              .group_by(Measurements.date).all()
email_bool = [email in good_emails for email in users.email]
p_converted2 = df2['converted'].mean() $ print('The probability of an individual converting: ', p_converted2)
validation.analysis(observation_data, BallBerry_resistance_simulation_0_5)
Mars_Weather_URL = 'https://twitter.com/MarsWxReport/status/1017925917065302016' $ Weather_response = requests.get(Mars_Weather_URL) $ Weather_soup = BeautifulSoup(Weather_response.text, 'html.parser')
finals.loc[(finals["pts_l"] == 1) & (finals["ast_l"] == 0) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'pure_scorers'
cohort_activated_df.head()
!rm SIGHTINGS.csv -f $ !wget https://www.quandl.com/api/v3/datasets/WIKI/IBM.csv
actual = X_age_notnull.iloc[na_index,1]
session_df = pd.DataFrame.from_dict(session_data) $ session_df = session_df[['subject', 'experiment', 'montage', 'session', $                          'date', 'session_summary', 'electrode_type', 'location', $                          'pulse_freq', 'stim_duration']] $ session_df.head()
subwaydf.iloc[153302:153306] # this one seems to be a correct high number and I assume ones below this are also accurate
df_test.shape
ngrams_summaries = cvec_3.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(10)
results=pd.read_csv("results.csv") $ results.tail() $
knn_grid.best_estimator_
datatest.loc[datatest.surface_total_in_m2 == 0, 'surface_total_in_m2'] = np.NaN
one_station.sort_values(by=['DATE'], inplace=True)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head() $ df_new.groupby('country')['country'].count()
uniqueArtists = userArtistDF.select("artistID").distinct().count() $ print("Total n. of artists: ", uniqueArtists) $
mean=parameters[0] $ std=parameters[1] $ print("Mean: ",mean) $ print("Variance: ",std)
type(R16)
pd.Series(data=y10_train).value_counts()
winpct = pd.read_csv('All_Games_Win_Pct.csv') $ winpct['text'] = winpct['playtext'] $ winpct['date'] = pd.to_datetime(winpct['Game Date'])
learning_periods = 255 $ rebalance_frequency = 5 $ agent.learn(learning_periods, rebalance_frequency)
goog.sort_values('Date',inplace=True) $ goog.set_index('Date',inplace=True) $ goog
f_counts_week_ip.show(1)
plt.hist(null_vals); $ plt.axvline(x=ob_dif, color='red');
print('Churn ratio before the end of the trial period: ') $ print(data[(data['Shop_Status']=='disabled') & (data['Shop_Existence_Days']<30)]['Merchant_ID'].count()/a)
redundant_features = ['exchange_rate', 'currency', 'source_system', 'description']
test= test.reset_index(drop = True) $ test['floor'] = pd.Series(predictions) $ datatest = pd.concat([train, test]) $ datatest = datatest.reset_index(drop = True)
word_freq_df.tail()
df[df['converted'] == 1].user_id.nunique() / df.user_id.nunique()
ax=nypd.groupby(by=nypd.index.week).count().plot(y='Unique Key', label='NYPD') $ dot.groupby(by=dot.index.week).count().plot(y='Unique Key', ax=ax, label='DOT') $ dpr.groupby(by=dpr.index.week).count().plot(y='Unique Key',ax=ax, label='DPR') $ hpd.groupby(by=hpd.index.week).count().plot(y='Unique Key', ax=ax, label='HPD') $ dohmh.groupby(by=dohmh.index.week).count().plot(y='Unique Key', ax=ax, label='DOHMH')
tweet_df.tweet_created_at.min()
df2=df1['booking_items'].apply(pd.Series)
sel = [Measurements.date, Measurements.prcp] $ initial_date = format_latest_date - timedelta(days=365) # This will be start date from 12 months before final date of 8/23/17 $ prcp_data = session.query(*sel).\ $     filter((Measurements.date >= initial_date)).all() $ prcp_data[:10] # Display the records
dto_median_predict(new_test_data,84,38)
groceries.drop('apples', inplace = True) $ groceries
import numpy as np 
import os $ params_file_path = os.path.join(resources_dir, "params.tsv") $ text_classifier.export_params(params_file_path) $ print("Saving the configuration parameters to {}".format(params_file_path))
from sklearn.linear_model import LogisticRegression $ from sklearn.svm import SVC $ from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, VotingClassifier $ from sklearn.naive_bayes import GaussianNB $ from sklearn.tree import DecisionTreeClassifier
so_head[s]
data.nunique()
jobs.loc[jobs.FAIRSHARE == 24].groupby('ReqCPUS').JobID.count().sort_values(ascending= False)
type(_)
from sklearn.model_selection import KFold $ cv = KFold(n_splits=10, random_state=None, shuffle=True) $ estimator = Ridge(alpha=850) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
automl.fit(X_train, y_train, dataset_name='psy_prepro')
tweetering['subjectivity'].describe()
len(orders.user_id.unique())
import pandas as pd $ import json $ from sklearn.feature_extraction.text import CountVectorizer
    forest.fit(X_train,y_train) $     y_pred = forest.predict(X_test) $     y_pred = pd.DataFrame({'ans': y_pred}) $     test = pd.DataFrame({'close':clean_prices.tail(90).reset_index().close, 'ratio':y_pred['ans']}) $     test['pred_price']= test['close'] * test['ratio']
cdate=[x for x in building_pa.columns if 'date' in x] $ cdate $
df_clean3.rating_denominator.value_counts()
print("Converted users proportion is "+str(df['converted'].mean()*100))
pumashp = pumashp.to_crs(epsg=2263) $ pumashp.crs
from gensim import corpora $ dictionary = corpora.Dictionary(texts) $ dictionary.save('bible.dict') $ print(dictionary)
pickleobj('diff_specs', diff_specs)
df.head()
np.exp(-0.0149), np.exp(0.0506), np.exp(0.0408)
logit = sm.Logit(df_new['converted'], df_new[['intercept', 'US','UK']]) $ results = logit.fit() $ results.summary()
users["created_at"] = users["created_at"].astype("datetime64")
df.iloc[0:5,:12]
autos["ad_created"].str[:10].value_counts(normalize=True, dropna=False).sort_index()[35:].plot(kind="bar", title="Ad_created (35th-last)", colormap="Blues_r")
suburban_avg_fare = suburban_type_df.groupby(["city"]).mean()["fare"] $ suburban_avg_fare.head() $
tweet_place_hist = pd.crosstab(index=tweets_df["tweet_place"], columns="count") $ tweet_place_hist['place_freq'] = tweet_place_hist['count'] * 100 / tweet_place_hist.sum()['count'] $ tweet_place_hist = tweet_place_hist.sort_values('place_freq', ascending=False) $ tweet_place_hist.head(10)
posts.count()
volume = [vol[6] for vol in afxdata['data']] $ print(sum(volume)/len(volume))
plt.hist(np.log(threeoneone_census_complaints[threeoneone_census_complaints['avg_fix_time_sec']>0]['avg_fix_time_sec']),bins=100) $ plt.show()
dd2=cfs.diff_abundance('Subject','Control','Patient')
n_new = df2.query('landing_page == "new_page"').user_id.count() $ print(n_new)
feature_matrix_duplicated.shape
train = pd.read_csv("wikipedia_train3.csv") $ test = pd.read_csv("wikipedia_test3.csv")
i_aux=[] $ for i in range(0,l2): $     if i not in seq: $         i_aux.append(interventions[i]) $ col.append(np.array(i_aux))
students.columns
data.head()
merged_feature_df
irr_df.to_clipboard()
dnvs_500 = {} $ for trio in trios.iterkeys(): $     dnvs_500[trio] = get_dnvs(trios[trio], data_dir, 500)
b = bnbAx[bnbAx['language']=='fr'].first_browser.value_counts()/len(bnbAx[bnbAx['language']=='fr']) $ b.head()
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, (1 - p_new)]) $ p_new_sim = new_page_converted.mean() $ p_new_sim
c.head()
engine = create_engine("sqlite:///hawaii.sqlite", echo=False) $
mentions_df = mentions_df[(mentions_df["epoch"] >= 1503288000) & (mentions_df["epoch"] < 1505188800)] $ len(mentions_df)
transform = am.tools.axes_check(np.array([x_axis, y_axis, z_axis])) $ b = transform.dot(burgers) $ print('Transformed Burgers vector =', b)
stops_heatmap = folium.Map(location=[39.0836, -77.1483], zoom_start=11) $ stops_heatmap.add_child(heatmap_full) $
df2 = df2.join(countries.set_index('user_id'), on='user_id')
f_ip_device_hour_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_device_hour_clicks"), header=True) $ print('Found %d observations.' %f_ip_device_hour_clicks.count())
def custome_roune(stock_price): $     return int(stock_price/100.0) * 100
df_countries.head()
print (predTree [0:5]) $ print (y_testset [0:5]) $
import numpy as np $ ok.grade('q04')
field_data =  pd.read_csv("fields_data.csv")
control_grp = df2.query('group == "control"') $ control_grp_prop = len(control_grp.query('converted == 1'))/len(control_grp) $ print ("probablility of the control group converted: ", control_grp_prop)
(df2.landing_page == 'new_page').sum() / len(df2)
shiny.head()
out_train.shape, out_test.shape
now = dt.datetime.now() $ past_yr = now.replace(year=2017) $ results = session.query(Measurement.date, Measurement.prcp).\ $             filter(Measurement.date > past_yr).all() $
%load_ext autoreload $ %autoreload 2
building_pa_prc_zip_loc.head(5)
sum(df.isnull().any())
df_new[(df_new['Region'] == 'NSW1') & (df_new['periodstart'] == '2017-08-16 12:30:00')]
if True: $     from sklearn.model_selection import train_test_split $     train_model, validation_model = train_test_split(train_norm, test_size=0.2, random_state=73777) $     train_model = validation_model.copy(deep=True)
df_new = pd.merge(df2, df_countries, on='user_id') $ df_new.head()
print(DataSet_sorted['tweetText'].iloc[2])
val.drop('overdue', axis=1, inplace=True)
display(stock_data.iloc[200]) $ print("The close value of the first entry is: ", stock_data.iloc[0, 3])
df.describe()
df2['intercept'] = 1 $ df2[['control', 'ab_page']]=pd.get_dummies(df2['group']) $ df2.drop(labels=['control'], axis=1, inplace=True) $ df2.head()
continentdict = {'China': 'AS', 'Korea': 'AS', 'Canada': 'NA', 'France': 'EU', 'BRAZIL': 'SA', 'Russia': 'EU'}
df['text']= df['text'].apply(lambda x: re.sub('RT |#|@', '', x)) $ df.head(3)
store_items.fillna(method = 'ffill', axis = 1)
mg_data = pd.concat([finnal_data.iloc[:,1:24],finnal_data.iloc[:,135:144]],axis = 1) $ spire_data = spire_event.iloc[:,6:] $ spire_data.head(1)
df[df['Complaint Type'] == 'Street Condition']['Descriptor'].value_counts()
train = pd.read_csv('data/train.csv') $ test = pd.read_csv('data/test.csv') $ store = pd.read_csv('data/store.csv')
plt.hist(p_diffs,bins=30);
X = zone_train.drop('zone', axis=1) $ y = zone_train['zone'] $ X_train, X_test, y_train, y_test = train_test_split(X, y, $                                                     test_size= 0.10, $                                                     random_state=1)
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation
from sklearn.preprocessing import LabelEncoder
scores = cross_val_score(model, X, y, scoring='roc_auc', cv=5) $ print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))
calendar = temp_cal.copy() $ temp_cal = calendar.copy()
df_countries.country.value_counts()
f_close_clicks_train = spark.read.csv(os.path.join(mungepath, "f_close_clicks_train"), header=True) $ f_close_clicks_test = spark.read.csv(os.path.join(mungepath, "f_close_clicks_test"), header=True) $ print('Found %d observations in train.' %f_close_clicks_train.count()) $ print('Found %d observations in test.' %f_close_clicks_test.count())
autos["num_photos"].describe()
df_city_reviews.limit(5).toPandas()
(360+1803)/len(users)
data.loc['Colorado', ['two', 'three']]
new = new.join([model, year, country, custom, payment, hand, body, purch])
cand_date_df['sponsor_class'].value_counts()
autos["price"].sort_values(ascending=False).head(50)
best_model = models_reg[0]
row_len=df.query("(group == 'control' and landing_page == 'new_page') or (group == 'treatment' and landing_page == 'old_page')").shape[0] $ print('Number of times new_page and treatment dont line up :: ',row_len)
experiment_run_uid = client.experiments.get_run_uid(experiment_run_details) $ print(experiment_run_uid)
twitter_archive_full['retweet_count'] = twitter_archive_full.retweet_count.fillna(0.0).astype(int) $ twitter_archive_full['favorite_count'] = twitter_archive_full.favorite_count.fillna(0.0).astype(int) $
n_new
df.status_message.fillna('NA', inplace=True) $
tweets = pd.read_csv('realDonaldTrump_poll_tweets.csv')
seasons_and_teams = ALLbyseasons.groupby(["Category", "Team"]) # Groups by specific season/offseason, then by team
sarima_mod = sm.tsa.statespace.SARIMAX(endog, exog=exog, order=(1,1,1), seasonal_order=(1,1,0,52), trend='c').fit() $ print(sarima_mod.summary()) $ plt.plot(sarima_mod.resid, "bo") $ print(plot_acf(sarima_mod.resid, lags=104)) $ print(plot_pacf(sarima_mod.resid, lags=104)) $
unique_users_count2 = df2.user_id.nunique() $ print(unique_users_count2)
learn = ConvLearner.pretrained(arch, data, precompute=True)
from IPython.display import SVG $ from keras.utils.vis_utils import model_to_dot $ SVG(model_to_dot(decoder_model_inference).create(prog='dot', format='svg'))
frame2
r_lol[:3] $
list_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lists] $ print(list_of_dicts[0]) $ print(list_of_dicts[1]) $
df2[df2.duplicated('user_id')==True].user_id
y.value_counts().values[0]/float(len(y))
tree = DecisionTreeClassifier(criterion='gini') $ model = tree.fit(X_train_total, y_train) $ model.score(X_test_total_checked, y_test)
grouped.describe()
codes = pd.read_csv('data/icd-main.csv') $ codes = codes[(codes['code'] != codes['code'].shift())].set_index('code')
sns.jointplot(x = "positive_ratio", y = "wow_ratio", data = news_df)
df.sort_values(['Year'], ascending=True).head(5)
texts_trigrams = [] $ for text in texts: $     texts_trigrams.append(text + ngrams(text,2) + ngrams(text,3))
df['VALUE'] = zip(df['DIVISION'], df['DATE'], df['TIME'], df['DESC'], df['ENTRIES'], df['EXITS']) $ df['VALUE'] = df['VALUE'].apply(lambda x: list(x))
s3 = pd.Series(['red', 'green', 'blue'], index=[0, 3, 5]) $ s3
n_new = df2[df2['group'] == 'treatment']['group'].count() $ n_old = df2[df2['group'] == 'control']['group'].count() $ print('n_new:', n_new, 'n_old:', n_old)
result = pd.DataFrame(eclf3.predict_proba(test[features]), index=test.index, columns=eclf3.classes_) $ result.insert(0, 'ID', mid) $ result.to_csv("gaurav5.csv", index=False)
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05/2))) $
dfSF.tail()
stop = stopwords.words('english') $ stop.extend(additional_stop_words) $ stop_words = list(ENGLISH_STOP_WORDS) $ stop_words.extend(additional_stop_words)
merge_event.groupby('location')['user_id'].nunique().argmax()
from sklearn.linear_model import Lasso $ lassoreg = Lasso(alpha=0.001, normalize=True) $ lassoreg.fit(X_train, y_train) $ print(lassoreg.coef_)
plots.top_n_port_plots(10,traffic_type = 1) #Normal traffic top 10 ports
df_input = df.select(["Unique Key", "Created Date", "Closed Date", "Agency", "Complaint Type", "Status", "Borough", \ $                       "Park Borough", "Resp_time", "HOD"])
result2[result2['dt_deces'].notnull()].shape
cust_demo.get_dtype_counts()
cnt = pd.DataFrame(features.groupby('features').count()) $ cnt[cnt.cnt>train_data.shape[0]*0.1]
grp = dta.groupby(dta.results.str.contains("Pass")) $ grp.groups.keys()
y_test = df_test['loan_status'].values $ y_test[0:5]
join_a = join_a.na.fill('a',["party_name_orig","party_name","address1_orig","address1","street_name_orig","street_name"])
df2.user_id.nunique()
elbow(cluster,range(1,20))
nyt_df = pd.DataFrame.from_dict(my_old_data, orient='index')
crimes['year'] = crimes.DATE_OF_OCCURRENCE.map(lambda x: x.year)
fulldata_copy = pd.DataFrame(fulldata_copy)
(~autos["registration_year"].between(1900,2016)).sum() / autos.shape[0]
student_info = [('Mary', 100, 62, 'F'), ('Mike', 120, 66, 'M'), ('Joe', 140, 68, 'M'), ('Janet', 110, 62, 'F'), ('Steve', 160, 70, 'M'), ('Alissa', 140, 63, 'F'), ('Alison', 140, 66, 'F'), ('Maya', 110, 63, 'F'), ('Ryan', 180, 72, 'M'), ('Paul', 190, 72, 'M'), ('Michael', 200, 73, 'M')] $ students = pd.DataFrame(student_info, columns = ['names', 'weight', 'height', 'gender']) $ students
taxi_hourly_df.shape
df2.converted[df2.group == 'treatment'].mean()
plt.figure(figsize=(15,5)) $ plt.title("Number of mentions in time") $ mentions_df["epoch"].hist(bins=500) $ plt.show()
import statsmodels.api as sm $ df2.head(5) 
all_df.head()
sns.countplot(y="action",data=firstWeekUserMerged) $ plt.show()
from pyspark.sql import functions as F $ onpromotions = train_holiday_oil_store_transaction_item_test.select("onpromotion").distinct().rdd.flatMap(lambda x: x).collect() $ exprs = [F.when(F.col("onpromotion") == onpromotion, 1).otherwise(0).alias('onpromotion_' + onpromotion) $          for onpromotion in onpromotions] $
df_new.to_csv(path_or_buf='data/df_fe_comma.csv', sep=',', $               header=True, index=False, $               encoding='utf-8')
TripData_merged3.isnull().sum()
input_sequence = test_docs[random.randint(0, len(test_docs))] $ print('input sequence: ', input_sequence, '\n\nhidden states:\n') $ vec = proc.transform([input_sequence])[:,1:] $ embedding_model.predict(vec)
support.sort_values("amount", ascending=False).head()
import ___  as ___
!head -n 5 msft.csv
tweets.info()
%%bash $ rm -rf aclImdb* $ rm movie_data.csv $ pwd $ ls
print("Probability of control group converting is", $       df2[df2['group']=='control']['converted'].mean())
norm.ppf(1-(0.05))
os.chdir(temp_path) $ data_sets = {} $ data_sets['15min'] = pd.read_pickle('final_15.pickle') $ data_sets['30min'] = pd.read_pickle('final_30.pickle') $ data_sets['60min'] = pd.read_pickle('final_60.pickle')
pnew = df2[df2['landing_page']=='new_page']['converted'].mean() $ pnew
ad_group_performance.loc[ $     [5, 6, 7, 8, 15, 25] $ ]
api_df = pd.DataFrame(api_, columns = ['retweets', 'tweet_id', 'times_faved']) $ api_df = api_df.sort_values('tweet_id').reset_index(drop=True) $ api_df.head()
model_x.summary2() # For categorical X.
count_dict
df_complete.head(1)
f = lambda x: 0 if x<0 else x $ f(-1)
master_file.to_csv(os.curdir + '/master.csv', index=False)
for i in files_to_manage: $     main_tables[i] = main_tables[i].append(new_data[i])
states = pd.DataFrame({'population': population, $                        'area': area}   ) $ states
twitter_archive_master = twitter_archive_master.drop(twitter_archive_master[twitter_archive_master['p1_dog'] == False].index)
cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']] $ cdf.head(9)
df_loc['country'].fillna('Unknown').head()
def manager(x): $     if 'Manager' in x: $         return 1 $     return 0 $ df_more['Manager'] = df_more['Title'].apply(manager)
extra_bases = baseball[['X2b','X3b','hr']].sum(axis=1) $ extra_bases.sort_values(ascending=False)
df_more[50:60]
threeoneone_census_complaints['ln_median_income_new']= np.log(threeoneone_census_complaints['median_income_new']+1) $ threeoneone_census_complaints['ln_complaint_density']= np.log(threeoneone_census_complaints['complaint_density']+1) $ threeoneone_census_complaints['population']=threeoneone_census_complaints['population'].astype(float)
from pyspark.sql.types import FloatType $ floatCol = a.cast(FloatType()) $ df.select(floatCol / 100).show(5)
import pandas as pd
autos['last_seen'].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True).hist()
mean = data['len'].mean() $ print("The average length of the tweets: {}".format(mean))
old_page_converted = np.random.choice([0, 1], size=n_old, p=[converted_rate, 1 - converted_rate])
for elem in table3['browser'].unique(): $     table3[str(elem)] = table3['browser'] == elem $ table3.head(3)
print (temp_cat_more) $ temp_cat_more.remove_unused_categories()
df['Complaint Type'].groupby(by=df.index.month).count().plot()
test_sentence = test_sentence.replace(re.compile(r"\s?[0-9]+\.?[0-9]*")) $ test_sentence[0]
day = lambda x: datetime.date(int(x[0]),int(x[1]),int(x[2])).weekday() $ emotion_big_df['date']=emotion_big_df['date'].apply(day)
labels = df['fan_of_team_playing'].values $ docs = df['comment_body'].values $ vocab_size = 100000 $ encoded_docs = [one_hot(str(d), vocab_size) for d in docs]
for tweet in tweepy.Cursor(api.search, q="place:%s" % place_id_L).items(maxitems): $     detected = detect(tweet.text) $     conn.commit()
df.shape
len(resdata)
all_zipcodes = pd.merge(df, zipcodes, on='zipcode', how='left') $ all_zipcodes[pd.isnull(all_zipcodes.city_x)].head()
p=pd.Panel({"ClassA":df_A,"ClassB":df_B})
mask = combined.country.isin(['Dominican Republic', 'Colombia']) $ combined.loc[mask, 'zika_cases'] /= 10
X_train.isnull().sum()
access_logs_df.printSchema()
for c in np.unique(c_pred): $     print('{}: {}'.format(c, np.sum(c_pred==c)))
!rm ssd_mobilenet_v2_coco_2018_03_29.tar.gz
x = np.arange(6).reshape((2, 3)); x  #x has shape (2,3).
shiftlog_entries_df.head()
lm = smf.ols(formula='vFA ~ IP', data=fx).fit() $ lm.params $ lm.summary()
df_2012 = pd.DataFrame(rows)
pca.explained_variance_ratio_[0]
tweet_df['created_at'] = pd.to_datetime(tweet_df['created_at']) $ tweet_df['date'] = tweet_df['created_at'].dt.date $ tweet_df['time'] = tweet_df['created_at'].dt.hour $ tweet_df.drop('created_at', axis=1, inplace=True)
recommendation_df['hacker_count'].describe()
df2.groupby('converted').count() $
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results = logit_mod.fit()
ab_data.isnull().sum()
textFromHTML('https://en.wikipedia.org/wiki/Contoso')
ssc = StreamingContext(sc, 3)  
c = R18df.rename({'Create_Date': 'Count-2018'}, axis = 'columns')
K = 1000 $ N = K * 23 $ BALANCE = 1.0 $ QUANTILE = 0.9
def dprint(msg): $     if (debugMode): $         print(msg) $
dataset_ja = dataset.join(longest_date_each_costumer, ['customer_id'],rsuffix="_day") $ dataset_ja.head(3)
c = df[df.msno == '++1Wu2wKBA60W9F9sMh15RXmh1wN1fjoVGzNqvw/Gro='] $ c
conv_control_users = df2.query('converted == 1 and group == "control"').shape[0] $ users_Control_group= df2.query('group == "control"').shape[0] $ p2 = float(conv_control_users / users_Control_group) $ print("Given that an individual was in the control group, the probability they converted is {:.4f}".format(p2))
user_actions.map(lambda pair: (pair[0], pair[1])).sortyByKey(False).take(5)
plot = seaborn.barplot(x='group', y='sentiment', data=name_sentiments, capsize=.1)
comp_rep.head()
from pyspark.sql.functions import monotonically_increasing_id $ modeling1 = (train $              .withColumn('id', monotonically_increasing_id()) $              .drop('click_time', 'attributed_time')) $ print("modeling size: ", modeling1.count())
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'country_UK','country_US']]) $ results = logit_mod.fit() $ results.summary()
contributions.info()
learns = [x for x in all_courses if '/learn/' in x] $ learn_groups = list(chunks(learns, 50)) $ len(learns)
autos = autos.drop(["num_photos", "seller", "offer_type"], axis=1)
%%timeit -n 100 $ summary = ser7.sum()
print(heights_A.shape,end='\n') $ print(heights_A)
year_prcp_df.describe()
result=results.set_index(['date','home_team','away_team']) $ result.head()
def web3_clientVersion(): $     return _call('web3_clientVersion')
lm = sm.Logit(df_joined['converted'],df_joined[['intercept','ab_page','CA','UK']]) $ res = lm.fit()
html = html.decode('utf-8') $ print(html)
clusterer = KMeans(n_clusters=3, random_state=324) $ cluster_labels = clusterer.fit_predict(cluster) $ cluster['cluster_labels'] = cluster_labels
print_groups(grouped)
len(got_data[got_data['total_likes']>50000])
X.head(5)
furniture = data.loc[(data['hired']==1)]
df_big['dog_value'] = df_big['doggo'] # Set dog value to none
df_new[['UK','US']] = pd.get_dummies(df_new['country'])[['UK','US']] $ df_new.head()
list(np.log(df2.cv)) $
feature_names = tfidf_vectorizer.get_feature_names() $ display_features(features_tfidf[0:10], feature_names)
nnew=df2[df2.landing_page=="new_page"].count()[0] $ nnew
firefox_binary = FirefoxBinary(tor) $ if sys.platform=="win32": $     browser = webdriver.Firefox(firefox_binary=firefox_binary, capabilities=capabilities) $ else: $     browser = webdriver.Firefox(firefox_binary=firefox_binary)
df_293_table = pd.read_sql(sql_293_table,conn_laurel) $ df_293_table.sort_values(by='Values',ascending=False)
df_uro = df_uro.rename(columns = {'METAC_SITE_NM1': 'METAC_SITE'})
a.argmin(1)
formula = "log_time_detained ~ imm_hold + charge_count + male + black + hispanic + amer_indian_alaskan + asian_pacific_island" $ reg = smf.ols(formula = formula, data = data).fit() $ reg.summary()
rng.asi8[0]
weather.precipitation_inches = pd.to_numeric(weather.precipitation_inches, errors = 'coerce')
len(op_)
tweets_df = tweets_df[tweets_df['Text'].apply(lambda x: lr.is_tweet_english(x))]
tizibika= pd.read_csv('tizibika_dataset.csv')
n_old = df[df.landing_page == 'old_page'].count()[0] $ n_old
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer $ analyser = SentimentIntensityAnalyzer()
preds_df = cleaned2[['pDelta', 'tc_sentiment']]
test_line = 'piweba4y.prodigy.com - - [01/Aug/1995:00:00:10 -0400] "GET /images/launchmedium.gif HTTP/1.0" 200 11853' $ test_row = parse_apache_log_line(test_line) $ test_row
summary_data
sub1.to_csv('sub_average_5models_nocv.csv',index=False)
season08["InorOff"] = "In-Season"
total = len(list(set(list(df['id']))))
print(df2[df2['group'] == 'treatment']['converted'].mean())
hmeq_card = cassession.CASTable('hmeq_card', where='_NMISS_ > 0') $ df_hmeq_card = hmeq_card.fetch().Fetch $ df_hmeq_card $
print(joined.info())
for i, (label, col) in enumerate(agency_borough.iteritems()): $     print(i, label, col)
ListJota= [1,3,5,np.nan,6,8] $ ListJota
closes = pd.concat([msftA01[:3], aaplA01[:3]], keys=['MSFT', 'AAPL']) $ closes
df3[['CA', 'UK', 'US']] = pd.get_dummies(df3.country) $ df3
from sklearn.metrics import roc_auc_score
so.loc[(so['score'] >= 5) & (so['ans_name'] == 'Scott Boston')]
df_predictions_clean.p2 = df_predictions_clean.p2.str.title()
df = pd.read_csv('YearPredictionMSD.txt', sep=",", header=None) $ df.head()
%matplotlib inline $ closingPrices.plot();
autos.describe(include='all')
for j in test_dum.columns: $     if j not in train_dum.columns: $         print j
new_page_converted=np.random.choice([1,0],p=(P_new,1-P_new),size=n_new) $ len(new_page_converted)
org_id_table.sort_values('active_proportion', ascending = False)
tweet_json_df.head(3)
import matplotlib.pyplot as plt $ df[['Date','OilPrice']].set_index('Date').plot()
df_new.head(2) $ df_new.country.unique() $ df_new[['CA','UK','US']]=pd.get_dummies(df_new['country']) $ df_new.head(2)
import re $ import numpy as np $ import pandas as pd $ autos = pd.read_csv('autos.csv', encoding='Latin-1') $
file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())
display('df', "df.groupby(df['key']).sum()")
import api $ import pandas as pd
pd.options.display.max_colwidth=100
dframe_list = draft_dframe[draft_dframe.Draft_Yr >= 2012] $ team_list = list(set(dframe_list['Team'])) $ if 'CHH' in team_list: team_list.remove('CHH')  $ if 'BRK' in team_list: team_list.remove('BRK')   $ len(team_list)
print('Features Engineering completed at : ', datetime.datetime.now())
print(tree.getroot()) $ print(etree.tostring(tree.getroot(), pretty_print=True, method="html"))
Y_btc, X_btc = ml4t.calc_features(trade_data_btc.df_h, rolling_periods=[10], y_binary=False)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='two-sided') $ print (z_score, p_value)
df_experiment.info()
findRights()
for i in files_to_manage: $     main_tables[i].to_csv('{}_2017.csv'.format(i))
df['Market'].head(1)
y_pred = grid.predict(Xtest) $ y_pred_prob = grid.predict_proba(Xtest)
cityID = 'a592bd6ceb1319f7' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Diego.append(tweet) 
band_names = [1975, 1990, 2000, 2014] $ os.environ["bandnames"] = str(band_names) $ !earthengine asset set -p "(string)bandname=$bandnames" users/resourcewatch/cit_014_built_up_areas_nodata_set_to_zero
y_pred1 = linreg.predict(X_test) $ print np.sqrt(metrics.mean_squared_error(y_test, y_pred1))
ds_ossm = xr.open_mfdataset(data_url2) $ ds_ossm = ds_ossm.swap_dims({'obs': 'time'}) $ ds_ossm = ds_ossm.chunk({'time': 100}) $ ds_ossm = ds_ossm.sortby('time') # data from different deployments can overlap so we want to sort all data by time stamp. $ ds_ossm
df.std() #standard deviation
titanic.dropna().head()
run txt2pdf.py -o '2018-06-22 2011 FLORIDA HOSPITAL Sorted by discharges.pdf'  '2018-06-22 2011 FLORIDA HOSPITAL Sorted by discharges.txt' $
df.pupper.value_counts()
df2.query('landing_page=="new_page"')['user_id'].count()/df2.shape[0]
%%time $ grid.fit(X, y)
n_user_days.hist() $ plt.axvline(x=4, c='r', linestyle='--')
df['sourceurl'] = df['sourceurl'].str.replace('http://','') $ df['sourceurl'] = df['sourceurl'].str.replace('https://','') $ df['sourceurl'] = df['sourceurl'].str.replace('www.','') $ df['sourceurl'] = df['sourceurl'].str.split('/').str[0]
import pandas as pd $ enrollments['content_availability'] = pd.to_datetime(enrollments.content_availability)
sentiments = tweets.groupby('SA').agg({'SA':'count'}).rename(columns=({'SA': 'Count'})) $ sentiments['% of total'] = sentiments['Count']/sentiments['Count'].sum() $ sentiments ['Sentiment'] = ['Negative', 'Neutral', 'Positive'] $ sentiments = sentiments.reindex(columns = ['Sentiment', 'Count','% of total']) $ sentiments
df_countries = pd.read_csv('countries.csv')
data = open("test_data//legislators.csv").read() $ print(data[0])
df['date'] = pd.to_datetime(df['date'])
df['TUPLEKEY2'] = zip(df['C/A'], df['UNIT'], df['STATION'])
sample.asfreq('H',method='ffill')
print(df_new['slot'].value_counts()) $ df_new.loc[:,'slot_top']=np.nan $ df_new.loc[df_new['slot']=='RHS','slot_top']=0 $ df_new.loc[df_new['slot']=='Top','slot_top']=1 $ print(df_new['slot_top'].value_counts())
plt.boxplot(daily_trade_volume) $ plt.ylabel('Daily Trading Volume') $ plt.show()
engineroute = "H:/Google Drive/WORK/Groundwater Chemistry" $ sys.path.append(engineroute) $ import enginegetter
zn_array = np.loadtxt(os.path.join("Freyberg_truth","hk.zones")) $ plt.imshow(zn_array)
soup.li.parent.text
len(master_df[['name', 'tweet_id']].groupby('name').filter(lambda x: len(x)==1))
scaler = MinMaxScaler() $ data[intFeatures] = scaler.fit_transform(data[intFeatures])
locationing['subjectivity'].describe()
train_data[train_data['totals.transactionRevenue'].notnull()].groupby(['device.browser']).agg({'visitNumber': 'count'}).reset_index().set_index("device.browser",drop=True).plot.bar()
stacked=football.stack() $ stacked.head() $ stacked.unstack().head()
for script in soup(["script", "style"]): $     print script $     script.extract()
archive_clean.drop(['in_reply_to_status_id','in_reply_to_user_id','expanded_urls'],axis=1,inplace=True )
DataSet.head(10) $ DataSet.tail(5)
subs = import_submissions(course_id = "C00199", dbname="test1")
df_cal.boxplot(column='is_all_day')
storm_period_threshold = 50
df_ll = pd.read_csv("loblaws_all.csv", encoding="latin-1")
df.count()[0]
data = users.merge(df_first, left_on=['UserID'], right_on=['UserID'], how='left' ) $ data.drop(['key'], axis = 1, inplace = True)  # Column 'key' was added in a previous step that is now dropped $ data
cityID = 'e444ecd51bd16ff3' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Cincinnati.append(tweet) 
comments_per_user = scores.groupby('author')['score'].count() $ comments_per_user.sort_values(ascending=False)
header = flowerData.first() $ flowerKV= flowerData.filter(lambda line: line != header) $ print (flowerKV.collect())
apple['Date'] = pd.to_datetime(apple['Date']) $ apple.dtypes
from sklearn import preprocessing $ scaler = preprocessing.StandardScaler() $ data1Scaled = pd.DataFrame(scaler.fit_transform(data1),columns=featureList)
import re $
len(train_data[train_data.notRepairedDamage == 'nein'])
gc.collect()
pd.read_pickle('data/city-util/proc/utility.pkl', compression='bz2').head()
df_new['US_ab_page'] = df_new['US'] * df_new['ab_page'] $ df_new['CA_ab_page'] = df_new['CA'] * df_new['ab_page'] $ df_new.head()
nt.head()
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'CA']]) $ results = mod.fit() $ results.summary()
import datetime $ now = datetime.datetime.now() $ print now
top_20_breed_archive = tweet_archive_master[tweet_archive_master.dog_breed.isin(top_20_breeds)]
two_day_sample = pull_act_range('2018-04-15', '2018-04-16', auth2_client)
c_df.size
%matplotlib inline $ import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import seaborn as sns $
def clean_links(text): $     return re.sub(r"http\S+", "", text)
movies.head(5) # Brings up the first 5 rows
API_KEY = 'MHso4itbSsk44SDbsyWv'
results = session.query(measurement.date, measurement.prcp).filter(measurement.date >= prev_year).all()
lg = LogisticRegression() #C= 500, n_jobs= -1, penalty= 'l2' $
df = pd.merge(left=df,right=df_parcel,how='left',left_on='Parcel Status Id',right_on='id',validate="many_to_one") $ df = pd.merge(left=df,right=df_methods,how='left',left_on='Shipping Method Id',right_on='id',validate="many_to_one")
rural_driver_total = rural_type_df.groupby(["city"]).mean()["driver_count"] $ rural_driver_total.head()
print('Scores') $ print("Mean is {:0.2f} and the Median is {:0.2f}".format(np.mean(df.score),np.median(df.score))) $ print('Number of coments') $ print("Mean is {:0.2f} and the Median is {:0.2f}".format(np.mean(df.comms_num),np.median(df.comms_num))) $
b_dist =bnbx[(bnbx['age']<80) & (bnbx['age']>=18)] $ bnbx['age'] = bnbx.age.fillna(28) $
db = client.insight_database $ collection = db.posts
df.groupby(['group', 'landing_page']).count() $ control_newpage = df.query('group == "control" & landing_page == "new_page"').count()[0] $ treatment_oldpage = df.query('group == "treatment" & landing_page == "old_page"').count()[0] $ control_newpage + treatment_oldpage $
df1 = ndf[ndf["tripduration"]<5000] $ df2 = df1[df1["gender"]=="Male"] $ df3 = df1[df1["gender"]=="Female"] $ df4 = df1[df1["gender"]=="Unknown"]
import seaborn as sns
df.columns    # what are all te
columns = inspector.get_columns('station') $ for c in columns: $     print(c['name'], c["type"]) $
last_year = stops[stops["date"] > datetime.datetime(year=2017, month=3, day=28)]
telemetry = pd.read_csv('Data/PdM_telemetry.csv') $ errors = pd.read_csv('Data/PdM_errors.csv') $ maint = pd.read_csv('Data/PdM_maint.csv') $ failures = pd.read_csv('Data/PdM_failures.csv') $ machines = pd.read_csv('Data/PdM_machines.csv')
type(data)
print(classification_report(y_test,test_prediction))
result
result = crossval.decisiontree.gbtreescore(model='gb_model', $                                             casout=dict(name='hmeq_scored_gb', replace=True), $                                             copyVars=['bad','_PartInd_']) $ hmeq_scored_gb = result.OutputCasTables.ix[0,'casTable'] $ result.ScoreInfo
Zn_post = model.predict_proba(X)[-1]
df.loc[df.full_sq < df.life_sq,'life_sq'] = np.NaN
fields_node = columns['fields'] $ fields_node.keys()
sb.heatmap(components2)
df_new['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'CA']]) $ results = log_mod.fit() $ results.summary()
data = df[columns] $ data = data.reset_index(drop=True)
negative_visits = cats_df[cats_df['number of vet visits'] < 0]['number of vet visits'] $ cats_df['remove'].iloc[negative_visits.index] = True $ del negative_visits
dups = df.duplicated().sum() $ print("Our dataframe contains {} duplicated rows".format(dups))
hpdpro[hpdpro['ComplaintID']=='8338184']['StatusDescription'].tolist()
clf = MultinomialNB() $ clf.fit(train_data, train_labels) $
tweet_archive_clean['name'] = tweet_archive_clean['name'].replace('None', np.NaN)
to_be_predicted_Day2 = 50.6689895 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
dd.plot(sample_field='Subject',gui='jupyter',bary_fields=['_calour_diff_abundance_group'])
top_supporters.head(5).amount.plot.bar()
tweet_counts_by_hour = tweet_archive_master[['timestamp', 'favorite_count', 'retweet_count']] $ tweet_counts_by_hour.timestamp = tweet_counts_by_hour.timestamp.map(lambda x: x.hour)
r.json()
data_vi = data_v.set_index(pd.DatetimeIndex(data_v['VIOLATION DATE']))
for name, val in zip(x.columns, adult_model.feature_importances_): $     print(f'{name} importance = {100.0*val:5.2f}%')
joined.to_feather(f'{PATH}joined') $ joined_test.to_feather(f'{PATH}joined_test')
regression_model.score(X_test, y_test)
eg = train.school_state.value_counts()
(autos["ad_created"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_values() $         )
pos['iss_position']['latitude'] $
%%gcs read --object gs://inpt-forecasting/Inpatient Census extract\t-\tPHSEDW\t71118.csv --variable icd
pd.options.display.precision = 2 $ obama.describe()
finals['type'].value_counts()
propnames.value_counts().reset_index()
df = pd.read_excel('accounts-annotations.xlsx', encoding='cp1252')
os.listdir()
bus.head(5)
autos.head(2)
clustered_hashtags.filter(clustered_hashtags.prediction == 12).show(100)
df_new[['ca','uk','us']] = pd.get_dummies(df_new['country']) $ df_new.drop(['ca'], axis=1, inplace=True) $ df_new.head()
%%javascript $ $('.math>span').css("border-left-color","transparent")
def create_wbl(mcap_mat, p=c): $     b = mcap_mat.apply(top_binarizer, p=p, axis=1) $     marketcap_sel = mcap_mat.fillna(0) * b#.shift(-1) # shift +1 or -1 $     w = marketcap_sel.div(marketcap_sel.sum(axis=1), axis=0)    $     return w, b
X_train.shape, y_train.shape, X_test.shape, y_test.shape
obj.rank(method='first')
df_raw['rank_this_week'] = pd.to_numeric(df_raw.rank_this_week) $ df_raw['rank_last_week'] = pd.to_numeric(df_raw.rank_last_week, errors='coerce')
builder.select('endcustomerlinefixed_data').distinct().show(10,False)
predicted_2017 = text_clf.predict(test_2017_cleaned) $ print('Accuracy of the model is:',accuracy_score(Y_test,predicted_2017)) $ print('Precision of the model is:',precision_score(Y_test,predicted_2017)) $ print('Recall of the model is:',recall_score(Y_test,predicted_2017)) $ print('confusion Matrix is:',confusion_matrix(Y_test,predicted_2017))
movies.head()
df = pd.read_csv('data/311_sf.csv') $ df.info()
cand_date_df.shape
poverty_2011_2015.to_csv('data/crime/poverty_2011_2015.csv')
raw_optimal_single_rebalance_etf_weights = get_optimal_weights(covariance_returns.values, index_weights.iloc[-1]) $ optimal_single_rebalance_etf_weights = pd.DataFrame( $     np.tile(raw_optimal_single_rebalance_etf_weights, (len(returns.index), 1)), $     returns.index, $     returns.columns)
df_main = pd.merge(df_clean, api_clean, on='tweet_id', how='inner')
df_goog.index = df_goog['Date'] $ df_goog.index = df_goog.index.to_datetime() $ df_goog.head()
StockData.head(20)
tmp1 = (x > 0.5) $ tmp2 = (y < 0.5) $ mask = tmp1 & tmp2
new_df
df[df2.columns[df2.columns.str.upper().str.contains('ORIGIN')]].sample(5)
from h2o.estimators.gbm import H2OGradientBoostingEstimator $ from h2o.estimators.random_forest import H2ORandomForestEstimator $ help(H2ORandomForestEstimator) $ help(h2o.import_file)
df_q = pd.read_sql(query, conn, index_col='Matrix_UID') $ df_q.head(5)
grouped_publications_by_author[grouped_publications_by_author['authorName'] == 'Lunulls A. Lima Silva']#['link_weight'].loc[97528]
df.to_csv('ab_data2.csv', index=False)
mlb = MultiLabelBinarizer() $ df = df.join(pd.DataFrame(mlb.fit_transform(df.pop('labels')), $                           columns=mlb.classes_, $                           index=df.index)) $ df.head(2)
auth = tweepy.OAuthHandler(consumer_key=consumerKey, consumer_secret=consumerSecret) $ api = tweepy.API(auth)
classifier_data = results_dict['classifier_evaluation_results'][0] $ print("Data is saved as a {} object.\nAUC for this session: {}".format(type(classifier_data), classifier_data.auc))
Z = np.ones((10,10)) $ Z[1:-1,1:-1] = 0 $ print(Z)
df.loc[df.userLocation == 'Manchester, PA', 'tweetText'] $
feature_cols = [tf.feature_column.numeric_column(col) for col in X.columns]
pd.to_datetime(['14-01-2012', '01-14-2012'], dayfirst=True)
output
to_be_predicted_Day4 = 48.64924294 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
p = getpass.getpass() $ try: $     conn = psycopg2.connect("dbname='postgres' user='rsouza' host='localhost' password='{}'".format(p)) $ except: $     print("I am unable to connect to the database")
df_small.info()
train_df, y, nas = proc_df(train_data, 'totals.transactionRevenue')
reviews.groupby('taster_twitter_handle').taster_twitter_handle.count()
data_full.describe()
df.groupby('Team') #object is returned 
n_old=df2[df2['landing_page']=='old_page']['user_id'].count() $ n_old $
parsed_email_data['institution'].value_counts()
auto.head()
b_cal_q1.head()
new_page_converted=np.random.choice([1,0],size=nnew,p=[pnew,1-pnew])
age = df_titanic['age'] $ print(age.describe())
movies.sample(5)
to_be_predicted_Day1 = 31.30 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
a = df2['group']=='treatment' $ actual_p_new =df2[a].converted.mean() $ print('Probability of individual converted if he is in treatment group:{}'.format(actual_p_new))
autos["model_brand"].value_counts(normalize = True).head(20)
import statsmodels.api as sm $ convert_old = df2.query('converted == 1 & landing_page == "new_page"')['user_id'].nunique() $ convert_new = df2.query('converted == 1 & landing_page == "old_page"')['user_id'].nunique()
authors_grouped_by_id_saved.count()
user1.plot.scatter(x='TowerLon', y='TowerLat', c='gray', alpha=0.1, title='Call Locations') $ plt.show()
goalkeeping_df
SMOOTH.plot_catchup_amplitudes(smooth)+expand_limits(y = 0)
temp_company_numbers = secret_corporate_pscs.company_number $ most_common_addresses_for_secret_pscs = active_companies[active_companies.CompanyNumber.isin(temp_company_numbers)].groupby(['first_and_postcode'])['CompanyNumber']\ $         .agg(lambda x: len(x.unique())).sort_values(ascending=False) $ most_common_addresses_for_secret_pscs.head(20)
df_clean3.loc[315, 'text']
education_data=education.iloc[education_data_rows, :]
api.ok(source)
df.Type.value_counts().plot(kind='pie', autopct='%1d%%', colors = ['g', 'c'])
df['week'] = df['datetime'].apply(lambda x:x.week) $ df['year'] = df['datetime'].apply(lambda x:x.year) $ df['date'] = df['datetime'].dt.date $
df['statement_type'].value_counts()
rating_corr_fav= df.rating_numerator.corr(df.favorite_count) $ rating_corr_rtwt= df.rating_numerator.corr(df.retweet_count) $ print ('Correlation between rating_numerator and favorite_count when grouped by month:{}'.format(rating_corr_fav)) $ print ('Correlation between rating_numerator and retweet_count when grouped by month:{}'.format(rating_corr_rtwt))
converted = len(df2[df2['converted'] == 1])/ len(df2['user_id']) $ print ("probability of the individual converted regardless of the page: ", converted)
x_train, x_test, y_train, y_test = train_test_split(bow_df, amazon_review['Sentiment'], shuffle= True, test_size=0.2)
candidates['party_type'] = candidates.party_name.apply( $     lambda x: x if x in ['DEMOCRATIC', 'REPUBLICAN'] else 'THIRD' $ )
col_eliminar = ['profile_banner_url','profile_image_url','protected'] $ usersDf=usersDf.drop(columns=col_eliminar) $
dfdaycounts = read_gbq(query=query, project_id='opendataproject-180502', dialect='standard')
BLINK.plot_count(blink,option="facet_subjects")
import numpy as np $ np.empty((3,2))
model = SGDClassifier(loss='log', random_state=0, n_iter=100) $ model.fit(train_vectors, train_targets)
load2017['timestamp'] = load2017.apply(lambda r : pd.datetime.combine(r['date'],r['hour']),1) $ load2017['timestamp'].head()
most_informative_features_top_and_bottom(vectorizer=vectorizer, classifier=lr, binary=False, n=15)
gender_vars.columns = gender_vars.columns.str.lower()
week26 = week25.rename(columns={182:'182'}) $ stocks = stocks.rename(columns={'Week 25':'Week 26','175':'182'}) $ week26 = pd.merge(stocks,week26,on=['182','Tickers']) $ week26.drop_duplicates(subset='Link',inplace=True)
lq.columns = lq.columns.str.replace('(','') $ lq.columns = lq.columns.str.replace(')','') $ lq.columns.values
extract_deduped.loc[(e)]
df2[df2['group'] == 'control'].mean()['converted']
cX_test['prob'] = M_NB_model.predict_proba(X_test_term)[:,1]
explotions['Deaths'] = explotions['Deaths'].map(lambda x:get_num(x))
mask=df.isnull() $ df=df.drop(range(390,398),axis=0) $ df=df.drop('Md',axis=1) $
to_be_predicted_Day4 = 48.60616693 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
ratings = ratings.unstack() $ ratings = pd.concat([ratings.numerator.mean(axis=1).rename('rating_numerator'), $                      ratings.denominator.mean(axis=1).rename('rating_denominator')], $                     axis = 1)
details.head()
a_set = set(a_list) $ a_set
url_with_author_data = pd.merge(url_data, author_data, left_on='author', right_on="name") $ url_with_author_data.head()
n_old = len(df2.query("group == 'control'")) $ (n_old)
import numpy as np $ rng = np.random.RandomState(42) $ x = rng.rand(1E6) $ y = rng.rand(1E6) $ %timeit x + y
df_master = pd.read_csv('twitter_archive_master.csv')
stations.head()
plt.matshow(df_raw.corr())
intervention_train.index.duplicated()
uber_14["hour_of_day"] = uber_14["Date/Time"].apply(lambda x: getHour(x)) $ uber_14.head()
dft[pd.datetime(2013, 1, 1, 10, 12, 0):pd.datetime(2013, 2, 28, 10, 12, 0)]
autos["kilometer"].head(3)
sensors_list = [entity[0] for entity in entity_id_list if entity[0].split('.')[0] == 'sensor'] # Print only the sensors $ sensors_list
pd.date_range('2005', periods=7*12, freq='M')
posts.columns
df.sample(20)
table1.head(3)
kick_projects = pd.merge(kick_projects, ks_ppb, on = ['category', 'launched_year','goal_cat_perc'], how = 'left')
import matplotlib.pylab as plt $ import numpy as np
graffiti2['created_year'] = graffiti2['created_year'].replace(2015, 1) $ graffiti2['created_year'] = graffiti2['created_year'].replace(2014, 1) $ graffiti2['created_year'] = graffiti2['created_year'].replace(2013, 1) $ graffiti2['graffiti_count'] = graffiti2['created_year']
dtObj = binary_sensors_df['last_changed'].iloc[0] $ device_id = 'device_tracker.robins_iphone' $ print("At {} {} is {}".format(dtObj, device_id, get_device_state(parsedDF, device_id, dtObj)))
from selenium import webdriver
unseen_predictions = rsskb_gbm_best.predict(X_unseen[possible_features]) $ print 'Accuracy: ', accuracy_score(y_unseen, unseen_predictions) $ print classification_report(y_unseen, unseen_predictions)
array_train = pokemon_train.values $ array_test = pokemon_test.values $ Y_train = array_train[:,1] $ Y_test = array_test[:,1]
microsoft = docx.Document('C:\\Users\\Andrew\\content-analysis-2018\\1-Intro\\2017_Annual_Report.docx')
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[n_new, n_old], alternative='larger') $ print('Z-score: ', z_score) $ print('P-value: ', p_value)
df2[['CA', 'UK', 'US']] = pd.get_dummies(df2['country']) $ df2.head(10)
S_lumpedTopmodel = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure09/summa_fileManager_lumpedTopmodel.txt')
del nba_df["Referee4"]
df.text.str.extractall(r'(MAKE AMERICA GREAT AGAIN)|(MAGA)').index.size
df['game_date'] = pd.to_datetime(df['game_date'])
station = pd.DataFrame(hawaii_measurement_df.groupby('Station').count()).rename(columns={'Date':'Count'}) $ station_count = station[['Count']] $ station_count 
regr = RidgeCV(cv=5, alphas=[0.0001, 0.001, 0.01, 0.1, 1]) $ regr.fit(experiment_X, experiment_y)
output= "Delete from user where user_id='@Pratik'" $ cursor.execute(output) $
monitor(ranked, strong_list, weak_list)
dat_dow = data.groupby(['admission_type', $                         'inday_icu_seq'])['hospital_expire_flag'].mean().reset_index() $ dat_dow = dat_dow.pivot(index='inday_icu_seq', $                         columns='admission_type', values='hospital_expire_flag') $ dat_dow
no_psc_and_psc = active_psc_records[(active_psc_records.company_number.isin(active_psc_statements[active_psc_statements.statement == 'no-individual-or-entity-with-signficant-control']\ $                                             .company_number))]['company_number'] $ print("Some examples:") $ active_companies[active_companies.CompanyNumber.isin(no_psc_and_psc)].to_csv('data/for_further_investigation/no_psc_and_psc.csv') $ active_companies[active_companies.CompanyNumber.isin(no_psc_and_psc)][['CompanyNumber','CompanyName','URI']].head(5)
df.info()
old_page_converted = np.random.choice([0,1], size = n_old, p = [1-p_old, p_old])
autos[["ad_created", $        "date_crawled", $       "last_seen"]].head()
p_treatment_converted = df2[df2['group'] == 'treatment']['converted'].mean() $ print('The probability of an individual in the treatment group converting: ', p_treatment_converted)
last_date_of_ppt = session.query(Measurement.date).group_by(Measurement.date==year_ago_ppt).order_by(Measurement.date.desc()).first() $ print(last_date_of_ppt) $
part = data[['created', 'freq']].copy() $ part = part.set_index('created').resample('60Min').sum().plot(kind="bar", figsize=[25, 10]) $
bands.columns = ['BAND_'+str(col) for col in bands.columns]
data = pd.read_csv(datafile,low_memory=False) $ data
datetime.datetime.fromtimestamp(0)
! ls data
for post in posts.find(): $      pprint.pprint(post)
from sklearn.cross_validation import train_test_split $ x_train, x_test, y_train, y_test = train_test_split(x_final,y_final,test_size = 0.2,random_state = 0) # Do 80/20 split
for slice in all_slices: $     if not relevant(slice): $         continue $     print(slice['name'], slice['foo'])
df_user[ $     df_user['collection'] == 'us_media_accounts_2016' $ ]['user.name'].tolist()[:5]
sns.regplot(filtered_df['number_of_reviews'],filtered_df['availability_30'],ci=95)
tweets = pd.DataFrame(res_dict['statuses']) $ print(tweets.shape) $ tweets.head()
count_nan(new_df_left) $
sales = graphlab.SFrame('Data/kc_house_data.gl/') $ sales.head()
type(ts.index[0])
autos["date_created"].str[:10].\ $ value_counts(normalize=True,dropna=False).\ $ sort_index(ascending=True).describe()
cat Data/microbiome_missing.csv
df2.dtypes
!wc -l $ml_data_dir/ratings.csv
flights2 = flights.set_index(["year", "month"])["passengers"] $ flights2.head()
joined['dcoilwtico'].interpolate(inplace=True, limit_direction='both')
first_values = grouped_months.first() $ first_values=first_values.rename(columns = {'Totals':'First_v_T'}) $
import pandas as pd $ import gensim $ import pickle $ import os $ import sqlite3
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old],alternative='larger') $ z_score,p_value
s1.values
twitter.info()
td_by_date = niners.groupby('Date')['Touchdown'].sum() $ td_by_date;
with open('D:/CAPSTONE_NEW/indeed_com-jobs_us_deduped_n_merged_20170817_113529266152000.json', encoding="utf-8-sig") as data_file: $     json_data4 = j.load(data_file)
pd.value_counts(ac['Bank'].values, sort=True, ascending=False) $
sub1.shape
against.amount.sum()
data.head(2)
print("Probability an individual recieved new page:", df2['landing_page'].value_counts()[0]/len(df2))
data.head()
with open('data/tweets.jsonl', 'r') as fh: $     df = pd.read_json(fh.read(), lines=True, convert_dates=True)
first_result.find('strong')
twitter_archive_master['rating_denominator'][1313] = 10 $ twitter_archive_master['rating_numerator'][1313] = 11
autos["last_seen"].str[:10].value_counts(normalize=True,dropna=False).sort_index(ascending=True)
plt.figure(figsize=(16,5)) $ plt.plot(daily)
df= pd.merge(df, df_weather_origin, on=['ORIGIN','YEAR','MONTH','DAY_OF_MONTH','DEP_HOUR'], how='left') $ df = pd.merge(df, df_weather_dest, on=['DEST','YEAR','MONTH','DAY_OF_MONTH','ARR_HOUR'], how='left')
data.iloc[1:10:3]
pd.DataFrame({'one' : [1., 2., 3.], 'two' : [3., 2., 1.]})
ctr_convert=df2.query('group=="control"').converted.mean() #Proportion of control group converted
converted = df.query('group == "treatment" and converted == 1')['user_id'].count() $ total = df.query('group == "treatment"')['user_id'].count() $ new_conv_rate = converted / total $ new_conv_rate
px.describe()
S_lumpedTopmodel.decision_obj.simulStart.value, S_lumpedTopmodel.decision_obj.simulFinsh.value
session.query(Measurement.id, func.max(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
df2[df2.duplicated(subset='user_id', keep=False) == True]
df_columns.set_index('created_at', inplace = True) $
arr2d = np.arange(30, dtype=np.float32).reshape(10, 3) $ arr2d
CRold = df2.converted.sum()/df2.count()[0] $ CRold
diffs = np.array(p_diffs)
dfs = sqlContext.createDataFrame(scenarios_rdd)
import pandas as pd $ located_data = pd.read_csv('located_data.csv') $ del located_data['replyToSN'], located_data['replyToSID'], located_data['replyToUID'], located_data['statusSource'] $ located_data.shape
test = df_main.index == 24 $ df_main[test]
rhum_us_full = rhum_nc.variables['rhum'][:, lat_li:lat_ui, lon_li:lon_ui]
twitter_data.describe()
train_cols = data.columns[1:] $ print(train_cols.values) $
from scipy.stats import norm $ print(norm.ppf(1-(0.05)))
save_model('model_random_forest_v1.mod', random_forests_grid)    
%matplotlib inline $ AAPL[['close','MA20']].plot()
BNB = SklearnClassifier(BernoulliNB()) $ BNB.train(train_set)
lasso.score(X_train,Y_train)
is_08A = (restaurants["VIOLATION CODE"] == "08A") $ inspections08A = restaurants[is_08A] $ inspections08A["DBA"].value_counts()[:10]
X_val
zone_props = [["upw.ss",0], ["rch.rech",0],["rch.rech",1]] $ k_zone_dict = {k:zn_array for k in range(m.nlay)} $
df.loc[['2018-05-21','2018-05-25'],['Open','Volume']]
for s in sentences: $     scores = analyzer.polarity_scores(s) $     print("\"{0}\" ==> {1}".format(s, scores['compound']))
autos["price_dollars"].value_counts().sort_index(ascending=False)
xmlData['street'] = [street[:-1] for street in xmlData['street']]
autos["price"].value_counts().sort_index(ascending=True).head(20)
yhat_tree = tree.predict(X_test)
USvideos['trending_date'] = pd.to_datetime(USvideos['trending_date'],infer_datetime_format = True, format = '%y.%d.%m') $ USvideos.info() $ USvideos['time_to_trend'] = (USvideos['trending_date'] - USvideos['publish_time']).astype('timedelta64[h]')
df.sort_index(axis = 1, ascending = True)
newdf.groupby([newdf.Hour_of_day]).Trip_distance.mean()
knn = KNeighborsClassifier( n_neighbors=3,  $                            weights='uniform') $ scores = cross_val_score(knn,  X_train, y_train,  cv=5) $ model = knn.fit(X_train, y_train) $ np.mean(scores), np.std(scores)   # scoring on my Training Data set was really worst
n_net3 = MLPRegressor(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5000,2500,1000,500,250,100,50), $                       random_state=1, verbose = True) $ n_net3.fit(x_train,y_train)
sp500.iloc[[i1, i2]]
AAPL.iloc[:,0:4].plot()
loans_df.isnull().sum()
tcga_target_gtex_expression_hugo_tpm.head()
with open('new_reddit_topics.pkl', 'rb') as pikkle2: $     sub_df = pickle.load(pikkle2)
missing_tweets.head(3)
math.exp(max(df['genes'])/1000)
tagged_df[tagged_df['is_rebalance']]['date'].unique()
sns.set_style('whitegrid') $ sns.distplot(data_final['countCollaborators'], kde=False,color="red", bins=25) $
y_test_under[fm_bet_under].mean()
events.head()
reviews.price.astype(str)
df.to_csv('trump_state_of_union_2018.csv.gz', index=False, compression='gzip')
n_old = df_control.user_id.nunique() $ n_old
lsi = models.LsiModel(corpus_tfidf, id2word=bag, num_topics=10) $ corpus_lsi = lsi[corpus_tfidf]
(data_2017_12_14[data_2017_12_14['text'].str.contains("felicidades", case = False)])["text"].head()
new_page_converted = np.random.binomial(145310, df2['converted'].mean())
fdonor = donations.groupby('Donor ID')['Donation Amount'].agg(['count', 'sum']).sort_values('count', ascending=False).head(10) $ fdonor
pd.unique(tweetsDF.location)
commits = Query(git_index) $ commits.get_cardinality("hash") $ total_commits = commits.get_aggs() $ print("total commits: ", total_commits)
dictionary = corpora.Dictionary(lemmatized_texts) $ terms_matrix = [dictionary.doc2bow(doc) for doc in lemmatized_texts]
plt.scatter(tbl['mkt_ret'],tbl['port_ret']) $ plt.plot(tbl['mkt_ret'], result2.fittedvalues,'g')
big_df_count.head()
merged2['Specialty'].isnull().sum(), merged2['Specialty'].notnull().sum()
df1['forcast'].head() #allthe data will be nan $
glm_multi_v3.hit_ratio_table(valid=True)
VIC = pd.DataFrame(cvModel2.bestModel.featureImportances.toArray(), columns=["values"]) $ features_col = pd.Series(["AgencyVec", "CompTypeVec", "BoroughVec", "HOD"])  $ VIC["features"] = features_col $ VIC
df_TempIrregular.head(100)
LetsGetMeta.__class__
autos["registration_year"].describe()
the_sites = [] $ the_sites.append(a) $ the_sites
FTE = pd.DataFrame(requirements, index = ['FTE']) $ FTE
plt.hist(MaxPercentage, bins=50,alpha=0.5) $ plt.hist(RandomPercentage, bins=50,alpha=0.5) $ plt.ylabel("Frequency") $ plt.xlabel("percentual change used for profit") $ plt.show()
prob_group3 = df2.query("landing_page == 'new_page'")["user_id"].count() $ prob = prob_group3 / df2.shape[0] $ print("The probability that an individual received the new page is {}.".format(prob))
df = df.loc[df['treatments'].notnull()]
bus["postal_code"].value_counts(dropna=False).sort_values(ascending = False).head(15)
likes.groupby(by='content').size()
print('Unique number of users notified: {}'.format(len(enroute_4x['vendorId'].unique())))
stmt1= session.query(employee).filter(employee.c.id=='Male').first() $ stmt2= session.query(employee).filter(employee.c.hours_per_week==45).first()
autos["brand"].unique().shape
rodelar.editCount()
mask_higher.describe()
Jarvis_rootDistExp_1 = Jarvis_rootDistExp[0] $ Jarvis_rootDistExp_0_5 = Jarvis_rootDistExp[1] $ Jarvis_rootDistExp_0_25 = Jarvis_rootDistExp[2]
details.head(10)
def date(x): $     return x.split("T")[0] $ def time(x): $     return x.split('T')[1][:-1]
counter=1 $ for index,row in df[ df["systemmessage"] ].iterrows(): $     counter = counter + row["nrmembers"] $     df.loc(index,"nrmembers") = counter $ df
df['body'] = df['body'].apply(lambda x: ''.join([i for i in x if not i.isdigit()])) $ df['body_tokens'] = df['body'].str.lower()
print('There are {:.0f} visa applications in this dataset.'.format(df_data.shape[0]))
html = browser.html $ weather_soup = BeautifulSoup(html, 'html.parser')
%run -i 'image_retraining/retrain.py' --image_dir 'train'
df_ab = pd.read_csv('ab_data.csv') $ df_ab.head()
weather_df.info()
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0]
dataAnio.to_csv("data/dataPorUbicacion_Anios_tmax.csv")
result_final.summary2()
a.lower()
tweet_json.info()
labmt.head()
cp311.head(2)
autos["ad_created_year"] = autos["ad_created"].dt.year $ autos["ad_created_month"] = autos["ad_created"].dt.month $
import pymongo $ from pymongo import MongoClient $ client = MongoClient() $ client = MongoClient('localhost', 27017)
plt.subplots(figsize=(6, 4)) $ sn.barplot(train_session_v2['isNDF'],train_session_v2['index'])
vol.resample('D', how='sum')
df_city_reviews.select('user_id','date','seq').orderBy('user_id','seq').limit(40).toPandas()
df_nodates = df_EMR_with_dummies.select_dtypes(exclude=['<M8[ns]']) $ df_EMR_dd_dummies = pd.concat([df_dd, df_nodates], axis=1) $ df_EMR_dd_dummies.shape $ df_EMR_dd_dummies.head()
df_test[0:5]
pd.Series([6,5,4,3,2,1], index=10 * np.arange(6)) # index par default : 0, 1 .. $
engine.execute('SELECT * FROM measurements LIMIT 15').fetchall() $
df_playlist_videos = pd.DataFrame() $ for playlist in tqdm(df_playlists['playlist_id'].tolist()): $     playlist = yt.get_video_urls_from_playlist_id(playlist, key, verbose=0) $     df_ = pd.DataFrame(playlist) $     df_playlist_videos = df_playlist_videos.append(df_, ignore_index=False)
spencer_bday_time.strftime("Spencer was born on %A, %B %dth at %I:%M %p")
!pip install boto3
perceptron = Perceptron() $ perceptron.fit(X_train, Y_train) $ Y_pred = perceptron.predict(X_test) $ acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2) $ acc_perceptron
df_new[['new_page', 'old_page']] = pd.get_dummies(df_new.landing_page)
df_c2['intercept']= 1 $ lom2 = sm.Logit(df_c2['converted'], df_c2[['intercept','UK', 'US', 'ab_page']]) $ results2 = lom2.fit() $ results2.summary()
X = [1,2,3,4,5,6] $ I = [1,3,9,3,4,1] $ F = np.bincount(I,X) $ print(F)
inspector = inspect(engine) $ inspector.get_table_names() $
unique_urls.query('num_authors >= 10').sort_values('avg_votes', ascending=False)[0:50][['url', 'avg_votes']]
df.rename(columns={'Indicator':'Indicator_id'},inplace=True) $ df.head(2)
df2['intercept'] = 1 $ df2[['control','ab_page']]= pd.get_dummies(df2['group']) $ df2 = df2.drop('control',axis = 1)
df.plot()
feature_columns
list_of_exclusion_words = list(tweet_archive_clean[tweet_archive_clean.name.map(lambda x: not x[0].isupper())].name.value_counts().index) $ list_of_exclusion_words.extend(['None'])
import numpy $ %timeit numpy.random.normal(size=100)
archive_copy['name'].loc[archive_copy['name'].str.islower()].count()
import pandas as pd $ import datetime $ from time import time $ import math
mylist $ len(mylist)
vecs.head()
def checkGroup(group,number): $     if group == number: $         return True $     else: $         return False
rf_grid.fit(X_train,y_train)
model.wv.most_similar('cost_function', topn=10)
grp_tweet = tweets.groupby("lan")
model=Sequential() $ model.add(Dense(12,input_dim=8,activation='relu')) $ model.add(Dense(12,activation='relu')) $ model.add(Dense(1,activation='sigmoid'))
final.head()
data['team'] $ data.team
df = pd.merge(df, df_cen, on=['CaseID'], how='left') $ df.sample(5)
grouped1 = df.groupby(['product_type', 'state'])['price_doc'].mean() $
import collections $ parnate = df_fda_drugs_reported[df_fda_drugs_reported['simple_name'].str.lower() == 'tranylcypromine'] $ a = [y for x in parnate['adverse_effects'].str.split("; ") for y in x] $ counter=collections.Counter(a) $ [(eff, p/len(counter)) for (eff, p) in counter.most_common(20)]
start_val=1000000 $ pos_vals=alloced*start_val $ pos_vals.head()
data_to_clean_title[0]
data.loc[data.expenses > 150000, 'expenses'] = np.NaN
bool_regyear = autos["registration_year"].between(1900, 2016) $ autos = autos[bool_regyear]
df.reorder_levels(order=['Date', 'Store', 'Category', 'Subcategory', 'UPC EAN', 'Description']).head(3)
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
cfs=cfs.filter_abundance(10)
ps['2011']
url = "https://www.analyticsvidhya.com/" $ r = requests.get(url) $ r
results2.summary()
df_user = pd.read_csv(user_source) $ df_user.head()
pd.ols(y=mr, x=lagged)
print((fit.shape, fit1_test.shape)) $ print((fit2.shape, fit2_test.shape)) $ print((fit3.shape, fit3_test.shape))
wash_park_matrix.todense()
plt.scatter(X,y2) $ plt.plot(X, np.dot(X_15, linear.coef_) + linear.intercept_, c='y') $ plt.xlabel('Hour of Day') $ plt.ylabel('Count')
rng.asobject
relevant_data['Invitee Name'].value_counts()
sp = s.str.split('_') $ sp
for key, value in r.json()['dataset'].items(): $     print(key, ":\n", value, "\n")
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new.head()
companyNeg.columns
fig, ax = plt.subplots(figsize=(12,18)) $ lgb.plot_importance(model, max_num_features=15, height=0.8, ax=ax) $ ax.grid(False) $ plt.title("LightGBM - Feature Importance", fontsize=15) $ plt.show()
df['processed2'] = processed_sent $ df.head()
df2.drop_duplicates(['user_id'], inplace=True)
df2.describe()
df.drop(6, inplace=True)
session.query(Measurement.station).distinct().count()
dataset.describe()
udacity_image_prediction_url = "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv" $ with open(udacity_image_prediction_url.split("/")[-1],"wb") as file: $     file.write(requests.get(udacity_image_prediction_url).content)
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=4000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
import pandas as pd $ date = pd.to_datetime("4th of July, 2015") $ date
git_log['sha'].count()
api.authorise() $ api.get_data(a=1)
import numpy as np
weather.info()
n = df.shape[0] $ n
train_set.head(10)
df_2003.dropna(inplace=True) $ df_2003
dul_final = dul1.rename(columns={'ISBN RegEx':'ISBN'}) $ dul_final
a400hz = a400hz.distinct()
df_data=pd.DataFrame({'time':(times+utcoffset).value[:-sa],'chips':final.chips.values}) $
Amazon.loc[Amazon['Close'] > Amazon['Open'], ['Close', 'Open']].index.day.value_counts()
condos = pd.merge(df, aru_df, on='SSL')
df_change_count['Count'].mean()
poverty_data.columns=poverty_data_columns
from gensim.models import Word2Vec
cols_to_export = ["epoch","src","trg","src_str","src_screen_str","trg_str","trg_screen_str","lang","text"] $ mentions_df.to_csv("/mnt/idms/fberes/network/ausopen18/data/ao18_mentions_with_names_and_text.csv",columns=cols_to_export,sep="|",index=False)
import twitter $ api = twitter.Api(consumer_key='XXXXXXXXXXXXXXXXXXXXX', $   consumer_secret='XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX', $   access_token_key='XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX', $   access_token_secret='XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')
X = newdf[['SMB','HML','Mkt-RF','Mom   ']] $ X1 = sm.add_constant(X) $ y = newdf['TSRet'] $ Wregression = sm.OLS(y,X1).fit() $ Wregression.summary()
analyze_set.sample(5)
savetoPostgres(bigdata,"bigdata")
len(df.columns)
df_combined = pd.merge(df, df_btc, on='created_at', how='inner')
facts_metrics.groupby(['dimensions_date_id', 'dimensions_item_id', 'id']).sum()
date_info['visit_date'] = pd.to_datetime(date_info['visit_date']) $ date_info['day_of_week'] = lbl.fit_transform(date_info['day_of_week']) $ date_info['visit_date'] = date_info['visit_date'].dt.date
print cust_data.head(5) $ print cust_demo.head(5) $ print cust_new.head(5) $
from pandas import Series $ df.instagrams = 50 $ ins = Series([10, 20, 30], index=[1, 3, 5]) $ ins
%matplotlib inline $ import seaborn; seaborn.set()
data = pd.read_csv('patient_data_test.csv')
df.loc[df.userTimezone == 'Mountain Time (US & Canada)', :]
help(es.search)
df_movies.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/movies.csv', sep=',', encoding='utf-8', header=True)
c = sqlc.cursor()
df_merge = df_clean.merge(df_twitter_clean, how='inner', left_on='tweet_id', right_on='id') $ df_merge.drop('id', axis=1, inplace=True)
result3 = sm.ols(formula="AMZNr ~ GSPCr", data=tbl2[-250:]).fit() $ result3.summary()
levelmean = pd.merge(pd.DataFrame(tokendata.level),groupmean,how="left",on="level")
test_dum_clean = test_dum[['ID'] + [col for col in train_dum.columns if 'Source_' in col or $                       'Customer_Existing_' in col]]
sns.regplot(x = np.arange(-len(my_tweet_df[my_tweet_df["tweet_source"] == "The New York Times"]), 0, 1),y=my_tweet_df[my_tweet_df["tweet_source"] == "fantastic ms."]["tweet_vader_score"],fit_reg=False,marker = "*",scatter_kws={"color":"orange","alpha":0.8,"s":100}) $ ax = plt.gca() $ ax.set_title("Sentiment Analysis (The New York Time)",fontsize = 12) $ plt.savefig('Sentiment_The_Ny_Times.png')
del_list = [] $ del_list = df_arch_clean[df_arch_clean['retweeted_status_timestamp'].notnull()]['tweet_id'] $ del_index = del_list.index $ df_arch_clean = df_arch_clean.drop(del_index) $
pd.concat([s1, s2, s3], axis=0, keys=['one', 'two', 'thr'])
tdf = sns.load_dataset('tips') $ tdf['size'].sample(5)
df = pd.read_csv('http://web.mta.info/developers/data/nyct/turnstile/turnstile_160917.txt') $ df.columns = df.columns.str.strip()
test.date.value_counts().shape
results = log_mod.fit() $ results.summary()
results = [] $ for pr_item in pr_list: $     results.append(download_and_parse_pr(pr_item))
df_geo.columns = ['CaseID', 'Address', 'Tract', 'Block'] $ df_geo['Tract'] = pd.to_numeric(df_geo.Tract)/100. $ df_geo[['CaseID', 'Block']] = df_geo[['CaseID', 'Block']].apply(pd.to_numeric)
datatest.loc[datatest.place_name == "Sourigues",'lat'] = -34.800140 $ datatest.loc[datatest.place_name == "Sourigues",'lon'] = -58.220011
print("The number of obtained records is ", len(data['data']))
import gensim, logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
df[df['Descriptor'] == 'Loud Music/Party'].groupby(by=df[df['Descriptor'] == 'Loud Music/Party'].index.dayofweek).count().plot(y="Borough")
twitter_archive_clean = twitter_archive.copy()
tweet.lang $ tweet.text $ tweet.retweet_count $ tweet.place $ tweet.geo
tweet_data.head()
final_names=[x.replace(primary_temp_column, 'Temp') for x in out_columns] $ final_names=[x.replace("Incremental", "_Precip") for x in final_names] $ save_dat.columns=final_names $ save_dat.index.name='Date'
display(Markdown(q8d_answer))
np.savetxt('similarity_score.dat',a.A.flatten()[:])
vc = data.loc[:,'C3 Complaint Type'].value_counts() $ vc.head(n=10)
%matplotlib notebook $ import pandas as pd $ import pandas_datareader.data as web $ from datetime import datetime
stanford_word_list['Word_stem'] = stemmed_dict_word_list
lm = smf.ols(formula='y ~ x', data=df).fit() $ lm.params
week32 = week31.rename(columns={224:'224'}) $ stocks = stocks.rename(columns={'Week 31':'Week 32','217':'224'}) $ week32 = pd.merge(stocks,week32,on=['224','Tickers']) $ week32.drop_duplicates(subset='Link',inplace=True)
f_close_clicks_app_train.show(3)
pattern = re.compile('AA') $ print(pattern.findall('AAbcAA')) $ print(pattern.findall('bcAA'))
test_number_con['booking_rate'] = ( $     test_number_con.booked_at/test_number_con.total_number_conversations) * 100
df_new.groupby(['ab_page'], as_index=False).mean()
autos["price"].value_counts().sort_index(ascending = False).head(20)
autos["odometer"].shape #autos[column name] will always give 1-d array which is a.k.a as Series in pandas, just like 1-d array in Numpy is known as vector $
data.sort_values('TMED', inplace=True, ascending=False) $ data.head()
funnel_by_group
from pyspark.sql import Row $ RowedRDD = RandomOneRDD.map(lambda x: Row(id=x[0], val1=x[1], val2=x[2])) $ print RowedRDD.collect()                                               
pd.Timestamp('2010-11-12')
sum(rfc_bet_under)
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','OriginationPhase']).closed_at.count() $ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'purple', 'red'], grid=False) $
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_autocorrelation(RN_PA_duration, params=params, lags=30, alpha=0.05, \ $     title='Weekly RN/PA Hours Autocorrelation')
cercanasAfuerteApacheEntre75Y100mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 75) & (cercanasAfuerteApache['surface_total_in_m2'] < 100)] $ cercanasAfuerteApacheEntre75Y100mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre75Y100mts.apply(descripcionDistancia2, axis = 1) $ cercanasAfuerteApacheEntre75Y100mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
fig = plt.figure(figsize=(10,10)) $ ax = fig.add_subplot(111) $ ax.axis('off') $ pumashp.plot(column='pcBB',ax=ax,legend=True)
weather = pd.concat([w10, w11, w12, w13, w14, w15, w16]) $ weather.tail() $
import pandas as pd $ test_data = pd.read_csv("data/person-video-sparse-multiple-choice.csv") $ test_data.head()
autos.loc[autos["registration_year"]>2017, "registration_year" ].shape
print('You are currently in ' + getcwd())
dep = ha.depot('demo depot', path='data', currency='EUR') $ dep.add_account(acc) $ dep.account_infos
disag_hart_elec = disag_hart.buildings[building_number].elec $ disag_hart_elec
filename = "../datasets/catalogs/fermi/gll_psc_v16.fit.gz" $ print([_.name for _ in fits.open(filename)]) $ extended_source_table = Table.read(filename, hdu='ExtendedSources')
df2['ab_page']=pd.get_dummies(df['group'])['treatment'] $ df2['intercept']=1 $ df2.head()
between_my_posts.describe()
import numpy as np $ from pandas import Series, DataFrame $ import pandas as pd $ import xport  ## write sas datasets to local drive $ from sas7bdat import SAS7BDAT  ## read SAS datasets from local drive
!nvidia-smi | head -n 15
local_sample_fractions
df2.head()
pd.DataFrame(usersDf.columns)
geo_irregularities.info()
model.doesnt_match("paris berlin london austria".split())
log_model = sm.Logit(y, X)
df[['state_cost','state_retail','sale']] = df[['state_cost','state_retail','sale']].apply(pd.to_numeric) $ df[['store_number','vendor_number','item_number',]]=df[['store_number','vendor_number','item_number']].astype(object) $ df['date']= pd.to_datetime(df['date'], format="%m/%d/%Y")
cv_fitted, cv_data, tfidf_fitted, tfidf_data = mf.vectorize_both_ways(epoch3_df, 'cleaned_text')
chromosomes_list = [str(i) for i in range(1, 23)] + ['X', 'Y', 'MT'] # list comprehensions $ gene_df = gene_df[gene_df['seqid'].isin(chromosomes_list)] $ chromosome_gene_count = gene_df.groupby('seqid').count().iloc[:, 0].sort_values().iloc[::-1] $ chromosome_gene_count
twitter_archive_clean.drop('expanded_urls', axis=1, inplace=True)
Y = data3.sales
df_B2
alltime_balk_king.head(15)
df_ad_airings_5.to_pickle('./TV_AD_AIRINGS_NO_NAN_LOC_DATASET_4.pkl')
rt_max = data['RTs'].max() $ rt_tweet = data[data['RTs'] == rt_max] $ print("The tweet with more retweets is: \n{}".format(np.array(rt_tweet['Tweets'])[0])) $ print("Number of retweets: {}".format(rt_max)) $ print("{} characters.\n".format(np.array(rt_tweet['len'])[0]))
scoring_input_data['DATE'] = pd.to_datetime(scoring_input_data['DATE']) $ scoring_input_data['ID'] = scoring_input_data['ID'].astype('category') $ scoring_input_data.dtypes
invoice = shoppingCart.map(priceCategory) $ invoice
pd.read_csv
tweet_json_clean = tweet_json_clean[['id', 'favorite_count', 'retweet_count']]
df_weather.loc[:, "events"] = df_weather.events.apply(lambda x: "Rain" if x == "rain" else x) $ df_weather.events.fillna(value="No-Event", inplace=True) $ print("\nThe types of weather events in the 'events' column are:") $ evnts = [str(x) for x in df_weather.events.unique()] $ print("".join([str(i+1) + ". " + evnts[i] + "\n" for i in range(len(evnts))]))
pdp = pd.DataFrame(data) $ pdp = pdp.tail(1000)
plt.hist(p_diffs,bins=25) $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('Plot of 10K simulated p_diffs');
sh50_df = pd.read_csv('../data/sh50.csv') $
labels = common.board.create_uci_labels() $ label2ind = dict(zip(labels,list(range(len(labels)))))
not_in_misk.head(3)
df.T.to_csv('../outputs/retrospective/CSFV2_outlook_weekly_90th_per_summary_table_from_{:%Y%m%d}.csv'.format(dates_range[0]))
df_CLEAN1A['AGE'].min()
rt_count = df_rt[['text', 'keyword']].groupby(['text', 'keyword']).size().reset_index() $ rt_count.columns = ['text', 'keyword', 'count'] $ rt_count.sort_values(by = ['count'], ascending = False, inplace = True)
stn_temp = session.query(Measurement.station, Measurement.date, Measurement.tobs).filter(Measurement.station == busiest_stn).filter(Measurement.date > data_oneyear).order_by(Measurement.date).all() $ stn_temp
free_data.dtypes
gs_lr_tfidf.fit(X_train, y_train) 
negative_amentities = pd.DataFrame(negative_amentities.groupby("Census Tract").mean().mean(axis=1)) $ negative_amentities.columns = ["Negative Amenities"] $ negative_amentities
r6s = pd.read_csv('r6s_post_from_2015.csv', parse_dates= ['created_utc']) $ r6s.shape
mask = (a + b).isnull() $ mask
df3 = pd.read_csv('countries.csv') $ df3.head()
tub=ask.merge(icecream.drop('Scoop',1),on='Type',how='left') $ tub
data
goodreads_users_df = pd.read_csv(goodreads_users_filename)
df['Complaint Type'][df['Agency'] == 'NYPD'].value_counts().head()
df_new = df.filter(['email','public_repos','followers','hireable','company','updated_at'], axis=1)
plt.plot(i[0:-1], d[2]) $ plt.show()
lims_query = "SELECT specimens.cell_depth, donors.weight, specimens.id, donors.id \ $               FROM donors INNER JOIN specimens ON donors.id = specimens.id" $ lims_df = get_lims_dataframe(lims_query) $ lims_df.tail()
data = convert_timestamp(data) $ data.head()
file3=file2.filter(file2.trans_start!=file2.meter_expire) $ file3.show(3)
events_popularity_summary
cd Dropbox/Python/jupyter-blog/content/Twitter_soccer
pred1 = pd.read_csv("prob1.txt") $ pred2 = pd.read_csv("prob2.txt").set_index('id') $ pred3 = pd.read_csv("prob3.txt").set_index('id')
tmp_df = tmp_df.drop('Case.Number.1', axis=1)
learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)
first_status['user'].keys()
model_rf = pipeline_rf.fit(train_data)
df_tweet_date_count=pd.DataFrame(data=date_tweet_count) $ tweets_per_day=convert_index_column(df_tweet_date_count) $ tweets_per_day_df=pd.DataFrame(tweets_per_day) $ tweets_per_day_df
len(churned_ix)
set(retweets_status_keys) - set(status_keys)
lgbm_train=train[['air_store_id','visit_date']] $ lgbm_train['visitors'] = lgbmrstcv.predict(train[col].values) $ lgbm_train.to_csv('stacking_input/lgbm_tscv_train_s2.csv',index=False)
len(data.url.unique())
mw['page_ptr_id'] = pd.Series(range(1,189))
data.plot(x='Date')
cpi_all = abs_to_df(response_cpi_all.json(),name_only=False) $ cpi_all.head(10)
weather_features.interpolate(method='linear',axis=0,inplace=True)
browser = webdriver.Chrome('chromedriver.exe') $ browser.get(url) $ time.sleep(3) $ html = browser.page_source $ soup = BeautifulSoup(html, "html.parser") $
ab_df = pd.read_csv('ab_data.csv') $ ab_df.head()
convo1.shape
writer.save()
re_split_raw = re.findall(r'\w+', raw) $ print(re_split_raw[100:150])
df_user.head()
duration_df = merged2[['Provider', 'Specialty', 'AppointmentCreated', 'AppointmentDate', 'AppointmentDuration', $        'ReasonForVisitName', 'DurationHours', 'ReasonForVisitDescription']]
top_songs['Position'].unique()
df_new['updated_at'] = df_new['updated_at'].apply(pd.to_datetime) $ df_new['updated_at'] $ df_new['updated_at'] = df_new['updated_at'].dt.date $ df_new['updated_at'] = df_new['updated_at'].apply(pd.to_datetime) $ df_new['updated_at']
from biopandas.mol2 import PandasMol2 $ pmol = PandasMol2().read_mol2('./data/1b5e_1.mol2') $ keto_coord = pmol.df[pmol.df['atom_type'] == 'O.2'][['x', 'y', 'z']] $ keto_coord
mb.Stool / mb.Tissue
s.asfreq('8BM')
%%bash $ cp -R models/research/object_detection/utils/. utils
from sklearn.model_selection import train_test_split as tts $ X_train, X_test, Y_train, Y_test = tts(X,Y, test_size=0.2, random_state = 56)
response = requests.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv') $ with open('image_preview.tsv', 'wb') as file: $     file.write(response.content)
df_master.groupby('dog_stages')['favorite_count','retweet_count'].sum().plot(kind='bar',figsize=(10,5)) $ plt.xlabel('Dog Stages') $ plt.ylabel('Count') $ plt.title('Retweet and Favorite by Dog Stages') $ plt.show()
df_members['gender'] = df_members['gender'].map(gender) # making category column to int
cpq_business['Building ID'] = cpq_business['Building ID'].apply(str) $ cpq_business['Building ID'].unique()
top20_mostfav = top20_mostfav.sort_values(by='favorite_count', ascending=False).copy() $ top20_mosttweeted = top20_mosttweeted.sort_values(by='tweet_count', ascending=False).copy()
pd.value_counts(appointments['Specialty'])
merged_data.drop('due', axis=1, inplace=True)
for x in range(len(twitter_df['Date'])): $     date = convert_time(twitter_df.loc[x,'Date']) $     twitter_df.loc[x, 'Date'] = date $ twitter_df.head()
df.loc[df.toes.str.match(pattern2)==True]
tweet_archive.head(40)
followup = pd.read_json('/Users/thomasmulhern/Downloads/pitchesDataJson/followup.json')
building_pa.columns = building_pa.columns.str.replace(' ', '_') $ building_pa.columns = building_pa.columns.str.replace('-', '_') $ building_pa.columns = building_pa.columns.str.lower() $ building_pa.head()
dominique = relevant_data[relevant_data['User Name'] == 'Dominique Luna'] $ df = dominique['Event Type Name'].value_counts() $ df_dominique = pd.Series.to_frame(df) $ df_dominique.columns = ['Count_Dominique'] $ df_dominique
stock_return = stocks.apply(lambda x: x / x[0]) $ stock_return.head()
! $command
data['Company'].groupby([data.index]).agg([ 'count']).resample('W').sum().plot();
df = genre_avg_rating.toPandas() $ df.head()
m = RandomForestClassifier(n_estimators=1, max_depth=4, bootstrap=False, n_jobs=-1, random_state=42, min_samples_leaf=10) $ m.fit(X_train, y_train) $ print_score(m)
ada_predict = adaboost.predict(testx)
height.isnull()
df.loc[['a','b','f','h'],['A','C']]
from sklearn.decomposition import LatentDirichletAllocation
df.head(20)
coef_uk = np.exp(0.0099) $ coef_ca = np.exp(-0.0408) $ print("UK: {0}, CA: {1}".format(coef_uk, coef_ca))
df.describe().T
click_condition_meta = click_condition_meta[click_condition_meta.dvce_type != 'Unknown'] # google bot
df_ad_airings_5['location'].unique()
train = train[train['Open'] != 0 ] $
MongoClient()
est = trading.regression()
import pandas as pd $ import numpy as np $ import datetime as dt $ import pandas_profiling $ from functools import reduce
autos["registration_year"].value_counts()
df[['date', 'emails'    ] ]
df = pd.read_csv('/Users/yaru/Downloads/dataset/last_2_years_restaurant_reviews.csv')
trend_de.head()
top_bike = df[df['id']==41000425]
mb.loc['Proteobacteria']
df_daily3 = df_daily.groupby(["C/A", "UNIT", "STATION", "DATE"]).DAILY_ENTRIES.sum().reset_index() $ df_daily3.head(5)
documents[:2]
newdf= newdf.join(mom, how='inner')
df = tables[0] $ df.columns = ['Mars_planet_profile', 'Value'] $ df
len(df2.query('converted == 1'))/len(df2)
weather.loc[:, 'PRCP':'SNWD']=weather.loc[:, 'PRCP':'SNWD'].fillna(value=0) $ weather.loc[:, 'TAVG': 'TMIN']=weather.loc[:, 'TAVG': 'TMIN'].fillna(method='ffill') $ weather.info()
x_test.shape
df.describe()
all_p = all_p.rename('all_p') $ tt_final = pd.concat([tt_final,all_p], axis = 1) $ tt_final.info()
df.loc[1:4,"Date"]
df_mean_time_diff = mean_time_diff.to_frame() $ df_mean_time_diff = df_mean_time_diff.reset_index(drop=False) $ df_mean_time_diff.rename(columns={0: 'Frequency of Donations'}, inplace=True) $ df_survival = df_survival.merge(df_mean_time_diff, how='inner', on= 'Donor ID')
type(month_year_crimes)
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=20000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
'Best Params: ', rs_gbm.best_params_, 'Best Score: ', rs_gbm.best_score_
prophet_model = Prophet(interval_width=0.95)  #default==0.8
import pandas as pd $ import matplotlib.pyplot as plt
from pyspark.sql import Window
extract_deduped.loc[(extract_deduped.APP_PROMO_CD=='SFL418PA') $                    &(extract_deduped.app_branch_state=='CA')].groupby('APPLICATION_DATE_short').size()
df_max
stations = session.query(Measurement).group_by(Measurement.station).count() $ stations
uber_14["month"] = uber_14["Date/Time"].apply(lambda x: int(x[0:1])) $ uber_14["day_of_month"] = uber_14["Date/Time"].apply(lambda x: int(x.split("/")[1])) $ uber_14["day_of_year"] = uber_14["Date/Time"].apply(lambda x: x.split(" ")[0]) $ uber_14.head()
start = datetime.now() $ print(start) $ recall, precision, f1_score = main(train_features,trainset,test_features,testset) $ end = datetime.now() $ print(end)
p_control = df2.query('group == "control"').converted.mean() $ p_control
by_day_by_hour14.to_json("../visualizations/by_day_by_hour14.json", orient="split")
ccl = pd.read_csv("CCL.csv", sep="\t") $ ccl.head()
testData.groupBy("label").count().show()
%sql \ $ SELECT avg(twitter.retweet_count) AS viral \ $ FROM twitter \ $ WHERE user_id = 42869268;
train_df.Q2_primary_target_of_opinion.value_counts()
col = col[col.bike_involved == 1]    # drop rows where no bike involved
dummy_clf = DummyClassifier() $ dummy_fit = dummy_clf.fit(X_train, y_train) $ dummy_predict = dummy_clf.predict(X_test)
data = fat.add_sma_columns(data, 'Close', [6,12,20,200])
le=LabelEncoder() $ X = df[['word_count','sentiment','subjectivity','domain','post_duration']] $ X_n=df[['word_count','sentiment','subjectivity','domain','title','post_duration']] $ y = le.fit_transform(df['subreddit'])
dataset.User.value_counts()
autos['ad_created'].str[:10].value_counts(normalize = True, dropna = False).sort_index()
Numerator=df2.loc[(df2['group']=='treatment') & (df2['converted']==1),].shape[0] $ Denominator=df2.loc[df2['group']=='treatment',].shape[0] $ TreatmentConverted=Numerator/Denominator $ print("The probability is", Numerator/Denominator)
dicttagger_service = DictionaryTagger(['service.yml'])
df.rename(columns={'Indicator': 'Indicator_id'}, inplace=True) # Now, it is permanent $ df.head(2)
raw_freeview_df, raw_fix_count_df = condition_df.get_condition_df(data=(etsamples_grid,etmsgs_grid,etevents_grid), condition='FREEVIEW')
0.118920 #. From the above table
z_score, p_value = sm.stats.proportions_ztest([convert_new,convert_old], [n_new,n_old],alternative='larger') $ (z_score,p_value)
df_transactions['amount_per_day'] = df_transactions['amount_per_day'].replace([np.inf, -np.inf], 0)
df.head(1)
df[['location_id', 'prev_location_id']].head()
mse_scores = -scores $ print(mse_scores)
word = 'convolution' $ close_words_vectors, word_labels = w2v.get_close_words_vectors(word) $ close_words_vectors
def get_rating(row): $     ratings = re.findall(r'(\d\d?\d?.?\d*\/\d*\d*\d*\d)', row.text) $     final_rate = str(ratings[-1]) $     return final_rate
yelp_dataframe.reset_index(inplace=True)
def walds_method(): $     return 1/(0.05**2) # +- 5% $ walds_method()
train_df.isnull().sum()
y = np.array(hours['Duration (ms)']) $ print(y)
plt.hist(threeoneone_census_complaints[threeoneone_census_complaints['perc_white']>0]['perc_white'],bins=100) $ plt.show()
movies.count()
libraries_df = libraries_df.merge(libraries_metadata_df, on="asset_id")
first_result
flight.show(100)
prices.head()
%load "solutions/sol_2_3.py"
count_id = df.groupby('subreddit').agg({'id': 'count'}) $ top_sub = count_id.sort_values('id', ascending = False).head() $ top_sub
pd.value_counts(merged1[merged1['AppointmentDuration'] > 90]['ReasonForVisitName'])
api_key = 'YOUR_API_KEY' $ series_id = 'NG.NW2_EPG0_SWO_R48_BCF.W' $ ng_stor = GetSeries(api_key=api_key, series_id=series_id)
normal,malicious = plots.top_n_ports(10) $ print(normal) $ print(malicious)
dataset.head(20)
full_data.cust_id.value_counts().unique()
df3 = pd.merge(df2_new, country_df , on=['user_id']) $ df3.head()
df_twitter_archive_copy['name'] = df_twitter_archive_copy.apply(extract_name_from_text, axis=1)
from carto.auth import APIKeyAuthClient $ USERNAME="wri-rw" $ USR_BASE_URL = "https://{user}.carto.com/".format(user=USERNAME) $ auth_client = APIKeyAuthClient(api_key=carto_api_token, base_url=USR_BASE_URL)
conditions_clean = conditions_lower.str.strip() $ conditions_clean
sns.boxplot(data.renta.values) $ plt.show() $
tweet_df = tweet_df[tweet_df.place != 'Myanmar'] $ tweet_df.describe()
obs_diffs = p_new - p_old $ (p_diffs > obs_diffs).mean()
%load_ext watermark $ %watermark -a 'Gopala KR' -u -d -v -p watermark,numpy,pandas,matplotlib,nltk,sklearn,tensorflow,theano,mxnet,chainer,seaborn,keras,tflearn,bokeh,gensim
df_afc_champ2018 = df_afc_champ2018.sort_values('playnumber')
x_final = sparse.hstack((scalingDF_sparse, categDF_encoded))
df_con_control = df_con1.query('group =="control"') $ x_control = df_con_control["user_id"].count() $ x_control $
X = preprocessing.scale(X) $
p_diffs = [] $ for _ in range(10000): $     new_page_converted=np.random.choice([0,1],size=nnew[0],p=[pnew,1-pnew]).mean() $     old_page_converted=np.random.choice([0,1],size=nold[0],p=[pold,1-pold]).mean() $     p_diffs.append((new_page_converted/nnew)-(old_page_converted/nold))
tweets_kyoto.head()
autos["registration_month"].value_counts().sort_index(ascending=True)
from ncbi_remap.io import remove_chunk, add_table
df.shape
from textblob import TextBlob $ from sklearn.feature_extraction.text import CountVectorizer $ from sklearn.feature_extraction.text import TfidfVectorizer $ import nltk $ from nltk import tokenize
search = api.search(q='football',count=5) $ print(type(search)) $ print(search)
for k in range(initial_year,final_year + 1): $     a = outputSnapshot_for_a_given_year(k)
austin.head()
n_old = len(df2[df2['group']=='control']) $ n_old
df = df.loc[df['job_id'] == '2572']
lda = LatentDirichletAllocation( $     n_topics=n_topics, learning_method="online", random_state=0) $ lda = lda.fit(tf)
import pandas as pd $ import json $ %cd ds_mtgjson
p_new = df2.query('converted == 1').user_id.nunique() / df2.user_id.nunique() $ p_new
multiple_party_votes_all = multiple_party_votes_all.loc[:,~multiple_party_votes_all.columns.duplicated()]
duplicate_row = df2.loc[df2.duplicated('user_id') == True] $ duplicate_row
autos['brand'].unique().tolist()
s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan,'CABA', 'dog', 'cat']) $ s
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print("z-score:", z_score, "\np-value:", p_value)
re_split_raw = re.split(r'[ \W]+', raw) $ print(re_split_raw[100:150])
x = np.arange(-2,2,0.01) $ plt.plot(x,x*x, 'blue', label='$x^2$') $ plt.plot(x,abs(x), 'orange', label='|x|') $ plt.legend(loc="upper center") $ plt.show()
engine.execute('SELECT * FROM stations LIMIT 10').fetchall()
len(nullCity2015)
df_clean3.rating_numerator.value_counts()[:10]
logit_control_1 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA_int_ab_page','UK_int_ab_page', 'US_int_ab_page']]) $ result_control_1 = logit_control_1.fit()
useless_variables = ['CODE_POSTAL', 'ETAGE', 'PAYS', 'POINTS_FIDEL', 'STOP_PHONING'] $ equipment.drop(useless_variables, axis=1, inplace=True)
print(df2.converted.mean())
df.count().tail()
df['score'].corr(df['comms_num'])
first_result.find('strong').text[0:-1] + ', 2017'  
from bs4 import BeautifulSoup
reg = linear_model.LinearRegression(fit_intercept=False, normalize=False)
sel_df.Beat.value_counts(dropna=False)
gcv.best_params_
import datetime $ df_date['date'] = map(lambda x: datetime.datetime(x.year,x.month,x.day,0,0), df_date.created_at)
X_valid, y_valid = X_valid.head(90), y_valid.head(90)
stock = stock.set_index("Date")
df_metric[df_metric['count']>30]
df_h1b_mv.pw_1.describe()
estimator.predict_proba(X1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=42) $ X_train.shape , y_train.shape,  X_test.shape, y_test.shape
shelter_df_only_idx.registerTempTable("shelter") $ sqlContext.sql("SELECT OutcomeType_idx, AnimalType_idx, SexuponOutcome_idx, Breed_idx, Color_idx, AgeuponOutcome_binned FROM shelter").show() $
cityID = 'c3f37afa9efcf94b' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Austin.append(tweet) 
grp.transform( lambda x:x+10 )
essential_genes.head()
df['polarity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity) $ df['subjectivity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
tweets_df[~(tweets_df.in_reply_to_screen_name.isnull())].count()
yah = ha.accounts.manual_current('Yahoo', path=os.path.join('data', 'manual_accounts'), $                                  currency='YHOO')
colors = ['gold', 'lightskyblue', 'lightcoral'] $ ridedata_fare = ridedata_df[['city', 'fare']] $ cityfare_driver= pd.merge(citydata_df, ridedata_fare, on='city', how='left') $
autos=autos[autos["price"].between(500,350000)]
usage_400hz_filter_pd.to_csv("400HZ_gefilterd_Q4_2017.csv")
df2.query('user_id==773192')
round(np.sqrt(model_x.scale), 3)
cats_df = detect_outliers(cats_df, 'hair length', 1.5) $ cats_df['breed'] = cats_df['breed'].apply(lambda x:x if x != 'Donald' else None)
X = pd.merge(X, meal_cuisine_types[['meal_id', 'is_ct1_portuguese', 'is_ct1_irish', 'is_ct1_mexican', 'is_ct1_chinese', 'is_ct1_german', 'is_ct1_chamorro', 'is_ct1_central_american', 'is_ct1_mediterranean', 'is_ct1_japanese', 'is_ct1_singaporean', 'is_ct1_desserts_bakeries', 'is_ct1_ecuadorian', 'is_ct1_persian', 'is_ct1_asian', 'is_ct1_latin_american', 'is_ct1_spanish', 'is_ct1_ice_cream_gelato', 'is_ct1_barbecue', 'is_ct1_cafe', 'is_ct1_brunch', 'is_ct1_paleo', 'is_ct1_caribbean', 'is_ct1_argentinian', 'is_ct1_vietnamese', 'is_ct1_tapas_small_plates', 'is_ct1_american', 'is_ct1_nordic', 'is_ct1_south_american', 'is_ct1_gastropub_food', 'is_ct1_peruvian', 'is_ct1_indian', 'is_ct1_guatemalan', 'is_ct1_brazilian', 'is_ct1_korean', 'is_ct1_health_food', 'is_ct1_european', 'is_ct1_indonesian', 'is_ct1_lao', 'is_ct1_hawaiian', 'is_ct1_jewish', 'is_ct1_african', 'is_ct1_middle_eastern', 'is_ct1_french', 'is_ct1_asian_noodle_soup', 'is_ct1_vegan', 'is_ct1_russian', 'is_ct1_thai', 'is_ct1_australian', 'is_ct1_other', 'is_ct1_balkan', 'is_ct1_cuban', 'is_ct1_filipino', 'is_ct1_east_european', 'is_ct1_seafood', 'is_ct1_turkish', 'is_ct1_malaysian', 'is_ct1_british', 'is_ct1_salvadorian', 'is_ct1_north_african', 'is_ct1_greek', 'is_ct1_burmese', 'is_ct1_hispanic', 'is_ct1_pizza', 'is_ct1_cajun_creole', 'is_ct1_north_american', 'is_ct1_californian', 'is_ct1_vegetarian', 'is_ct1_soul_food', 'is_ct1_italian', 'is_ct2_portuguese', 'is_ct2_irish', 'is_ct2_cajun_creole', 'is_ct2_chinese', 'is_ct2_german', 'is_ct2_hispanic', 'is_ct2_central_american', 'is_ct2_mediterranean', 'is_ct2_japanese', 'is_ct2_singaporean', 'is_ct2_spanish', 'is_ct2_ecuadorian', 'is_ct2_pizza', 'is_ct2_persian', 'is_ct2_asian', 'is_ct2_latin_american', 'is_ct2_barbecue', 'is_ct2_ice_cream_gelato', 'is_ct2_mexican', 'is_ct2_cafe', 'is_ct2_thai', 'is_ct2_caribbean', 'is_ct2_turkish', 'is_ct2_tapas_small_plates', 'is_ct2_burmese', 'is_ct2_desserts_bakeries', 'is_ct2_south_american', 'is_ct2_gastropub_food', 'is_ct2_peruvian', 'is_ct2_indian', 'is_ct2_korean', 'is_ct2_colombian', 'is_ct2_european', 'is_ct2_british', 'is_ct2_indonesian', 'is_ct2_balkan', 'is_ct2_srilankan', 'is_ct2_hawaiian', 'is_ct2_jewish', 'is_ct2_taiwanese', 'is_ct2_african', 'is_ct2_middle_eastern', 'is_ct2_vegan', 'is_ct2_asian_noodle_soup', 'is_ct2_seafood', 'is_ct2_french', 'is_ct2_polynesian', 'is_ct2_russian', 'is_ct2_brunch', 'is_ct2_australian', 'is_ct2_cuban', 'is_ct2_filipino', 'is_ct2_vegetarian', 'is_ct2_vietnamese', 'is_ct2_malaysian', 'is_ct2_lao', 'is_ct2_health_food', 'is_ct2_north_african', 'is_ct2_greek', 'is_ct2_american', 'is_ct2_east_european', 'is_ct2_nordic', 'is_ct2_north_american', 'is_ct2_italian', 'is_ct2_other', 'is_ct2_soul_food', 'is_ct2_californian', 'is_ct3_portuguese', 'is_ct3_mexican', 'is_ct3_chinese', 'is_ct3_thai', 'is_ct3_peruvian', 'is_ct3_central_american', 'is_ct3_mediterranean', 'is_ct3_japanese', 'is_ct3_singaporean', 'is_ct3_persian', 'is_ct3_asian', 'is_ct3_latin_american', 'is_ct3_spanish', 'is_ct3_ice_cream_gelato', 'is_ct3_barbecue', 'is_ct3_south_american', 'is_ct3_caribbean', 'is_ct3_vietnamese', 'is_ct3_tapas_small_plates', 'is_ct3_desserts_bakeries', 'is_ct3_gastropub_food', 'is_ct3_other', 'is_ct3_korean', 'is_ct3_health_food', 'is_ct3_european', 'is_ct3_indonesian', 'is_ct3_indian', 'is_ct3_hawaiian', 'is_ct3_jewish', 'is_ct3_taiwanese', 'is_ct3_north_american', 'is_ct3_middle_eastern', 'is_ct3_french', 'is_ct3_asian_noodle_soup', 'is_ct3_vegan', 'is_ct3_german', 'is_ct3_brunch', 'is_ct3_cuban', 'is_ct3_filipino', 'is_ct3_vegetarian', 'is_ct3_turkish', 'is_ct3_malaysian', 'is_ct3_north_african', 'is_ct3_hispanic', 'is_ct3_american', 'is_ct3_pizza', 'is_ct3_african', 'is_ct3_californian', 'is_ct3_seafood', 'is_ct3_soul_food', 'is_ct3_italian']], on='meal_id', how='inner')
control_df = df2.query('group == "control"') $ control_df.user_id.nunique()
mb.loc[('Firmicutes', 2)]
pd.date_range('2017', periods=4, freq='QS-FEB')  # 4 quarters starting from beginning of February
conv_prop = df.query('converted == "1"').user_id.nunique() / df.user_id.nunique() * 100 $ print("The proportion of users converted is", round(conv_prop, 4)) $
a.capitalize().lower()
df_birth.head()
crime_wea.info(null_counts=True)
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=uEEsBcH3CPhxJazrzGFz&start_date=2017-01-01&end_date=2017-12-31") $
df.head()
df.groupby('key').aggregate(['min', np.median, max])
import os.path as op $ my_gempro.save_pickle(op.join(my_gempro.model_dir, '{}.pckl'.format(my_gempro.id)))
new_df_left['parcelid'].sort_values() $ new_df_left[new_df_left['parcelid'].duplicated(keep=False)].reset_index()
headers = pd.read_csv("raw/headers.csv", header = None) $ head2 = [x[0].strip() for x in headers.values] $ data = pd.read_csv("raw/surveys.csv", header=None) $ data.columns = head2 $ data = data.set_index("SurveyResultId")
tqdm.pandas()
unordered_df = USER_PLANS_df.iloc[unordered_timelines]
items = find_rome_dataset_by_name(rome_data, 'item') $ new_items = set(items.new.code_ogr) - set(items.old.code_ogr) $ obsolete_items = set(items.old.code_ogr) - set(items.new.code_ogr) $ stable_items = set(items.new.code_ogr) & set(items.old.code_ogr) $ matplotlib_venn.venn2((len(obsolete_items), len(new_items), len(stable_items)), (OLD_VERSION, NEW_VERSION));
cohort_activated_df = pd.DataFrame(index=daterange,columns=daterange)
cast_data = pd.read_csv('../datasets/mad-men-cast-show-data.csv')
index=pd.Index(np.arange(3)) $ index
ab_df_new.head()
draft_df.language.value_counts()
mr = dd.from_pandas(rentals, npartitions=3)
algo=tree.DecisionTreeClassifier(max_features=4) $ train = algo.fit(X_train, y_train) $ res=train.predict(X_val)
df = df.set_index('user_id').join(df2.set_index('user_id'));
sort_df.describe()
gMapAddrDat.set_timeDelay(0)
n = 1
(p_diffs > ab_data_diff).mean()
reddit.shape
p_old = p_new
import pandas as pd
corpus[300][:1000]
tobs_data = [] $ for row in temperature_data: $     tobs_data.append(row[0])
import itertools $ from keras.layers import Dropout
_dm2 = pd.read_sas('./data/in/adtteos.sas7bdat', format='sas7bdat') $ _dm2.head()
(etsamples,etmsgs,etevents) = be_load.load_data()
apache_people_df.count()
def get_user_id_from_file(f): $     return f.split('url_csvs/')[-1].replace('_urls.csv', '') $ df_link_yt['twitter_id'] = df_link_yt['filename'].apply(get_user_id_from_file)
obs_diff = new_page_converted/n_new - old_page_converted/n_old $ obs_diff
import zipfile $ with zipfile.ZipFile(ml_1m_path, "r") as z: $     z.extractall(dataset) $ with zipfile.ZipFile(ml_20m_path, "r") as z: $     z.extractall(dataset)
a = [1,2,0,0,4,0] $ nonz = np.nonzero(a) $ print(nonz)
print(cc['ranknow'].describe())
df.iat[6,2] = 666 $ df
twitter_final.info() 
conv_ind = df2.query('converted == 1') $ num_conv = conv_ind.shape[0] $ print('Probability of converted individual regardless of the page: {:.4f}'.format(num_conv/df2.shape[0]))
ticker = []
h.end_time # Shows us the end of that hour
pd.set_option("display.max_columns", 65) $ df
au.find_some_docs(uso17_coll,sort_params=[("id",1)],limit=3)
betas_argmax = np.zeros(shape=mcmc_iters) $ betas_argmax = beta_dist.argmax(1)
clientes_path="data/clientes_sociodemografica.csv" $ transacciones_path="data/transacciones.csv"
df_best_chart
sales_update_nan = sales_update.dropna() $ sales_update_nan.isnull().sum()
print(system_info)
df_mes[(df_mes['extra']!=0)&(df_mes['extra']!=0.5)&(df_mes['extra']!=1)&(df_mes['extra']!=1.5)].extra.unique()
import numpy as np $ import matplotlib.pyplot as plt $ country_comp = free_data.groupby('country')['y'] $ print('Most people in',country_comp.sum().idxmin(), 'feel they are more free than repressed.')
testObjDocs.outDF[984:991]  ## as expected - bad rows dropped but we now have an indexing issue
predicted_outcome_first_measure.head()
train_topics_df=train_topics_df.fillna(0) $ test_topics_df=test_topics_df.fillna(0)
model.save('model/my_model_current.h5') $ model.summary()
upper = df2_no_outliers.mean() + df2_no_outliers.std()*3 $ lower = df2_no_outliers.mean() - df2_no_outliers.std()*3 $ df2_no_outliers.loc[df2_no_outliers['y'] > upper['y'], 'y'] = None $ df2_no_outliers.loc[df2_no_outliers['y'] < lower['y'], 'y'] = None
state_DataFrames.keys()
genes = pd.concat( [ LT906474,CP020543 ] )
(series + pandas.Series({'a': 2, 'c': 2})).dropna()
day_of_year15["avg_day_of_month"] = day_of_year15.index.map(lambda x: int(x.split("-")[2])) $ day_of_year15.head()
with open(url.split('/')[-1], mode='wb') as file: $     file.write(response.content)
properati.info()
plt.title("homework") $ plt.xlabel("Date") $ plt.ylabel("Prcp")
ben_final[ben_final['pagetitle'].str.contains('/')]
df2.user_id.duplicated().sum()
ncptable = data.pivot_table(index=["author"], values=["num_comments"], aggfunc=sum).reset_index() $ ncptable.sort_values(inplace=True, by="num_comments", ascending=False) $ ncptable[0:9]
livetweets.count()
cust_data = cust_data.drop("MonthlySavings1",axis = 1).head(3) $ cust_data.drop("MonthlySavings1",axis = 1,inplace = True).head(3)
sampled_authors_saved.show()
options = webdriver.ChromeOptions() $ options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit 537.36 (KHTML, like Gecko) Chrome')
pd.Series({2:'a',1:'b',3:'c'}, index=[3,2])
ticker = 'F'
rf.feature_importances_
for country in list(df_joined['country'].unique()): $     df_joined['it_'+country] = df_joined.apply(lambda row: row['ab_page']*row[country], axis=1) $ df_joined.head()
compound_df.columns = target_terms $ compound_df.head()
run txt2pdf.py -o"2018-06-19 2015 UNIVERSITY OF MIAMI HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2015 UNIVERSITY OF MIAMI HOSPITAL Sorted by discharges.txt"
p_old = p_new $ p_old
channel_meta['topic_ids_json'] = json.loads(channel_meta['topic_ids']) $ channel_meta['topic_ids_json']
dfAll = pd.read_csv('All.csv',index_col=0) $ df_ohc = pd.get_dummies(dfAll,columns=['events','day_of_week']) $ df_ohc.head()
DUMPINGS = data[data.zscore_model>(2)] $ DUMPINGS = DUMPINGS[DUMPINGS.zscore_brand>(2)] $ print '\n {}% of all offers were classified as dumpings \n'.format(100*float(len(DUMPINGS))/len(data)) $ DUMPINGS
BDAY_PAIR_df.head()
rng_pytz.tz == tz_pytz
outcome.head()
not_creditworthy_user=merkmale.groupby(level=['dwh_country_id','id'])['not_creditworthy', $                                                                'not_creditworthy_US', $                                                                'not_creditworthy_KRMLKWKX_US'].max() $ not_creditworthy_user.sum()
week3 = week2.rename(columns={21:'21'}) $ stocks = stocks.rename(columns={'Week 2':'Week 3','14':'21'}) $ week3 = pd.merge(stocks,week3,on=['21','Tickers']) $ week3.drop_duplicates(subset='Link',inplace=True)
[x[:6] for x in results_parallel[0][:3]]
df_new.groupby(['converted', 'country'])['day_of_week'].value_counts()
df.tail(50)
df3=pd.read_csv('countries.csv') $ df3.head(3)
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $ print(new_page_converted)
index # Automatically converted to DatetimeIndex
train.
df_final.to_csv('twitter_archive_master.csv')
old_compiled_data.iloc[0]
defs=actual_payments_combined.loc[(actual_payments_combined.iso_date==datetime.date(2015,4,30))& (actual_payments_combined.in_arrears_since_days_30360>90),['fk_loan','iso_date','in_arrears_since_days_30360']].fk_loan.unique() $ defs
clustering_df = classification_data[['company_uuid','company_name','company_category_list','category_group_list', $                                     'short_description','description','founded_on']].copy() $ clustering_df.drop_duplicates('company_uuid',inplace = True)
proj_df['Project Subject Category Tree'].fillna('').apply(has_literacy_and_language)
graf_counts = graf_counts.reset_index()
df.isnull().values.any() #check if the dataset has any null value
print(data["Time Stamp"][0].day) $ print(data["Time Stamp"][0].month) $ print(data["Time Stamp"][0].year)
class Car(object): $     wheels = 4 $     def __init__(self, make, model): $
mediaMask = results['ActivityMediaSubdivisionName'] == "Surface Water" $ hydroMask = ~results['HydrologicEvent'].isin(("Storm","Flood","Spring breakup","Drought")) $ charMask = results['CharacteristicName'] == "Nitrogen, mixed forms (NH3), (NH4), organic, (NO2) and (NO3)" $ sampFracMask = results['ResultSampleFractionText'] == 'Total'
scores.IMDB.describe()
probs = model2.predict_proba(X_test) $ print probs
dict_urls = json.loads(df.loc[row,'urls']) $ pprint(dict_urls)
(new_page_converted).mean() - (old_page_converted).mean()
uber_15["day_of_week"].value_counts()
dmap = {0:'Mon',1:"Tue",3:"Wed",4:"Thurs",5:"Fri",6:"Sat",7:"Sun"} $ twitter_final['dayofweek'] = twitter_final['dayofweek'].map(dmap)
df_arch_clean = df_arch.copy()
df = pd.DataFrame({'A': np.arange(50), $                'B': np.arange(50) + np.random.randn(50), $                'C': np.sqrt(np.arange(50)) + np.sin(np.arange(50)) }) $ print df[:10]
print("Comments Included: {0}".format(len(recent_comments_included))) $ print("Experiment Comments: {0}".format(len(experiment_comments)))
df.tail(10)
scr_retention_df = scr_activated_df - scr_churned_df
df.drop( columns=df.columns[3:6] )       # delete columns by range of column number
cur.execute('SELECT * FROM postgres LIMIT 2;') $ for r in cur.fetchall(): $    print(r)
bb['low'].plot()
time_series['count'].sum() == (nodes.shape[0] + ways.shape[0])
tmp = possible_bots.groupby(['followers_count']).size().reset_index() $ tmp.rename(columns={0:'nusers'},inplace=True) $ ax = sns.scatterplot(data=tmp,y='nusers',x='followers_count') $ ax.set_yscale('log')
query = session.query(Measurement.date,Measurement.prcp) $ rain = query.order_by(Measurement.date).filter(and_(Measurement.date >= year_ago_str, $                     Measurement.date < fake_today)) $ df = pd.read_sql(rain.statement,session.bind) $ df.head(10)
beta_XOM , alpha_XOM = np.polyfit(daily_returns['SPY'],daily_returns['XOM'],1) $ print(beta_XOM , alpha_XOM)
 user_profiles = [ $      {'reinsurer_id': 278, 'reinsurer': 'Atlantic Re'}, $      {'reinsurer_id': 275, 'reinsurer': 'XL Re'}] $  result = db.profiles.insert_many(user_profiles)
merged.sort_values("amount", ascending=False)
rootDistExp = Plotting(S.setting_path.filepath+S.para_trial.value)
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.binomial(n_new, p_new) $     old_page_converted = np.random.binomial(n_old, p_old) $     p_diffs.append(new_page_converted/n_new - old_page_converted/n_old) $
sessions.to_csv('sessions_cleaned.csv')
other_text_feature.shape
s519281_df = pd.DataFrame(s519281) $ print(len(s519281_df.index)) $ s519281_df.info() $ s519281_df.head(5)
prices = aapl.loc[dates] $ prices
for date, score_dict in zip(dates, vader_scores): $     score_dict['created_at'] = date
cam_sheet_names
X_train, X_validate, y_train, y_validate = train_test_split(X.values, y)
rf.fit(X, y)
train, test = dogscats_h2o.split_frame(ratios=[0.7])
sns.distplot(autodf.price)
df.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/ratings.csv', sep=',', encoding='utf-8', header=True)
train.head(3)
resultvalue_df.head()
import pandas as pd       $ train = pd.read_csv("movie-data/labeledTrainData.tsv", header=0, \ $                     delimiter="\t", quoting=3)
dedup[dedup.job_type == 'Sponsored'].hash.value_counts()[0:3]
 new_page_converted = np.random.choice([0,1], n_new, [(1-p_new), p_new])
has_text[has_text.url=='http://www.nytimes.com/2012/07/30/opinion/keller-the-entitled-generation.html']
df_tweet = pd.DataFrame(list_tweets_je) $ df_tweet.head(10)
logreg = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])
pd.pivot_table(more_grades, index="name")
c.head()
number_of_commits = len(git_log) $ number_of_authors = len(git_log.dropna()['author'].unique()) $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
news_dict_df.to_json("Newschannel_tweets_df.json")
import pandas as pd $ import numpy as np $
games_2017 = nba_df.loc[(nba_df.index.year == 2017), ] $ pd.pivot_table(games_2017, values = "Tm.Pts", index = "Team", aggfunc = np.mean).sort_values(by = "Tm.Pts", ascending = False).head(5)
print 'df_average : ', df_average.shape $ print 'df_categories : ', df_categories.shape $ print 'df_categories_svd : ', df_categories_svd.shape
weather.drop(['STATION', 'NAME'], axis=1, inplace=True)
facts_res = requests.get(facts_url)
df2['DepTimeStr'].head()
new_page_converted = np.random.choice([1,0],n_new,p=[p_new,(1-p_old)]) $ new_page_converted
ts = df.groupby(pd.Grouper(key='created_at', freq='D')).mean()
sentiments = df[['created_at', 'neg', 'pos']] $ sentiments = sentiments.sort_values(by="created_at") $ sentiments = sentiments.reset_index(drop=True) $ sentiments.head()
user_week_day_frequency = user_df["week_day"].value_counts() $ ax = plt.subplot(111) $ ax.bar(user_week_day_frequency.index, user_week_day_frequency.data) $ plt.show()
train_agent_and_comment.head()
train = pd.read_csv(dirs[-2], dtype={"ip":"int32", "app":"int16", "device":"int16", "os":"int16", "channel":"int16"})
intake = pd.read_csv('Data files/Austin_Animal_Center_Intakes.csv') $ outcome = pd.read_csv('Data files/Austin_Animal_Center_Outcomes.csv')
df_archive["doggo"].value_counts()
df.dtypes
print(np.exp(res.params))
df2 = pd.merge(df2, df3, left_on='user_id', right_on='user_id', how='left')
conn.summary(table=dict(name='data.iris', caslib='casuser'))
overall_ng         = all_text.sum().sort_values(ascending=False)[5:40] $ gucci_ng           = all_text[all_text.index == "gucci"].sum().sort_values(ascending=False).head(25) $ mcqueen_ng         = all_text[all_text.index == "Alexander McQueen"].sum().sort_values(ascending=False).head(25) $ stellamccartney_ng = all_text[all_text.index == "Stella McCartney"].sum().sort_values(ascending=False).head(25) $ burberry_ng        = all_text[all_text.index == "Burberry"].sum().sort_values(ascending=False).head(25)
twitter_archive_clean[~twitter_archive_clean['in_reply_to_status_id'].isnull()]
image_copy.tweet_id=image_copy.tweet_id.astype(str) $ df_copy.tweet_id=df_copy.tweet_id.astype(str) $ tweet_data_copy.tweet_id=tweet_data_copy.tweet_id.astype(str)
with path.open() as f: $     aSingleLine = f.readline() $ aSingleLine
repos = pd.read_csv('data/new_subset_data/new_subset_repos.csv', sep='\t') $ users = pd.read_csv('data/new_subset_data/new_subset_users.csv', sep='\t') $ owns = pd.read_csv('data/new_subset_data/new_subset_owns.csv', sep='\t') $ stars = pd.read_csv('data/new_subset_data/new_subset_stars.csv', sep='\t') $ final_data = pd.read_csv('data/new_subset_data/ratings_data.csv', sep='\t') $
dfChile = dfChile[dfChile["latitude"] < -17.000000]
replace_name = dict() $ for i in [203106, 203490, 201985, 201228, 200839, 101236]: $     replace_name[i] = str(player_details[player_details.PLAYER_ID==i].DISPLAY_FIRST_LAST.unique()[0]) $ for i in [203106, 203490, 201985, 201228, 200839, 101236]: $     p_stats.loc[p_stats["PLAYER_ID"]==i, "PLAYER_NAME"] = replace_name[i]
c.find_one({'name.middle': 'Joseph'})
len(df2.loc[(df2['group'] == 'treatment') & (df2['converted'] == 1)]['user_id'])/len(df2.loc[df2['group'] == 'treatment']['user_id'])
df_trips = pd.merge(df_trips, pilot_planets, how="left", on="pilot")
print(bitc)
logreg = LogisticRegression() $ scores = cross_val_score(logreg,  X_test, y_test,  cv=5) $ np.mean(scores), np.std(scores)    # scoring on my Testing Data under performs also
lm.score(x_test, y_test)
tmp = df[selected_features].join(outcome_scaled).reset_index().set_index('date') $ tmp.dropna().resample('Q').apply(lambda x: x.corr()).iloc[:,-1].unstack().iloc[:,:-1].plot() $
cm_dt = confusion_matrix(y_test, y_tree_predicted)
trigram_model_filepath = paths.trigram_model_filepath
p5_result = p2_table.sort_values('Profit', ascending=True) $ p5_result.head()
df_new.country.unique()
avg_values = [] $ for x in range(0,30): $     avg_values.append(dfs[x]['Outside Temperature'].mean()) $ print (avg_values)
grouped = mergedLocationDF.groupby('OA11CD').agg({'count': np.sum}).reset_index() $ grouped.head()
logit = sm.Logit(ab_data['converted'], ab_data[['intercept','treatment']]) $ logit_results = logit.fit()
rnd_clf2 = RandomForestClassifier(random_state=42) $ t0 = time.time() $ rnd_clf2.fit(X_train_reduced, y_train) $ t1 = time.time()
BAL = pd.read_excel(url_BAL, $                     skiprows = 8) # This reads in each team's respective spreadsheet, ignoring the header.
X = best_worst.text $ y = best_worst.stars $ X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4) $ X_train_dtm = stfvect.fit_transform(X_train) $ X_train.shape
print( "The best accuracy was with", mean_acc.max(), "with k=", mean_acc.argmax()+1) 
params = {'figure.figsize': [4,4],'axes.grid.axis': 'both', 'axes.grid': True, 'axes.labelsize': 'Medium', 'font.size': 12.0, \ $ 'lines.linewidth': 2} $ plot_partial_autocorrelation(series=dr_num_new_patients.diff()[1:], params=params, lags=30, alpha=0.05, title='PACF {}'.format('first difference of dr number of new patients')) $ plot_partial_autocorrelation(series=dr_num_existing_patients.diff()[1:], params=params, lags=30, alpha=0.05, title='PACF {}'.format('first difference of dr number of existing patients'))
print(df_B.columns) $ print(df_B)
obs_diff = df2.query('group == "treatment"').converted.mean() - df2.query('group == "control"').converted.mean() $ obs_diff
df2['intercept'] = 1 $ df2_dummy = pd.get_dummies(df['landing_page']) $ df2['ab_page'] = df2_dummy['new_page'] $ df2.head()
np.exp(results.params)
print(my_tree_one.feature_importances_) $ print(my_tree_one.score(features_one, target))
preci_data = session.query(Measurement.date, Measurement.prcp).\ $     filter(Measurement.date > last_year).\ $     order_by(Measurement.date).all()
ngrams_summaries = cvec_2.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(10)
region_return_amt = returned_orders_data.groupby(['Region'])['Sales'].sum() $ print(region_return_amt)
own_star.to_pickle('data/pickled/new_ratings_data.pkl')
g = train_df.groupby('interest_level') $ g.describe(include = 'all')
sns.countplot(x='currency', data=data_for_model, hue='final_status')
math.exp(4.165)
ks_goal_failed = ks_goal_failed.sort_values(by = ['counts'], ascending = False) $ ks_goal_failed.head(10)
len(U_B_df)
treino.head()
station_tobs = session.query(Measurement.tobs).filter(Measurement.station == 'USC00519281', Measurement.date >= '2016-08-23').all() $
hpdpro[hpdpro['ComplaintID']=='8338184']
BID_PLANS_df.head()
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[n_new, n_old], alternative = 'larger') $ print(' z-score = {}.'. format(round(z_score,4))) $ print(' p-value = {}.'. format(round(p_value,4))) $
places = api.geo_search(query="Brighton", granularity="city") $ place_id_B = places[0].id $ print('Brighton id is: ',place_id_B)
dfU = df.drop_duplicates('user_id') $ dfU.info()
df_sample=df.sample(n=len(df)/200) $ print "The original size is:",len(df) $ print "The sample size is:",len(df_sample)
Imagenes_data.tweet_id.duplicated().value_counts()
data.to_csv("data.csv",index=False, encoding='utf-8')
visits.head()
pd.Series(data=predicted10).value_counts()
df2.head()
plt.hist(data, 30, facecolor='green', alpha=0.5, log = True) $ plt.xlabel('Views') $ plt.ylabel('Count') $ plt.title('Histogram of the Log of Number of Views') $ plt.show() $
model.compile(optimizer='RMSprop', loss='mse')
opened_prs = PullRequests(github_index).get_cardinality("id_in_repo").by_period() $ print("Trend for month: ", get_trend(get_timeseries(opened_prs))) $ opened_pr = PullRequests(github_index).get_cardinality("id_in_repo").by_period(period="quarter") $ print("Trend for quarter: ", get_trend(get_timeseries(opened_pr)))
def sinplot(flip=1): $     x = np.linspace(0, 14, 100) $     for i in range(1, 7): $         plt.plot(x, np.sin(x + i * .5) * (7 - i) * flip)
rent_db3.price.hist(bins=50)
from statsmodels.tsa.stattools import adfuller as ADF $ print() $ print(ADF(resid_713.values)[0:4]) $ print() $ print(ADF(resid_713.values)[4:7])
link = 'http://eforexcel.com/wp/wp-content/uploads/2017/07/1500000%20Sales%20Records.zip'
def calc_daily_ret(closes): $     return np.log(closes/closes.shift(1))[1:]
plt.scatter(compound_final.index, compound_final.Compound) $ plt.show()
ca_pl_path = cwd + '\\data_for_pl_ca_simu.csv' $ ca_pl_all = pd.read_csv(ca_pl_path) $ print(ca_pl_all.shape)
title = mandelaSpeechText[:192] $ text = mandelaSpeechText[193:] $ mandelaSpeechDict = {'speech-title' : title, 'speech-text' : text}
train_shifted = train.copy() $ train_shifted['y_t+1'] = train_shifted['load'].shift(-1, freq='H') $ train_shifted.head(10)
shannon_sakhalin_relative = shannon_sakhalin / np.log2(len(sakhalin_freq)) $ shannon_sakhalin_relative
df_total = df_total[ls_columns_reordered]
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ z_score, p_value
import pandas as pd
bg3 = bg2.drop(bg2.index[0]) # drop first row $ bg3
day_counts.show(5)
df_all = df_submissions.append(df_comments) $ df_all['comment'] = np.where(df_all['comment_id'].isnull(),0,1) $ df_all['submission'] = np.where(df_all['comment']==0,1,0) $ print(df_all.shape) $ df_all.columns
archive_clean[archive_clean['tweet_id'] == 778027034220126208].rating_numerator 
print("The proportion of users converted: {}".format(df["converted"].mean()))
clean_test_reviews_sw[0][0:80]
df = data.dropna() $ df
df_max.index.name = 'month'
data['year'] = 2013 $ data
new_page_converted = df2.sample(n_new, replace = True)
print(df.group.unique()) $ print(df.landing_page.unique()) $
df.select_dtypes(exclude='number')
repos = pd.read_csv('data/new_subset_data/new_subset_repos.csv', sep='\t') $ repos.info() $ repos.head()
1/np.exp(results.params[2])
shows.columns
scaled = scaled.reset_index() $ scaled.head()
df
df['converted'].sum()/df['user_id'].nunique()
sp500.ix[['MSFT', 'ZTS']]
ff3.head()
import pandas as pd
df3.values
import warnings $ warnings.simplefilter("ignore", UserWarning) $ warnings.simplefilter("ignore", FutureWarning)
d - Week(weekday = 1)
df_ab_converted_portion.head()
df_tweets = pd.DataFrame(tweets) $ df_tweets
from sklearn.neighbors import KNeighborsClassifier $ k = 7 $ kNN_model = KNeighborsClassifier(n_neighbors=k).fit(X_train,y_train) $ kNN_model
sales_2016.head()
m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), $                    0.04, 1, [1000,500], [0.001,0.01], y_range=y_range) $ lr = 1e-3
X_test, y_test = get_minibatch(doc_stream, size=5000) $ X_test = vect.transform(X_test) $ print('Accuracy: %.3f' % clf.score(X_test, y_test))
df2[mask_dupe]
type(df_master.tweet_id[1])
p_obs_diff = old - new $ p_obs_diff
df_copy['stage']=df_copy['stage'].replace('doggoNonepupperNone','pupper') $ df_copy['stage']=df_copy['stage'].replace('doggoNoneNonepuppo','puppo') $ df_copy['stage']=df_copy['stage'].replace('doggoflooferNoneNone','floofer')
df = pd.read_csv('small_df_for_developing_and_debugging.csv', parse_dates = ['date']) $ df.head()
r=session.get('https://python.org/')
labels = df.author $ true_k = np.unique(labels).shape[0]
p_diffs = np.array(p_diffs) $ (p_diffs > obs_diff).mean()
print("Original data shape:", data.shape) $ data = utils.get_coordinates(data) $ print("Updated data shape:", data.shape)
prob_convert_t = df2_treatment.converted.mean() $ prob_convert_t
gMapAddrDat = GMapsLoc_DFBuilder(endRw=4, time_delay=3, return_null=True)
ret_aapl.plot(figsize=(8,6));
len(free_data.country.unique())
print(corr)
df.query('converted==1').user_id.nunique() / df.user_id.nunique()
df_genre = df['genre'].to_frame() $ print(df_genre)
y_preds = lgr_grid.best_estimator_.predict(X_test) $ lgr_scores = show_model_metrics('Logistic Regression', lgr_grid, y_test, y_preds)
df = pd.read_csv('https://raw.githubusercontent.com/jackiekazil/data-wrangling/master/data/chp3/data-text.csv') $ df.head(2)
df_master[df_master.favorite_count == [df_master['favorite_count'].max()]]
valid_labels = create_word_pos_label(valid_pos, val_label) $ test_labels = create_word_pos_label(test_pos, test_label)
pd.date_range('2018 May 1st', '2018 Jul 3', freq = 'B')
df_likes_grouped['Total'] = df_likes_grouped.sum(axis=1) $ df_likes_grouped = df_likes_grouped.loc[:, (df_likes_grouped != 0).any(axis=0)]
df.isnull().sum().any()
index_weighted_returns = generate_weighted_returns(returns, index_weights) $ etf_weighted_returns = generate_weighted_returns(returns, etf_weights) $ project_helper.plot_returns(index_weighted_returns, 'Index Returns') $ project_helper.plot_returns(etf_weighted_returns, 'ETF Returns')
contractor.info()
orig_iphone_tweets = filter_iphone(orig_tweets) $ orig_android_tweets = filter_android(orig_tweets)
education_data.drop(list(range(539, 546)), inplace=True)
print(finder.best_params_)
%matplotlib inline $ ventas_dptos.plot('bar', title = 'Ventas de departamentos por anio', figsize=(15,10))
!tail -n 10 p32cf_results.txt
tokens['one_star'] = tokens.one_star + 1 $ tokens['five_star'] = tokens.five_star + 1
cerberus_dict = {} $ cerberus_dict["title"] = cerberus_title $ cerberus_dict["img_url"] = cerberus_full_img $ print(cerberus_dict) $ hemisphere_image_urls.append(dict(cerberus_dict)) $
xpdraft2 = xpdraft1.groupby("name").status.value_counts() $ xpdraft2.head()
to_be_predicted_Day2 = 36.44907637 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
mb_file = pd.ExcelFile('../data/microbiome/MID1.xls') $ mb_file
df_columns[df_columns['Complaint Type']=='Noise - Residential']['Descriptor'].value_counts().head() $
import pandas as pd $ import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline $ sns.set_style("white")
m3 = md2.get_model(opt_fn, 1500, bptt, emb_sz=em_sz, n_hid=nh, n_layers=nl, $            dropout=0.1, dropouti=0.4, wdrop=0.5, dropoute=0.05, dropouth=0.3) $ m3.reg_fn = partial(seq2seq_reg, alpha=2, beta=1) $ m3.load_encoder(f'adam3_10_enc')
val = np.zeros(len(dates)) $ sample = pd.DataFrame(val, index = dates) $ sample['Date']=sample.index
store1_open_data[['Sales']]
df_coun = pd.read_csv("countries.csv") $ df_coun.head() $ df_coun.count()
P_new = ab_df2['converted'].mean() $ print(P_new)
df_new['UK_ab_page'] = df_new['UK'] * df_new['ab_page'] $ df_new['US_ab_page'] = df_new['US'] * df_new['ab_page'] $ log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'UK_ab_page', 'US', 'US_ab_page']]) $ results = log_mod.fit() $ results.summary()
sim_closes.plot(figsize=(8,6));
df2 = df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == True] $ df2.head()
len(df_measurement['station'].unique()) $
data2_df = pd.concat([start_df, pd.get_dummies(start_df.UserType)], axis=1) $ data2_df = pd.concat([data2_df, pd.get_dummies(start_df.Season)], axis=1)
pd.Series(HAMD.min(axis=1)<0).value_counts()  # no
df_new.sample(5)
subred_counts = pd.DataFrame(reddit['subreddit'].value_counts()) $ subred_counts
merged.amount.sum()
autos['odometer_km'].describe()
data = {'Integers' : [1,2,3], $         'Floats' : [4.5, 8.2, 9.6]} $ df = pd.DataFrame(data, index = ['label 1', 'label 2', 'label 3']) $ df
df2.head()
print("Logistic Regression") $ param_grid = {'solver':['newton-cg'], 'n_jobs':[-1], 'max_iter':[100, 300, 400], 'C':[0.01, 0.1, 1.0, 10.0, 30, 70, 100, 300, 1000]} $ train_give_me_a_value(LogisticRegression(solver='newton-cg', n_jobs=-1), param_grid)
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response = requests.get(url) $ response
1/_
assert np.isclose(trump.loc[690171032150237184]['hour'], 8.93639)
df = pd.read_csv('estimating_quartiles.csv')
hashed_train_sample = hashingTF.transform(parsed_train_sample).select('id', 'label', 'features').cache() $ hashed_train1 = hashingTF.transform(parsed_train1).select('id', 'label', 'features').cache() $ hashed_dev1 = hashingTF.transform(parsed_dev1).select('id', 'label', 'features').cache() $ hashed_modeling2 = hashingTF.transform(parsed_modeling2).select('id', 'label', 'features').cache() $ hashed_test = hashingTF.transform(parsed_test).select('id', 'label', 'features').cache()
graf_counts2.head()
(autos["date_crawled"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index().tail() $ )
tz_dog = timedog_df.groupby('userTimezone')[['tweetRetweetCt', 'tweetFavoriteCt']].mean() $ tz_dog.head()
todaysTweets[todaysTweets[date].isnull()] #check nulls
len(model.wv.vocab)
archive_copy.doggo.unique()
stops["color"].value_counts()
json_tweets = pd.DataFrame(API_tweet_data, columns = ['tweet_id','retweet_count', 'favorite_count', $                                                'text','retweeted', 'date_time']) $ json_tweets.to_csv('tweet_json.txt', encoding = 'utf-8', index=False)
sum(df2.duplicated())
tables = [pd.read_csv(f'{PATH}{fname}.csv', low_memory=False) for fname in table_names]
df.iloc[:].plot(title="Precipitation By date") $ plt.xticks(rotation=70) $ plt.tight_layout() $ plt.savefig("precipitation.png") $ plt.show() $
linked
"issid" in df_protest.columns
df_ab.count()[0]
soup.select('table.type_2 a[href]')
dog_ratings.source.value_counts()
df_archive_clean.rating_numerator = df_archive_clean.rating_numerator * 10 / df_archive_clean.rating_denominator
df2 = pd.DataFrame(df, index = ['b', 'c', 'd', 'a', 'e']) $ df2
raw_data.info()
twitter_archive[twitter_archive['rating_denominator'].astype(int)< 10]
tweets_original = pd.DataFrame(ndarray_original)
variables
data['User_Score'] = data['User_Score'].apply(pd.to_numeric, errors='coerce') $ data['User_Score'] = data['User_Score'].mask(np.isnan(data["User_Score"]), data['Critic_Score'] / 10.0)
X_train = train.drop('label', axis=1) $ Y_train = train['label'] $ X_test = test.drop('label', axis=1) $ Y_test = test['label']
svd = TruncatedSVD(true_k) $ lsa = make_pipeline(svd,Normalizer(copy=False))
sensor.description
3 + 4
users[3:6]
df.head()
vec_test = [(7, 21), (0, 21),(18,17)] $ print(tfidf[vec_test]) $ for i in range(len(corpus)): $     vec = corpus[i] $     print(i," ", tfidf[vec])
Check_Results(2015,445)
session.query(Measurement.station, func.count(Measurement.tobs)).filter(Measurement.tobs!=None).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).first()
logit_mod = sm.Logit(df_merge['converted'], df_merge[['intercept', 'ab_page', 'UK', 'US']]) $ res = logit_mod.fit() $ res.summary()
data2010.set_index('state', inplace=True) $ density = data2010['population'] / data2010['area (sq. mi)']
import os $ print(os.listdir())
post_id_as_str = str(post_id) $ post_id_as_str  ## output is a string
correlation_matrix = np.corrcoef(data_matrix.T)
master_file = pd.read_csv(os.curdir + '/master.csv', encoding='iso-8859-1', dtype=object)
df = pd.read_csv("../data/20180220-wikia_stats_users_birthdate.csv") $ df['datetime.birthDate'] = pd.to_datetime(df['datetime.birthDate'], infer_datetime_format=True, errors='coerce') $ df.set_index(df['datetime.birthDate'], inplace=True) $ df.head()
a = description.values.tolist()
template = 'Lee wants a {} right now' $ menu = ['sandwich', 'pizza', 'hot potato'] $ for snack in menu: $     print(template.format(snack))
(null_vals > act_diff).mean()
df.groupby('Agency').count().sort(desc("count")).show(10)
data = fat.add_ema_columns(data, 'Close', [3, 6, 12, 26, 50])
df = pd.read_csv('data/cornwall_phones.csv') $ df.head()
from bs4 import BeautifulSoup $ import urllib.request
close_prices.groupby([close_prices.index.year, close_prices.index.month]).min()
df.dtypes
WholeDf.created_at $ dateparse = lambda x: WholeDf.created_at.strptime(x, '%Y-%m-%d %H:%M:%S') $ datecreated = WholeDf $ datecreated = pd.to_datetime(WholeDf.created_at, format='%Y-%m-%d')
from sklearn.linear_model import LogisticRegression $ from sklearn import metrics $ logreg = LogisticRegression() $ y = train.target.values $ logreg.fit(train1, y)
wrsids[np.where(ids=='105001')[0][0]] $
df_prep7 = df_prep(df7) $ df_prep7_ = pd.DataFrame({'date':df_prep7.index, 'values':df_prep7.values}, index=pd.to_datetime(df_prep7.index))
tran_time_diff[tran_time_diff.msno == '++1Wu2wKBA60W9F9sMh15RXmh1wN1fjoVGzNqvw/Gro=']
top_songs.head()
ga.shape
sentiment_analysis(S2)
p_cont = df2.query('group=="control"').converted.mean() $ p_cont
archive_clean['timestamp'] =pd.to_datetime(archive_clean.timestamp)
data.order_date.min(),data.order_date.max()
X_test.shape
df['Col_1'].cumsum()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)
properati.shape
segmentData['opportunity_conversion'] = segmentData.apply(oppConversion, axis=1)
def scorecard_by_year(df): $     df['year'] = df.index.get_level_values('date').year $     return df.groupby('year').apply(calc_scorecard).T $ print(scorecard_by_year(df))
twitter_archive_master.to_csv('twitter_archive_master.csv')
pizza_corpus, pizza_train_model_final, pizza_dictionary = create_LDA_model(pizza_train, 'reviews_without_rare_words', 10, 40)
print("The proportion of users converted is: {}".format(df['converted'].mean()))
us_prec = prec_fine.reshape(844,1534).T #.T is for transpose $ np.shape(us_prec)
bool(0), bool(1)
p_diffs = [] $ for _ in range (10000): $     new_page_converted = np.random.choice([0,1], 145310, p=[p_mean,(1-p_mean)]) $     old_page_converted = np.random.choice([0,1], 145274, p=[p_mean,(1-p_mean)]) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean())   
pred.head(rows=3)
df_select = df[select5features].copy(deep=True)
users.head()
mentions_df_begin = pd.DataFrame(mentions_begin,columns=["epoch","src","trg","lang","text"]) $ print(len(mentions_df_begin))
Xs=pd.DataFrame(Xs) $
G8.index
start_date=dac_train_date.min() $ start_date
first_values_liberia = grouped_months_liberia.first() $ first_values_liberia=first_values_liberia.rename(columns = {'National':'First_v_T'}) $ first_values_liberia.head()
for year in range(2001,2017): $     num_missing = len(epa_adj.loc[(epa_adj['adj CO2 (kg)'].isnull()) & $                                   (epa_adj['YEAR']==year), 'ORISPL_CODE'].unique()) $     total = len(epa_adj.loc[epa_adj['YEAR']==year, 'ORISPL_CODE'].unique()) $     print 'In', str(year) + ',', num_missing, 'plants missing some data out of', total
tweet_df_clean.head()
%pylab inline
firstday_df = firstday_df.sort_values(by="created_at", ascending = True) $ firstday_df
data.sort_values(by='Likes',ascending=False,inplace=True) $ data.head()
df2.drop(repeated_user.index, inplace=True) $ len(df2)
df_archive_clean["retweeted_status_timestamp"].unique()[0:10]
p = fp7.append(h2020) $ fp7 = 0 $ h2020 = 0
swcolumns = snow_weather.columns $ swindex = pd.date_range('2017-05-14', periods = 2).date $ sw_append = pd.DataFrame([[0.0, 0], [0.0, 0]], columns=swcolumns, index=swindex) $ snow_weather = snow_weather.append(sw_append) $ snow_weather.tail()
!gsutil cp gs://solutions-public-assets/smartenup-helpdesk/ml/issues.csv $CSV_FILE
import pandas as pd $ df = pd.DataFrame({'a': 1, 'b': range(4)}) $ df
len(y) 
print("Kickstarter dataframe shape (just failed, successful, and dropped cols): " , kickstarter_failed_successful.shape) $ kickstarter_failed_successful
y = np.log1p(y) $ y.hist(grid=True, bins=50)
pivoted.T.shape $
indexed_df[:5]
twitter_archive_df.sample(3)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], value=0, alternative='larger', prop_var=False) $ print(z_score, p_value)
games_2017_sorted.loc[:, ["Team", "Opp", "Home", "Home.Attendance"]].head(10) # games are duplicated 
total_df = total_df[['Res_id', 'Name','Neighbourhood', 'Url', 'Categories','Review_count','Rating']]
ts_utc
areas_dataframe = pandas.DataFrame(areas, columns=['company', 'company_id', 'count', 'rate'])
new_conver_rate = df2.converted.mean() $
for v in data.values(): $     if v['answers']['Q1'] == 'yes': $         v['answers']['Q1A'] = 0
cols = ['screenName', 'id_str', 'name', 'description', 'url', $         'location', 'cityOrState', 'country', 'lang', 'created_at', 'verified', $         'followers_count', 'friends_count', 'favourites_count', 'statuses_count'] $ users = users[cols]
df2['intercept']=1 $ dumm=pd.get_dummies(df2['landing_page']) $ df2=df2.join(dumm)
def inverse_regression_predictions(output, intercept, slope): $     estimated_feature = float(output - intercept) / slope $     return estimated_feature
air_3d7 = air_rsrv_by_date[air_rsrv_by_date['air_store_id'] == 'air_0f0cdeee6c9bf3d7'] $ air_rsrv_april = air_3d7[air_3d7['visit_date'] >  '2017-04-01' ] $ air_rsrv_april.head(10)
from sklearn.model_selection import train_test_split $ X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split( X, y, test_size=0.2, random_state=4) $ print ('Train_knn set:', X_train_knn.shape,  y_train_knn.shape) $ print ('Test_knn set:', X_test_knn.shape,  y_test_knn.shape)
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == 1').count()[0] $ convert_new = df2.query('landing_page == "new_page" and converted == 1').count()[0] $ n_old = df2.query('landing_page =="old_page"').count()[0] $ n_new = df2.query('landing_page =="new_page"').count()[0]
class PhDStudent(Student): $
liberiaCasesSuspected = liberiaCasesSuspected.fillna(0) $ liberiaCasesProbable = liberiaCasesProbable.fillna(0) $ liberiaCasesConfirmed = liberiaCasesConfirmed.fillna(0)
team_names['regular_occurrences'] = [c[name.lower()] for name in names] $ team_names.sort_values(['regular_occurrences'],ascending=False).head()
test.shape
autos['date_crawled'] = autos['date_crawled'].str[:10].str.replace('-', '').astype(int) $ autos['date_crawled'].head()
mw = pd.read_csv('mw.csv', sep='~') $ mw.rename(columns={'body':'content', 'dst':'slug'}, inplace=True) $ mw['created'] = pd.to_datetime(mw.timestamp, utc=True, unit = 's') $ mw = mw[['title','slug','created','content']] $ mw.info()
top_betweenness = sorted_betweenness[:20] $ for tb in top_betweenness: # Loop through top_betweenness $     degree = degree_dict[tb[0]] # Use degree_dict to access a node's degree, see footnote 2 $     print("Name:", tb[0], "| Betweenness Centrality:", tb[1], "| Degree:", degree)
y_pred_rf = rf.predict(X_test) $ y_train_pred_rf=rf.predict(X_train) $ print("Accuracy of logistic regression classifier on on test set: {:0.5f}".format(rf.score(X_test, y_test)))
cashflows_plan_investor_20150430=generate_cashflows_pp(payment_plans_combined,loan_fundings1,loans,'2015-04-30') $ cashflows_plan_investor_all=generate_cashflows_pp(payment_plans_combined,loan_fundings1,loans)
crime_list
confusion_matrix(Y_valid, final_valid_pred_nbsvm1)
f_ip_app_minute_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_app_minute_clicks"), header=True) $ print('Found %d observations.' %f_ip_app_minute_clicks.count())
test_df.info()
frame.apply(f, axis='columns')
ytx_do
p_diffs=np.array(p_diffs) $ plt.hist(p_diffs) $ plt.axvline(d, color='red') 
from sklearn import svm $ SVM = svm.SVC() $ SVM.fit(X_train, y_train) 
datetime(1, 1, 1).toordinal()
within_sakhalin_request_url  #it is good to inspect an url before sending a request
import pandas as pd
pd.to_datetime('2018-4-14')
count_by_source = data.groupby('Source').size()
metrobus = pd.read_csv('../Datos Capital/estaciones-de-metrobus.csv',low_memory=False)
def get_keys_in_list(input_): $     return input_.keys() $ grouped_publications_by_author['authorCollaborators'] = grouped_publications_by_author['link_weight'].apply(get_keys_in_list) $ grouped_publications_by_author['authorCollaboratorIds'] = grouped_publications_by_author['link_weight_v2'].apply(get_keys_in_list)
df = pd.read_csv("en-wikipedia_traffic_200801-201709.csv", sep='\t')
revisions = [revision for revision in page.revisions(reverse=True, content=True)] $ revisions[0]
m_df = pd.merge(raw_full_df, pd.DataFrame(raw_train_y), left_index=True, right_index=True)
df['created_at'] = pd.to_datetime(df['Created Date'], format= '%m/%d/%Y %I:%M:%S %p')
train_view.sort_values(by=5, ascending=False)[0:10]
ocsvm_cleaned_tfidf.fit(trump_cleaned_tfidf, y = y_true_cleaned_tfidf) $ prediction_cleaned_tfidf = ocsvm_cleaned_tfidf.predict(test_cleaned_tfidf) $ prediction_cleaned_tfidf
n_old = df2[df.group == 'control'].shape[0] $ n_old
airports_by_city.map(lambda x: len(x)).sort_values(ascending=False).head(10)
train_data_df.columns
print('First Example :\n=====\n', [ppm_body.id2token[x] for x in vectorized_body[0] if x > 1]) $ print('\nSecond Example :\n=====\n', [ppm_body.id2token[x] for x in vectorized_body[1] if x > 1])
X = df.drop('num_comments', axis = 1) $ y = df.num_comments
r = requests.get('https://www.quandl.com/api/v3/datasets/OPEC/ORB.json?start_date=2013-01-01&end_date=2013-01-01&api_key='+API_KEY)
dsmith_total_hours = sum(overview_table["Dsmith time (hrs)"]) $ clsmith_total_hours = sum(overview_table["CLSmith time (hrs)"]) $ print(f"DeepSmith total testing time: {dsmith_total_hours:.1f} hours") $ print(f"CLSmith total testing time: {clsmith_total_hours:.1f} hours")
stats.norm.cdf((active_marketing - inactive_marketing) / SD_marketing)
len(USER_PLANS_df)
train_data.loc[(train_data.vehicleType == 'kleinwagen') & (train_data.gearbox.isnull()), 'gearbox'] = 'manuell' $ test_data.loc[(test_data.vehicleType == 'kleinwagen') & (test_data.gearbox.isnull()), 'gearbox'] = 'manuell'
tokens = pickle.load(open("pickle_files/tokens.p","rb"))
n_old = df2.query('group == "control"').count()[0] $ n_old
blame = pd.concat([pd.DataFrame(first_line).T, blame]) $ blame.head()
df2['user_id'].index.name == countries_df['user_id'].index.name
import pandas as pd $ import numpy as np $ import re
from fastai.structured import * $ from fastai.column_data import * $ np.set_printoptions(threshold=50, edgeitems=20) $ PATH='data/rossmann/'
from quantopian.pipeline.classifiers.fundamentals import Sector $ morning_sector = Sector()
ADP.dtype
if 'read_lenght' in articles: $     del articles['read_lenght']
software_techniques[0]
results.sort_index().head() $ results.sort_index(ascending=False).head() $ results.sort_index(axis=1).head() $ result.sort_values() $ results[['home_score','away_score','home_team']].sort_values(ascending=[False,True],by=['home_score','away_score']).head()
df_image_tweet = pd.merge(df_tweet_clean2, df_image_clean2, on='tweet_id', how='left')
style.use('ggplot')
page_interaction_log = sm.Logit(df4['converted'], df4[['ab_page', 'country_UK', 'country_US', 'intercept']]) $ page_interaction_result = page_interaction_log.fit()
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_ipb_brace_crown[1][0])) $ perr = np.sqrt(np.diag(pcov_ipb_brace_crown[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
df.index.month.value_counts().sort_index().plot()
categoryColumns=['category','country','location_type'] $ for col in categoryColumns: $     print(col) $     df_fail_success=AddDummyColumnsToDataFrame(df_fail_success,col) $
ppm_body.token_count_pandas().head()
dateCounts = pd.DataFrame(all_dates.value_counts().reset_index()) $ dateCounts.columns = ['dates','countsOnDate'] $ dateCounts.head()
len(df2_treatment) / len(df2)
df2.query("landing_page == 'new_page'").count()[0]/df2.shape[0]
saver.restore(sess,tf.train.latest_checkpoint('/tmp/testing/stock_prediction_12_21/'))
emails_dataframe['address'].str.split("@").str.get(1)
gs_from_model_under.score(X_test, y_test_under)
condition = (notus['cityOrState'].str.contains('Canada', case=False)) & (notus['country'] == 'Canada') $ notus.loc[condition]
model.wv.similarity('like', 'love')
df.query('landing_page == "new_page"').query('group != "treatment"').info()
Google_stock.tail()
plt.savefig('Output_Graphs/tweets_scatter.png', bbox_inches = 'tight') $
from htsworkflow.submission.encoded import Document $ from htsworkflow.submission.aws_submission import run_aws_cp
df['Long'].value_counts(ascending=False).head(5)
education_data.reset_index(drop=True, inplace=True)
writers.groupby("Country").mean()
df.head(3)
url = "http://www.hotashtanga.com/p/letoltesek-downloads.html" $ html_txt = urllib.request.urlopen(url).read() $ dom =  lxml.html.fromstring(html_txt) $ [line for line in dom.xpath('//a/@href')][0:10]   
import string $ def pig_it(text): $     return " ".join([word[1:] + word[:1] + "ay"  if word not in string.punctuation else word for word in text.split()]) $ pig_it("simon was o here !")
liberiaDf.head()
dictionary["c"]=5 $ print(dictionary)
mb.ix[('Firmicutes', 2)]
daily_averages = pivoted.resample('1d').mean()
for row in schemaExample.take(2): $     print row.ID, row.VAL1, row.VAL2
pd.read_sql('desc actor', engine)
url = 'http://space-facts.com/mars/'
offset = pd.tseries.offsets.BMonthEnd()
baseball.ab.corr(baseball.h)
twitter_archive_master.info()
results = model.transform(test) $ results=results.select(results["label"],results["prediction"],results["FRAUD"],results['predictedLabel'],results["probability"]) $ results.toPandas().head(10)
ipAddress_to_country.info()
user_df.head()
zone_train = df_run[df_run['zone'].notnull()] $ zone_test = df_run[df_run['zone'].isnull()] $ zone_train.head()
to_be_predicted_Day4 = 17.67907235 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
lead_classes = labels_dedupe.pivot('funding_round_uuid','investor_uuid','is_lead_investor')
column_check = inspector.get_columns('measurement') $ for check in column_check: $     print(check['name'],check['type'])
train_data.vehicleType.isnull().sum()
messages_with_dates_ext.head(100)
auth = tweepy.OAuthHandler(consumer_key=consumerKey, $     consumer_secret=consumerSecret) $ api = tweepy.API(auth) $
root_cell = openmc.Cell(name='root cell') $ root_cell.fill = assembly $ root_cell.region = +min_x & -max_x & +min_y & -max_y & +min_z & -max_z $ root_universe = openmc.Universe(universe_id=0, name='root universe') $ root_universe.add_cell(root_cell)
df[df.converted==1].shape[0]/unique_users
first_result.find('strong').text[0:-1]
df2_converting = df2.converted.mean() $ print('The probability of converting regardless of the page is: {}'.format(round(df2_converting, 4)))
dataset_rows = df.shape[0] $ print(dataset_rows)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller') $ (z_score, p_value)
x = slack.channels.list()
data = [{'a': i, 'b':2 * i} $        for i in range(3)] $ pd.DataFrame(data)
parser.HHParser.keys
df[df['diff_log_EBAY'].isnull()==True]
df = pd.read_parquet('training_data.parquet')
df_valid["Died"] = pd.to_datetime(df_valid['Died'], unit="ms") $ df_valid = df_valid[df_valid["Died"] < datetime.strptime("2018-01-01", "%Y-%m-%d")]
del merged_portfolio_sp['Date_y'] $ merged_portfolio_sp.rename(columns={'Date_x': 'Latest Date', 'Adj Close_x': 'Ticker Adj Close' $                                     , 'Adj Close_y': 'SP 500 Initial Close'}, inplace=True) $ merged_portfolio_sp.head()
new_page_converted=np.random.choice([1,0],size=n_new,p=[p_new,1-p_new]) $ new_page_converted.mean()
df.info()
df = es_rdd.map(lambda l: Row(**dict(l))).toDF()
m_grid, c_grid = np.meshgrid(m_vals, c_vals)
tbl
new_obj
jobs_data.nunique()
with tb.open_file(filename='data/my_pytables_file.h5', mode='r') as f: $     print(f.root.my_vlarray[:])
model = word2vec.Word2Vec.load("200features_30minwords_10context")
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True) $
df_ctr = df2[df2['group'] == 'control']['converted'].mean() $ print("{} is the probability they converted.Thus, given that an individual was in the control group.".format(df_ctr))
df.nunique()
average_daily_sales = data[['Sales', 'Open']].resample('D').mean() $ print('Correlation with last day: {}'.format(average_daily_sales['Sales'].autocorr(lag=1))) $ print('Correlation with last month: {}'.format(average_daily_sales['Sales'].autocorr(lag=30))) $ print('Correlation with last year: {}'.format(average_daily_sales['Sales'].autocorr(lag=365)))
df['user_id'].nunique()
obs_diffs= 0.1188 - 0.1204 $ obs_diffs
ideas.drop(['Authors', 'Link', 'Tickers', 'Title', 'Strategy'],axis=1,inplace=True) $ ideas = ideas.applymap(to_datetime)
temp_df.groupby('reorder_interval_group').corr()
df["text"] = df["extended_tweet"].combine_first(df["text"])
session = Session(engine)
control_convert = df2.query('group =="control"').converted.mean() $ print("Probability of control group converting is :", control_convert)
merged1.drop('Id', axis=1, inplace=True)
quandl.ApiConfig.api_key = API_KEY $ quandl.get('FSE/AFX_X', start_date='2018-08-08', end_date='2018-08-08')
t.days
newdf.drop('yearmonth', axis=1, inplace=True)
from collections import defaultdict $ piotroski_size = defaultdict(list)
mentioned_bills_all.shape
df_providers.columns
df2.columns[0] $ df2.drop('comments.data' , 1 , inplace=True) $ df2.drop( df2.columns[[0,1,5,6,7]] , 1 , inplace=True)
eval = pd.read_csv(eval_file, header=0, sep='\t', na_values=[''], keep_default_na=False) $ X_eval = eval.iloc[:,features_index] $ y_eval = eval['Label'] $ np.all(eval.columns == train.columns)
english_df.show(5)
targetUsersRank['norm_rank']=(targetUsersRank['label']-targetUsersRank['label'].min())/(targetUsersRank['label'].max()-targetUsersRank['label'].min()) $ print targetUsersRank.shape $ targetUsersRank.head()
image_predictions_clean[image_predictions_clean.jpg_url=='https://pbs.twimg.com/media/CkjMx99UoAM2B1a.jpg'] $
test = datatest[datatest['floor'].isnull()] $ train = datatest[datatest['floor'].notnull()]
df_transactions['membership_duration'] = df_transactions['membership_expire_date'] - df_transactions['transaction_date']
sort_a_asc = noNulls['a'].asc()
df.info() $ df.isnull().sum()
import getpass $ account = "root@r2lab.inria.fr" $ password = getpass.getpass(f"Enter password for {account} : ")
new_values = [] $ for i in X.values: $     new_values.append(" ".join(i))
total_sales['q1_pct_change'] = total_sales['2016_q1_sales']/total_sales['2015_q1_sales']
def twitter_setup(): $     auth = tweepy.OAuthHandler(twitter_credentials.CONSUMER_KEY, twitter_credentials.CONSUMER_SECRET) $     auth.set_access_token(twitter_credentials.ACCESS_TOKEN, twitter_credentials.ACCESS_TOKEN_SECRET) $     api = tweepy.API(auth) $     return api
_ = notus['country'].value_counts(dropna=False).index.tolist()[1:45] $ _
df['MeanFlow_cfs'].plot();
for index, row in op_ed_articles.iterrows(): $         i = row['full_text'].lower() $         i = text_cleaner(i) $         op_ed_articles.loc[index, "full_text"] = i $
log_user2 = df_log[df_log['user_id'].isin([df_test_user_2['user_id']])] $ print(log_user2)
with pd.option_context('max.rows', 15): $     print(result["Fail"].sort_values(ascending=False)) $
df2.country.unique()
feedex.shape
corTable
crimes.groupby('PRIMARY_DESCRIPTION').size()
grouped_authors_by_publication.head()
np.exp(-0.0408), 1/(np.exp(-0.0408))
print("Train before", train['StateHoliday'].unique()) $ train.replace({"StateHoliday": {"0": 0}}, inplace=True) $ print("Train Before", train['StateHoliday'].unique())
bacteria[[name.endswith('bacteria') for name in bacteria.index]]
import json $ import requests $ import itertools
df_uniname.iloc[list_index_one,:]
ioDF = pd.read_pickle('../data/io.pkl')
autos.drop(['dateCrawled','dateCreated','lastSeen','postalCode'],axis='columns',inplace=True)
df = pd.merge(train.to_frame(), hsi["rise_nextday"].to_frame(), left_index=True, right_index=True)
frame2.values[0].dtype
new_page_converted=np.random.choice([1,0],size=n_new,p=[p_new,(1-p_new)]) $ new_page_converted.mean()
user = user[user.is_enabled >= 0]
d_bnbAx.head()
train_dum = pd.get_dummies(TrainData, columns=['Source', 'Customer_Existing_Primary_Bank_Code'], $                           dummy_na=True, sparse=True, drop_first=True) $ test_dum = pd.get_dummies(TestData, columns=['Source', 'Customer_Existing_Primary_Bank_Code'], $                           dummy_na=True, sparse=True, drop_first=True)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ print (z_score, p_value)
filter = titanic.notnull() $ filter.head()
tipsDF.dtypes
lancaster = nltk.LancasterStemmer() $ print([lancaster.stem(t) for t in tokens[550:600]])
injuries_hour=pd.merge(df_weather,col_totals,left_on="date_time",right_on="date_time",how="left")
df7 = df[df['hired'] ==1] $ df7.groupby('category')['position','hourly_rate','num_completed_tasks'].agg({'median','mean','min','max'}).reset_index() \ $     .rename(columns={'category': 'category', 'position': 'position_stats_hired','hourly_rate':'hourly_rate_stats_hired', \ $                  'num_completed_tasks':'num_completed_tasks_stats_hired'}) $
reddit = pd.get_dummies(reddit, columns=['whitelist_status', 'media'])
cluster_label_group_1 = df.groupby("Cluster_Labels_GaussianMixture").mean()
print('Data type of each column: ') $ data.dtypes
monthly_gain_summary = pd.DataFrame() $ monthly_gain_summary['Infy'] = infy_df.gain_perc.resample('M').mean() $ monthly_gain_summary['Glaxo'] = glaxo_df.gain_perc.resample('M').mean() $ monthly_gain_summary['BEML'] = beml_df.gain_perc.resample('M').mean() $ monthly_gain_summary['Unitech'] = unitech_df.gain_perc.resample('M').mean()
dfNYC.head()
df2[df2.duplicated(['user_id'],keep=False)]
def get_list_username(the_posts): $     list_username = [] $     for i in list_Media_ID: $         list_username.append(the_posts[i]['shortcode_media']['owner']['username']) $     return list_username
autos['last_seen'].str[:10].value_counts(normalize=True, dropna=False).head(10)
autos['seller'] = autos['seller'].map(seller_translate) $ autos['vehicle_type'] = autos['vehicle_type'].map(vehicle_translate) $ autos['gearbox'] = autos['gearbox'].map(gearbox_translate) $ autos['fuel_type'] = autos['fuel_type'].map(fuel_translate) $ autos['unrepaired_damage'] = autos['unrepaired_damage'].map(damage_translate)
addresses_df.reset_index().set_index('Case.Number').loc[ $     joined_df.reset_index().loc[joined_df.reset_index()['Match'].isnull(),'Case.Number'] $ ]
df.head(3)
bmp_series = pd.Series(brand_mean_prices).sort_values(ascending=False)
tweetsPerDay = dict(final.groupby('Date')['tweetCount'].sum()) $ tweetsPerDay = pd.DataFrame(list(tweetsPerDay.items()), columns=['dt', 'TotCount'])
city_eco = city_pop.copy() $ city_eco["eco_code"] = [17, 17, 34, 20] $ city_eco
df_cod3 = df_cod2.copy() $ df_cod3["Cause of death"] = df_cod3["Cause of death"].apply(classify_natural) $ df_cod3
tweet_json_clean = tweet_json
df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']] $ df_new.head()
s3_upload.upload_file(local_merge_key, s3_bucket, s3_key_merge, $            Callback=ProgressPercentage(local_merge_key))
import requests $ election_results = 'https://en.wikipedia.org/wiki/List_of_United_States_presidential_election_results_by_state' $ response = requests.get(election_results)
test_df = dftouse[continuous_test_features]
df_providers.groupby(['drg3']).get_group(39).head(3)
pd.Series({'a' : 0., 'c' : 1., 'b' : 2.})  # from Python dict, autosort by default key
users.to_csv('Data/users3.csv', index=False, encoding='utf-8')
import numpy as np
plt.scatter(dataframe['lat'], dataframe['lon'],c = dataframe['label'],cmap=plt.cm.Paired) $ plt.scatter(center['lat'],center['lon'],marker='s') $ plt.show()
df_a.join(df_b)
temp_series_paris = temp_series_ny.tz_convert("Europe/Paris") $ temp_series_paris
min_ab = 450
np.datetime64('2015-07-04 12:00')
flight6.coalesce(2).write.parquet("C:\\s3\\20170503_jsonl\\flight6.parquet")
df_old = df2.query('group == "control"') $ n_old = df_old.shape[0] $ n_old
df2 = df[((df.group == 'treatment') & (df.landing_page == 'new_page')) | $          (df.group == 'control') & (df.landing_page == 'old_page')] $ df2.shape[0]
result_set
model.most_similar("queen")
forked = repos[['owner_id', 'forked_from', 'created_at']] $ forked = forked.rename(columns={'owner_id': 'user_id', 'forked_from': 'repo_id'}) $ forked['forked'] = 4 $ forked.head() $ forked.info()
codes.shape
customer_with_purchases_3ormore = df_p2["Counter amax"] >= 3 $ df_p3 = df_p2.loc[customer_with_purchases_3ormore]
res = skafos.engine.create_view( $     "weather_noaa", {"keyspace": "weather", $                       "table": "weather_noaa"}, DataSourceType.Cassandra).result() $ print("created a view of NOAA historial weather data")
twitter_delimited_daily= create_twitter_data() $ stock_delimited_daily = create_stock_data() $ daily_df = pd.concat([twitter_delimited_daily, stock_delimited_daily], axis=1, join='inner') $ daily_df.head()
elms_all_0611 = elms_all_0604.append(elms_all, ignore_index=True) $ elms_all_0611.drop_duplicates('ACCOUNT_ID',inplace=True)
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline
archive_clean.drop(['Unnamed: 0'], axis=1, inplace = True) $ images_clean.drop(['Unnamed: 0'], axis=1, inplace = True) $ popularity_clean.drop(['Unnamed: 0'], axis=1, inplace = True)
1/np.exp(results.params)
df2[df2.duplicated('user_id', keep=False)]
tweets['chars'] = tweets['full_text'].str.len()
!pip3 install seaborn
df[df.province==400][df.city<=16].groupby("city").sum().sort_values(by="postcount",ascending=False)["postcount"][:10]
index_to_change = df3[df3['group'] == 'treatment'].index $ df3.set_value(index = index_to_change, col='ab_page', value = 1) $ df3.set_value(index = df3.index, col ='intercept', value=1) $ df3[['intercept', 'ab_page']] = df3[['intercept', 'ab_page']].astype(int) $ df3 = df3[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
clf.fit(Xtrain, ytrain) $
WholeDf['text']
ID_list = ['1','2','3'] $ data_df[data_df.ID.isin(ID_list)]
os.environ["s3_key"] = "s3://wri-public-data/" + s3_key_merge $ os.environ["gs_key"] = "gs://resource-watch-public/" + s3_key_merge $ !gsutil cp $s3_key $gs_key
df_loudparties = df[df['Descriptor'] == 'Loud Music/Party'] $ df_loudparties.groupby(df_loudparties.index.hour).apply(lambda x: len(x)).plot()
import numpy as np $ preTest = [4, 4, 31, 2, 3], $ postTest =  [25, 25, 57, 62, 70] $ print np.mean(preTest) $ print np.mean(postTest)
rspm = aqmdata['rspm'].value_counts() $ rspm.iloc[0:10]
data.loc[data.rooms.isnull(),'rooms'] = data['rooms*price']/data['price_aprox_usd'] $ del data['rooms*price']
new_cols_cust.to_csv('tags_cust_proc') $ new_cols_prosp.to_csv('tags_prosp_proc')
weekly = logins.groupby('weekday').count()/15
melted_total.head()
df[unique_cols].head(2)
resource_id = hs.createHydroShareResource(title=title, content_files=files, keywords=keywords, abstract=abstract, resource_type='genericresource', public=False)
op_add_comms.drop('body', axis=1, inplace=True)
df_2.drop_duplicates('title', inplace = True)
malenewnew  = malenew[['Male']].copy() $ malenewnew.head(3) $
ks_name_failed = ks_name_failed.sort_values(by = ['counts'], ascending = False) $ ks_name_failed.head(10)
cars= cars.drop(['dateCrawled','name','seller','offerType','abtest','nrOfPictures','postalCode'], axis=1) $ cars.head()
import datetime as dt
import os $ REPO = "/content/datalab/training-data-analyst" $ os.listdir(REPO)
%matplotlib inline
api_copy = api_copy[pd.isnull(api_copy['retweeted_status']) == True] $ api_copy.drop('retweeted_status',axis =1 , inplace= True)
s['d'] # ---> Returns '13'
df_new['new_UK']=df_new['UK']*df_new['ab_page'] $ df_new['new_CA']=df_new['CA']*df_new['ab_page'] $ df_new['new_US']=df_new['US']*df_new['ab_page']
display(pd.read_csv('/home/denton/Downloads/2013_311_Service_Requests.csv', nrows=2).head()) $ display(pd.read_csv('/home/denton/Downloads/2013_311_Service_Requests.csv', nrows=2).tail())
df2.user_id.drop_duplicates(inplace = True)
w_mat.shape, b_mat.shape $ fundvolume_binary = b_mat.fillna(0).as_matrix() * volume_m $ totvolume = volume_m.T.sum() $ fraq_fundvolume_binary = fundvolume_binary / totvolume $ fraq_fundvolume_binary.shape
data = pd.read_csv('Dumping.csv', delimiter = ',', skiprows=0, squeeze=False, skip_blank_lines=True, index_col=None) $
df_hate.shape
!spark-submit --properties-file spark.conf script.py data/*
df = pd.read_csv('kickstarter-projects/ks-projects-201801.csv', $                  parse_dates=['deadline', 'launched'], $                  encoding = "ISO-8859-1")
X_cat = pd.concat([tfidfdummiee, skill_dummies, aldf[['salary_predict','duration_int']]], axis=1) $ X_cat.shape
contractor_clean.shape 
mask = ((combined.state=='Colombia') & combined.city.str.startswith('Pe')).pipe(np.invert) $ combined = combined.loc[mask]
start = datetime.now() $ modelrf250 = RandomForestClassifier(n_estimators=250, n_jobs=-1) $ modelrf250.fit(Xtr.toarray(), ytr) $ print(modelrf250.score(Xte.toarray(), yte)) $ print((datetime.now() - start).seconds)
consumer_key = ' ' $ consumer_secret = ' ' $ access_token = ' ' $ access_token_secret = ' '
sss = "SELECT * from paudm.forced_aperture_coadd_deprecated as coadd limit 10" $ data3 = pd.read_sql(sss,engine)
df_old = df2[df2['landing_page'] == 'old_page'] $ converted = df_old['converted'] $ old_page_converted = np.random.choice(converted,n_old)
assignee = "AMAZON TECH%" $ df_target = pd.read_gbq(query, project_id=PROJECT_ID, dialect='standard') $ print(df_target.shape) $
ans = pd.pivot_table(df, values='D', index=['A','B'], columns=['C']) $ ans
print(pd.isnull(dfx))
tables = pd.read_html(fact_url) $ tables
df['Ranking Full URL on Sep  1, 2017'] = df['Protocol for Sep  1, 2017'] + "://" + df['Ranking URL on Sep  1, 2017']
final_df = df3.join(dummy_country) $ final_df.head() $
plt.figure(figsize=(10,5)) $ sns.heatmap(data = test_data.isnull(), yticklabels=False, cbar = False)
dictionary = corpora.Dictionary(texts) $ dictionary
archive_copy.name.sort_values()
model_props = {client.repository.ModelMetaNames.NAME: "XGBoost_model_for_Retail_Churn"} $ model_details = client.repository.store_model(model, model_props)
np.dtype('S10,i4,f8')
from keras.datasets import mnist $ (x_train, y_train), (x_test, y_test) = mnist.load_data()
movie_ratings['year'].unique()
merged_predictions_archive.head(1)
pold = df2['converted'].mean() $ pold
(z_score, p_value)
(autos["ad_created"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index().head(10) $ )
dta.loc[dta.risk == "Risk 1 (High)"]
df2.head()
df = pd.DataFrame.from_dict(last_12_months) $ df = df.set_index('date') $ df.head()
s.str.upper()
learn.save("dnn80")
plt.hist(testdata.rating) $ plt.title("Testing Data Star to Own Distribution");
normalized_delta_recall = [normalized_item_delta_recall(summary) for summary in session_summaries]
def strip_singlequote_and_tolowercase(text): $     try: $         return text.strip('\'').lower() $     except AttributeError: $         return text 
n_training = len(training_X) $ n_holdout = len(holdout_X) $ print(n_training,n_holdout)
authors = EQCC(git_index) $ authors.get_cardinality("author_uuid").by_period() $ print(pd.DataFrame(authors.get_ts()))
df_count_clean.shape
print(len(positive_words)) $ print(len(negative_words))
tbl = pd.merge(df,tbl, on = ['msno','is_cancel']) $ tbl.tail(n= 1)
flights2.passengers.plot()
k1 = data[['cust_id','is_month_start','total_spend']].groupby(['cust_id','is_month_start']).agg('mean').reset_index() $ k1 = k1.pivot(index='cust_id', columns='is_month_start', values='total_spend').reset_index().replace(np.nan,0) $ k1.columns = ['cust_id', 'is_month_start0', 'is_month_start1'] $ train = train.merge(k1,on=['cust_id'],how='left') $ test = test.merge(k1,on=['cust_id'],how='left')
img1 = img1.resize((256, 256),pil_image.NEAREST) $ x1 = np.asarray(img1, dtype='float32') $ img2 = img2.resize((256, 256),pil_image.NEAREST) $ x2 = np.asarray(img2, dtype='float32')
df.drop([5,12,23,56], axis = 0).head(13)
random_series = pd.Series(np.random.randn(1000), $                           index=pd.date_range('1/1/2000', periods=1000)) $ random_series.plot();
dd_df['gearbox'].fillna('gear_unknown', inplace=True)
df2[((df2.group == 'treatment') == (df2.landing_page == 'new_page')) == False].shape[0]
df4.groupby(df4.index).median().plot()
top.to_csv('top50tweets.csv', index=False)
total.iloc[:3]
days_alive / pd.Timedelta(nanoseconds=1)
import numpy as np $ import pandas as pd
df_joined.columns
loans_df.issue_d = pd.to_datetime(loans_df.issue_d) $ loans_df.issue_d.value_counts()
p_new_sample = new_page_converted.mean() $ p_old_sample = old_page_converted.mean() $ p_new_sample - p_old_sample
honeypot_df['honeypot_flag'] = 1
df_parsed.head(3)
new_conv_rate = df2.query('group == "treatment"')['converted'].mean() $ new_conv_rate
dep.get_ages(exclude='ing-diba')
affordability = SCC_med_inc.merge(SCC_MSP_year, left_on = 'Year', right_on='Year') $ affordability
sys.path
new_df= preprocessing.StandardScaler().fit(new_df).transform(new_df) $ new_df[0:5]
tfav_k1.plot(figsize=(16,4), label="Likes", legend=True) $ tret_k1.plot(figsize=(16,4), label="Retweets", legend=True);
import re $ re.sub(r'@[A-Za-z0-9]+','',df.text[343])
import seaborn as sns $ sns.set(style='whitegrid', color_codes=True)
score_100.shape[0] / score.shape[0]
group_sizes
unsegmented_users.groupby(['course_id', 'segment']).user_id.count()
[endpoint_deployments] = [x.get('entity').get('deployments').get('url') for x in json.loads(response_get.text).get('resources') if x.get('metadata').get('guid') == saved_model.uid] $ print endpoint_deployments
ca_de_xml.shape
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ z_score, p_value
df.nunique()['user_id']
pct.to_csv('pct_data_4_18.csv')
sc.textFile("file:/path/file.txt") \ $     .filter(lambda line: ".txt" in line) \ $     .count()
json_data = r.json() $ print(json_data)
MetaMetaclass.__call__(MetaClass,'Example', (), {}) $
df.set_index('created_at', inplace=True) $
address_df = address_df.drop(['organisation', 'address'],axis=1)
glm_multi_v1 = H2OGeneralizedLinearEstimator( $     model_id='glm_v1',            #allows us to easily locate this model in Flow $     family='multinomial', $     solver='L_BFGS')
twitter_archive.info()
btc_wallet.head()
import pickle
negatives = NB_results.loc[NB_results.sentiment == 'N'] $ sample_size = 10 $ for tweet in negatives.tweet.sample(sample_size): $     print(tweet)
tweets_clean.dog_type.value_counts()
wine_reviews.head().to_csv("wine_reviews.csv")
df_master.head()
plot
temp_series_freq_2H.plot(kind="bar") $ plt.show()
gmap.heatmap(data['Latitude'], data['Longitude'])
logit_mod1 = sm.Logit(df_new['converted'], df_new[['intercept','ab_page', 'CA','US']]) $ result1 = logit_mod1.fit() $ result1.summary()
df_place = pd.DataFrame(list_places) $ df_user = pd.DataFrame(list_users) $ df_coor = pd.DataFrame(list_coor)
countries_df.country.unique()
tt4Dict = ttTimeEntry.iloc[:,[0,1,2,3,7,6]]
plot_contingency(train_users, "language", "country_destination")
ephys_query = "SELECT * FROM ephys_roi_results LIMIT 5" $ lims_df = get_lims_dataframe(ephys_query) $ lims_df
list_of_values = [ $     '1 series bmw', $     'ford deals' $ ] $ df_filtered = df[df["Keyword"].isin(list_of_values)]
df=pd.read_csv('ab_data.csv') $ df.head()
autos["brand"].value_counts(normalize=True).head(20)*100
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin' $ S_1dRichards.executable = excutable +'/summa.exe'
data.loc[data.surface_covered_in_m2 < 1, 'surface_covered_in_m2'] = np.NaN
daterange
intersections.info()
countries_df = pd.read_csv('countries.csv') $ df_comb = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
fulldata_copy.head()
archive_clean = twitter_archive.copy() $ predictions_clean = images_predictions.copy() $ tweet_clean = tweet_json.copy()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new],[n_old, n_new], alternative='smaller') $ z_score, p_value
dft = df2.T $ dft
train_ = processed_tweets.loc[processed_tweets.set == 'train'] $ test_ = processed_tweets.loc[processed_tweets.set == 'test']
news_period_df
print 'There are %d unique names in the data set' % len(np.unique(df['Name'].values)) $ df['has_name'] = df['Name'].apply(lambda x: 0 if pd.isnull(x) else 1) $ del df['Name'] $
import pandas as pd $ import numpy as np
df.topicmax.value_counts()
test_pixelData = df_test.as_matrix() $ print ("test_pixelData.shape:", test_pixelData.shape, test_pixelData.shape[0]) $
f, ax = plt.subplots(figsize=(10, 10)) $ correlation_df = df_train.corr() $ sns.heatmap(correlation_df, mask=np.zeros_like(correlation_df, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True)) $ plt.show()
model.userFeatures().first()
deaths_per_year = defaultdict(int) $ for row in df_valid["Died"]: $     deaths_per_year[row.year] += 1 $ deaths_sorted = OrderedDict(sorted(deaths_per_year.items()))
btime3 = ph.phimage(dirname) $
hs_path = '/media/sf_pysumma/a0105d479c334764ba84633c5b9c1c01/a0105d479c334764ba84633c5b9c1c01/data/contents'
s.iloc[1]
autos["date_crawled"].str[:10].value_counts(normalize=True,dropna=False).sort_index(ascending=True)
len(df2[treatment_group]) / len(df2)
session = tf.Session() $ session.run(tf.global_variables_initializer())
new_page_converted = np.random.binomial(n_new, p_new) $ print(new_page_converted)
pd.Timestamp('9/3/2016')-pd.Timestamp('9/1/2016')
counts = tweets[['hour', 'text']].groupby('hour').agg('count') $ counts_sorted = counts.sort_values(by='text') $ counts_sorted.transpose()
len(input_data[:256])
print (country_dict["Germany"] * 100)
df_new.head(3)
inoroffseason_teams = ALLbyseasons.groupby(["InorOff", "Team"]) # Groups by In or Offseason, then by team
malenew = malebyphase.rename(columns={"Offense":"Male"}) $ malenew.head(3)
van_final['revtime'] = pd.to_datetime(van_final['revtime'])
job_requirements.columns
trips_data['bike_id'].hist(bins=20) $ plt.show()
sydney_friends=sydney.groupby('userTimezone')[['userFriendsCt','userFollowerCt']].mean() $ sydney_friends=sydney.sort_values(['userFriendsCt'],ascending=False) $ sydney_friends.head(10) $ len(sydney_friends)
!pip3 install xlwt
from sklearn.datasets import make_classification $ from sklearn.model_selection import train_test_split $ X, y = make_classification(n_samples=500, n_features=20, n_classes=2, $                            n_informative=5, n_redundant=0, n_repeated=0, $                            random_state=None, shuffle=True) $
sns.pairplot(df, x_vars=['Bottles Sold','Volume Sold (Liters)'], y_vars='Sale (Dollars)', size=7, aspect=0.7, kind='reg')
df3[df3['group']=='treatment'].head()
paired_df.head(2)
retweet_lst_names
hour = noise_data["Created Date"].dt.floor("60min").to_frame("Hour of Day") $ hour.head()
f_ip_os_minute_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_os_minute_clicks"), header=True) $ print('Found %d observations.' %f_ip_os_minute_clicks.count())
selected_leases = proxy.GetLeases( $     auth, $     {'>t_from' : ifrom, '<t_from' : iuntil} $ ) $ print(f"there have been {len(selected_leases)} reservations made during the period")
health_data_row['HR']  # that's a Series!
GSW_2017["Tm.3PM"].mean()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret, callback_url)
prediction_clean.head(3)
twitter_archive_clean[twitter_archive_clean.tweet_id==883482846933004288]
df.dropna(axis="columns")
data[data.close==0].sort_values(['order','lastAttacked'],ascending=[0,1])
pax_raw.paxstep.describe()
a=contractor_clean.groupby(['contractor_id','contractor_bus_name'])['contractor_bus_name'].nunique() $ print a[a >1] $ contractor_clean[contractor_clean.contractor_number.duplicated() == True]
feature1 = X_train[:,0] $ feature2 = X_train[:,1] $ feature3 = X_train[:,2]
xmlData['county'].value_counts()
import pywikibot as pb $ from pywikibot import pagegenerators $ from datetime import datetime
eng = dmh.SQLTable.create_engine('sqlite:///iris.db')
df=users
raw_train_ids = indexes_processing(train_ids) $ raw_test_ids = indexes_processing(test_ids)
type(futures_data)
bitc = bitc[(bitc.w_sentiment_score != 0.000)] $ bitc = bitc.reset_index(drop=True) $ print(bitc)
cityID = '5d231ed8656fcf5a' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         St_Petersburg.append(tweet) 
stn= session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all() $ dfA_stn = pd.DataFrame(A_stn, columns=['Station','TOBS']) $ dfA_stn.head(10) $
df_mes = df_mes[df_mes['travel_time'] > 0] $ df_mes.shape[0]
p_old = prob_convert $ p_old
twitter_ar.info()
df_goog['year'] = df_goog.index.year $ df_goog['month'] = df_goog.index.month $ df_goog['weekday'] = df_goog.index.strftime('%A') $ df_goog.head()
from datetime import datetime $ import pyspark.sql.functions as functions
amount.describe()
num_users = df.nunique()['user_id'] $ print("The number of unique users in the dataset is : {}".format(num_users))
df3 = df2.set_index('user_id').join(df_new.set_index('user_id'))
df
float(1)
np.random.randn(5, 5)
frame = pd.DataFrame(np.arange(8).reshape((2, 4)), index=['three', 'one'], columns=list('dabc'))
df.query('group == "control"')['converted'].mean()
X_new = pd.DataFrame({'TV': [40]}) $ X_new.head()
precipitation_measurement_df.describe()
df['Date'] = pd.to_datetime(df['Date']) $ df.set_index('Date', inplace=True)
tips_sentiment =  tips_analysisDF[['business_id', 'Positve Percentage', 'Negative Percentage', 'Business_Focus']]
journalists_retweet_summary_df[['retweet_count']].describe()
data_archie.info(verbose=True)
client.training.get_details('training-WWmHAB5ig')
rspmhigh = aqmdata[aqmdata['rspm'] > 55] $ 'Ahmedabad' in rspmhigh
print(nd1[0:3][0:3])
a = re.match('AA', 'AAbc')
actual_diff = np.array(p_diffs) $ proportion = actual_diff > observed_diff $ sum(proportion)/len(p_diffs)
from nltk.stem.porter import PorterStemmer $ stemmed = [PorterStemmer().stem(w) for w in words] $ print(stemmed)
convert = df[df['converted']==1].converted.count() $ convert $
from pytrends.request import TrendReq $
result3.head(20)
the_list
diff = [] $ for ele in r.json()['dataset']['data']: $     change = ele[2]-ele[3] $     diff.append(change) $ print('The largest change in any one day based on High and Low price: ' + str( max(diff)))
products['key']=0 $ users['key']=0 $ userproduct=pd.merge(users,products,on='key')[['ProductID','UserID']] $ users.drop('key',1,inplace=True) $ userproduct
df2['c2_bin'] = df['c2_date'].map(lambda x: 1 if pd.isnull(x) == False else 0)
new_page_converted = np.random.binomial(1, p_equal_old_new, length_of_new) $ new_page_converted.mean()
print(dir(tree))
accuracy_score(y_test,y_pred) 
ml.drop(columns='7dayforecast',inplace=True)
numeric_cols = ['price','security_deposit','cleaning_fee','extra_people'] $ filtered_df[numeric_cols].head(5)
date_df = pd.DataFrame({'Date': [datetime.datetime(2017, 3, 1), datetime.datetime(2017, 3, 2)]})
stat_info_st = stat_info[0].apply(fix_space) $ print(stat_info_st)
dates = [datetime(2014,8,1),datetime(2014,8,2)] $ ts = pd.Series(np.random.randn(2),dates) $ ts
users = df.groupby('user_id').sum() $ num_unique_users = len(users) $ num_converted_users = users.converted.sum() $ num_converted_users / num_unique_users
iterables = [eia_total_monthly.index.levels[0], range(2001, 2017), range(1, 13)] $ index = pd.MultiIndex.from_product(iterables=iterables, names=['type', 'year', 'month']) $ eia_extra = pd.DataFrame(index=index, columns=['total fuel (mmbtu)', 'generation (MWh)', $                                                'elec fuel (mmbtu)'])
clients = pd.read_csv('data/clients.csv', index_col=0) $ clients
files = ['colors', 'inventories', 'inventory_parts', 'inventory_sets', 'part_categories', 'parts', 'sets', 'themes']
df.dollar_change_open = (df.open_price-df.offer_price) $ df.dollar_change_close = (df.first_day_close-df.offer_price)
df2['time_of_day'] = df2['as_date'].apply(label_time_of_day)
income_raw = data['income'] $ features_raw = data.drop('income', axis = 1) $ vs.distribution(data)
predictions = knn.predict(test[['property_type', 'lat', 'lon','surface_covered_in_m2','surface_total_in_m2','floor','rooms']])
cd ..
df_mes = df_mes[df_mes['tolls_amount']>=0] $ df_mes.shape[0]
df.head()
balance_df.shape
num_stations = session.query(func.count(hi_stations.STATION.distinct())).scalar() $ print("Number Of Stations: " + str(num_stations))
ind_train, ind_test, dep_train, dep_test = \ $     train_test_split(x, y, test_size=frac, random_state=23)
X = df2[['intercept', 'ab_page']]
continent = ['Asia', 'Africa'] $ searchfor = ['s', 'w'] $ drinks[drinks.country.str.contains('|'.join(searchfor), case=False) & $        drinks.continent.isin(continent) & $        (drinks.beer_servings > 100)]
len(churn0_index)
mu = ret_aapl.mean().AAPL $ sigma = ret_aapl.std().AAPL $ mu, sigma
print(testObj)
df.columns
git_log = git_log.sort_index(ascending=False)
del bacteria_data['month'] $ bacteria_data
old_page_converted = np.random.choice(2, size = n_old, p=(p_old,1-p_old)) $ old_page_converted
len(SCR_PLANS_df) + len(free_mo_churns)== len(USER_PLANS_df)
for col in ('building_id', 'manager_id', 'display_address'): $     X_train, X_test = designate_single_observations(X_train, X_test, col)
df.to_csv('timestamps.csv')
p_old = convert_rate $ p_old
df.dtypes
df_afc_champ2018['awayWinPercentage'] = df_afc_champ2018['homeWinPercentage'].apply(lambda x: 1-x)
d = {'date': date_col, 'lang': lang, 'text':text} $ df = pd.DataFrame(data=d)
indexed_return(fundret.loc['2017-03':])
total_min_prc_mon_day = hawaii_measurement_df[["Percipitation"]] \ $ .groupby([ \ $     hawaii_measurement_df["Date"].dt.strftime('%m-%d') $ ]).min() $ total_min_prc_mon_day.head()
df.info()
X = tf.placeholder(tf.float32, shape=(x_data.size)) $ Y = tf.placeholder(tf.float32,shape=(y_data.size)) $ m = tf.Variable(3.0) $ c = tf.Variable(2.0) $ Ypred = tf.add(tf.multiply(X, m), c) $
daily_normals("08-03")
BroncosBillsTweets = BroncosBillsTweets.drop_duplicates(subset=['text'])
print(autos["brand"].value_counts().head(10))
dif_dict = {} $ for lis in answer1: $     dif = lis[2] - lis[3] $     dif_dict[lis[0]]=dif
linkpp = gpd.sjoin(pumashp,linkNYC).groupby('puma').count()[['date_link_']] $ linkpp.head()
df_tweets['id'] = list(map(lambda tweet: tweet['id'], tweets)) $ df_tweets['retweet_count'] = list(map(lambda tweet: tweet['retweet_count'], tweets)) $ df_tweets['favorite_count'] = list(map(lambda tweet: tweet['favorite_count'], tweets)) $ df_tweets['user_retweet'] = list(map(lambda tweet: tweet['user_retweet'], tweets)) $ df_tweets['user_favorites'] = list(map(lambda tweet: tweet['user_retweet'], tweets))
to_be_predicted_Day3 = 21.20401737 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
train_geo.shape
X.shape, y.shape
squares.items()  # Iterable
images.info()
df['speaker_party'].value_counts()
salary_df2 = pd.read_csv('../Data/playersalary.csv')
goog['2015']
walk.resample("1Min").first()
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.choice([0, 1], size = n_new, p = [p_new, 1 - p_new]).mean() $     old_page_converted = np.random.choice([0, 1], size = n_old, p = [p_old, 1 - p_old]).mean() $     p_diffs.append(new_page_converted - old_page_converted)
max(twitter_archive['tweet_id'].value_counts())
normalizedDf = finalDf $ normalizedDf = normalizedDf.drop('kmLabels', axis=1);
train.created.dtype
rate = api.rate_limit_status() $ rate_df = pd.io.json.json_normalize(rate).T $ rate_df = rate_df[rate_df.index.str.endswith("remaining")] $ rate_df = rate_df[rate_df.index.str.contains("search")] $ rate_df
for max_depth in range(1,10): $     dtree = tree.DecisionTreeClassifier(max_depth=max_depth) $     scores = cross_val_score(dtree, X_train, y_train, cv=5) $     average_cv_score = scores.mean() $     print("For depth=", max_depth, "average CV score = ", average_cv_score)  $
col=col[['ZIP CODE','LONGITUDE','LATITUDE','NUMBER OF PERSONS INJURED','date_time']] $ col.head()
nba_df.columns = ['Rk', 'Pk', 'Tm','Player','College', 'Yrs','G', 'MP', 'PTS','TRB','AST','FG%', $                     '3P%', 'FT%', 'MP', 'PTS', 'TRB', 'AST', 'WS', 'WS/48', 'BPM', 'VORP'] $ nba_df.head()
(df2.shape[0])
p_diff = (new_page_converted/N_new) - (old_page_converted/N_old) $ print("The p-difference for the simulated values is: {}".format(p_diff))
notes.isnull().sum()
goo = pd.read_csv('data/goog.csv', encoding='utf_8')
pst.parameter_data
master_df = archive_df.join(tweets_df) $ master_df = master_df.join(images_df)
no_of_rows_in_the_data_set = df.shape[0] $ print("no: of rows in the data set = {}".format(no_of_rows_in_the_data_set))
autos["price"].describe().apply(lambda x: format(x, 'f'))
results_station = session.query(Stations.station,Stations.name,Stations.latitude, Stations.longitude,Stations.elevation).all() $ results_station
print("Number of Malware in Mobile ATT&CK") $ print(len(all_mobile['malware'])) $ df = all_mobile['malware'] $ df = json_normalize(df) $ df.reindex(['matrix', 'software', 'software_labels', 'software_id', 'software_description'], axis=1)[0:5]
rf_sentiment = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1) $ scores = cross_val_score(rf_sentiment, X, y, cv=skf) $ print "Cross-validated RF scores based on sentiment:", scores $ print "Mean of scores:", scores.mean()
tweet_scores_clean=tweet_scores_clean[tweet_scores_clean.retweet_count!='failed']
mc_samples = 10000
sl[['status_binary','today_preds','one_year_preds','five_year_preds']].describe()
data_final.head()
train[train['is_rushhour'] == 1].head(3)
pd.timedelta_range(0, periods=10, freq='H')
df['binary_ruling'].value_counts()
file_path = '/home/scidb_bio/kg/ALL.chr22.phase3_shapeit2_mvncall_integrated_v4.20130502.genotypes.vcf.gz' $ fifo_path = '/tmp/load.fifo' $ num_presample_attributes = 9 #CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO, FORMAT $ num_samples = 2504 $ num_attributes = num_presample_attributes + num_samples
print(result.inserted_primary_key)
ocsvm_stemmed_tfidf.fit(trump_stemmed_tfidf, y = y_true_stemmed_tfidf) $ prediction_stemmed_tfidf = ocsvm_stemmed_tfidf.predict(test_stemmed_tfidf) $ prediction_stemmed_tfidf
df_twitter_archive_copy['source'] = df_twitter_archive_copy.apply(extract_source, axis=1) $ df_twitter_archive_copy['source'] = df_twitter_archive_copy.source.astype('category')
prob_convert = df2[df2["converted"] == 1].count() / (df2[df2["converted"] == 0].count() + df2[df2["converted"] == 1].count()) $ prob_convert = prob_convert[0] $ prob_convert
national_holidays=actual_holidays[actual_holidays.locale=='National'] $ print("Rows and columns:",national_holidays.shape) $ pd.DataFrame.head(national_holidays)
room_temp.temperature = 5
df_train.median(axis=0)
df = pd.DataFrame({'A': list('x' * 5) + list('y' * 5), $                    'B': list('abcde' * 2), $                    'C': np.random.randint(0, 100, 10)}) $ df.set_index(['A', 'B'], inplace=True) $ df
for result in results: $     print result.text
df_vow.head()
house_data.columns
path = os.path.join('Data for plots', 'Monthly generation.csv') $ eia_gen_monthly.to_csv(path, index=False)
model_preds.sort_values('prob_off').head(100)
duplicated
df2 = df1.join(df1_stdev, how='inner')
data_get['SCA'].Close.plot()
from nltk.corpus import stopwords $ from sklearn.feature_extraction.text import CountVectorizer
countries = pd.read_csv('./countries.csv') $ countries.head()
df[df['price'] > 1e6].head()
logit_control_3 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page','CA_int_ab_page', 'US_int_ab_page']]) $ result_control_3 = logit_control_3.fit() $ result_control_3.summary()
test = Plot['mLayerVolFracWat'].data $ dates = Plot['time'].data $ test = np.squeeze(test) $ df = pd.DataFrame(data = test, index=dates) $ df.replace(to_replace=-9999.0, value = 0, inplace=True)
commits = commits[commits['timestamp'] <= 'today'] $ initial_commit_date = commits[-1:]['timestamp'].values[0] $ commits = commits[commits['timestamp'] >= initial_commit_date] $ commits.head()
df2[df2.duplicated(['user_id'], keep=False)]['user_id'] $ df2[df2.duplicated(['user_id'], keep=False)]
geo_db = rent_db3[['latitude','longitude', 'interest_level']].copy()
oto = data[findings] $ oto
precipitation_measurement_df.plot() $ plt.title("12 Months of Precipitation") $ plt.ylabel("Precipitation") $ plt.show()
sorted_m3 =m3.ravel() $ sorted_m3[::-1].sort() $ sorted_m3=sorted_m3.reshape(2,2) $ print("sorted m3: ", sorted_m3)
show_crosstab('split_llpg1')
ridge2 = linear_model.Ridge(alpha=317) $ ridge2.fit(X_17, y2) $ (ridge2.coef_, ridge2.intercept_)
pnew =  df[df.landing_page == 'new_page']['converted'].mean() $ print("Probablity of conversion for new page = ",pnew) $ pold = df[df.landing_page == 'old_page']['converted'].mean() $ print("Probablity of conversion for old page = ",pold)
condo_6 = condos[condos.MAR_WARD == 'Ward 6']
p_diffs = [] $ pn = np.random.binomial(n=n_new, p=p_null, size=10000) / n_new $ po = np.random.binomial(n=n_old, p=p_null, size=10000) / n_old $ p_diffs.append(pn - po)
df = df.drop(7)
df_subset.dtypes
null_values = np.random.normal(0, p_diffs.std(), 10000)
so[criteria].head()
sum(abs(removing_features_validate[:, 2] - clf.predict_proba(X_validate)[:, 1]))
result = pd.DataFrame(result)
AAPL.tail(3)
titanic3 = pd.read_excel('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls') $ titanic3.head()
def answer_five(): $     np.mean(answer_one()['Energy Supply per Capita']).item() $ answer_five()
(autos['registration_year'] $  .between(0,1900) $  .value_counts() $  .sort_index() $ )
tweet_archive_clean.name.value_counts().head()
df1 = df.loc[:, ('Close')].reset_index().rename(index=str, columns={"Date": "ds", 'Close': 'y'})
grouped_dpt.tail(1)
model = ARIMA(AAPL_array, (2,2,1)).fit() $
df2['intercept'] = 1 $ df2[['control', 'ab_page']] = pd.get_dummies(df2['group']) $ df2.drop('control', inplace=True, axis=1) $ df2.head()
crimeData = pd.read_csv('/home/hoona/Python/mpug/exampleData.csv') $ crimeData.head(2)
dfData.dtypes
df['charge_count'] = charge_counts
os.listdir('../input')
new_c2 = float(df2.query('converted == 1 and  landing_page == "new_page"')['user_id'].nunique()) $ new_users2 =float(df2.query('landing_page == "new_page"')['user_id'].nunique()) $ print(" Given that an individual was in new landing page, the probability they converted is {0:.2%}".format(new_c2 /new_users2))
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth)
df2[df2.duplicated('user_id')]['user_id']
%%bash $ ls -a
non_feature_cols = ['team', 'date', 'home_game', 'game', 'round', 'venue', 'opponent', 'season'] $ afl_data = afl_data.rename(columns={col: 'f_' + col for col in afl_data if col not in non_feature_cols})
autos["price"] = autos["price"].str.replace("$","") $ autos["price"] = autos["price"].str.replace(",","") $ autos["price"] = autos["price"].astype(int) $ autos["price"].head(3)
autos.describe(include='all')
df2.drop_duplicates(subset=['user_id'], keep='first',inplace=True) $ df2.shape $
posts_groupby.mean().num_comments.plot(kind='barh', figsize=[8,8])
model_gb_14 = GradientBoostingClassifier(max_depth = 14, random_state=42) $ model_gb_14.fit(x_train,y_train) $ print("Train: ", model_gb_14.score(x_train,y_train)*100) $ print("Test: ", model_gb_14.score(x_test,y_test)*100) $ print("Difference between train and test: ", model_gb_14.score(x_train,y_train)*100-model_gb_14.score(x_test,y_test)*100)
(act_diff < p_diffs).mean()
import dill as dpickle $ with open('ppm_body.dpkl', 'rb') as file: $     ppm_body = dpickle.load(file) $ with open('ppm_title.dpkl', 'rb') as file: $     ppm_title = dpickle.load(file)
old_page_converted = np.random.choice([0,1],N_old, p=(p_old,1-p_old)) $ old_page_converted
testing[(testing.Open!= 0.0)&(testing.Open!= 1.0)]
eg.index.values
plt.rcParams['figure.figsize'] = (20.0, 10.0)
plot_hist_dict(signup_completed, feature='gender_str', top_k=10) 
control_gp=df2[df2['group']=='control'] $ control_gp['converted'].mean()
number_rows = df.shape[0] $ print("Number of rows: {}".format(number_rows)) $
df_timeseries = df_history[['symbol','date','open','high','low','close','marketvalue','quantity','movement_delta','peaks_delta','cryptdex_value','cryptdex_quantity','cryptdex_index']].copy()
p.to_timestamp('M', 's')
gender[gender == -1].index
josh_tweets = pd.DataFrame(Josh) $ josh_tweets
sphere = TextBlob("The Earth is a sphere") $ sphere.sentiment
session.query(Stations.station,Stations.name,Stations.latitude, Stations.longitude,Stations.elevation).first() $
archive_df[archive_df.in_reply_to_status_id.isnull() == False]
print(df_w_topics[['title', 'author gender', 0]].sort_values(by=[0], ascending=False))
obj = pd.Series(range(3), index=['a', 'b', 'c'])
reddit_comments_data = reddit_comments_data.withColumn('subjectivity',subj_udf(reddit_comments_data.body))
import numpy as np $ import pandas as pd $ autos = pd.read_csv("datasets/autos.csv",encoding="Latin-1")
df_merged.head()
chunk_size = 250 $ shift_size = 5 $ all_rebalance_weights = rebalance_portfolio(returns, index_weighted_returns, shift_size, chunk_size)
print X
print(sorted(list(pol_users.columns))) $ print() $ print(sorted(list(troll_users.columns)))
pd.crosstab(index=np.array(labels), columns=np.array(preds))
plt.hist(p_diffs); $ plt.ylabel('Frequency') $ plt.xlabel('p_diffs')
week1_df = courses[0] $ week2_df = courses[1] $ week1_df.append(week2_df)
df2_copy.shape == df2.shape
 new_page = np.random.binomial(1,p,df2.query('landing_page == "new_page"').shape[0])
pd.read_csv('myfile.csv', index_col=0)
appleNegs = companyNeg[companyNeg.author_id == 'AppleSupport']
fashion_db = 
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
print('No. of unique users: ', df['user_id'].nunique())
cvecdata1 =cvec.fit_transform(X_test)
df = pd.read_sql('select * from hundred_stocks_twoyears_daily_bar', conn_helloDB) $ df.tail()
to_be_predicted_Day4 = 52.4480597 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
for t in df[df.sentiment == df.sentiment.min()].text[:10]: $     print(t + '\n')
df.loc['2018-05-21','Open'] 
rsp = requests.get(api_url_base + ','.join(vid_list))
model.infer_vector(["system", "response"])
res=driver.find_elements_by_class_name('conference-item')
conn.autocommit = True $ c = conn.cursor()
grid_tfidf.best_params_
df = read_xml_file("data/safety/toronto_200k.xml") $ df.head()
intervention_train['CRE_DATE_GZL'].min(), intervention_train['CRE_DATE_GZL'].max()
interests = [] $ for i in vlc.topics: $     for j in i: $         interests.append(j)
df1 = pd.read_csv('twitter-archive-enhanced.csv')
swPos = companyPos[companyPos['author_id'].str.contains('SouthwestAir') | $                        companyPos['text'].str.contains('SouthwestAir')]
p_new = len(df2.query("converted=='1'"))/ len(df2) $ p_new
model = DecisionTreeClassifier() $ X = data[train_cols] $ y = data['Churn'] $ model.fit(X, y)
assert isinstance(sent, pd.DataFrame) $ assert sent.shape == (7517, 1) $ assert list(sent.index[5000:5005]) == ['paranoids', 'pardon', 'pardoned', 'pardoning', 'pardons'] $ assert np.allclose(sent['polarity'].head(), [-1.5, -0.4, -1.5, -0.4, -0.7])
user_table = events[['user_id', 'login']].drop_duplicates() $ user_table.to_sql(name='users', con=con, if_exists='append', index=False)
accuracy_train = pipeline.predict(X_train) $ print("prediction output dataframe fields: " + str(accuracy_train.columns)) $ accuracy = np.mean(np.array(accuracy_train['PredictedLabel'].astype(int)) == y_train) $ print('training accuracy: ' + str(accuracy))
df["web_booking"] = df["booking_application"].isin(["web", "web-desktop", "web-mobile", "web-tablet"]).astype(int) $ df["iphone"] = df["booking_application"].isin(["iphone-appstore"]).astype(int) $ df["android"] = df["booking_application"].isin(["android"]).astype(int)
ddf = dd.read_csv('test-data/output/sample-xls-case-multifile1.xlsx-*.csv') $ ddf.compute().head() $
autos[~autos["registration_year"].between(1900,2016)]
m = model_attention_nmt(len(human_vocab), len(machine_vocab)) $ m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) $ print(m.summary())
df.plot(kind='bar', stacked=True, figsize=(12, 5)) $ plt.savefig('stackedBarcharts.jpeg')
s.dt.daysinmonth
df['domain_d'].nunique()
predictions.select('label', 'prediction').coalesce(1).write.csv('D://Data Science//pySpark//check_pred6.csv')
us_grid = np.array(np.meshgrid(grid_lon, grid_lat)).reshape(2, -1).T $ np.shape(us_grid)
(p_diffs > p_diff).mean()
sel_df.Beat.unique()
n_old = df2[df2.landing_page == 'old_page'].shape[0] $ n_old
plt.plot(glons, glats, marker='.', color='k', linestyle='none') $ plt.show()
type(state_DataFrames['OH_R'])
y_test_under[rfc_bet_under].sum()
brand_dict = {} $ for brand in top_5_percent.index: $     price_mean = autos_pr[autos_pr['brand'] == brand]['price'].mean() $     brand_dict[brand] = price_mean $ print(brand_dict)
related_docs_indices
engine = create_engine("sqlite:///Resources/hawaii.sqlite")
df = pd.DataFrame(list('abcd'), index = [pd.Period('2017-01'), pd.Period('2017-02'), $                                          pd.Period('2017-03'), pd.Period('2017-05')]) $ df
X_train = pd.read_json("../Data//train.json").sort_values(by="listing_id") $ X_test = pd.read_json("../Data//test.json").sort_values(by="listing_id")
dule2.drop(columns=['ElectronicCollection'],inplace=True) $ dule2
R_trip.head()
class display(object): $
len(train)
tm['energy'].mean() $ tm['energy'].var() $ tm['friendliness'].mean() $ tm['friendliness'].var()
twitter_archive_clean[twitter_archive_clean['tweet_id'] == 854010172552949760]
bacteria2.mean(skipna=False)
df_clean['rating_numerator'].value_counts()
df2[df2.duplicated(['user_id'], keep=False)]
pd.read_sql_query("SELECT * FROM person, grades WHERE person.id = grades.person_id" , conn)
data_matrix = num_data.as_matrix()
deck_data[['created_at_dt_date', 'username']].head()
stocks.tail()
from lifetimes import BetaGeoFitter
res = res.json() $ res
import psycopg2 $ import pandas as pd $ import numpy as np $ from apyori import apriori
my_query = "SELECT * FROM specimens LIMIT 10" $ my_result = limsquery(my_query) $ first_element = my_result[0] $ print first_element $
properati[(pd.isnull(properati['place_name']))]
html = browser.html $ soup = bs(html, 'html.parser')
app_install = df_nona.groupby('create_date').apply(lambda v: sum(v.accounts_provisioned)/float(sum(v.district_size))) $ app_install.resample('w', np.mean).plot(title='Install Rate by Adopted Time') $
prop_conv = df.converted.mean() $ prop_conv
price = pd.Series(price_by_brand) $ odometer = pd.Series(odometer_by_brand)
df2.iloc[2862]
new_page_converted = np.random.binomial(n_new,p_new) $ print('The new_page_converted is: {}.'.format(new_page_converted))
precip_data_df.set_index("date",drop=True,inplace=True) $ precip_data_df.columns $ precip_data_df.tail()
bitcoin_df.head()
list_of_hospitals = list(set(list_of_hospitals))
m3.fit(lrs, 7, metrics=[accuracy], cycle_len=2, cycle_save_name='imdb2')
import pandas as pd $ cust_df = pd.read_csv("Cust_Segmentation.csv") $ cust_df.head()
series1.corr(series2, method='pearson')
new_page_converted = np.random.choice([0, 1], size = n_new, p = [1 - p_new_null, p_new_null])
pivoted.shape
fb.uuid
weather_data3 = pd.read_csv('201508_weather_data.csv'); weather_data3.head()
from sklearn.metrics import precision_recall_fscore_support as prf $ from sklearn.metrics import accuracy_score as acc
mean_sea_level["mean_global"] = global_sea_level["msl_ib_ns(mm)"] $ mean_sea_level
s2.loc["bob"]
df = pd.read_csv(reviews_file_name, encoding = "ISO-8859-1") $ df.head(n = 10)
data.columns
p_value
months
senateAll[["last_name", "full_name", "filerName", "filerIdent"]][:5]
df_tot.replace('-', np.nan, inplace=True)
results_m.summary()
kickstarter_failed_successful['difference'] = kickstarter_failed_successful['usd_pledged_real'] - \ $                                                  kickstarter_failed_successful['usd_goal_real'] $ kickstarter_failed_successful[['difference']]
(~autos["registration_year"].between(1910,2016)).sum() / autos.shape[0]
pwd
print(any([False, False])) $ print(any([False, True]))
survey.columns 
X = Train.drop('Approved', axis=1) $ y = Train['Approved']
breaches.groupby('IsVerified')
ab_file2[((ab_file2['group'] == 'treatment') == (ab_file2['landing_page'] == 'new_page')) == False].shape[0]
verbose_mannwhitneyu( "treatment", users.treatment["revs"], "control", users.control["revs"] )
prec_us = prec_nc.variables['pr_wtr'][1, lat_li:lat_ui, lon_li:lon_ui] $ np.shape(prec_us)
terror[]
%timeit pd.read_sql(f'explain {sql}', engine).head()
df.groupby(['userid', 'website'])['price'].sum()
data['strategy'] = data['position'].shift(1) * data['returns'] $ data[['returns', 'strategy']].sum()
selected_features=selected.index $ X_train_new=X_train[selected_features] $ X_test_new=X_test[selected_features]
len(delte)
df2[df2['group'] == "treatment"]['converted'].mean()
doc_top_mat['MaxTopicWt'] = doc_top_mat.iloc[:, :15].max(axis=1) $ doc_top_mat['PrimaryTopic'] = [topic_list[x] for x in doc_top_mat['TopicNum']]
vec_code = modal_model.predict(encoder_model.predict(source_proc.transform(source_docs))) $ vec_code.shape
plot_votes_by_genre() $ plt.show()
crimes_df = pd.read_sql('select * from crimes limit 10;', conn)
autos['brand'].value_counts(normalize = True)
combined_df.shape
df_final[df_final['rating_denominator']<10]
np.exp(final_results.params)
facebook_urls = unique_urls[(unique_urls.domain == domain)] $ facebook_urls.sort_values('num_authors', ascending=False)[0:50][['url', 'num_authors']]
lr.predict(X)
ts.tshift(3)
plt.figure() $ evol = pd.Series([s[0] for s in steps]) $ evol.plot()
df_exp
ibm_hr_target_small
df['duration'] = np.round((df['deadline'] - df['launched']).dt.days / 7)
base3 = df3[['placeId', 'hashtags']].groupby('placeId').aggregate(lambda x: [i for l in x for i in l ])
dfFull.GarageArea = dfFull.GarageArea.fillna(dfFull.GarageArea.mean())
hp.listdate = hp.listdate.astype('datetime64[ns]', inplace=True)
crimes.DATE_OF_OCCURRENCE = pd.to_datetime(crimes.DATE_OF_OCCURRENCE)
import numpy as np $ import pandas as pd $ import geoip2.database $ import ipaddress $ import os
user_lookup_df.head()
df_stars_count = df_stars.groupby('stars').size() $ print(df_stars_count)
autos['offer_type'].value_counts()
cityID = '9a974dfc8efb32a0' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Kansas_City.append(tweet) 
countries_df.head()
average_chart_upper_control_limit = average_of_averages + 3 * d_three * average_range / \ $                                     (d_two * math.sqrt(subgroup_size))
nu_fission_rates = fuel_rxn_rates.get_slice(scores=['nu-fission']) $ nu_fission_rates.get_pandas_dataframe()
df = df.merge(shifts, on='atbat_pk', suffixes=('_old', ''))
stock.head()
vehicle_collision = df[df['CATEGORY'] == 'Vehicle Collision'] $ crimes = df[df['CATEGORY'] != 'Vehicle Collision']
print('Scanning for all greetings:') $ for key, row in table.scan(): $     print('\t{}: {}'.format(key, row[column_name.encode('utf-8')])) $
contractor_clean=contractor_clean.drop(['fax', 'email','ignore','address3'], axis=1)
crime_df.info()
mrow=[] $ mrow.append('id,name,city,country,latitude,longitude,zip_code,price_range,rest_rating,rating_text,votes,image\n') $ for i in range(len(datazom)): $     mrow.append(str(datazom[i]['restaurant']['id'])+','+str(datazom[i]['restaurant']['name'].replace(',',''))+','+str(datazom[i]['restaurant']['location']['city'])+','+str(datazom[i]['restaurant']['location']['country_name'])+','+str(datazom[i]['restaurant']['location']['latitude'])+','+str(datazom[i]['restaurant']['location']['longitude'])+','+str(datazom[i]['restaurant']['location']['zipcode'])+','+str(datazom[i]['restaurant']['price_range'])+','+str(datazom[i]['restaurant']['user_rating']['aggregate_rating'])+','+str(datazom[i]['restaurant']['user_rating']['rating_text'])+','+str(datazom[i]['restaurant']['user_rating']['votes'])+','+str(datazom[i]['restaurant']['thumb'])+'\n') $
df2 = df_ab_raw[(df_ab_raw['line_up'] ==0)]
df.head() $
stopword = nltk.corpus.stopwords.words('english')
get_upload_playlist_id(channel_id)
data = data[data['Borough'] != 'Unspecified']
print(ndarray_original)
data = data.dropna(how = 'any')
df.iloc[1]
autos.columns = colNames
waterlevels.to_sql(con=engine, name='waterlevels', if_exists='replace', flavor='mysql',index=False)
twitter_df.rating_numerator.nunique()
n_old=len(df2[df2['landing_page']=='old_page']) $ n_old
tweet_clean.rename({'id': 'tweet_id'}, axis=1, inplace=True)
merged_df.head()
parks_info.head(2)
flood_reports.head()
print(df.shape) $ print("The number of rows in the dataset is: {}".format(df.shape[0]))
fdist_top = fdist.most_common(20) $ fdist_top
print("Number of rows: {}".format(len(df)))
tweets_kyoto = tweets_kyoto[np.isfinite(tweets_kyoto['ex_lat'])]
autos['price'].describe()
it_df.columns
apple = pd.read_csv('appl_1980_2014.csv') $ apple.head()
goog.Open.plot() $ goog.Open.resample('M').plot()
for v in list(seen)[:10]: $     if v != 'nan': $         print v, type(v)
df2 = df2.drop(df2.index[2893]) $ df2.info()
sql = "SELECT * FROM paudm.production as prod where prod.pipeline='memba' order by - prod.id  limit 5" $ df2 = pd.read_sql(sql,engine)
tweet_archive_clean = tweet_archive_clean[['tweet_id', 'timestamp', $        'text', 'rating_numerator', 'rating_denominator', $                                            'name', 'doggo', 'floofer', 'pupper', 'puppo']]
with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f: $     gr1 = f.create_group(where='/', name='gr1') $     gr2 = f.create_group(where=f.root.gr1, name='gr2') $     f.create_array(where='/gr1/gr2',  name='some_array', obj=[0, 1, 2, 3]) $     f.create_hard_link(where='/', name='hard_link', target='/gr1/gr2/some_array')
df_selected.select('user_id','date','seq', 'numOfReviews').orderBy('user_id', 'seq').limit(40).toPandas()
df = scattering.get_pandas_dataframe() $ df.head(10)
df.drop( columns=df.columns[[3,4,5]] )   # delete columns by list of column number
df_more[df_more.Manager != 0]
questions.set_index('createdAt', inplace=True)
df.head()
worldbible.head()
def generate_weighted_returns(returns, weights): $     assert returns.index.equals(weights.index) $     assert returns.columns.equals(weights.columns) $     return None $ project_tests.test_generate_weighted_returns(generate_weighted_returns)
percipitation_year = session.query(Measurements.date,func.avg(Measurements.prcp)) \ $              .filter(Measurements.date >= '2016-05-01').filter(Measurements.date <= '2017-06-10') \ $              .group_by(Measurements.date).all()
clean_stations.drop(['name'], axis=1, inplace=True)
classes.fillna(0,inplace = True)
plt.savefig(str(output_folder)+'NB01_3_landscape_image02_'+str(cyclone_name)+'_'+str(location_name)+'_'+time_slice02_str) 
type(df['Created date'][0])
y_axis = output['retweeted_status.retweet_count'].astype(int).values.tolist()
adj_close_acq_date_modified = adj_close_acq_date[adj_close_acq_date['Date Delta']>=0] $ adj_close_acq_date_modified.head()
print (summarize(text, split=True))
df['edition'].value_counts()
X, y = get_training_data(doc2vec_model, classifications)
round((model_x.rsquared), 3)
df.loc[:,'name':'year1']
data[data['Body_Count']>200]
options_frame['ImpliedVolatilityMid'] = options_frame.apply(_get_implied_vol_mid, axis=1)
x.drop(range(2))
df = df.groupby(["C/A", "UNIT", "SCP", "STATION", "DATE"]).ENTRIES.first().reset_index() $ df.head() $
with open('Al.meam', 'w') as f: $
trunc_df.description = trunc_df.description.astype(str)
temp_series.plot(kind="bar") $ plt.grid(True) $ plt.show()
returns.MSFT.cov(returns.IBM)
len(train_data[train_data.yearOfRegistration > 2017])
autos.info() $ autos.head()
dat_missing_zip = datAll[pd.isnull(datAll['zip'])]
data_folder = '/Users/masa/Downloads/Udacity/data/'
reqs = pd.read_csv(r'311_service_requests.zip') $ reqs.head()
multiindex_output = table_to_compare.set_index(keys=['email', 'SKU']) $ multiindex_output
df_res = sqlContext.createDataFrame(aaa)
obs_diff_control = df2.query('group == "control"').query('converted == 1')['user_id'].count() / df2.query('group == "control"')['user_id'].count() $ obs_diff_control
sess = data_t[data_t['Session'] == data_t.ix[0, 'Session']] $ fg, ax = plt.subplots() $ sess.plot(y='X', ax=ax) $
df2[df2['landing_page']=='new_page']['converted'].mean()
columns = ['contributors', 'coordinates', 'geo', 'quoted_status', 'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink'] $ archive_copy.drop(columns, axis=1, inplace=True)
schema = StructType([ $     StructField("movieId", StringType()), $     StructField("title", StringType()), $     StructField("genres", StringType()), $ ])
p0_old
calls_df[["length_in_sec"]].plot(kind="density")
result[(result['age']<18)&(result['dt_deces'].notnull())].shape
ts
df_test['is_test'] = np.repeat(True, df_test.shape[0]) $ df_train['is_test'] = np.repeat(False, df_train.shape[0]) $ df_total = pd.concat([df_train, df_test]) $ df_total.info()
df.head()
tmp = combined[['city','state','lat','long']].drop_duplicates()
target_column = 'DGS30'
tweets_clean.timestamp.sample(10)
daily = data.set_index('created_at').resample('D').size() $ monthly_mean = daily.resample('M').mean() $ monthly_mean.index = monthly_mean.index.strftime('%Y/%m') $ iplot(monthly_mean.iplot(asFigure=True, dimensions=(750, 500), vline=['2017/09', '2017/03', '2016/09', '2016/03', '2015/09', '2014/12', '2014/04', '2013/10']))
cust_demo.head(3)
female_journalists_retweet_summary_df[['retweet_count']].describe()
cats_df.describe()
df_datecols = df_EMR_with_dummies.select_dtypes(include=['<M8[ns]']).drop(columns=['index_date', 'lookback_date']) $ df_datecols.shape
taxiData2.loc[taxiData2.Tip_amount < 0, "Tip_amount"] = 0
precipitation_df.dropna(inplace=True) $ precipitation_df.head()
output= " select tag_id, sum(retweets) as crt from tweet_details as td inner join tweet_tags as tt where td.tweet_id=tt.tweet_id group by tag_id order by crt desc limit 3; " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['tag_id','Sum'])
!ls
data['property_type'] = data['property_type'].apply(lambda x: property_type_to_num(x))
twitter_archive_full.info()
df.shape
!ls -lh data
def brandNumber(brand): $     return brand, len(data[data.brand==brand])
df4['intercept'] = 1 $ logistic_reg = sm.Logit(df4['converted'], df4[['CA', 'US']]) $ results = logistic_reg.fit() $ results.summary()
df.eval('D = (A - B) / C', inplace=True) $ df.head()
plt.hist(p_diffs) $ plt.axvline(x=o_diff, color='red');
tweet_text = df2['text']
pipe = pc.PipelineControl(data_path='examples/simple/data/varying_data.csv', $                           prediction_path='examples/simple/data/predictions.csv', $                           retraining_flag=True) $ pipe.runPipeline()
area_dict = {'Illinois': 149995, 'California': 423967, $              'Texas': 695662, 'Florida': 170312, $              'New York': 141297} $ area = pd.Series(area_dict) $ area
min_open = [] $ for entry in d["dataset_data"]["data"]: $     min_open.append(entry[1]) $ min_open = filter(None, min_open) $ print("The lowest opening price was $"+str(min(min_open)))
df.loc[mask,:]
merkmale.xs(99550,level='id').to_clipboard()
len(news_df)                                                                  # checks dataframe length    
example_time = tweet_archive_clean.timestamp[0] $ example_time
result_set=session.query(Adultdb).filter_by(relationship='Not-in-family').all()
obj
shows['stemmed_plot'] = shows['plot_cleaned'].dropna().apply(split_and_stem)
zipincome['ZIPCODE'] = zipincome['ZIPCODE'].astype(float)
year9 = driver.find_elements_by_class_name('yr-button')[8] $ year9.click()
TestData_ForLogistic.to_csv('test_logistic.csv')
df_weekly = df_mean.merge(df_count[['date', 'id']], suffixes=('_average','_count'), on='date').merge( $     df_max[['date', 'week', 'text', 'polarity', 'negative', 'retweets']], on='date', suffixes=('_average','_max'))
session.query(Stations.station,Stations.name).all()
import requests $ from collections import defaultdict
len(df_campaigns['Title']) == len(set(df_campaigns['Title']))
assert 'expanded_urls' not in twitter_archive_clean.columns
twitter[twitter.tweet_id.duplicated()] $
import numpy as np $ stock_change = stocks.apply(lambda x: np.log(x) - np.log(x.shift(1))) # shift moves dates back by 1. $ stock_change.head()
sum(clean_rates.name.str.contains(r'^[a-z]'))
leadConvDataGrouped = segmentData.groupby(['lead_source', 'opportunity_conversion']).opportunity_conversion.count() $ leadConvpct = leadConvDataGrouped.groupby(level=[0]).apply(lambda x: 100* x / float(x.sum())); 
df3[['CA', 'UK', 'US']] = pd.get_dummies(df3['country']) $ df3.head() $
users[users['friends_count'].isna()]
joined_hist = pd.crosstab(index=goodreads_users_df['joined'], columns="count") $ joined_hist['joined_freq'] = joined_hist['count'] * 100 / joined_hist.sum()['count'] $ joined_hist = joined_hist.sort_values(ascending=False, by='joined_freq') $ joined_hist.head(10)
ex=savedict['2017-11-15'] $ ex.head()
data.sample(5)
autos = autos.drop(index = impossible_year)
df['source'].value_counts()
extract_all.loc[extract_all.app_id_short=='5b155a8df1f17915'].shape
validation.analysis(observation_data, BallBerry_simulation)
Quantile_95_disc_times_pay.head(5)
top_10_authors = git_log['author'].value_counts().iloc[:10] $ print(top_10_authors)
timedog_df['tweetFavoriteCt'].max() $ timedog_df.index[timedog_df['tweetFavoriteCt'] == timedog_df['tweetFavoriteCt'].max()].tolist() $ timedog_df['userLocation'][6825] #this one is empty $ timedog_df['Hashtag'][6825]      #hashtag: dog
fare_urban = urban['fare'].sum() $ fare_suburban = suburban['fare'].sum() $ fare_rural = rural['fare'].sum() $ total_fare = [fare_urban, fare_suburban, fare_rural]
print (series_of_converted_ages.mean())/365
df_members.info()
lots.ix[-1]
dates = pd.date_range('2018-05-01', '2018-05-06') $ temps1 = Series([80, 82, 85, 90, 83, 87], index = dates) $ temps1
image_clean['p1'] = image_clean['p1'].str.lower() $ image_clean['p2'] = image_clean['p2'].str.lower() $ image_clean['p3'] = image_clean['p3'].str.lower()
base_model = ResNet50(weights='imagenet', include_top=False) $ x = base_model.output $ x = GlobalAveragePooling2D()(x) $ x = Dense(1024, activation='relu')(x) $ predictions = Dense(1, activation='sigmoid')(x)
_ = ok.grade('q07a') $ _ = ok.backup()
all_pre = lift.get_all_pre()
pd.Timestamp('2018-01-01') + Hour(3) + Minute(5)
print(kickstarters_2017.shape) $ print(kickstarters_2017.info()) $ print(kickstarters_2017.describe()) $ print(kickstarters_2017.nunique())
query4 = \ $
n_users = df2.user_id.nunique() $ n_users
actor = pd.read_sql_query('select * from actor where last_name like "%%GEN%%"', engine) $ actor.head()
temp_us_full = temp_nc.variables['air'][:, lat_li:lat_ui, lon_li:lon_ui]
df2 = df.drop(df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].index)
df3[df3['E'].isin(['t','f'])]
p_old = df2['converted'].mean() $ print ("convert rate for p_old under the null :{} ".format(round(p_old, 4)))
yarr, ynames = pd.factorize(cat_outcomes['outcome_subtype'])
(null_val < actual_diff).mean()
print(test_df.info())
pd.concat([test,future_forecast],axis=1).plot(figsize= (12,8)) $ plt.legend(('Market data','Prediction' ))
e_p_b_one.index = e_p_b_one.TimeCreate $ del e_p_b_one['TimeCreate'] $
import statsmodels $ import statsmodels.api as sm $ statsmodels.__version__
import statsmodels.api as sm $ convert_old = df2.query('group == "control"').converted.sum() $ convert_new = df2.query('group == "treatment" ').converted.sum() $ n_old = df2.query('group == "control"').shape[0] $ n_new = df2.query('group == "treatment" ').shape[0]
ax = data.resample('D').sum().rolling(365).sum().plot() $ ax.set_ylim(0,None);
theft.head()
train_df7 = train_df6.copy() $ train_df7['bath_categorical'] = ['bt<=1' if x<=1 else 'bt>1' for x in train_df7.bathrooms] $ del train_df7['bathrooms'] $ train_df7.head()
Counter(weather_df["weather_main"].values)
autos['last_seen'].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True)
if 'http' not in url.lower(): $     url='http://'+ url $ print url $ page = requests.get(url, timeout=5)
df_tot[['Tract', 'Poverty', 'Income', 'Unemployment', 'Percent_Male']] = \ $ df_tot[['Tract', 'Poverty', 'Income', 'Unemployment', 'Percent_Male']].apply(pd.to_numeric)
coin_data = quandl.get("BCHARTS/ITBITSGD") $
df_companies = pd.read_csv('data/companies.csv') $ df_acquisitions = pd.read_csv('data/acquisitions.csv') $ df_investments = pd.read_csv('data/investments.csv') $ df_rounds = pd.read_csv('data/rounds.csv')
json.loads(page.text)
from pprint import pprint # ...to get a more easily-readable view. $ pprint(example_tweets[0]._json)
print('Decission tree F1_score: ', f1_score(y_final, dt_predicted)) $ print('Decission tree JAccard: ', jaccard_similarity_score(y_final, dt_predicted))
dir = '/Users/Collier/Dropbox/Skills/\ $ Python/Projects/Real_Estate/htx_crime_scraping/'
field_details(df_sched)
Y_df = classify_df['Y'] $ Y_df.shape
freq_station['id']
df_r = df2.copy()
print (train["review"][0]) $ print ("\n", example1.get_text())
len(favorites)
c_df = new_df.dropna(how='all') $ c_df.size
firsts = live[live.birthord == 1].birthwgt_lb.dropna() $ others = live[live.birthord != 1].birthwgt_lb.dropna()
!head data/train_users_2.csv
for cat in dfm['cat_type'].unique(): $     dfm['goal_'+cat] = dfm['goal_log'] $     dfm.loc[dfm.cat_type != cat, 'goal_'+cat] = 0.0 $ dfm = dfm.drop('goal_log', axis=1)
to_be_predicted_Day1 = 48.57 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
station.shape
gDate_content_count = gDateProject_content.groupby(level=0) $ gDate_content_count = gDate_content_count.sum() $
html = browser.html $ soup = bs(html, 'html.parser')
all_rows = engine.execute('SELECT * FROM station LIMIT 10').fetchall() $ print(all_rows)
nltk_text = nltk.Text(tokens) $ nltk_text.collocations()
df.sort_values(by='hash_id')[['hash_id', 'cause_num']].head(15)
df['datetime'] = pd.to_datetime(df['datetime'],format=('%Y-%m-%d')) $ df.dtypes
big_df_avg.head()
house_data = data.loc[mask == 1]
df_new['weekday'] = pd.get_dummies(df_new['day'])['weekday'] $ df_new.head()
contribs.info()
conditions_counts.index
df = pd.DataFrame(np.random.rand(5,2)) $ df.index = [ 'row_' + str(i) for i in range(1, 6) ] $ df
executable_path = {'executable_path': '/usr/local/bin/chromedriver'} $ browser = Browser('chrome', **executable_path, headless=False) $ featured_image_url = 'https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA22374_hires.jpg' $ browser.visit(featured_image_url)
from scipy.stats import norm $ print("cdf: {}".format(norm.cdf(z_score))) $ print("ppf: {}".format(norm.ppf(1-(0.05/2)))) $
'this is {} number {}'.format( "string", "1")
rnd_reg = RandomForestRegressor(n_estimators=500, max_leaf_nodes=16, $                                 n_jobs=-1, oob_score=True)
df.columns  # Display the column names only
df_merged.nlargest(1, 'favorite_count')
facts_metrics.groupby('dimensions_item_id').sum()
kick_projects = df_kick[(df_kick['state'] == 'failed') | (df_kick['state'] == 'successful')] $ kick_projects['state'] = (kick_projects['state'] =='successful').astype(int)
tweet1.user.description
import re
en_translation_counts = en_es.groupby(by='en').size() $ en_translation_counts[en_translation_counts > 1].hist(bins=10)
raw_full_df[raw_full_df.building_id=='96274288c84ddd7d5c5d8e425ee75027'].head()
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)])
data = pd.read_csv('csvs/datosFiltrados.csv', low_memory=False)
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response_tweet_image = requests.get(url) $ response_tweet_image
consumer_key = apikeys.TWITTER_CONS_KEY $ consumer_secret = apikeys.TWITTER_CONS_SECRET $ access_token = apikeys.TWITTER_ACC_TOKE $ access_token_secret = apikeys.TWITTER_ACC_TOKE_SECRET
data['y'] = data['y'].apply(lambda y: 'yes' if y == True else 'no') $ model_data = pd.get_dummies(data)
plt.hist(reddit['Upvotes'], range=(20,1000)) $ plt.xlabel('Number of Upvotes',fontsize='large') $ plt.ylabel('Number of Reddit Posts',fontsize='large') $ plt.title('The distribution of upvotes has a similar shape to the comments distribution', fontsize='large') $ plt.show()
newdf = pd.merge(bc, ts, left_index=True, right_index=True)
columns = ['No-show', 'Scholarship', 'Hypertension', 'Diabetes', $            'Alcoholism', 'Handicap', 'SMS_received'] $ clean_appt_df[columns].groupby('No-show').mean()
pumashp.head()
import pandas as pd $ git_log = pd.read_csv("datasets/git_log.gz", sep='#', encoding='latin-1', header=None, names=['timestamp', 'author'], compression='gzip') $ git_log.head(5)
reputation_score(author_data.iloc[39]['reputation'])
escaped = formatted_sample_pq_saved.select( $     list(map(lambda col: html_escape_udf(col).alias(col), formatted_sample_pq_saved.columns))) $ formatted_sample_csv_saved = non_blocking_df_save_or_load_csv( $     escaped, $     "{0}/formatted_sample_csv_14".format(fs_prefix))
tweet_tokenizer.tokenize(tweet_hour.loc[1340,'tweet_text'])
mask1 = (df.group=='control') & (df.landing_page=='new_page') $ mask2 = (df.group=='treatment') & (df.landing_page=='old_page')
new_page_converted = np.random.choice([0, 1], size=n_new, p=[(1-p_new), p_new])
print 'Percentage of total amount for data with City but no state: {:.3f}'.format(100*sum(df[df.state == ''].amount)/sum(df.amount)) $ df[((df.state == '') & (df.city != ''))][['city','zipcode','amount']].sort_values('city', ascending=True).to_csv('out/0/City_No_State.csv')
feature_cols = ['premisetype', 'occurrenceyear', 'occurrencemonth', $                 'day_id', 'occurrencedayofyear', 'occurrencehour', $                 'Hood_ID', 'Neighbourhood', 'Lat', 'Long'] $ X = crimes[feature_cols] $ y = crimes.MCI
cats_df[cats_df['remove']==True]
dul_isbns = dul['ISBN RegEx'] $ dul_isbns.size
df2[df2.user_id == duplicate_userid]
details.head(10)
obj
total_prob_of_conv=df2['converted'].mean() $ print('The probability of an individual converting regardless of page is:  ' + str(total_prob_of_conv)) $
def internship(x): $     if 'Intern' in x: $         return 1 $     return 0 $ df_more['Intern'] = df_more['Title'].apply(internship)
def combine_names(row): $     if row.contributor_fullname.startswith("SEAN PARKER"): $         return "SEAN PARKER" $     return row.contributor_fullname
directory_name = 'network/source_input/' $ input_nodes_file        = directory_name + 'nodes.h5' $ input_models_file       = directory_name + 'node_types.csv' $ input_edge_types_file   = directory_name + 'edge_types.csv' $ input_edges_file        = directory_name + 'edges.h5'
inputMovies['rating']
df_prep8 = df_prep(df8) $ df_prep8_ = pd.DataFrame({'date':df_prep8.index, 'values':df_prep8.values}, index=pd.to_datetime(df_prep8.index))
df2['intercept'] = 1 $ df2[['control','treatment']] = pd.get_dummies(df['group']) $ df2['ab_page'] = df2['treatment'] $
week44 = week43.rename(columns={308:'308'}) $ stocks = stocks.rename(columns={'Week 43':'Week 44','301':'308'}) $ week44 = pd.merge(stocks,week44,on=['308','Tickers']) $ week44.drop_duplicates(subset='Link',inplace=True)
df_pivot_days = pd.pivot_table(df_convert, values='delta_to_convert', index=['user_id','nd_key_formatted'],columns=['course_key'], aggfunc=np.sum).fillna(0).reset_index()
X, y = merged_data.drop('overdue', axis=1), merged_data['overdue']
df.plot() $
print(attend_with.sum(axis=0).sum()) $ print() $ print(attend_with.sum(axis=0))
df2_conv = df2[df2['converted'] == 1].count() $ total = df2['converted'].count() $ prop2 = (df2_conv['converted'] / total) $ print(prop2)
f = np.linspace(0, np.pi, 100) $ f = np.sin(f) $ print("f: ", f)
csvData['yr_renovated'].value_counts()
log_CA = sm.Logit(new['converted'],new[['intercept','country_CA']]) $ r = log_CA.fit() $ r.summary()
tweets_kyoto_filter = tweets_kyoto[tweets_kyoto['ex_lat']<35.031461] $ tweets_kyoto_filter = tweets_kyoto_filter[tweets_kyoto_filter['ex_lat']>34.931461] $ tweets_kyoto_filter = tweets_kyoto_filter[tweets_kyoto_filter['ex_long']<135.785300] $ tweets_kyoto_filter = tweets_kyoto_filter[tweets_kyoto_filter['ex_long']>135.685300] $
train = pd.read_csv(train_file, header=0, sep='\t', na_values=[''], keep_default_na=False)
from pandas.tseries.holiday import * $ cal = USFederalHolidayCalendar() $ for d in cal.holidays(start='2014-01-01', end='2014-12-31'): $     print(d)
df_h1b_nyc[df_h1b_nyc.pw_1.isnull()].pw_1
my_gempro.uniprot_mapping_and_metadata(model_gene_source='ENSEMBLGENOME_ID') $ print('Missing UniProt mapping: ', my_gempro.missing_uniprot_mapping) $ my_gempro.df_uniprot_metadata.head()
try: $     data['a': 'b'] $ except KeyError as e: $     print(type(e)) $     print(e)
hpd = hpd[hpd['MajorCategory']=='HEAT/HOT WATER'] $ print len(hpd)
pca_full = PCA() $ pca_full.fit(crosstab) ## note: This takes 1:20 minutes to complete 20,000 records
a@a
precip_data_df.head(3) $
y = df['comments'] $ X = df[['title', 'age', 'subreddit']].copy(deep=True)
err = (actual - expected[np.newaxis,:,:]).reshape(-1) $ err.shape
test_data.loc[(test_data.gearbox == 'automatik') & (test_data.vehicleType.isnull()), 'vehicleType'] = 'limousine'
y = np.log(df['avgPrice']).values $ X = df.loc[:,['km','year','powerPS']].values
with open('../data/channel_topic.json', 'r') as f: $     topics = json.load(f)
import time $ from sklearn.model_selection import RandomizedSearchCV
properati = properati[properati['country_name'] == 'Argentina']
results.summary()
countrydata
! ls ./data/raw-news_tweets-original/dataset1/news
logit_countries2 = sm.Logit(df3['converted'], $                            df3[['ab_page', 'country_UK', 'country_US', 'intercept']]) $ result2 = logit_countries2.fit()
vals1.sum()
trans = trans.sort_values(['msno', 'transaction_date', 'membership_expire_date'])
df2.query("group == 'treatment'").converted.mean()
df2.query('user_id==773192')
indeed.head()
df_CLEAN1A = pd.read_csv(url_CLEAN1A,sep=',') $ df_CLEAN1B = pd.read_csv(url_CLEAN1B,sep=',') $ df_CLEAN1C = pd.read_csv(url_CLEAN1C,sep=',')
duplicated_user = df2[df2.duplicated(['user_id'], keep=False)]['user_id'] $ duplicated_user
latest_df.classification_id.nunique() == len(latest_df)
!sed -i -e 's/if self.max_leaf_nodes == "None":/if self.max_leaf_nodes == "None" or not self.max_leaf_nodes:/' \ $   /usr/local/lib/python3.5/dist-packages/autosklearn/pipeline/components/regression/gradient_boosting.py
yc_new2.rename(columns={'Tip_Amt':'tipPC'}, inplace=True) $ yc_new2.head()
p2_table = profits_table.groupby(['Country']).Profit.sum().reset_index() $ p2_result = p2_table.sort_values('Profit', ascending=False) $ p2_result.head()
actual_pold=(df2.query('group=="control"')['converted']==1).mean() $ actual_pold
secondary_temp_dat=dat[secondary_temp_columns].copy() $ secondary_temps_exist= not secondary_temp_dat.dropna(how='all').empty $
p_new=df2['converted'].mean() $ p_new
print(full_df.dtypes)
df_new['country'].value_counts()
over_thresh_dict = {} $ over_winnings_dict = {} $ for thresh in np.linspace(.5, .66, 16): $     over_thresh_dict[thresh] = sum([x[1] > thresh for x in pred_probas_over_fm]) $     over_winnings_dict[thresh] = np.mean(y_test_over[[x[1] > thresh for x in pred_probas_over_fm]]) $
heap = [h for h in heap if len(set([t.author_id for t in h.tweets if t.author_id in names])) == 1]
y_pred = rf_pred $ print('precision: {:.2f}\nrecall: {:.2f}\naccuracy: {:.2f}'.format(precision_score(y_test,y_pred), $                                                        recall_score(y_test,y_pred), $                                                        accuracy(y_test,y_pred)))
articles.min()
convert_rate = df2.converted.mean() $ convert_rate
plt.style.use('seaborn') $ location_ax = user_extract.location.value_counts()[1:49].plot(kind = "bar", figsize = (25,8)) $ location_ax.set_title("Most Popular Locations from WeRateDogs Tagged Users", fontsize = 24) $ location_ax.set_xlabel("") $ plt.savefig('plots/popular_locations.png', bbox_inches='tight')
pmol.df['atom_type'].value_counts().plot(kind='bar') $ plt.xlabel('atom type') $ plt.ylabel('count') $ plt.show()
lsl = [pd.read_csv(filename) for filename in glob.glob("data/ebola/sl_data/*.csv")] $ sl_data = pd.concat(lsl, join='outer')
sleep.groupby('group').mean()['extra']
pd.groupby(vlc, by=vlc.visited.dt.year).size()
np.exp(-0.0150)
df.set_index('Currency',inplace=True) $ print(df)
res = requests.get("https://steamcommunity.com/market/search?appid=570&key=36C87A2958CA864AF19E1CBA635CA32D&format=json&q=%D0%B0%D0%B2%D1%82%D0%BE%D0%B3%D1%80%D0%B0%D1%84")
    Note: Data for a stock is stored in file: data/<symbol>.csv $     for symbol in ['AAPL','IBM','HCP']: $         print ('The max close for %s is %.2f.' % (symbol, get_max_close(symbol))) $ if __name__ == '__main__': $     test_run()
shows
pivoted = Fremont.pivot_table('Total', index=Fremont.index.time, columns=Fremont.index.date) $ pivoted.plot(legend=False, alpha=0.01);
actual_diff = treatment_converting - control_converting $ print('Actual difference:', actual_diff)
oppose.head(1)
df_concat_2.hist(column="message_likes_rel",by = "page", bins=20)
target_user = "@MarsWxReport"
yt = YoutubeDataApi(key)
tweet_archive_clean = tweet_archive_clean.round({'rating_numerator': 2})
data_url3 = datasets3[0],datasets3[2] $ data_url3
print(autos.price.describe(percentiles=[0.10, 0.90])) #Find the 10th percentile and 90th percentile $ autos = autos[(autos["price"] > 500 ) & (autos["price"] < 14000 )] #Remove outliers using the percentile values
dfMeanFlow = df['MeanFlow_cfs']
type(customer_visitors.DateCol.dt.dayofweek)
feed=pd.read_csv("feed.csv")
size = int(num_lines / 1000)
young.select(young.name, young.age+1)
df2  = df $ df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
import xgboost as xgb $ model = xgb.XGBClassifier() $ x_train, y_train = train.loc[trainIndex, needful_features], train.loc[trainIndex, 'is_fake'] $ model.fit(x_train, y_train) $ prediction = model.predict_proba(test[needful_features])
%matplotlib inline
bucket_name = buckets[0] $ bucket_obj = cos.Bucket(bucket_name)
b.loc[1:7]
x = ['00' + str(i) for i in range(10)] $ x
tweets_df_clean.columns
import statsmodels.api as sm $ convert_old = df2.query('group == "control" and converted == 1').user_id.nunique()  #  we had this from part I Q4. $ convert_new = df2.query('group == "treatment" and converted == 1').user_id.nunique() # this one too. $ n_old = df2.query('group == "control"').user_id.nunique() $ n_new = df2.query('group == "treatment"').user_id.nunique()
print('LGBM score: {:.4f}'.format(scores))
col_names = list(zip(df_test.columns, df_train.columns)) $ for cn in col_names: $     assert cn[0] == cn[1]
import statsmodels.api as sm $ convert_old = 17489 $ convert_new = 17264 $ n_old = 145274 $ n_new = 145310
crime_df = pd.read_csv(crime_file) $ print "Crime data loaded." $ crime_df.head()
svc.score(X_tfidf_test, y_tfidf_test)
fin_coins_r.shape, fin_r_monthly.index.min(), r_top10_mat_na.index.min(), fundret.index.min()
for num, entity in enumerate(parsed_review.ents): $     print 'Entity {}:'.format(num + 1), entity, '-', entity.label_ $     print ''
le_data_all = wb.download(indicator="SP.DYN.LE00.IN",country=countries['iso2c'],start='1980',end='2012') $ le_data_all
PrintMostLikedTweet(data) $ print() # Add a newline $ PrintMostRetweetedTweet(data) $
df.eval('D = (A + B) / C', inplace= True) $ df.head()
f_app_hour_clicks.show(1)
def trip_start_date(x): $     return re.search(r'(\d{4})-(\d{2})-(\d{2})', x).group(0)
 35237/294478
df.head(6)
181412/(181412+457881)
plt.hist(p_diffs); $ plt.axvline((p_new-p_old), c='red');
df[['total_spend','month','day','year','cust_id']].groupby(['year','cust_id','month','day','total_spend']).agg('sum').reset_index() $
d = Table.read('/Users/khawkins/Desktop/AHW/LtaP/students.csv',format='csv') #load in data for students $ d.colnames #print columns $ enthu = np.array(d['On a scale of 1 to 5 stars, how excited are you about becoming a scientist?']) #grab student enthusasium $
autos["abtest"].value_counts()
d = df[df.msno == '++1Wu2wKBA60W9F9sMh15RXmh1wN1fjoVGzNqvw/Gro='] $ d
date_agg=pd.DataFrame(merged['date'].value_counts()) $ date_agg.columns=['frequency'] $ date_agg['date']=date_agg.index $ pd.DataFrame.head(date_agg)
(float(signedup_cstmrs)/float(tot_cstmers))*100
print("Number of Groups in Enterprise ATT&CK") $ groups = lift.get_all_enterprise_groups() $ print(len(groups)) $ df = json_normalize(groups) $ df.reindex(['matrix', 'group', 'group_aliases', 'group_id', 'group_description'], axis=1)[0:5]
pst.instruction_files
analyze_set['date']=pd.to_datetime(analyze_set['date']) $ analyze_set['year']=analyze_set['date'].dt.year $ analyze_set['year'].value_counts()
df2.groupby([str.lower, mapping]).mean()
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_partial_autocorrelation(RN_PA_duration, params=params, lags=30, alpha=0.05, \ $     title='Weekly RN/PA Hours Partial Autocorrelation')
df['loan_status']= df['loan_status'].apply(lambda x: 0 if (x == "PAIDOFF")  else 1) $ y = df['loan_status'].values $ y[0:5]
p = getpass.getpass() $ r = requests.get('https://api.github.com/user', auth=('rsouza', p)) $ r.status_code
df_birth.describe()
p = p.set_index(["frameworkProgramme"])
df_clean.rating_denominator = 10
data.loc[data.LRank == 'NR', ['LRank']] = np.nan
disag_hart = DataSet(disag_filename) $ disag_hart
import pprint $ pprint.pprint(posts.find_one())
churned_unordered.head()
my_query = "SELECT * FROM specimens LIMIT 1" $ my_result = limsquery(my_query) $ first_element = my_result[0] $ print sorted(first_element)
sample['text_clean'].iloc[0]
preds = gcv.best_estimator_.predict(fb_test.message)
print (train[train.age < 15].age.shape) $ print (train[train.age > 80].age.shape) $ print (test[test.age < 15].age.shape) $ print (test[test.age > 80].age.shape)
df_Q1_3 = df_Q123.set_index('Time')
vhd = pd.read_excel('input/Data.xlsm', sheet_name='52', usecols='A:AO', header=6, skipfooter=16)
gnb = GaussianNB()
recipes.ingredients.str.len().describe()
file  = 'ExactOnline_V32_2016.xaf'
ca_de = f_remove_extract_fields_ft(ca_de)
df_master.to_csv('twitter_archive_master.csv', encoding = 'utf-8',index= False)
print('The API status code is {}'.format(ng_stor.response)) $ print('The URL sent to the API is {}'.format(ng_stor.response.url)) $ print('The API header is {}'.format(ng_stor.response.headers))
p_new = (df2['converted'] == 1).mean() $ p_new $
N_old = df2.query("landing_page == 'old_page'")["user_id"].count() $ print("The dataset consists of {} old pages.".format(N_old))
loan_stats["earliest_cr_line"].head(rows=2)
log_reg_under.score(X_test, y_test_under)
api.rate_limit_status()
trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn]) $ val_lm = np.array([[stoi[o] for o in p] for p in tok_val])
pd.get_dummies(df.C,dummy_na=True)
np.sum(~( (inches <= 0.5) | (inches >= 1) ))
pred = pipeline.predict(ogXfinaltemptf) $ print classification_report(pred, ogy) $ print confusion_matrix(ogy, pred) $ print "cross val score accuracy :"+str(sum(cross_val_score(pipeline, ogXfinaltemptf, ogy))/3) $ print 'roc_auc score :'+str(sum(cross_val_score(pipeline, ogXfinaltemptf, ogy, scoring='roc_auc'))/3) $
nold = df2.query('group == "control"').shape[0] $ print(nold)
(active_mailing - inactive_mailing) / SD_mailing
autos["brand"].value_counts(normalize=True).plot(kind="bar", title="Brand", cmap="Blues_r") $ plt.axhline(0.05, c="green")
df3[['CA','UK', 'US']] = pd.get_dummies(df3['country']) $ df3 = df3.drop('CA', axis = 1) $ df3.head()
df_users.sum()
unique_users = df.user_id.nunique() $ print('The dataset includes {} unique users.'.format(unique_users))
google_stock
poverty=pd.read_csv('data/crime/final_poverty.csv')
select5features = ['gk_kicking', 'potential', 'marking', 'interceptions', 'standing_tackle'] $ select5features
graf_train=pd.concat([graf_train, train_embedding], axis=1)
import locale $ import sys $ def p(f): $     print('%s.%s():%s'%(f.__module__,f.__name__,f()))
bacteria_data
multi_col_lvl_df.query("Date == '2018-07-10' and Category == 'Wine'")
help(soup.find_all)
brand_counts = autos["brand"].value_counts(normalize=True) $ common_brands = brand_counts[brand_counts > .05].index $ print(common_brands)
df.reset_index(inplace=True)
raw_df.head(5)
naimp.get_isna_ttest('age', type_test='ks')
transactions.info()
df = pd.read_csv('ZILLOW-Z49445_ZRISFRR.csv',index_col=0) $ df.columns=['Price'] # Changing the name of the column. (Index is not treated as a column so in our df we have only 1 column) $ df.head()
multi_col_lvl_df.stack().dropna().reset_index().to_csv('index_removed_output.csv') $ read_df = pd.read_csv("data.csv", index_col=[0, 1, 2, 3, 4, 5], skipinitialspace=True, parse_dates=['Date']) $ read_df.unstack('Store').head(3)
reviews_sample.describe()
eclf1 = VotingClassifier(estimators=[('lr', alg), ('calC', alg7)], voting='soft') $ eclf1.fit(X_train, y_train) $ probs = eclf1.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
errors = pd.read_csv('errors.csv', encoding='utf-8') $ errors.head()
df_clean['rating_numerator'] = df.text.str.extract('(\d+)', expand=True) $ df_clean['rating_numerator'] = pd.to_numeric(df_clean['rating_numerator']) $ print (" The text: %s \n The new grade in rating_numerator: %.1f \n The old grade: %.1f" % (df_clean['text'].ix[2326], df_clean['rating_numerator'].ix[2326],df['rating_numerator'].ix[2326])) $ print (" The text: %s \n The new grade in rating_numerator: %.1f \n The old grade: %.1f" % (df_clean['text'].ix[1689], df_clean['rating_numerator'].ix[1689],df['rating_numerator'].ix[1689]))
autos.dtypes
df2.query("group == 'treatment'")["converted"].mean()
plt.hist(p_diffs, bins = 25) $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('Simulated p_diffs (x10000)');
abc.head()
from dateutil.relativedelta import relativedelta
plotdf.head()
df['statement_type'].value_counts()
image_predictions_clean = pd.concat([image_predictions_clean, dog_breeds.rename('dog_breed')], axis=1)
from sklearn.ensemble import RandomForestClassifier
df_arch_clean['source'].value_counts() $
hashtags.index
Aussie_df = toDataFrame(aussie_results) $ Aussie_df.head(5)
PTruePositive + PFalsePositive
Project = pd.read_csv('ProjectInventorySales_Daily.csv', encoding='ISO-8859-1') $ Project = Project.copy(deep=True)
type(df['release_dt'][0])
X.head(10)
open('test_data//open_close_test.txt').close()
actual_payments.loc[actual_payments.fk_loan==loan_test,['iso_date','actual_amount_month','in_arrears_since']]
df2[df2.user_id.duplicated()]
r.xs('AMD', level=1)[['share']].plot()
free_data.mean(),free_data.count()
def days_since_last_order(d): $     timedelta = pd.to_datetime('2017-10-17') - d.max() $     return timedelta.days
builder = contest_data.select(*(F.col(x).alias(x + '_cd') for x in contest_data.columns)) $ builder = contest_savm.join(builder, builder.sales_acct_id_cd == contest_savm.sales_acct_id, how = 'inner') # Exploding 68 times
to_be_predicted_Day4 = 26.69300296 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
y_hat = nb.predict(train_4)
train.show(3)
weather_warm['Station Name'].value_counts()
EPEXprices = dfEPEX[::2].transpose().stack() # take every second row $ EPEXprices.index = index # assign generated index to price data $ EPEXprices.head() # verify extracted price data
model = model_simple_nmt(len(human_vocab), len(machine_vocab), Tx) $ model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
test.columns
categorical = free_data.dtypes[free_data.dtypes == "object"].index $ free_data[categorical].describe()
run txt2pdf.py -o "MONTEFIORE MEDICAL CENTER  Sepsis.pdf"   "MONTEFIORE MEDICAL CENTER  Sepsis.txt"
df = pd.DataFrame.from_dict(dot311_list)
df_conv = df[df['converted'] == 1].count() $ total = df['converted'].count() $ prop = (df_conv['converted'] / total) * 100 $ print(prop)
pd.Timestamp("2018-04-23")
allPeople = read.getPeople() $ pd.DataFrame.from_records([vars(person) for person in allPeople]).head()
df['intercept'] = 1 $ df['ab_page'] = 0 $ df['ab_page'] = df.apply(lambda row: get_ab_page(row), axis = 1) $ df.head()
learner.clip = 0.3
stocks.groupby(['ticker', 'dow']).mean()
Results_kNN1000 = pd.DataFrame({'Approved': prob1_kNN1000, 'ID': Test.index}) $ Results_kNN1000.head()
type(t2.index)
users_visits.visits = 1 * (users_visits.visits > 0) $ users_visits = users_visits.assign(regs=1) $ users_visits.head()
trip_prec_df = pd.DataFrame(sq.history_rainfall_trip(), columns=["Station", "Total prec"]) $ trip_prec_df
vader_df = pd.DataFrame(vader_scores)[['text', 'created_at','compound', 'neg', 'neu', 'pos']] $ vader_df = vader_df.sort_values('compound', ascending=True) $ vader_df.head(7)
tweets_clean.rating_numerator.mean()
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page'")['converted'].sum() $ convert_new = df2.query("landing_page == 'new_page'")['converted'].sum() $ n_old = df2.query("landing_page == 'old_page'").shape[0] $ n_new = df2.query("landing_page == 'new_page'").shape[0]
bronx = facilities[facilities['bbl']=='2027770417'] $ bronx
import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
ab_file2.info()
data_activ['n_date'] = data_activ['created_date'].apply(lambda x:datetime.strptime(x,'%Y-%m-%d %H:%M:%S')if not pd.isnull(x) else '') $ data_activ['new_date']= data_activ['n_date'].apply(lambda x: x.strftime('%Y/%B')if not pd.isnull(x) else '') $ data_activ['new_date_daily'] = data_activ['n_date'].apply(lambda x:x.strftime('%Y/%B/%d')if not pd.isnull(x) else '')
all_data_merge.shape
df_t[df_t['Shipping Method name']==271]['Updated Shipped diff'].hist() $ pd.DataFrame(df_t[df_t['Shipping Method name']==271]['Updated Shipped diff'].describe())
flattened_pandas_df = flatten_df(pandas_df)
sales.head()
df.shape
max_tweets=1 $ for tweet in tweepy.Cursor(api.search,q="vegan").items(max_tweets): $     print(tweet)
plt.scatter(X,y2) $ plt.plot(X, np.dot(X_17, linear.coef_) + linear.intercept_, c='red') $ plt.plot(X, np.dot(X_17,ridge.coef_) + ridge.intercept_, c='b') $ plt.xlabel('Hour of Day') $ plt.ylabel('Count')
Google_stock.head()
result.inserted_primary_key
paired_df_grouped.n_best_co_occurence.hist(bins=20)
subwaydf.iloc[174568:174572] #this high number seems to be because entries and exits messes up.
(df2.query('group == "control"')['converted'] == 1).sum()/(df2.query('group == "control"')['converted'] >= 0).sum()
first_commit_timestamp = git_log.loc[699070,'timestamp'] $ last_commit_timestamp = pd.to_datetime('today') $ corrected_log = git_log[(git_log.timestamp >= first_commit_timestamp) & (git_log.timestamp <= last_commit_timestamp)] $ corrected_log['timestamp'].describe()
features_df.shape
pres_df['subject_count_tmp'] = pres_df['subjects'].apply(subject_count) # can use apply instead of map $ pres_df.head()
from runtimestamp.runtimestamp import runtimestamp # for reproducibility $ from docs.build_docs import *                      # auto-generates docs $ runtimestamp('Leon') $ generate_docs()
import findspark $ findspark.init() $ import pyspark $ sc = pyspark.SparkContext()
cityID = '6ba08e404aed471f' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Riverside.append(tweet) 
print(train_data_features.shape)
stream_measures.columns = ['stream', 'min_stream_occurances', 'max_stream_occurances', 'mean_occurances']
import math
year_ago = session.query(Measurement.date).order_by(Measurement.date.desc()).first() $ print(year_ago)
FREEVIEW.plot_number_of_fixations(raw_fix_count_df, option='facet_subjects')
from sklearn.feature_extraction.text import TfidfVectorizer $ tfidf = TfidfVectorizer(ngram_range=(2,3), stop_words='english', max_features=2000) $ tfmod = tfidf.fit_transform(aldf['summary_clean']) $ tfidfdummiee = pd.DataFrame(tfmod.todense(), columns=tfidf.get_feature_names()) $ tfidfdummiee.head()
df2.head()
ridgeregcv = RidgeCV(alphas=alpha_range, normalize=True, scoring='neg_mean_squared_error') $ ridgeregcv.fit(X_train, y_train) $ ridgeregcv.alpha_
tweet_df.describe()
df['Ranking Full URL on Sep  1, 2017'] = df['Protocol for Sep  1, 2017'] + "://" + df['Ranking URL on Sep  1, 2017'] $ df['Ranking Full URL on Jan  1, 2017'] = df['Protocol for Jan  1, 2017'] + "://" + df['Ranking URL on Jan  1, 2017']
date_crawled_range == last_seen_range
major_cities_l1_features = major_cities_l1_fset.features $ len(major_cities_l1_features)
df_predictions['p2_dog'].value_counts()
for x,y in zip(X_train.iloc[0].index,X_train.iloc[0]): $     print(str(x)+'_'+str(y))
print(datetime1) $ print(type(datetime1))
train_users_path = '../../Data/train_users_2.csv' $ train_users = pd.read_csv(train_users_path) $ print(len(train_users)) $ train_users.head()
print watch_table.shape $ watch_table.head()
joined_with_project_info = sampled_contirbutors_human_agg_by_gender_and_proj_df.join( $     project_human_cleaned_df, $     on="project").join( $     agg_post_sentiment, $     on="project")
old_page_converted = np.random.binomial(N_old, P_old) $ old_page_converted
df, err = ek.get_data(['GOOG.O','MSFT.O', 'FB.O'], $                       [ 'TR.Revenue','TR.GrossProfit']) $ df
df.iloc[:,[0,3]]
total_base_dict_by_place.loc[place]['hashtags']
loan_stats["revol_util"] = loan_stats["revol_util"].ascharacter() $ loan_stats["revol_util"] = loan_stats["revol_util"].gsub(pattern = "%", replacement = "") $ loan_stats["revol_util"] = loan_stats["revol_util"].trim() $ loan_stats["revol_util"] = loan_stats["revol_util"].asnumeric() $ loan_stats["revol_util"].head(rows = 2)
df.corr()
save_n_load_df(promo_df, 'promo_df3.pkl')
pivoted = departures.pivot_table(index='Start Date', columns='Start Station', values='Duration')
sample_item = [df_stars.iloc[0].business_id] $ content_rec.recommend_from_interactions(sample_item)
tickerdf = tickerdf.dropna(axis=0,how='any')
raw_df = raw_df.reset_index() $ raw_df = raw_df.rename(index=str, columns={"index": "Date", 1: "Twitter Names", 2:"Words", 3:"Retweeted"}) $ raw_df.index.names = ["IndexLabel"]
    test_size = 3 $     X = clean_prices.copy().loc[:,'EMA_ratio':'5dayvol'] $     y = clean_prices.copy().loc[:, ['5_day_target']]
games_2017 = nba_df.loc[(nba_df.index.year == 2017), ] $ pd.pivot_table(games_2017, values = "Opp.Pts", index = "Team", aggfunc = np.mean).sort_values(by = "Opp.Pts", ascending = False).head(5)
print('The dataset contains {} tweets.'.format(strNb(len(tw))))
def set_position(lots): $     position=store['POSITION'] $     position.write('Current',lots)
result = logit.fit() $ result.summary()
df_goog.head(22)
df[df['Descriptor'] == 'Pothole'].count()
df = df.loc[:, ["id", "created_at", "text", "retweet_count", "user_name"]]
reddit.head(2)
results.summary()
df_nott = df.query('landing_page == "old_page"') $ df_4 = df_nott.query('group != "control"') $ df_4.nunique() $
grades_ord > 'C'
df = format_results(alltweets)
[k for val in train_x[0] for k,v in words.items() if v==val]
parsed_test.printSchema()
top_songs['Track Name'].isnull().sum()
print df.shape[0] + noloc_df.shape[0]
df = $ df = $ df
no_name_list = df_twitter_copy[df_twitter_copy.name.str.contains('(^[a-z])')].name.tolist()
dtunit = np.array(data_dict['Traded Volume'],dtype='float64')
df_EMR_dd_dummies_no_sparse.head()
cell_df.cell_name[0]
X_train_predict_i = scaler.inverse_transform(X_train_predict) $ y_train_i = scaler.inverse_transform(y_train) $ X_test_predict_i = scaler.inverse_transform(X_test_predict) $ y_test_i = scaler.inverse_transform(y_test)
btc_price_df.sort_index(inplace=True)
df.info()
data = pd.merge(irl, billstargs, left_on='bill_id', right_on='bill_id') $ data.drop_duplicates(['bill_id'], keep='last') $
year_tobs = session.query(Measurement.date, Measurement.tobs).\ $     filter(Measurement.date > '2017-07-02').\ $     filter(Measurement.station == "USC00519281").\ $     order_by(Measurement.date).all() $ year_tobs
from bs4 import BeautifulSoup as bs $ from splinter import Browser $ import requests
driver.find_element_by_xpath('//*[@id="body"]/table[2]/tbody/tr/td/table[2]/tbody/tr[2]/td[1]/b/font/a').click()
from sklearn.feature_extraction.text import TfidfVectorizer $ from sklearn.naive_bayes import MultinomialNB $ tfidf_vectorizer = TfidfVectorizer(min_df = 5, max_df = 1000)
has_text[has_text.grade_levels==6.0]
stock = data2.Close
interest_dict = {'low':0, 'medium':1, 'high':2} $ data.replace(to_replace=interest_dict, inplace=True)
first_result.find('a')
treat_convert = df2.query('group == "treatment"')['converted'].mean() $ treat_convert
twitter_archive_master['Stage'] = 'Unknown' $ twitter_archive_master.loc[(twitter_archive_master.doggo == 'doggo'),'Stage']='doggo' $ twitter_archive_master.loc[(twitter_archive_master.floofer == 'floofer'),'Stage']='floofer' $ twitter_archive_master.loc[(twitter_archive_master.pupper == 'pupper'),'Stage']='pupper' $ twitter_archive_master.loc[(twitter_archive_master.puppo == 'puppo'),'Stage']='puppo'
autos["odometer"] = (autos["odometer"].str.replace("km", "") $                                      .str.replace(",", "") $                                      .astype(int)) $ autos.rename({"odometer": "odometer_km"}, axis=1, inplace = True) $ autos["odometer_km"].head()
n_old = df2[df2['group'] == 'control']['converted'].count() $ n_old
prop_converted = (df['converted'] == 1).mean() $ prop_converted
energy_indices = energy_cpi.drop(['Measure','Region','Adjustment Type','Frequency'], $                                  axis=1).set_index(['Index','Time']).unstack('Index') $ energy_indices
pold = df2_convert $ pold
temperature_sensors_df['state'][ $     temperature_sensors_df['entity_id'] != 'sensor.darksky_sensor_temperature'].hist( $     bins=50); $ plt.title("Inside temperature"); $ plt.xlabel("Temperature $^\circ$C");
dateset.drop('index',inplace=True,axis=1)
data.head(50)
image_predictions.describe()
transfered_holidays=holidays_events[(holidays_events.type=='Holiday') & (holidays_events.transferred==True)] $ print("Rows and columns:",transfered_holidays.shape) $ pd.DataFrame.head(transfered_holidays)
state_party_df.info()
xmlData.describe(include = ['O'])
crimes.PRIMARY_DESCRIPTION[3:10]
df2[df2['user_id'].duplicated(keep=False)]
tweet_df_clean.reset_index(inplace=True) $ tweet_df_clean.drop('index', axis=1, inplace=True) $
data['processing_time'].describe() $
numerical = condo_6[list(set(condo_6.columns) - set(['SSL', 'SALEDATE', 'FULLADDRESS', 'UNITNUM']))] $ numerical.info()
zip_counts=bus["business_id"].groupby(bus["postal_code"]).size().sort_values(ascending=False) $
ab_df2.user_id.nunique()
n_unique_users = df2['user_id'].nunique() $ print ("Number of unique users in the dataset: {}".format(n_unique_users))
import pandas as pd $ import cx_Oracle $ xedb = cx_Oracle.connect('hr/hr@localhost/xe') $ cur = xedb.cursor()
df_final.loc_country.value_counts()
regression_line = [] $ for x in xs: $     regression_line.append((m*x)+b)
num_users = df.user_id.nunique() $ num_users
betas_race_samples = trace['betas_race'] $ alpha_samples = trace['alpha'] $ beta_income_samples = trace['beta_income'] $ beta_complaint_samples = trace['beta_complaint']
df_combined = df_combined.join(pd.get_dummies(df_combined['country'], prefix='country')) $ df_combined.head()
open_percentage = (train['is_open'].value_counts()[1] / train.shape[0]) * 100 $ print('Percentage of total emails that get opened: {0:.2f}%'.format(open_percentage))
cols = list(df_mes2.columns) $ cols.remove('tip_amount')
selfharmm_topic_names_df = mf.compile_topics_df([nmf_cv, lsa_cv, lda_cv], [nmf_tfidf, lsa_tfidf, lda_tfidf], cv_fitted, tfidf_fitted, 20)
sns.distplot(master_list[master_list['Count'] < 5]['Count'])
Test_extra.head()
df_arch.head()
import json $ from pymongo import MongoClient $ client = MongoClient(port=12345) $ db = client.stocks $ tweets = db.stock_whisperer
data = response.json() $ data
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn; seaborn.set()
sns.countplot(tweets.hashtag,label="Count", $              order = tweets.hashtag.value_counts().index) $ print(tweets.hashtag.value_counts(normalize=True))
r_np= df[df['landing_page']=='new_page'] $ p_np=r_np['landing_page'].count()/df['landing_page'].count() $ print('The probability that an individual received a new page is:  ' + str(p_np))
knn_10.score(X_test, y_test)
QUIDS_wide.dropna(subset =["y"], axis =0, inplace=True)
trainset = dataset_filtered.iloc[tr_indices,:].copy() $ testset = dataset_filtered.iloc[t_indices,:].copy()
fig, ax = plt.subplots(figsize=(14, 6)) $ varlabel = ts_df.columns[0] $ ts_df[varlabel].plot(style='-', ax=ax) $ ax.set_ylabel(varlabel) $ ax.set_title("{} | {}".format(dataset['id'], dataset['title']));
print('Number of SLPs with at least one declared PSC: ' + str(len(active_psc_records[active_psc_records.company_type == 'Limited Partnership for Scotland'].company_number.unique()))) $ print('Proportion of SLPs in active companies dataset that declared a PSC: ' + str(round(len(active_psc_records[active_psc_records.company_type == 'Limited Partnership for Scotland'].company_number.unique()) / len(slps.CompanyNumber.unique()),2)))
tlen_k1 = pd.Series(data=kayla['len'].values, index=kayla['Date']) $ tfav_k1 = pd.Series(data=kayla['Likes'].values, index=kayla['Date']) $ tret_k1 = pd.Series(data=kayla['RTs'].values, index=kayla['Date'])
lr_model_newton = LogisticRegression(C=0.01, class_weight='balanced', max_iter=50, solver='newton-cg')
engine = create_engine('sqlite:///results.db') $ pd.read_sql_query('SELECT * FROM demotabl LIMIT 5;',engine)
offseason07 = ALL[(ALL.index < '2007-09-06')] # This means every transaction before 9-6-07 belongs to the 2007 Offseason.
len(df2_treatment.index)/len(df2.index)
prob_ind_conv = df2[df2["converted"] == 1].shape[0]/df2.shape[0] $ prob_ind_conv
from sklearn.externals import joblib $ date_txt = '0402' $ joblib.dump(gbm, "gbm_"+ date_txt + '.pkl') $ joblib.dump(rfc, "rfc_"+ date_txt + '.pkl') $ feature_imp.to_csv("Feature importance 0420.csv", index = False)
ave_ratings_over_time.plot()
b = 2. * np.random.randn(*a.shape) + 1. $ b.shape
gdax_trans['Timestamp'] = gdax_trans.apply(lambda row: fix_timestamp(row["Timestamp"]), axis=1)
access_logs_parsed = access_logs_raw \ $ .map(parse_apache_log_line) \ $ .filter(lambda x: x is not None) $ access_logs_parsed.count()
scores = pd.read_csv('game_scores.csv') $ scores.head(3)
unseen_predictions_df = pd.concat([pd.DataFrame(unseen_predictions, columns=['Predictions'], index=X_unseen.index), y_unseen], axis=1) # Test predictions $ unseen_predictions_df $
rng = pd.date_range('1/1/2012', periods=5, freq='M')
stocks.columns  # Inspecting progress
weather1.head()
absorption_to_total = absorption.xs_tally / total.xs_tally $ absorption_to_total.get_pandas_dataframe()
github_data.isnull().sum()
tweet_df_clean.head()
df3.head(5)
terms_single = set(terms_all) $ count_single = Counter() $ count_single.update(terms_single) $ print(count_single.most_common(5))
!cp ../../drive/ColabNotebooks/AV_innoplexus_html/train_cont.zip .
sets_columns_names = [column[DATA].name for column in get_child_data_nodes(sets_node).values()] $ display(sets_columns_names)
p_old = p_new # df2[df2['landing_page']=='old_page']['converted'].mean() $ p_old
p_new = (df2.query('converted == 1')['user_id'].nunique())/(df2.user_id.nunique()) $ p_new
lr_all_user[lr_all_user.id_loan_request==120973].T.to_clipboard()
batting_df2 = batting_df.set_index(['playerid', 'yearid']) $ salary_df2 = salary_df2.set_index(['playerid', 'yearid'])
hashtag_df = pd.DataFrame.from_dict(list(dict(counts).items())) $ hashtag_df.columns = ['keyword', 'count'] $ sorted_hashtag_df = hashtag_df.sort_values(by='count', ascending=False)
YS1517.Close.plot(kind='kde')
cryptos['market_cap_usd_billions'] = cryptos.market_cap_usd / 1e9  $ cryptos.head()
y=interactive(func,amplitude=[1,2,3,4,5],ideal_mu=(-5,5,0.5), $               ideal_sigma=(0,2,0.2), $               noise_sd=(0,1,0.1),noise_mean=(-1,1,0.2)) $ display(y)
Lab7 = pd.read_excel(r"C:\Users\Tanushree\Desktop\UNIVERSITY\Quarter 4\Dashboard\Lab1_Session\Session1.xlsx",sheetname = 0)
week11 = week10.rename(columns={77:'77'}) $ stocks = stocks.rename(columns={'Week 10':'Week 11','70':'77'}) $ week11 = pd.merge(stocks,week11,on=['77','Tickers']) $ week11.drop_duplicates(subset='Link',inplace=True)
filtered_active_sample_sizes.groupby().agg(F.sum("sample_size_1")).show()
ts.tshift(5, freq='D')
X_test.shape
df2.head(3) $
origin = 'general+assembly+singapore' $ dest = 'johor+bahru'
stations = session.query(Measurement).group_by(Measurement.station).count() $ print(f"{stations} stations")
import datetime $ from matplotlib import style
record_count = len(df) $ action = df[df.action == 'Yes'] $ action_count = len(action) $ pct_whole = (action_count / record_count) * 100
df.columns # pandas Index object
dedups.isnull().sum()
full_url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars" $ full_image = "https://www.jpl.nasa.gov/spaceimages/details.php?id=PIA16726" $ more_info_url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars"
features.to_csv('cbg_features_4_19.csv')
pgh_311_data.resample("M").count()
df2=df2.drop(df2.index[2893])
n_old = df2[(df2['group'] == 'control')].shape[0] $ n_old
final_topbikes['Distance'].mean()
def prepare(ticket): $     one = cleaning(ticket).split() $     two = dictionary.doc2bow(one) $     return two
old_page_converted = np.random.choice([0,1],size = n_new, p = [1-p_old,p_old]) $ old_page_converted
df2.drop_duplicates('user_id', inplace=True) $ df2[df2['user_id']==773192]
unicode = text.encode('unicode_escape') $ print(unicode) $ unicode.find(b'\U0001f60e')
df.index
df_geo = df[DESCRIBED_COLUMNS]
def indexed_price(pricevector): $     r = pricevector / pricevector.shift(1) - 1 $     r = r.replace([np.inf, -np.inf], 0) # this implies you buy after 1 full day of existence, i.e. 12h or 24h after ico begins $     r = r.fillna(0) $     return 100*np.exp(r.cumsum())
users.columns
accuracy = accuracy_score(y_test, y_pred) $ print('Accuracy: {:.1f}%'.format(accuracy * 100.0))
scores.IMDB.mean()
print(chr(65)) $ print(ord('A'))
f = lv_workspace.get_data_filter_object(step=1, subset='A') $
odo_counts = odometer.value_counts() $ print(odo_counts) $ odo_counts.sort_index(ascending=False) $
df_never_moved.to_csv('never_moved.csv')
df[['Principal','terms','age','Gender','education']].head()
a = df4.groupby('placeId').count()
foo.sort( foo['count'].desc() ).show()
top_10_4 = scores.loc[20].argsort()[::-1][:11] $ trunc_df.loc[list(top_10_4)]
autos.loc[expensive, "price"].count()
import pandas as pd $ from pandas.io.gbq import read_gbq $ import numpy as np $ import seaborn as sns $ import matplotlib.pyplot as plt
asf_agg_by_gender_and_proj_df = non_blocking_df_save_or_load_csv( $     group_by_project_count_gender(cleaned_asf_people_human_df_saved).repartition(1), $     "{0}/asf_people_cleaned_agg_by_gender_and_proj_3c".format(fs_prefix))
sh_max_df.tobs = sh_max_df.tobs.astype(float)
subcols = dropcol(subcols, ['srch_rm_cnt','srch_destination_id','posa_continent']) $ print(len(subcols)) $ print(subcols)
len(df2.loc[df['landing_page'] == 'new_page'])/len(df2['user_id'])
low_dollar_volume = ~high_dollar_volume
!head data/sample_submission_NDF.csv
key_name = 'aws-master.pem' $ pem_path = '/Users/Ev/.ssh' + '/' + key_name $ dns = 'ec2-52-37-101-84.us-west-2.compute.amazonaws.com' $ user = 'ubuntu' $ print 'ssh -X -i {} {}@{}'.format(pem_path, user, dns)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new['country'].unique()
p_r_curve(y_test, y_fit_proba[:,1])
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=870) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
df.shape
model = 'MO_080526771943' $ print(kimanalysis.shortcode(model)) $ print(kimanalysis.shortid(model)) $ print(kimanalysis.extendedid(model))
columns = ["Date", "Store", "Promo", "StateHoliday", "SchoolHoliday"]
agency_borough = data.groupby(['Agency','Borough']) $ agency_borough.size().plot(kind='bar')
%%time $ with open('df_3.out','r') as inFile: $     fullDf=pickle.load(inFile)
dul_final.to_csv(folder + "\\" + "Duluth-all.txt", sep="\t", index = False)
ttDaily['DailyEntry'] = ttDaily.ENTRIES.diff()
df_experiment = pd.read_csv('data/experiment_details_new.csv') # to-do: need to use "_new" dataset? $ df_experiment.head()
word_vocab_file_path = os.path.join(working_dir, 'word_ngrams_vocabulary.tsv') $ text_classifier.get_step_by_name("text_word_ngrams").save_vocabulary(word_vocab_file_path) $ char_vocab_file_path = os.path.join(working_dir, 'char_ngrams_vocabulary.tsv') $ text_classifier.get_step_by_name("text_char_ngrams").save_vocabulary(char_vocab_file_path) $
archive_df_clean=archive_df[archive_df.retweeted_status_id.isnull() == True] $
print(type(df.groupby(level=0)['POPESTIMATE2010','POPESTIMATE2011'])) $ print(type(df.groupby(level=0)['POPESTIMATE2010']))
len(penalties)
test_stationarity(pass_values)
v_aux=[] $ for i in range(0,l2): $     if i not in seq: $         v_aux.append(votes[i]) $ col.append(np.array(v_aux))
a = np.cos(np.pi) $ a # will be printed as a default in Jupyter without using a `print` command. 
def sale_lost(count, minutes): $     crepe_per_min = count // (4 * 60) * 0.002 $     crepes_lost = crepe_per_min * minutes $     return crepes_lost
df['user_id'].nunique()
df1=data.rename(columns={'SA':'Polarity'}) $ df1.head()
points.loc['r0', :]  # .loc() is label-based indexing
df = pd.read_csv('ab_data.csv')
temp = df
assert pd.notnull(ebola).all().all()
df.isnull().sum()
autos_p['registration_year'].describe()
df = pd.read_sql('SELECT * FROM users_ukr', engine).set_index('id')
learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn) $ learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1) $ learn.clip=25. $ learn.metrics = [accuracy]
import pandas as pd $ from pandas import Series,DataFrame
challenge_contest = challenges[['challenge_id', 'contest_id']]
file = '/Users/sumad/Documents/DS/Python/UM Spcialization/DS_with_Python' + '/mpg.csv' $ with open(file) as con: $     df = pd.read_csv(con)
SVPOL(data/'realdata'/'H.dat').to_dataframe().head()
cohort_retention_df.head()
props.info()
len(interactions_bookmarked), len(red_inter_book)
X_colset = set(X_cols) $ y_cols = [col for col in df_train.columns if col not in X_colset] $ y = df_train[y_cols]
%%sql $ Select status_change_date,hour.hour_key $ FROM facts,hour $ WHERE TO_CHAR(facts.status_change_date, 'YYYY-MM-DD HH24:00:00') = hour.hour $ limit 10;
X = reddit['title'].values $ y = reddit['engagement'] $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
word_docx = 'https://dl.dropboxusercontent.com/s/tlbu2ao89bmzjiw/transcripci%C3%B3n%20Kathya%20Araujo.docx'
future_dates = prophet_model.make_future_dataframe(periods=forecast_steps, freq='W') $ future_dates.tail(3)
news_df = pd.DataFrame(news_dict) $ news_df.head()
tweets[0]._json['retweet_count'], tweets[0]._json['favorite_count'], tweets[0]._json['entities']['media'][0]['media_url']
feature_filter = SelectKBest(f_regression, k=2) $ model = linear_model.LinearRegression(fit_intercept=False) $ my_pipeline = Pipeline([('feature_selection',feature_filter), ('linear_model', model)]) $ fitted_pipeline = my_pipeline.fit(features,y) $ my_pipeline.named_steps['linear_model'].coef_
dfClientes.shape[1]
df['body'] = df['body'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))
df.head()
new_page_converted = np.random.choice([0, 1], size=n_new, p=[converted_rate, 1 - converted_rate])
squares.index
df = df[df.user_location_country.notnull() & df.user_location_region.notnull()& df.srch_ci.notnull() & df.srch_co.notnull()]
store_items
s.iloc[-3:]
n_old = (df2['landing_page'] == 'old_page').sum(); $ n_old
companies = data['Company'].value_counts() $ print('Number of Companies compained about: {}'.format(len(companies)))
plot_result(stock_name, df, p, y_test)
df.converted.mean()
loan_fundings.columns
def convert_to_one_hot(Y, C): $     Y = np.eye(C)[Y.reshape(-1)] $     return Y
df.get_dtype_counts()
lgComp_df = wcPerf1_df.groupby(['lgID','name']).sum() $ lgComp_df
autos[autos["price"].between(50,1000000)]["price"].describe()
test_df.shape
tickerdata
stacked_geo_df.head(10)
scoring_data = {'values': payload_data.tolist()}
dataframe.columns
rounds_df[(rounds_df.announced_on >= '2015-08-01')].shape[0]
lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_data_filter_settings['ntot_winter'].settings.df $
condos.head()
autos["odometer_km"].value_counts()
pro_result = pro_table.sort_values('Profit', ascending=False) $ pro_result.head()
df2[df2['group']=="control"].count()[0]
last_id(soup)
n_new = df_treatment.nunique()['user_id']
pres_df.head()
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
plotone = normal_model.plot_components(normal_forecast)
control_proportion = df2.query('group=="control" & converted==1')['user_id'].count()/df2.query('group=="control"')['user_id'].count() $ treatment_proportion = df2.query('group=="treatment" & converted==1')['user_id'].count()/df2.query('group=="treatment"')['user_id'].count() $ obs_diff = treatment_proportion - control_proportion $ obs_diff
df.drop(df.query("group == 'treatment' and landing_page != 'new_page'").index, axis = 0,inplace = True) $
googletrend.head()
len(guineaFileList) == len(guineaDf)
del adj_close_acq_date['Quantity'] $ del adj_close_acq_date['Unit Cost'] $ del adj_close_acq_date['Cost Basis'] $ del adj_close_acq_date['Start of Year'] $ adj_close_acq_date.sort_values(by=['Ticker', 'Acquisition Date', 'Date'], ascending=[True, True, True], inplace=True)
import pandas as pd $ from pandas import DataFrame, Series $ import json
df = pd.read_csv('ab_data.csv') $ df.head()
if 'users_orders' in globals(): $     del users_orders
aux = df2[df2.group == 'treatment'] $ aux[aux.converted == 1].shape[0] / aux.shape[0]
old_conv_rate = df2.query('group == "control"')['converted'].mean() $ old_conv_rate
reviewsDFslice = reviewsDFslice.drop(reviewsDFslice.index[[11577]]) $ reviewsDFslice = reviewsDFslice.drop(reviewsDFslice.index[[12957]]) $ reviewsDFslice = reviewsDFslice.drop(reviewsDFslice.index[[15437]]) $ reviewsDFslice = reviewsDFslice.drop(reviewsDFslice.index[[16839]]) $ reviewsDFslice = reviewsDFslice.drop(reviewsDFslice.index[[23361]])
run txt2pdf.py -o"2018-06-18  2015 871 discharges.pdf"  "2018-06-18  2015 871 discharges.txt"
raw_text = raw_text.replace('\n','') $
reg_traffic_with_flags = search_algo(regular_traffic,honeypot_df,['id.orig_h','src'],['ts','ts_unix'])
i = int(dogscats_df.shape[0] * 0.75) $ dogscats_df.loc[i:, 'pred_00'] = np.load('predictions.npy') $
dayofweek = pd.DatetimeIndex(pivoted.columns).dayofweek
model.load_weights('best.hdf5')
df_sentiment.head()
df.loc[:, "last_name"]
df2['intercept'] = 1 $ df2['ab_page'] = np.where(df2['group'] =='control',0,1)
df2[df2['landing_page']=='new_page'].count()[0]/df2.shape[0]
plt.bar(left=range(1, len(per_var) + 1), height = per_var, tick_label=labels) $ plt.show()
strike = pd.DataFrame(K*np.ones(ndays*nscen).reshape((ndays,nscen)),index=dates) $ call=pd.DataFrame({'Prima':np.exp(-r*ndays)*np.fmax(sim_closes-strike,np.zeros(ndays*nscen).reshape((ndays,nscen))).T.mean()},index=dates) $ call.plot();
data['Closed Date'] = data['Closed Date'].apply(lambda x:datetime. $                                                 datetime. $                                                 strptime(x,'%m/%d/%Y %H:%M')) $
new_order.dtypes
subs_and_comments = subs_and_comments.merge(op_add_comms, on='id', how='outer')
engine = create_engine('sqlite:///hawaii.sqlite')
df2[df2.user_id.duplicated()]
rm_indices = events['type'][events['type']== 'type'].index $ events = events.drop(events.index[[rm_indices]])
results = cosine_similarity(X[0:1], X).flatten()
df2.loc['2016-03-01':'2016-03-31', ['GrossOut', 'NetOut']].sum()
import os $ print(os.getcwd())
public_tweets = pd.read_csv("C:\\Users\\Parth\\Desktop\\result3.csv")
test_data = prepare_data(intervention_test_df).drop(drop_columns, axis=1)
ts.shift(5, freq='BM')
kfpd.plugin = DataSynthesizerPlugin(mode='correlated_attribute_mode') $ fdf = time_method(kfpd, verbose = True, rerun_query = False, repetitions = 10) $ fdf.head()
feature_layer.properties.capabilities
df['created'] = pd.to_datetime(df['created']) $ df['dow'] = df['created'].apply(lambda x: x.date().weekday()) $ df['is_weekend'] = df['created'].apply(lambda x: 1 if x.date().weekday() in (5,6) else 0)
all_sites_with_unique_id_nums_and_names.head()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], $                                               [n_old, n_new], alternative='smaller') $ z_score, p_value
print("Number of Techniques in Mobile ATT&CK") $ techniques = lift.get_all_mobile_techniques() $ print(len(techniques)) $ df = json_normalize(techniques) $ df.reindex(['matrix', 'id','tactic', 'technique', 'tactic_type','contributors'], axis=1)[0:5]
df2.head()
%timeit max(MyList) $
sales_location = pd.DataFrame(true_sales.ix[0:,'City':]) $ sales_city = sales_location.groupby(by='City',as_index=False).sum() $ city_margin_2015_mean = sales_city.sort_values(by='2015 Margin mean', ascending=False) $ top_city_sales = city_margin_2015_mean.head(10) $ top_city_sales $
scaler.fit(x_test)
data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz))
hk["catfathername"].unique()
party_crosstab = by_party.set_index([ $     "election_year", "party_name" $ ]).unstack(1).reset_index().fillna(0)
pd.DataFrame(d, index=['d', 'b', 'a']) # uses d, not df, as data input
top_brand_mileages = dict() $ for brand in top_brands: $     top_brand_mileages[brand] = autos.loc[autos['brand']==brand, 'odometer_km'].mean() $ top_brand_mileages
player_details = pd.read_csv("detailed_player_stats_2017.csv").drop_duplicates() $ player_details.rename(columns={"PERSON_ID":"PLAYER_ID"}, inplace=True)
condensed_xs.print_xs()
bigdf_read = pd.read_csv('Combined_Comments-Fixed.csv', index_col=0)
GBR.fit(X_train,Y_train)
train
f_lr_hash_modeling2.show(3)
fp7_proj["projectAcronym"] = fp7_proj["projectAcronym"].str.upper() $ fp7_part["projectAcronym"] = fp7_part["projectAcronym"].str.upper()
x.plot()
hu.varinfo(tips)
for files in df.files[:10]: $     print('---') $     for f in files: $         print(f['name'])
bmp_series = pandas.Series(carprice) $ bmm_series = pandas.Series(carmileage) $ df_price_mil = pandas.DataFrame(bmp_series, columns=['mean_price']) $ df_price_mil['mean_mileage']=pandas.Series(carmileage) $ print(df_price_mil)
print(explained_variance_score(preds,Y_test))
len(hundred)
df2[((df2['group'] == 'control') == (df2['landing_page'] == 'old_page')) == False].shape[0]
transactions.join(users.set_index('UserID'), on='UserID')
help(Widget)
a1 = a[a.a>40] $ a2 = a[a.a<40]
from sklearn.metrics import r2_score $ predictions = automl.predict(X_test) $ print("R2 score:", r2_score(y_test, predictions))
march_2016 = pd.Period('2016-03', freq='M') $ march_2016.start_time $ march_2016.end_time
print ("The number Unique userid in dataset is {}".format(df.user_id.drop_duplicates().count()))
df_clean.head()
response = urllib.request.urlopen('http://example.com/') $ html = response.read() $ print(html)
tweet_archive_clean.name.value_counts()
df2[df2['user_id'].duplicated()]
gs_from_model_under.score(X_test, y_test_under)
len(prediction)
festivals.at[2,'latitude'] = 41.9028805 $ festivals.head(3)
popC15.groupby(by='content').size()
%timeit pd.read_sql(f'explain {sql_on}', engine).head()
all_df.isnull().any() $
df[df < 0] = -df $ df
hours.dropna(subset=['Specialty'], how='all', inplace=True)
np.mean(investors_df.investment_count)
meters =['SM15R-01-000000ED','SM15R-01-0000028A','SM15R-01-000001F6','SM15R-01-00000219','SM15R-01-00000260']
data3['sales'].hist(bins=20, figsize=(10,5))
url='http://www.ign.com/articles/2017/10/26/super-mario-odyssey-review?watch' $ html=requests.get(url).content $ soup=BeautifulSoup(html,'html5lib') $ article = soup.find_all('article') $ plist = article[0].find_all('p',text=True)
complete_df = complete_df.join(pd.get_dummies(complete_df['group_country'])) $ complete_df.head()
nClusters=15 $ silhouette_scores, calinski_score = hierarchical_performance_metrics(crosstab_transformed, nClusters)
counts = tokaise.groupby(Grouper(key="stamp",freq='3H')).agg({"id":"count"}).rename(columns={"id":"count"}) $ counts.head()
a
res_list=[result_momentum,result_vol,result_mwu,result_pm]
lm=sm.Logit(df2['converted'],df2[['intercept','new','ab_page']]) $ results=lm.fit() $ results.summary()
autos.describe()
print(reddit.user.me())
open_prs = PullRequests(github_index).is_open()\ $                                      .get_cardinality("id_in_repo") $ print("Number of open PRs: ", get_aggs(open_prs)) $ print() $
MyAppToken = key_token['token'] $ username = key_token['username'] $ password = key_token['password']
file = 'injuries.csv' $ injury_df = pd.read_csv(file)
len(df[~(df.user_properties == {})])
nitrodata['Month'].value_counts().sort_index()
results = pd.concat([pd.Series(preds).reset_index(drop=True), Y_test.reset_index(drop=True)], axis = 1) $ results.columns = ["predicted", "actual"] $ results["diff"] = (results["predicted"] - results["actual"])/results["actual"]
df16 = pd.read_csv('2016.csv')
print('ADM total: %6d'%adm.ec550.count()) $ print('in_latlon: %6d'%ec550.count())
search1 = search.merge(user1, how='left', left_on='user_uuid', right_on='uuid')
googletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0] $ googletrend['State'] = googletrend.file.str.split('_', expand=True)[2] $ googletrend.loc[googletrend.State=='NI', "State"] = 'HB,NI'
n_old=df2.query("landing_page =='old_page'").shape[0] $ n_old
archive = pd.read_csv('twitter-archive-enhanced.csv')
SHARE_ROOT = "./stockdemo-model/" $ LSTM_MODEL = SHARE_ROOT + 'modellstm.json' $ MODEL_WEIGHTS = SHARE_ROOT + 'modellstm.h5' $ SCHEMA_FILE = SHARE_ROOT + 'service_schema.json'
excelDF.groupby('Ship Mode').Quantity.max()
train = K.function(inputs=[x, target], outputs=[loss], updates=updates)
to_be_predicted_Day2 = 38.4923792 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
df2 = df2.join(countries.set_index('user_id'), on='user_id') $ df2.head()
sc
np.mean(new_page_converted) - np.mean(old_page_converted)
useless_variables = [ 'CONTRACT_NUMBER', 'CONTRACT_MODIFICATEUR', 'CRE_DATE', $        'CONDITION_REGLEMENT', 'MOTIF_RESILIATION', 'RENOUVELLEMENT_AGENCE', $        'PRIX_FORMULE', 'PRIX_OPTION', 'NUM_CAMPAGNE', 'DATE_RESILIATION'] $ contract_history.drop(useless_variables, axis=1, inplace=True)
plot_two = no_outliers_model.plot_components(no_outliers_forecast)
stemmer = SnowballStemmer('english') $ s_words = stopwords.words('english')
df = pd.read_csv('ab_data.csv')
twitter_master2.to_csv('twitter_archive_master.csv',index=False)
cols = list(startups_USA) $ cols.append(cols.pop(cols.index('status'))) $ startups_USA = startups_USA.ix[:, cols]
text = df_pr[df_pr.item==1].iloc[0].body $ text
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative="larger") $ print(z_score, p_value)
data.loc[[pd.to_datetime("2016-12-01"), pd.to_datetime("2016-12-03")], ["TMAX", "TMIN"]]
(p_diffs > pop_diff).mean()
df.fillna({'text': 'NaN'}, inplace=True) $ df['health'] = df.text.str.contains('Health', case=False).astype(int)
troll_tweets.drop(columns=different_tweet_cols, axis=1, inplace=True)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ (z_score, p_value)
print(1 + 2 * 3) $ print((1 + 2) * 3)
dates = np.array(["2013-09-01","2013-09-02"],dtype='datetime64') $ print(dates)
tweet_data_copy.info()
title = 'Impact versus effort' $ subtitle = 'Potential controls' $ yaxislabel = 'Impact' $ xaxislabel = 'Effort'
bremen.info()
stationtotals = data.groupby(['STATION','LINENAME'])['EntriesDifference'].agg(pd.np.sum).sort_values(ascending=0) $
print("Probability of control group converting:", $       ab_file2[ab_file2['group']=='control']['converted'].mean())
vi['ENDDATE']
min_result = optimize_f(f)
store_items.fillna(method = 'backfill', axis = 0)
LT906474.rename(columns={'# feature':'feature'}, inplace=True) $ CP020543.rename(columns={'# feature':'feature'}, inplace=True)
plt.hist(null_value); $ plt.axvline(x=obs_diff, color= 'red');
print(df.shape) $ df = df[pd.notnull(df['is_shift'])] $ print(df.shape)
city = pd.read_sql_query('select * from city', engine) $ city.head()
df.asfreq('W', method='ffill')
encoder_input_data, doc_length = load_encoder_inputs('train_body_vecs.npy') $ decoder_input_data, decoder_target_data = load_decoder_inputs('train_title_vecs.npy')
pd.crosstab(cust_demo.Martial_Status,cust_demo.Own_House).plot(kind='bar', color=['R','G'],alpha=0.5, stacked=True)
model2 = LogisticRegression() $ model2.fit(x_train, y_train)
print(top20)
df_ad_state_metro_2.set_index('state', inplace=True) $ df_state_victory_margins.set_index('state', inplace=True)
injury_df.iloc[:40,:]
train_tfidf_X = train_tfidf.drop([0, 1, 2, 4, 5, 6], 1) $ train_tfidf_X = scalar.fit_transform(train_tfidf_X) $ train_tfidf_y = train_tfidf[4].astype(int) $ tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test = train_test_split(train_tfidf_X, train_tfidf_y, random_state=24)
tweet_archive_enhanced_clean = pd.merge(tweet_archive_enhanced_clean,tweet_image_predictions_clean[['tweet_id','p1','p1_conf','p1_dog']], how ='inner', left_on ='tweet_id',right_on ='tweet_id')
maxActiveDay = max(activityByUserByday["activeDay"]) $ print("\nThe highest number of active day is :",maxActiveDay,"days") $ numberOfUser = activityByUserByday[activityByUserByday["activeDay"]==maxActiveDay].userid.count() $ print("There are",numberOfUser,"users who keep playing the mobile application for", maxActiveDay,"days") $ X = pd.merge(X, activityByUserByday[['userid','activeDay']] , on ="userid")
df_birth.info()
df2.query("landing_page == 'new_page'").count()[0]/df2.shape[0]
dataM = pd.read_sql("SELECT Measurement.station, COUNT(Measurement.station) FROM Measurement GROUP BY Measurement.station ORDER BY COUNT(Measurement.station) DESC", conn) $ dataM
clean_archive.info()
mask = f>=0.5 $ print(f[mask])
!which chromedriver $ executable_path = {'executable_path': '/usr/local/bin/chromedriver'} $ browser = Browser('chrome', **executable_path, headless=False) $
idx = pd.IndexSlice $ df.loc[idx[:, :, 'x'], :]
response = requests.get(url) $ soup = BeautifulSoup(response.text, 'html.parser')
df.to_csv('data/Airline+Weather_data.csv',index=False)
merge[merge.columns[40:]].head(3) $
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[n_new, n_old], alternative = 'smaller') $ print(' z-score = {}.'. format(round(z_score,4))) $ print(' p-value = {}.'. format(round(p_value,4))) $
plt.hist(tobs_data, bins=12, label='tobs', color = 'skyblue') $ plt.xlabel("Temparture Observation Data") $ plt.ylabel("Frequency") $ plt.savefig("station_analysis.png") $ plt.show()
df_mas.in_reply_to_status_id_x = df_mas.in_reply_to_status_id_x.astype(str) $ df_mas.in_reply_to_user_id_x = df_mas.in_reply_to_user_id_x.astype(str) $ df_mas.retweeted_status_id = df_mas.retweeted_status_id.astype(str) $ df_mas.retweeted_status_user_id = df_mas.retweeted_status_user_id.astype(str)
(df.loc[df['user_id']== 17])['customer_number'].unique()
cust_data.plot(kind='scatter', x='RevolvingUtilization', y='MonthlyIncome', title = 'Scatterplot', color='R')
All_tweet_data_v2[['rating_numerator','rating_denominator']][All_tweet_data_v2.rating_denominator!=10]
lines  = df.rdd
nyse_filter = exchange.eq('NYS') $ nyse_filter
json_key = 'XXX.json' $ scope = ['https://spreadsheets.google.com/feeds'] $ credentials = ServiceAccountCredentials.from_json_keyfile_name(json_key, scope) $ gc = gspread.authorize(credentials) $
url ="https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv" $ response = requests.get(url)
import os $ import pandas as pd $ import matplotlib.pyplot as plt $ import numpy as np
keybyday = data.groupby(['KEY','DATE'])['EntriesDifference'].agg(pd.np.sum) $
mean_diff = dfNiwot["TDIFF"].mean() $ print("Mean Temp Diff = {:.3f}".format(mean_diff))
(df.converted == 1).mean()
convert_new = df2.query('landing_page=="new_page" and converted==1').count()[0] $ convert_new $
data = {'date': ['2014-05-01 18:47:05.069722', '2014-05-01 18:47:05.119994', '2014-05-02 18:47:05.178768', '2014-05-02 18:47:05.230071', '2014-05-02 18:47:05.230071', '2014-05-02 18:47:05.280592', '2014-05-03 18:47:05.332662', '2014-05-03 18:47:05.385109', '2014-05-04 18:47:05.436523', '2014-05-04 18:47:05.486877'], $         'battle_deaths': [34, 25, 26, 15, 15, 14, 26, 25, 62, 41]} $ df = pd.DataFrame(data, columns = ['date', 'battle_deaths']) $ df
data.occupation.unique()
b2 = a.sort_values('id').tail(20)
tcga_target_gtex_expression_hugo = tcga_target_gtex_expression.copy() $ tcga_target_gtex_expression_hugo.index = ensemble_to_hugo.reindex(tcga_target_gtex_expression.index).geneName.values $ tcga_target_gtex_expression_hugo = tcga_target_gtex_expression_hugo[tcga_target_gtex_expression_hugo.index.notnull()] $ tcga_target_gtex_expression_hugo.head()
bkk_lat = 13.685780 $ bkk_long = 100.484605 $ diff_wid = 0.1/grid_size
matches = tweets.text.str.contains('dead') $ tweets[matches]
transactions[~transactions['UserID'].isin(users['UserID'])]
consumer_key = 'ibYsFmAdHS8fhnupeA3opTRHN' $ consumer_secret = 'cSfliluzlYkSJ8EPJEOvQA5kKG9BE6MG1ddF8kHAyNc5ZJ6601' $ access_token = '943311356534472704-3tToKzZ2RMDtNOo4frlY6IEAg6iWGL1' $ access_token_secret = '42jS6EeOwV5ZaWde0LwxpL4dPyozVt5rv3URu7ZlP9m17'
articles.head()
pn_qty = {pn: {'QTY': qty} for pn, qty in itertools.izip(pn_and_qty['PN'], pn_and_qty['QTY'])}
spt = slightly_positive_trump $ spt_sorted = list(spt["time"].sort_values()) $ [spt_sorted[len(spt_sorted)//4], $  spt_sorted[len(spt_sorted)//2], $  spt_sorted[3 * len(spt_sorted)//4]]
DataAPI.write.update_factors(factors=['GROWTH'],trading_days=trading_days, override=True, log=False)
reddit_info.titles.duplicated()
topics = ['thoughts', 'needs_and_volunteering', 'victims', 'donnations', 'floods'] $ topic_words.columns = topics $ display(topic_words.head())
data.drop(['view_count_by_category', 'suma'], axis=1, inplace=True)
stores.head()
idx = df_providers[ (df_providers['id_num']==287560)].index.tolist() $ print('length of IDX',len(idx)) $ assert len(idx) > 0, 'Length of IDX is NULL' $ print( df_providers.loc[idx[0],'name'] ) $
data = clean(path_train, now, cat_cols, num_cols, date_cols, ids_col, label_col) $ print('dimension:', data.shape) $ data.head()
df.tail(20)
churned_df['end_date'] = pd.to_datetime(end_date).strftime('%Y-%m')
learn.sched.plot_loss() # Loss is still going down. We could keep training this guy if necessary
df.loc['r_one'] # first row
eia_gen_annual.head()
test=weekly[:7].sort_values(by=['weekday']) $ plt.figure(figsize=(10,3)) $ plt.plot(test['weekday'],test['DAILY_ENTRIES'])
FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH) $ md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)
reg=KNeighborsRegressor(n_neighbors=5)
train_data["totals.transactionRevenue"] = train_data["totals.transactionRevenue"].astype('float') $ revenue = train_data.groupby("fullVisitorId")["totals.transactionRevenue"].sum().reset_index()
comps_df = comps_df[['entity_uuid','competitor_uuid','company_name_x','company_name_y']]
a = bnbAx[bnbAx['language']=='en'].first_browser.value_counts()/len(bnbAx[bnbAx['language']=='en']) $ a.head()
pickle.dump(lsa_cv, open('iteration1_files/epoch3/lsa_cv.pkl', 'wb'))
tweets_pp = pd.concat([multi.reset_index(), pp.reset_index()], axis=1) $ tweets_pp.drop('index', axis=1, inplace=True) $ tweets_pp.head(10)
len(park)
bb['close'].apply(rank_performance).value_counts().plot()
0 * np.nan
autos.describe(include='all')
data.columns
dfEPEXbase.corr()
gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_
to_be_predicted_Day4 = 50.62613762 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
df_temperature = df_temperature.replace('M', 0) $ df_precipitation = df_precipitation.replace('M', 0)
len(df['user_id'].unique())
xmlData.drop('living_area_unit', axis = 1, inplace = True) $ xmlData.drop('lot_area_unit', axis = 1, inplace = True) $ xmlData.drop('upper_area_unit', axis = 1, inplace = True) $ xmlData.drop('basement_area_unit', axis = 1, inplace = True)
url_CIN = "https://bengals.seasonticketrights.com/Images/Teams/CincinnatiBengals/SalesData/Cincinnati-Bengals-Sales-Data.xls"
df.resample('3D').mean()
site_id_number = Pick_a_site_using_letters() $ site_id_numbers = [site_id_number] $ list_drgs = [870,871,872] $ df_discharges_totals, df_payments_totals =\ $ generate_table_GROUP_sites_drgs(site_id_numbers,list_drgs)
error_set.groupby('converted').nunique()['user_id']/error_set.nunique()['user_id']
out_test['display_address'] = out_test['display_address'].map(lambda x: x.replace('\r','')) $ out_test['street_address'] = out_test['street_address'].map(lambda x: x.replace('\r',''))
tmp["asset_name"] = [re.sub(".zip$", "", x) for x in tmp["data_asset_name"]]
df.describe()
control_converted = df2[df2['group']=="control"]['converted'].mean() $ control_converted
outcome=[0,1] $ old_page_converted=np.random.choice(outcome,n_old, p=(1-p_old,p_old))        $ old_page_converted
precipitation_stations = session.query(Station.station).distinct().all() $ print(str(len(precipitation_stations)))
twitter_archive.info()
df.isnull().values.any()#any missing value
results = model.fit() $ results.summary()
i1 = sp500.index.get_loc('MMM') $ i2 = sp500.index.get_loc('A') $ i1, i2
start_end_points_df = pd.DataFrame(start_end_points, columns = ['Lat0', 'Lat1', 'Lon0', 'Lon1']) $ start_end_points_df.head()
dt_features_test['deadline'] = pd.to_datetime(dt_features_test['deadline'],unit='s')
len(tweet_archive_df[tweet_archive_df.name.isin(['a', 'an', 'None'])][['tweet_id', 'name']])
conn.execute(q)
users_ben = ben_final.dropna(axis=0) $ users_ben.apply(lambda x: sum(x.isna()))
df2.user_id.drop_duplicates(inplace=True)
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=17000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
for f in train_preprocessed.columns: $     print f
df.to_pickle(data_file_path + 'convo_df.pkl') $ print("... saved as pickle")
findNumbers = r'\d+' $ regexResults = re.search(findNumbers, 'not a number, not a number, numbers 2134567890, not a number') $ print(regexResults.group(0))
df_new.reset_index(inplace=True) $ df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new.head()
autos.columns
df.drop(['ARR_DELAY'],axis=1,inplace=True)
csgo_profiles = csgo_profiles.dropna(subset=['total_kills','total_deaths','total_time_played','total_rounds_played', $                                              'total_shots_fired']) $ csgo_profiles.head()
Base.classes.keys()
my_tweet_df["tweet_source"].unique()
print('No. of rows: ',len(df))
comment_sentences = [x.replace("'", "") for x in comment_sentences] $ comment_sentences = [x.replace("-", "") for x in comment_sentences]
data.info()
df.dtypes
del df['Unnamed: 0'] $ del df['index'] $ del df['countyId'] $ del df['helmet'] $ del df['imei']
for station in nullziplist: $     response = gmaps.reverse_geocode(station_geo[station]) $     station_zipcode[station] = response[0]['address_components'][len(response[0]['address_components'])-1]['long_name'] $     time.sleep(2) $ checknullzip(station_zipcode)
import numpy as np $ import pandas as pd $ import datetime as dt $ from datetime import datetime
year4 = driver.find_elements_by_class_name('yr-button')[3] $ year4.click()
p + pd.tseries.offsets.MonthEnd(3)
y_train_pred = model.predict(X_train.as_matrix()) $ utils.metrics(y_train, y_train_pred)
pd.period_range('2020Q1','2020Q4', freq='Q')
projFile = "Projects.csv" $ schedFile = "Schedules.csv" $ budFile = "Budgets.csv"
tweets['full_text'] = tweets['full_text'].str.decode('utf-8')
df2['dategroup'] = df2['date'].apply(date_cat)
sns.distplot(calls_df["length_in_sec"])
sns.countplot(x='approved',data=data, palette='hls') $ plt.show()
df_enhanced = df_enhanced.query('retweeted_status_id == "NaN"')
general_converting = df2.converted.mean() $ print('Probability of an individual converting regardless of the page:') $ print(general_converting)
df = df.drop(columns=['index'])
profits_table = data_df[['ID','Segment','Country','Product','Profit']].copy() $ profits_table.head()
breakdown[breakdown != 0].sort_values().plot( $     kind='bar', title='Russian Trolls Number of Links per Topic' $ );
len(us) + len(notus) == len(geo)
base2 = df2[['placeId', 'hashtags']].groupby('placeId').aggregate(lambda x: [i for l in x for i in l ])
zipfile = "2013_ERCOT_Hourly_Load_Data"
points_dic={"India":345,"Bangladesh":456,"Pakistan":789,"China":90} $ points=pd.Series(points_dic) $ points $
individuals_metadata_df = pd.DataFrame(individuals_metadata_dicts).T $ individuals_metadata_df.head()
df3.sample(5)
df.info()
channel_id == playlist_id
new_page_converted = np.random.binomial(nnew, p = pnew)
1/np.exp(-0.0149), 1/np.exp(-0.0408), np.exp(0.0099)
driver = webdriver.Chrome(executable_path="./chromedriver")
autos['price'].dtype
import statsmodels.api as sm $ logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])
draft_df['license_issue'] = draft_df['words'].apply(mh_to_cat, args = (license_words,)) 
max_length = max(len(l) for l in data['text'].str.split()) $ min_length = min(len(l) for l in data['text'].str.split()) $ print(max_length, min_length)
df_new[['CA','UK','US']]= pd.get_dummies(df_new['country']) $ df_new.head()
%%time $ df['created_at'] = pd.to_datetime(df['Created Date'], format='%m/%d/%Y %I:%M:%S %p')
print('admit: {} rows and {} columns'.format(*admit.shape)) $ print('claims: {} rows and {} columns'.format(*claims.shape))
df_user_extract_copy['user_id'] = df_user_extract_copy['user_id'].astype(str) $ df_user_extract_copy['created_at'] = pd.to_datetime(df_user_extract_copy['created_at']) $ df_user_extract_copy = df_user_extract_copy.drop('Unnamed: 0', axis = 1) $ df_user_extract_copy['location'] = df_user_extract_copy['location'].fillna("not given")
tweets = df["text"].values
twitter_archive_clean=twitter_archive.copy() #creating copy of each data set. $ image_predictions_clean=image_predictions.copy() $ tweet_scores_clean=tweet_scores.copy()
activityByUserByday = firstWeekUserMerged.groupby('userid')["day_time_stamp2"].value_counts().index.tolist() $ activityByUserByday = pd.DataFrame(activityByUserByday,columns=["userid","activeDay"]) $ activityByUserByday = activityByUserByday.groupby('userid')["activeDay"].count().reset_index() $ activityByUserByday.head(5)
%%bash $ aws s3 ls s3://olgabot-maca/lung_cancer/sourmash/ | cut -f 9 -d ' ' | cut -f 1 -d'.' > samples_with_signatures.txt $ ls -lh | grep --fixed-strings --file samples_with_signatures.txt -v | grep log > failed_samples.txt
project_human_raw_df = session.read.format("csv").option("header", "true") \ $                 .option("inferSchema", "true").load( $     "{0}/human_data/projects".format(fs_prefix))
train_col = train.Trainer(model,column_dataloaders['train'],column_dataloaders['val'],optimizer_col)
daily['annual'] = (daily.index - daily.index[0]).days / 365.
dist = np.sum(bag, axis=0) $ dist
reviews.isnull().sum()
autos["seller"].value_counts()
df2[df2['user_id'].duplicated(keep=False)]
git_blame.path.value_counts().head()
print(df_result.head(50))
eegRaw_df.to_csv(outputData, index=False, header=False) 
station_counts = station_df.groupby('Station ID').count().sort_values(by = 'Precip', ascending = False) $ station_counts
all_sets.cards = all_sets.cards.apply(lambda x: pd.read_json(json.dumps(x), orient = "records"))
pd.options.display.max_columns = None $ pd.options.display.max_colwidth = 250 $ pd.options.display.float_format = '{:,.4f}'.format
import pandas as pd $ df_resolved_links = pd.DataFrame(resolved_links) $ df_resolved_links.tail(2)
train = pd.read_csv("labeledTrainData.tsv", header=0, delimiter="\t", quoting=3) $ unlabeled_train = pd.read_csv( "unlabeledTrainData.tsv", header=0, delimiter="\t", quoting=3 ) $ test = pd.read_csv( "testData.tsv", header=0, delimiter="\t", quoting=3 )
df_clean['rating_numerator']=df.text.str.extract('(\d+\.\d+)', expand=True) $ df_clean['rating_numerator']= pd.to_numeric(df_clean['rating_numerator']) $ print (" The text: %s \n The new grade in rating_numerator: %.1f \n The old grade: %.1f" % (df_clean['text'].ix[2326], df_clean['rating_numerator'].ix[2326],df['rating_numerator'].ix[2326])) $ print (" The text: %s \n The new grade in rating_numerator: %.1f \n The old grade: %.1f" % (df_clean['text'].ix[1689], df_clean['rating_numerator'].ix[1689],df['rating_numerator'].ix[1689]))
p = c_conv/(c_conv + c_no_conv) $ p
groupby_breed = df_master.groupby('dog_breed').sum().reset_index() $ groupby_breed.sort_values('rating_numerator',inplace=True,ascending=False) $
results = logit.fit() $ results.summary()
content_counts = Counter(content_words)
A + B
petropavlovsk_filtered = petropavlovsk_data_in_bbox[[vincenty((lat, lon), petropavlovsk_coords).km < 200.0 for lat,lon in zip(petropavlovsk_data_in_bbox.latitude, petropavlovsk_data_in_bbox.longitude)]]
pd.read_html('https://en.wikipedia.org/wiki/Python_(programming_language)', header=0)[1]
from xgboost.sklearn import XGBClassifier $ xgb = XGBClassifier() $ %time xgb.fit(X_train, y_train) $ print('Accuracy:', round(xgb.score(X_test, y_test) * 100, 2), '%')
tweets_master_df.ix[309, 'text']
import tqdm as tqdm $ import os $ import time $ for file in tqdm.tqdm(os.listdir('.')): $     time.sleep(0.5)        
avg_sale_table = data_df[['ID','Segment','Country','Product','Sale Price']].copy() $ avg_sale_table.head()
ab.loc[mismatched.index].isnull().sum()
(autos['date_crawled'] $          .str[:10] $          .value_counts(normalize=True, dropna=False) $          .sort_values(ascending=False) $         )   
random_integers.idxmax(axis=1)
df_city_restaurants.select('name','city','categories').limit(10).toPandas()
df = pd.read_excel('PPB_gang_records_UPDATED_100516.xlsx')
df3 = df2.copy() $ df3['intercept'] = 1 $ df3[['ab_page','old_page']] = pd.get_dummies(df3['landing_page']) $ df3.head(5)
len(calls_nocontact.location.unique())
contractor_clean['last_updated'].head() $
plt.plot(actual, imputed, 'go')
b_cal_q1.columns
fin_r = returns_calc(fin_p)
AAPL.shape
mean_sea_level.plot(kind="kde", figsize=(12, 8))
days = [] $ for day in range(0, num_days_online): $     days.append(day) $ days[0:10]
mean_temp.head()
pd.Series(bnbA.age).isnull().any()
dfX = df2 $ dfX['intercept'] = 1 $ dfX[['X','ab_page']] =pd.get_dummies(dfX['group']) $ dfX = dfX.drop(['X'],axis=1) $ dfX.head()
def label_encoding(df, col_name): $     df[col_name] = c_df[col_name].astype('category') $     df[col_name+"_CAT"] = c_df[col_name].cat.codes $     return $
pattern = re.compile('AA') $ print(pattern.match('AAbc')) $ print(pattern.match('bcAA'))
d2['three'] $ d2
old_page_converted = np.random.choice([0, 1], n_old, p = [p_old, 1-p_old])
sqlContext.sql("select * from RandomTwo").toPandas()
table_name = "Nathan_Carto_SQL_API_test" $ res = sql_api(carto_url, select_all_sql, carto_api_token) $ print(res.text)
df_new['intercept'] = 1 $ lm = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']]) $ results = lm.fit() $ results.summary()
p_new = round(df2['converted'].mean(),4) $ print(p_new)
df[df['Complaint Type'].str.contains("Noise")].head()
display(data.head())
y.value_counts(normalize=False).apply(lambda x: len(y)/(100*x)).to_csv('classweights.csv', header=None)
gucci_ng.plot(kind='barh', figsize=(20,16));
df_merge = pd.read_csv('df_merge.csv')
tweets092815 = tweets092815[['id','text']]
convo2
us['country'] = 'USA' $ us['country'].value_counts(dropna=False)
endpoint_published_models = json.loads(response_get_instance.text).get('entity').get('published_models').get('url') $ print endpoint_published_models
yc.shape
df_moved = pd.DataFrame(rel_list_count)
knn.fit(train[['property_type', 'lat', 'lon']], train['surface_covered_in_m2'])
item_categories.head()
Info_Dataframe(wildfires_df)
 --> now try the regression
sns.kdeplot(pm_final.date_bw_obs[pm_final.status == 1], shade = True).set_title('Gap between observations for failure event')
treatment_cr
np.sum(df["pickup_hour"].isin([7, 8, 9, 10, 11, 12]))
len(d_par)
control_new_pageUU = df2.loc[(df2['group'] == 'control')] $ len(control_new_pageUU[(control_new_pageUU['converted']==1)] )/ control_new_pageUU.shape[0]
import networkx as nx $ import pandas as pd $ import numpy as np $ import pickle
grouped_dpt.size()
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US']]) $ results = log_mod.fit() $ results.summary()
crime_month_window, stops_month_window = stops_vs_crime(zip_1_df,zip_1_sns,'MS','MS') $ crime_two_week_window, stops_month_window = stops_vs_crime(zip_1_df,zip_1_sns,'14D','MS') $ crime_week_window, stops_month_window = stops_vs_crime(zip_1_df,zip_1_sns,'7D','MS') $ crime_day_window,stops_month_window = stops_vs_crime(zip_1_df,zip_1_sns,'D','MS')
bloomfield_pothole_data.info()
new_page_converted=np.random.choice([0,1],size=nnew[0],p=[pnew,1-pnew]) $ new_page_converted = new_page_converted[:145274] $ print(len(new_page_converted))
%%R $ AirportCode('Chicago')
print(df.tail()) 
df['Complaint Type'].groupby(by=df.index.month).value_counts()
customer_emails = sales_data_clean[['Email', 'Paid at']].drop_duplicates() $ customer_emails.dropna(inplace=True)
pclass2_survived = df_titanic.loc[df_titanic['pclass'] == 2, 'survived'] $ print(pclass2_survived.value_counts()) $ pclass2_survived.value_counts().plot(kind='pie', autopct='%.2f', fontsize=20, figsize=(6, 6))
df.sum(0)
model.load_weights(MODEL_WEIGHTS_FILE) $ loss, accuracy = model.evaluate([q1_data1, q2_data2], labels_, verbose=0) $ print('loss = {0:.4f}, accuracy = {1:.4f}'.format(loss, accuracy))
converted_df = df.query('converted==1') $ converted_df.user_id.nunique()/df.user_id.nunique()
data.head()
new_model =  gensim.models.KeyedVectors.load_word2vec_format(path_database+'lesk2vec.bin', binary=True)
popular_trg_df = popular_trg_df[~(popular_trg_df["name"] == "#AusOpen")]
tm_2050 = pd.read_csv('input/data/trans_2050_m.csv', encoding='utf8', index_col=0)
ideas.dtypes  # Inspecting progress
(df2['landing_page']== 'new_page').mean()
from sklearn.model_selection import train_test_split
trip_data["hours"] = trip_data.lpep_pickup_datetime.apply(lambda x: pd.to_datetime(x, infer_datetime_format=False).hour)
import os # To use command line like instructions $ if not os.path.exists(DirSaveOutput): os.makedirs(DirSaveOutput)
from geopy.geocoders import Nominatim $ geolocator = Nominatim() $ location = geolocator.reverse("40.735806, -73.985437") $ print(location)
stats.apply(np.median)
print('Unique number of users notified: {}'.format(len(atdist_opp_dist[atdist_opp_dist['infoIncluded']]['vendorId'].unique())))
cv = CountVectorizer(lowercase=True) $ data_name = cv.fit_transform(review_df.author) $ uniname_array = data_name.toarray() $ name_array = uniname_array.sum(axis=1)
ggplot(twitter_df_clean, aes(x='favorite_count', y='retweet_count')) +\ $     geom_jitter() +\ $     xlim(0,40000) +\ $     ylim(0,25000) +\ $     ggtitle("favorite_count to retweet_count corelation") 
df_person = pd.read_sql_query("SELECT * FROM person", conn) $ df_grades = pd.read_sql_query("SELECT * FROM grades", conn) $ df_person.merge(df_grades, how="left", left_on = "id", right_on="person_id")
id_list = [x for x in sat_spike['id']] $ tweets = pd.DataFrame() $ spike_tweets = pd.read_csv('df_tweets.csv', low_memory=False, usecols=['id', 'full_text', 'user_name', 'hashtags']) $ spike_tweets = spike_tweets[spike_tweets['id'].isin(id_list)]
df_posts
df3 = df2
import pandas as pd $ %time hundred_stocks_df = pd.read_csv('../data/hundred_stocks_twoyears_daily_bar.csv')
fish.stack()
df = df[(df.yearOfRegistration >= 1990) & (df.yearOfRegistration < 2017)] $ df = df[(df.price >= 100) & (df.price <= 100000)] $ df = df[(df.powerPS < 600)]   $ print "Dimensiones de dataset acotado: ",df.shape
pd.merge(left, right, on = 'key')
df_user_count = df_stars.groupby('user_id').size() $ print(df_user_count )
merged = pd.merge(prop, contribs, on="calaccess_committee_id")
tizibika.info()
compound_sentiment=sentiment_df.groupby("Company").mean()["compound"] $ company_data=pd.DataFrame(compound_sentiment)
df[df['Descriptor'] == 'Pothole'].resample('M').count().head()
results.summary()
p = pd.Period('2014-07-01 09:00', freq='H')
last_hours.to_period()
%matplotlib inline $ commits_per_year.plot(kind="line", title="Commits Per Year", legend=False)
transactions.join(users.set_index('UserID'), on='UserID', how = 'inner') $
df[df['Descriptor'] == 'Loud Music/Party'].index.weekday.value_counts()
datAll.shape
clf = decomposition.NMF(n_components=6, random_state=1)
score_a = score[(score["score"] < 100) & (score["score"] >= 90)] $ score_a.shape[0]
all_data.index.is_unique
userID = np.array(users["id"]) $ my_solution = pd.DataFrame(my_prediction, userID, columns = ["country_destination"]) $ print(my_solution)
x = [0, 1, 2, 3] $ y = [tUnderweight, tNormal, tOverweight, tObese] $ plt.bar(x, y, 0.5) # call the plot $ plt.show()
run txt2pdf.py -o "2018-06-12-1308 FLORIDA HOSPITAL title_page.pdf"  "2018-06-12-1308 FLORIDA HOSPITAL title_page.txt"
df = pd.read_csv('GageData.csv', dtype={'site_no':'str'}) 
n_old=df2.query('group == "control"').shape[0] $ n_old
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
count_vectorizer = CountVectorizer(min_df = 10, ngram_range=(1, 2), max_df=.5, $                                    stop_words=nltk_stopwords, token_pattern="\\b[a-z][a-z]+\\b") $ count_vectorizer.fit(final_tweets)
pd.set_option('max_colwidth',150) $ df_en['text']
autos.describe(include='all') $
twelve_months = session.query(Measurements.date, Measurements.prcp).filter(Measurements.date > year_before) $ twelve_months_prcp = pd.read_sql_query(twelve_months.statement, engine, index_col = 'date')
print(q7a_answer)
intr = df[y_col].value_counts(1).head(50) $ plot = intr.plot(kind='bar', figsize=(16,8)); $ plot.set_xticklabels(intr.index, {'rotation' : 90});
gbm_predictions = pd.concat([pd.Series(gbm_pred, index=y_test.index, name='Predictions'), y_test], axis=1) $ gbm_predictions.head()
df['min'] = df.index - datetime.timedelta(seconds=1) $ df['max'] = df.index + datetime.timedelta(seconds=1) $ df.head()
autos['odometer_km'].unique().shape
len(pscore)
bday = datetime(1991, 5, 1).toordinal() $ date_now.toordinal() - bday
thisWeek = pd.concat([dummy,thisWeek.iloc[:]]).reset_index(drop=True)
people.shape
temp_df = segment_reorder_cadence()
df2.head(2)
summary = records.describe(include='all') $ summary
df2['country'].unique()
df_countries = pd.read_csv('countries.csv') $ df_countries.head()
industries = pd.read_csv("industries.csv")
obj
top_songs.head()
help(fahr_to_kelvin)
room_temp.temperature =3
from h2o.automl import H2OAutoML $
pres_df['ad_length'].describe()
from ggplot import *
p_old = round(p_all,4) $ p_old
df.tail(5)
test_data.head()
test_orders_prodfill_final=test_orders_prodfill[test_orders_prodfill['reordered']==1] $ test_orders_prodfill_final.head()
print(sorted(df_mes['RatecodeID'].unique()))
X, y = shuffle(X, y, random_state = 123)
for df in (joined, joined_test): $     for c in df.columns: $         if c.endswith('_y'): $             if c in df.columns: df.drop(c, inplace=True, axis=1)
mgxs_lib = openmc.mgxs.Library.load_from_file(filename='mgxs', directory='mgxs')
two_day_sample.groupby('date')[['steps']].apply(compute_last_step)
main_deduped = dedupe(main_subset.filter(lambda p: p.get("clientId") in all_clients), "id") $ main_deduped_count = main_deduped.count()
raw_data[:5]
import pandas as pd $ data_for_model = pd.read_pickle('data_for_model')
prop57=props[props.prop_name=="PROPOSITION 057 - CRIMINAL SENTENCES. JUVENILE CRIMINAL PROCEEDINGS AND SENTENCING. INITIATIVE CONSTITUTIONAL AMENDMENT AND STATUTE."]
selected_feature
from sklearn.feature_extraction.text import TfidfVectorizer $ vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words="english")
serious_count = 0 $ for row in data: $     if re.search("\[[Ss]erious\]", row[0]): $         serious_count = serious_count + 1 $ print(serious_count)
to_be_predicted_Day5 = 14.85524742 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
month3 = oanda.get_history(instrument_1, $                         start = '2018-3-1', $                         end = '2018-3-31', $                         granularity = 'M10', $                         price = 'A')
foxnews_df = constructDF("@FoxNews") $ display(constructDF("@FoxNews").head())
sp500 = pd.read_csv('sp500.csv', index_col='Symbol', usecols=[0,2,3,7]) $ sp500.head()
(df['converted'] == 1).mean()
df2['intercept'] = 1 $ df2['ab_page'] = np.where(df2['group'] == 'control', 0, 1) $ df2.tail()
nypd = df[df['Agency'] == 'NYPD'] $ dot = df[df['Agency'] == 'DOT'] $ ax = nypd.groupby(by=nypd.index.hour).count().plot(y='Unique Key', label='NYPD') $ dot.groupby(by=dot.index.hour).count().plot(y='Unique Key', label='DOT', ax=ax)
learner.save('lm1')
dt_christmas = datetime(2017,12,25) $ print dt_christmas
df = pd.read_csv('ab_data.csv') $ print(df.head())
for t in tables: display(DataFrameSummary(t).summary())
df['release_dt'] = pd.to_datetime(df['release_dt']) $ df['booking_dt'] = pd.to_datetime(df['booking_dt'])
mv_lens = pd.merge(movies, ratings)
df_geo_unique_asn = df_geo_unique[['a_saddr_asn','a_saddr_asorg','a_saddr_lat','a_saddr_long', $                                    'b_saddr_asn','b_saddr_asorg','b_saddr_lat','b_saddr_long']].drop_duplicates()
rng2 = pd.date_range(start='4/7/2018', end='4/8/2018', freq='3H') $ rng2 $
autos.head()
import numpy as np $ from sklearn.linear_model import LogisticRegression
df_providers_pared = df_providers_pared[['id_num', 'drg', 'name', 'street', 'city', 'state', 'zipcode', 'region', $        'discharges', 'avg_charges', 'total_payment', 'medicare_payment', $        'year', 'drg3', 'disc_times_pay', 'Zip', 'zip_length']] $ df_providers_pared.head()
df2['intercept']=1 $ df2[['control', 'ab_page']]=pd.get_dummies(df2['group']) $
twitter_archive_full.drop(twitter_archive_full[(twitter_archive_full.tweet_id.isin(duplicated_list)) & $                                               (twitter_archive_full.stage != 'doggo')].index, inplace=True)
df[df['Descriptor'] == 'Loud Music/Party'].groupby(by=df[df['Descriptor'] == 'Loud Music/Party'].index.hour).count().plot(y='Agency')
vs=len(itos); vs
log_loss(y_train, y_hat)
print_models(models_tree)
results = five_number_summary(df['y']) $ results
countries_df = pd.read_csv('./countries.csv') $ df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']] $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
TestIndex = SampleIndex[NumTrain:] $ StockData.loc[TestIndex, 'Set'] = 'test'
import matplotlib.pyplot as plt $ %matplotlib inline
plt.plot(MaxPercentage) $ plt.plot(RandomPercentage) $ plt.ylabel("percentual change used for profit") $ plt.xlabel("time") $ plt.show()
df_ind_site = df_providers[['id_num','drg3','discharges','disc_times_pay','year']].sort_values(['drg3','year'],\ $               ascending=[True,True]) $ df_ind_site.head()
treatment_set, treatment_mapping, max_treatment_length = parse_treatment_definitons(open("data/treatment_definitons.txt", 'r'))
autos.head() $
train.num_points.describe()
y = w * x + b $ loss = K.mean(K.square(y-target))
feature_cols = list(train.columns[7:-1]) $ feature_cols
countries = pd.read_csv('countries.csv') $ countries.head()
autos["odometer_km"].describe()
print adjmats[0,:,:].squeeze() $ print true
s = pd.Series(['Tom', 'William Rick', 'John', 'Alber@t', np.nan, '1234','SteveSmith']) $ s
print('The number of rows in the dataset is {}.'.format(df.shape[0]))
CONSUMER_KEY    = '' $ CONSUMER_SECRET = '' $ ACCESS_TOKEN  = '' $ ACCESS_SECRET = ''
image_df_tomerge = image_df_clean.loc[:, ['tweet_id', 'prediction_1', 'prediction_1_confidence','prediction_1_result']] $ image_df_tomerge['tweet_id'] = image_df_tomerge['tweet_id'].astype(str) $ twitter_df_merged = pd.merge(pd.merge(archive_df_clean, status_df, on='tweet_id'), image_df_tomerge, on='tweet_id')#pd.merge(status_df, image_df_clean, on = 'tweet_id', how='inner')
df.head() $
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller') $ z_score, p_value
df_final.shape[0]
print(cvModel1.bestModel._java_obj.getNumTrees()) $ print(cvModel1.bestModel._java_obj.getMaxDepth()) $ print(cvModel1.bestModel._java_obj.getMaxBins())
df_cod2 = df_cod.copy() $ df_cod2 = df_cod2.dropna()
con = presto.Connection(host=cred['host_presto'], port=80)
df_new['US_page'] = df_new['US'] * df_new['ab_page'] $ df_new['UK_page'] = df_new['UK'] * df_new['ab_page'] $ df_new.head()
dyn_data = hp.get_data_dynamic(sensortype='electricity', head=head, tail=tail)
df2.head()
df_categorical = df_categorical.applymap(clean_string)
joined_logit_mod = sm.Logit(joined['converted'], joined[['intercept', 'US', 'CA']]) $ country_result = joined_logit_mod.fit()
trump.dtypes
index_of_wrong_mapped_cols = df.query('(group=="treatment" and landing_page!="new_page") or (group=="control" and landing_page!="old_page")').index
y_.median()
model_uid = client.repository.get_model_uid(model_details) $ print(model_uid)
tweet_archive_clean[tweet_archive_clean['rating_denominator'] < 10] $
crime_month_window_, stops_month_window_ = stops_vs_crime(zip_2_df,zip_2_sns,'MS','MS') $ crime_two_week_window_, stops_month_window_ = stops_vs_crime(zip_2_df,zip_2_sns,'14D','MS') $ crime_week_window_, stops_month_window_ = stops_vs_crime(zip_2_df,zip_2_sns,'7D','MS') $ crime_day_window_,stops_month_window_ = stops_vs_crime(zip_2_df,zip_2_sns,'D','MS')
remove_cols = [ 'img_num','p1', 'p1_conf', 'p1_dog', 'p2', 'p2_conf', 'p2_dog', 'p3', 'p3_conf', 'p3_dog' ] $ images_copy.drop (remove_cols, axis =1 , inplace= True) $ images_copy.tail()
s519397_df = pd.DataFrame(s519397) $ print(len(s519397_df.index)) $ s519397_df.info()
DummyDataframe2 = DummyDataframe2.apply(lambda x: update_values_category(x, "Tokens"), axis=1) $ DummyDataframe2
reviewsDF.dtypes
countries_df = pd.read_csv('countries.csv') $ ab_df_new = countries_df.set_index('user_id').join(ab_df2.set_index('user_id'), how='inner')
w.get_step_object(step = 2, subset = subset_uuid).load_indicator_settings_filters()
from __future__ import print_function $ import pandas as pd $ import numpy as np $ import io $ import os
f_ip_device_clicks.show(1)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], $                            [n_new, n_old], $                            value=None, alternative='larger', prop_var=False) $ z_score, p_value
Quandl_DF['Month'] = Quandl_DF['Date'].dt.month $ Quandl_DF['Year'] = Quandl_DF['Date'].dt.year $ Quandl_DF['WeekNo'] = Quandl_DF['Date'].dt.week
import sys $ list(sys.modules.keys())[:20]
lower_case = letters_only.lower()        # Convert to lower case $ words = lower_case.split()               # Split into words
census_tracts_df = pd.read_csv('./Datasets/Census Tracts.csv') $ census_tracts_df.head()
import pickle $ p_file = 'data/test.p' $ with open(p_file, 'wb') as fout: $     pickle.dump(airports, fout)
USvideos = pd.read_csv('data/USvideos_no_desc.csv', parse_dates=['trending_date', 'publish_time'])
filtered_tweets = [] $ for tweet in collect.get_iterator(): $     filtered_tweets.append(tweet) $ len(filtered_tweets)
cm = metrics.confusion_matrix(y_train, res_train) $ print(cm) $ print(classification_report(y_pred=res_train,y_true=y_train)) $ print(np.round(f1_score(y_pred=res_train,y_true=y_train),3))
d - pd.tseries.offsets.Week()
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=26500) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
transactions.merge(users, how='outer', on='UserID')
join_c.registerTempTable("x") $ predictions_table = sqlContext.sql("SELECT party_id_orig,aggregated_prediction,predicted  FROM( SELECT *, ROW_NUMBER()OVER(PARTITION BY party_id_orig ORDER BY aggregated_prediction DESC) rn FROM x) y WHERE rn = 1").cache()
raw[raw.job_type == 'Sponsored'].hash.value_counts()[0:5]
intervention_train.reset_index(inplace=True) $ intervention_train.set_index(['INSTANCE_ID', 'CRE_DATE_GZL', 'INCIDENT_NUMBER'], inplace=True)
df_productIds
pd.Series([4, np.nan, 7, np.nan, -3, 2]).sort_values()
autos = autos[(autos["registration_year"]>=1886)&(autos["registration_year"]<=2016)] $ autos.head()
url = "http://sealevel.colorado.edu/files/2015_rel2/sl_ns_global.txt" $ global_sea_level = pd.read_table(url, sep="\s+") $ global_sea_level
decoder_model_inference = extract_decoder_model() $ decoder_model_inference.summary()
xmlData.head(5)
df_user_extract_copy.info()
df2 = df2.drop("treatment", axis=1)
reddit['comm_range'].value_counts()
snake_case = ['date_crawled', 'name', 'seller', 'offer_type', 'price', 'ab_test', 'vehicle_type', $               'registration_year', 'gear_box', 'power_PS', 'model', 'odometer', $               'registration_month', 'fuel_type', 'brand', 'unrepaired_damage', 'ad_created', $               'nr_of_pictures', 'postal_code', 'last_seen']
print(sum(vip_pivot_table['Payment']))
tmdb_movies.head()
email_age_unique = discover_data[['email', 'request.ageRange']].drop_duplicates(keep='last') $ age_range_breakdown = email_age_unique.groupby('request.ageRange').count() #apply(lambda x: 100 * x / x.sum()) $ age_range_breakdown['Percentage'] =age_range_breakdown.apply(lambda x: 100 * x / x.sum()) $ age_range_breakdown # age_range_breakdown['Percentage'] = age_range_breakdown['email'].map(lambda x: 100 * x / x.sum())
%%sql $ UPDATE facts $ SET Last_Update_Date_key = hour.hour_key $ FROM hour $ WHERE hour.hour  = TO_CHAR(facts.Last_Update_Date, 'YYYY-MM-DD HH24:00:00')
raw_data[['charged', 'objective']].loc[raw_data.charged > 0].groupby('objective').mean()
gb_page = df2.groupby(['landing_page']).count() $ gb_page
nba_df.loc[nba_df.loc[:, "Tm.Pts"] > 145, "Team"].tolist()
df.sort_values('total_value', ascending=False, inplace=True) $ display(df.head(n=10))
df.describe(include=['object'])
df_mysql = psql.read_sql('select * from test.test_table;', con=conn)
aapl = quandl.get('WIKI/AAPL') $ aapl.head()
data_words_nostops = remove_stopwords(data_words) $ data_words_bigrams = make_bigrams(data_words_nostops) $ nlp = spacy.load('en') $ data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']) $ print(data_lemmatized[:5])
df_onc_no_metac['ONC_LATEST_N'] = df_onc_no_metac['ONC_LATEST_N'].replace({'N0(i-)': 'N0(i_minus)', 'N0(i+)': 'N0(i_plus)'}) $ df_onc_no_metac['ONC_LATEST_N'].nunique()
print len(y_train) $ y_train = y_train.reshape(64155,1)
model_df['true_grow'] = model_df[['target', 'next_day_open']].apply(lambda x: 1 if x[0] - x[1] >= 0 else 0, axis=1)
df.loc[:, 'username'] = df.loc[:, 'username'].str[1:] 
type(ds) $ ds.keys()
X_train, X_test, y_train, y_test = train_test_split(X_, y_q, test_size=0.2, random_state=2)
df2.query('is_duplicated==True')
df.head()
df.info()
filtered_tweets_df.to_csv("../../data/clean/filtered_tweets.csv",index=False)
lm = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ r = lm.fit()
print("The number of unique users in the dataset is:  " + str(df['user_id'].nunique()))
twitter_ar['source_url'] = twitter_ar['source'].apply(lambda row: row.split('rel')[0]) $ twitter_ar['source_type'] = twitter_ar['source'].apply(lambda row: row.split('rel')[1])
overallMoSold = pd.get_dummies(dfFull.MoSold)
perm_replicates = draw_perm_reps(df2_treatment.converted, df2_control.converted, $                                  diff_of_means, size=10000) $ p = (perm_replicates >= obs_diff).mean() $ print('p-value =', p)
test_float_pca = test_float.loc[:, ['surface_total_in_m2', 'surface_covered_in_m2',\ $                                     'lat', 'lon']] $
polarity_avg.rename(columns={'index':'canton'}, inplace=True) $ polarity_count.rename(columns={'index':'canton'}, inplace=True)
y = df_selection['Shop_Status'] $ X = df_selection.drop('Shop_Status',axis=1)
prob1_ZeroFill = pd.Series(x[1] for x in prob_ZeroFill) $ prob1_ZeroFill.head()
df_new['intercept'] = 1 $ df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop('CA', axis=1) $ df_new.head()
test_text3 = df['text'][11] $ print(test_text3) $ re.sub(r'[:=;] [oO\-]?[D\)\]\(\]/\\OpP]', '', test_text3)
soup = BeautifulSoup(page_html, "lxml")
files = [local_orig, local_edit] $ for file in files: $     with rio.open(file, 'r') as src: $         profile = src.profile $         print(profile)
quarters.asfreq("A")
top_supporters['contributor_cleanname'] = top_supporters.apply(combine_names, axis=1)
Z = np.random.randint(0,3,(3,10)) $ print((~Z.any(axis=0)).any())
def add_constant_index_level(df: pd.DataFrame, value: Any, level_name: str): $     return pd.concat([df], keys=[value], names=[level_name]) $ df = add_constant_index_level(df, "Booooze", "Department") $ df = df.reorder_levels(order=['Date', 'Store', 'Department', 'Category', 'Subcategory', 'UPC EAN', 'Description']) $ df.head(3) $
p_control_converted = df2.query('group == "control"')['converted'].mean() $ p_control_converted
df_vow.head()
import statsmodels.api as sm $ convert_old = df2[(df2['group'] == 'control') & (df2['converted'] == 1)]['user_id'].count() $ convert_new = df2[(df2['group'] == 'treatment') & (df2['converted'] == 1)]['user_id'].count() $
grades = grades.drop(grades.index[(grades.Mark == 0) & (grades.ATAR == 0)]) $ noatar = grades[(grades.ATAR == 0)] $ noatar.Mark.hist(bins=25)
uber_14["day_of_month"].value_counts().head()
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])
Grouping_Year_DRG_discharges_payments_for_specific_year = \ $    Grouping_Year_DRG_discharges_payments.loc[the_year] $ Grouping_Year_DRG_discharges_payments_for_specific_year.head()
xml_in_sample['authorId'].nunique()
df_ad_airings_5.head(2)
test1.shape, test2.shape
daily = data.resample('D').sum() $ daily.rolling(30, center=True).sum().plot(style=[':', '--', '-']) $ plt.ylabel('mean hourly count');
combined_df4.keys()
result = pd.merge(df_pivot2,df_pat, how='inner', on=['id_ndaj1']) $ result.shape
name = 'specs210618' $ full_path_to_file = 'C:\\Users\\lukas\\PycharmProjects\\monimporter\\specs210618.json'
df = pd.read_csv('twitter_archive_master.csv')
from jira.client import JIRA $ options = {'server': 'https://intertec.atlassian.net'} $ jp = JIRA(options=options, basic_auth = ('Carlos.Sell', 'Nadia#663s494'))
print(convert_old, convert_new, n_old, n_new) $ z_score, pval = sm.stats.proportions_ztest([convert_new, convert_old],[n_new, n_old], alternative='larger') $ print('Zscore: {:.3f}\np-val: {:.4f}'.format(z_score, pval))
Active_stations[0][0:2]
for dataset in combine: $     dataset['Age*Class'] = dataset.Age * dataset.Pclass $ train_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)
ship_hash = itemTable['Content'].str.contains("ship") $ print ("Total Shipments : " + str(ship_hash.sum()))
from sklearn.datasets import california_housing $ data = california_housing.fetch_california_housing()
ffr.index.day
PARTITION = '100' $ BASE_DIR = 's3://customer-churn-spark/' $ PARTITION_DIR = BASE_DIR + 'p' + PARTITION
gdax_trans_btc['Timestamp'] = gdax_trans_btc['Timestamp'].map(lambda x: x.replace(second=0))
top_songs.rename(columns={'Region': 'Country'}, inplace=True)
weather_df = pd.read_sql('select * from weather limit 10;', conn)
X = pd.concat([pd.get_dummies(Fraud_Data[['country','source','browser']]),data[['purchase_value','difference','usage_count']]],axis=1) $ y = data['class'] $ X.head()
negative_topic_dataferame=topic(negative_list,num_topics=20)
def tracking_error(benchmark_returns_by_date, etf_returns_by_date): $     assert benchmark_returns_by_date.index.equals(etf_returns_by_date.index) $     tracking_error = np.sqrt(252) * np.std((etf_returns_by_date - benchmark_returns_by_date), ddof=1)  $     return tracking_error $ project_tests.test_tracking_error(tracking_error)
url_p1 = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=' $ url = url_p1 + API_KEY $ r = requests.get(url) $ data = r.json()
df_joy.hist() $ plt.show()
def getTopics(row): $     newsletters = getNewsletterList(row) $     return sumTopics(newsletters)
raw_full_df.bedrooms.value_counts()
ufos_df = spark_df.toPandas()
p_diff_ab = (pct_conv_trt/trt_total['user_id']) - (pct_conv_cont/cont_total['user_id']) $ print(p_diff_ab)
search['search_weekday'] =  search.timestamp.dt.dayofweek+1 $ search['trip_start_date_weekday'] =  search.trip_start_date.dt.dayofweek+1 $ search['trip_end_date_weekday'] =  search.trip_end_date.dt.dayofweek+1
token["month"] = token.date.map(lambda x : x.year*12 + x.month)
pd.value_counts(ac['Status'].values, sort=True, ascending=False)
wb.save('most_excellent.xlsx')
from nltk.sentiment.vader import SentimentIntensityAnalyzer $ from nltk.corpus import stopwords $ sid = SentimentIntensityAnalyzer()
sample_submission = read_csv_zip("data/sample_submission_NDF.csv")
dfpf.collect()
dfFull.BsmtFinSF2 = dfFull.BsmtFinSF2.fillna(dfFull.BsmtFinSF2.mean())
df_datecols = df_uro.select_dtypes(include=['<M8[ns]']).drop(columns=['index_date', 'lookback_date']) $ df_datecols.shape
access_logs_raw.count()
df_questionable_3[df_questionable_3['state_MI'] == 1]['link.domain_resolved'].value_counts()
with open('data/kochbar_12.json') as data_file:    $     kochbar12 = json.load(data_file) $ koch12df = preprocess(kochbar12) $ koch12df.info()
df
tweets["retweet"].sum()/len(tweets.retweet)
fin_df.dog_stage.value_counts()
print('band width between first 2 bands =',(wavelengths.value[1]-wavelengths.value[0]),'nm') $ print('band width between last 2 bands =',(wavelengths.value[-1]-wavelengths.value[-2]),'nm')
timePassed
print pearsonr(weather.max_wind_Speed_mph[weather.max_gust_speed_mph >= 0], $                weather.max_gust_speed_mph[weather.max_gust_speed_mph >= 0])
import pandas as pd $ url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls' $ xl = pd.read_excel(url, sheetname = None) $ print(xl.keys()) $ print(xl['1700'].head())
def filter_columns(row, all_cards_cols): $     set_cols = list(row.columns) $     intersection = list(set(set_cols) & set(all_cards_cols)) $     return row.filter(intersection) $ only_cards = only_cards.apply(lambda x: filter_columns(x, all_cards_columns))
test_stemmed
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, 1-p_new]).mean() $ new_page_converted
df_gnis_test = df_gnis.dropna(axis=0, subset=['PRIM_LAT_DEC','PRIM_LONG_DEC'],thresh=1) $ df_gnis_test.shape
parsed_events[parsed_events.swimstyle.isnull()]
txSenate = tx[tx['filerHoldOfficeCd'].isin(['STATESEN'])][tx['filerFilerpersStatusCd'] != "NOT_OFFICEHOLDER"] $ txHouse = tx[tx['filerHoldOfficeCd'].isin(['STATEREP'])][tx['filerFilerpersStatusCd'] != "NOT_OFFICEHOLDER"]
df = pd.DataFrame({'temp':pd.Series(28 + 10*np.random.randn(10)), $                    'rain':pd.Series(100 + 50*np.random.randn(10)), $                    'location':list('AAAAABBBBB') $ }) $ print(df.head(2))
ab_dataframe['user_id'].nunique()
one_week_in_seconds = 60*60*24*7 $ for comment in eligible_comments: $     comment['first.comment.week.diff'] = int((current_time - comment['created']).total_seconds() / $                                              one_week_in_seconds) $
import sqlite3 $ import pymysql
df = pd.DataFrame.from_csv("metoo_full_backup_3M.csv")
from config import (consumer_key, consumer_secret, access_token, access_token_secret)
all_data_wide['pagecount_all_views'] = all_data_wide['pagecount_desktop_views'].fillna(0) + all_data_wide['pagecount_mobile_views'].fillna(0) $ all_data_wide['pageview_all_views'] = all_data_wide['pageview_desktop_views'].fillna(0) + all_data_wide['pageview_mobile_views'].fillna(0) $ all_data_wide[:3]
df_unique_providers.to_csv('Unique Providers December 31, 2017.csv')
borehole.to_sql(con=engine, name='borehole', if_exists='replace', flavor='mysql',index=False)
crime_10H40_10_17 = sel_df[sel_df.Beat =='10H40'] $ crime_10H10_10_17 = sel_df[sel_df.Beat =='10H10'] $ crime_10H60_10_17 = sel_df[sel_df.Beat =='10H60'] $ crime_10H70_10_17 = sel_df[sel_df.Beat =='10H70']
xgb_learner = XGBLearner(dtrain, evals, True, 0.01, 'binary:logistic', 'auc', seed=7)
autos['gearbox'].unique()
not_fit = ['pp_xoffset_min', 'pp_yoffset_min', 'pp_xoffset_max', 'pp_yoffset_max', 'tstamp', $           'collector_tstamp_x', 'etl_tstamp', 'collector_tstamp_y', 'dvce_created_tstamp', 'event', 'domain_userid', $           'domain_sessionid'] $ consistent_with_action = ['action_label', 'action_type'] $ click_condition_meta.drop(not_fit + consistent_with_action, axis = 1, inplace = True)
consumer_key = '' $ consumer_secret = '' $ access_key = '' $ access_secret = ''
data_test = data_s[data_s['date'] < datetime(2017,1,1)] $ data_test['date'].max()
df_gauge = df.set_index("posted_date") $ df_gauge = df_gauge.resample("1d").sum().fillna(0).rolling(window=3, min_periods=1).mean() $ df_gauge.reset_index(inplace=True)
yr15 = likes[likes.year == 2015] $ len(yr15)
df_img_algo_clean = df_img_algo.copy() $ df_img_algo_clean.head()
Lda = gensim.models.ldamodel.LdaModel $ ldamodel = Lda(corpus=doc_term_matrix, id2word=dictionary, num_topics=topics)
com_grp.agg(np.mean)
closingPrices = dataFrame['Close']
df2 = df2.drop(1899, axis=0)
b_cal_q1.loc[:,'date'] = pd.to_datetime(b_cal_q1['date'])
scores.describe()
df_loc = df_loc.loc[:,['country','input_string']]
festivals.at[2,['latitude']] = 41.6756732 $ print((festivals.at[2,'latitude'])) $ print((festivals.at[2,'longitude'])) $ festivals.head(3)
my_refinedquery = "SELECT name, id, ephys_roi_result_id FROM specimens LIMIT 5" $ refined_df = get_lims_dataframe(my_refinedquery) $ refined_df.tail()
girls_shuffled = girls_by_name.sort_values('Year') $ girls_shuffled.head()
twitter_ar.text[1]
df.index.get_level_values('Subcategory').unique() $
pd.read_csv("Data/microbiome.csv", nrows=4)
full_data.dtypes
twitter_archive = pd.read_csv ('C:\\Users\\Teresa\\twitter-archive-enhanced.csv') 
control_df2=df2.query('group=="control"') $ P_c=control_df2['converted'].mean() $ print("Given that an individual was in the control group,the probability they converted is {}".format(P_c))
random_labels = random_sample['labels'].tolist()
df_new.groupby('country').mean()['converted']
geometry = openmc.Geometry(root_universe)
dat.describe()
rng = np.random.RandomState(42) $ ser = pd.Series(rng.rand(5)) $ ser
glm_model.std_coef_plot(num_of_features = 10) $ print(glm_model.confusion_matrix(valid = True))
save_filepath = '/media/sf_pysumma'
sns.distplot((df['kilometer']),hist=False, color="g",kde_kws={"shade": True}) $ plt.xlabel("kilometer " , fontsize=20) $ plt.ylabel("percentage" , fontsize=20) $ plt.title('Vehicle kilometers', fontsize=20)
df2 = PredClass.date_cut(df, '2017-10-01') $ df2 = df $ X, y = make_x_y(df2)
print(reframed.shape) $ print(bitcoin_market_info.shape)
X_train_feature_counts
len(results)
bad_rows
new_cont = df.query("landing_page == 'new_page' and group == 'control'") $ old_trt = df.query("landing_page == 'old_page' and group == 'treatment'") $ len(new_cont)+len(old_trt)
titanic_df['cabin'] = titanic_df['cabin'].apply( $     lambda x: None if not x else str(x).split()[0])
treatment_old.index
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_ipb_chord_crown[0][0])) $ perr = np.sqrt(np.diag(pcov_ipb_chord_crown[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
wed11 = wed11.drop('id', axis=1)
import seaborn as sns $ import tweepy as tp $ import pandas as pd
last_date = dt.date(2017,8,23) - dt.timedelta(days=365) $ Recent_Measurements = session.query(Measurement.date, Measurement.prcp).\ $ filter(Measurement.date >= '2016-08-23').all()
subred_num_avg.head(10)
df2.user_id.duplicated().sum()
conn.rollback()
total_df['Review_count'].plot.hist(bins = 20) $ plt.title('Overall Review Count Distribution') $ plt.ylabel("Frequency") $ plt.xlabel("Review Count")
y
heatmap(ddf) $ plt.show()
print(z_score, p_value)
def calc_polarity(i): $     tweet = TextBlob(dataset['Text'][i]) $     return tweet.polarity $ dataset['Sent_Polarity'] = [calc_polarity(i) for i in range(len(dataset))]
results_Jarvis, out_file3 = S.execute(run_suffix="Jarvis_hs", run_option = 'local')
df.set_index('Timestamp', inplace=True)
ans = pd.pivot_table(df, values='E', index=['A','B'], columns=['C']) $ ans
import re $ pattern = re.compile('\$\d*\.\d{2}') $ result = pattern.match('$17.89') $ print(result) $ print(bool(result))
BDAY_PAIR_df.pair_age.describe()
busy_three=stations_des[0:3] $ print("The three most active stations and their associated counts:",busy_three)
!pwd
pd.datetime.today() - pd.Timestamp('1/1/1970')
lr = 0.0005 $ lrs = lr
print(d)
import statsmodels.api as sm $ df2['intercept']=1 $ df2[['aa_page','ab_page']]=pd.get_dummies(df2['group']) $ df2.head()
df['gts'].max() #Gross ticket sales??
df_converted = df[df['converted'] == 1] $ df_converted.shape[0] / df['user_id'].nunique() $
df.columns
returns = returns_raw.set_index('group_name')
offices.info()
df2.loc[df2.user_id.duplicated(), :] $
google_stock['Adj Close'].describe()
data['Language'] = ['German', 'Danish', 'Dutch']  #Add a new column from a list.
os.getcwd()
ac['Compliance Review Start'].groupby([ac['Compliance Review Start'].dt.year]).agg('count')
ffr2 = ffr["2007-06":"2011-03"] $ df_info(ffr2)
lat = temp_nc.variables['lat'][16:26] $ lon = temp_nc.variables['lon'][94:118]-360 $ time = temp_nc.variables['time'][833:846] $ temp = temp_nc.variables['air'][833:846,16:26,94:118] $ np.shape(temp)
newfile.shape
results = session.query(func.count(Station.station)).all() $ for result in results: $     print(result)
df["ab_test_group"] = ["A" if date is not None else "B" for date in df.fitness_test_date] $ df.head(3)
time_window_tops = time_window[time_window["screen_name"].isin(["MyFancyOne","Jeanne_vanced","browngravy_93","AmericanMom2","toksikshok","ResistOpression","UTHornsRawk","RandieK"])] $ myplot_parts = [go.Scatter(x=time_window_tops["stamp"],y=time_window_tops["screen_name"],mode="markers")] $ mylayout = go.Layout(autosize=False, width=1000,height=500) $ myfigure = go.Figure(data = myplot_parts, layout = mylayout) $ iplot(myfigure,filename="crisis")
intervention_test[intervention_test.index.duplicated(keep=False)]
df.Date[0]
precip_data_df = pd.DataFrame(precip_data_dict) $ precip_data_df = precip_data_df.rename(columns={"prcp":"Precipitation"}) $ precip_data_df.head() $
X = df.text $ y = df.label
tf_idf = tfidf_vectorizer.fit_transform(df['lemma'])
train_texts = train_texts[train_idx] $ val_texts = val_texts[val_idx]
df.isnull().sum()
import sagemaker as sage $ from time import gmtime, strftime $ sess = sage.Session()
df_countries = pd.read_csv('./countries.csv') $ df_countries.head()
temperature_2016_df = pd.DataFrame(Temperature_year)  # .set_index('date') $ temperature_2016_df.head()
for col_x in np.arange(len(col_list)): $     df.rename(columns={col_list[col_x]: rename_list[col_x]}, inplace=True)
mean = daily_returns['SPY'].mean() $ print(mean) $ std = daily_returns['SPY'].std() $ print(std)
np.array(np.datetime64('2015-12-25 12:00:00.00', 'us'))  # use microseconds 
dfn= df[['Date', 'Autor', 'News']]
for user in session.query(User).\ $ ...             filter(text("id<224")).\ $ ...             order_by(text("id")).all(): $ ...     print(user.name)
df_columns.dtypes $
def yelp_search(client, query): $     response = client.search(query) $     return response.total , response.businesses
data['Created Date'][0:20]
path = '/Users/atharvabhandarkar/Downloads/citi_trips_sep2017.csv' $
knn_grid.fit(X_train, y_train) $
df.head()
b = R17df.rename({'Create_Date': 'Count-2017'}, axis = 'columns') $
    i = 'ABT' $     window = 250
ppt_date.describe() $
DIR = '../Clfs' $ n_clfs = len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))])-1 $ n_clfs
pi_year10lambapoint9_PS11taskG = 2.02
df=df[['gender','SeniorCitizen','Partner','Dependents','tenure','PhoneService','InternetService','Contract','PaperlessBilling','PaymentMethod','MonthlyCharges','Churn']] $ df.head()
negative=Stockholm_data_final[Stockholm_data_final.iloc[:,10]==0] $ positive=Stockholm_data_final[Stockholm_data_final.iloc[:,10]==1] $ negative = negative.sort_values('tweet_created_at') $ positive = positive.sort_values('tweet_created_at')
sub_data = data[data["place"].notnull()] $ sub_data.head()
df2_control = df[df.group == 'control'] $ (df2_control.converted == 1).sum()/len(df2_control)
counter_clinton = Counter() $ secclintondf.tags.apply(lambda s: counter_clinton.update(s))
t0, t1 = lin_reg.intercept_, lin_reg.coef_ $ t0, t1
df2['intercept']=1
doctype_by_day.iloc[:, doctype_by_day.columns.isin(doctype_by_day.min().sort_values(ascending=False)[:10].index)]
p_diffs = np.array(p_diffs)
df['2016-01']
tweets.head()
clf=DecisionTreeClassifier() # define a new classifier clf
df = pd.read_csv("AAPL.csv",na_values = ["null"]) $ df.dtypes
weather1=pd.read_csv('data/Crime/weather1.csv') $ weather2=pd.read_csv('data/Crime/weather2.csv') $ print(weather1.shape, weather2.shape)
chal_counts = set(list(recommendation_df['challenge_count']))
df2.query('converted == "1"').user_id.nunique() / df2['user_id'].nunique()
top_supports.apply(combine_names, axis=1)
df.info()
from sklearn.model_selection import GridSearchCV $ param_grid = {'learning_rate': [0.05,0.1],'num_leaves': [40,60,80]}
tweet_not_dog = stacked_image_predictions[stacked_image_predictions['is_dog']==False].index $ stacked_image_predictions.drop(tweet_not_dog, inplace=True) $ stacked_image_predictions.reset_index(drop=True, inplace=True) $ stacked_image_predictions.head(10)
df = pd.DataFrame({'temp':pd.Series(28 + 10*np.random.randn(10)), $                 'rain':pd.Series(100 + 50*np.random.randn(10)), $              'location':list('AAAAABBBBB')}) $ print(df.info())
print(sorted(data['id'])[:5]) $ print(sorted(data['id'])[-5:])
sum(clintondf.text.apply(lambda s: s.endswith(' -H')))
tree_features_df.loc[0,'p_hash']
!python3 -m nltk.downloader all
print(sp500['Price'].head(3)) $ print(sp500[['Price','Sector']].head(3))
body = body.replace('\n', ' ') $ body
%matplotlib inline $ import matplotlib $ import matplotlib.pyplot as plt $ matplotlib.rcParams['figure.dpi'] = 100 $ df.plot(x='timestamp', y='value')
df_new[['Canada','UK','US']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop('US', axis=1) $ logit_reg_country = sm.Logit(df_new['converted'], df_new[['intercept','Canada','UK']]) $ result_country = logit_reg_country.fit() $ result_country.summary()
seaborn.boxplot(areas_dataframe.rate)
a.iloc[:3]
len(df2)
df_users_mvp=pd.merge(df_users_mvp,df_first_name[['entity_id','field_first_name_value']],left_on='uid',right_on='entity_id',how='left') $ df_users_mvp=pd.merge(df_users_mvp,df_last_name[['entity_id','field_last_name_value']],left_on='uid',right_on='entity_id',how='left')
df_loc = pd.read_csv('/project/stack_overflow/Data/geocoded/geocoded_QueryResults_(9).csv') $ df_loc2 = pd.read_csv('/project/stack_overflow/Data/geocoded/geocoding_locations.csv') $ df_loc = pd.concat([df_loc,df_loc2,df10]).reset_index(drop=True)
n_new = df2.query("group == 'treatment'").user_id.nunique() $ n_new
group_software[0]
sum(df['unknown'])
df=pd.read_csv('E:/DA_nanodegree/PROJECT4/analyzeabtestresults-2/AnalyzeABTestResults 2/ab_data.csv') $ df.head(3)
number_of_favs = pd.Series( $     data=data['Likes'].values, $     index=data['Date']) $ number_of_favs.plot(figsize=(16,4), color='r'); $
df[df['Agency'] == 'NYPD']['Complaint Type'].value_counts().head()
disposition_df.shape
validation.analysis(observation_data, Jarvis_resistance_simulation_0_5)
print('No. of uniques users in dataset:',df.user_id.nunique())
pd.read_csv("../../data/msft.csv", skiprows=100, nrows=5, header=0,names=['open','high','low','close','vol','adjclose'])
df['pg_ratio'] = df['pledged'] / df['goal'] $ df['pg_avg'] = df[['pledged', 'goal']].mean(axis=1) $
x_train.shape
def daily_normals(date): $     sel = [func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)] $     return session.query(*sel).filter(func.strftime("%m-%d", Measurement.date) == date).all() $ daily_normals("01-01")
uni_users = df['user_id'].nunique() $ uni_users
from sklearn import decomposition
df.sum()
integratedData.to_csv('merged.csv', index = False) $ integratedData.drop('True', axis = 1, inplace = True) $ integratedData.head()
%matplotlib inline
vectorizer = CountVectorizer() $ vectorizer.fit(articles_train) $ train_features = vectorizer.transform(articles_train)
first_imdb = float(first_movie.strong.text) $ first_imdb
fwd.head(10)
scores $
rain=pd.merge(rainfall,prcp) $ rain=pd.DataFrame(rain,columns=["station","name","latitude","longitude","elevation","total precipitation"]) $ rain.sort_values(by=["total precipitation"],ascending=False) $ rain
s3 = s1 + s2 $ s1 + s2 $
df_cprc.plot(kind='hist', alpha=0.5, bins=20)
beijing.index = beijing['Date']
print(new_seen_and_click.shape) $ new_seen_and_click.head()
len(df2.loc[new_page]) / len(df2)
from sklearn.ensemble import RandomForestRegressor $ from sklearn.ensemble import GradientBoostingRegressor
df_cal.apply(lambda x: sum(x.isnull()), axis = 0)
autos['price'].value_counts().sort_index(ascending=True).head(30)
FAFRPOS_pdf = 'https://github.com/miriammidoun/Content-Analysis/blob/master/1-Intro/FA_FR_Pos_pdf.pdf'
template = '{0:.2f} {1:s} are worth US${2:d}'
timeseries_yearly = timeseries_yearly.reset_index() $ timeseries_daily = timeseries_daily.reset_index() $ timeseries_yearly.rename(columns={'index': 'year'}, inplace=True) $ timeseries_daily.rename(columns={'index': 'day'}, inplace=True)
oppose.amount.sum()
c = df2[df2['group']=='treatment']['converted'].mean() $ b = df2[df2['group']=='control']['converted'].mean() $ actual_diff =c-b; $ actual_diff
names = list(vedanta_data.keys()) $ values = list(vedanta_data.values())
df_master.to_csv('twitter_archive_master.csv', index= False, encoding='utf-8')
from sklearn.svm import LinearSVR $ model = LinearSVR() $ print ('SVR') $ reg_analysis(model,X_train, X_test, y_train, y_test)
status = api.destroy_favorite(status.id) $ status.favorited, status.favorite_count
w.index.freq, b.index.freq, r.index.freq
type(temp_mean)
pd.get_dummies(df_goog.Open > df_goog.Close)
plt.hist(p_diffs); $ plt.axvline(x=p_diff_real, color='red'); 
browser.quit()
n_old = (df2['landing_page'] == 'old_page').sum() $ n_old
comments = pd.concat([comments_df, comments_df_middle]) $ comments = comments.drop_duplicates() $ comments.to_csv('seekingalpha_top_comments.csv') $
noNulls.orderBy(sort_a_desc).show(5)
red_flags = [registered_at_mailbox,controlled_by_popular_person,company_controlled_by_trust,beneficial_owners_or_officers_are_based_in_secrecy_jurisdiction,politician_officer_or_psc_of_company,changes_name_a_lot,disqualified_directors,company_shares_officer_psc_or_postcode_with_suspected_ml_company] $ for function in  red_flags: $     active_companies = function(active_companies)
column_datasets = {'train': ColumnarDataset.from_data_frame(trn_df, cat_vars, trn_y), $                    'val': ColumnarDataset.from_data_frame(val_df, cat_vars, val_y)}
df_full.drop(['index'],axis=1,inplace=True)
data.drop('month',axis=1) $
from dfast.jupyterutils.togglecode import hideCode $ hideCode()
dat = pd.merge(comm_merge,cand_merge,on='CAND_ID',how='inner') $ dat = dat[dat.CAND_ELECTION_YEAR=='2014'] $ dat.columns
rejected.shape, approved.shape
data.groupby('Agency').size().sort_values(ascending=False)
WorldBankdf = sqlContext.read.json("world_bank.json.gz")
def save_tweets(tweets, path): $     ...
df_CLEAN1A.head(5)
data.head()
train.show(3)
old_page_converted = np.random.choice([0,1],n_old,p_old); $ old_page_converted
df6 = df[df > 0]
s[[1,3]]
RatingSampledf.to_csv("..\\Output\\SampleRatings.csv")
evaluator.get_metrics('precision')
countries_df = pd.read_csv('countries.csv') $ countries_df.head()
karma_per_user = scores.groupby('author')['score'].sum() $ karma_per_user.sort_values(ascending=False)
tweets_sample = tweets_1[0:20] $ count_vect_sample = CountVectorizer(analyzer = clean_text) $ X_counts_sample = count_vect_sample.fit_transform(tweets_sample['text']) $ print(X_counts_sample.shape) $ print(count_vect_sample.get_feature_names())
CO_profit.sort_values(by='Profit Including Build Cost', ascending=False, inplace=True) $ TX_profit.sort_values(by='Profit Including Build Cost', ascending=False, inplace=True) $ GA_profit.sort_values(by='Profit Including Build Cost', ascending=False, inplace=True)
start = time.clock() $ Google_model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True) $ print '{:.2f}s'.format(time.clock() - start)
daily_df['Price_Change'].value_counts()
for key,value in deaths_sorted.items(): $     print( "Deaths in " + str(key) + " = " + str(value))
fit1.resid.hist();
snow.select ("select * from ST_ftd_psp limit 5")
df["DATETIME"] = df["DATE"] $ df["DATETIME"] = pd.to_datetime(df.DATETIME, infer_datetime_format=True)
df_prep2 = df_prep(df2) $ df_prep2_ = pd.DataFrame({'date':df_prep2.index, 'values':df_prep2.values}, index=pd.to_datetime(df_prep2.index))
adopted_cats.loc[adopted_cats['Color']=='Cream Tabby/Cream Tabby','Color'] = 'Cream Tabby' 
dup_user = df2['user_id'][df2['user_id'].duplicated()] $ dup_user
tweetsDf = tweetsDf.fillna('nulo') $ tweetsDf.country.hist()
p_value = (null_vals > obs_mean).mean() $ p_value
BTC
from sklearn.cluster import KMeans $ kmeans = KMeans(n_clusters=5, random_state=1).fit(X_feat) $ X_cluster = kmeans.predict(X_feat) $ df_tweets = pd.DataFrame({'text': X_tweets, 'cluster': X_cluster, 'position': txt_tweets_position}) $ df_tweets.cluster.value_counts(normalize=True)
df[df['Sentiment']=='negative']
shelter_pd = missingValuesRemover(shelter_pd, "AgeuponOutcome") $ shelter_pd = missingValuesRemover(shelter_pd, "SexuponOutcome") $ del shelter_pd['AnimalID'] $ del shelter_pd['OutcomeSubtype'] $ del shelter_pd['Name']
msft.head()
data_issues=pd.read_json('/Users/JoaoGomes/Dropbox/Xcelerated/assessment/data/avro-issues.json',lines=True)
obj.value_counts()
plt.hist(null_vals) $ plt.axvline(x=obs_diff,color ='red');
twitter_archive_clean['source'].astype('category') $ twitter_archive_clean.head()
print len(cbg['AFFGEOID'].unique()) $ print len(noise['AFFGEOID'].unique()) $ print len(graf_counts['AFFGEOID'].unique()) $ print len(noise_graf['AFFGEOID'].unique())
delimited_hourly = delimited_twitter_df.groupby([pd.Grouper(freq="H"), 'company']).count()['text'].to_frame() $ delimited_hourly.columns = ['Number_of_Tweets'] $
total_fit_time = (end_fit - start_fit) $ print(total_fit_time/60.0)
train.shape
tweet_json.sample(5)
p_val = (p_diffs > (p_new-p_old)).mean() $ print('The proportion of p_diffs greater than the actual difference:',p_val)
model_att = model_attention_nmt(len(human_vocab), len(machine_vocab)) $ model_att.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
df["screen_name"].value_counts().head(10)
complete_df[complete_df['Totals'].apply(lambda x: int(x)) < 0]
airbnb_df['room_type'].value_counts()
df["tokenized_text"] = tok_text
X = trip_data_sub.ix[:,0:trip_data_sub.shape[1]].values $ y = trip_data_sub.ix[:,trip_data_sub.shape[1]-1].values $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
least_retweeted = tweets.loc[tweets.snspostid.isin(grouped.iloc[-100:,:].parentPost.astype(np.str).values)] $ least_retweeted.head(2)
for index, row in df_new.iterrows(): $     substrate = getSubstrate(row['barrelID']) $     df_new['substrate'][index] = substrate $     startDate = getStartDate(row['endDate']) $     df_new['startDate'][index] = startDate
bacteria2.dropna()
model_w_country = sm.Logit(df2['converted'], $                            df2[['intercept', 'ab_page', 'CA', 'UK']]) $ results_w_country = model_w_country.fit() $ results_w_country.summary()
twitter = Twython(APP_KEY, APP_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET)
gs_k150_under = GridSearchCV(pipe ,params, scoring='roc_auc', cv = ts_split) $ gs_k150_under.fit(X_train, y_train_under)
tweet_df_raw.head(5)
au.find_some_docs(ao18_coll,limit=3)
print( df['ghost_crabs'].where((df['survey'] == '201704') & ( df['group'] == 'control')).sum()) $ print(df['ghost_crabs'].where((df['survey'] == '201704') & ( df['group'] == 'treatment')).sum())
words_only_scrape_freq = FreqDist(words_only_scrape) $ print('The 20 most frequent terms (terms only): ', words_only_scrape_freq.most_common(20))
p_new = df2[df2['landing_page']=='new_page']['converted'].mean() $ print("The probability of conversion for the new page is: " + str(p_new))
print(x_train.shape) $ print(x_test.shape) $ print(y_train.shape) $ print(y_test.shape)
df.columns
rnd_reg.oob_score_
geoCodeList = dfCleaningData['geo_code'].unique() $ geoCodeList
import pandas as pd $ import requests
col.head(1)
mv_df = movies.ix[movies.movieId.isin(mov_idx)].dropna() $ print(60*'-') $ for movie in mv_df.title.values: $     print(movie) $ print(60*'-', end='\n\n')
test = [char_index[char] if char in vocab else char_index['specialchar'] for char in titleslist[0].lower()] $ for i in range(maxlength-len(title)): $         test.append(char_index["END"])
sess.get_data('ibm us equity','interval avg', overrides={'CALC_INTERVAL': 'YTD'})
print('Difference in Variance for MS: {}%  p-value: {}'.format( $     round(F_ms*100,2), stats.f.cdf(F_ms, degrees1ms, degrees2ms)))
s519397 = session.query(weather.date, weather.prcp).\ $  filter(and_(weather.date.between('2015-01-01','2015-12-31'), weather.station == 'USC00519397')).\ $  order_by(weather.station, weather.date.asc()).all() $ s519397
rows_todrop_1=[i*9 for i in range(int(len(education)/9))]
autos['registration_year'].value_counts(normalize=True).head(10)
quartiles = np.percentile(births['births'], [25, 50, 75]) $ mu = quartiles[1] $ sig = 0.74 * (quartiles[2] - quartiles[0])
datatest['rooms'] = datatest['rooms'].apply(lambda x: (float)((int)(x)))
show_train_history('loss','val_loss')
trips_data.groupby('weekday').duration.mean().reset_index().plot.scatter('weekday','duration') $ plt.show() # It can be seen that average duration per trip is higher on weekends
people
import quilt $ quilt.install("aics/random_sample")
print("The probability of converted in treatment group:", df2.query('group == "treatment"')['converted'].mean())
a = df_2015.sales_jan_mar.sum() $ a
accuracy_score(clf.predict(x_train),y_train)
trump_origitnals["lText"] = trump_o["text"].map(lambda x: x if type(x)!=str else x.lower())
c.execute(query) $
sns.heatmap(ndvi_us, vmin = -.1, vmax=1)
x_axis = output['user.followers_count']
from sklearn.metrics import accuracy_score $ for clf in (knn_clf,log_clf, rnd_clf, svm_clf, voting_clf): $     clf.fit(X_train, y_train) $     y_pred = clf.predict(X_test) $     print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[n_new, n_old], alternative = 'larger') $ print("Z-score: ",z_score) $ print("P_value: ",p_value)
%%time $ X_train_term  = vectorizer.fit_transform(X_train['text'])
df_new.groupby(['country'], as_index=False).mean()
df_new.columns
chained2 = itertools.chain.from_iterable([[1, 2, 3, 4], [5, 6, 7, 8]]) $ for letter in chained2: $     print(letter)
A = pd.Series([2,4,6], index=[0,1,2]) $ B = pd.Series([1,3,5], index=[1,2,3]) $ A + B
print data_df.clean_desc[26]
first_movie.div
%timeit re.sub('[^a-z ]+','','hello 1923worl 234 shad54awo')
sentiment_track = compute_sentiment(title_date_dict, title_text_dict, file_title, team_name, team_city)
for i in sorted(_): $     print(f"{repl}'{i}', '')")
data.to_csv('data/Building_Permits_proc_2.csv')
scores_firstq = np.percentile(raw_scores, 25) $ scores_thirdq = np.percentile(raw_scores, 75) $ print('The first quartile is {} and the third quartile is {}.'.format(scores_firstq, scores_thirdq))
tweets_df.tail()
sentiments_pd = pd.DataFrame.from_dict(sentiments) $ sentiments_pd.head()
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
Grouping_Year_DRG_discharges_payments, Year_levels, DRG_levels =\ $         Generate_Grouping_Year_DRG_discharges_payments(False) $ Grouping_Year_DRG_discharges_payments.head()
users.info()
p_old = df2['converted'].mean() $ print("Actual values of Probability of conversion for old page (p_old):", p_old)
df.iloc[0:4]
print('start one-hot encoding') $ kick_projects_ip = pd.get_dummies(kick_projects, prefix = [ 'category', 'main_category', 'currency','country'], $                              columns = [ 'category', 'main_category', 'currency','country']) $ print('ADS dummy columns made')
pct = pd.merge(pct, graf_counts2, left_on='Precinct', right_on='precinct')
sample = df2.sample(df2.shape[0], replace=True) $ new_page_converted = sample.query('landing_page == "new_page"')['converted'] $ new_page_converted.mean()
gs.score(X_test_all, y_test)
weather.loc[weather.precipitation_inches.isnull(), $             'precipitation_inches'] = weather[weather.precipitation_inches.notnull()].precipitation_inches.median()
twitter_archive_clean.info()
def sigmoid(x, Beta_1, Beta_2): $      y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2))) $      return y
TBL_FCInspevnt_query = "SELECT brkey, scourcrit, suff_rate, nbi_rating, in_modtime FROM TBL_FCInspevnt" $ data_FCInspevnt = pd.read_sql(TBL_FCInspevnt_query, cnxn) $ data_FCInspevnt
equipment = pd.read_csv(data_repo + 'equipment.csv', dtype={'CODE_POSTAL': object}, **import_params)
ctrl_con = df2.groupby('group', as_index=False).describe()['converted']['mean'][0] $ print("P(ctrl_con) = %.4f" %ctrl_con)
top_50_channels = df_link_yt['channel_id'].value_counts().head(50).index $ channel_id = top_50_channels[1]
all_df = pd.read_pickle('data/all_political_ads.pickle')
df_new.groupby(['country','ab_page'], as_index=False).mean()
flux = sp.get_tally(name='flux') $ flux = flux.get_slice(filters=[openmc.CellFilter], filter_bins=[(fuel_cell.id,)]) $ fuel_rxn_rates = sp.get_tally(name='fuel rxn rates') $ mod_rxn_rates = sp.get_tally(name='moderator rxn rates')
x = np.zeros(4, dtype=int)
for letter in list('ABCDEFGH'): $     ab_groups[letter].drop(ab_groups[letter].tail(1).index, inplace=True)
obj2
min_train = 3 #months $ max_train = 3 #months $ splits = TimeSeriesSplit(n_splits=n_training-1,max_train_size = max_train)
all_df = pd.read_pickle('data/all_political_ads.pickle') $ all_df.head(2)
df.hist(column = 'funding_rounds',bins = 20)
reddit_comments_data.groupBy('author').agg({'score': 'mean'}).orderBy('avg(score)', ascending = False).show()
df['comments'].value_counts()/len(df['comments'])
from sentiment_impact import measure_impact $ ex0 = np.array([.01,-.03,-.04,.5,.6,.8,.82]) $ plt.plot(range(len(ex0)), ex0) $ plt.show() $ print('The sentiment impact is {}'.format(measure_impact(ex0)))
df_a = df.query('landing_page == "old_page" and group == "control"') $ df_b = df.query('landing_page == "new_page" and group == "treatment"') $ df2 = df_a.append(df_b, ignore_index=True)
no_dog_stage = df_complete.loc[:,['text','dog_stage']].query("dog_stage=='None'") $ no_dog_stage.sample(10)
%time headdtm = tfvect.fit_transform(revs.head(20000)) #Time comparison
logit4 = sm.Logit(df_new['converted'], df_new[['intercept','new_page','CA_new_page','US_new_page','CA','US']]) $ results4 = logit4.fit() $ results4.summary() $
%%time $ coverage_error(validation_labels, val_prediction)
sales_change = pd.DataFrame(over_time_2015['Sale (Dollars)']).reset_index() $ sales_change1 = sales_change[sales_change['month'] == 1] $ sales_change2 = sales_change[sales_change['month']==12] $
(trn_texts $  .groupby('SDG') $  .count())
import seaborn as sns; $ titanic = sns.load_dataset('titanic') $ titanic.head()
!ls $REPO/courses/machine_learning/feateng/taxi_trained/export/exporter
dfh.columns
df.groupby("two_day_reminder_profile_incomplete_sent")["cancelled"].mean()
w_counts = Counter(lemmatized_words) $ df = pd.DataFrame(w_counts.most_common(100), columns=['Word', 'Count']) $ df.to_csv('word_counts.csv')
np.datetime64('2015-04-09') + 12
!$TARGETCALIBPATH/apply_calibration -h
additional_limit_outstanding_user.head()
print(props.info())
date_df.iloc[0]['date'].strftime('%A')
contentPTags = SAEMSoup.body.findAll('p') $ for pTag in contentPTags[:3]: $     print(pTag.text)
with open('youtube_urls.json', 'r') as fp: $     youtube_urls = json.load(fp)
df3 = df2 $ df3.head()
spark = pyspark.sql.SparkSession(sc) $ df = spark.read.csv('../data/somenulls.csv', header=True)
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])[['CA','UK','US']] $ df_new['country'].astype(str).value_counts() $ df_new.head()
parties['Unique Key'].groupby(by= parties.index.hour).count().head(1)
giss_temp.columns
sfs1 = sfs1.fit(X_df, y_series)
conf_matrix = confusion_matrix(training_test_labels, lr.predict(preproc_training_test), labels=[1,2,3,4,5]) $ conf_matrix
contribs.committee_name.value_counts()
energy_indices.pct_change().mul(100)
print('Do we need bread:\n', groceries[[2]]) 
train['number_of_features'] = train['features'].map(len) $ test['number_of_features'] = test['features'].map(len)
p=table.find(text='Passengers').find_next('td').text $ passengers=re.search(r'\d+', p).group() $ passengers
tweets_raw = tweets_raw.drop(axis= 1, labels=  ["id", "user_name"])
bnb2 = bnb[bnb['age']<1000] $ bnb2[bnb2['age']>80].plot(kind='hist', y='age', bins=20)
autos.head()
def string_to_datetime(string): $     return datetime.strptime(string,'%Y-%m-%d %H:%M:%S %z') $ tweet_archive_clean.timestamp = tweet_archive_clean.timestamp.map(string_to_datetime)
conn_string = "host='localhost' dbname='qua-kit'" $ print ("Connecting to database\n	->%s" % (conn_string)) $ conn = psycopg2.connect(conn_string) $ print ("Connected!\n")
SGDC = SGDClassifier() $ model2 = SGDC.fit(x_train, y_train)
archive_copy['tweet_id']= archive_copy['tweet_id']. astype('str') $ type(archive_copy['tweet_id'].iloc[0])
combined_df2.keys()
p_old = len(df2.query('converted==1'))/len(df2.index) $ p_old
result[['id_ndaj1', '11_x', '12_x', '14_x', '15_x', '10102_x', '10120_x']].hist(figsize=(15, 10), bins=50, alpha=0.7)
model_df['forecast'] = results.predict(dynamic=False)
filename_output = f'en-wikipedia_traffic_{PAGECOUNTS_START[:6]}-{PAGEVIEWS_END[:6]}.csv' $ filename_output
text[15:18]
tweet_archive_clean.to_csv('twitter_archive_master.csv')
history_with_target[['target', 'time_delta', 'ACTUAL_START_DATE', 'ACTUAL_END_DATE', 'INCIDENT_TYPE_NAME', 'SCHEDULED_START_DATE', 'SCHEDULED_END_DATE',  'INCIDENT_STATUS_NAME', 'TYPE_BI', 'MOTIF_ANNULATION_CODE', 'MOTIF_ANNULATION_DESC']][0:50]
df = pd.DataFrame(cleaner_tweets) $ df.tail()
df3 = df2.join(df_countries.set_index('user_id'), on='user_id') $ df3.head()
df.query('group=="treatment" and landing_page != "new_page" or group=="control" and landing_page=="new_page"').count()
overall_topics = pd.DataFrame(v)
pres_df.ix[362865]['subjects'] # how to get row info by index number and column
def percentdonationoptional(num): $     print('\n', f'{num} dollars') $     print(donations.loc[donations['Donation Amount'] == num, 'Donation Included Optional Donation'].value_counts()) $     print('percentage') $     print(donations.loc[donations['Donation Amount'] == num, 'Donation Included Optional Donation'].value_counts(normalize=True))
buckets_to_df(contributors.fetch_aggregation_results()['aggregations']['0']['buckets']).head()
shows.dtypes
if False: $     plt.scatter(train.diff_lat, train.duration) $     plt.show()
df.shift(1)
dfs[0].head()
gbm_2 = joblib.load(r'C:\Users\nkieu\Desktop\Python\Loan data\2018-04-09\gbm_0409.pkl')
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
df_pop = df.groupby('userLocation')[['tweetRetweetCt', 'tweetFavoriteCt']].mean() $ df_pop $
df = pd.concat([pd.read_csv('PoolHeaterPhoton' + f, names=['event_name','event','photon','day'], usecols=['event','day'], parse_dates=['day']) for f in ['.csv', '1.csv', '2.csv']]) $ df.head()
contractor_merge['contractor_bus_name'] = contractor_merge['contractor_bus_name']+" - "+contractor_merge['contractor_number'].astype(str)
RFC_grid.best_params_
fig, axarr = plt.subplots( 1, 1, figsize=(20,10) ) $ plt.hist(users.control["revs"], alpha = 0.5, range = (0, 15), log = True, bins = 15, label = "Source editor only") $ plt.hist(users.treatment["revs"], alpha = 0.5, range = (0, 15), log = True, bins = 15, label = "Both editors") $ plt.legend(loc="upper right") $ plt.show()
df_test_user_2 = df_test_user.copy() $ df_test_user_2['created_on'] = '2017-09-20 00:00:00'
records3.loc[(records3['Graduated'] == 'Yes') & (records3['GPA'].isnull()), 'GPA'] = grad_GPA_mean $ records3.loc[(records3['Graduated'] == 'No') & (records3['GPA'].isnull()), 'GPA'] = non_grad_GPA_mean
print('Index zero, header Date:', data.loc[0,'Date'])
kick_projects.groupby(['main_category','state']).size() $
bg, bs = benchmark_model(X_train, y_train, X_valid, y_valid) $ print ('Best Score: %0.5f' % bs ) $ print ('Grid:', bg)
new_dems.Sanders.describe()
to_be_predicted_Day3 = 50.63320999 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
cols=df.columns.tolist() $ cols.remove('date') $ cols.remove('store_nbr')
import pandas as pd $ import numpy as np $ import os $ import requests
worst_score = df.rating_score.min() $ print('The worst beer got a {}. Ouch!'.format(worst_score))
testObj.buildOutDF(tst_lat_lon_df[-25:])  ## redo end of DF .. should be entirely blank since we're out of licenses $
csvData[csvData['street'].str.match('.*East.*')]['street']
df.loc[d[5], 'A']
stars.value_counts().sort_index()
traindf.to_csv('train.csv', header=False, index=False, encoding='utf-8', sep='|') $ evaldf.to_csv('eval.csv', header=False, index=False, encoding='utf-8', sep='|')
df2['landing_page'].value_counts()['new_page']/df2['landing_page'].count()
import os $ print("Running all tests...") $ _ = [ok.grade(q[:-3]) for q in os.listdir("ok_tests") if q.startswith('q')]
weather.isnull().any()
sum(clean_rates.text.str.contains('\n'))
%matplotlib inline
temp_df = weather_mean[['Temp (deg C)']] $ temp_df.head()
orgs.loc[0]
from sklearn.decomposition import PCA
p1 = dto_mean_predict(train,84,38) $ p2 = dto_median_predict(train,84,38) $ display(p1) $ display(p2)
df.drop('date_first_booking',axis=1,inplace=True)
log_mod = pickle.load(open('../data/model_data/log_pred_mod.sav', 'rb'))
menu_about_latent_features_column_names = ['menu_id'] + labels_for_menu_about_latent_features
dem = pd.read_csv(processed_path+"blight_demolition.csv") $ dem.head()
not_aligned = len((df[(df['group'] == 'control') & (df['landing_page'] == 'new_page')])) + \ $ len((df[(df['group'] == 'treatment') & (df['landing_page'] == 'old_page')])) $ print(not_aligned)
import pandas as pd $ import numpy as np
autos['ad_created'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
team_names.Conference = team_names.Conference.apply(lambda x : 0 if x=="E" else 1)  # Numeric Encoding
df.describe()
df_archive_clean["source"] = df_archive_clean["source"].replace('<a href="http://vine.co" rel="nofollow">Vine - Make a Scene</a>', $                                                                "Vine - Make a Scene")
tweets['retweeted'] = tweets['retweeted_status'].notna() $ tweets = tweets.drop(columns=['retweeted_status'])
import folium $ from folium import plugins $ print(folium.__version__)
vi_ok['VIOLATIONCODE'].values
volumn = pd.DataFrame({ticker: data['Volume'] for ticker, data in all_data.items()})
results_ball_rootDistExp, output_ball_rootDistExp = S.execute(run_suffix="ball_rootDistExp", run_option = 'local')
print type(first_element)
columns = inspector.get_columns('stations') $ for c in columns: $     print(c['name'], c["type"]) $
df[df['Descriptor'] == 'Pothole']['Unique Key'].groupby(df[df['Descriptor'] == 'Pothole'].index.dayofweek).count().plot()
reddit_comments_data.show(10)
p_diffs = []
dfm = dfn.drop(['usd_pledged','goal','state','slug','currency','deadline','state_changed_at','created_at','backers_count','spotlight','period'], axis=1).copy()
dcrime_in = dcrime.apply(within_area,axis=1) $ print "Number of incidents within area",sum(dcrime_in),"total",len(dcrime_in) 
import requests $ import pandas as pd $ import json $ import datetime as dt $ import matplotlib.pyplot as plt
control_df = df2.query('group == "control"') $ control_conv = control_df.query('converted == 1').user_id.nunique() / control_df.user_id.nunique() $ control_conv
asf = after_sherpa["Date"].value_counts() $ asf.to_csv("GG_a.csv") $ cols = ['Day', 'Count'] $ asfr = pd.read_csv("GG_a.csv", header=None, names=cols) $
tweet_data_copy[['id', 'id_str']].head()
words_sk = [term for term in all_tokens_sk if term not in stop and not term.startswith('http') and len(term)>2] $ corpus_tweets_streamed_keyword.append(('meaningful words', len(words_sk))) # update corpus comparison $ print('Total number of meaningful words (without stopwords and links): ', len(words_sk))
session.query(Measurement.station, func.count(Measurement.tobs)).\ $ group_by(Measurement.station).\ $ order_by(func.count(Measurement.tobs).desc()).all() $
autos['price'].value_counts().sort_index()
test.head()
df_onc_no_metac['ONC_LATEST_N'].unique()
from sklearn.cross_validation import cross_val_score $ scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10) $ print scores $ print scores.mean()
df.groupby('episode_id')['id'].nunique().hist()
print(lyra_lightcurve.meta)
import pandas as pd $ %matplotlib inline $ import pylab as plt
taxi_sample["tip_pct"] = taxi_sample["tip_amount"] / taxi_sample["total_amount"]
stocks_happiness_rev1=stocks_happiness_rev
files_to_manage = ['game_date', 'main_players', 'more_home_away', 'more_team_stats', $                    'more_wins_losses', 'referees', 'start_bench', 'team']
b=[1,2,3]
for i in cols: $     del train[i] $     if (kaggle | sim): del test[i] $ if (kaggle|sim): del tfidf_test $ del tfidf_train, tfidf
x = store_items.isnull().sum().sum() $ print(x)
plt.figure(figsize=(15,10)) $ sns.countplot(auto_new.CarModel)
print("Number of Techniques in ATT&CK") $ techniques = lift.get_all_techniques() $ print(len(techniques)) $ df = json_normalize(techniques) $ df.reindex(['matrix', 'tactic', 'technique', 'technique_id', 'data_sources'], axis=1)[0:5]
serious_count = 0 $ for row in data: $     if re.search("[\[\(][Ss]erious[\]\)]", row[0]): $         serious_count = serious_count + 1 $ print(serious_count)
kmeans = pd.DataFrame(km_res.cluster_centers_) $ kmeans.columns = finalDf.columns[:-1] $ kmeans.head(2)
giss_temp.boxplot();
ab_data.isnull().values.any()
cityID = '67b98f17fdcf20be' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Boston.append(tweet) 
%%time $ in_degree_centrality = convert_dictionary_to_sorted_list(nx.in_degree_centrality(network_friends))
df_count.sample(5)
merged_df_cut.head(5)
finals.loc[(finals["pts_l"] == 1) & (finals["ast_l"] == 1) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'game_winners'
df.iloc[0]['date_added']
pd.read_sql_query('SELECT * FROM samples', con)
TH=0.45 $ results=topUserItemDocs[topUserItemDocs['score']>TH].groupby(['item_id']).apply(lambda g: [x[0] for x in sorted(zip(g['user_id'],g['score_weight']),key=lambda t: -t[1])[0:100]]) $ print results.shape $ print results.head()
den.shape
combined_df = pd.merge(sales_data, returns_data, how = 'left', on = 'Order ID') $ print(combined_df)
df.head(2)
most_common_dog.plot(kind = 'barh', figsize = (8, 8)) $ plt.title ('Top 5 most rated dog breeds') $ plt.xlabel ('Amount of dogs') $ plt.ylabel ('Dog breed');
passenger_timestamps = timestamps_between(str_2017, end_2017, 1000) $ df_passengers["created"] = passenger_timestamps
before_sherpa = df.loc[df["index"] <= 1685.0] $ after_sherpa = df.loc[df["index"] > 1685.0] $
results=logit_mod.fit();
s = pd.Series(lst) $ s.isnull()
import numpy as np $ import pandas as pd $ class display(object): $     <p style='font-family:"Courier New", Courier, monospace'>{0}</p>{1} $
cols_to_keep = ['Churn','gender', 'SeniorCitizen', 'Partner','Dependents','tenure','PhoneService','PaperlessBilling','MonthlyCharges'] $ data = df[cols_to_keep].join(dummy_IntServ.loc[:, 'IntServ_Fiber Optic':]) $ data = data.join(dummy_Contract.loc[:, 'Contract_One year':]) $ data = data.join(dummy_PayMethod.loc[:, 'PayMethod_Credit card (automatic)':]) $ print(data.head())
data_set
holdout_results =append_preds(sl_holdout, dholdout_predictions, dholdout_predprob) #this is for all measurements
len(df2[df2['landing_page'] == 'new_page'])/len(df2)
filtered_greater_100.count()
results_1dRichards, output_R = S_1dRichards.execute(run_suffix="1dRichards_hs", run_option = 'local')
images.describe()
df["Views-PercentChange"].hist() $ plt.show()
fb_cleaned
nnew_sim = df2.loc[(df2['landing_page'] == 'new_page')].sample(nnew,replace = True) $ new_page_converted = nnew_sim.converted $ new_page_converted.mean()
cars= cars[(cars.price >= 500) & (cars.price <=160000) & (cars.yearOfRegistration >=1950) &(cars.yearOfRegistration <=2016) & (cars.powerPS >=10) & (cars.powerPS<=500)] $ cars.info()
plt.scatter(USvideos['likes'], USvideos['views'])
tbl
temp_df = pd.DataFrame(calc_temps ('2017-08-02', '2017-08-09'), columns=['min', 'avg', 'max']) $ temp_df
pd.cut(tips.tip, np.r_[0, 1, 5, np.inf], $       labels=["bad", "ok", "yeah!"]).sample(10)
first_result.contents
df.query('MeanFlow_cfs < 50')
results["units"]["unit_annotation_score"].head()
KoverY_50_splus = __ # In the simulation run boosting the savings rate, the value of $ KoverY_50_nplus = __ # In the simulation run boosting the labor force growth rate, the $ KoverY_50_gplus = __ # In the simulation run boosting the efficiency of labor growth $
Z = np.zeros((5,5)) $ Z += np.arange(5) $ print(Z)
df = df_raw.filter(items=['year','list_date','rank_this_week','rank_last_week','song_title', $                           'artist','peak_to_date','weeks_on_chart']) $ df.columns = ['year','date','tw','lw','title','artist','peak_to_date','weeks_on_chart'] $ df = df.sort_values(by=['year','date','tw']) $ df
red_4['age'] = red_4['age'].astype('timedelta64[h]') $ red_4.head()
twitter_archive_df_clean['timestamp'].sort_values(ascending=False).head()
datatmp=data[["Postal Code","Total Gallons"]][data["Customer Class"]=="Residential"] $ datatmp.index=datatmp.index.map(lambda x: datetime.strftime(x,"%Y")) $ datatmp.pivot_table(index=datatmp.index,columns="Postal Code",values="Total Gallons",aggfunc=sum).plot(legend=False) $ plt.show() $
df_new_dummy = pd.get_dummies(data=df_countries, columns=['country']) $ df_latest = df_new_dummy.merge(df_regression, on='user_id') $ df_latest.head(1) $ df_latest[['user_id', 'country_CA', 'country_UK', 'country_US','converted','intercept','ab_page']]=df_latest[['user_id', 'country_CA', 'country_UK', 'country_US','converted','intercept','ab_page']].astype(int) $
factors = web.DataReader("Global_Factors","famafrench") $ factors
hist(df2.tripduration, bins = 20, label = "Male", normed=1) $ plt.xlabel("Trip Duration in seconds", fontsize=12) $ plt.ylabel("Amount of trips", fontsize=12) $ plt.title("Trip Duration Histogram male users", weight='bold', fontsize=14) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
sp_500_adj_close = sp500[['Adj Close']].reset_index()
import matplotlib.pyplot as plt $ import numpy as np $ y = df.price.values $ plt.plot(np.sort(y),'.') $ plt.show()
df2_copy = df2.copy()
plt.hist(p_diffs); $ plt.axvline(obs_diff, color='red')
close_series.plot()  # plots Series directly $ plt.show()
100.0*df_h1b_nyc_ft[df_h1b_nyc_ft.pw_unit_1=='Hour'].shape[0]/(df_h1b_nyc_ft.shape[0])
df['year_month']=df['timestamp'].values.astype('datetime64[M]') $
x = dft[pd.datetime(2013, 1, 1, 10, 12, 0):pd.datetime(2013, 2, 28, 10, 12, 0)]
import statsmodels.api as sm $ convert_old = df2.groupby([df2['landing_page']=='old_page',df2['converted']==1]).size().reset_index()[0].iloc[3] $ convert_new = df2.groupby([df2['landing_page']=='new_page',df2['converted']==1]).size().reset_index()[0].iloc[3] $ n_old = df2.query('landing_page == "old_page"').user_id.nunique() $ n_new = df2.query('landing_page == "new_page"').user_id.nunique() $
autos["odometer_km"].describe()
one_station['weekday']=one_station['DATE'].apply(dt.weekday) $ one_station.head()
recipes.ingredients.str.contains('[Cc]innamon').sum()
df_users.index
transactions.head(2)
file_name = "./data/train_preprocessed1.csv" $ train_df1 = pd.read_csv(file_name, low_memory = False, index_col = False) $ train_df1.drop(train_df1.columns[0], axis = 1, inplace = True) $ train_df1.head()
fb_all = farebox.copy() $ fb_day_time = fb_all.groupby(['service_day','day_of_week','service_time','service_datetime']).agg({'entries':np.sum}).reset_index() $ fb_day_time.rename(columns={'entries':'entries_green'}, inplace=True) $
users['Registered'] = pd.to_datetime(users['Registered']) $ users['Cancelled'] = pd.to_datetime(users['Cancelled']) $ sessions['SessionDate'] = pd.to_datetime(sessions['SessionDate']) $ transactions['TransactionDate'] = pd.to_datetime(transactions['TransactionDate'])
df.head()
test0 = start_coord_list[9] $ test1 = end_coord_list[9] $ print(test0) $ print(test1)
plt.figure(figsize=(16,8)) $ dateCreated_count = df2['dateCreated'].value_counts().plot(kind='bar') $ dC = dateCreated_count.set_xticklabels(dateCreated_count.get_xticklabels(), rotation=90) $ len(df2['dateCreated'].unique())
sample_groups_sorted_by_events = sample_groups.sort_values([ 'MSA_CODE', 'past_event_count' ], ascending=False) $ sample_groups_sorted_by_events.groupby([ 'MSA_NAME' ]).apply(lambda x: x.head(5))
states_df.shape
X_tfidf_df = pd.DataFrame(X_tfidf.toarray()) $ X_tfidf_df.columns = tfidf_vect.get_feature_names()
print("Number of rows in slice", reviewsDFslice.shape)
df.rename(columns={'Indicator':'Indicator_id'}, inplace=True) $ df.head(2)
re.findall('[A-Z]....\s', synopsis)
es_client = Elasticsearch('http://localhost:9200')
lrm = sm.Logit(df_regression['converted'],df_regression[['intercept', 'ab_page']])
tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', '')) $ tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\d+\.\d+', x)[0]) $ print(tips.head())
walk = walk.cumsum()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key={}'.format(API_KEY))
final_data.groupby('rating').agg(['count'])
dfs.set_index('DATE_TIME', inplace=True) $ dfs_resample = dfs.groupby(['C/A', 'UNIT', 'STATION', 'SCP'], as_index = False).apply(lambda df1: df1.resample('6H') $                                                               .first() $                                                              .interpolate()) $
preproc_titles = pipeline.fit_transform(review_title) $ pipe_cv = pipeline.named_steps['cv']
dfTransacciones.describe(percentiles=[.1, .25, .5, .75, .9, .95])
case_mask = (lower_vars.str.contains('new') $              & (lower_vars.str.contains('case') | lower_vars.str.contains('suspect')) $              & ~lower_vars.str.contains('non') $              & ~lower_vars.str.contains('total'))
intervention_history['time_delta'] = groups.ACTUAL_START_DATE.transform(get_time_delta)
c = crime_data.reset_index() $ c = c.merge(gt_enrollment_data, how='left', on=['year', 'semester']) $ c = c.merge(atl_data, how='left', on='year') $ c = c.set_index('time') $ c.head()
md = ColumnarModelData.from_data_frame(PATH, val_idx, df, yl.astype(np.float32), cat_flds=cat_vars, bs=128, $                                        test_df=df_test)
joined = ConvertTimeDeltaToInt(joined, ['Promo2Days', 'Promo2Weeks', 'CompetitionDaysOpen']) $ joined_test = ConvertTimeDeltaToInt(joined_test, ['Promo2Days', 'Promo2Weeks', 'CompetitionDaysOpen'])
click_condition_meta = pd.merge(clicking_conditions, users_meta, on = 'user_id')
lines = sc.textFile(csv_filename) $ parts = lines.map(lambda l: l.split(",")) $ rows = parts.map(parse)
words_not_used = pca_plot(model, interesting_words,'Word2Vec: Interesting Words') $ print('Words not in interesting list not used:') $ print(', '.join(words_not_used))
ogh.switchUpVICSoil(input_file=os.path.join(homedir,'soil_base'), $                     output_file='soil', $                     mappingfile=mappingfile, $                     homedir=homedir)
in_region = circle_region.contains(coord)
def get_year(date): $     try: $         return re.match('([0-9]{4}).*', date).group(1) $     except: $         return np.nan
autos.describe(include = "all")
samp311 = cp311.sample(frac = .2,random_state=757859 )
tweet_archive_clean.head(2)
df_sched2.dtypes
retention.reset_index(inplace=True)
def load_file(file_name): $     with codecs.open(file_name, 'rb', 'utf8') as infile: $         text = infile.read() $     return text
df['converted'].mean()
present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm), $                               name="present_error_raw") $ present_error = tf.reshape(present_error_raw, shape=(-1, 10), $                            name="present_error")
autos["ad_created"].str[:10].value_counts(normalize=True,dropna=False).sort_index(ascending=True)
largest__diff = max(diff) $ print("Largest Closing Price Difference : " + str(largest__diff))
baseball.hr.corr(baseball.X2b)
len(y_test) #Test dataset length
pd.date_range('2017-12-30', '2017-12-31', freq='h')  # Hourly frequency
pickle_in = open("C:/Users/Fabian/Documents/FinancialForecasting/Data/X_score-2018725.pickle", "rb") $ X_score = pickle.load(pickle_in)
modelDir = S3_HOME + "/modelDir/"
data.set_index("date", inplace=True) $ data.head()
import os $ PROJECT = 'cloud-training-demos'    # CHANGE THIS $ BUCKET = 'cloud-training-demos-ml'  # CHANGE THIS $ REGION = 'us-central1' # CHANGE THIS
train['author_popularity'] = train.author.map(authors['mean'])
%%time $ ward_total = 20000 $ params = { 'n_clusters' : 17, 'linkage' : 'ward' } $ ward = AgglomerativeClustering(**params) $ ward.fit(tfidf_matrix[:ward_total].toarray())
dfMonth['Date'] = dfMonth['Date'].dt.to_period("M")
np.random.randint(1,100, 10)
all_countries["Golf"]
df1.tail(36)
res = process_all_articles('mdpi-only/**', pf_get_citation, doi=doi, title=title, n_sentences=2) $ len(res)
trunc_df.iloc[94]
measure.head()
investors_df['most_recent_active_year'] = investors_df['most_recent_investment'].dt.year $
fig = plt.figure() $ df['OT Temp'].plot()
logit2 = sm.Logit(df4['converted'], $                            df4[['intercept', 'country_UK', 'country_US']]) $ result2 = logit2.fit() $ result2.summary()
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2['ab_page'].mean()
law = tc_final $ law['YBP sub-account'].replace("195099", "590099", inplace= True) $ law
df_geo_segments.info()
X = reddit['subreddit'] $ y = reddit['comm_range']
data = pandas.concat(sheets, names=["Worksheet Name","Worksheet Row"])
dates= pd.date_range('20180831', periods=50, freq='M') $ dates
cols = ['id', 'host_location', 'city', 'state', 'host_response_time', 'host_response_rate', 'host_acceptance_rate', $         'host_is_superhost', 'host_neighbourhood', 'host_listings_count', 'host_total_listings_count', $         'host_verifications', 'host_has_profile_pic', 'host_identity_verified', 'property_type', 'room_type', $         'accommodates', 'bathrooms', 'bedrooms', 'beds', 'bed_type', 'price', 'weekly_price', 'monthly_price', $         'security_deposit', 'cleaning_fee', 'minimum_nights', 'number_of_reviews', 'first_review', 'last_review']
import statsmodels.api as sm $ convert_old = df2.query('group=="control"').converted.sum() $ convert_new = df2.query('group=="treatment"').converted.sum() $ n_old = df2.query('group=="control"').shape[0] $ n_new = df2.query('group=="treatment"').shape[0]
len(tt)
t0 = time() $ auc = calculateAUC( cvData , bAllItemIDs, model.predictAll) $ t1 = time() $ print("auc=",auc) $ print("finish in %f seconds" % (t1 - t0)) $
temp_long_df.tail()
model_unigram = models.Word2Vec(sentences = flatSentenceList,iter=5)
column_mean = df.mean(1) $ result1 = df['A'] + column_mean $ result2 = df.eval('A + @column_mean') $ np.allclose(result1, result2)
pd.DataFrame(df.date.str.split(' ').tolist(), columns = "datepart timepart".split())
ticks = data.ix[:, ['Price', 'Volume']] $ ticks.head()
liquor2016_q1_volume = liquor2016_q1.VolumeSoldLiters.groupby(liquor2016_q1.StoreNumber).agg(['sum']) $ liquor2016_q1_volume.columns = ['Volume'] $ liquor2016_q1_volume.tail()
twitter_archive_master['rating_numerator'] = twitter_archive_master['full_text'].str.extract('([0-9][0-9]?\.?[0-9]?)\/\d\d\d?[^\/]').astype(float) $ twitter_archive_master['rating_denominator'] = twitter_archive_master['full_text'].str.extract('[0-9][0-9]?\.?[0-9]?\/(\d\d\d?)[^\/]').astype(float)
tree_features_df[~tree_features_df['p_hash'].isin(manager.image_df['p_hash'])] #Let's look at what's missing:
SSE=((lmscore.predict(X_test)-y_test)**2).sum() $ SSE
print("Saving data to elasticsearch - please be patient") $ df = df.withColumn("datetime",df["datetime"].cast("string")) # elasticsearch needs datetimes in a string type $ es.saveToEs(df,index=es_dataindex,doctype=es_datatype)
tmp = 'is ;) :) seven.<br /><br />Title (Brazil): Not Available' $ print(preprocessor(tmp)) $
print df.pivot(index='date', columns='item', values='status')
S_distributedTopmodel.decision_obj.thCondSoil.options, S_distributedTopmodel.decision_obj.thCondSoil.value
summary.groupby(0).count().sort_values(by='documents',ascending=False).head(10)
sql("show tables").show()
knn = KNeighborsClassifier(n_neighbors=20) $ print(cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())
transactions = transactions.head(n = 500)
iberia = df[df["text_3"].str.contains("iberia", case = False)] $ iberia.text_3.str.split(expand=True).stack().value_counts().head(10).reset_index()
trump_originals = trump[trump.is_retweet == False] $ trump_originals.shape $
twitter_archive_enhanced.info()
check_max_loan(pipeline, X_valid[0:30], y_valid[0:30])
weather_yvr_dt.dtypes
image_pred.head(2)
hstotal[hstotal.notnull()]
X_train_dtm = vect.transform(X_train) $ X_test_dtm = vect.transform(X_test)
del rhum_long_df $ del rhum_wide_df
df.apply(func8, axis=1)
click_condition_meta.os_timezone.unique()
df_all=pd.DataFrame(all_prcp,columns=['date','Prcp']) $ df_all.head()
merged1['Specialty'].isnull().sum(), merged1['Specialty'].notnull().sum()
get_items_purchased('alpa.poddar@gmail.com', product_train, customers_arr, products_arr, item_lookup)['child_sku'].unique() $
inspector = inspect(engine) $ columns = inspector.get_columns('measurement') $ for column in columns: $     print(column["name"], column["type"])
control_converted = (df2.query('group == "control" and converted == 1').count()[0]) $ control_total = (df2.query('group == "control"').count()[0]) $ control_prob = float(control_converted) /  float(control_total) $ print ("Probability of Control Group converted is {}".format(control_prob))
tmax_day_2018.tmax[100].plot();
print(autos['registration_year'].value_counts(normalize = True).sort_index(ascending = False)) $ print(autos['registration_year'].describe())
(autos["registration_year"] $  .value_counts(normalize=True) $  .sort_index(ascending=False).head(10))
grp = company.groupby('Company') $ grp.size()
logit = sm.Logit(df2.converted, df2[['intercept', 'new_page', 'CA_new', 'UK_new', 'CA', 'UK']]) $ result = logit.fit() $ result.summary()
start = time.time() $ print(list(map(sum_prime, [300000, 600000, 900000]))) $ print("Time taken = {0:.5f}".format(time.time() - start))
question_3_dataframe = question_3_dataframe.merge(population_by_zip, how='left', on=['incident_zip']) $ question_3_dataframe.head(5)
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, inspect, func, desc
avg_usercart_size = priors_reordered.groupby(["user_id"])['add_to_cart_order'].aggregate('count').reset_index(name='avg_user_cart_size') $ avg_usercart_size.head()
u = so.user('1600172') $ print('reputation is' , u.reputation.format()) $ print('no of questions answered - ', u.answers.count)
len(projects)
print df_nona.groupby('segment').apply(lambda v: float(sum(v.accounts_provisioned))/sum(v.district_size)) $ df_nona.groupby('segment').install_rate.describe() $
poverty_2011_2015=poverty_data.unstack(level=2, fill_value=0)
df_users.head()
y_pred
print df.nrOfPictures.sum(), '\n' $ print df.seller.value_counts(), '\n' $ print df.offerType.value_counts(), '\n' $ df.drop(['nrOfPictures', 'seller', 'offerType' ], axis=1, inplace=True)
r6s = r6s[['created_utc', 'num_comments', 'score', 'title', 'selftext']] $ r6s['created'] = pd.to_datetime(r6s['created_utc'],unit='s') $ r6s = r6s[r6s['created']>datetime.date(2017,12,1)]
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop =  pd.read_pickle('df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p')
bymin['2014-8-1 12:30':'2014-8-1 12:59']
gdf["color"] = gdf.apply(lambda feat: 'green' if feat['SiteTypeCV'] == 'Stream' else 'red', axis=1)
autos["brand"].describe()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new,n_old],alternative='larger') $ print("z-score",z_score) $ print("p-value",p_value)
import quandl
data1=pd.read_csv('data/allcoinUSD.csv',header=None)
df_ca[df_ca['ab_page'] == 1]['converted'].mean()
new_sample = new.copy()
svm_model.fit(X_train, y_train)
(p_diffs > ob_diffs).mean()
df["date_time"] = pd.to_datetime(df["date_time"]) $ df['srch_ci'] = pd.to_datetime(df['srch_ci']) $ df['srch_co'] = pd.to_datetime(df['srch_co'])
df = pd.read_sql_query('SELECT ComplaintType, Descriptor, Agency ' $                        'FROM data ' $                        'WHERE Agency = "NYPD" ' $                        'LIMIT 10', disk_engine) $ df.head()
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_mean, (1-p_mean)]) $ new_page_converted.mean()
client.repository.list_models()
aggregates.info()
print("Column names before change") $ df.head(2)
lm=sm.Logit(sub_df2['converted'], sub_df2[['intercept', 'ab_page','Monday']]) $ results=lm.fit() $ results.summary()
df2 = df2.drop('control', axis=1)
for c in classes: $     for i in df[c].iteritems(): $         if math.isnan(i[1]): $             df[c][i[0]] = j $         j = i[1] # prev number
np.corrcoef(data['int_rate'], data['fico_range_high'])
len(interactions_replied), len(red_inter_rep)
df.describe()
df = df.withColumn('city', lit(city)) $ hz.addTransformDescr('city','"city" assigned by harmonization code') $ print("Add 'city' variable")
plt.hist(p_diff) $ plt.ylabel("Frequency", fontsize=12) $ plt.xlabel("Pnew - Pold") $ plt.title("Histogram of the difference between Pnew and Pold") $ plt.show()
variable_name = ['image_or_not', 'format_']+body_feature_name+title_feature_name+['fake_review']+['y']
gb = df2.groupby(['group', 'converted']).count() $ gb
row_num = df.shape[0] $ print("Total rows : {}".format(row_num))
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\FourFiles.zip" $ stories_zip = zipfile.ZipFile('C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\FourFiles.zip') $ print("",stories_zip.namelist()) $ stories_zip.close()
goog = goog['Close']
regions = df.groupby('location') $ print(regions.mean())
c['country'].unique()
keyli = token.sender.tolist() + token.receiver.tolist()
'TensorFlow version: ' + K.tf.__version__
mb.sortlevel('Patient', ascending=False).head()
autos.describe(include = "all")
response = requests.get(form_url('apiVersions')) $ print_body(response, max_array_components=3)
merge=pd.merge(contribs,prop57,on="calaccess_committee_id")
run txt2pdf.py -o"2018-06-18  2015 460 discharges.pdf"  "2018-06-18  2015 460 discharges.txt"
url_reputation = grouped['reputation_score'].agg({'total_reputation': 'sum', 'avg_reputation': 'mean'})
df_int.plot() $ plt.show()
dataA = pd.read_csv('data/dataPorUbicacion_Anios_tmin.csv', header=None)
autos["date_crawled"].str[:10].value_counts(normalize=True, dropna=False).sort_values()
df['room_size']=df['life_sq']/df['num_room']
daily_averages['Weekday'] = daily_averages['Weekday'].apply(lambda x: calendar.day_abbr[x])
df.index.strftime("%A")
intersections_irr = pd.concat([intersections_irr, x_normalized], axis=1, sort=False)
data.describe()
df_cont.shape
hs2017=results.loc[results.date>"2017-01-01",'date'] $ hs2018=results.loc[results.date>"2018-01-01",'date'] $ hstotal=hs2017+hs2018 $ hstotal
my_model_q7_eval = SuperLearnerClassifier(clfs=[clf_base_lr, clf_base_rf, clf_base_nb, clf_base_knn, clf_base_dt, clf_base_svc], stacked_clf=clf_stack_knn, training='probability') $ my_model_q7_eval.fit(X_train, y_train) $ y_pred = my_model_q7_eval.predict(X_valid) $ accuracy = metrics.accuracy_score(y_valid, y_pred) $ print("Accuracy: " +  str(accuracy))
logmod = sm.Logit(ab_df2['converted'], ab_df2[['intercept','ab_page']]) $ results = logmod.fit()
vocabulary_expression['component_1'].sort_values(ascending=False).head(7) $
def auc(preds, targs): $     score = roc_auc_score(to_np(targs), to_np(preds)[:, 1]) $     return score
df_20180113 = pd.read_csv('data/discovertext/hawaii_missile_crisis-export-20180221-113313.csv', $                           infer_datetime_format=True) $ df_20180113.head()
grades >= 5
df.convert_objects(convert_dates='coerce',convert_numeric=True)
d
load2017['actual'] = load2017['actual'][load2017['actual']!='-'] 
df_final_NA.to_csv('2013_attarction_f_rate_NA.csv')
str(1).rjust(5)
sales = pd.read_csv('data/employee_sales.csv') $ print(sales.shape) $ sales.head()
length = [] $ for i in range(len(df_new.author)): $     length_name = len(df_new.iloc[i, 0]) $     length.append(length_name)
ex2.sort_values(ascending = False) $ ex2
model.fit(team_names[predictor_cols],team_names.regular_occurrences)
df.isnull().values.any() $ df.info()
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ % matplotlib inline $ plt.style.use("ggplot")
config = tf.ConfigProto() $ try: $     sess = tf.Session(config=config) $ except: $     sess = tf.Session(config=config)
raw_df = data.import_raw_data() $ raw_df['Assessment'].value_counts()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ print("z-score: {}".format(z_score)) $ print("p-value: {}".format(p_value))
import dateutil.parser $ values = df.values[::-1] $ ticks = map(dateutil.parser.parse, values[:,0]) $ dataset = values[:,1] $ dataset.shape
result = run_pipeline(make_pipeline(), '2015-05-05', '2015-05-05') $ result.head()
df1.info()
from sqlalchemy import func
ts = pd.Series(np.random.randn(3), periods) $ ts
frame.sub(series3, axis='index') # or axis=0
df_passengers.head(3)
dicttagger_location = DictionaryTagger(['location.yml'])
from matplotlib_venn import venn2, venn2_circles $ venn2(subsets={'10': n_jobroles-n_jobroles_tags, '01': n_tags-n_jobroles_tags, '11': n_jobroles_tags}, set_labels = ('Job Roles', 'Tags')) $ print "Total of",len(bag_jobroles.intersection(bag_tags)),"common keywords btw Job Roles and Tags from a total of",\ $       len(bag_jobroles.union(bag_tags)),"different keywords"
loans_df.loan_status.value_counts()
h_aux=[] $ for i in range(0,l2): $     if i not in seq: $         h_aux.append(histories_b5[i]) $ col.append(np.array(h_aux))
idx = df_providers[ (df_providers['drg3']==39) &(df_providers['year']==2011) ].index.tolist() $ print('length of IDX',len(idx))
df_dates_final.shape[0] - df_dates_new.shape[0] $
pivot_ui(df_unit)
m3.fit(lrs, 1, metrics=[accuracy], cycle_len=1)
dfs[index_max]['Time'][dfs[index_max]['Outside Temperature'] == dfs[index_max]['Outside Temperature'].max()]
twitter_archive_clean[twitter_archive_clean['retweeted_status_id'].isnull()==False].shape[0]
new_page_converted = np.random.binomial(1, pnew, nnew)
so = pd.read_csv('../../data/stackoverflow_qa.csv') $ so.head()
repos = pd.read_pickle('data/pickled/new_subset_repos.pkl') $ users = pd.read_pickle('data/pickled/new_subset_users.pkl') $
index_max = max(range(len(values)), key=values.__getitem__)
from skmultilearn.adapt import MLkNN $ classifier = MLkNN(k=20) $ classifier.fit(X_tr[0:len(X_train)-40-1], y_ls) $ classifier.score(X_tr[0:len(X_train)-40-1], y_ls)
image_pred_df.info()
transactions[(transactions.transaction_date < datetime.strptime('2017-01-01', '%Y-%m-%d'))]
import numpy as np
for idx, match_id in full_dataset.match_id.iteritems(): $     date = data.loc[data.match_id == match_id, 'date'].tolist()[0] $     full_dataset.loc[idx, 'date'] = date
statistics = powerConsumptionsPerDay.groupby(['timestamp']).agg(['sum', 'mean', 'min', 'max', 'median']) $ statistics $
df.pipe(adder,2)
test_df = pd.read_csv('house/test.csv')[list(feature_dict.keys())] $ test_df
prob_converted = df2.converted.mean() $ prob_converted
twitter_ar['dogType'] = twitter_ar.text.apply(lambda row: adddogtype(row))
details.head()
%%time $  db = detective.HassDatabase(DB_URL)
df[df['Agency'] == 'NYPD']['Unique Key'].resample('M').count().plot() $ df[df['Agency'] == 'DOT']['Unique Key'].resample('M').count().plot()
intersections_final.head()
house_data.corr()
df_clean3.nsmallest(10, 'rating_numerator')[['text', 'rating_numerator', 'rating_denominator']]
import pandas as pd $ import numpy as np $ pd.set_option('display.max_columns', 500) $ import warnings $ warnings.filterwarnings('ignore')
to_be_predicted_Day2 = 56.14260511 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
calculated_average_df.head()
train = pd.read_csv('train.csv' , parse_dates= ['date_created', 'user_date_created'] ) $ test = pd.read_csv('test.csv' ,parse_dates= ['date_created', 'user_date_created'] )
injury_df.drop(['Acquired','Injury'], axis=1, inplace=True) $ injury_df.drop(injury_df[injury_df['Player_Full_Name'] == ' '].index, inplace=True)
list_of_non_rle_companies = non_rle_pscs.company_number.unique().tolist() $ non_rle_company_at_top_of_chain = pd.DataFrame(graph.run("MATCH p=(c1:Company)<-[:CONTROLS*0..]-(c2:Company)\ $ WHERE c2.uid IN {list_of_non_rle_companies}\ $ RETURN DISTINCT (c1.company_number)",list_of_non_rle_companies=list_of_non_rle_companies).data()) $ len(non_rle_company_at_top_of_chain)
col_short['str_date']=col_short['date_time'].astype(str) $ df_weather['str_date']=df_weather['date_time'].astype(str)
result_df.columns
df_gnis = pd.read_csv(file_name+'_20180601.txt', sep='|', encoding = 'utf-8')
modifications_per_authors_over_time = modifications_over_time.reset_index().pivot_table( $     index=modifications_over_time['timestamp'], $     columns=modifications_over_time['author'], $     values='modifications_norm') $ modifications_per_authors_over_time.head()
tx = tx[tx["filerTypeCd"] == 'COH'] $ tx = tx.rename(columns={"filerHoldOfficeDistrict": "district"})
df[y_col] = df['variety'].str.lower().replace(repl_dir) $ df_noblends = df[df[y_col].replace(repl_dir).str.lower().isin(keep_vars)] $ df_noblends[y_col].unique().size
grid_heatmap.head()
tweets.dtypes
flight.show(2)
cvecdata = cvec.fit_transform(X_train) $ cvec_df  = pd.DataFrame(cvecdata.todense(), $              columns=cvec.get_feature_names())
dfn.News.iloc[0] 
n_old = df2.query('landing_page == "old_page"').landing_page.count()
from sklearn.preprocessing import StandardScaler $ X = df.values[:,1:] $ X = np.nan_to_num(X) $ Clus_dataSet = StandardScaler().fit_transform(X) $ Clus_dataSet
model_df.news_text = model_df.news_text.fillna('') $ model_df.tesla_tweet = model_df.tesla_tweet.fillna('') $ model_df.elon_tweet = model_df.elon_tweet.fillna('')
hot_df.to_csv('data_redditv2.csv')
df_input_clean.fillna(-99999, subset=['Resp_time']).filter("`Resp_time` == -99999").count()
@pyimport numpy as np $ a = PyObject(np.array(1:10)) $ jpyArr = a[:reshape](5,2)  # uses the Python reshape method for an ndarray object (not Julia's reshape function)
browser = Browser('chrome', headless=True) $ url = 'https://twitter.com/marswxreport?lang=en' $ browser.visit(url)
import statsmodels.api as sm $ convert_old = 17489 $ convert_new = 17264 $ n_old = 145274 $ n_new = 145310
import pyspark $ sc = pyspark.SparkContext.getOrCreate() $ sc $
job_requirements.ndim $
print("Probability of individual user converting is", df2.converted.mean())
run txt2pdf.py -o"2018-06-14 2148 OROVILLE HOSPITAL Sorted by Payments.pdf"  "2018-06-14 2148 OROVILLE HOSPITAL Sorted by Payments.txt"
def intermediate_cohort(row): $     if row['unit_flag']=='Completed atleast 10 units' and row['atleast_one_course_completed_or_not']=='Did not complete any course (Course started after 1st July 2018)': $         row['cohort']='Intermediate User' $     return row['cohort'] $ df_users_6['cohort']=df_users_6.apply(intermediate_cohort,axis=1)
popular_centers.plot()
collect.set_custom_filter(exclude_retweets)
df_con = pd.read_csv("countries.csv") $ df_con = df_con.merge(df3) $ df_con = df_con.join(pd.get_dummies(df_con['country'])) $ df_con.head()
noise_graf.head()
most_active = station_activity[0][0] $ most_active $
accuracy = metrics.r2_score(y_test, predictions) $ print("R2 Accuracy:", accuracy)
isiterable([1, 2, 3]) $ isiterable(5)
df2.info()
len(g)
last_year = dt.date(2018, 7, 29) - dt.timedelta(days=365) $ print(last_year)
import ds100_utils $ source_data_url = 'http://www.ds100.org/fa18/assets/datasets/hw2-SFBusinesses.zip' $ target_file_name = 'data.zip' $ data_dir = '.' $ dest_path = ds100_utils.fetch_and_cache(data_url=source_data_url, data_dir=data_dir, file=target_file_name, force=False)
df2[(df2['group']=='treatment') & (df2['converted']==1)].converted.sum()/df2[df2['group']=='treatment' ].group.count()
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_ipb_chord_crown[1][0])) $ perr = np.sqrt(np.diag(pcov_ipb_chord_crown[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
df[df['Updated At'] < df['Shipped At']].shape
week2_df.keys()
user_log_counts.head()
df_TempIrregular.describe()
export_path=cwd+'\\ca_simu_from_python.csv' $ ca_de.to_csv(export_path, index=False)
print(df2[df2['group']=='control']['converted'].mean())
autos.loc[:,"ad_created"] = pd.to_datetime(autos.loc[:,"ad_created"]) $ autos.loc[:,"last_seen"] = pd.to_datetime(autos.loc[:,"last_seen"])
weather_df['TAVG'] = (weather_df['TMIN'] + weather_df['TMAX']) / 2
url = '/Users/jennawhite/documents/wild_west/abnb_project/data/train_users_2.csv' $ bnb = pd.read_csv(url, index_col='id') $ bnb.shape $
building_pa[building_pa.duplicated()]
min_IMDB = scores.IMDB.min() $ min_IMDB
topC = popC15[popC15.content == 'photo'].sort_values(by='counts', ascending=False).head(1) $ topC.reset_index(inplace=True) $ topC = topC.contact[0] $ topC = likes.loc[(likes['contact'] == topC) & (likes['content'] == 'photo')] $ topC.head()
full_data = create_emb_cols(full_data, cats)
dft.groupby(['stamp', 'time_min'])['log_followers_count'].sum()
train_commits[['user_id', 'repo_id', 'log10_commits']].to_csv('data/new_subset_data/train_commits.csv', index=False) $ test_commits[['user_id', 'repo_id', 'log10_commits']].to_csv('data/new_subset_data/test_commits.csv', index=False)
df = frame_masher()
obs_diff = new_page_converted.mean() - old_page_converted.mean()
def getJson(name): $     response = urlopen("http://d.yimg.com/autoc.finance.yahoo.com/autoc?query="+'%20'.join(name.split(' '))+"&region=1&lang=en").read().decode('utf-8') $     responseJson = json.loads(response) $     return responseJson $
worldbible = df[['Verse', 'World English Bible']]
em_sz = 200  # size of each embedding vector $ nh = 500     # number of hidden activations per layer $ nl = 3       # number of layers
after = newdf[newdf["date"] > '2014-07-13']
train_reduced = train_pos.union(train_neg).orderBy(func.rand(seed=seed)) $ train_reduced.cache() $ validation = validation.cache() $ print('reduced training set size:', train_reduced.count()) $ print('validation set size:', validation.count())
plt.scatter(x = listings['host_response_time_categories'], y = listings['review_scores_value'])
df['profit']=(df.state_retail-df.state_cost)*df.bottles_sold $ df['ppb']=(df.state_retail-df.state_cost) $ df['profit_margin']=(df.ppb/df.state_retail)*100
small_movies_data.take(3)
re.findall(r'([+-]?\d+())')
df.loc[df.toes.str.match(pattern1)==True]=df.loc[df.toes.str.match(pattern1)==True].replace(" ","",regex=True) $ df.loc[df.toes.str.match(pattern2)==True]=df.loc[df.toes.str.match(pattern2)==True].replace(" ","-",regex=True) $ df.loc[df.toes.str.match(pattern3)==True]=df.loc[df.toes.str.match(pattern3)==True].replace("'","",regex=True)
df_train = pd.DataFrame({'text': train_texts, 'labels': train_labels}, columns=col_names) $ df_val = pd.DataFrame({'text': val_texts, 'labels': val_labels}, columns=col_names)
dff=df2.drop('aa_page', axis=1)
taxi_hourly_df.index.max()
pickle.dump(lda_cv, open('iteration1_files/epoch3/lda_cv.pkl', 'wb'))
pd.set_option('display.max_columns', None)  $ df = pd.read_csv('autos.csv', encoding='latin-1') $ df1 = pd.read_csv('autos.csv', encoding='latin-1') $ det = pd.read_csv('cnt_km_year_powerPS_minPrice_maxPrice_avgPrice_sdPrice.csv')
listings = pd.read_csv('../final_project/listings.csv') $ listings.head()
page_details = train.Page.str.extract(r'(?P<topic>.*)\_(?P<lang>.*).wikipedia.org\_(?P<access>.*)\_(?P<type>.*)') $ page_details[0:10]
roc_auc_score(preds, fb_test.popular)
bb_pivot_table_min = pd.pivot_table(bb_df, values = ["Inches", "Wt"], index = ['From'], aggfunc = np.min) $ bb_pivot_table_max = pd.pivot_table(bb_df, values = ["Inches", "Wt"], index = ['From'], aggfunc = np.max) $ bb_pivot_table_2 = pd.merge(bb_pivot_table_min, bb_pivot_table_max, left_index=True, right_index=True)
promo_df['after_onpromotion'].value_counts()
df2['intercept'] = 1 $ df2['ab_page'] = pd.Series(np.zeros(len(df2)), index=df2.index)
logger = logging.getLogger() $ logger.setLevel("WARN") $ def lookup_relevant_meetup(project_name, max_meetup_events=0): $
bkk_idx, bkk, b_rank, b_sims = bkk_all
plt.scatter(training_pending_ratio[:,0], training_pending_ratio[:,1], color ='g') $ plt.xlabel('Actual Pending Ratios') $ plt.ylabel('Polynomial Pending Ratios') $ plt.title('Plotting the Pending Ratios V/s Polynomial Pending Ratios', fontweight = 'bold') $ plt.tight_layout()
rent_db.boxplot(column='price', by='bedrooms')
treatment_group = df2.query('group == "treatment"') $ print(treatment_group.shape[0])
def slugging(x): $     bases = x['h']-x['X2b']-x['X3b']-x['hr'] + 2*x['X2b'] + 3*x['X3b'] + 4*x['hr'] $     ab = x['ab']+1e-6 $     return bases/ab $ baseball.apply(slugging, axis=1).round(3)
type(h5)
sorted_posts = gram_collection.find({"contains_tattoo": 1}).sort([("likes", pymongo.DESCENDING)])
santos_tweets.loc[santos_tweets['date'] == '2017-09-08']['text'].unique() # only 2017-09-08 Tweets
day_access = log_with_day.groupBy('dayOfWeek').count() $ day_access.show()
df3 =pd.get_dummies(df3,prefix=['country'], columns=['country']) $ df3.drop('country_CA', inplace=True, axis=1) $ df3.head()
from bs4 import BeautifulSoup             $ example1 = BeautifulSoup(train["review"][0], 'html.parser')  
len(us_companies)
autos["registration_year"].value_counts(normalize=True).sort_index(ascending=False).head(10)
X_cols_stacked = [col for col in df_train.columns if '_past_' in col] $ X_cols_caldata = [col for col in df_train.columns if 'weekday_' in col or 'month_' in col or 'year' in col] $ X_cols = X_cols_stacked + X_cols_caldata $ X = df_train[X_cols]
no_outliers_forecast_exp[(no_outliers_forecast_exp.index >= '2018-06-05') & (no_outliers_forecast_exp.index <= '2018-12-31')].astype(int)
pres_df['location'].isnull().sum(), pres_df.shape
full_df.created.min(), full_df.created.max()
autos['price'].sort_index(ascending=False).head(9)
df.index $
df = pd.read_csv("ab_data.csv") $ df.head(20)
modeling2 = modeling1.join(f_lr_hash_modeling2.select('id'), on='id', how='inner') $ print("modeling2 size: ", modeling2.count())
from sklearn.metrics import jaccard_similarity_score $ jaccard_similarity_score(y_test, yhat)
commits = Query(git_index).get_cardinality("hash") $ print("Total commits: ", get_aggs(commits)) $ all_commits = commits.fetch_results_from_source("hash", "commit_date", dataframe=True) $ print("All commits: ", all_commits.head())
sc.stop()
rf=RandomForestClassifier(labelCol="label", featuresCol="features") $ labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel", labels=labelIndexer.labels) $ pipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6,labelIndexer, OH1, OH2, OH3, OH4, OH5, OH6, assembler, rf, labelConverter])
cities['Mumbai']
for c in df_train.columns: $     print(c, df_train[c].dtype)
df.shape # output is in format of rows, columns
summer[summer['Mean TemperatureC'] <= 24]
boundary = 10
import pandas as pd $ d = pd.read_json('testtweets.json') $ d.head()
import statsmodels.api as sm $ convert_old = df2[df2['group'] == 'control']['converted'].sum() $ convert_new = df2[df2['group'] == 'treatment']['converted'].sum()
df.shape
df3[['CA', 'US']] = pd.get_dummies(df3['country'])[['US', 'CA']]
df
df_bkk.describe()
executable_path = {'executable_path': '/usr/local/bin/chromedriver'} $ browser = Browser('chrome', **executable_path, headless=False) $ url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars" $ browser.visit(url)
random_sample = trainset.sample(5000)
df_joined_dummy['ab_page_new_CA'] = df_joined_dummy['ab_page_new_page'] * df_joined_dummy['country_CA'] $ df_joined_dummy['ab_page_new_UK'] = df_joined_dummy['ab_page_new_page'] * df_joined_dummy['country_UK'] $ df_joined_dummy['ab_page_new_US'] = df_joined_dummy['ab_page_new_page'] * df_joined_dummy['country_US']
df_new['last_active'] = df_new['last_active'].astype(str).str[0] $ df_new['last_active']= df_new['last_active'].apply(pd.to_numeric) $ df_new['last_active']
fi = pd.DataFrame(rf.feature_importances_, X_train.columns) $ fi.columns = ['Importance'] $ fi.sort_values(by = 'Importance', ascending=False)[0:10]
dat_hcad['blk_lower'] = dat_hcad['0'] - dat_hcad['0'] % 100 $ dat_hcad['blk_upper'] = dat_hcad['blk_lower'] + 99 $ dat_hcad['blk_range'] = dat_hcad['blk_lower'].map(str)+'-'+dat_hcad['blk_upper'].map(str)+' '+dat_hcad['COMMERCE'].map(str)
df.columns
day_of_year14.to_excel(writer, index=True, sheet_name="2014")
datascience_tweets[datascience_tweets['text'].str.contains("RT")]['text'].count() # 322
combined_df[combined_df['classifier'].isnull() == True]
2893
ffr.resample("2w")
value=ratings['rating'].unique() $ value
output_fn = "News_Sentiment_Analysis.csv" $ sentiment_df.to_csv(output_fn)
output= "Select * from ABC limit 10" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['user_id','Tweet Content','Retweets'])
ax = ign.groupby('release_year').size().plot(kind = 'bar') $ ax.set_ylabel('Games released')
df_goog['2015']
r1 = requests.get("https://API_KEY@www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31")
df_cleaned3_grouped = df_cleaned3 \ $ .groupby('STATION').sum() \ $ .drop(['ENTRIES','EXITS',"TOTALENTRIES",'TOTALEXITS'], axis=1) \ $ .reset_index()
store.info()
crime.head()
git_blame[git_blame.line == git_blame.line.max()]
for idx, row in df_trips.iterrows(): $     passengers_from_planet = passenger_planets[passenger_planets["planet"] == row["planet"]] $     df_trips.loc[idx, "passenger"] = row["passenger"] = np.random.choice(passengers_from_planet["passenger"]) $ df_trips["passenger"] = df_trips["passenger"].astype("int64")
temp = temp.reset_index()
new_page_converted = np.random.choice([0,1],size=n_new,p=[(1-p_new),p_new])
crsr = cnxn.cursor()
predictions_proba_svc = svc.predict_proba(X_test_scaled) $ predictions_proba_dtc = dtc.predict_proba(X_test_scaled) $ predictions_proba_rfc = rfc.predict_proba(X_test_scaled)
def dp(dataFileName): $     return path.join(pardir,pardir, 'data', dataFileName) $
dfWeek = dfDay.copy(deep=True)
df2.head()
print(len(free_data.country.unique()))
projects.actual_hours.sum()
treatment_df = df2.query('group == "treatment"') $ treatment_cr = treatment_df.query('converted == 1').user_id.nunique()/treatment_df.user_id.nunique() $ new_page_converted = treatment_cr $ new_page_converted
df4.describe()
%matplotlib inline $ cat_group_counts = df.groupby("category").size().sort_values(ascending=False)[:10] $ cat_group_counts.plot(kind="bar", title="Top 10 Meetup Group Categories")
print('Logreg intercept:', log_reg.intercept_) $ print('Logreg coef(s):', log_reg.coef_)
    clean_prices = prices.loc[(prices['EMA_200'].notnull() & prices['5_day_target'].notnull())] $
data.groupby(['Date received', 'State'])['Company'].agg(['count']).pivot_table( $     'count', index='Date received', columns='State', fill_value=0).resample('M').sum().plot(y=['CA', 'FL','TX','NY','IL'], legend=False);
loans_df.earliest_cr_line.value_counts()
titles_list = temp_df2['titles'].tolist()
author_data.head()
sudptable = data.pivot_table(index=["author"], values=["score", "ups", "downs"], aggfunc=sum).reset_index() $ sudptable.sort_values(inplace=True, by="score", ascending=False) $ sudptable[0:9]
df2_unique_users = len(df2['user_id'].unique().tolist()) $ df2_unique_users
print(new_df.isnull().sum()) $
payments_all_yrs = \ $ df_providers.groupby(['id_num','name','year'])[['disc_times_pay']].agg(['sum', 'count']) $ payments_all_yrs.head()
df.head()
gear_box_dict = {"manuell": "manual", "automatik": "automatic"} $ autos["gear_box"].replace(gear_box_dict,inplace=True) $ autos["gear_box"].fillna("not_specified", inplace=True)
DataSet.head(5)
def train_cats(df): $     for n,c in df.items(): $         if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()
data_archie.loc[data_archie['user_id'].isnull()].head(5) $
len(ordered_timelines) + len(list(chain(*unordered_timelines))) == len(USER_PLANS_df)
del donors_c
pbptweets.head()
import pandas as pd $
len(BID_PLANS_df) - len(USER_PLANS_df)
n_unique_users = len(df['user_id'].unique()) $ n_unique_users
df.groupby("cancelled")["created_as_guest"].value_counts()
topic_dist = lda.transform(tf) $ topic_dist
df_state_votes.hill_trump_diff.hist(bins=np.arange(-100, 105, 5))
df2['intercept'] = 1 $ df2['ab_page'] = df2['group'].map({ 'treatment': 1, 'control': 0 })
durConv(100)
bb['close'].mean()
NB_results = pd.DataFrame({'tweet': sentiment_test_tweets,'sentiment':test_predict})
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
dhp = dh[dh["author"].isin(topauthors)].pivot_table(index="publishdateone",values="posts",columns="author", aggfunc=np.sum) $ fig, ax = plt.subplots(figsize=(15,7)) $ dhp.plot(ax=ax, kind='bar', stacked=True) $ ticklabels = [item.strftime('%Y %b') for item in dhp.index] $ ax.xaxis.set_major_formatter(ticker.FixedFormatter(ticklabels)) $
print("Total number of people who follow me: {}".format(len(my_followers)))
people.groupby(lambda x: GroupColFunc(people, x, 'a')).groups
dfFull['ScreenPorchNorm'] = dfFull.ScreenPorch/dfFull.ScreenPorch.max()
df = pd.DataFrame([ [1,2,3],[5,6,7] ], columns = ["col_1", "col_2","col_3"]) $ df
df2_control = df2.query('group == "control"')
y.mean()
datacamp[datacamp["publishdate"]>='2017-01-01'].sort_values(by="publishdate", ascending=True).groupby([datacamp['publishyymm']],sort=False).size().plot(kind='bar', figsize=(15,7), color='b')
df2.tail()
adjmats, timepoints = pt1.runltvmodel(rawdata, numwins=10)
print(dir(ent))
df_everything_about_DRGs.head() $
model.add(Dropout(rate=0.25))
pp.barplot(df=df, filters={'technology': tecs, 'variable': ['CAP_NEW']}, $            title='CAP_NEW - light')
salesDF = sqlContext.read.format('jdbc').\ $           options(url='jdbc:db2://dashdb-entry-yp-dal09-10.services.dal.bluemix.net:50000/BLUDB:user=dash6431;password=wXDVudMyrnia;',\ $                   dbtable='GOSALES.BRANCH').load()
sortedprecip_12mo_df.describe()
listings.dtypes $ calendar.dtypes $ reviews.dtypes $ listings.tail() $
june_acj_data.head()
def weekday(day_int): $     days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'] $     day = days[day_int] $     return day $ df['weekday'] = [weekday(day) for day in df['weekday']]
    import seaborn as sns $     import pandas as pd $     print pd
csvData['street'] = csvData['street'].str.replace(' Avenue', ' Ave') $ csvData['street'] = csvData['street'].str.replace(' Court', ' Ct') $ csvData['street'] = csvData['street'].str.replace(' Drive', ' Dr')
duplicate_row_indexes = list(duplicate_rows.index) $ duplicate_row_indexes
archive_clean[archive_clean['tweet_id'] == 786709082849828864].rating_numerator
speaker_dummies = pd.get_dummies(speeches_metadata_evidence['speaker_bioguide'], prefix = 'speaker_id') $ speeches_metadata_evidence = pd.concat([speeches_metadata_evidence,speaker_dummies], axis = 1) $ speeches_metadata_evidence2 = speeches_metadata_evidence.drop_duplicates(subset = ['bill_id'])
len(df[katespadeseries].userid.unique())
temporal_group = 'weekly' $ df = pd.read_csv('../data/historical_data_{0}.csv'.format(temporal_group)) $ df['date'] = pd.to_datetime(df['date']) $ df = df.set_index('date')
full_data.order_date.min(),full_data.order_date.max()
pickle.dump(lsa_tfidf_df, open('iteration1_files/epoch3/lsa_tfidf_df.pkl', 'wb'))
plt.rcParams['axes.unicode_minus'] = False $ dta_690.plot(figsize=(15,5)) $ plt.show()
df_new['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US']]) $ results = log_mod.fit() $ results.summary() $
train[train.url.isnull()].head()
df['dog_count'].value_counts()
'my string'.index('s')
with_countries = pd.read_csv('countries.csv').join(df2.set_index('user_id'), on='user_id') $ print(with_countries.info()) $ with_countries.head()
df.info() 
dh = datacamp[datacamp["publishdate"]>='2017-01-01'].sort_values(by="publishdate", ascending=True).set_index(["publishdate"]) $ dh["publishdateone"] = pd.to_datetime(dh.publishdate.astype(str).str[0:7]+'-01')
Lab7.head()
x_train[:3]
page_new = df2.converted.mean() $ page_new
import json $ import requests $
df = pd.read_csv('./data/ab_data.csv') $ df.head()
capa2017onshore.head()
columns = inspector.get_columns('measurement') $ for c in columns: $     print(c['name'], c["type"])
g = sns.jointplot(x='longitude', y='latitude', data=data, kind='kde', stat_func=None) $ plt.show()
df_predictions_clean.p2_dog.value_counts()
loanlr = LogisticRegression(C=0.01, solver='liblinear').fit(X,y) $ loanlr
df_users_6.loc[df_users_6['created']<'2017-07-01','DSA_account_created_before_or_after']='Before'
df.groupby(by = ['City']).sum().sort_values(by = ['Sale (Dollars)'], ascending = False).head()
p = df2[df2['converted']==1]['converted'].count()/df2['converted'].count() $ p
import warnings $ warnings.filterwarnings("ignore") $ import numpy as np $ import pandas as pd
countries = wb.get_countries() $ countries.iloc[0:10].ix[:,['name', 'capitalCity', 'iso2c']]
intFeatures = ['ride_distance','ooSuburbanDistance','ooCarSearchTime','quarter_int','month_int','day_int','dayofweek_int','ooSurgeMultiplier','Temp,C','Wind,km/h','Humidity','Barometer,mbar']
df2['intercept']=1 $ df2['ab_page'] = 1 $ df2['ab_page'][df['group'] =='control'] = 0 $ df2['ab_page'][df['group'] =='treatment'] = 1 $ df2.info()
temps_df['Missouri'] > 82
n_new, n_old = df2['landing_page'].value_counts() $ print("new:", n_new, "\nold:", n_old)
n_new = len(df2.query('landing_page=="new_page"'))
re.sub('https?://[A-Za-z0-9./]+','',df.text[0])
tweet_image_clean.tweet_id = tweet_image_clean.tweet_id.astype(str)
bnbAx['language_english'] = np.where(bnbAx['language']=='en', 1, 0) $
archive_copy['source'].head()
data_final.to_csv('Data_final.csv')
df2.user_id.count() == df2.user_id.nunique()
one_hot_domains_questionable = df_questionable_2.groupby('user.id')[media_classes].sum().fillna(0) $ one_hot_domains_questionable = one_hot_domains_questionable.apply(normalize, axis=1).fillna(0) $ tsne = TSNE(n_components=2, learning_rate=150, verbose=2).fit_transform(one_hot_domains_questionable)
df_questionable_3[df_questionable_3['state_MO'] == 1]['link.domain_resolved'].value_counts()
Actual_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ Actual_diff
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='larger') $ z_score, p_value
n_old = df2[df2['group'] == 'control']['user_id'].count() $ print('Elements of treatment group n_old: ',n_old)
1/np.exp(results_countries.params[1])
df2.loc[2893]
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])
plot_helpers.hist_in_range(postings.creation_date, datetime.datetime(2014, 8, 1));
sentiment = stock['elon_tweet'].apply(sia.polarity_scores) $ sent = pd.DataFrame(list(sentiment)) $ sent.index = stock.index $ sent.columns = ['elon_compound', 'elon_neg', 'elon_neu', 'elon_pos'] $ stock = pd.merge(stock, sent, how='left', left_index=True, right_index=True)
pvt['ga:date'] = pd.to_datetime(pvt['ga:date'], format='%Y%m%d', errors='coerce') $ pvt.rename(columns={'ga:transactionId': 'transactionId', 'ga:date': 'date'}, inplace=True) $ pvt
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p0, (1-p0)])
import pandas as pd $ import numpy as np $ import seaborn as sns $ import matplotlib.pyplot as plt $ %matplotlib inline
nobel[nobel['Category'] == 'Literature'].groupby('Birth Country').size().sort_values(ascending=False).head()
old_page_converted = np.random.choice([0, 1], size = n_old, p = [1 - p_old_null, p_old_null])
items = pd.DataFrame.from_records(breadcrumbs_xmlrpc.values()) $ items.postid = items.postid.astype(int) $ items = items.set_index('postid') $ print(items.dtypes) $ items[['title', 'dateCreated']].sort_values('dateCreated').head()
df_usnpl.columns = [remap.get(c,c) for c in df_usnpl.columns]
table.head(20)
bc.head()
df.isnull().any()
bnb.columns
users = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/users.csv' ) $ sessions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/sessions.csv' ) $ products = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/products.csv' ) $ transactions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/transactions.csv') 
1 / _
house_data = house_data.set_index('listing_id')
contribs.info()
def YearColumn(text): $     if isinstance(text,str): $         time1 = parser.parse(text) $         return time1.year $     return np.nan
R16df.head()
pct_passing_math_score = 100 * (total_students_with_passing_math_score / total_students) $ pct_passing_math_score
df = pd.DataFrame(np.random.randn(periods,4), index=dates, columns=list('ABCD')) $ df['A'].head(5)
df_Sentences_not_stops=read_cvs_by_pands(path_database,Sentences_not_stops,None,0) $ df_Sentences_not_stops
no_newpage_treatment = df[(((df.group == 'treatment') & (df.landing_page != 'new_page')) | ((df.group != 'treatment') & (df.landing_page == 'new_page')))] $ no_newpage_treatment.shape[0]
ser6.mean()
x = np.random.normal(size=4)
jobs_data.tail()
quantiles = df_final[features].quantile(q=[0.25,0.5,0.75]) $ quantiles = quantiles.to_dict() $ quantiles
ex4.drop([1,2])
data_year_df['Date'] = pd.to_datetime(data_year_df['Date']) $ data_year_df.head() $
pos_data_train['rating'].count()
dum_dum = DummyClassifier() $ dum_dum.fit(X_train, y_train) $ print("The accuracy of the dummy classifier is {:.2f}".format(dum_dum.score(X_train,y_train)))
linreg = linear_model.LinearRegression() # call the sklearn.linear_model function $ linreg.fit(quadratic, y) # the fit() function creates the best fit line
gMapAddrDat.set_endRow_OutDf_caching(3)
end_date = pd.Series(pd.to_datetime(end_date).strftime('%Y-%m'),index=churned_ix)
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.binomial(1, p_null, n_new) $     old_page_converted = np.random.binomial(1, p_null, n_old) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean())
hundred_stocks_df.to_sql(con=conn_helloDB, name='hundred_stocks_twoyears_daily_bar', if_exists='replace', index=False)
df.T #transpose
hours = appointments.copy()
len(data.thumbnail.unique())
df_ab_page.reset_index().head()
len(corpusDF)
save_n_load_df(promo_df, 'promo_df1.pkl')
df.loc[ [100,103] ]  # by row labels
df_combined['rank_diff'] = df_combined['home_rank'] - df_combined['away_rank']
students[(students.gender == 'F') & (students.weight >= 140)]
d = datetime.date(2016, 7, 8) $ d.strftime("On %A %B the %-dth, %Y it was very hot.")
result[result['dt_deces'].notnull()].shape
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\olympics.dta" $ df = pd.read_stata(path) $ df.head(5)
Y_df = pd.DataFrame(train_df12.iloc[:,0]) $ Y_df.head()
prop_caba_gba = propiedades[propiedades['state_name'].str.contains('Capital Federal') | propiedades['state_name'].str.contains('G.B.A.')] $ prop_caba_gba.sample()
pivoted = bdata.pivot_table('Total', index=bdata.index.time, columns=bdata.index.date)
now = datetime.datetime.now() $ dt = relativedelta(years=1) $ year_ago = now - dt $ year_ago_str = year_ago.strftime("%Y-%m-%d") $ print(year_ago_str)
very_pop_df = au.filter_for_support(popular_trg_df, min_times=7) $ au.plot_user_dominance(very_pop_df)
logs.date
model_w.summary2() # For categorical X.
df = df[(df.Opened > '01/01/2009')]
portfolio_df.reset_index(inplace=True) $ adj_close_acq_date = pd.merge(adj_close, portfolio_df, on='Ticker') $ adj_close_acq_date.head()
plt.scatter(sing_fam.rooms.values, sing_fam.rp1lndval.values);
token_send_add_receiveAvg_month = token_sendreceiveCnt.groupby("ID").agg({"sendReceiveCnt":mean_except_outlier}).reset_index()
%sql select * from mysql.user; $
filepath = os.path.join('input', 'input_plant-list_SK.csv') $ data_SK = pd.read_csv(filepath, encoding='utf-8', header=0, index_col=None) $ data_SK = data_SK[data_SK.energy_source != 'Solar'] $ data_SK.head()
print("Loaded Data contains {} rows and {} columns".format(*taxiData.shape))
df_protest.columns.tolist()
dfNew=pd.concat([df['Country'],df],axis=1) $ df[dfNew.columns.unique()]
logistic_mod_pagecountry = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'UK','US']]) $ results_pagecountry = logistic_mod_pagecountry.fit() $ results_pagecountry.summary()
fraq_volume_m_coins = volume_m.div(volume_m.sum(axis=1), axis=0)
g8_aggregates = g8_groups.agg({ $     'area': ['min', 'max'], $     'population': ['mean', 'std'], $     'gdp': ['mean', 'std'], $ })
df_users_6.shape
import pandas as pd $ import numpy as np $ import re $ pd.set_option('display.max_columns', None)
Customers_df.boxplot(by = 'New_or_Returning', column='Sales_in_CAD', figsize=(10,7)) $ plt.ylim(0, 45000) $ plt.title('Sales to New vs. Returning customers from 2010 - 2015') $ plt.xlabel('Type of Customer') $ plt.ylabel('Sales in CAD') $
df.resample('A').mean()
sns.distplot(reddit_master['age'], kde = False) $ plt.xlabel("Age (hours)") $ plt.ylabel('Frequency') $ plt.title('Post Age Distribution');
weather = join_df(weather, state_names, "file", "StateName")
df = df.dropna() $ df.tail(5)
my_gempro.map_uniprot_to_pdb(seq_ident_cutoff=.3) $ my_gempro.df_pdb_ranking.head()
log_mod=sm.Logit(df_new['converted'], df_new[['intercept','CA','UK']]) $ results_mod = log_mod.fit() $ results_mod.summary()
from pysumma.Simulation import Simulation $ from pysumma.Plotting import Plotting
All_tweet_data_v2.name[All_tweet_data_v2.name.str.len() < 3].value_counts()
country_df = pd.read_csv('countries.csv') $ country_df.head()
df_weather = pd.read_csv("../data/weather.csv", parse_dates=["date"]) $ display(df_weather.head()) $ print("This table has {npnts} data points and {col} columns.".format(npnts=df_weather.shape[0], col=df_weather.shape[1]))
recipes.iloc[135598]['ingredients']
percipitation_measurement_df = pd.DataFrame(percipitation_year[:], columns=['Date','prcp',]) $ percipitation_measurement_df['Date'] =  pd.to_datetime(percipitation_measurement_df['Date']) $ percipitation_measurement_df.set_index('Date', inplace=True) $ percipitation_measurement_df.head()
contractor[contractor['contractor_id'].isin([139,140,228,236,238])]
theft.sort_values('DATE_OF_OCCURRENCE', inplace=True, ascending=True) $ theft.head()
df.Date = pd.to_datetime(df["Date"]) $ df.head()
df.fillna(0, inplace=True)
reduced_trips_data.shape # not much outliers removed as data is not normally distributed 
pd.Series(np.random.randint(0,10,10)).plot();
people.iloc[2]
p_old = df2.query('converted == 1').user_id.count() / df2['user_id'].count() $ print(p_old)
df_precipitation.head()
fit = sm.tsa.ARIMA(df[y], (1,0,1), exog = exogx).fit()
df['custcat'].value_counts()
from donthackme import CONSUMER_KEY, CONSUMER_SECRET, TOKEN, TOKEN_SECRET
import numpy as np $ Serial_No = [] $ for i in range(len(tweets)): $     Serial_No.append(i) $ tweets['Serial'] = np.array(Serial_No)
print(fruits + 2) $ print(fruits - 2) $ print(fruits * 2) $ print(fruits / 2)
( $     autos["ad_created"].str[:10] $     .value_counts(normalize=True,ascending=False) $     .head(10) $ )
df2.groupby([df2['group']=='treatment',df2['converted']==1]).size().reset_index()[0].iloc[3]/df2.query('landing_page == "new_page"').user_id.nunique()
new_page_converted = np.random.choice(2, size=n_new ,p=[p_new,1 - p_new]) $ new_page_converted.mean()
data = pd.read_csv('sample.csv')
cust_age = pd.read_csv('../data_clean/customer_age.csv') $ cust_age.head()
n_new = df2.query('landing_page == "new_page"').landing_page.count() $ n_new
ab_data.timestamp.describe()
count/len(p_diffs)
scores = pd.read_csv(mypath+'/tfidf_cosine.csv', index_col = 0)
autos.head()
autos['year_of_registration'].describe()
start_error_set = error_set['timestamp'].min() $ end_error_set = error_set['timestamp'].max() $ print('The ERROR started on the {} and has been ended on {}'.format(start_error_set,end_error_set))
df3 = sm.Logit(df2['converted'], df2[['intercept','ab_page']])  #Implementing OLS model to fit to the regression line. $ df3 = df3.fit()
f0.exclude_list_filter
game()
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $ new_page_converted.mean()
autos = autos[autos["registration_year"].between(1900, 2100)]
parsed_liberia_df = pd.concat([liberia_df_new_cases, liberia_df_new_deaths]) $ parsed_liberia_df.rename(columns={'Date': DEFAULT_NAME_COLUMN_DATE, $                                   'Variable': DEFAULT_NAME_COLUMN_DESCRIPTION, $                                   'National': DEFAULT_NAME_COLUMN_TOTAL}, inplace=True) $ parsed_liberia_df[DEFAULT_NAME_COLUMN_COUNTRY] = countries['liberia']
player_attributes = pd.read_sql_query('select * from Player_Attributes', conn)  # don't forget to specify the connection $ print(player_attributes.shape) $ player_attributes.head()
import gmplot $ gmap = gmplot.GoogleMapPlotter.from_geocode("New York",10)
c_df = e_p_b_one.groupby(e_p_b_one.index).size().reset_index() $ c_df.index = c_df.TimeCreate $ c_df.rename(columns={0:"Crime Count in 5k02"},inplace=True)
proj_df['math_science'] =proj_df['Project Subject Category Tree'].fillna('').apply( $     lambda x: 1 if 'Math & Science' in x else 0 $ ) $ proj_df.math_science.sum()
archive_clean.info()
m3 = np.dot(m2.T,m) $ print("m3: ", m3)
count_polarity=pd.concat([count_polarity_2012,count_polarity_2013,count_polarity_2014 $                            ,count_polarity_2015,count_polarity_2016], axis=1)
df2[['old_page','new_page']] = pd.get_dummies(df2['landing_page']) $ df2[['treatment', 'ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('new_page', axis=1) $ df2 = df2.drop('treatment', axis=1)
f.exclude_list_filter
qthis = datetime(2018,3,1)
litecoin_github_issues_url = blockchain_projects_github_issues_urls[3] $ litecoin_github_issues_df  = pd.read_json(get_http_json_response_contents(litecoin_github_issues_url))
print("Value in cell (row 3, col 2):", ) $ print(sheet.cell_value(3, 2))
plt.scatter(X,y2) $ plt.plot(X, np.dot(X_17, linear.coef_) + linear.intercept_, c='red')
tips.head()
lgComp_df = wcPerf1_df.groupby(['lgID']).sum() $ lgComp_df
datetime.strptime('20091031', '%Y%m%d')
all_cards = all_cards.loc[~(all_cards.printings.map(lambda x: bool(set(invalid_sets) & set(x))) & all_cards.supertypes.map(lambda x: x != ["Basic"]))]
from sklearn import preprocessing $ max_abs_scaler = preprocessing.MaxAbsScaler() $ X_train = max_abs_scaler.fit_transform(pokemon_train.iloc[:,2:8]) $ X_test = max_abs_scaler.fit_transform(pokemon_test.iloc[:,2:8])
top_10_authors = git_log.author.value_counts().head(10) $ top_10_authors
dtc_features = sorted(list(zip(test_features, dtc.feature_importances_)), key=lambda x: x[1], reverse=True) $ dtc_features
row = X_train_mybag[10].toarray()[0] $ non_zero_elements_count = len([number for number in row if number!=0]) $ grader.submit_tag('BagOfWords', str(non_zero_elements_count))
cols = ['sentiment','id','date','query_string','user','text']
!python -m pip install --upgrade pip
git_log.author.head()
df_raw = pd.read_csv('top40.csv') $ print(df_raw.info()) $ df_raw.drop(['link', 'list_title', 'page_title', 'record_label'], axis=1, inplace=True) $ df_raw.sample(10) $
p
tweet_data.retweet_count.hist(bins=70)
emails_dataframe['institution'] = emails_dataframe['address'].str.split("@").str.get(1) $ emails_dataframe
xp.status.value_counts()
df = pd.read_excel('SampleData.xlsx', sheet_name='SalesOrders')
consumer_key = '' $ consumer_secret = '' $ access_token = '' $ access_token_secret = ''
lr = LinearRegression() $ cv_score = cross_val_score(lr, features_regress_vect, overdue_duration, $                            scoring='neg_median_absolute_error', cv=5) $ 'MedianAE score: {:.3f}, std: {:.3f}'.format(np.mean(cv_score), np.std(cv_score))
logs
snowshoe_prob = snowshoe_df[['month','snowshoes']].groupby(['month']).mean() $ snowshoe_prob.plot.bar() $ demo.apply_plot_settings()
features = cv.get_feature_names() + features $ feature_importances = model.feature_importances_ $ features_df = pd.DataFrame({'Features': features, 'Importance Score': feature_importances}) $ features_df.sort_values('Importance Score', inplace=True, ascending=False)
at.ADF(pair[stock1 + '_return'])
df['Ranking Full URL on Sep  1, 2017'] = df['Ranking Full URL on Sep  1, 2017'].fillna('')
df['Mo'] = df['datetime'].map(lambda x: x.month) $ df.head()
grouped = df_providers.groupby(['year','drg3']) $ (grouped.count) $ grouped.quantile
DataSet.head()
df_uk = df_new[df_new['country'] == 'UK'] $ df_uk['converted'].mean()
result = vocabulary_expression.sort(['components_1':'component_8'], ascending=False).head(10)
%matplotlib inline $ import matplotlib.pyplot as plt $ plt.figure() $ df_q.plot(x = 'UNIQUE_DATE')
cityID = '04cb31bae3b3af93' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Miami.append(tweet) 
potholes = df[df['Descriptor'] == 'Pothole']
top500=df15.groupby(df15.store_number).sum().nlargest(500,'sale').index
merge.sort_values("amount", ascending=False)
twitter_archive.info()
df_birth.population.value_counts(dropna=False).head()
p(locale.getdefaultlocale)
countries_df.nunique()
I decided to merge the snow data into one snowtotal field that merges snowfall and snow depth measurements.
t1.year
train_df.head()
df["pickup_hour"] = df["pickup"].dt.hour $ df["pickup_dow"] = df["pickup"].dt.dayofweek $ df["pickup_week"] = (df["pickup"].dt.day - 1) // 7 + 1 $ df["pickup_month"] = df["pickup"].dt.month $ df["pickup_year"] = df["pickup"].dt.year
cur.execute('SELECT COUNT(*) FROM materials') $ print(cur.fetchone())
df.drop_duplicates(['text'], keep='first',inplace=True) $ df.reset_index(drop=True, inplace=True)
m3.freeze_to(-1) $ m3.fit(lrs/2, 1, metrics=[accuracy])
import numpy as np $ import pandas as pd $ from matplotlib import pyplot as plt
z =  np.round(sum(top_users.values())/len(pitches), 3)*100 $ print('The top 20 users have {}% of the pitches'.format(z))
stmt = text("SELECT name, id, fullname, password " $ ...             "FROM users where name=:name")
df2 = df2.drop_duplicates(['user_id'])
slug_not_found[:10]
df_us.sample(5) $ df_us[df_us['ab_page'] == 1]['converted'].mean()
autos['price'].sort_index(ascending=False).head(9)
food["created_date"].head()
last_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first() $ last_date
reddit['Time'] = reddit['Time'].dt.tz_localize('UTC').dt.tz_convert('US/Eastern') 
len(vocab)
TC = ["birth certificate", "birthcertificate", "benghazi", "whitewater", "jfk", "fake news", "fakenews", "deep state", "deepstate", "global warming", "globalwarming","vaccines","autism"] $
pd.Series(pd.date_range("2018-07-04", periods=3, freq="D"))
bild = bild[bild.message != "NaN"] $ spon = spon[spon.message != "NaN"]
df_test.dtypes
print('Best score:', gs.best_score_)
print('Most dril: {}'.format(tweets_pp[tweets_pp.handle == 'wint'].sort_values('dril_pp', ascending=False).text.values[0])) $ print('Least dril: {}'.format(tweets_pp[tweets_pp.handle == 'wint'].sort_values('dril_pp', ascending=True).text.values[0]))
sns_plot = sns.lmplot(x='score',y='retweet_count',data=rating_and_retweet, fit_reg=False,scatter_kws={'alpha':0.05}) $ sns_plot.savefig("score_vs_retweet.jpg")
df.dtypes
pd.concat(pieces)
SelectedHighLows = AAPL.loc["2017-06-20":"2017-07-20", ["high", "low"]] $ SelectedHighLows
df.info()
print(str(inspector.get_pk_constraint("stations"))) $ print(str(inspector.get_pk_constraint("measurements")))
browser = Browser("chrome", headless=True)
with open('nutrition_tweets.pkl', 'rb') as f: $     nutrition_tweets = pickle.load(f)
y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled) $ mse = mean_squared_error(y_test, y_pred) $ np.sqrt(mse)
import pandas as pd $ train_df = pd.read_csv("train.csv") $ test_df = pd.read_csv("total_.csv", skiprows = 1)
dupes_to_delete = dfd.duplicated(subset=['brand', 'outdoor_model', 'indoor_model']) $ dupes_to_delete.value_counts()
train.OPTION.value_counts()
last_obs_date = session.query(Measurement.date).\ $ filter(Measurement.station == mostactive_station).\ $ order_by(Measurement.date.desc()) $ last_obs_date[0]
len(nouns)  # number of noun tokens
results_countries.summary()
mvoid_to_bson_id(df_map[2,1])
session.query(func.count(Stations.station)).all() $
noatar_grades = noatar.groupby('Grade').size() $ print(noatar_grades) $ noatar_grades.plot.bar()
n_old = len(df2[(df2['landing_page'] == 'old_page')]) $ n_old
filtered_df = classification_df $ plt.figure() $ plt.title('Reward in Dollar') $ plt.scatter(filtered_df['rewardInDollar'], filtered_df['best'], marker= 'o', s=20) $ plt.show()
aug2014 = pd.Period('2014-08', freq='M') $ aug2014 , aug2014.start_time, aug2014.end_time
df.shape  # there are 8 columns and 20k rows, read is successful
today = "2018-07-25" $ past = pd.to_datetime(today) - pd.DateOffset(years=18) $ print(past) $ birth_dates["Name"].loc[birth_dates["BirthDate_dt"]<=past].head()
df_img_predictions_copy.info()
ls_metac_colnames = ['MET_DATE2', 'METAC_SITE_NM2', 'MET_DATE3', 'METAC_SITE_NM3', 'MET_DATE4', 'METAC_SITE_NM4', 'MET_DATE5', $                      'METAC_SITE_NM5', 'MET_DATE6', 'METAC_SITE_NM6', 'MET_DATE7', 'METAC_SITE_NM7']
to_remove = creations[ $     (creations["length_at_creation"] < 100) & $     (creations["creator_autoconfirmed"] == True) $ ] $ creations = creations.drop(to_remove.index)
df_sb['cleaned_text'] = df_sb['text'].apply(lambda x : text_cleaners(x))
ao18_coll, db = au.get_coll("ausopen18") $ ao18_qual_coll, _ = au.get_coll("ausopen18_qual")
gbdt_grid.fit(X_train, y_train) $
guardian_data.info()
jail_census.loc["2017-02-01"]
new_page_converted = np.random.binomial(1, p_new, n_new) $ len(new_page_converted)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],alternative="smaller") $ print(f"z-score: {z_score}") $ print(f"p-value: {p_value}")
import matplotlib.pyplot as plt
tweets_df.isnull().sum()
autos['ad_created'] = autos['ad_created'].str[:10] $ autos['ad_created'].value_counts(normalize=True, dropna=False).sort_index()
my_columns = list(data.columns) $ my_columns
ndf = ndf.assign(Norm_reviewText = normalize_corpus(ndf.reviewText))
outlier_detection.basic_cutoff
for i in cpi_all.Measure.cat.categories.tolist(): $     print i
comment_author_counts = merged.comment_author.value_counts() $ data['comment_author_num_comments'] = merged.comment_author.apply(lambda x: comment_author_counts.loc[x])
season_groups.ngroups
df.head(1)
autos["price"].value_counts().head(20) #Print the number count of each price point
missing_info = list(df_.columns[df_.isnull().any()]) $ missing_info
with open('library-Al.meam', 'w') as f: $
my_animals = ["Dog", "Cat", "Hippo", "Dog"] $ print(my_animals)
train_set.head(4)
new_texas_city.set_index("Measurement_date", inplace = True) $ texas_city = new_texas_city $ print(texas_city.head()) $ print(type(texas_city.index))
print("There are %d documents in this collection." % (index.maximum_document() - index.document_base()))
df2['CA_new'] = df2['new_page'] * df2['CA'] $ df2['UK_new'] = df2['new_page'] * df2['UK']
pwd = [i[0] for i in pwd]
test_df = data.iloc[-10:].drop(columns=['Adj. Open','Adj. High','Adj. Low','Adj. Volume','Adj. Close', 'Ex-Dividend', 'Split Ratio'])
control_group = len(df2.query('group=="control" and converted==1'))/len(df2.query('group=="control"')) $ control_group
lmscore.summary() ##I think I can use any one of the two summary types
df2.head()
hp.init_tmpo() $ hp._tmpos.debug = False $ hp.sync_tmpos()
param_grid = {} $ param_grid['penalty'] = ['l1', 'l2'] $ param_grid['C'] = [0.1, 1.0, 10]
purch = pd.get_dummies(auto_new.Purchased) $ purch = purch.drop(["No"], axis=1) $ purch = purch.rename(columns={"Yes":"Purchased"}) $ purch.head()
df = {} $ for RunID in RunIDs.values(): $     db.call_non_select_stored_proc(DBConnections.FRAME_USER, '[Frame_User].[FinFC].[USP_HHdataVarAnalysis]', params=(RunID, '2017-10-01', '2018-01-01', 'Forecast', 'all'), print_sql=True) $     df[RunID] = db.simple_query(sql, DBConnections.FRAME_USER)
store_items = store_items.drop(['watches', 'shoes'], axis=1) $ store_items
df_1 = pd.read_csv('tweet_csvs/realDonaldTrump_tweets.csv', index_col = None, header = 0, $                      parse_dates=['created_at'], infer_datetime_format = True, dayfirst = True)
pd.crosstab(index=mydf.comp, columns=mydf.dept)
param_grid = { $     "days": [7, 10, 12, 14, 25, 30] $ } $ grid_search = GridSearchCV(MedianReg(), cv=CVIterator, param_grid=param_grid)
positive_model=topic(positive_list,num_topics=20)
target_google.head()
help( pd.tseries.offsets.DateOffset)
engine = create_engine('postgresql+psycopg2://aact:aact@aact-db.ctti-clinicaltrials.org:5432/aact') $ df = pd.read_sql_query("SELECT * FROM studies WHERE plan_to_share_ipd != 'Null'", engine)
HTML(string=html_out).write_pdf('../reports/report.pdf')
most_informative_features_top_and_bottom(vectorizer=vectorizer, classifier=rdg2, binary=False, n=15)
tfav.plot(figsize=(16,4), label="Likes", legend=True)
import datetime $ rng = pd.date_range('2015-12-01 20:00', periods=100, freq='S') $ df = DataFrame({'sales':np.random.randint(0, 500, len(rng))}, index=rng) $ df.head()
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)])
df2=df.copy()
tweets['text_fmt_stem'] = tweets['text'].apply(lambda x:stemmer.stem(_get_tokens(x)))
def tokenizer(text): $     return text.split() $ tokenizer('runners like running and thus they run')
weather = pd.read_csv('weather.csv', sep=',', parse_dates=['date'], $                       infer_datetime_format=True,low_memory=False)
s - pd.tseries.offsets.Day(2)
geo_segments_all_simple = geo_segments_all[['Segment ID','Street','speed','level','SHAPE_Leng','date_time_hour','avg_traffic_flow','estimated_number_vehicles','CO2_grams']]
%%time $ max_key = max( r_dict.keys(), key = get_nextday_chg ) $ print('largest change in price between any two days: '+ str( get_nextday_chg(max_key) ) )
dfClientes.shape[0]
users.creation_source.value_counts()
name_filter = girls['Name'] == 'FATIMA' $ girls[name_filter]
oneMinusP = 1-pMean $ oneMinusP
grinter1 = inter1.groupby(['user_id','item_id']).size().reset_index() $ grinter1.columns = ['user_id','item_id','number_int'] $ grinter1.shape
run txt2pdf.py -o"2018-06-19 2013 FLORIDA HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2013 FLORIDA HOSPITAL Sorted by discharges.txt"
X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']]) $ X[0:5]
top10_topics_1 = events_enriched_df[['event_id','topic_id','topic_name']].groupby(['topic_id','topic_name']).count() $ top10_topics_1.rename(columns={'event_id':'count_event_per_topic'}, inplace=True) $ top10_topics_1.reset_index(inplace=True) $ top10_topics_1.head(5)
%load "solutions/sol_2_18.py"
users.groupby('CreationDate')['LastAccessDate'].count().plot() $
tweet_archive_clean.drop(['new'], axis= 1, inplace= True)
Test.SetFlowVals
most_retweeted_tweeps_df=convert_index_most_retweeted(temp) $
df_subset['diff'] = df_subset.apply(diff_money, axis = 1, pattern = pattern) $ print(df_subset.head()) $
query_date = datetime.now().strftime("%m/%d/%Y")
datatest.info()
samples_query.execute_sql('SELECT ID, Name, DOB, SSN FROM Sample.Person')
nameList = ["BACK", "BAIS", "SUN", "PREAUTHORIZED"] $ nameAmtDict = {} $ for name in nameList: $     updateDict(name) $ nameAmtDict
type( temp_cat )
df3 = df2.join(countries_df.set_index('user_id'), on='user_id') $ df3[['CA', 'UK', 'US']] = pd.get_dummies(df3['country']) $ df3.drop('CA', axis =1, inplace=True); $ df3.head()
url='https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key'+API_KEY+'&start_date=2017-01-01&end_date=2017-12-31' $ json_response= requests.get(url) $ json_response.status_code $ RenderJSON(json_response.text) $
top_supports['contributor_fullname'] = top_supports.contributor_firstname + " " + top_supports.contributor_lastname
assert len(index_missing) == 0# all missing values before last 6 months of 2016 filled
[h for h, v in opener.addheaders]
newdf.yearmonth = newdf.yearmonth.astype("str")
autos.odometer_km.describe()
df_users_mvp=df_users_6[df_users_6['cohort']=='MVP User']
import numpy as np $ import pandas as pd $ rainfall = pd.read_csv('data/Seattle2014.csv')['PRCP'].values $ inches = rainfall / 254.0  # 1/10mm -> inches $ inches.shape
nrn = nm.load_neuron(example_swc)
df_int = (df / df.iloc[0]) - 1 $ df_int.head()
length_of_old = (df2[df2['landing_page'] == "old_page"].user_id.count()) $ length_of_old
logit_countries_model2 = sm.Logit(df4['converted'], df4[['abs_page','country_UK', 'country_US', 'intercept']]) $ results_countries_2 = logit_countries_model2.fit()
X_train_predict = model.predict(X_train) $ X_test_predict = model.predict(X_test)
forecast_df['male_pop'] = 0 $ for ind, row in SANDAG_age_df[SANDAG_age_df['SEX'] == 'Male'].iterrows(): $     forecast_df.loc[ind, 'male_pop'] += row['POPULATION'] $ forecast_df.head()
df1 = df_arch_clean.copy() $ df1['timestamp'] = pd.to_datetime(df1['timestamp']) $ df1['day_of_week'] = df1['timestamp'].dt.weekday_name $
rfc = RandomForestClassifier() $ model=rfc.fit(X_train, y_train) # just to run the next line
df_clean2['timestamp'] = pd.DatetimeIndex(df_clean2['timestamp'])
joined = joined.merge(tr_roll, 'left', ['date', 'store_nbr'])
header=['timestamp', 'imsi', 'msisdn', 'cell_tower', 'event_type', 'lat', 'lng', 'imei']
) $ pd.read_sql('desc actor', engine)
for letter in list('GH'): $     ab_groups[letter].rpv.plot() $
from scipy import stats $ virginica = iris_new[iris_new['Species'] == 'Iris-virginica']['PetalWidth'] $ versicolor = iris_new[iris_new['Species'] == 'Iris-versicolor']['PetalWidth']
df.breed.value_counts()[0:19].plot(kind='bar');
df['cleanText'] = np.array([clean_tweet(tweet) for tweet in df['text']]) $ df['label'] = np.array([ analyze_sentiment(tweet) for tweet in df['cleanText'] ])
df['critic'].value_counts()
features = cv.get_feature_names() $ feature_importances = model.feature_importances_ $ features_df = pd.DataFrame({'Features': features, 'Importance Score': feature_importances}) $ features_df.sort_values('Importance Score', inplace=True, ascending=False)
SCN_BDAY_qthis.scn_age.describe()
article_project = pd.read_csv('data_old/article_project.csv') $ print(article_project.shape) $ article_project.head()
sample_x1.shape,sample_y1.shape,sample_value.shape
autos["registration_year"].describe()
df_user_count.apply(np.log).hist() $ plt.show()
to_be_predicted_Day3 = 22.34206034 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
import quandl $ with open("quandl_apkey.txt", "r") as keyfile: $     key = keyfile.read() $ quandl_key = key.rstrip() $ quandl.ApiConfig.api_key = quandl_key # "evWfebtKvTVN_dxvWqau"
openSenate = openstates[openstates["chamber"] == "upper"] $ legSenate = leg[leg["filerHoldOfficeCd"] == "STATESEN"]
ts = pd.Series(np.random.randn(3), dates) $ ts
df2[df2.user_id == 773192]
plt.hist(null_value) $ plt.axvline(obs_diffs, c="r")
df_columns[ ~df_columns['Descriptor'].isnull() & df_columns['Descriptor'].str.contains('Loud Music/Party') ].groupby(["Hour of day","AM|PM"]).size().reset_index().sort_values([0], ascending=[False]) $
analysis = None
mean_dist = reddit['Above_Below_Mean'].value_counts()/len(reddit) $ mean_dist
ew_portfolio_ts = pd.read_pickle('../output/ts_base_portfolio.pkl')
df3_holidays = df3.copy() $ df3_holidays['y'] = np.log(df3_holidays['y'])
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).head(10)
svc = SVC() $ svc.fit(X_train, Y_train) $ Y_pred = svc.predict(X_test) $ acc_svc = round(svc.score(X_train, Y_train) * 100, 2) $ acc_svc
data.index[0] = 15
df2.rename(columns={'ID':'id'}, inplace=True)
n_new = df2[df2['landing_page']=='new_page'].user_id.count() $ n_new
make_simple_english(MyText, threshold=10.0, dictionary=my_dict, save_bypass=True)
body
twitter_archive.loc[(twitter_archive['name'].str.islower()) & (twitter_archive['text'].str.contains('name is'))]
player_unique = baseball.player + baseball.team + baseball.year.astype(str) $ baseball_newind = baseball.copy() $ baseball_newind.index = player_unique $ baseball_newind.head()
data_df.describe()
df = pd.DataFrame()  #If in doubt, start with an empty DataFrame. $ df['SP500'] = np.log(p['Adj Close']) - np.log(p['Adj Close']).shift(1)  #Make sure there's no ^ in the column name. $ p = web.DataReader("^VIX", 'yahoo', start, end) $ df['VIX'] = np.log(p['Adj Close'])-np.log(p['Adj Close']).shift(1) $ df.tail()
n_new = df2.query('group=="treatment"').shape[0] $ n_new
hr[hr["icustay_id"]==14882].plot(x="new charttime", $                                  y=["systolic", "diastolic"]) $ hr[hr["icustay_id"]!=14882].plot(x="new charttime", $                                  y=["systolic", "diastolic"])
stations = session.query(Stations.name, Stations.station).all() $ stations_df  = pd.DataFrame(stations,columns=["Station Name", "Station ID"]) $ stations_dict = stations_df.to_dict(orient='records') $ stations_dict #Display the dictionary response of all the stations name and their respective station IDs.
s=table.find(text='Survivors').find_next('td').text $ survivors=re.search(r'\d+', s).group() $ survivors
test[['id','visitors']].to_csv('submission_avg.csv',index=False)
internet
df_twitter_archive.rating_numerator.value_counts()
soup.find_all('p')
kick_projects.isnull().sum()
sns.kdeplot(utility_patents_subset_df.number_of_claims, shade=True, color="purple") $ plt.show()
dfMovies.head(30)
retweets = tweets["retweet_count"] $ print (retweets.dtype) $ print (retweets.mean()) # calculate mean $ print (retweets.drop(1).head()) #drop using index $ print (tweets["screen_name"].drop_duplicates().head()) #drop duplicated names
sentiment.iloc[309402]['datetime']
utility_patents_subset_df['number-of-figures'].describe()
to_seconds(between_all_posts).plot( $     kind='hist', $     range=(0, 100_000) $ )
bikedataframe.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)
prop = props[props.prop_name == "PROPOSITION 062- DEATH PENALTY. INITIATIVE STATUTE."]
import pickle $ pkl_file = open('mentioned_bills.pkl', 'rb') $ mentioned_bills_all = pickle.load(pkl_file)
df_new['page_US'] = df_new['ab_page'] * df_new['US'] $ df_new['page_CA'] = df_new['ab_page'] * df_new['CA']
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\ $     filter(Measurement.station == 'USC00519281').all() $
sim_val = new_page_converted/n_new - old_page_converted/n_old $ print('Simulated vaues are {}.'.format(sim_val)) $ print('Simulated vaues are {}.'.format(round(sim_val, 4)))
donald_breed = cats_df[cats_df['breed'] == 'Donald'] $ cats_df['remove'].iloc[donald_breed.index] = True $ del donald_breed
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
%time aod550 = collocate(emep.EXT_550nm, ec550) $ aod550
import numpy as np $ print('Original grocery list of fruits:\n', fruits)
control = df2.query('group=="control"') $ control_converted = control.query('converted==1') $ prob_control_converted = control_converted.shape[0]/control.shape[0] $ prob_control_converted
y_pred = log_clf.predict(X_test) $ accuracy_score(y_test, y_pred)
df_users.invited_by_user_id.isnull().values.ravel().sum()
my_data.filter(lambda line: ".txt" in line)
df2 = df.drop(['wait_estimate', 'ta_id', 'id', 'semester'], axis=1) $ df2 = df2.dropna(axis=0, how='any') $
! more RaceRocksDailySalTemp.txt
z_score, p_value
a = tweets_df.favorite_count.hist() $ a.set(yscale="log")
test_number_con = convers_con.groupby('added', as_index=False).count() $ test_number_con.head(2)
df2['ab_page']=0
typesub2017['Solar'] = typesub2017['Solar'].astype(int) $ typesub2017['Wind Offshore'] = typesub2017['Wind Offshore'].astype(int) $ typesub2017['Wind Onshore'] = typesub2017['Wind Onshore'].astype(int)
df2.all()
annotations_df['HOWMANY'] = annotations_df.HOWMANY.astype(float)
p_old=df2[df2['landing_page']=='old_page']['converted'].mean() $ p_old $ p_diff = p_new-p_old $ p_diff
areas = pd.read_csv(areas_csv_path)
combined_df['intercept'] = 1 $ logistic_model = sm.Logit(combined_df['converted'], combined_df[['intercept', 'CA', 'US']]) $ result = logistic_model.fit()
df["Age"].sum() #Specific column sum
for x in articles['body']: $     if len(x) < 1000: $         print x, '\n'
spark_df.registerTempTable("ufo_sightings")
df_pagecounts_desktop.head()
from sklearn import datasets
df_combined.shape
weather_mean.loc['HALIFAX', 'Rel Hum (%)']
data.head()
print "Number of rows with 'production_id'==701   : ", len(np.where(df1['production_id']==701)[0]) $ print "Total number of rows of table 'photoz_bcnz': ", len(df1) $ df1
df = df1.append(df2, ignore_index=True) $ df
obs = np.array([list(clean_users[clean_users['active']==1]['creation_source'].value_counts()),list(clean_users[clean_users['active']==0]['creation_source'].value_counts())]) $ obs
tmp = API.GetStatus(df_tweets.index[0])
df = data_archie.copy()
'Remember when we learned about split?'.split()
G8.index = ['CAN', 'FRA', 'DEU', 'ITA', 'JPN', 'RUS', 'GBR', 'USA'] $ G8
filteredPingsDF.take(10)
p_new = df2['converted'].mean() $ print("{} is the convert rate for  Pnew under the null.".format(p_new))
df.take(1)
df_r1.loc[~df_r1["Counter"].isin(["3"])].to_csv("data excluded/excluded 7_customers without less than 3 purchases.csv", $                                        encoding="utf-8", sep=",") $ print("Customers with less than 3 purchases: " + str(len(df_r1.loc[~df_r1["Counter"].isin(["3"])])))
Merge the three separate datasets into one dataset: $ twitter_archive_clean $ image_predictions_clean $ tweet_json_clean
dutchPhoneNumber = re.compile(r'(^\+[0-9]{2}|^\+[0-9]{2}\(0\)|^\(\+[0-9]{2}\)\(0\)|^00[0-9]{2}|^0)([0-9]{9}$|[0-9\-\s]{10}$)') $ for number,quantity in phoneNumbers: $     strippedNumber = number.replace(" ", "") $     if not dutchPhoneNumber.match(strippedNumber): $         print strippedNumber
q_pathdep_obs = c.submit_query(PTOQuerySpec().time("2018-06-10", "2018-06-12").set_id(0xc) $                                              .condition("ecn.multipoint.negotiation.path_dependent"))
print len(duplicate_cols)
pdata = {'Ohio': frame3['Ohio'][:-1], $         'Nevada': frame3['Nevada'][:2]}
data.info()
lfiles
for lyr in freeways.layers: $     print(lyr.properties.name)
user_logs['completed_songs_ratio'] = user_logs.num_completed_songs/ (user_logs.num_incompleted_songs + user_logs.num_completed_songs)
def MSE(df, df_true): $     output = [] $     for i in range(df.shape[1]): $         output.append(sum((df.ix[:,i]- df_true.ix[:,i])**2)/df.shape[1]) $     return output
sns.countplot(y="source_cleaned_2", data=tweet_table, palette="Greens_d")
daily_ret_b = calc_daily_ret(closes) $ daily_ret_b.loc[:,'BOND'] = 0.015/252 $ daily_ret_b
pd.read_csv("msft.csv", skiprows=100, nrows=5, header=0, $            names=['open', 'high', 'low', 'close', 'val', 'adjClose'])
twitter_archive_full[twitter_archive_full.name.isin(invalid_name_list)][['tweet_id','name','text']]
pres_df['time_from_creation_tmp'].tail(10)
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[n_new, n_old], alternative='larger') $ print('Value of z-score is:',z_score) $ print('p-value is the following:',p_value)
print("Average item-level delta recall: {:.2f}%".format(session_df['delta_recall'].mean())) $ print("Biggest item-level delta recall: {:.2f}%".format(session_df['delta_recall'].max())) $ print("Biggest adverse item-level delta recall: {:.2f}%".format(session_df['delta_recall'].min()))
_delta=_delta.to_dict()
til_today = pd.date_range(hits_df.index[0], hits_df.index[-1])
cursor.execute("SELECT column_name FROM information_schema.columns WHERE table_name='dot_311'") $
qa_data = pd.read_csv("qa_data.csv")
fig2, ax=subplots(1,1,figsize=(12,6)) $ outDf.plot(ax=ax, title='Growth Rate Examination')
06|136
df.info()
df = pd.read_sql_query('SELECT Agency, COUNT(*) as `num_complaints`' $                        'FROM data ' $                        'GROUP BY Agency ', disk_engine) $ df.head()
t.hour
df.cdescr.head()
max(close, key=close.get)
from nltk.corpus import stopwords # Import the stop word list $ print(stopwords.words("english"))
pl[['elapsed','cost']].plot()
data.groupby('Agency').size().sort_values(ascending=False).plot(kind='bar', $                                                                figsize=(20,4))
ab_data.converted.value_counts()
gs = GridSearchCV(lg,param_grid=lg_params,verbose=1,n_jobs = 3) $ gsmodel= gs.fit(X_train,y_train)
results2.summary()
inspector = inspect(engine) $ inspector.get_table_names()
corn.get_group("beef")
daily_returns.kurtosis()
damd['created'].head(3)
print ("dataset size reduction: ",int(100*((loaded_shape[0] - reduced_shape[0])/loaded_shape[0])),"%")
print('The probability of an individual converting regardless of the pay they receive is %.5f'%(df2.query('converted==1').shape[0]/df2.shape[0]))
sorted_errors_idx = options_frame['ModelError'].map(abs).sort_values(ascending=False).head(50) $ errors_20_largest_by_strike = options_frame.ix[sorted_errors_idx.index] $ errors_20_largest_by_strike[['Strike', 'ModelError']].sort_values(by='Strike').plot(kind='bar', x='Strike') $
from bmtk.analyzer import plot_potential, plot_calcium $ plot_potential(cell_vars_h5='output/cell_vars.h5') $ plot_calcium(cell_vars_h5='output/cell_vars.h5')
def sigmoid(z): $     s =  1.0 / (1.0 + np.exp(- z)) $     return s
timeMDict = averagebytime(typesDict, 'M')
df_tsv.info()
tfidfnmf_topics['TopicNum'] = tfidfnmf_topics.idxmax(axis=1) $ tfidfnmf_topics['TopicWt'] = tfidfnmf_topics.iloc[:, :15].max(axis=1) $ tfidfnmf_topics['PrimaryTopic'] = [topic_list[x] for x in tfidfnmf_topics['TopicNum']]
df_release = df_release.dropna(axis=1, how='all') $ df_release.shape
hr[hr["icustay_id"]==14882].plot(x="new charttime", $                                  y="value1num") $ hr[hr["icustay_id"]!=14882].plot(x="new charttime", $                                  y="value1num", color='red')
day_of_month14.to_excel(writer, index=True, sheet_name="2014")
df_goog.info()
len(fraud_data.user_id.unique()) == len(fraud_data)
adjectives.most_common(10)  # most frequent adjectives
summaries = "".join(df.title) $ ngrams_summaries = cvec.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(10)
df['Sentiment'] = ([sentiment_textblob(i) for i in df['description'] ])
testing_array = np.concatenate((testing_active_listing_dummy,test_pending_ratio),axis=1)
desc_stats.transpose().unstack(level=0)
from __future__ import print_function, division
van15_fin.describe()
lda_tf.show_topic(5)
len(df2.user_id.unique())
df2[df2.user_id.duplicated()==True]['user_id']
Xs = pd.get_dummies(df.subreddit, drop_first = True)
actual_diff = df2.query('group == "treatment"')['converted'].mean() - df2.query('group == "control"')['converted'].mean() $ actual_diff
tobs_df = pd.DataFrame(tobs_data, columns=['Date', 'tobs']) $ tobs_df.set_index('Date', inplace=True ) $ tobs_df
import statsmodels.api as sm $ convert_old = df2.query('group=="control" & converted==1')['converted'].count() $ convert_new = df2.query('group=="treatment" & converted==1')['converted'].count() $ n_old = 145274 $ n_new = 145310
diff_weekly_mean = day_counts.select('week','hashtag', diff_weekly_mean_comp.alias('sq_diff'))
df2.index[df2.index.duplicated()].unique()
df
train_Features, test_Features, train_species, test_species = train_test_split(Features, species, train_size=0.5, random_state=0)
!wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-GB,en-US;q=0.8,en;q=0.6" "https://storage.googleapis.com/kaggle-competitions-data/kaggle/6322/train_v2.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1532362274&Signature=ilplDkEyN7y1SAZo52C%2Br3aNH%2FXDkZTGBpamwuU3eyWInNqtouIXH3SJcCSsPs1M7zEyGZqlDKzrimvexYNM9qotATf1jwG6%2FXUk2agw%2F95B3gur7w6uC9zoiKghLdGkT3NSYQum7YeCYpa8FlvAcMlbPO64z9eARMj5LZfo8MTbi1TOr5l4RX9UCLFxjJamdXfJogylJZ1VblalFPjrv5Jv8Bp07kZ6u1FFEF8bJaIAVqRaw6%2F3AhNk74AcsiXa1DCiZJwN1%2BTnbZe8UPNx4dwgrxYPYxWVHVLIyezV2qe1X7VDhf237RBgAvF7LSaE3tH3knSAmzWIQPXGyp5tVQ%3D%3D" -O "train.zip" -c -d /data
s_empty = pd.Series() $ s_empty.empty
wednesdays = pd.date_range('2014-06-01','2014-08-31', freq='W-WED') $ wednesdays.values
query_result1.spatial_reference
start = 0 $ start = pd.to_datetime(start, unit='s') $ print(start)
logdir = log_dir("mnist_dnn")
clean_train_df.Q1_mood_of_speaker.value_counts()
significance_level = 0.05 $ confidence_level = 1 - significance_level $
train_dum_clean.head()
p_new = df2.query('converted == 1').user_id.count() / df2['user_id'].count() $ print(p_new)
airbnb_df.memory_usage(deep=True)
df2.shape[0]
twelve_months_prcp.head()
details.head()
p_new = ab_file2['converted'].mean() $ print(p_new)
df[((df['group'] == 'treatment') != (df['landing_page'] == 'new_page')) == True].shape[0]
samples_query.execute_sql("CALL Sample.Person_Extent") $ samples_query.display_records(5)
%%time $ params = { 'n_components' : 22, 'random_state' : 4444 } $ nmf = NMF(**params) $ nmf_factors = nmf.fit_transform(tfidf_matrix)
df2.shape
print("The probability of converted in control group:", len(df2.query('landing_page == "new_page"'))/len(df2))
reddit_body = data.drop(['created', 'is_video', 'thumbnail', 'url', 'timestamp', $                     'time_up', 'time_up_sec', 'time_up_clean'], axis=1)
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=10000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
print(arma_res.summary())
train = train.replace(['M', 'F'], [1, 0]) $ test = test.replace(['M', 'F'], [1, 0])
df2['user_id'].nunique(), df2.shape
fillna_with_zeros(joined, 'state_hol')
df['converted'].sum()/df['converted'].count()
import statsmodels.api as sm $ convert_old = df2.query('group == "control" and converted == 1').shape[0] $ convert_new = df2.query('group == "treatment" and converted == 1').shape[0] $ n_old = df2.query('group == "control"').shape[0] $ n_new = df2.query('group == "treatment"').shape[0]
colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k','lime','olive','tomato' ] $ drg_number,year,number_sites = 871,2015,10 $ the_cities = get_DRG_data(df_providers, drg_number,year,number_sites,1,True) $ print(the_cities) $ plot_DRG_year(the_cities,drg_number,year,number_sites) #,colors[0])
missing_info = list(df_users_first_transaction.columns[df_users_first_transaction.isnull().any()]) $ missing_info
day_of_week = pd.DatetimeIndex(pivoted.columns).dayofweek
mars_facts_url = "https://space-facts.com/mars/" $ browser.visit(mars_facts_url)
frames = [data['title_len'], dfcv] $ my_features = pd.concat(frames, axis=1, ignore_index=True )
df1.boxplot(column="tripduration",by='gender') $ xlabel("Gender. (0: Unkown, 1: Male; 2: Female)") $ ylabel("Trip duration (seconds)") $ title("Boxplot trip Duration by gender") $ suptitle("");
df = cust_df.drop('Address', axis=1) $ df.head()
logreg = LogisticRegression() $ logreg.fit(X_train,y_train)
out = pd.concat((out_train,out_test))
import urllib.request
print(mbti_text_collection)
len(open_day('2016-11-06')), len(reduce_day('2016-11-06'))
print 'mean: {} std: +/- {}'.format(np.mean(score), np.std(score))
new_page_converted = np.random.choice([0,1], size=n_new, p=[p_new, (1-p_new)])
df['margin_val'] =   df['cur_sell_price'] - df['cur_purchase_price']
df.head(2)
X.columns
df2.shape[0]
weather = twitter_news.find('p', class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text') $ weather
axes[0, 0].plot(np.random.randn(50).cumsum(), 'r--') $ axes[1, 1].scatter(np.arange(30), np.log10(np.arange(30))) $ fig
plt.plot(x, y) $ X=np.linspace(1,len(x),len(x)) $ X = X.reshape((len(x),1)) $ X.shape
eur_usd = df.loc['EUR-USD']['Change'] #This is chained indexing $ df.loc['EUR-USD']['Change'] = 1.0 #Here we are changing a value in a copy of the dataframe $ print(eur_usd) $ print(df.loc['EUR-USD']['Change']) #Neither eur_usd, nor the dataframe are changed
Sun_index  = pd.DatetimeIndex(pivoted.T[labels==0].index).strftime('%a')=='Sun' $ pd.DatetimeIndex(pivoted.T[labels==0].index)[Sun_index]
import requests $ r = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&output=csv') $ data = r.content
df = pd.read_sql_query('SELECT ComplaintType, Descriptor, Agency ' $                        'FROM data ' $                        'WHERE Agency IN ("NYPD", "DOB")' $                        'LIMIT 10', disk_engine) $ df.head()
news_df = pd.DataFrame(news_dict) 
for title, artist in unique_title_artist[current_len:min(current_len+batch_size, len_unique_title_artist)]: $     youtubeurl = urllib.parse.quote_plus(YOUTUBE_URL_TEMPLATE.format(gc.search(title +' '+artist)), safe='/:?=') $     youtube_urls[str((title, artist))] = youtubeurl
purchases['Zipcode'].apply(lambda x: len(x)).max()
update_date = groups.max()
zone_df = zone_df.merge(tz_tmp,on='zone_id')
stocks.write.format("parquet").save("stocks.parquet")
stories = pd.concat([stories, tag_df], axis=1)
df_final.columns
tfav.plot(figsize = (16,4), label = "Likes", legend = True) $ tret.plot(figsize = (16,4), label = "Retweets", legend = True);  $
x_new = df2[ (df2['group'] == 'treatment') & (df2['landing_page'] != 'new_page')].shape[0] $ y_new= df2[ (df2['group'] != 'treatment') & (df2['landing_page'] == 'new_page')].shape[0] $ x_new+y_new
twitter_ar.head(2)
%matplotlib inline $ import matplotlib.pyplot as plt $ import matplotlib.dates as mdates $ import matplotlib $ matplotlib.rcParams.update({'font.size': 14})
stocks_pca_m1
nba_pred_modelv1 = pickle.load(open(filename, 'rb'))
emb_szs
length = [] $ for x in sequences: $     length.extend(x) $ max(length)
var = odm2rest_request('variables')
train.show(3)
from oauth2client.client import GoogleCredentials $ creds = GoogleCredentials.get_application_default()
verify_response.json()['name']
df8_lunch.count()
predictions_clean.head()
df['UP'].sum()/df['UP'].count()
df_goog.Open
df_regression['intercept']=1 $ df_regression[['drop', 'ab_page']] = pd.get_dummies(df_regression['group']) $ df_regression.drop(['drop'], axis=1, inplace=True) $ df_regression.head()
%matplotlib notebook $ a.toPandas().plot();
sqlContext.registerFunction("simple_function", simple_function)
daily_cases[('liberia', '2014-09-02')]
v_invoice_link.columns[~v_invoice_link.columns.isin(invoice_link.columns)]
tweet_archive.name.value_counts().head()
score = cross_val_score(rf, Xs, y, cv = cv, verbose = 1)
our_nb_classifier = engine.get_classifier("nhtsa_classifier")
csvData['yr_built'].value_counts()
p_new = .1196
reg_traffic_with_flags['Time_stamp'] = reg_traffic_with_flags['ts'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(pd.to_numeric(x))))
lr = 1e-3
import requests $ import collections
git_project_data_df.schema
house_id = os.listdir('house_image')
from sklearn.cross_validation import train_test_split $ x = my_df.text $ y = my_df.target $ SEED = 2000 $ x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.10, random_state=SEED)
df = df.sort_values(by='retweets', ascending=False)
plt.title('McQueen ngram', fontsize=18) $ mcqueen_ng.plot(kind='barh', figsize=(20,16)); $ plt.savefig('../visuals/mcqueen_ngram.jpg')
get_descendant_frame(observations_node, data).info()
annual_precip_df = pd.DataFrame(annual_precip, columns=['date', 'prcp']) $ annual_precip_df.set_index(pd.DatetimeIndex(annual_precip_df['date']), inplace=True) $ annual_precip_df.describe()
closemeans = cc.groupby(['name'])['close'].mean()
column_name = data.columns[3] $ column_name.split(SEPARATOR)
FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH) $ md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)
hp2 = pd.Series(np.random.uniform(-0.05, 0.15, 36), index=pd.date_range('1/1/2011', periods=36, freq='MS')) $ hp2.tail()
print date_now.isoweekday() $ print date_now.weekday()
vocabulary_expression['component_3'].sort_values(ascending=False).head(7) $
df
M7_RMSE,M7_pred,M7_actual = ts_walk_forward(training_X_scaled,training_y.values,15,15,regr_M7)
sns.heatmap(temp_us)
LABELS = ["Underweight", "Normal", "Overweight", "Obese"] $ plt.bar(x, y, 0.5) # call the plot $ plt.xticks(x, LABELS) # format the X axis with labels $ plt.show()
print('{0:.2f}%'.format((scores[:1.625].sum() / total) * 100))
print(df_subset.info())
frame.loc['a', 'Ohio']
result['createdAtUTC'] = pd.to_datetime(result['createdAt'], unit='ms')
url1 = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&collapse=null&api_key=my_API_KEY'
df_predictions_clean.p3 = df_predictions_clean.p3.str.title()
neo_github_issues_url = blockchain_projects_github_issues_urls[2] $ neo_github_issues_df  = pd.read_json(get_http_json_response_contents(neo_github_issues_url))
import pandas as pd $ import json $ from pprint import pprint
titanic['is_adult'] = titanic.age >= 18 $ titanic.head()
df.isnull().sum() $
segments.st_time.dt.month.head()
lasso2 = Lasso(alpha=0.0002) $ lasso2.fit(train_data, train_labels)
conn.tableinfo(name='data.iris', caslib='casuser')
user_group = df.groupby('user') $ size = user_group.size() $ size.sort_values(inplace=True) $ size.plot() $
session.query(Measurement.station,func.count(Measurement.station))\ $       .group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()
df['price'] = df['price'].astype(float) $ df.info()
for user1, count in Counter(hashtags).most_common(10): $       print(user1 + "\t" + str(count))
cityID = '3df4f427b5a60fea' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Antonio.append(tweet) 
grouped_publications_by_author.tail(10)
data['Date'] = pd.to_datetime(data['Date']) #we like dates because you can query against dates $ data.set_index('Date', inplace=True) $ data['Year'] = data.index.year $ data['Month'] = data.index.month
response = Query(github_index).get_terms("user_org")\ $                                        .fetch_aggregation_results() $ buckets = response['aggregations']['0']['buckets'] $ organizations = pd.Series([item['key'] for item in buckets]) $ print(organizations.head()) $
clinton_df = make_dataframe(clinton_tweets)
to_seconds(between_all_posts).plot( $     kind='hist', $     range=(0, 100_000) $ )
plt.scatter(X2[:, 0], X2[:,1], c=labels, cmap='rainbow') $ plt.colorbar()
df = pd.read_pickle("dfSentences.p")
to_be_predicted_Day3 = 21.38790453 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
df_birth.columns
print(twitter_data_v2.shape) $ print(tweet_data_v2.shape) $ print(Imagenes_data_v2.shape) $
c2.sum()
print("Information of the duplicate entry.") $ df2[df2.duplicated('user_id', keep=False)]
from swat.cas import datamsghandlers as dmh
X_train_tokens
probs_test
score_merkmale=scoring[['id_loan_request',u'loan_request_nr',  u'fk_user','id', $                         'is_failed', u'ignore_for_scoring',u'score', 'created_at' ]].\ $ merge(merkmale.reset_index(), on='id')
seats_per_hour = clean_seats('../clean_data/passengers_per_hour_since_2014.csv')
d.year
data_dir = '/Users/GJWood/nilm_gjw_data/HDF5/' $ gjw = DataSet(join(data_dir, 'nilm_gjw_data.hdf5')) $ print('loaded ' + str(len(gjw.buildings)) + ' buildings') $ building_number=1
top10 = twitter_final[twitter_final.dog_species != ' '].groupby('dog_species')['tweet_id'].size().reset_index(name="Count").sort_values(by='Count', ascending=False).head(10) $ top10.plot.bar('dog_species')
data.info()
with open(os.path.join(outputs, 'clean_reviews.pkl'),'wb') as f: $     pickle.dump((clean_train_reviews, $                  clean_test_reviews, $                  clean_train_reviews_sw, $                  clean_test_reviews_sw),f)
data=data.dropna(subset=['description'])
a = test.toarray() #makes an array based on term frequency for the same test sample. $ a
%matplotlib inline
pd.DataFrame(random_integers, columns=['b', 'a'])
dat[dat.ward.isnull()]
eastern = ts_utc.tz_convert('US/Eastern')
df['CurrentRatio'] = df['CurrentRatio'].map(transform1)
bigdf.dtypes
startOut = df.index.searchsorted(dt.datetime(2017, 11, 24, 16,0,3,0)) $ endOut = df.index.searchsorted(dt.datetime(2017, 11, 24, 16,0,7,0))
train_holiday_oil_store_transaction_item_test_004 = train_holiday_oil_store_transaction_item_test_004.drop('onpromotion') $
import pandas as pd                                                          # pandas library
df.head()
dft.loc['2013-1-15 12:30:00']
weather_sorted = weather_all.sort_values('Temp (deg C)') $ weather_sorted.head()
%matplotlib inline $ from ggplot import *
changes = [] $ for lobbyist in newl: $     changes.append([lobbyist[0], lobbyist[3], 1]) $     changes.append([lobbyist[1], lobbyist[3], -1]) $
logit_mod2 = sm.Logit(df_new2['converted'], df_new2[['intercept', 'ab_page', 'CA', 'UK']]) $ results2 = logit_mod2.fit() $ results2.summary()
df_users_4.head()
client = MongoClient() $ db = client['fraud_db'] $ tab = db['predictions'] $
opening_prices = [x for x in opening_prices if x is not None] $ print('Highest opening price - {} \nLowest opening price - {}'.format(max(opening_prices), min(opening_prices)))
pats_chiefs_nov8_tweets = pbptweets.loc[(pbptweets['screen_name'] == 'patriots_pbp') | $                                         (pbptweets['screen_name'] == 'chiefs_pbp')]
p_diffs=np.array(p_diffs) $ (actual_difference < p_diffs).mean()
pres_df.rename(columns={'subject_count': 'subject_count_test'}, inplace=True) $ pres_df.head(2)
for (name, sex) in c_list: $     get_all_tweets(name, sex) $     df = df.append(pd.read_csv('%s_tweets.csv' % name))
trump_month_distri.plot(kind='bar', figsize=(10,5), rot= 45,title="# of Twitters of Donald Trump") $ plt.savefig('fig/trump_month.png');
final_word_df = grouped[grouped.Word_stem.isin(stemmed_dict_list)]
html = browser.html $ soup = bs(html, 'html.parser') $ browser.find_by_css("div.carousel_container div.carousel_items a.button").first.click() $ time.sleep(3)  #allow time for page to load $
cursor = db.TweetDetils.aggregate([ {"$group" : {"_id":"$user_name", "score":{"$sum":"$fav_cnt"}}},  {"$sort":{"score" : -1}},{"$limit":5}]) $ for rec in cursor: $     print(rec["_id"], rec["score"])
print("Select single value at index 2, x[2]=",x[2]) $ print("Select slice index 2:3, x[2:4]=",x[2:4]) $ print("One can reference the (first) index of a vlaue e.g. x.index(6) is",x.index(6))
df_2008.dropna(inplace=True) $ df_2008
(autos['last_seen'] $  .str[:10] $  .value_counts(normalize=True,dropna=False) $  .sort_index() $ )
testheadlines = test["text"] $ advancedtest = advancedvectorizer.transform(testheadlines) $ advpredictions = advancedmodel.predict(advancedtest)
data.tail()
noise_df= df[df['Complaint Type'].str.contains("Noise")]
tSVD2 = TruncatedSVD(n_components=300) $ blurb_SVD2 = tSVD2.fit_transform(blurbs_to_vect) $ print(np.sum(tSVD2.explained_variance_ratio_))
series
%%time $ df_from_hdf = pd.read_hdf('store.h5', key='losses')
american_train_model.print_topics(num_topics = 10 ,num_words = 10)
sfs1.k_feature_idx_
info_cut = parks_info[['parkid','acreage','supdist']] $ info_cut.parkid = info_cut.parkid.values.astype(int) $ info_cut.head(2)
len(nn_X_train[0])
s.index
bb.head()
f = open(data_path+"metadata.json","r") $ the_posts = json.load(f)
bacteria2
na_df.notnull() # check elements that are not missing
pandas_ufos_withyears = sqlContext.sql(query).toPandas() $ pandas_ufos_withyears.plot(kind='bar', x='year', y='count', figsize=(12, 5))
df2=df[['fatalities','origin','description']]
vect_tfidf = TfidfTransformer()
df_2011.dropna(inplace=True) $ df_2011
pd.concat([msftA01.head(3),msftA02.head(3)])
merge[merge.columns[17]].value_counts().sort $
fiss_rate = openmc.Tally(name='fiss. rate') $ abs_rate = openmc.Tally(name='abs. rate') $ fiss_rate.scores = ['nu-fission'] $ abs_rate.scores = ['absorption'] $ tallies_file += (fiss_rate, abs_rate)
df_prep5 = df_prep(df5) $ df_prep5_ = pd.DataFrame({'date':df_prep5.index, 'values':df_prep5.values}, index=pd.to_datetime(df_prep5.index))
shirt_1.price
desc_count = CountVectorizer(max_features=100, stop_words='english')
contractor_final.to_sql('contractor', con=engine,if_exists='append',index = False)
%%bash $ p2o.py --enrich --index git_raw --index-enrich git -e http://localhost:9200 --no_inc --debug git https://github.com/grimoirelab/perceval.git
result = cur.fetchall() $
logit_mod = sm.Logit(df3['converted'], df3[['intercept','ab_page']]) $ results=logit_mod.fit()
MultData.groupby('to_account').mean()
def logtorec(x): $     ypred = list(zip(x, list(range(100)))) $     ypred5 = ' '.join([str(t[1]) for t in sorted(ypred, reverse=True)][:5]) $     return ypred5
dflunch = df[(df['TIME'] == '15:00:00') | (df['TIME'] == '16:00:00')] $ dfstationmeanlunch = station_mean(dflunch)
print(results.summary())
!whoami $ !id root
df.isnull().any().any(), df.shape
print('atoms.atype ->', atoms.atype) $ print('atoms.pos[2] ->', atoms.pos[2])
p_converted_user = df.query('converted==1').user_id.nunique()/i_unique_user $ p_converted_user
(etsamples_engbert,etmsgs_engbert,etevents_engbert) = be_load.load_data(algorithm='') $ raw_large_grid_df_engbert = condition_df.get_condition_df(data=(etsamples_engbert,etmsgs_engbert,etevents_engbert),condition='LARGE_GRID')
df=pd.read_csv('twitter_archive_master.csv')
result = json.loads(get_req.content)['result']
git_log.timestamp = pd.to_datetime(git_log.timestamp, unit='s') $ print(git_log.timestamp.describe())
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head(5)
testing.info()
users = df2.nunique()['user_id'] $ sessions = df2.count()['user_id'] $ print('Now, in df2 we have {} unique users and a total of {} sessions.'.format(users,sessions))
week41 = week40.rename(columns={287:'287'}) $ stocks = stocks.rename(columns={'Week 40':'Week 41','280':'287'}) $ week41 = pd.merge(stocks,week41,on=['287','Tickers']) $ week41.drop_duplicates(subset='Link',inplace=True)
df_new = df2 
predictions = model.predict(test_words)
country_dummies=pd.get_dummies(df_new['country']) $ df_new=df_new.join(country_dummies) $ df_new.head()
plot_data(389) $ digits.target[389]
active_countries[active_countries['All'] > 20].sort_values('% of Total')['% of Total'].tail(15).plot('barh', color = 'g') $ plt.title('Top 15 Active Countries') $ plt.xlabel('% of Active Users in Country') $ plt.ylabel('')
animals = ['lion', 'tiger', 'crocodile', 'vulture', 'hippo'] $ for creature in animals: $     print(creature + ',', end="")
df_test_index = df_test_index.drop(['event_start_at','time_stamp', 'num_of_people', 'payment_method', 'total_price'], axis=1) $ df_test_index = df_test_index.iloc[:,[1, 0, 2]] $ df_test_index['user_id'] = df_test_user['user_id'] $ df_test_index = df_test_index.fillna(0) $ df_test_index                                   
pickle_it(BNB, 'bnb_classifier') $ BNB = open_jar('bnb_classifier')
result = api.search(q='%23Australia')  # "%23" == "#" $ len(result)
options_frame['DaysUntilExpiration'] = options_frame.apply(_get_days_until_expiration, axis=1) $ options_frame['TimeUntilExpiration'] = options_frame.apply(_get_time_fraction_until_expiration, axis=1) $ options_frame['InterestRate'] = options_frame.apply(_get_rate, axis=1) $ options_frame['Mid'] = options_frame.apply(_get_mid, axis=1)
df2 = winpct.groupby('Game Title Date')['date','Team1'].count()
versions = {} $ with open('obj/version_dict.pkl', 'rb') as handle: $     versions = pickle.load(handle) $
df2.query('landing_page == "new_page"').landing_page.count()/df2.shape[0]
plt.hist(new_page_converted);
results
from bson.objectid import ObjectId $ fs.find_one({'_id': ObjectId('56fe67a3b143762514358154')})._id
merged1.shape
import matplotlib.pyplot as plt $ import seaborn as sns $ from sklearn import metrics
clinton_df['source'].value_counts().plot.barh()
dfleft=pd.DataFrame({'key1':['K0','K1','K2','K3'],'key2':['K0','K1','K2','K3'],'A':['A0','A1','A2','A3'],'B':['B0','B1','B2','B3']}) $ dfright=pd.DataFrame({'key1':['K0','K1','K4','K3'],'key2':['K0','K1','K2','K3'],'C':['C0','C1','C2','C3'],'D':['D0','D1','D2','D3']}) $ print(dfleft) $ print(dfright)
todrop1 = df.loc[(df['group'] == 'treatment') & (df['landing_page'] == 'old_page')].index
autos.head()
dict1 = {1: "world", 2: "politics", 3: "finance", 4: "technology", 5: "entertainment", 6: "sports", 7: "nature", 8 :"crime"}
cols = df_os.columns.tolist() + df_usnpl_one_hot.columns.tolist() $ media_classes = [c for c in cols if c not in ['domain', 'notes']] $ breakdown = df_questionable_2[media_classes].sum(axis=0) $ breakdown.sort_values(ascending=False)
def pct(a, b): $     return 100.0 * a / b $ subset = raw_subset.filter(lambda p: p["meta/creationTimestamp"] is not None and p["meta/Timestamp"] is not None) $ print("'new-profile' pings with missing timestamps:\t{:.2f}%".format(pct(ping_count - subset.count(), ping_count)))
data.describe()
print('Number of unique ids in df2 is {}'.format(df2.user_id.nunique()))
print(pd.date_range('2018-01-01',periods=30,freq='D'))
logit_mod4=sm.Logit(df3['converted'],df3[['ab_page','intercept', 'CA','UK', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday' ]]) $ fit4=logit_mod4.fit() $ fit4.summary()
daily_returns=compute_daily_returns(df) $ plot_data(daily_returns,title="Daily Returns")
%%time $ model.fit(train_x, train_y, epochs=100)
TBL_FCBridge_query = "SELECT bridge_id, brkey, struct_num, yearbuilt, latitude, longitude FROM TBL_FCBridge" $ data_FCBridge = pd.read_sql(TBL_FCBridge_query, cnxn) $ data_FCBridge
X = reddit['Titles'] $ y = reddit['Above_Below_Median'] $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
ts.resample('D', how='mean')
active_obs = active_station[0][1] $ active_obs 
df.data.head()
if True == 0: $     rr_tmp = rr.replace(0, np.nan).apply([np.mean, np.std]) $     rr_tmp.T.plot.barh() $     plt.xlabel('Monthly return (not annualized)') $     plt.show() $
pd.set_option("display.max_rows", 20) $ np.random.seed(12)
df['Month'].head()
plt.scatter(X2[:, 0], X2[:, 1]);
weather_warm['Station Name'].nunique()
archive_copy['name'].loc[archive_copy['name'] == 'None'] 
ved['season'] = ved.index.str.split('.').str[0] $ ved['term'] = ved.index.str.split('.').str[1]
df3=pd.DataFrame({'key':['K0','K1','K4','K3'],'C':['C0','C1','C2','C3'],'D':['D0','D1','D2','D3']})
df2.shape
df = pd.merge(train, $ membership_loyalty[(membership_loyalty.transaction_date < datetime.strptime('2017-03-01', '%Y-%m-%d'))], $ on=['msno'], $ how='left')
actor = pd.read_sql_query('select concat(ucase(first_name)," ", ucase(last_name)) as "Actor Name" from actor', engine) $ actor.head()
val[0]
api_clean['tweet_id'] = api_clean['tweet_id'].astype(str)
roc_auc_score(y_train, y_hat_lr) 
import logging
rng = pd.date_range(start = '1/1/2017', periods = 72, freq = 'B') $ rng
np.count_nonzero(np.any(nba_df.isnull(), axis = 0))
df_new['us_page'] = df_new['us'] * df_new['ab_page'] $ df_new['uk_page'] = df_new['uk'] * df_new['ab_page'] $ df_new.head()
cig_data_SeriesCO[cig_data_SeriesCO > cig_data_SeriesCO.median()]
df2 = df2.drop_duplicates('user_id', keep ='first') $ assert df2.user_id.duplicated().sum() == 0 #To make sure the duplicated rows are removed
sum(df2.converted==1) / df2.shape[0]
df_condo['LOT'].value_counts()
print("conversion old: " + str(convert_old)) $ print("conversion new: "+ str(convert_new)) $ print("n_old: "+str(n_old)) $ print("n_new: "+ str(n_new))
robust_cov_matrix= pd.DataFrame(skcov.ShrunkCovariance().fit(daily_ret).covariance_,columns=daily_ret.columns,index=daily_ret.columns) $ robust_cov_matrix
archive_clean.rating_numerator = archive_clean.rating_numerator.astype(float) $ archive_clean.rating_denominator = archive_clean.rating_denominator.astype(float)
confusion_matrix(y_test,knn_pred)
from collections import Counter $ Counter(df_2.titles)
error_count = pd.get_dummies(errors.set_index('datetime')).reset_index() $ error_count.columns = ['datetime', 'machineID', 'error1', 'error2', 'error3', 'error4', 'error5'] $ error_count = error_count.groupby(['machineID', 'datetime']).sum().reset_index() $ error_count.head(13)
data_compare['SA_mix'].mean(), data_compare['SA_google_translate'].mean(), data_compare['SA_textblob_de'].mean()
tweet_archive_clean['stage'] = tweet_archive_clean[['doggo', 'floofer','pupper', 'puppo']].apply(lambda x:''.join(x), axis= 1)
store1_data = data[data.Store == 1]
df.values  # Display the values only
test_url = requests.get(url) $ if test_url.status_code != 200: $     print(f'Hey there, something went wrong with our request.\nIt returned as status code of {test_url.status_code}.\nAre we sure about {site} as the site name?') $ else: $     print("The request returned some data")
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\mydata.json" $ df = pd.read_json(path) $ df.head(5)
clean_pandas_df = clean_up(pandas_df) $ clean_pandas_df['tags'] = [" ".join(x) for x in clean_pandas_df['tags']] $ clean_pandas_df['tags'] = clean_pandas_df['tags'].astype('category') $ clean_pandas_df['tag_id'] = clean_pandas_df['tags'].cat.codes
total_users = df2['user_id'].nunique() $ converted_users = df2[df2['converted'] == 1].count() $ conversion_prob = converted_users/total_users $ print(conversion_prob) $
p_old = df2['converted'].mean() $ printmd("**P*old***: {:0.4}".format(p_old))
crimes[(crimes['PRIMARY_DESCRIPTION']=='CRIMINAL DAMAGE')].head() $
c.execute('SELECT city, min(average_high) FROM weather') $ print(c.fetchall())
print clean_text1[1:8]
(autos['price'] $  .value_counts() $  .head() $  .sort_index(ascending=False))
s1['c']
pop.unstack(level=0)
for key,value in df.iteritems(): $     print(key,value)
ids = {Id: station for station, Id in stations.items()}
print(cc['slug'].describe()) $ print(cc['symbol'].describe())
tsla_30 = mapped.filter(lambda row: row[3] > T0) $ tsla_30_DF = tsla_30.toDF(["cid","ssd","num_ssd","tsla","tuna"]) $ tsla_30_pd = tsla_30_DF.toPandas()
users_visits = pd.merge(users_visits, Relations, how='outer', on=['name', 'id_partner']) $ users_visits = users_visits[['visits', 'regs', 'chanel']]
from sklearn import linear_model $ model = linear_model.LinearRegression() $ print ('Linear Regression') $ reg_analysis(model,X_train, X_test, y_train, y_test)
train.head()
df2[df2['user_id'].duplicated(keep = False)]
weather = pd.read_csv("weather.csv", parse_dates={"date" : ['Date']}) $ weather = weather.drop(['HDD', 'CDD'], axis=1)
site.devices
test_ag = aggregate_features(test_pp) $ train_1m_ag = aggregate_features(train_1m) $ train_10m_ag = aggregate_features(train_10m) $ train_50m_ag = aggregate_features(train_50m)
model_tree = DecisionTreeClassifier(random_state=42) $ model_logit = LogisticRegression(random_state=42) $ model_rf = RandomForestClassifier(random_state=42) $ model_gb = GradientBoostingClassifier(random_state=42) $ model_nn=MLPClassifier(random_state=42)
df =pd.read_csv('https://raw.githubusercontent.com/jackiekazil/data-wrangling/master/data/chp3/data-text.csv') $ df.head(2)
df_users_1.shape
ab_data[((ab_data.group == 'treatment') & (ab_data.landing_page != 'new_page')) | $         ((ab_data.group != 'treatment') & (ab_data.landing_page == 'new_page')) $        ].shape[0]
df = df[df['Views-PercentChange']>0]
df_h1b_nyc = df_h1b_nyc.drop([33359, 66488, 104536, 166920, 279967, 342136,426706])
prop_grthan_p_diffs = len(p_diffs[np.where(p_diffs > calc_diff)]) / len(p_diffs) $ prop_grthan_p_diffs
offset.rollback(d)
df2.drop_duplicates(inplace=True)
import pandas as pd $ import numpy as np $ A = pd.Series([2, 4, 6], index=[0, 1, 2]) $ B = pd.Series([1, 3, 5], index=[1, 2, 3]) $ A + B
filtered.groupby('group_id').tweet_id.count()
probly = pd.read_csv('probly.csv') $ ttest(probly["Likely"], probly["Unlikely"])
dict1 = {"a":1,"b":2} $ dict2 = {"c":3,"b":4,"a":5} $ list_of_dict = [dict1,dict2] $ df2 = pd.DataFrame(list_of_dict) $ print(df2)
assert rows.pop(0) == ['C/A', 'UNIT', 'SCP', 'STATION', 'LINENAME', $                        'DIVISION', 'DATE', 'TIME', 'DESC', 'ENTRIES', $                        'EXITS'] 
sns.violinplot(x='precipType', $                y='polarity', $                data=twitter_moves[twitter_moves['precipType'] != 'snow'])
news_df = (news_df.groupby(['topic']) $               .filter(lambda x: len(x) >= 25))
iris.head().iloc[:,[0]]
autos["price"]=autos["price"].str.replace("$","").str.replace(",","").astype(float)
autos["ad_created"].str[:10].value_counts(normalize = True, dropna = False).sort_values()
%%gcs list --objects gs://inpt-forecasting
import statsmodels.api as sm $ convert_old = df2.query('group == "control" and converted == "1"').count() $ convert_new = df2.query('group == "treatment" and converted == "1"').count() $ convert_old[0] , convert_new[0], n_old , n_new
df2['user_id'].duplicated().sum()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old],[n_new, n_old], alternative='larger') $ z_score, p_value
bixi_hourly=bixi_hourly.set_index('start_date')
buildings = gpd.read_file('Moscow_buildings.shp') $ landuse = gpd.read_file('Moscow_landuse.shp') $ buildings_tree = scipy.spatial.cKDTree(buildings[['long', 'lat']].as_matrix()) $ landuse_tree = scipy.spatial.cKDTree(landuse[['long', 'lat']].as_matrix()) $
results.home_score.rank(ascending=False)
data_lda = {i: OrderedDict(ldamodel.show_topic(i,15)) for i in range(total_topics)}
bad_df = df.index.isin([5,12,23,56]) $ df[~bad_df].head()
df_copy.sample(2)
nocachedf
images_clean.set_index(images_clean.columns[0], inplace = True) $ images_clean.head()
print df.set_index(['date', 'item']).unstack() $
dfFull['TotalBsmtSFNorm'] = dfFull.TotalBsmtSF/dfFull.TotalBsmtSF.max()
plt.hist(newshows['first_year']) $ plt.title('Distribution of Release Years') $ plt.ylabel('Frequency') $ plt.xlabel('Year Released')
df.reset_index(inplace=True)
dfClientes.shape
engine = create_engine('mysql+mysqlconnector://root:{}@localhost:3306/github'.format(password), echo=False)
cercanasAfuerteApacheEntre125Y150mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 125) & (cercanasAfuerteApache['surface_total_in_m2'] < 150)] $ cercanasAfuerteApacheEntre125Y150mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre125Y150mts.apply(descripcionDistancia2, axis = 1) $ cercanasAfuerteApacheEntre125Y150mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
last_date=df1.iloc[-1].name $
pd.date_range(start,end, freq='M')
result.shape
train_data = pd.read_csv('Airbnb/train_users_2.csv') $ test_data = pd.read_csv('Airbnb/test_users.csv')
data_float_pca = data.loc[:,['surface_total_in_m2', 'surface_covered_in_m2',\ $                           'lat', 'lon']].dropna(axis= 0, how='any')
count_15 = logins.set_index(keys='datetime').resample('15min').count()
twitter_archive_master[twitter_archive_master.name == 'None'].head()
DF_filtered = ds_complete_temp_CTD_1988.to_dataframe()
station_count = session.query(Station.id).count() $ print(" There are {} stations within the data set".format(station_count))
df_providers_pared.columns
cpi_sdmx.lookup_code('housing',position=2)
raw = fat.get_price_data(ticker) $ ohlcv = raw
horror_readings=horror_readings[['visitor_id','id','visit_id','tracking_time']]
df.sort_index(inplace=True) $ df.head()
df_corr = result.groupby(['type', 'scope'])['user'].sum().reset_index() $ display(df_corr.sort_values('user',ascending=False).head(10)) $ plot2D(df_corr, 'type', 'scope','user')
for column in ls_other_columns: $     df_uro[column].unique()
suspects_with_25_1['in_cp'] = suspects_with_25_1.apply(in_checkpoint, axis=1)
new_page = df2.query('landing_page == "new_page"') $ total_pages = df2['landing_page'] $ new_page.count()[0]/total_pages.count()
prcpDF.describe()
train_data['fuelType_int'] = train_data['fuelType'].apply(get_integer4) $ test_data['fuelType_int'] = test_data['fuelType'].apply(get_integer4) $ del train_data['fuelType'] $ del test_data['fuelType']
idf = tfidf.idf_ $ idf_map = dict(zip(tfidf.get_feature_names(), idf))
prec_nc = Dataset("../data/nc/pr_wtr.mon.mean.nc")
x.drop(["sum"], axis=1, inplace=True) $ x
len(trn_texts), len(val_texts)
twitter_archive_master = twitter_archive_master.drop(twitter_archive_master[twitter_archive_master['rating_denominator'] != 10].index)
tokenizer('running like running and thus they run')
with pd.option_context('display.max_colwidth', 130): $     print(news_titles_sr)
fig, axes = plt.subplots(2,figsize=(20,10),sharex=True) $ axes[1].plot(tbl2.index,tbl2['250alpha']) $ axes[1].set_title('Rolling Alpha') $ axes[0].plot(tbl2.index,tbl2['250beta']) $ axes[0].set_title('Rolling Beta')
wk_prob['is_rmv_site'] = draft_df['url'].apply(mh_to_cat, args = (rmvs,)) $ wk_prob = wk_prob[wk_prob.is_rmv_site == False]
rf = RandomForestClassifier(n_jobs = -1) $ k_fold = KFold(n_splits=5) $ cross_val_score(rf, X_features, tweets_1['sentiment'],cv = k_fold, scoring = 'accuracy', n_jobs=-1)
target_user= "@MarsWxReport"
price_mat = df.filter(regex='^open.').fillna(0) $ mcap_mat = df.filter(regex='^market').fillna(0) $ volumes = df.filter(regex='^volume').fillna(0)
twitter_archive_clean['stage']=twitter_archive_clean['text'].str.extract('(puppo|pupper|floofer|doggo)', expand=True) $ twitter_archive_clean.drop(columns=['doggo','floofer','pupper','puppo'],inplace=True)
twitter[twitter.name==twitter.name.str.lower()].name.value_counts()
print(autos["odometer_km"].unique().shape) $ autos["odometer_km"].describe()
pandas_small_frame = small_frame.as_data_frame() $ print(type(pandas_small_frame)) $ pandas_small_frame
h = (df2.user_id.tolist())
uk, us, abs_pg = 1/np.exp(0.0506), 1/np.exp(0.0408), 1/np.exp(-0.0150) $ uk,us, abs_pg
reviews_w_sentiment=reviews_w_sentiment[reviews_w_sentiment.comments != ' '] $ reviews_w_sentiment[reviews_w_sentiment['date']>='2018-01-01'].head() $ len(reviews_w_sentiment)
 df2.user_id.nunique()
doctors = ['Psychiatry', 'Child & Adolescent Psychiatry', ] $ RN_PAs = ['Medical', 'Psych/Mental Health, Child & Adolescent', 'Psych/Mental Health', 'Physician Assistant'] $ therapists = ['Marriage & Family Therapist', 'Psychologist', 'Specialist/Technologist, Other', 'Clinical' ]
a.max(1)
%matplotlib inline
number_string = 'aaaXXaaaXXaaa' $ number_string.count('XX')
todays_datetimes = [datetime.datetime.today() for i in range(100)]
image_array = to_array_variabel(image, (153,1)) $ format_array = to_array_variabel(review_format, (153,1)) $ fake_array = to_array_variabel(fake, (153,1)) $ X_tfidf_test = np.hstack((image_array, format_array, body.toarray(), title.toarray(), fake_array))
def users_within_a_block(df): $     return [user for user, dist in df.items() if dist < 0.2 ] $ smaller_closer_user = users_within_a_block(user_distances) $ print(smaller_closer_user)
X_train_matrix = tvec.fit_transform(X_train) $ X_test_matrix = tvec.transform(X_test) $ forest.fit(X_train_matrix, y_train) $ forest.score(X_test_matrix, y_test)
p_value_obs_gt = (p_diffs > obs_p_diff).mean() $ _, p_value_ztest_gt = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ p_value_obs_gt, p_value_ztest_gt
posts.groupby(['from', 'hostname_clean'])['post'].aggregate(sum)
station_count["Count"].hist(bins=12, color="darkblue") $ plt.title("Histogram: Observation count by station") $ plt.savefig("Histogram Observation count by station") $
df_2002['bank_name'] = df_2002.bank_name.str.split(",").str[0] $
import pandas as pd
df.describe()
from nltk.stem.porter import PorterStemmer $ porter = PorterStemmer() $ def tokenizer_porter(text): $     return [porter.stem(word) for word in text.split()] $ tokenizer_porter('runners like running and thus they run')
f.visititems?
groupby_x = df['y'].groupby(df['x']) $ round(groupby_x.describe(), 3)
df_c2['country'].value_counts()
%time df2=get_web_indebtedness(lrs)
%%time $ pd.to_datetime(df['Created Date'][:10000], format='%m/%d/%Y %H:%M:%S %p').head()
df.describe()
obs_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ obs_diff
plt.hist(p_diffs); $ plt.ylabel('# of Simulations') $ plt.xlabel('p_diffs') $ plt.title('Plot of 10,000 Simulated p_diffs');
traffic_df_rsmpld = traffic_df_rsmpld.sort_index().loc[idx['2016-01-01':'2017-12-31',:],:].sort_index() $ traffic_df_rsmpld.info() $ traffic_df_rsmpld.head()
df3=pd.read_pickle('./city_service_requests_neighborhoods')
cohort_activated_df.to_csv('activation_cohorts_518.csv')
df_new = df2[(df2['landing_page'] == 'new_page')] $ converted = df_new['converted'] $ new_page_converted = np.random.choice(converted, n_new)
gscv.fit(x_train, y_train)
screendf.to_sql(con=engine, name='wellscreens', if_exists='replace', flavor='mysql',index=False)
combined_df
df.groupby("cancelled")["created_as_guest"].mean()
reader.monitor.columns
SANDAG_jobs_df.loc[(2012, '1')]
d
from pyspark import SparkContext $ from pyspark.streaming import StreamingContext $ import json $ import time $ from datetime import datetime
autos["date_crawled_10"].value_counts(normalize = True, dropna = False).sort_index(ascending = False)
fb.head(20)
reg_traffic_with_flags['Hour'] = pd.to_datetime(reg_traffic_with_flags.Time_stamp).dt.hour $ reg_traffic_with_flags['Day'] = pd.to_datetime(reg_traffic_with_flags.Time_stamp).dt.day $ reg_traffic_with_flags['Month'] = pd.to_datetime(reg_traffic_with_flags.Time_stamp).dt.month
r = q_multi.results() $ r
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = plot_acf(dta_713.values.squeeze(), lags=40, ax=ax1)
to_be_predicted_Day4 = 31.29300329 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
table3.head(3)
duplicated_sites = all_sites_with_unique_id_nums_and_names['id_num'].duplicated() $ indices_duplicated_sites = [i for i, x in enumerate(duplicated_sites) if x] $ print('There are',len(indices_duplicated_sites),'sites that have duplicates') $ print('For example:')
df2= df[(df['landing_page'] == 'new_page') & (df['group'] == 'treatment') | (df['landing_page'] == 'old_page') & (df['group'] == 'control')]
postsM = postsW $ postsM.reset_index().head(2)
import locale $ print('Current locale: {}'.format(locale.getdefaultlocale())) $ locale.setlocale(locale.LC_TIME, '') $ time_string = time.strftime('%A %d %b %Y', time.localtime()) $ print(time_string.encode('latin1').decode('cp1251')) $
petropavlovsk_freq = petropavlovsk_filtered.genus.value_counts() / len(petropavlovsk_filtered) $ petropavlovsk_freq
cityID = 'c47c0bc571bf5427' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Honolulu.append(tweet) 
pl = pf $ pl = pl.set_index('day') $ pl = pl.groupby(pd.Grouper(freq='D')).sum()
df_pol_t = df_pol[['word_count','sentiment','domain_d','title','post_duration','text']] $
timeslot = 120 $ y2_train = y_train.apply(lambda x : x // timeslot) $ print(y_train.head()) $ print(y2_train.head())
support.amount.sum()
sl[sl.status_binary==0][(sl.today_preds<sl.one_year_preds)].shape
metrics = pd.read_json('../metadata/metrics.json') $ metrics
df_MC_mostVisits = df_MC_mostVisits.set_index(0) $ MC_mostVisits = df_MC_mostVisits.ix[df_MC_mostVisits['number_months'].idxmax()] $ print 'In the given dataset, the marketing channel with most visits is ', MC_mostVisits.name, 'which had the most visits for ', MC_mostVisits[0], ' months'
def lookup_project_git(org, project): $
sgd.fit(trainx,trainy)
feats_dict = dict() $ for i, name in enumerate(dummies_df.keys()): $     feats_dict[i] = name $
t.label, ' '.join(t.text[:16])
StateAverages_raw.head()
df.isnull().sum()
image_predictions = pd.read_csv('data/image-predictions.tsv', sep = '\t') $ image_predictions_copy = image_predictions.copy() $ image_predictions_copy.head()
rhum_long_df = pd.melt(rhum_wide_df, id_vars = ['grid_id', 'glon', 'glat'], $                       var_name = "date", value_name = "rhum_perc") $ rhum_long_df.head()
tf.value_counts()
sf_business = graphlab.SFrame(df_business.reset_index())
df['com_label'] = np.where(df['comms_num']>=np.median(df.comms_num), 'High', 'Low') $ df['score_label'] = np.where(df['score']>=np.median(df.score), 'High', 'Low')
auth = HydroShareAuthBasic(username='****', password='****') $ hs = HydroShare(auth=auth) $ resource_id = hs.createResource(rtype, title, resource_file=fpath, keywords=keywords, abstract=abstract, metadata=metadata, extra_metadata=extra_metadata)
df_image_tweet.head()
sns.set_style("darkgrid") $ plt.figure(figsize=(8, 4)) $ errors['errorID'].value_counts().plot(kind='bar') $ plt.ylabel('Count')
newdf['score'].fillna(0.187218571, inplace=True)
data_compare['SA_textblob_de'] = np.array([ analize_sentiment_german(tweet) for tweet in data_compare['tweets_original'] ]) $ data_compare['SA_google_translate'] = np.array([ analize_sentiment(tweet) for tweet in data_compare['tweets_translated'] ])
df_user.rename(columns={"id_str":"id_user_str"}, inplace=True)
!wget -O loan_test.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_test.csv
X_test = pd.DataFrame(im.transform(X_test),columns=X_train.columns)
df2[df2['landing_page'] == 'new_page'].shape[0] / df2.shape[0]
df2 = df_imputed_mean_NOTCLEAN1A.copy()
sp.describe()
df_2004 = pd.DataFrame(rows)
results2 = model_selection.cross_val_score(gnb, X_test, Y_test, cv=loocv) $ results2.mean()
pd.Series(bnb.first_affiliate_tracked).fillna('untracked', inplace=True)
from dotce.report import (most_informative_features, $                           ascii_confusion_matrix)
import nltk $ from nltk.corpus import stopwords  #gets German stopwords from the nltk corpus
guinea_data4 = guinea_data3.set_index(['Description', 'Date']).sort_index() $ guinea_data4.columns = ['Guinea'] $ guinea_data4
df_daily = df_daily.reset_index() $ df_daily.head()
!python -m spacy download en
df2.info()
!wget https://images.pexels.com/photos/635529/pexels-photo-635529.jpeg
df.query("(group == 'control' and landing_page == 'new_page') or (group == 'treatment' and landing_page == 'old_page')").count()
df.nunique()['user_id'] $
pulledTweets_df[['text', 'sentiment_predicted_nb', 'sentiment_predicted_lr', 'emojis']][:20]
S_1dRichards.basin_par.filename
type_list = list(pokemon['Type 1'].unique()) $ pokemon_test = pd.DataFrame(columns = pokemon.columns) $ for i in type_list: $     pokemon_test = pokemon_test.append((pokemon[pokemon['Type 1'] == i]).sample(frac=0.05))
user.iloc[1, 1:5]
td + pd.tseries.offsets.Minute(15)
dd_df = dd_df.drop(['manuell'], axis=1)
twitter.rating_denominator.value_counts()
fed_reg_dataframe = pd.DataFrame.from_records(tuple_lst, columns=['date','str_text'], index = 'date')
for instance in session.query(User).order_by(User.id): $ ...     print(instance.name, instance.fullname)
df.dtypes
SraMTMmvt = delta(df_new, df_old, ['SraMTM']).rename('SraMTMmvt') $ PremiumMvt = delta(df_new, df_old, ['OtherPremium']).rename('PremiumMvt') $ ManAdjMvt = delta(df_new, df_old, ['FixedManualAdj']).rename('ManAdjMvt')
stock['volatility'] = stock.high - stock.low
twitter_archive.rating_numerator.value_counts()
df_c_merge.groupby('country')['converted'].mean()
scoring_ind.info()
crime_file = '../My_Project/data/crimes_jan_2015.csv' $ weather_file = '../My_Project/data/weather_jan_2015.csv'
1/np.exp(-0.0149), 1/np.exp(-0.0408), np.exp(0.0099)
notus
mgxs_lib.domain_type = 'cell' $ mgxs_lib.domains = geometry.get_all_material_cells().values()
doesnt_meet_credit_policy.head(rows=2)
os.remove('myfile.csv')  #Clean up.
terror.loc[138]
cohort_retention_df
wines = np.unique(dataset_filtered['variety'].tolist())
desktop = nvidia.filter(lambda p: not devices[int(p['adapter']['deviceID'], 16)].endswith("M")) $ desktop.map(lambda p: len(p['adapters'])).countByValue()
top_10_authors = git_log['author'].value_counts().head(10) $ top_10_authors
weekday_stats_df
autos_p.describe(include = 'all')
df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'], $                    'salary': [70000, 80000, 120000, 90000]}) $ display('df1', 'df3', 'pd.merge(df1, df3, left_on="employee", right_on="name")')
sum(df2['converted'])/df2.shape[0]
dtm = vectorizer.fit_transform(df['lemma'])
df = tables[0] $ df.columns = ['State', 'Abr.', 'State-hood Rank', 'Capital', $               'Capital Since', 'Area (sq-mi)', 'Municipal Population', $               'Metropolitan Population', 'Population Rank In State', 'Population Rank in US', 'Notes'] $ df.head()
df.Date = pd.to_datetime(df.Date)
tweet_json_clean['followers_count'].sample(3)
serc_pixel_df.head(5)
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 2) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
import statsmodels.api as sm $ model = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ res = model.fit() $
yxe_tweets = pop_tweets('stoonTweets.json')
ltcc = ltcc[(ltcc.w_sentiment_score != 0.000)] $ ltcc = ltcc.reset_index(drop=True) $ print(ltcc)
data['2015']
print(trump.favorite_count.describe()) $ print("   ") $ print(trump.retweet_count.describe())
health_data.iloc[:2, :2]
cur.fetchmany(5)
df.is_shift.value_counts()
import matplotlib.pyplot as plt $ %matplotlib inline $ plt.rcParams['figure.figsize'] = 16,8
department_df.groupby("Department", as_index = False).sum() # if we want the group keys as a column and not index 
dfjoined = dfjoined.merge(weather, on = ['created_date'], how = 'left')
xmlData[xmlData['zipcode'] == ' 98011-9478']
archive_copy['dog_description'] = archive_copy['text'].str.extract('(puppo|pupper|floofer|doggo)', expand=True)
index = pd.date_range(start='2018-01-01', periods=len(sales), freq='D') $ index
ann_ret_SP500[0].describe()
all_text = [pape['article'] for pape in all_papers]
figure_density_df = utility_patents_subset_df.dropna() $ sns.distplot(figure_density_df.figure_density, color="red") $ plt.show()
fps.working_dir
joined.head().T
p_new = (df2.converted).mean() $ p_new
from sklearn.ensemble import RandomForestRegressor $ model = RandomForestRegressor() $ print ('Random forest') $ RandomForestRegressor_model = reg_analysis(model,X_train, X_test, y_train, y_test)
itemTable.iloc[0:30]
df.show()
df_ad_state_metro_1['sponsor_types'].value_counts()
df_users
len(id_list)
kfpd.plugin = TrivialPlugin() $ fdf = time_method(kfpd, verbose = False, rerun_query = False, repetitions = 10) $ fdf.head()
%matplotlib inline $ import pandas as pd $ questions = pd.DataFrame.from_csv("data/multilingual_embeddings/questions.csv") $ questions.head()
df_goog.describe()
np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm) $ np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm) $ pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))
df_2001['bank_name'] = df_2001.bank_name.str.split(",").str[0] $
station = pd.read_sql("SELECT * FROM station", conn) $ station.head()
def display_features(features, feature_names): $     df = pd.DataFrame(data=features, $                       columns=feature_names) $     print (df)
df.rename(columns={'value':'lux'},inplace=True)
train_posts = data['post'][:train_size] $ train_tags = data['tags'][:train_size] $ test_posts = data['post'][train_size:] $ test_tags = data['tags'][train_size:]
med_comments = reddit['Comments'].median() $ reddit['Above_Below_Median'] = np.where(reddit['Comments']>=med_comments, 'Above', 'Below')
with open('data/chefkoch_02.json') as data_file:    $     chef02 = json.load(data_file) $ clean_new(chef02) $ chef02df = convert(chef02) $ chef02df.info()
data_nomonth = data.drop('month', axis=1) $ data_nomonth
df_t[df_t['Shipping Method name']=='DHLForYou Drop Off']['Updated Shipped diff'].hist() $ pd.DataFrame(df_t[df_t['Shipping Method name']=='DHLForYou Drop Off']['Updated Shipped diff'].describe())
%timeit pd.to_numeric(df_census.loc[:, "ward"])
uber = pd.read_csv('https://assets.datacamp.com/production/course_2023/datasets/nyc_uber_2014.csv') $ print(uber.shape) $ print(uber.head())
interpolated = bymin.resample("S").interpolate() $ interpolated
type(bday_us)
predict_actual['description_predict'] = description_predict $ predict_actual['predicted_label'] = predicted_2018
print("date_crawled:", autos["date_crawled"].str[:10].unique().shape[0]) $ print("last_seen:", autos["last_seen"].str[:10].unique().shape[0]) $ print("ad_created:", autos["ad_created"].str[:10].unique().shape[0])
%matplotlib inline $ train.num_points.value_counts(normalize=True)[0:20].plot()
n_new = df2.query('group == "treatment"')['user_id'].count() $ print(n_new)
df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', nrows=1000000, usecols=['Created Date', 'Closed Date', 'Agency Name', 'Complaint Type', 'Descriptor', 'Borough'])
run txt2pdf.py -o"2018-06-12-0815 MAYO CLINIC-ST MARY'S HOSPITAL - 2012 Percentiles.pdf" "2018-06-12-0815 MAYO CLINIC-ST MARY'S HOSPITAL - 2012 Percentiles.txt"
onehot = pd.get_dummies(wheels['drive_wheels']).head() $ onehot
bwd = df[['store_nbr']+ holidays].sort_index().groupby('store_nbr').rolling(7, min_periods=1).sum() $ fwd = df[['store_nbr']+ holidays].sort_index(ascending=False).groupby('store_nbr').rolling(7, min_periods=1).sum()
loans_df.drop(axis=1, labels=['earliest_cr_line', 'issue_d'], inplace=True)
max_div_stock=df.iloc[df["Dividend Yield"].idxmax()] $ max_div_stock $ print("The stock with the max dividend yield is %s with yield %s" % (max_div_stock['Company Name'],max_div_stock['Dividend Yield']))
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"').query('converted == 1').count()[0] $ convert_new = df2.query('landing_page == "new_page"').query('converted == 1').count()[0] 
df2.query('converted == 1').count()['user_id']/df2.shape[0]
df_new[['CA','UK','US']] = pd.get_dummies(df_new['country']) $ df_new.head()
date_cols = ["date_crawled", "last_seen", "ad_created"]
year_ago = dt.date(2017,8,23)-dt.timedelta(days=365) $ year_ago 
prices.plot()
dfs1 = pd.DataFrame.from_dict(team_gts_dict) $ dfs1 = dfs1.T.reset_index() $ dfs1 = pd.melt(dfs1,["index"], var_name="team", value_name="gts") $ dfs1 = dfs1.rename(columns={'index':'date'}) $ dfs1.head() $
live_weights.describe()
from stacking import stacking_regression $ from sklearn.metrics import mean_squared_error $ import numpy as np
session = Session(engine)
utils.add_coordinates(data)
p_diffs = np.random.binomial(n_new, p_new, 10000)/n_new - np.random.binomial(n_old, p_old, 10000)/n_old $
def clean_data(dataframe): $
model = sm.tsa.ARIMA(train, (1, 1, 0)).fit() $ print(model.params)
dataframe=pd.read_csv('merged_price_sentiment.csv') $ dataframe.drop("Unnamed: 0",axis=1,inplace=True) $ dataframe[:8]
sum(cm.diagonal())/sum(cm.flatten())
autos["brand"].unique().shape
len(labels)
endDate = setDate('arr')
displacy.serve(docs=doc, style='dep') $
df = pd.read_csv('CPSC-LOA-DATA.csv') $ df.head()
cityID = 'a307591cd0413588' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Buffalo.append(tweet) 
with open('data/kochbar_03.json') as data_file:    $     kochbar03 = json.load(data_file) $ koch03df = preprocess(kochbar03) $ koch03df.info()
df = pd.read_csv("data/airline_data.csv") 
unique_users = len(df.user_id.unique()) $ unique_users
p_old = ab_file2['converted'].mean() $ print(p_old)
import pandas as pd   # pd aliasing is convention
print("Number of mislabeled points out of a total %d points : %d" % \ $       (X_clf.shape[0], (y_clf != y_pred).sum()))
contrib_amounts = contribs[['contributor_state','amount']]
d.weekday()
df_main.info()
converted_users = df[df["converted"] == 1].shape[0] $ prop_converted = converted_users/df.shape[0] $ prop_converted
db=client.tweets
first_week = pd.date_range("2018/04/23", periods = 7, freq = "D") # first week of class $ first_week
m = RandomForestClassifier(n_estimators=15, min_samples_leaf=30,max_features=0.7, n_jobs=8) $ m.fit(X_data, y_data) $ f_05(y_data, m.predict(X_data))
url = 'http://cs.carleton.edu/cs_comps/0910/netflixprize/final_results/knn/img/knn/cos.png' $ Image(url,width=300, height=500)
df[df.msno == '+++hVY1rZox/33YtvDgmKA2Frg/2qhkz12B9ylCvh8o=']
df3 = df2
df_count = pd.DataFrame({"tweet_id":id_list,"favorite_count":favorite_count,"retweet_count":retweet_count}, $                         columns = ["tweet_id","favorite_count","retweet_count"])
gMapAddrDat.getGeoAddr(40.699100, -73.703697, test=True)  ## function called by buildOutDF() $
a = 'this is the first half ' $ b = 'and this is the second half' $ a + b
plt.figure(figsize=(15, 5)) $ sns.barplot(x="purpose", y="default", data=train,estimator=np.mean) $ plt.xticks(rotation='vertical');
df_twitter_copy['tweet_id'] = df_twitter_copy['tweet_id'].astype('str')
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key={}'.format(API_KEY), auth=('user', 'pass'))
merged.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount", ascending=False)
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).sort_index() * 100
import pandas as pd $ counts = pd.read_csv('FremontBridge.csv', index_col='Date', parse_dates=True) $ weather = pd.read_csv('data/BicycleWeather.csv', index_col='DATE', parse_dates=True)
driver.find_element_by_xpath('//*[@id="leftnav"]/li[2]/form/input[1]').send_keys('Avengers: Infinity War') $ driver.find_element_by_xpath('//*[@id="leftnav"]/li[2]/form/input[2]').click()
autos["registration_year"].describe()
session.query(Measurements.date).order_by(Measurements.date).first()
weather.to_csv(path_or_buf='weather_clean.csv', sep=',',index_label=False)
autos["price"].value_counts()
births = pd.read_csv('data/births.csv') $ births.head()
autos['ad_created'].str[:10].value_counts(normalize = True).sort_index()
merged1['DaysFromAppointmentCreatedToVisit'] = (merged1['AppointmentDate'] - merged1['AppointmentCreated']).dt.days
import pandas as pd $ import numpy as np $ pd.set_option('display.max_columns', 150) $ pd.set_option('display.max_colwidth', -1)
udois = dataset["article_doi"].drop_duplicates() $ invalid_dois = udois[~udois.apply(re.compile(r"10(\.\d+)+/.+$").match).apply(bool)] $ invalid_dois
for url in urls: $     print(url)
import pandas as pd $ import numpy as np
pos_sent = open("../data/positive_words.txt", encoding='utf-8').read() $ neg_sent = open("../data/negative_words.txt", encoding='utf-8').read() $ print(pos_sent[:101])
conn = pg8000.connect(user='dot_student', host='training.c1erymiua9dx.us-east-1.rds.amazonaws.com', port=5432, database='training', password='qgis')
df_archive_clean["source"].unique()
url='https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key={}'.format(API_KEY) $ r=requests.get(url)
omegafft = numpy.fft.rfftfreq(len(Q1), 1) $ Gfft = y/u
dd=cfs.diff_abundance('Subject','Control','Patient')
testing_active_listing=Binarizer(10000) $ testing_active_listing.fit(X_test['Active Listing Count '].values.reshape(-1,1)) $ testing_active_listing_dummy=testing_active_listing.transform( $     X_test['Active Listing Count '].values.reshape(-1,1))
autos.loc[autos.price<50.0,'price']=np.nan
inputNetwork.add_nodes(**filter_models['inputFilter'])
ldamodel.print_topics()
raw_data.head()
fs = gridfs.GridFS(db)
(~autos["year_of_registration"].between(1910,2016)).sum() / autos.shape[0]
fundvolume_binary.T.sum().plot(logy=True) $ plt.title('Fund') $ plt.ylabel('Volume') $ plt.show()
data.loc[:'Utah', 'two']
predictions = pd.Series.from_array(model.predict(res_c_i.as_matrix())) $ res = res_c_i $ res['predicted_clv'] = model.predict(res_c_i.as_matrix())
new_page_converted = np.random.choice([0, 1], size=n_new, p=[1-convert_new, convert_new]) $ new_page_converted
if False: $     plt.scatter(train.diff_lng, train.duration) $     plt.show()
autos["nr_of_pictures"].value_counts()
score_c = score[(score["score"] < 80) & (score["score"] >= 70)] $ score_c.shape[0]
%reload_ext sql
len(autos[autos['kilometer']>100000])/len(autos)
(df2['converted']==1).mean()
tweets ['apple'] = (tweets['hashtags'] + tweets['user_mentions']).apply(lambda x: True if 'apple' in x else False) $ tweets ['samsung'] = (tweets['hashtags'] + tweets['user_mentions']).apply(lambda x: True if 'samsung' in x else False)
len(p_diffs)
clf = RandomForestClassifier(n_estimators=100)
product.iloc[product[product['MATNR'] == 19112395].index[0]]
target_user = "MarsWxReport"
test_data_features = vectorizer.transform(test.review) $ test_data_features = test_data_features.toarray()
d1 = pd.DataFrame({'city': ['Seattle', 'Boston', 'New York'], 'population': [704352, 673184, 8537673]}) $ d2 = pd.DataFrame({'city': ['Boston', 'New York', 'Seattle'], 'area': [48.42, 468.48, 142.5]}) $ pd.merge(d1, d2)
search['flight_type'] = search['message'].apply(flight_type)
df_2016.dropna(inplace=True) $ df_2016
url = 'http://fantasysports.yahooapis.com/fantasy/v2/leagues;league_keys=nfl.l.427049' $ r = s.get(url, params={'format': 'json'}) $ r.status_code
frame = pd.DataFrame({x: np.log10(pd.Series(list(y))) for x, y in grouped.items()}) $ plt.figure(figsize=(17, 7)) $ frame.boxplot(return_type='axes') $ plt.ylabel('log10(firstPaint)') $ plt.show()
predictortVar = ['intercept', 'ab_page'] $ logReg = sm.Logit(df2['converted'], df2[predictortVar]) $ answer = logReg.fit()
groceries.loc['eggs']  # explicity using a labeled index
%matplotlib inline
stations['name']
twitter_final.head(2)
s = requests.Session() $ s.get(url, headers=headers)
located_data.head(3)
start = datetime.now() $ modelxg_t1000 = xgb.XGBClassifier(max_depth=50, learning_rate=.1, n_estimators=1000, n_jobs=-1) $ modelxg_t1000.fit(Xtr.toarray(), ytr) $ print(modelxg_t1000.score(Xte.toarray(), yte)) $ print((datetime.now() - start).seconds)
df.head(2)
promo_df['after_onpromotion'] = promo_df['after_onpromotion'].astype(np.int16) $ promo_df['before_onpromotion'] = promo_df['before_onpromotion'].astype(np.int16)
mean_for_each_weekday = delineate_in_weekdays.mean().reset_index() $ mean_for_each_weekday
spacy_tok = spacy.load('en')
print("numNodes = ", dtModel.numNodes) $ print("depth = ", dtModel.depth)
np.abs(frame)
app_ver = joined_df.groupby(['id'])['initial_app_version'].first().to_frame() $ app_ver.head()
get_all_columns(tweet)
my_df.columns = ['text', 'polarity'] $ my_df.head()
%matplotlib inline
poiModel = Sequential() $ poiModel.add(Embedding(num_POI+1, output_dim=vec_dim, input_length=1)) $ poiModel.add(Reshape((vec_dim,)))
speeches_df4 = speeches_df3[['chamber', 'doc_title', 'extension', 'id', 'itemno', $         'num', 'pages', 'speaker', 'speaker_bioguide', 'text', $        'title', 'turn', 'date']]
sessions.head()
flight = spark.read.parquet("/home/ubuntu/parquet/flight.parquet")
c.info()
subway2_df['datetime'] = subway2_df[['DATE','TIME']].apply(lambda x: ' '.join(x), axis=1) $ subway2_df['datetime'] = pd.to_datetime(subway2_df['datetime'])
tickerdf.head()
cc['logspread'] = np.log1p(cc['spread']) $ plt.hist(cc['logspread']) $ plt.show()
bad_dates = mapped.filter(lambda row: (row[3] == -2 or row[4] == -2)) $
collection.write('AAPL', aapl[:-1], metadata={'source': 'Quandl'})
A = np.random.uniform(0,1,(5,5)) $ B = np.random.uniform(0,1,(5,5)) $ np.diag(np.dot(A, B)) $ np.sum(A * B.T, axis=1) $ np.einsum("ij,ji->i", A, B)
df_all_wells_wKNN
data = pd.read_csv('csvs/datosSinDuplicados.csv', low_memory=False)
week16 = week15.rename(columns={112:'112'}) $ stocks = stocks.rename(columns={'Week 15':'Week 16','105':'112'}) $ week16 = pd.merge(stocks,week16,on=['112','Tickers']) $ week16.drop_duplicates(subset='Link',inplace=True)
def concat_weather_types(vals): $     return ",".join(list(set(vals)))
tipsDF = tipsDF.drop(tipsDF.columns[0], axis = 1) $ tipsDF.head()
df_countries = pd.read_csv('./countries.csv') $ df_countries.head()
lda = np.linspace(20, 50, 300) $ dist = [f(a) for a in lda] $ plt.figure(figsize=(10, 5)) $ _ = plt.plot(lda, dist)
df_merged.boxplot(column='retweet_count');
liberia_data4 = liberia_data3.set_index(['Description', 'Date']).sort_index() $ liberia_data4.columns = ['liberia'] $ liberia_data4
train.corr()["hotel_cluster"]
Pop_df = pd.DataFrame(Population) 
customers_arr[125]
rf.fit(fX, fy) $ cm = confusion_matrix(fy, rf.predict(fX)) $ sns.heatmap(cm, annot=True)
print(df.shape) $ df.head(3)
tweets_df.retweet_count
news_title = soup.title.text $ print(news_title)
print('Most similar word to \"malignancy\":') $ print(modelWord2Vec.most_similar(positive="malignancy", topn=3))
from sklearn.model_selection import KFold $ cv = KFold(n_splits=10, random_state=None, shuffle=True) $ estimator = Ridge(alpha=3000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
df.drop(['parsed', 'github_id', 'node_id'], axis=1, inplace=True)
reliableData = df_data_1[df_data_1.DATE >= "2017-07-03 00:00:00"] $ print reliableData
smape_fast(y_pred, y_true)
tweet_image_clean.shape $
df.head(3)
X__ = X.dropna() $ X_ = X__.iloc[:,7:] $ y_ = X__['age at death'] $ y_q = pd.qcut(y_,[.5, ], [0,1,2,3]) 
train_data_features.shape
len(calls)
temp["time"] = pd.to_datetime(temp["time"])
df = df[-df.Address.str.contains('Intersection')] $ df = df[df.Address.str.count(',') == 3]
import pandas as pd $ import numpy as np $ from pandas import Timestamp $ import os $ from datetime import datetime, timedelta
x1 = pd.DataFrame(df.groupby(['donor_id','zipcode']).zipcode.nunique()) $ x1[x1.zipcode != 1]
periods = [pd.Period('2012-01'), pd.Period('2012-02'), pd.Period('2012-03')]
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
churned_ordered_end_date = [churned_ordered.loc[bid,'canceled_at'][-1] if churned_df.loc[bid,'cancel_at_period_end'][-1] == False else churned_ordered.loc[bid,'current_period_end'][-1] if churned_ordered.loc[bid,'cancel_at_period_end'][-1]==True else None for bid in churned_ordered.index]
regex = re.compile('[^A-Za-z ]+')
data.filter(data['Height'] > 161).select(data['id']).show()
print (df.iloc[1,2])
intersections_final = intersections_irr[final_columns]
ls_data = pd.read_excel(cwd+'\\LS_mail_0604_from_python.xlsx') $ ls_columns=ls_data.columns.values
reviews.groupby('variety').price.agg([min,max])
bigram_sentences = LineSentence(bigram_sentences_filepath) $ for bigram_sentence in it.islice(bigram_sentences, 230, 240): $     print u' '.join(bigram_sentence) $     print u''
learner.fit(3e-3, 4, wds=1e-6, cycle_len=1, cycle_mult=2)
texts = [tokens for _, tokens in tokenised]
autos["fuel_type"].unique()
tweet_archive_clean[tweet_archive_clean.name.isin(["O'Malley", "Gin & Tonic"])]
get_gts('2018-08-01', '2018-08-31') $ get_gts('2018-09-01', '2018-09-09')
pre_analyzeable = pre[(pre['id'].isin(ids_match_pre))&(pre['Internal ID']!=86221654)] $ len(pre_analyzeable)
total_rows = df.shape[0] $ print(total_rows)
sns.pairplot(segmentData, diag_kind="kde", hue="opportunity_stage");
breakdown
c = np.concatenate([a, b]); c  #Concatenate along an existing axis.
merged2.dropna(subset=['Specialty'], how='all', inplace=True)
mod2 = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']]) $ mod2.fit().summary()
len(df2.query('group=="treatment"').query('converted==1'))/len(df2.query('group=="treatment"'))
ethereum_github_issues_url = blockchain_projects_github_issues_urls[1] $ ethereum_github_issues_df  = pd.read_json(get_http_json_response_contents(ethereum_github_issues_url))
store.head()
r=requests.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv') $ with open('image_predictions.tsv','wb') as file: $     file.write(r.content)
df = pd.DataFrame(np.random.randn(7,4), index=dates, columns=list('ABCD')) $ df
from sklearn.preprocessing import PolynomialFeatures $ poly1 = PolynomialFeatures(degree=5) $ x = np.array(hours['hour']).reshape(-1,1) $ print(x)
df_series
subrcvec = pd.DataFrame(cvecdata.todense(),      $              columns=cvec.get_feature_names())      # subreddit cvec $ subrcvec.shape
payload_scoring = {"values": [test_1, test_2]} $ print(payload_scoring)
sgd_predict = sgd.predict(testx)
bacteria['Actinobacteria']
age_groups(writers,0,"Age")
pos_tweets_rf = [ tweet for index, tweet in enumerate(data_rf['Tweets']) if data_rf['SA'][index] > 0] $ neu_tweets_rf = [ tweet for index, tweet in enumerate(data_rf['Tweets']) if data_rf['SA'][index] == 0] $ neg_tweets_rf = [ tweet for index, tweet in enumerate(data_rf['Tweets']) if data_rf['SA'][index] < 0] $
import pandas as pd
files
print(data.columns) $ print(type(data)) $ data.describe()
df2 = ab_df[((ab_df['group'] == 'treatment') & (ab_df['landing_page'] == 'new_page')) | ((ab_df['group'] == 'control') & (ab_df['landing_page'] == 'old_page'))]
join_a.count()
collection = db.test_collection
articles['created_at'] = articles.apply((lambda row: dateparser.parse(row.date[0])), axis=1)
lda = prep.get_lda_model(from_scratch=False)
shows = pd.read_pickle("ismyshowcancelled_raw_pull.pkl")
df2[df2['user_id']==id_duplicated[0]]
df2['sales'] = df2.new_sales + random_differences $ df2 = df2.round({'sales':2}) $ df2.head()
lmdict[lmdict.Negative != 0].head()
def get_list_User_ID(the_posts): $     list_User_ID = [] $     for i in list_Media_ID: $         list_User_ID.append(the_posts[i]['shortcode_media']['owner']['id']) $     return list_User_ID
twitter_df_clean.info()
df2[['cntry_US','cntry_UK','cntry_CA']]=pd.get_dummies(df2['country']) $ df2.head()
df=pd.read_csv("dataset_quora/quora_train_test.csv")
batting_df.head(10)
df2['intercept'] = 1 $ df2[['control_page','ab_page']] = pd.get_dummies(df2['group']) $ df2.drop(['control_page'],axis=1,inplace=True) $ df2.head()
files = [f for f in listdir('Twitter_SCRAPING/scraped/') if f.endswith('.csv') and isfile(join('Twitter_SCRAPING/scraped/', f))] $ d_scrape = pd.concat([pd.read_csv('Twitter_SCRAPING/scraped/'+f, encoding='utf-8') for f in files], keys=files) $ print(d_scrape.head())
from scipy.stats import norm $ norm.cdf(z_score)  # significance of our z_score
previous_month_date = end_date - timedelta(days=30) $ PR = PullRequests(github_index).is_closed().get_percentiles("time_to_close_days")\ $                                .since(start=previous_month_date)\ $                                .until(end=end_date) $ get_aggs(PR)
%time train_4_reduced = tsvd.transform(train_4)
for row in session.query(Measurements).limit(5).all(): $     print(row) $
from numpy import *
df_sub.sort_values(by=['Prediction'],ascending=False)
df2 = df2.drop_duplicates(['user_id'], keep='first')
df_namb = (st_streams['/streams/amb'].count() * 100.0) / namb_instance $ df_ndoor = (st_streams['/streams/door'].count() * 100.0) / ndoor_instance $ df_nmcu = (st_streams['/streams/mcu'].count() * 100.0) / nmcu_instance $ df_nrelay = (st_streams['/streams/relay'].count() * 100.0) / nrelay_instance $ df_ntemp = (st_streams['/streams/temp'].count() * 100.0) / ntemp_instance
df2[df2.duplicated('user_id', keep=False) == True]
df_test = full_text_classifier.predict(df_test) $ df_test.head()
captions
(df2.query('group == "treatment"')['converted'] == 1).sum()/(df2.query('group == "treatment"')['converted'] >= 0).sum()
print(loadedModelArtifact.name) $ print(saved_model.uid)
!!from time import sleep $ for i in range(0, 100): $     print(i) $     sleep(0.1) $ assert False
cat_col_index = loan_stats.columns_by_type(coltype='categorical') $ loan_stats[cat_col_index].head(rows=1)
test_sentence = pd.Series(test_sentence) $ test_sentence[0]
folder_name = '/home/workspace' $ if not os.path.exists(folder_name): $     os.makedirs(folder_name)
def get_tweet_sentiment(tweet): $     return TextBlob(tweet).sentiment.polarity
df_average = df.groupby(['business_id']).mean() $ df_average.head()
train_data.notRepairedDamage.fillna('nein', inplace = True) $ test_data.notRepairedDamage.fillna('nein', inplace = True)
baseball_h.index[:10]
for date in trueDate: $     for comp in company: $         df[comp][date] = HMM(comp, date-timedelta(211), date-timedelta(1)) $ df.head(10)
df.drop_duplicates(['title'],keep= 'first',inplace =True)
from datetime import datetime $ df['created_at'] = df['created_at'].apply(lambda x: datetime.strptime(x , "%a %b %d %H:%M:%S %z %Y"))
reviews['listing_id'].value_counts().plot(kind='hist', figsize = (20,10), fontsize = 16, title = 'Frequency of Reviews') $ axes = plt.gca() $ axes.set_xlim([0,250]) $ axes.set_ylim([0,2500]) $ plt.savefig('freq reviews.jpg') $
df_unit_after=pd.merge(df_unit,df_users_6_after[['uid']],left_on='uid',right_on='uid',how='inner')
sub = pd.DataFrame() $ sub['click_id'] = test_df['click_id'] $ print("Sub dimension "    + str(sub.shape))
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=YxSY4z-2vyxvJ15WjtFa&start_date=2017-01-01&end_date=2017-12-31" $ req = requests.get(url)
train['year'] = pd.to_datetime(train['date']).dt.year $ train['month'] = pd.to_datetime(train['date']).dt.month $ train['weekday'] = pd.to_datetime(train['date']).dt.weekday
gcv.best_score_
INC = pd.read_csv('s3://datapd/SS_DUN_Incidents_Dem_Zone_join.csv') $
posts.reset_index(inplace=True)
Senate = pd.read_csv('~/Desktop/Senate.csv', index_col='state')
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline
def rmprecent(s): $     return s.split("%")[0] $ hsi["rise_nextday"] = (hsi["Change %"].apply(rmprecent).astype("float") >= 1).astype("int")
plt.figure(figsize=(8, 5)) $ plt.hist(train_df.views_lognorm); $ plt.title('The distribution of the property views_lognorm');
filename = 'continentdict.csv' $ print '%s is created at %s, last modified at %s, full path is %s' %( $     filename, os.path.getctime(filename), os.path.getmtime(filename), os.path.abspath(filename))
pred.p1_dog.value_counts()
rf200 = RandomForestClassifier(n_estimators=200) $ rf200.fit(X, y)
from statsmodels.tsa.stattools import adfuller as ADF $
t2['p1'] = t2['p1'].str.replace('_', ' ') $ t2['p2'] = t2['p2'].str.replace('_', ' ') $ t2['p3'] = t2['p3'].str.replace('_', ' ')
ds = odm2rest_request('datasets')
tweets_clean['p1_probability_of_dog'] = tweets_clean.apply (lambda row: real_prob (row),axis=1) $ tweets_clean['p2_probability_of_dog'] = tweets_clean.apply (lambda row: real_prob2 (row),axis=1) $ tweets_clean['p3_probability_of_dog'] = tweets_clean.apply (lambda row: real_prob3 (row),axis=1) $ tweets_clean.head(40)
list(itertools.permutations('ABCD', 2))
print (Ralston.TMAX.std(), Ralston.TMAXc.std())
dfChile = dfChile[dfChile["longitude"] > -76.000000]
column = ['US'] $ df_new2 = df_new.drop(column, axis=1) $ df_new2.head()
print(ndarray)
df = df.sort_values('DATE')
output_file_path = "tweets.ftvec" $ with open(output_file_path, "r") as f: $     lines = f.readlines() $ lines = [l.split(" ")[:-1] for l in lines]
(null_value > obs_diff).mean()
run_augmented_Dickey_Fuller_test(series=RNPA_existing_hours, num_diffs=2)
df_goog[['Open', 'High', 'Low', 'Close', 'Adj Close']].plot()
df.head()
print('My PSA will reach 3.0 on', gregorian_predicted, '.')
df_A['Gender'] = ['M', 'F', 'M', 'M', 'F'] $ df_A
billboard.head() $ tracks = billboard[['year','artist','track','time']] $ print(tracks.info()) $ tracks_no_duplicates = tracks.drop_duplicates() $ print(tracks_no_duplicates.info()) $
github_data['project_year'] = github_data.project_creation_date.apply(lambda x: x[:4]) $ github_data['user_year'] = github_data.user_creation_date.apply(lambda x: x[:4])
p = r.columns
d.month
pixel_to_plot = (100,100) $ pixel_to_plot
df_pol_d.shape
from sklearn.externals import joblib $ ab = joblib.load('adaboost.pkl') 
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs) $ plt.grid() $ plt.axvline(p_diffs.mean(), color='r', label='mean') $ plt.legend();
len(cbg)
autos.describe(include='all')
temp_mean.plot(kind='bar')
dfjoined.head()
movies.printSchema()
df_twitter_archive_master.info()
df.loc[df.index[df.handle == 'Jim Cramer'], 'label'] = 2
import pandas as pd $ import seaborn as sns $ import matplotlib.pyplot as plt $ pd.set_option('display.float_format', lambda x: '%.2f' % x)
columns = ['date_crawled', 'name', 'seller', 'offer_type', 'price', 'abtest', $        'vehicle_type', 'year_of_registration', 'gear_box', 'power_ps', 'model', $        'odometer', 'month_of_registration', 'fuel_type', 'brand', $        'not_repaired_damage', 'date_created', 'nr_of_pictures', 'postal_code', $        'last_seen']
import pandas as pd $ from math import sin, cos, sqrt, atan2, radians $ import numpy as np $ import matplotlib.pyplot as plt
def time_to_str(time): $     if time != time: $         return time $     else: $         return '{:04}'.format(int(time))
from sklearn.cross_validation import train_test_split $ SEED = 2000 $ x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.10, random_state=SEED) $ x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)
logit_mod2 = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'US', 'UK']]) $ results2 = logit_mod2.fit() $ results2.summary()
url = form_url(f'organizations/{org_id}/playerPositions', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response.json()['items'])
df.drop(df[(df.state == 'YY') & (df.amount >= 45000)].index, inplace=True)
from pyspark.sql.functions import desc $ df.groupby('Park Facility Name').count().sort(desc("count")).show()
!curl -o FremontBridge.csv https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD
df_link_meta['link.domain_resolved'] = df_link_meta['link.domain_resolved'].astype(str)
round(df2[df2['group'] == 'treatment']['converted'].mean(), 4)
df_discharges_totals.head()
(autos["date_crawled"] $         .str[:10] $         .value_counts(dropna=False) $         .sort_values() $         )
df_495 = pd.read_sql(sql_495,conn_laurel) $ df_495.groupby(['accepted','paid','preview_clicked','preview_watched','preview_finished'])['applicant_id'].count().unstack().fillna(0)
print(sample_df.shape) $ sample_df.head()
sentiments_df = df.apply(get_polarity_scores, axis=1) $ sentiments_df.head()
unique_users_count = df['user_id'].nunique() $ print(unique_users_count)
diff = new_page_converted/n_new - old_page_converted/n_old $ diff 
languages = [] $ def lang_detection(tweet): $     for tweet in tweet_textblob_de: $         lang = translator.detect(tweet).lang $         languages.append(lang)
window = pdf.loc['2008-1-1':'2009-3-31'] $ portfolio_metrics(window) $ window.plot(); $
df.head()
df['B']
obs_diff = trtmt_conv_rt - ctrl_conv_rt $ obs_diff
transactions[~transactions["UserID"].isin(users["UserID"])] $
print('Largest Change in any one day:', change.max())
df = df_with_metac_with_onc.drop(columns = ls_other_columns) $ df_total = pd.concat([df, df_other_dummies], axis = 1)
ab_page = np.exp(-0.0155) $ intercept = np.exp(-1.9879) $ (ab_page, intercept,)
print('\nPerplexity: ', ldamodel.log_perplexity(doc_term_matrix)) $ coherence_model_lda = CoherenceModel(model=ldamodel, texts=tweets_list, dictionary=dictionary, coherence='c_v') $ coherence_lda = coherence_model_lda.get_coherence() $ print('\nCoherence Score: ', coherence_lda)
train_ratio = 0.95 $ train_size = int(train_ratio*len(joined_train_df)) $ joined_train = joined_train_df[:train_size] $ joined_valid = joined_train_df[train_size:] $ len(joined_train)+len(joined_valid)==len(joined_train_df)
sentiment_df["Company"]=screen_names $ sentiment_df
for column in has_stage_archive.iloc[:,8:12].columns: $     avg_favorite_count = has_stage_archive[has_stage_archive[column] == 1]['favorite_count'].mean() $     print('The average favorite count for {}s is {}'.format(column, avg_favorite_count))
df.tail(5)
training_pending_ratio.shape
print("Probability of individual converting:", df2.converted.mean())
data.columns
add_plane_data.registerTempTable('tmp_400hz_usage')
df1 = df.drop('Cabin', axis=1) $ df1.shape
import pandas as pd $ import pymysql $ from sqlalchemy import create_engine $ engine = create_engine('mysql+pymysql://root:root@localhost/nutridb_sr25_sanitized')
final_topbikes.groupby(by=final_topbikes.index.weekday)['Distance'].count().plot(kind='bar', figsize=(12,6))
uusers = df['user_id'].nunique() $ print("Our dataset contains {} unique users.".format(uusers))
print(df2.shape) $ print(countries.shape)
unique_users = df.user_id.unique().size $ unique_users
sns.violinplot(autos["odometer_km"]);
fig, ax = plt.subplots(1, figsize=(12,4)) $ plot_with_moving_average(ax, 'MA Doctors', doc_duration) $ fig.savefig('./images/dr_MA6.png')
pd.io.json.json_normalize(playlist['tracks']['items'][2])
df2[df2['user_id'].duplicated() == True]['user_id']
df['Complaint Type'].groupby(by=df.index.month).count()
df.dtypes
print('UK is having {:0.4f} times more converted users than US, all other features held constant.'.format(np.exp(0.0099)))
features_regress_vect = vectorizer.transform(features_regress)
condos.drop(['OBJECTID', 'STATUS', 'UNITTYPE', 'METADATA_ID'], axis=1, inplace=True)
d + pd.tseries.offsets.Week()
birthyears
m_vals = np.linspace(m_true-3, m_true+3, 100) $ c_vals = np.linspace(c_true-3, c_true+3, 100)
print(df_by_donor.head())
import pandas as pd
recommend.dtypes # Returns the dtypes of a Dataframee
nold = len(df2.query('group =="control"')) $ nold
df1 = pd.DataFrame() $ df1['ACME'] = np.random.randint(low=20000, high=30000, size=100) $ df1['JUBII'] = np.random.randint(low=20000, high=40000, size=100) $ df1.index = pd.date_range('1/1/2014', periods=100, freq='H')
from sklearn.metrics import classification_report $ labels = ['Setosa', 'Versicolor', 'Virginica'] $ y_pred = rfc.predict(d_test_sc) $ print(classification_report(l_test, y_pred, \ $                             target_names = labels))
full_orig[full_orig['Patient'] == 'fc443307-b9d7-4a74-8ac5-314aa1b0803a']
park.named_fda_drug.notnull().sum()
col_to_remove = list(set(train_data.columns).difference(set(test_data.columns)))
print((df_pol_d.select_dtypes(include=['O']).columns.values))
x["A+B"] = x["A"] + x["B"] $ x
plt.plot(pipe.tracking_error)
fix_comma = lambda x: pd.Series([i for i in reversed(x.split(','))])
filter_frame = event_schedule['n_frames'] > 10 $ event_schedule[filter_frame]
consistently_delayed = delays_by_origin[delays_by_origin['count'] > 14] $ highly_delayed = consistently_delayed.sort_values('mean', ascending=False).head(100) $ highly_delayed.head(10)
twitter_master1.info()
p + pd.tseries.offsets.Hour(2)
query.all() #all() returns a list:
uniqueUsers = userArtistDF.select("userID").distinct().count() $ print("Total n. of distinct users: ", uniqueUsers)
dmh.PandasDataFrame.__subclasses__()
sessions.head(2) 
train.head()
pwd = os.getcwd() $ df_users = pd.read_csv( pwd+'/users.052317.csv', encoding='utf-8') #user, need to find a way to link them, since it is only individual record. $
pandas_df = df.toPandas().join(df_loc)
data.shape
train = pd.read_csv("../data/wikipedia3/wikipedia_train3.csv", parse_dates=['date']) $ test = pd.read_csv("../data/wikipedia3/wikipedia_test3.csv", parse_dates=['date'])
import statsmodels.api as sm $ convert_old = df2[df2["landing_page"] == 'old_page']['converted'].sum() $ convert_new = df2[df2["landing_page"] == 'new_page']['converted'].sum() $ n_old = df2[df2["landing_page"] == 'old_page'].shape[0] $ n_new = df2[df2["landing_page"] == 'new_page'].shape[0]
[each for each in df2.columns if 'ORIGIN' in each.upper()]
unique_instance.content $
prop = df2.groupby(["converted","group"]) $ prop.count()
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA','UK']]) $ results = logit_mod.fit() $ results.summary()
df_sites_not_yet_done.head()
B2.allowed_indicator_settings_steps
log_model_us = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','US']]) $ result = log_model_us.fit() $ result.summary()
df['Unique Key'].groupby(df.index.hour).count().plot()
first_comb = df[(df['group']=='treatment') & (df['landing_page']!='new_page')]
df_person.merge(df_grades, how="right", left_on = "id", right_on="person_id")
Pnew=df2['converted'].mean() $ Pnew
days_alive = (datetime.datetime.today() - datetime.datetime(1981, 6, 11)) $ days_alive.days
RF=RandomForestRegressor(max_depth=10,n_estimators=40,random_state=20)
print(json.dumps(experiment_details, indent=2))
len(fb_train), len(fb_test)
df_new = pd.get_dummies(df_new, columns=['country']) $ df_new.head()
shopping_carts.shape
results 
LT906474.head(2)
pwd
test = pd.read_csv(dirs[2], dtype={"ip":"int32", "app":"int16", "device":"int16", "os":"int16", "channel":"int16"})
with open('translated_tweet','rb') as f: $     tweet_df = pickle.load(f)
utc_dt_str = '2017-05-03T14:15:00Z' $ import pytz $ dt = datetime.datetime.strptime(utc_dt_str, '%Y-%m-%dT%H:%M:%SZ') $ utc_tz = pytz.timezone('UTC') $ utc_dt = utc_tz.localize(dt)
to_be_predicted_Day5 = 31.29300322 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
soup = BeautifulSoup(page, 'html5lib') $ print(soup.prettify()[:1000])
print ("Jumlah Data Tweets Indonesia :", len(tweetsIn22Mar) + len(tweetsIn1Apr) + len(tweetsIn2Apr)) $ print(f"Jumlah Data:\n22 Maret: {len(tweetsIn22Mar)}\n1 April: {len(tweetsIn1Apr)}\n2 April: {len(tweetsIn2Apr)}\n")
models.save('models')
plantlist = getbnetzalist(url_bnetza) $ plantlist.head()
march_2016.start_time
df_sub_headline = df_tweets[(df_tweets["sub-headline"].str.len() > 0)] $ sub_headline_date_count = df_sub_headline.groupby(["headline", "sub-headline", "date"]).size() $ sub_headline_date_count[sub_headline_date_count > 1].index.get_level_values(1)
norm.cdf(z_score)
df_new[['CA', 'UK', 'US']]=pd.get_dummies(df_new["country"])
df.apply(np.cumsum)
print(data.shape) $ data = data.drop_duplicates() $ print(data.shape)
raw = get_excel_range(input_data, "Raw", "A1:AH1095") $ raw.head()
prod_ch = pd.read_csv("2188_production_nomment.csv",sep=",",comment="#")
print('Level 1 Keys: ' + str(data.keys())) $ for key, value in data['dataset_data'].items(): $     print(key,':',value) $ data2 = data['dataset_data']['data']  # only data
results_BallBerry, out_file2 = S.execute(run_suffix="BallBerry_hs", run_option = 'local')
pd.merge(left=users,right=sessions,how="inner",left_on=['UserID','Registered'],right_on=['UserID','SessionDate'])
df.loc[1,"last_name"] = "Kilter" $ df
store_items
monte.str.findall(r'^[^AEIOU].*[^aeiou]$')
from sklearn.decomposition import PCA $ import sklearn
students.iloc[8]
print_query(3443)
user = api.get_user(twitter_names[0])
from sklearn.linear_model import LogisticRegression $ logit = LogisticRegression(max_iter=200) $ logit.fit(train[simple_features], y_train)
df_ratings.describe()
f, ax = plt.subplots(figsize=(15, 5)) $ sns.countplot(x="complaint_type", data = samp311);
y.value_counts()
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new.drop(['CA'], axis=1, inplace=True) # drop Canada column $ log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US']]) $ results = log_mod.fit() $ results.summary()
ab_file.to_csv('ab_edited.csv', index=False)
tranny = df.T $ tranny
user_unique = df.user_id.nunique(); $ user_unique
tweets_df = pd.read_csv('C:\\Users\\ericr\\Google Drive\\schoolwork\\current classes\\CS 230\\Project\\230_crypto\\data\\twitter\\hourly\\labeled_tweets_hourly.csv', encoding='ISO-8859-1') $ print("There are {} tweets".format(tweets_df.shape[0])) $ tweets_df.head()
log_mod = sm.Logit(df_new['converted'],df_new[['ab_page','US','UK','intercept']]) $ result = log_mod.fit() $ result.summary()
y_preds = gbdt_grid.best_estimator_.predict(X_test) $ gbdt_scores = show_model_metrics('Gradient Boosting Decision Trees', gbdt_grid, y_test, y_preds)
df_boost = df_train[['interest_level','latitude','longitude','bathrooms','bedrooms','price','num_photos','num_features','num_description_words','created_year','created_month','created_day']]
data_libraries_df.to_csv("output/data_libraries_{}.tsv".format(date), index=False, sep="\t", na_rep="NA")
pr('Making the new dataframe of additionnal rows and appending it to the original dataframe..') $ addedHashtagsDf = pd.DataFrame(addedHashtagsRowsList) $ addedHashtagsDf.set_index(['createdAt'], inplace=True) $ tw6 = tw5_1.append(addedHashtagsDf) $ pr('Done: Original dataframe size was: {} - New dataframe size is: {}'.format(strNb(len(tw5_1)),strNb(len(tw6))))
grid_tfidf.cv_results_['mean_test_score']
pd.merge(left=users, right=sessions, how='inner', left_on=['UserID', 'Registered'], right_on=['UserID', 'SessionDate'])
df.resample('M').count()
a2 = 1 $ b2 = 0.2 $ df2['W'] = a2 + b2 * norm.rvs(size=N, loc=df['X']) $ df2['Y'] = a2 + b2 * norm.rvs(size=N, loc=df['W'])
jobPostDFSample = jobPostDFSample.reset_index()
df2['user_id'].nunique() $ print ("Total Number of Unique row : {}".format(df2['user_id'].nunique()))
df2.reindex_like(df1,method='nearest')
def combine_DfOfAllWells_with_knnDf(dask_df_all_wells_basic,knn_df): $     return dask_df_all_wells_wKNN
rows = df.shape[0] $ print('The data set contains ' +str(rows)+ ' rows')
coinbase_btc_eur_min['Timestamp'] = pd.to_datetime(coinbase_btc_eur_min['Timestamp'], format="%Y/%m/%d %H:%M")
avg_day_of_month15 = day_of_year15.groupby("avg_day_of_month").mean() $ avg_day_of_month15.head()
display(data.describe())
sanders_df = pd.DataFrame(sanders) $ sanders_df.head()
my_employee.__str__()
pres_df.head(2)
CRnew = df2.converted.sum()/df2.count()[0] $ CRnew
(grades > 5).all(axis = 1)
type(tag_df.stack().index)
df_new['intercept'] = 1 $ df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new.head()
store_items = store_items.rename(columns={'bikes': 'hats'}) $ store_items
dictionary.save('experiment/dictionary.dict')
data.tasker_id.nunique()
df_vow['Close'].unique()
monte.str.len()
pattern = re.compile('AA') $ print(pattern.search('AAbcAA')) $ print(pattern.search('bcAA'))
raw_data.isnull().sum()
AAPL.tail()
kick_projects.loc[:,'goal_reached'] = kick_projects['pledged'] / kick_projects['goal'] # Pledged amount as a percentage of goal. $ kick_projects.loc[kick_projects['backers'] == 0, 'backers'] = 1 $ kick_projects.loc[:,'pledge_per_backer'] = kick_projects['pledged'] / kick_projects['backers'] # Pledged amount per backer.
targetUserItemInt=userItemInt.join(targetUsers.set_index(['user_id']),on='user_id',how='inner') $ print targetUserItemInt.shape $ targetUserItemInt.head()
import pandas as pd $ apple = pd.read_csv('D:\\Pandas\\CodeBasics\\datasets\\15_applnodates.csv', sep = '\t') $ apple.head() # My df has no dates which is pretty important for most df nevermind stock prices
from bs4 import BeautifulSoup $ import requests $ from splinter import Browser
logit=sm.Logit(df2['converted'],df2[['intercept', 'treatment']])
labels=[i.replace(i[0: i.index('.')+1], '') for i in labels]
sns.heatmap(prec_fine)
m.fit(lr, 3, metrics=[exp_rmspe])
print("No missing values")
s.iloc[2:3]
df_pol[df_pol['pol_id']=='Conservative']['domain'].value_counts().head(20).plot(kind='bar');
pd.isna(df1)
import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
data_issues_csv=pd.read_csv('/Users/JoaoGomes/Dropbox/Xcelerated/assessment/data/avro-issues.csv')
apps_by_dist = df_nona.groupby(['segment', 'district_id']).app_id.nunique() $ print apps_by_dist.groupby(level='segment').mean() $ apps_by_dist.groupby(level='segment').describe() $
merged1 = merged1.rename(columns={'MeetingReasonForVisitId': 'ReasonForVisitId', 'Name':'ReasonForVisitName', 'Description':'ReasonForVisitDescription'})
grades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]]) $ grades = pd.DataFrame(grades_array, columns=["sep", "oct", "nov"], index=["alice","bob","charles","darwin"]) $ grades
nypd_unspecified = data[(data['Borough']=='Unspecified') & (data['Agency']=="NYPD")]['Borough'].count() $ nypd_unspecified
wd=1e-7 $ bptt=70 $ bs=52 $ opt_fn = partial(optim.Adam, betas=(0.8, 0.99))
submit.to_csv('log_reg_baseline.csv', index = False)
filtered_file = master_file.copy(); $ filtered_file.drop(a[index_].index.values, axis=1, inplace=True) $ filtered_file.dropna(subset=a[~ index_].index.values, inplace=True) $ filtered_file = filtered_file.reset_index(drop=True)  # Removing data messes with the indexing
snow.drop_table("nk_als_ref")
f, ax = plt.subplots() $ ax.set_ylim(ymax=4); $ ax.set_xlabel('Shift duration [h]'); $ ax.set_ylabel('Count'); $ df['duration'].astype('timedelta64[h]').hist(bins=75, ax=ax);
two_day_sample['date'] = two_day_sample.timestamp.dt.date
print recordSeries[400] $ print '' $ print recordSeries[100]
path = os.getcwd() $ files = os.listdir(path) $ files.remove('.DS_Store') $ files.remove('.ipynb_checkpoints') $ files.remove('SimBand_Initial_Exploration.ipynb')
help(h5py)
nx.write_gexf(G2, '../datasets_graphs/graph_NEW.gexf')
autos.rename({"odometer": "odometer_km"}, axis=1, inplace=True)
so[no_answer].head()
df_tweet.describe()
norm.ppf(1-(0.05/2)) $
exiftool -csv -createdate -modifydate ciscih8/CISCIH8_cycle1.mp4 ciscih8/CISCIH8_cycle2.mp4 ciscih8/CISCIH8_cycle3.mp4 ciscih8/CISCIH8_cycle4.mp4 ciscih8/CISCIH8_cycle5.mp4 ciscih8/CISCIH8_cycle6.mp4 > ciscih8.csv
dictionary = {} $ for i in range(len(list_atlas_prob_stat)): $     dictionary[df_map[i,0]]=[mvoid_to_bson_id(a)   for a in df_map.A[i,1:] if mvoid_to_bson_id(a) != mvoid_to_bson_id('000000000000000000000000')]
cashflows_plan_origpd_noshift_all[(cashflows_plan_origpd_noshift_all.id_loan==574) & (cashflows_plan_origpd_noshift_all.fk_user_investor==31192)].to_clipboard()
train_df = train_df.drop(['AgeBand'], axis=1) $ combine = [train_df, test_df] $ train_df.head()
df2.head()
df = pd.read_feather(f'{PATH}df')
! cat ../outputs/data2.dat
exploration_airbnb = DataExploration(df_airbnb)
from statsmodels.api import Logit $ from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
session.query(measurement.station, func.count(measurement.tobs))\ $ .group_by(measurement.station)\ $ .order_by(func.count(measurement.tobs).desc()).all() $
X.dropna().shape
dfMonth = dfMonth.groupby(['Date', 'Project Name',   $                          'Estimated Start', 'Estimated End', 'Estimated Duration (Days)',  $                          'Sales Stage', 'Likelihood of Win', 'Likelihood Percent', 'Contract Value', $                          'JTBD', 'Big Bet', 'Client Type: Direct Impact', 'Client Type: Funder', 'New/Repeat Client', $                          'Contract Band Size'])[['Weighted Value', 'Contract Value (Daily)']].sum().reset_index()
df = pd.read_sql_query("select * from track limit 3 offset 5 ;", conn) $ df
df.state.unique()
image_df_clean = image_df.copy() $ image_df_clean.rename(columns={'p1': 'prediction_1', 'p1_conf': 'prediction_1_confidence', 'p1_dog': 'prediction_1_result'}, inplace=True) $ image_df_clean.rename(columns={'p2': 'prediction_2', 'p2_conf': 'prediction_2_confidence', 'p2_dog': 'prediction_2_result'}, inplace=True) $ image_df_clean.rename(columns={'p3': 'prediction_3', 'p3_conf': 'prediction_3_confidence', 'p3_dog': 'prediction_3_result'}, inplace=True)
result = pd.DataFrame(eclf2.predict_proba(test[features]), index=test.index, columns=eclf2.classes_) $ result.insert(0, 'ID', mid) $ result.to_csv("gaurav6.csv", index=False)
sales_agg = sales[sales['Store Number'].isin(good_stores)]     #iterate through stores in the good store list and pass them to dataframe $ print sales_agg.shape   $
csv = 'F:/web mining/webmining project/crawler/annotation_data/samsung/LR_clean_tweet.csv' $ my_df = pd.read_csv(csv,index_col=0) $ my_df.head()
listings_column_names = pd.DataFrame((listings.columns)) $ listings_column_names_list = list(listings.columns) $ object_first_observation = [listings[x].head(1) for x in listings_column_names_list] $ observations = pd.DataFrame(object_first_observation)
tfidf_vectorizer, tdidf_features = tfidf_extractor(sdf.Norm_reviewText) $ features = tdidf_features.todense() $ features_tfidf = np.round(features, 2)
type(data4)
new_page_converted = np.random.choice([0, 1], $                                       p=[1. - p_convert_treatment, p_convert_treatment], $                                       size=n_treatment, $                                       replace=True) $ new_page_converted.mean()
@udf(returnType=IntegerType()) $ def weekday(date): $     return datetime.strptime(date, DATETIME_PARSE_PATTERN).weekday()
bikedataframe[['count', 'predicted']].tail(30)
for df in (joined,joined_test): $   df['CompetitionOpenSince']=pd.to_datetime(dict(year=df['CompetitionOpenSinceYear'], $                                                 month=df['CompetitionOpenSinceMonth'], $                                                 day=15)) $   df['CompetitionDaysOpen']=df['Date'].subtract(df['CompetitionOpenSince']).dt.days
stories[['created_hour','created_dow']].head()
twitter_archive_master[twitter_archive_master.rating_denominator != 10].head()
pd.read_sql("SELECT * from related_content order by content_id desc limit 10", conn)
df_data.head()
tweet_json.iloc[0]
Guinea_deaths_Mean = pd.DataFrame({'August':August_deaths_Guinea, $                      'September':September_deaths_Guinea, $                      'October':October_deaths_Guinea }, index=['Guinea_deaths_Mean']) $ Guinea_deaths_Mean
df2[['CA', 'UK', 'US']] = pd.get_dummies(df2['country']) $ lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'CA', 'UK']]) $ result = lm.fit() $ result.summary()
date_news = valid_news.copy() $ date_news.dates = pd.to_datetime(date_news.dates) $ date_news.head()
map_estimate['betas_race_interval__'].reshape((-1,1)).T.dot(X[:,1:5].T).shape
gaming_products = pd.read_csv("../data/top-things/reddits/g/gaming.csv") $ gaming_products['subreddit'] = "r/gaming" $ movie_products = pd.read_csv("../data/top-things/reddits/m/movies.csv") $ movie_products['subreddit'] = "r/movies" $ pd.concat([gaming_products,movie_products])
inter1.head()
close
df.puppo.value_counts()
jobs.loc[jobs.FAIRSHARE == 180].groupby('ReqCPUS').JobID.count().sort_values(ascending= False)
treatment = df2.query('group=="treatment"') $ treatment_converted = treatment.query('converted==1') $ prob_treatment_converted = treatment_converted.shape[0]/treatment.shape[0] $ prob_treatment_converted
fit.summary()
_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled) $ accuracy_score(y_train, y_pred)
team_slugs_mini = team_slugs_df[['new_slug','nickname']] $ team_slugs_mini.set_index('nickname', inplace=True) $ team_slugs_dict = team_slugs_mini.to_dict()['new_slug']
pd.to_datetime(0)
init_notebook_mode(connected=True)
RF.score(X_test,Y_test)
station_distance.insert(loc=1, column='Trip Duration(Minutes)', value=tripduration_minutes)
df_countries.country.unique()
com_grp.agg({'Age':np.mean, 'Salary': [np.min,np.max]})
some_data = ac_tr.iloc[:5] $ some_labels = ac_tr_label.iloc[:5] $ some_data_prepared = full_pipeline.transform(some_data) $ print("predictions:\t", lin_reg.predict(some_data_prepared)) $ print("Labels:\t\t", list(some_labels))
backup = clean_rates.copy() $ sources = clean_rates.source.str.extract(r'>(?P<src>.*)<', expand=True) $ clean_rates.loc[:, 'source'] = sources.src.copy() $ clean_rates.source = clean_rates.source.astype('category')
lr=df2.copy() $ lr['intercept']=1 $ lr[['control','ab_page']]=pd.get_dummies(lr['group'])
movies
recipes = pd.read_json("https://s3.amazonaws.com/openrecipes/20170107-061401-recipeitems.json.gz", $                        compression='gzip', $                        lines=True)
len(df_test)
url = form_url(f'organizations/{org_id}/memberTypes', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
userModel = Sequential() $ userModel.add(Embedding(num_user+1, output_dim=vec_dim, input_length=1)) $ userModel.add(Reshape((vec_dim,)))
noaa_data = pd.read_csv(r'./CRNS0101-05-2018-NC_Durham_11_W.txt',delimiter='\s+',header=None,names=headers.values,parse_dates=[['LST_DATE','LST_TIME']])
aml.leader.save_mojo()
twitter_data.index=pd.to_datetime(twitter_data['created_at']) $ hourly = twitter_data.groupby(pd.Grouper(freq="H")).count()['_id'].to_frame() $ hourly.columns = ['Count'] $ hourly.plot(figsize=(16,6))
filtered_df.dropna(axis=0,subset=[['name','summary']],inplace=True) $ values = {'host_is_superhost':'f','host_has_profile_pic':'f','host_identity_verified':'f','host_listings_count':1,'bathrooms':1,'bedrooms':1,'beds':1} $ filtered_df.fillna(value = values, inplace=True) $ print (filtered_df.apply(num_missing, axis=0))
df_ml_692.tail(5)
df = pd.read_csv("../../data/msft.csv",header=0,names=['open','high','low','close','volume','adjclose']) $ df.head()
class myBirthDay(AbstractHolidayCalendar): $     rules = [ Holiday('Noushad Khan Birthday', month=8, day=20, observance = nearest_workday) ] $ myc2 = CustomBusinessDay(calendar = myBirthDay()) $ myc2
df = pd.concat([df1, df2], sort=True)
df.loc[df['offerType'] != 'Angebot','offerType'].values
tweet = api.get_status(id_list[0],tweet_mode='extended') $ tweet._json
import pandas as pd $ git_log['timestamp']=pd.to_datetime(git_log['timestamp'],unit='s') $ git_log.timestamp.describe() $
df.info()
df_h1b_nyc[map(lambda x: 'DELOITTE' in x[1].lca_case_employer_name, $                df_h1b_nyc.iterrows())].lca_case_employer_name.value_counts()
df2.shape
vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1, 2))  $ transformed = vectorizer.fit_transform(cleaned_texts.values) $ features = vectorizer.get_feature_names()
Logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','UK','US']]) $ results = Logit_mod.fit() $ print(results.summary()) $ print(np.exp(-0.0149),np.exp(0.0506),np.exp(0.0408))
tmp.created_at_in_seconds
!head twitter.csv
autos = autos[autos['price'].between(1, 400000)]
ax2 = sns.scatterplot(subs, comments, hue=senti_vader, ); $ ax2.set_ylabel('Comment Count'); $ ax2.set_xlabel('Subreddit Sub Count');
df = pd.DataFrame(one_year_prcp, columns=['station', 'date', 'prcp', 'tobs']) $ df.head()
sysdict = {'systype':systype, 'systemnum': sysnum, 'link':link, 'sysname':sysname, 'city':city, $            'county':county, 'syscat':syscat, 'indust code':indcode, 'number of sources':numberofsources, $           'huc':huc, 'pwsid':pwsid,'deqcat':deqcat, 'systemid':system_id} $ systems = pd.DataFrame(sysdict)
autos.info()
pd.value_counts(ac['IAM'].values, sort=True, ascending=False)
print('Get subset data of rows no. 11, 24, 37') $ df.loc[[11,24,37]] 
contest_data.where(F.col('end_customer_party_ssot_party_id_int_sav_party_id')== 7689590).take(1)#.select('endcustomerlinefixed','end_customer_party_ssot_party_id_int_sav_party_id','prior_party_ssot_party_id_int_sav_party_id','sales_acct_id','decision_date_time').take(10)
fig,ax1 = plt.subplots(1,1) $ ax1.plot(data[['Close','Bollinger High','Bollinger Low']]) $ y = ax1.get_ylim() $ plt.show(); $
committees.prop_name.value_counts()
mean_mileage_by_brand = {} $ for brand in brands: $     mean_mileage_by_brand[brand] = autos.loc[autos["brand"] == brand, 'odometer_km'].mean() $ mean_mileage_by_brand
cand_date_df['party'] = cand_date_df['sponsor_class'].map(party_dict) $ cand_date_df.head()
X = stock.iloc[915:-1].drop(['target', 'target_class', 'volatility', 'high', 'low', 'close', 'volume', 'open', 'news_sources', 'news_text', 'tesla_tweet', 'elon_tweet', 'daily_gain', 'daily_change'], 1) $ y = stock.iloc[915:-1].volatility
soup.li.parent
autos['brand'].value_counts(normalize = True)
coins_infund
df.tail()
jobs_data['clean_titles'] = jobs_data['record.title'].apply(lambda x: " ".join([stemmer.stem(i) for i in re.sub("[^a-zA-Z]", " ", str(x)).split() if i not in s_words]).lower())
train_col.train_model(num_epochs=2)
temp = pd.read_csv('tweets_i_master.csv') $ tweets_clean = temp.copy() $ tweets_clean.info() $
RatingSampledf=pd.DataFrame()
train_df['bathrooms'].ix[train_df['bathrooms']>=3.5] = 3.5
image_path1 = "pexels-photo-635529.jpeg" $ img1 = pil_image.open(image_path1) $ img1 = img1.convert('RGB')
data.retained.value_counts()
recipes.head()
df.loc[df['seller'] != 'privat'].shape
date_cols = ['CRE_DATE_GZL', 'SCHEDULED_START_DATE', 'SCHEDULED_END_DATE'] $ date_cols_history = ['ACTUAL_START_DATE', 'ACTUAL_END_DATE'] $ intervention_test = pd.read_csv(data_repo + 'intervention_test.csv', sep='|', encoding='latin-1', parse_dates=date_cols) $ intervention_history = pd.read_csv(data_repo + 'intervention_history.csv', sep='|', encoding='latin-1', parse_dates=date_cols+date_cols_history) $ intervention_train = pd.read_csv(data_repo + 'intervention_train.csv', sep='|', encoding='latin-1', parse_dates=date_cols)
print(np.info(np.dtype))
results.summary2() # since in my system results.summary() is giving an error i have used results.summary2()
y_test_pred = gs.predict(X_test) $ print('Root Mean Sequare Error on the Test Data:') $ print(np.sqrt(mean_squared_error(y_test_pred, y_test)))
df2_control = df2.query('group == "control"') $ df2_control['converted'].mean()
df.Day_of_week.value_counts().plot(kind='bar')
model = Sequential()
s5 + s6
with open('image_predictions.tsv', 'wb') as handle: $     for block in response.iter_content(1024): $         handle.write(block) $
print('Number of unique users in the dataset is {}.'.format(df2.user_id.nunique()))
del sess['Session']
(p_diffs>obs_diff).mean()
categorical_features = df.select_dtypes(include=[np.object]) $ categorical_features.columns $
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['closerToBotOrTop'] = np.where(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell']<=df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'], 'FromTopWell', 'FromBotWell')
cohorts['TotalUsers'].unstack(0).head(10)
stories[['created_dow', 'score']].corr()
import pickle $ data = [1, 1.2, 'a', 'b'] $ with open(outputs / 'data.pkl', 'wb') as f: $     pickle.dump(data, f)
crime_1k=pd.read_csv('data/crime/crimes_chi.csv', nrows=1000) $ crime_1k.head()
total_rows = df.shape[0] $ print("Number of rows are: {}".format(total_rows))
last_year
total_users = df2['user_id'].nunique() $ converted_users = df2[df2['converted'] == 1].count() $ conversion_prob = converted_users/total_users $ print(conversion_prob)
train_df[['user_id', 'repo_id', 'rating']].to_csv('data/new_subset_data/final_train_data.csv', index=False) $ test_df[['user_id', 'repo_id', 'rating']].to_csv('data/new_subset_data/final_test_data.csv', index=False)
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS $ custom_stop_words = list(ENGLISH_STOP_WORDS)
sale_prod_table = avg_sale_table.rename(columns = {'Sale Price':'SalePrice'}) $ sale_prod_table.head()
df_all_users['Email Address'] = df_all_users['Email Address'].apply(lambda x: str(x.lower().strip())) $ df_all_users['Email Address'] = df_all_users['Email Address'].astype(str)
len(train_small_sample), len(val_small_sample)
y_test_over[bet_over].mean()
msk = np.random.rand(len(df_raw_tweet)) < 0.9 $ df_train = df_raw_tweet[msk] $ df_test = df_raw_tweet[~msk] $ print('Training set size: ' + str(len(df_train))) $ print('Test set size: ' + str(len(df_test)))
list_of_genre_1990s_clean
lst= [abs(df.Close[i] - df.Close[i+1]) for i in range(0,len(df.Close)-1)] $ print("The largest change between any two days is %3.3f"%(max(lst)))
q = pd.read_sql_query(query, engine) $ q
df["used_promo"] = (df["promo_code_id"].notnull()).astype(int) $ df["same_location"] = (df["pickup_location_id"] == df["dropoff_location_id"]).astype(int)
df2.converted.sum()/df2.shape[0]
featured_image_url = soup.select_one("figure.lede img").get("src") $ print(featured_image_url)
for i in categories: $     df[i] = 0        
hs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284') $ shapefile = hs.content['wbdhuc12_17110006_WGS84.shp']
df_users_6_mvp=df_users_6[df_users_6['cohort']=='MVP User'] $ df_users_6_after_mvp=df_users_6_after[df_users_6_after['cohort']=='MVP User']
df.shape
import requests $ r = requests.get('https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html')
rain_result = session.query(Measurement.date, Measurement.prcp).\ $     filter(Measurement.date > year_ago).\ $     order_by(Measurement.date).all() $ rain_result
y = pd.Series(data.target) $ y
index[1]='a'
df2['intercept']=1 $ df2[['control','treatment',]]=pd.get_dummies(df2['group']) $
import datetime $ day = lambda x: x.split(' ')[0].replace('-',',') $ data_2012['date']=data_2012['createdAt'].apply(day)
station_obs_df = pd.DataFrame(sq.station_obs(), columns = ["Station name", "Observation counts"]) $ station_obs_df
sns.distplot(users.age,kde=False) $ plt.xlabel('Age') $ plt.ylabel('Population Size')
tweets_master_df.iloc[tweets_master_df['retweet_count'].nlargest(10).index, :]
raw.fillna(0, inplace=True)
display(heading('Bad:'), $         set(sheets_with_bad_column_names.keys()))
test_results_final = cvModel2.bestModel.transform(test_data)
pn = 'B012828' $ table_store = data_from_store[data_from_store['Part Number'] == pn] $ table_1c = data_from_1c[data_from_1c['PO no.'] == pn] $ pn_qty[pn]['storeinfo'] = [] $ pn_qty[pn]['1cinfo'] = []
df['intercept']=1 $ df['ab_page'] = pd.get_dummies(df['group']) ['treatment'] $
for col in ["power", "toughness", "loyalty"]: $     all_cards.loc[:, col] = all_cards.loc[:, col].infer_objects()
details.dtypes
exp_rmspe(x,y)
question_2_dataframe = question_2_dataframe.merge(population_by_zip, how='left', on=['incident_zip']) $ question_2_dataframe.head(5)
models = autos["model_brand"].value_counts(normalize = True) $ represented_models = models[models > .02].index $ print(represented_models)
tips.sort_values(["sex", "day"]).set_index(["sex", "day"]).head(12) $
twitter_archive_enhanced[ $     (twitter_archive_enhanced['doggo'] == 'None') & $     (twitter_archive_enhanced['floofer'] == 'None') & $     (twitter_archive_enhanced['pupper'] == 'None') & $     (twitter_archive_enhanced['puppo'] == 'None')].text.head()
users = pd.read_csv('LaManada_new/tbluserinfo.csv',sep=SEP) $ users.shape
mydata.tail()
rdb.drivers()
old_page_converted = np.random.binomial(n, p_old, n_old) $ old_page_converted
sns.factorplot('pclass', data=titanic3, hue='sex', kind='count')
with open("sensor_data.csv","a") as f: $     df.to_csv(f,header=True,index=False)  
gmm = GaussianMixture(2).fit(X) $ labels = gmm.predict(X)
path = '../data/yard_move_saint_nazaire.csv' $ df1 = pd.read_csv(path, parse_dates=['time_in', 'time_move', 'time_created']) $ print(df1.shape, df1['time_move'].dtype) $ df1.head()
print result.summary()
train.corr()
fb['popular'] = fb['share_count'] > 200
dblight["ViolationStreetName"] = dblight["ViolationStreetName"].apply( $      lambda x: re.sub("( ST\.?| DR\.?| RD\.?| BLVD\.?)","",x).strip() $ ) $ blight_gb = dblight.groupby(["ViolationStreetNumber","ViolationStreetName"],as_index=False).sum() $ print "Before aggregation", len(dblight),"rows. After aggregation",len(blight_gb),"rows"
print("the day of the month is: ", today.day) $ print("we are curretly in month number", today.month) $ print("The year is", today.year)
pd.read_csv("Data/microbiome_missing.csv").head(20)
random_integers
from pyspark.ml.classification import LogisticRegression $ lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8) $
filename = 'subway_data.txt' $ subway2_df = pd.read_csv(filename)
df_vimeo_selected=df_vimeo[df_vimeo['course_name']!='TBD']
y_pred = lassoreg.predict(X_test) $ print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
a[a.find(':') + 1:].lstrip()
df = pd.read_excel(excel_file_Path, sheet_name = 'UP') $ df.head(100)
print(df['approx_payout_date'].min(), $ df['approx_payout_date'].max())
df_drug_counts.dropna(axis = 1, thresh = 20).plot(kind = 'bar', $                                                  figsize = (10,6))
df_clean.head()
title_list_lower = list(full_data.titlelower) $ title_list = list(full_data.title)
rshelp.query("SELECT COUNT(*) FROM postgres_public.parking_spot WHERE ev_charging IS NOT NULL;")
!./flow --imgdir sample_img/ --model cfg/tiny-yolo.cfg --load bin/tiny-yolo-voc.weights --json --gpu 0.85
twitter_df_clean['retweet_count'] = pd.to_numeric(twitter_df_clean.retweet_count) $ twitter_df_clean['favorite_count'] = pd.to_numeric(twitter_df_clean.favorite_count)
df1=data $ del df1['day'] $ df1.head(5) $
price2017 = price2017.rename(columns={'DE-AT-LUX':'Price_Germany'})
Columns = StockData.columns.values $ Columns[-4:] = ['Date-Q{}'.format(N) for N in range(1,5)] $ StockData.columns = Columns $ print(StockData.columns)
pickle.dump(tfidf_fitted, open('iteration1_files/epoch3/tfidf_fitted', 'wb'))
dates.to_period('D')
len(unique_titles)
train.columns
test_features = users[["timestamp_first_active", "signup_flow", "age"]].values
test_kyo2 = tweets_kyoto_filter[tweets_kyoto_filter['ex_lat']<35.1] $ test_kyo2 = test_kyo2[test_kyo2['ex_long']<135.725300] $ test_kyo2 = test_kyo2[test_kyo2['ex_lat']>34.05] $ test_kyo2 = test_kyo2[test_kyo2['ex_long']>135.705300]
dfgts.shape
ad_created_count_norm.describe()
txt_tweets = tweets['text']
df.info()
points2.isnull()
precipitation_df.describe() $
x = success_order_with_cust['Age'] $ y = success_order_with_cust['TOTAL_PRICE']
URL = "http://www.reddit.com/hot.json" $ res = requests.get(URL, headers = {'User-agent':'Caitlin Bot 0.1'})
options_data = h5['vstoxx_options']
autos.head()
archive_copy.floofer.unique()
g = logs.groupby(logs.fm_ip) $ type(g) $
df.loc[n]['skills']
my_data = pd.read_csv("drug200.csv", delimiter=",") $ my_data[0:5]
df2_vif = pd.DataFrame() $ df2_vif["VIF Factor"] = [vif(df_teamX_scaled.values, i) for i in range(df_teamX_scaled.shape[1])] $ df2_vif["features"] = teamX.columns $ df2_vif
test_portfolio['weight'] = 0.5 $ test_portfolio['capital'] = 10000 $ test_portfolio['share'] = test_portfolio['weight'] * test_portfolio['capital'] / test_portfolio['price'] $ test_portfolio['net'] = test_portfolio['price'] * test_portfolio['share']
tokens
train['answer_dt'].describe()
sentlex_analysis(df['text'][432673])
payment_plans.sort(['fk_loan','interval'],inplace=True) #prob unnecc? but need sorted for rebase
import pods $ from ipywidgets import IntSlider
grouped_merged = merged.groupby(['contributor_firstname','contributor_lastname','committee_position'])['amount'].sum().reset_index().sort_values('amount', ascending = False)
flight_phase = faa_data_pandas['PHASE_OF_FLT'].value_counts() $ print(flight_phase)
cc.info()
number_unpaids = data.groupby('customer_id').agg({'score': max, 'paid_status': lambda x: np.sum(x == 'UNPAID')})
print(forecast_set, confidence, forecast_out)
train_ratio = 0.75 $ train_size = int(samp_size * train_ratio); train_size $ val_idx = list(range(train_size, len(df)))
print('The number of rows in the dataset:', df.shape[0], 'rows')
df.set_index(['Date', 'Store', 'Category', 'Subcategory', 'Description'], inplace=True) $ df.head(3)
to_be_predicted_Day1 = 14.52 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
for element in y: $     print element['id'] $     print(element['full_text']) $     print('--')
df_c2 = df_c.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_c2.head()
np.std(null_vals),np.std(p_diffs)
new_page = df2.query('landing_page=="new_page"') $ prob_new_page = new_page.shape[0]/df2.shape[0] $ prob_new_page
df.isnull().sum().sum()
import sys $ sys.version
df_c_merge = countries_df.set_index('user_id').join(df_regression.set_index('user_id'), how='inner') $ df_c_merge.head()
grouped = df2.groupby('group') $ grouped.describe()
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=17000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
kl["catfathername"].unique()
for i in range(0,100,5): $     print(pros[i])
k = df2[df2.user_id.duplicated()]; k
tw_clean.timestamp.head()
autos["odometer"]= autos["odometer"].replace("[,km]",'',regex=True) $ autos["odometer"]
df = pd.DataFrame(np.random.randn(8, 4), columns = ['A', 'B', 'C', 'D']) $ df
lr_final = LogisticRegression(penalty='l1', C=0.001) $ accuracies = cross_val_score(lr_final, scl.fit_transform(X_train), y_train, cv=cv_object) $ print np.average(accuracies)
df2_countries = countries.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df2_countries.head()
df3['timestamp'].max()-df3['timestamp'].min() $
store.delete_collection('NASDAQ.EOD')
t_ave=(weather.TMAX+weather.TMIN)/2 $ weather['TAVG']=t_ave $ weather.info()
pred_probas_over_fm = gs_from_model.predict_proba(X_test) $ fm_bet_over = [x[1] > .62 for x in pred_probas_over_fm]
RN_PA_duration.head()
for col in ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']: $     df_goog[col].plot() $     plt.show()
timelog = timelog.drop(['Email', 'User', 'Amount ()', 'Client', 'Billable'], axis=1)
x = x.assign(A2 = x["A"]**2) $ x
predicted_probs_first_measure = holdout_results[holdout_results.second_measurement==0].\ $ groupby('wpdx_id').cv_probabilities.sum() $ predicted_probs_first_measure.head()
srctake.to_sql(con=engine, name='sourceuse', if_exists='replace', flavor='mysql',chunksize=10000)
df_questionable[df_questionable['political'] == 1]['link.domain_resolved'].value_counts(25).head(25)
predictions_train.show()
session.query(Measurements.date).order_by(Measurements.date.desc()).first()
m.__repr__() $
df_symbols = nasdaq.get_ipo_list(start_date) $ print('symbols', df_symbols.shape[0])
sns.factorplot('lgID',hue='divID',kind='count',data=wcPerf1_df)
conn.close()
price2017.head()
new_page_converted = np.random.choice([0,1], size=n_new, p=[p_new, 1-p_new])
apple.resample('BM').mean().head()
team_attributes._get_numeric_data().columns.tolist()
conv_control_prob = len((df2[(df2['group'] == 'control') & (df2['converted'] == 1)])) / (df2['group'] == 'control').sum() $ print(conv_control_prob)
transactions[~transactions['UserID'].isin(users['UserID'])]
positive = '/Users/EddieArenas/desktop/Capstone/positive-words.txt' $ positive = pd.read_table('/Users/EddieArenas/desktop/Capstone/positive-words.txt')
df.groupby('key').sum()
autos
df.info()
df_clean.columns
run txt2pdf.py -o '2018-06-22  2014 872 discharges.pdf'  '2018-06-22  2014 872 discharges.txt'
plt.hist(p_diffs); $ plt.axvline(actual_diff, c='red');
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ print(act_diff)
manager.syncs_df
result = pd.concat([df1, df3], axis = 0) # concatenate one dataframe on another along rows $ result
google_stock.tail()
foursquare_data_dict.keys()
run txt2pdf.py -o "CEDARS-SINAI MEDICAL CENTER  Sepsis.pdf"   "CEDARS-SINAI MEDICAL CENTER  Sepsis.txt"
s1 = pd.Series([True, False, False, True]) $ s1
filtered_df = full_df[['id','name','summary','space','access','interaction','house_rules','host_since','host_response_time','host_is_superhost','host_listings_count','host_verifications','host_has_profile_pic','host_identity_verified','neighbourhood','zipcode','latitude','longitude','property_type','room_type','accommodates','bathrooms','bedrooms','beds','bed_type','amenities','price','security_deposit','cleaning_fee','extra_people','minimum_nights','availability_30','availability_60','availability_90','availability_365','number_of_reviews','review_scores_rating','review_scores_accuracy','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_location','review_scores_value','instant_bookable','cancellation_policy']] $ filtered_df.to_csv('filtered_data.csv') $ filtered_df.dtypes
full_globe_temp = pd.read_table(filename, sep="\s+", names=["year", "mean temp"]) $ full_globe_temp
column = inspector.get_columns('station') $ for c in column: $     print(c['name'], c["type"])
test_users
df2.loc[df2['user_id'] == 773192] #Verify that drop happened
import pickle $ output = open('speeches_cleaned_2016.pkl', 'wb') $ pickle.dump(speeches_df4, output) $ output.close()
html = browser.html $ twitter_news = bs(html, 'html.parser')
brand_pminfo
frame3
df_new.country.unique()
options_frame['BidAskSpread'] = options_frame['Ask'] - options_frame['Bid'] $ errors_20_largest_by_spread = options_frame.ix[sorted_errors_idx.index] $ errors_20_largest_by_spread[['BidAskSpread', 'ModelError']].sort_values(by='BidAskSpread').plot(kind='bar', x='BidAskSpread')
print finalData.shape $ print X.shape $ print dat.shape $ print Stockholm_data.shape
R_trip.head()
sns.boxplot(x=tmdb_movies_production_countries_revenue['production_countries'], y=tmdb_movies_production_countries_revenue['revenue'], showmeans=True) $ plt.show()
df2_new_page = len(df2.query("landing_page == 'new_page'")) / df2.shape[0] $ print('The probability that an individual received the new page is: {}.'.format(round(df2_new_page, 4)))
df0 = df[df['message_likes_dummy']==0].sample(n=3000).reset_index(drop=True) $ df1 = df[df['message_likes_dummy']==1].sample(n=3000).reset_index(drop=True)#gets shuffled sample of each subset $ df = pd.concat([df0, df1])
results = session.query(func.max(Station.latitude)).all() $ for result in results: $     print(result)
[tweet for tweet in df_clusters[df_clusters.cluster_cat==15].text[:10]]
imax = amount.idxmax()
rent_db3.boxplot(column='price', by='bedrooms')
v_invoice_sat.columns[~v_invoice_sat.columns.isin(invoice_sat.columns)]
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ lobbyFrame = pd.DataFrame(newl, columns=('Start', 'End', 'Lobbyist', 'Client', 'Industry', 'Cluster', 'Address', 'Confidence', 'IndustryCat1','IndustryCat2')) $ lobbyFrame.describe()
consumer_key = 'OnnL3a3Rl1L64G3mNvnn6w6YR' $ consumer_secret = 'M0p49XPYYjo48DWb8aTmTYl9M7gJ0Fe7D2KRQM5wZPqpa6wJaX' $ access_token = '946781751589957638-S6dJ0Z7bZdyYViHR1MPSW0Cw0RlIXJG' $ access_token_secret = 'Q1TmGZanEuHl9zPifruVObUkwqV9NCQFrwy9XIdNvo4ze'
categories_feat = df.groupby(['business_id']).categories.apply(np.unique) $ categories_feat.head()
data_df['tone'] = data_df.apply(tone, axis=1)
df_members.head()
features=list(kick_projects_ip) $ features.remove('state') $ response= ['state']
missing_conditions = conditions_m.isnull() $ missing_conditions
annual_returns.plot(figsize = (15, 6)) $ plt.show() $
ds2.plot('x', 'y', kind='scatter')
tok_trn = np.load(LM_PATH / 'tmp' / 'tok_trn.npy') $ tok_val = np.load(LM_PATH / 'tmp' / 'tok_val.npy')
from utils_for_demo_of_on_the_fly_selector import Helper
e=data.iloc[2]
print(norm.cdf(z_score))
startups_USA.to_csv('data/startups_pre_processed.csv')
large_df.tail(n=2)
dummies = pd.get_dummies(df_new.country) $ df_new = df_new.join(dummies) $ df_new.head()
v_to_c['time'] = v_to_c.checkout_time - v_to_c.visit_time
cell_df.dtypes
df[df['Agency']=='NYPD']['Complaint Type'].value_counts()
pd.DataFrame(data)
dataframe = pd.DataFrame(np.random.randn(10,5)) $ dataframe
df2['intercept'] = 1 $ df2[['control', 'ab_page']] = pd.get_dummies(df2.group)
tmdb_movies['day_of_week'] = tmdb_movies['release_date'].dt.weekday_name
df_joined[sorted(list(df_joined['country'].unique()))] = pd.get_dummies(df_joined['country']) $ df_joined.head()
y.shape
cgm = data[data["type"] == "cbg"].copy() $ cgm.head()
test_data.head()
col_arr=df["WHO Region"].values $ col_arr
print('The probability of an individual converting regardless of the page they received is {}'.format(df2.converted.mean()))
df2.iloc[4,3]=np.nan
comment_column = feedback.iloc[:, 0] $ comment_column = [x.replace("'", "") for x in comment_column] $ comment_column = [x.replace("-", "") for x in comment_column]
new_prob_mean = new_page_converted.mean() $ old_prob_mean = old_page_converted.mean() $ print(new_prob_mean) $ print(old_prob_mean) $ print(new_prob_mean - old_prob_mean)
thecmd = 'curl -v -F file=@'+curDir+'/'+dataDir+'processing/new-york_new-york_points.csv "https://'+USERNAME+'.carto.com/api/v1/imports/?api_key="'+APIKEY $ os.system(thecmd) #run the command to curl the input file, this should work as its not GDAL/OGR
df=pd.DataFrame() $ for year in range(2010,2018): $     df_year=pd.read_csv('../311NYC/WeatherUnderground/output/'+str(year)+'.csv') $     df=df.append(df_year) $ df['Date']=df['Date'].astype('datetime64[ns]') $
talks.text[0]
kickstarter.info()
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US', 'ab_page']]) $ results = lm.fit() $ results.summary()
start = datetime.now() $ modelgb = GaussianNB() $ modelgb.fit(Xtr.toarray(), ytr) $ print(modelgb.score(Xte.toarray(), yte)) $ print((datetime.now() - start).seconds)
view = events.pivot_table(index=["distance", "swimstyle", "category"], columns='course', values='time').reset_index() $ view.sample(5)
high_hba1c.limit(10).toPandas()
airbnb_df.info(memory_usage='deep')
df2 = df2.drop_duplicates('user_id'); $ df2.shape[0]
filtered_flatten_plot_df = flatten_plot_df[flatten_plot_df['tag'].isin(flatten_plot_df.groupby(['tag']).size().nlargest(10).index)]
season_team_groups.aggregate(np.mean).sort_values(by = "Tm.3PM", ascending = False).head(5)
vals = data.value $ vals $ vals[5] = 0 $ vals
import matplotlib.pyplot as plt
tesla_days.plot(y='Close') $ plt.show()
df_tweet_json_clean = df_tweet_json.copy()
df = pd.read_csv("https://raw.githubusercontent.com/YingZhang1028/practicum/master/Data_DataMining/test_score.csv") $ test_df["description_score"] = df["description_score"] $ test_df["description_score"].ix[np.isnan(test_df["description_score"]) == True] = test_df["description_score"].mean()
df.Date
to_be_predicted_Day1 = 17.37 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
np.dot(returns, weights)
old_page_converted  = np.random.choice([1, 0], size=n_old, p=[convert_p_old, (1-convert_p_old)])
print(df)
treatment_group = df2.query('group == "treatment"') $ converted_treatment = treatment_group[treatment_group.converted == 1] $ p_treatment = converted_treatment.count()/treatment_group.count() $ p_treatment
twitter_Archive['timestamp'] = pd.to_datetime(twitter_Archive['timestamp']) $ type(twitter_Archive['timestamp'].iloc[0])
import datetime $ import pandas_datareader.data as web
with open('./data/processed/y_train.pkl', 'rb') as picklefile: $     y_train = pickle.load(picklefile)
tt.sum()*1./len(test_df)
stock = stock.fillna(0)
df2[df2.user_id.duplicated()]
%%time $ model = GaussianNB() $ model.fit(train_x, train_y)
df_reviews.count()
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s')
df[['B','A']] #Note that the column identifiers are in a list
active_distinct_authors_latest_commit = authors_grouped_by_id_saved.filter( $     (F.date_sub(F.current_date(), 365)) < authors_grouped_by_id_saved.latest_commit)
%env CUDA_VISIBLE_DEVICES='' $ import tensorflow as tf $ import numpy as np $ import pandas as pd
print len(cat_pizza) , len(cat_sandwich) , len(cat_american)
opt_fn = partial(optim.Adam, betas=(0.7, 0.99))
df = pd.read_csv('loan_train.csv') $ df.head()
sl[sl.index.isin(sl_train.index)].shape
print bnb2[bnb2['age']>80].shape $ print bnb[bnb['age']>80].shape $ print bnb['age'].shape $ 2771/213451
df_amznnews_clsfd_2tick = df_amznnews_clsfd_2tick.set_index('publish_time') $ df_amznnews_clsfd_2tick.info()
from IPython.display import HTML $
df_joined[['CA', 'UK', 'US']] = pd.get_dummies(df_joined.country) $ df_joined = df_joined.drop(['CA'], axis=1) $ df_joined.head()
print len(hpdcom) $ print len(hpdpro) $ print len(hpdvio)
df_vow['Closed_Higher'] = df_vow.Open > df_vow.Close $ df_vow['Closed_Higher'] = pd.get_dummies(df_vow.Open > df_vow.Close).values
logit_country = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'country_US', 'country_UK']]) $ results3 = logit_country.fit()
store = pd.HDFStore('store.h5') $ df = store['df'] $ df
df.head(1)
oppose.sort_values("amount", ascending=False).head(10)
le_data_all = wb.download(indicator="SP.DYN.LE00.IN", $                          start='1980', $                          end='2014') $ le_data_all
utility_patents_df.number_of_claims.describe()
x1 = poly1.fit_transform(x)
foursquare_data_dict['response'].items()[2][1][0]
df2[df2['group'] == 'treatment'].converted.mean()
from functools import reduce $ dfs = [df_CLEAN1A, df_CLEAN1B, df_CLEAN1C] # lift of the dataframes $ data = reduce(lambda left,right: pd.merge(left,right,on='MATCHKEY', how='inner'), dfs) $
transformed_six_month.count()
class_data['Census Tract'].value_counts()
dff2 = pd.melt(dff,["index"], var_name="team", value_name="gts") $ dff2 = dff2.rename(columns={'index':'date'})
df2['monthOfRegistration'].value_counts().plot(kind='bar')
prob_kNN100 = kNN100.predict_proba(Test) $ prob_kNN100
table1.head(10)
df2 = df2.set_index('user_id') $ df2.head()
df.tail(1).index
news_df.head()                                                                # displays dataframe
tt1.head()
df = df[[u'id', u'season', u'episode_id', u'number', u'raw_text', u'timestamp_in_ms', $        u'speaking_line', u'character_id', u'location_id', $        u'raw_character_text', u'raw_location_text', u'spoken_words', $        u'normalized_text', u'word_count']]
df.head()
words_mention_sk = [term for term in words_sk if term.startswith('@')] $ corpus_tweets_streamed_keyword.append(('mentions', len(words_mention_sk))) # update corpus comparison $ print('List and total number of mentions: ', len(set(words_mention_sk))) #, set(terms_mention_stream))
contour_sakhalin = np.array(sakhalin_shp.shapes()[0].points)  #convert contour points to numpy array
data1 = pd.read_csv("os_combined-ww-monthly-201610-201710.csv") $ print("row count in OS csv file is: " + str(data1.shape[0] - 1)) $ data1 = pd.read_csv("browser-ww-monthly-201610-201710.csv") $ print("row count in Browser csv file is:" + str(data1.shape[0] - 1)) $
stocks = pd.concat([AAPL, GOOGL], keys=['Apple', 'Google']) $ stocks
from plotly import __version__ $ from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot $ import plotly.graph_objs as go $ print(__version__) # requires version >= 1.9.0 $ init_notebook_mode(connected=True)
STD_reorder_stats = calculate_reorder_stats('STD',(len(STD_customer_order_intervals),7))
scr_activated_df.to_csv('activation_paid_518.csv')
time1 = datetime.strptime(dfnychead["Created Date"][1],'%M/%d/%Y %H:%m:%S AM') $ print "time1 =", time1 $ print "time1.year =", time1.year
cutoff_times = generate_labels('/7/KMLZlMBnmWtb9NNkm3bYMQHWrt0C1BChb62EiQLM=',  trans, $                                label_type = 'SMS', churn_period = 14) $ cutoff_times[cutoff_times['churn'] == 1].head()
mb = pd.read_csv("../data/microbiome/microbiome.csv", index_col=['Taxon','Patient']) $ mb.head()
matthew['SOCIAL_NETWORKS'] = matthew.text.apply(lambda text: pd.Series([x in text for x in SOCIAL_NETWORKS]).any()) $ matthew['DECISION_MAKING']  = matthew.text.apply(lambda text: pd.Series([x in text for x in DECISION_MAKING]).any()) $ matthew['ADAPTIVE_CAPACITY']  = matthew.text.apply(lambda text: pd.Series([x in text for x in ADAPTIVE_CAPACITY]).any())
df2.drop(index=2893,axis=0,inplace=True)
reviews=pd.read_table("ign.csv", sep=',') $ reviews.head()
df_twitter_archive.describe()
gs_rfc_under.score(X_train, y_train_under)
ftp.login("anonymous", "anonymous")
df.isnull().any() #there is no missing values
cm = confusion_matrix(y_true=y_train, y_pred=y_t)
repos_users[repos_users['user'] == 'donnemartin']
print('Predicted\t', 'Actual') $ print('MPG    ACC\t', 'MPG   ACC') $ print(30*'-') $ for i, j in zip(pred_m[:5], dep_test_m.as_matrix()[:5]): $     print(f'{i[0]:5.2f} {i[1]:5.2f}\t{j[0]:5.2f} {j[1]:5.2f}')
y_pred = lgb_model.predict(val)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head(2)
czn
import statsmodels.api as sm $ convert_old = len(df2[(df2['landing_page'] == 'old_page') & (df2['converted'] == 1)]) $ convert_new = len(df2[(df2['landing_page'] == 'new_page') & (df2['converted'] == 1)]) $ n_old = len(df2[df2['landing_page'] == 'old_page']) $ n_new = len(df2[df2['landing_page'] == 'new_page'])
cols = ['GP', 'GS', 'MINS', 'G', 'A', 'SHTS', 'SOG', 'GWG', 'HmG', 'RdG', \ $         'G/90min', 'SC%', 'Year', 'PKG', 'PKA'] $ goals_df[cols] = goals_df[cols].apply(pd.to_numeric)
df2.nunique().user_id
intersections_irr['date_time_hour'] = [datetime.datetime.strptime( dateStr,'%Y-%m-%d %H:%M:%S').time().hour for dateStr in intersections_irr['updateTimeStamp'] ]
df_html_extract = pd.read_csv('data/df_html_extract.csv') $ df_html_extract_copy = df_html_extract.copy() $ df_html_extract_copy.head()
p__new = df2['converted'].mean() $ p__new
s.asfreq('Q').head()
typesub2017 = typesub2017.drop(['MTU','MTU2'],axis=1)
pca=decomposition.PCA() $ stocks_pca_t2= pca.fit_transform(stocks_pca_m2)
all_data = pd.concat(datasets).reset_index(drop=True) $ all_data.head()
discounts_table.groupby(['Discount Band'], sort=False)['Discount %'].min()
print('len(abc.index) is the number of total DRGs when considering ALL years',len(abc.index))
print(knn5.score(X_train, y_train), knn5.score(X_test, y_test))
pd.DataFrame(np.random.rand(3, 2), $             columns=['foo', 'bar'], $             index=['a', 'b', 'c'])
units_purchased  = df['units_purchased'] $ total_spend = df['total_spend'] $ pearsonr(units_purchased,total_spend)
dog_stages = Counter(df_clean.dog_stage) $ dog_stages
ratio = twitter_archive_master['favorite_count'] / twitter_archive_master['retweet_count']
n_new = treatment.shape[0] $ print(n_new)
plot_feature_importances(RF)
df1.columns = df2.columns $ df=pd.concat([df1,df2]) $ df.head()
us.loc[us['country'].isna(), 'country'] = us.loc[us['country'].isna(), 'cityOrState']
aTL.shape, eTL.shape
!wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.119 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-GB,en-US;q=0.9,en;q=0.8" "https://storage.googleapis.com/kaggle-competitions-data/kaggle/8076/test.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1519109659&Signature=CKW8rJJNahupAjFgWS%2FVIlGs2tj0CugO9%2FNrFpMz953KGQCxweyHuwLWm2Fl%2FxQEsyIJJFQYvvfl94gk9NAr4OnMen2UVebjZ98%2BIeGSljiwYdTiUGyH9QPk9CwMKlrBouuTVKmZELo1eQX4FmiD7a1Iy1oI0jbJKWZqrCqq2%2BrkMzmtAT91CyGUUxhhrIW47mE8Ff%2Bx4%2BIta%2Fcpi3HFJ6a0bZAR6jlJynJ4MBxtUjmOPTdAoJ6VNmdo0tgnn5xO%2Bkn5e1H6vk6M0QY0qA6j1uZJOr3gjKxBZroWDatDR8liiEuGdGSCkxTJ2X3HvpHe2D7A7%2Bq%2B21SeigLc4Hma9A%3D%3D" -O "test.csv.zip" -c
import pandas as pd $ import re
p_new = (rows_converted/total_rows_in_treatment) $ p_old = (rows_converted/total_rows_in_control) $ print(p_old,p_new)  
sns.distplot(dataset['FollowerCount'], bins=10) $ plt.title('Distribution Plot')
df_ad_airings_filter_3.head(2)
tlen_a.plot(figsize=(16,4), color='r'); $ tlen_b.plot(figsize=(16,4), color='b');
np.exp(-0.0150)  #ab_page coefficient = -0.0150
SCOPES = ['https://www.googleapis.com/auth/analytics.readonly'] $ KEY_FILE_LOCATION = './Gerdau-83372fd33b23.json' $ ACCOUNT_ID = '35160809' $ WEB_PROPERTY_ID = 'UA-35160809-5' $ VIEW_ID = '112966131'
with open('unicode_file.txt', 'r', encoding='utf-16le') as f: $     print(f.read())
display_topics(nmf3, features3, no_top_words)
mb.index
notus.loc[notus['country'].isin(canada)]
import matplotlib.pyplot as plt $ %matplotlib inline $ plt.hist(data['comments'], color='orange');
price = regression_model.predict([[14500, 76.02, 23.1]])[0] $ print("We would expect to pay ${} for a Big Mac in Macedonia".format(round(price, 2)))
df.index
to_be_predicted_Day3 = 26.68060428 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
chinapostnumber = len(df[df.province<99]) $ chinapostnumber
locationsDF.head()
np.dtype({'names':('name', 'age', 'weight'), $           'formats':('U10', 'i4', 'f8')})
tweet_json.nunique()
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $ print(len(new_page_converted)) 
one_station['week_num']=week_num $ one_station.head(20)
msftAR = msftA.reset_index() $ msftVR = msft[['Volume']].reset_index() $ msftAR[:3]
graph.number_of_nodes()
goog
df_h1b_ft_US = df_h1b[map(lambda x: x[1].lca_case_workloc1_state in states $                           and x[1].full_time_pos=='Y' $                           and (x[1].status=='CERTIFIED' or x[1].status=='DENIED'), $                           df_h1b.iterrows())] $ df_h1b_ft_US = df_h1b_ft_US.drop([33052, 44675, 45556, 48967, 51744, 53898, 135911, 188139, 225285, 332969, 366595])
latest_version = str(d.id[0]) $ print latest_version
train = train.drop(['Unnamed: 0'], axis=1)
read_in = pd.read_pickle(processing_test.data()) $ read_in
list_val = ['request'] $ x = ((pd.to_datetime(pd.DataFrame(file_lines)['TimeStamp']) + pd.DateOffset(hours=-32)) - pd.to_datetime(pd.DataFrame(file_lines)['TimeStamp']))
clf.fit(random_features,random_labels)
users = pd.read_csv('data/new_subset_data/new_subset_users.csv', sep='\t') $ users.info() $ users.head()
result_1.summary()
df.loc[:,'A']
df.set_index('Opened')['CaseID'].resample('A').count()
df4['page_CA'] = df4['ab_page'] * df4['country_CA'] $ df4['page_UK'] = df4['ab_page'] * df4['country_UK']
con = sqlite3.connect('db.sqlite') $ con.execute("DROP TABLE tbl") $ con.commit() $ con.close()
df2.drop(df2[df2.duplicated('user_id')==True].index,inplace=True) $ df2.shape
np.log(0.00025/0.0028)
train_labels = [reuters.categories(doc_id)for doc_id in doc_id_list if 'training' in doc_id]
price_dict = price_data.to_dict()
from sklearn.feature_extraction.text import CountVectorizer
data_for_model.info()
len(topUserItemDocs['user_id'].unique()) $
for obj in bucket_obj.objects.all(): $     print('Object key: {}'.format(obj.key)) $     print('Object size (kb): {}'.format(obj.size/1024))
df.groupby('funding_rounds').size()
pres_date_df = pres_df.copy() $ pres_date_df.head(2)
columns = inspector.get_columns('station') $ for c in columns: $     print(c['name'], c["type"])
Plot_Boxplot(wildfires_df)
plt.hist(p_diffs) $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('Plot of 10K simulated p_diffs');
tls.set_credentials_file(username='ddeloss', api_key='0uwykwidtt')
rnd_reg_2.oob_score_
first_commit_timestamp = pd.to_datetime('2005-04-16 22:20:36') $ last_commit_timestamp = pd.to_datetime('2017-10-03 12:57:00') $ corrected_log = git_log[(git_log.timestamp >= first_commit_timestamp) & $                         (git_log.timestamp <= last_commit_timestamp)] $ corrected_log['timestamp'].describe()
earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5)
pop_dog = twitter_archive_master.groupby('p1')['retweet'].mean().reset_index() $ pop_dog.sort_values('retweet', ascending=False).head()
df_big.info()
difference = (actual_diff < p_diffs).mean() $ perc = difference * 100 $ print("{}% are greater than the actual difference.".format(perc))
plt.rcParams['axes.unicode_minus'] = False $ dta_54.plot(figsize=(15,5)) $ plt.show()
df_all['datetime_utc'] = pd.to_datetime(df_all['created_utc'], unit='s') $ df_all['datetime_utc'] = pd.DatetimeIndex(df_all['datetime_utc']) $ print(df_all['datetime_utc'][0:5])
oppose = merged[merged.committee_position == "OPPOSE"]
type(df_elect)
profit_calculator(stock.iloc[1640:], 'predict_grow', -1)
plt.clf() $ plt.plot(res.seasonal[:7],color = 'black') $ plt.show()
import quandl $ quandl.ApiConfig.api_key = 'RGYoyz3FAs5xbhtGVAcc'
ticket4 = data_df.clean_desc[26] $ parsed4 = nlp(ticket4) $ for ent in parsed4.ents: $     print 'Entity: {}'.format(ent.text) $     print '\t Entity label: {}'.format(ent.label_) $
df2["converted"].mean() * 100
archive_clean.info()
bug_count[423]
dac = np.vstack(df_all.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values) $ df_all['dac_year'] = dac[:,0] $ df_all['dac_month'] = dac[:,1] $ df_all['dac_day'] = dac[:,2] $ df_all = df_all.drop(['date_account_created'], axis=1)
data.iloc[140:170,]
param_test3 = {'min_samples_split':range(1,400, 20), 'min_samples_leaf':range(30,71,10)} $ gsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=80,max_depth=11,max_features='sqrt',subsample=0.8),\ $                         param_grid = param_test3, scoring='log_loss',n_jobs=4,iid=False, cv=5) $ gsearch3.fit(drace_df[feats_used],y)
df_download_node=pd.merge(df_download,df_node,left_on='nid',right_on='nid',how='left') $ df_download_node=df_download_node[df_download_node['title'].notnull()] $ df_download_node=df_download_node[['uid','nid','title','date','url']] $ df_download_node=df_download_node[df_download_node['date']>='2017-07-01'] $ df_download_node=df_download_node.rename(columns={'title':'download_title','date':'download_date'})
guinea = concatenated.loc[concatenated['country'] == 'guinea'] $
df.head() # default --> prints first 5 rows
for col in list(df.columns) : $     k = sum(pd.isnull(df[col])) $     print(col, '{} nulls'.format(k))
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, 1-p_new])
def expand_counts(source_counts): $     return np.repeat(np.arange(len(source_counts)), source_counts)
y_hat = model.predict(X_test)
age = pd.cut(titanic['age'], [0, 18, 100])  # Assume no-one is over 100 $ age.head()
print(df_weather_origin.shape) $ print(df.shape)
active_companies['sum_red_flags'] = active_companies[red_flag_columns].sum(axis=1) $ active_companies.sum_red_flags.value_counts()
tweet_archive_enhanced_clean['name'].replace(to_replace =['a','an','the'],value = 'None',inplace = True)
active_listing_count_binarizer = Binarizer(10000) $ active_listing_count_binarizer.fit(X_train[['Active Listing Count ']].values) $ training_active_listing_dummy = active_listing_count_binarizer.transform( $     X_train[['Active Listing Count ']].values) $ print(training_active_listing_dummy[0:5], training_active_listing_dummy.mean())
net.add_nodes(pop_name='inhibitory', $               ei='i', $               model_type='population', $               model_template='dipde:Internal', $               dynamics_params='inhibitory_pop.json')
who_purchased.columns = ['BOUGHT_'+str(col) for col in who_purchased.columns]
sm = CustomBusinessDay(weekmask = 'Sun Mon') $ sm
code_eau_chaude = pd.read_csv(data_repo + 'nature_code_eau_chaude.csv', sep='|') $ code_energie = pd.read_csv(data_repo + 'nature_code_energie.csv', sep='|') $ code_fonction = pd.read_csv(data_repo + 'nature_code_fonction.csv', sep='|') $ code_installation = pd.read_csv(data_repo + 'nature_code_installation.csv', sep='|') $ code_specification = pd.read_csv(data_repo + 'nature_code_specification.csv', sep='|')
retention_10_per
passengers_per_cabin_floor = {} $ for floor in titanic_df['cabin_floor'].unique(): $     if str(floor) != 'nan': $         passengers_per_cabin_floor[int(floor)] = titanic_df[titanic_df['cabin_floor'] == floor]['cabin_floor'].count()
my_person.__str__()
df2['ab_page'] = pd.get_dummies(df['group']) ['treatment'] $ df2.head()
sample = df.sample(n=1000, replace=False)
model.save('model')
intersections_irr.head()
missing_sample = pd.read_csv("../data/microbiome/microbiome_missing.csv", $                              na_values=['?', -99999], nrows=20) $ missing_sample
len(df_proj_agg)
n_valid_users = df2['user_id'].unique().shape[0] $ print (n_unique_users)
df_epi_new = df_epi[df_epi.index>max(df_mysql.endDate)] $ df_epi_new = df_epi_new.drop(['latitude','longitude'], axis=1) $ df_epi_new
groups = contract_history[['INSTANCE_ID', 'UPD_DATE']].merge(intervention_train[['INSTANCE_ID', 'CRE_DATE_GZL']])
df2 = df[((df.group == 'treatment') & (df.landing_page == 'new_page'))|((df.group == 'control')&(df.landing_page == 'old_page'))]
%run process_twitter2tokens.py -i ../data/test_promotion_people.csv -ot ../data/test_promotion_people.txt -oc ../data/test_promotion_people_tokenized.csv -co text
twitter_master.info()
top_songs['Track Name'].unique()
import pandas as pd $ import quandl $ df = quandl.get("WIKI/GOOGL") $ print(df.head())
df1= pd.read_csv('Monthly_data_cmo.csv') $ df2=pd.read_csv('CMO_MSP_Mandi.csv')
stacking_prediction.mean(axis=1)[1458]
basket = (order $           .groupby(['ID', 'Lineitem name'])['Lineitem quantity'] $           .sum().unstack().reset_index().fillna(0) $           .set_index('ID'))
save_obj('translated_kyt_tweet', df_kyt)
1/np.exp(result.params[1])
autos.describe(include = "all")
df2[df2.landing_page == 'new_page'].shape[0]/df2.shape[0]
def get_counts_and_averages(ID_and_ratings_tuple): $     nratings = len(ID_and_ratings_tuple[1]) $     return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1]))/nratings)
df_download_node=pd.merge(df_download_node,df_users_2[['uid','no_of_downloads','downloaded_or_not']],left_on='uid',right_on='uid',how='inner')
cats_df['breed'].value_counts()
Xholdout = Xholdout[cols_final]
suspects_with_1T_25 = suspects_with_1T[suspects_with_1T['event_type'] == 25]
df.groupby('episode_id').id.nunique().head()
OAUTH_SERVER_URL = 'https://accounts.team.blastmotion.com/oauth2/token'
data.drop_duplicates('id', keep = 'last', inplace = True)
for i in range(5): $     print(len(top_movies_list[i]['results']))
df2['new_sales_perc'] = df2.new_sales.divide(df2.sales) $ df2.head()
import tensorflow as tf $ n_inputs = 28*28  # MNIST $ n_hidden1 = 300 $ n_hidden2 = 100 $ n_outputs = 10
import pandas as pd $ stocks_info_df = pd.read_csv('../data/stocks_info.csv')
770093767776997377 in retweets
predict_house_price
print(json.dumps(tweets._json, indent=4))
poverty_data_columns[0]='Category'
ser = pd.Series([1, np.nan, 2, None]) $ ser
from pyspark.mllib.evaluation import MulticlassMetrics $ predictionAndLabel = predictions.select("prediction", "label").rdd $ metrics = MulticlassMetrics(predictionAndLabel) $ print metrics.confusionMatrix()
print(broadband.shape) $ print(broadband.head(10))
df.head()
topics_data = [] $ for i in range(number_of_topics): $     topics_data.append([i]+[x[0] for x in ldamodel.show_topic(i)]+[x[1] for x in ldamodel.show_topic(i)]) $ topics_df = pd.DataFrame(topics_data,columns=["topic_id"]+["word_%s"%i for i in range(10)]+["word_%s_weight"%i for i in range(10)])
prob_conv = sum(df2.converted) / len(df2) $ prob_conv
train['air_genre_name']=train['air_genre_name'].fillna('Other')
DataSet['userTimezone'].value_counts()
def caps_fix(value): $     value = value.lower() $     return value $ tags['Tags'] = tags['Tags'].map(caps_fix) $ tags['Category'] = tags['Category'].map(caps_fix)
prediction_and_counts.head()
payments_all_yrs.head()
df_pol[(df_pol['domain'] == 'nationalreview.com' )]
therm_fiss_rate = sp.get_tally(name='therm. fiss. rate') $ fast_fiss = fiss_rate / therm_fiss_rate $ fast_fiss.get_pandas_dataframe()
sns.countplot(x="created_day", hue="interest_level", hue_order=['low', 'medium', 'high'], data=train_df)
ccl["rise_in_next_week"] = (ccl["CCL"].shift(-1)/ccl["CCL"] >= 1).astype("int")
print(cc.head())
DATA_FILE_2014 = '~/Google Drive/bayeshack2016-prep-HHS/data-raw/ServiceArea/HHS_Service_Area_PUF_2014.csv' $ DATA_FILE_2015 = '~/Google Drive/bayeshack2016-prep-HHS/data-raw/ServiceArea/HHS_Service_Area_PUF_2015.csv' $ DATA_FILE_2016 = '~/Google Drive/bayeshack2016-prep-HHS/data-raw/ServiceArea/HHS_Service_Area_PUF_2016.csv' $ DATA_FILE_cleaned = '/Google Drive/bayeshack2016-prep-HHS/data-cleaned/'
for article in articles: $     footer = article.find('footer') $     featured_image_url = 'https://www.jpl.nasa.gov' + footer.a['data-fancybox-href'] $     print (featured_image_url)
print metrics.conftusion_matrix(y_test, predicted) $ print metrics.classification_report(y_test, predicted) $
fb['created_time'] = pd.to_datetime(fb['created_time']) $ fb.set_index('created_time', inplace=True) $ fb.sort_index(inplace=True)
len(test_pl)
(null_vals - obs_diff).mean()
df_clean = df_clean.drop(['in_reply_to_status_id', 'in_reply_to_user_id','retweeted_status_id', $               'retweeted_status_user_id', 'retweeted_status_timestamp', 'source' ], axis = 1)
from sklearn.feature_extraction.text import CountVectorizer
nltk_text.concordance("PETA") $ print $ nltk_text.concordance("protest") $
years = data.set_index("year") $ years.head() $ years.tail()
not_in_misk.head(3)
logit_mod = sm.Logit(df_new2['converted'], df_new2[['intercept', 'ab_page', 'US_pages']]) $ results = logit_mod.fit() $ results.summary()
grouped_by_date_df.to_csv('interest_level_by_date_df.csv',index=False)
df2.loc[df['user_id'] == 773192]
import time $ now = time.strftime("%c") $ print ("Current date & time " + time.strftime("%c")) $ todays_date = time.strftime("Current date & time" + time.strftime("%c")) $ send_text(now.split(':')[0].replace('  ',' ') + ':' + now.split(':')[1]) $
obs_diff = treatment_conv - control_conv $ obs_diff
about.find('a')[0].html
Sort2 = stores.sort_values(by = ["Location","TotalSales"])
dt.minute
open_prices = df['Open'] $ open_prices.head()
msftA = msft[['Adj Close']] $ closes = pd.concat([msftA, aaplA], axis=1) $ closes[:3]
for res_key, df in entso_e.items(): $     entso_e[res_key], nan_tables[res_key + ' ENTSO-E'] = find_nan( $         df, res_key, headers, patch=True)
ix_cape_town = df_protest.TownCity_Name=='Cape Town' $ df_protest.loc[ix_cape_town].head()
df2 = df2[~df2.user_id.duplicated(keep='first')] $ df2.shape #total rows minus one 'dup' - done!
word_counts = bow_features(sentences, words_in_articles) $ word_counts.shape $
gs_k150_under.score(X_test, y_test_under)
lgbm = lgb.LGBMRegressor() $ lgbmrscv = model_selection.RandomizedSearchCV(lgbm, params, n_iter=10,cv=tscv,n_jobs=-1)  $ lgbmrscv.fit(train[col],np.log1p(train['visitors'].values)) $ print(lgbmrscv.best_params_)
titanic.describe()
df.orderBy(df.followers_count.desc()).show(10) $ top10_df=df.orderBy(df.followers_count.desc())
new_df = pd.merge(df,orgs[['uuid','funding_rounds','funding_total_usd','status','homepage_url']],left_on = 'company_uuid',right_on = 'uuid') $ trunc_df = new_df[new_df.funding_rounds > 1].copy() $ trunc_df.to_csv('/Users/Lucy/Google Drive/MSDS/2016Fall/DSGA1006/Data/unsupervised/trunc_clustering.csv')
pd.options.display.max_colwidth = 200 $ data_df[['ticket_id','type','clean_desc','nwords']].head(30)
y = m_true*x+c_true
s.ix[:3].resample('W')
temp_df = pd.DataFrame(data = us_temp) $ temp_df.columns = ts.dt.date[:843]
(day_counts.select('day', 'count', 'hashtag', daily_rank) $            .filter('rank <= 5') $            .sort('day', 'rank') $            .show(20))
df2['intercept'] = 1 $ df2['ab_page'] = np.where(df2['group'] == 'treatment', 1, 0) $ df2.head()
df[race_cols].head()
len(df[(df['Complaint Type'] == 'Homeless Encampment')&(df.index.month.isin([12,1,2]))])
df_ml_713 = df.copy() $ df_ml_713.index.rename('date', inplace=True) $ df_ml_713_01=df_ml_713.copy()
LT906474.head()
!git clone https://github.com/tensorflow/models.git
df_day=df[(df["day"] == 1)] $ df_night=df[(df["day"] == 0)] $ scipy.stats.ks_2samp(df_day["tripduration"],df_night["tripduration"])
jobs_data['clean_description'] = jobs_data['record.description'].apply(lambda x: " ".join([stemmer.stem(i) for i in re.sub("[^a-zA-Z]", " ", str(x)).split() if i not in s_words]).lower())
new_fan = questions.loc[questions['years_attend'] == 0] $ return_fan = questions.loc[questions['years_attend'] != 0]
fig, ax = plt.subplots(1, figsize=(12,4)) $ plot_with_moving_average(ax, 'MA Doctors', doc_duration, window=16) $ fig.savefig('./images/dr_MA16.png')
import matplotlib.pyplot as plt $ %matplotlib inline $ ax = delays_by_origin.plot.scatter('mean', 'count', alpha=.2) $
scores.shape
positive_deaths_per_year = defaultdict(int) $ for death,trend in zip(df_valid["Died"],df_valid["Trends"]): $     if trend >= 1: $         positive_deaths_per_year[death.year] += 1 $ positive_deaths_sorted = OrderedDict(sorted(positive_deaths_per_year.items()))
df2[df2.duplicated('user_id', keep=False)]
autos.columns = new_col_names
len(simple_features)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.groupby('country').count()
feature_cols_2016 = ['Profit','Volume','Days'] $ x_2016 = liquor2016_q1_features[feature_cols_2016]
train_small_data = reduce_mem(train_small_data) $ val_small_data = reduce_mem(val_small_data)
min_risk = minimize(risk_, W, args=(ER, COV), method='SLSQP', bounds=b_, constraints=c_) $ if not min_risk.success: $     print 'risk_ not optimized: ', min_risk.message $ Results = Results.join(pd.Series(min_risk.x, name='min_risk').round(4)) $
display_f1(y_test, test_preds)
tmp_df.reset_index().to_file('geocoded_evictions_deidentified.geojson', driver='GeoJSON')
with open("email_hash_list_gh.csv", "w") as outfile: $     for entries in user_gh_list_hash: $         outfile.write(str(entries)) $         outfile.write("\n")
oil_interpolation=daily_sales[['date','dcoilwtico']] $ pd.DataFrame.head(oil_interpolation)
converted_users = df[df['converted']==1] $ df['converted'].count() $ proportion_converted = round(converted_users['user_id'].nunique()/df['user_id'].nunique(),2) $ print('The proportion of converted users is: ' +  str(proportion_converted )) $
rsvp_df = pd.read_csv("last_five_rsvp_means.csv") $ rsvp_df.head()
allqueryDF.columns = ['state_id', 'domain', 'entity_id', 'state', 'attributes', $                       'origin', 'last_changed', 'last_updated', $                       'created'] # 'event_id' no longer exists?
joined=pd.merge(df2, countries, on=['user_id','user_id']) $ joined.head()
my_df["distance"] = np.sqrt((my_df["xcoordinate"] - 0.5) ** 2 + (my_df["ycoordinate"] - 0.5) ** 2) $ print(my_df.head())
df2_with_country = df_countries.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df2_with_country.head()
itemTable['Content'] = itemTable['Content'].str.lower() $ meeting_hash = itemTable['Content'].str.contains("meeting") $ print ("Total Meeting Count : " + str(meeting_hash.sum()))
idx = df_providers[ (df_providers['year']==2015) & \ $                   (df_providers['drg3']==987)  ].index.tolist() $ len(idx) $
dusseldorf.info()
nba_df.head(3)
df_new['intercept']=1 $ Xvars=[ 'intercept','CA','US'] $ lm = sm.Logit (df_new['ab_page'],df_new[Xvars ] ) $ results = lm.fit() $ results.summary()
options = {'Organization': 'name', 'Person': 'name'} $ draw(objects, options, physics = True)
topic_word = model.topic_word_  # model.components_ also works $ vocab = tf_vectorizer.get_feature_names()
np.exp(-0.0150), np.exp(-0.0408),np.exp(0.0099)
df_dont_line_up = df.query("(landing_page == 'new_page' and group == 'control') or (landing_page == 'old_page' and group == 'treatment')") $ print('The number of times the new_page and treatment don\'t line up is {}.'.format(df_dont_line_up.shape[0]))
data.columns
extract_deduped_cmp = extract_deduped[f_remove_extract_fields(extract_deduped.sample()).columns.values].copy()
dfsize=df.count()[0] $ dfsize
full['LOS'].unique().shape[0]
df_new['intercept'] = 1 $ lm = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA','US']]) $ results = lm.fit() $ results.summary()
df_new = pd.merge(data_df, shiny, left_on = 'patched_cell_container', right_on = 'Unnamed: 0')
t = pd.read_hdf('../t.hdf','table') $ dd = pd.read_hdf('../dd.hdf','table')
len(train), len(test)
import pandas as pd $ t=trumpint.describe() $ c=cliint.describe() $ tc=pd.concat([t, c],keys=['Trump','Clinton'], axis=1 ) $ tc
df = df[df['DATE'] < '2017-07-01']
all_words
df_group2=df.query("landing_page=='new_page'and group=='treatment'")
CONSUMER_KEY    = '' $ CONSUMER_SECRET = '' $ ACCESS_TOKEN  = '' $ ACCESS_SECRET = '' $
final_word_df.tail()
dates = pd.date_range('2018-01-01', '2018-05-23') $ dates
data = stock_dict['data'] $ type(data)
model_lr = LinearRegression().fit(X_train, y_train) $ print('Training set accuracy: {:.2f}:'.format(model_lr.score(X_train, y_train))) $ print('Test set accuracy: {:.2f}:'.format(model_lr.score(X_test, y_test)))
r2
file = export_geojson(outfile_name, collection) $ outfile_zip = zip_geojson(outfile_name)
p_old = df2[df2['landing_page']=='old_page']['converted'].mean() $ print("P of conversion of old page (p_old):", p_old)
mydata = np.load('my_filename2.npz') $ print(mydata['a']) $ print(mydata['b'])
df_more.to_csv('Indeed_Project_3_df_cleaned.csv', encoding='utf-8')
github_data.drop_duplicates('user_id').user_type.value_counts()
op = [] $ for row in json_data['dataset_data']['data']: $     op.append(row[1]) $ print (max(op), min(op))
from sklearn.model_selection import train_test_split $ from sklearn.neighbors import KNeighborsClassifier $ from sklearn.preprocessing import StandardScaler $ from sklearn.model_selection import cross_val_score $ from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
with pd.option_context('max.rows', 15): $     print(result["Pass"].sort_values(ascending=False))
print(article.shape) $ article.head()
status = status.retweet() $ status.retweeted, status.retweet_count
testObj.outDF.tail()  ## this check shows default behavior $
start_time = '2017-06-21' $ end_time = '2017-07-08' $ experiment_id = '8436781476'
print("there is null value on column botometer, this null indicated this account already deactivate their account :") $ len(compiled_data[pd.isnull(compiled_data['botometer'])==True])
df.rename(columns={'PUBLISH STATES':'Publication STATUS','WHO region':'WHO Region'},inplace=True) $ df.info()
result_df = sqlClient.get_result(jobId) $ print("OK, we have a dataframe for the SQL result that has been stored by SQL Query in " + sqlClient.get_job(jobId)['resultset_location'])
deployment_details = client.deployments.create(model_guid, name="Keras LSTM deployment")
print("Printing the vector of 'inc': {} ...".format(model['inc'][:10])) $ print("Printing the similarity between 'inc' and 'love': {}"\ $       .format(model.wv.similarity('inc', 'love'))) $ print("Printing the similarity between 'inc' and 'company': {}"\ $       .format(model.wv.similarity('inc', 'company')))
gi = convert_game_value(testset.filelist[ind],testset.feature_list,pgn2value)
zipfile = "2013_ERCOT_Hourly_Load_Data"
d = docx.Document(downloadIfNeeded(example_docx, example_docx_save, mode = 'rb')) $ for paragraph in d.paragraphs[:7]: $     print(paragraph.text)
p_r_curve(y_test, y_fit[:,1])
writers.groupby('Country').sum()
parsed.foreachRDD(lambda rdd: handler(rdd))
df_kws.plot(kind='area')
forecast_data
df.groupby("cancelled")["awards_referral_bonus"].mean()
data.info()
df['tweet_location_coord'] = df['tweet_location_coord'].astype('object') $ df['user_timezone_coord'] = df['user_timezone_coord'].astype('object')
unique_users = df2['user_id'].nunique(); $ unique_users
df_archive = pd.read_csv('twitter-archive-enhanced.csv')
TestData = pd.read_csv('test/test.csv')
bd $
col='Case.Status' $ set(tmp_df.loc[np.logical_not(tmp_df[col].isnull()),col])
y.mean()
le.classes_
datacamp.groupby([datacamp['publishdate'].dt.year, datacamp['publishdate'].dt.month]).size().plot(kind='bar', figsize=(15,7), color='b') $
data.loc[(80,slice('20150117','20150417'),'put'),:].iloc[:,0:4]
sns.barplot(x='user',y='number_of_likes',data=most_liked_tweets.tail(5))
gene_df.drop('attributes', axis=1, inplace=True) $ gene_df.head()
os.listdir()
np.info(np.unravel_index)
csgo_profiles = pd.read_sql("SELECT * FROM csgo.profiles", conn) $ csgo_profiles.head()
r6s = r6s[~r6s['selftext'].isin(['[removed]','[deleted]'])] $ r6s = r6s.dropna(subset=['title','selftext']) # drop when the content is null $ r6s = r6s[r6s['selftext'].str.len() > 50] $ r6s.shape
ordered.head()
n_old = df2[df2['landing_page'] == 'old_page'].shape[0] $ n_old
df[['beer_name', 'simple_style', 'brewery_name', 'brewery_country']][df.rating_score == worst_score]
reviews.country.unique()
import numpy as np $ unsegmented_users['segment'] = np.random.randint(num_user_segments, size=len(unsegmented_users))
v_item_hub.columns[~v_item_hub.columns.isin(item_hub.columns)]
options_data['IMP_VOL'] = 0.0
df['converted'].sum()/df.shape[0]
data = pd.DataFrame.from_dict(raw_data) $ a = pd.DataFrame(raw_data['data']) $ with open('data/raw_data.json', 'w') as fp: $     json.dump(raw_data['data'], fp)
result[result['uid'].notnull()].head()
for topic_idx, topic in enumerate(nmf.components_): $     print "Topic %d:" % (topic_idx) $     print " ".join([tfidf_feats[i] $                     for i in topic.argsort()[:-10 - 1:-1]])
year_prcp = session.query(Measurement.date, Measurement.prcp).\ $     filter(Measurement.date > '2017-07-02').\ $     order_by(Measurement.date).all() $ year_prcp
adj_close_latest = adj_close[adj_close['Date']==stocks_end] $ adj_close_latest
 print(match_model.summary())
lines = sc.textFile("data.txt") $ lineLengths = lines.map(lambda s: len(s)) $ totalLength = lineLengths.reduce(lambda a, b: a + b) $ print(totalLength) $ lineLengths.persist()
print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $ imagelist = [i for i in os.listdir() if i.endswith(".txt")  ] $ imagelist
df_2017.shape
!pip -q install ibmcloudsql $ !pip -q install sqlparse
autos = autos[autos["price"].between(350,155000)]
ffr.resample("MS").last().head()
tweetsIn22Mar = tweets22Mar[tweets22Mar.lang == 'in'] $ tweetsIn1Apr = tweets1Apr[tweets1Apr.lang == 'in'] $ tweetsIn2Apr = tweets2Apr[tweets2Apr.lang == 'in']
post_df = pd.DataFrame(data) $ post_df.head()
linkpp.reset_index(inplace=True)
files8.EndDate=pd.to_datetime(files8.EndDate) $ files8.StartDate=pd.to_datetime(files8.StartDate) $ files8.head()
item_prediction_df=arrayToDataframe(item_prediction,customerList,productList)
xmlData.drop('county', axis = 1, inplace = True)
train['desc_length'] = train['description'].map(len) $ train['num_photo'] = train['photos'].map(len)
full['<=30Days'].mean()
observed_difference = len(df2.query('group == "control" and converted == 1')) / len(df2.query('group == "control"')) - len(df2.query('group == "treatment" and converted == 1')) / len(df2.query('group == "treatment"')) $ observed_difference
datetime=pd.DataFrame() $ datetime= pd.DataFrame({'year': df['Year'], $                        'month': df['Month'], $                        'day': df['Date'], $                        'hour':df['Time(Hrs)']})
rtitle = [x.text for x in soup.find_all('a', {'data-event-action':'title'})] $ rtitle.pop(0)
pax_raw = pax_raw[pax_raw.paxcal==1].copy() # Keep only users with a calibrated device $ pax_raw.drop(columns='paxcal', inplace=True) # Drop the calibration column, we no longer need it
txSenate.info()
df[df['Complaint Type']=='Noise - Residential']['Descriptor'].value_counts()
autos['price'].value_counts().sort_index(ascending=False).head(20)
mentions_df = pd.read_csv("/mnt/idms/fberes/network/ausopen18/data/ao18_mentions_with_names.csv",sep="|") $ mentions_df.head()
retweet_max = np.max(data['RTs']) $ retweets = data[data.RTs == retweet_max].index[0] $ print("Tweet with most retweets is: \n{}".format(data['Tweets'][retweets])) $ print("Number of retweets: {}".format(retweet_max)) $ print("{} characters.\n".format(data['len'][retweets]))
pd.concat([msftAV[:5], aaplAV[:3]], axis=1, keys=['MSFT', 'AAPL'])
twitter_df.query('tweet_id==884441805382717440')
df_planets
aggreg
autos.columns = snake_case
df_loan2[['fk_loan',0]].head()
def flatten(x_tensor): $     return tf.layers.flatten(x_tensor) $ tests.test_flatten(flatten)
grid_lat = np.arange(np.min(lat_us), np.max(lat_us), 1) $ grid_lon = np.arange(np.min(lon_us), np.max(lon_us), 1) $ glons, glats = np.meshgrid(grid_lon, grid_lat)
posts=posts_w
team_unique=results.date.astype(str)+results.home_team+results.away_team $ result_copy=results.copy() $ result_copy.index=team_unique $ result_copy.head()
from sklearn.cross_validation import cross_val_score
weekdays_avg_2012,weekdays_count_2012,weekends_avg_2012,weekends_count_2012=weekdays_weekends(2012) $ weekdays_avg_2013,weekdays_count_2013,weekends_avg_2013,weekends_count_2013=weekdays_weekends(2013) $ weekdays_avg_2014,weekdays_count_2014,weekends_avg_2014,weekends_count_2014=weekdays_weekends(2014) $ weekdays_avg_2015,weekdays_count_2015,weekends_avg_2015,weekends_count_2015=weekdays_weekends(2015) $ weekdays_avg_2016,weekdays_count_2016,weekends_avg_2016,weekends_count_2016=weekdays_weekends(2016)
country_dummies = pd.get_dummies(df2['country'])
reddit_comments_data.groupBy('created_date').count().orderBy('created_date').show(100)
df2['converted'][(df2['group'] == 'control')].sum()/df2['converted'][(df2['group'] == 'control')].count()
df2_unique = df2.user_id.nunique() $ print('The number of unique user_ids in df2 is {}.'.format(df2_unique))
data = data.reset_index(drop = True)
t1.info()
json_data = r.json() $ json_data
path = os.path.join('Data for plots', 'Annual index.csv') $ annual_index.to_csv(path, index=False)
df2 = df2.round({'css_score':2, 'new_sales':2, 'orders':0, $                  'calls_per_day':0, 'css_count':0, 'cust_avail_v3':4, $                  'new_conv':4})
cols_final = topicfeat + usrfeat + docfeat
del sentences
con = sqlite3.connect('db.sqlite') $ print(pd.read_sql_query("SELECT * FROM temp_table ORDER BY total DESC", con)) $ con.close()
old_df=df2[df2['landing_page']=='old_page'] $ n_old=len(old_df) $ n_old
sns.lmplot(x="apparentTemperatureHigh", y="subjectivity", data=twitter_moves,lowess=True,size=8,aspect=1.5)
data_df = pd.read_csv('bouldercreek_09_2013.txt', sep='\t', keep_default_na=False, na_values=[""]) $ data_df['datetime'] = pd.to_datetime(data_df['datetime'])
del dfRegMet["idUser"]
pd.Timestamp('now', tz='UTC')
fh_1 = FeatureHasher(input_type='string', non_negative=True) # so we can use NaiveBayes $ %time fit = fh_1.fit_transform(train.device_model)
sql_to_df(q76)
df.groupby(by='STATE_ALPHA')['FEATURE_ID'].count()
s - pd.tseries.offsets.DateOffset(months=2)
prediction.info()
df['body'] = df['body'].str.replace('[^\w\s]','')
df = pd.DataFrame(analysis)
explotions['decade'] = explotions['Date'].map(stringDate_to_decade)
top = final.groupby('text').agg({'tweetCount':'sum', 'latitude':'first', 'longitude':'first', 'text':'first'}) $ top = top.sort_values('tweetCount', ascending=False)[0:50]
df.rating_numerator.value_counts()
target_var = 'win_differential' $ y_data = use_data.loc[:, target_var]
df.seqid.unique().shape
common_names = df_merge.name.value_counts().head(6) $ common_names
df.head()
data_frame_lengths = [len(sheet) for sheet in sheets.values()] $ display(data_frame_lengths) $ sum(data_frame_lengths)
sim = TagSimilarity.load()
newp = len(df2.query('landing_page == "new_page"')) $ total = len(df2) $ newp_pr = newp / total $ newp_pr
df_mod[['US','UK']] = pd.get_dummies(df_mod['country'])[['US','UK']] # 'CA' being the reference $ df_mod['country'].value_counts()
fraud_data.head()
data_fp = 'data/brain2body.txt' $ b2b_df = pd.read_csv(data_fp) $ b2b_df.head()
print(' '.join(TEXT.vocab.itos[int(w)] for w in batch[0][:,0]))
prices.fillna(method='ffill', inplace=True) $ prices.iloc[:, 0:10].head()
df_episodes = lds.dataframes['simpsons_episodes'] $ len(df_episodes)
percipitation_2017_df.plot() $ plt.show()
xml_in.dtypes
linear_svc = LinearSVC() $ linear_svc.fit(X_train, Y_train) $ Y_pred = linear_svc.predict(X_test) $ acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2) $ acc_linear_svc
with open("demo.log", "r") as log_file: $     for line in log_file: $         print(line)
trn_df = train_data[usecols] $ val_df = val_data[usecols] $ trn_y = train_data.is_attributed $ val_y = val_data.is_attributed
y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled) $ accuracy_score(y_test, y_pred)
tweets_df.head()
session = Session(engine)
from pyspark.sql.functions import col, countDistinct $
data['a':'b']
d = pd.datetime(2016, 11, 15, 13, 0)
abcd = pd.read_csv('Completed sepsis pass.csv') $ abcd.head()
from sklearn.metrics import mean_absolute_error $ mean_absolute_error(y_test, pred)
wrd_full.groupby(['year'])['favorite'].mean()
cityID = 'b004be67b9fd6d8f' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Norfolk.append(tweet) 
import numpy as np $ dist = np.sum(train_data_features, axis=0) $ for tag, count in zip(vocab, dist): $     print(tag, count)
article.nlp() $ print("Keywords:", article.keywords, "\n") $ print("Summary:", article.summary, "\n")
val_small_data.head(1)
department_df_sub.sum(axis = 1)
extract_all.loc[(extract_all.APP_SSN.isin([570810796])),['APP_SSN','app_source_v2']]
df = pd.read_csv('../data/rawdata/HPD_Heat_HotWater.csv' $                   , dtype = {'Master SR #': str, 'SR #': str})
decision_tree = DecisionTreeClassifier() $ decision_tree.fit(X_train, Y_train) $ Y_pred = decision_tree.predict(X_test) $ acc_decision_tree = round(decision_tree.score(X_test, Y_test) * 100, 2) $ acc_decision_tree
m_df['interest_level_ishigh'] = (m_df.interest_level == 'high')*1 $ m_df['interest_level_islow'] = (m_df.interest_level == 'low')*1
import matplotlib.pyplot as plt $ import pandas as pd $ import numpy as np $
df_cust_data['Registration Date'] = pd.to_datetime(df_cust_data['Registration Date'])
df.groupby("cancelled")[["web_booking", "iphone", "android"]].mean()
for i in categorical: $     train_binary[i] = train_binary[i].astype('category')
p + pd.tseries.offsets.timedelta(minutes=120)
stocks.index.names
data_for_model.to_pickle('data_for_model')
df.head()
no_specialty.shape
noise = [np.random.normal(0,noise_level*p,1) for p in weather.power_output]
yc_new1 = yc_new1[['Unnamed: 0', 'Fare_Amt', 'tripDurationHours', 'Trip_Distance', 'Tip_Amt', 'income_departure', 'zip_dest']] $ yc_new1.head()
%%capture $ casActionsets = ['cardinality','dataPreprocess','varReduce','clustering','pca','sampling','decisionTree','dataStep','neuralNet','svm','astore','fedsql','percentile'] $ [cassession.loadactionset(actionset=i) for i in casActionsets] $ cassession.actionsetinfo(all=False)
to_be_predicted_Day4 = 55.27537048 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
df = pd.read_csv("pgh_meetup_groups_Jan_27_2018.csv") $ df.head()
stdev_r = np.std(r) $ print(f'Standard deviation of the noise component is {stdev_r}')
cotradicted_pairs = pd.read_csv('Extracted_Contradicted_Tweet_Pairs.csv',encoding='utf-8')
import matplotlib.pyplot as plt $ plt.style.use('ggplot')
archive = pd.read_csv('twitter-archive-enhanced.csv') $ archive
S_check2 = session.query(Station).statement $ S_df = pd.read_sql(S_check2,session.bind) $ S_df.head()
conn.commit() $ conn.commit() $ conn.commit()
df.loc[6:10]
tweet_df.head()
for x in range(5): $     print donald[x]['text'] $     print('--')
s.str.repeat(2)
image_clean['p1'] = image_clean['p1'].str.replace('_', ' ') $ image_clean['p2'] = image_clean['p2'].str.replace('_', ' ') $ image_clean['p3'] = image_clean['p3'].str.replace('_', ' ')
df['event_life'][:10]
sort=df.sort_values('score',ascending=False,axis=0)[:25] $ sort.loc[:,['author','content','score']][:5]
dogscats_df = pd.DataFrame(np.load('dogscats_features.npy')) $ dogscats_df.columns = ["X_" + str(col) for col in dogscats_df.columns] $ predictors = dogscats_df.columns.tolist() $ dogscats_df['label'] = np.load('labels_sample.npy') $ dogscats_df['imagePath'] = np.load('imagePaths_sample.npy')
pumaPop = pumaPop[['B00001_001E','public use microdata area']].rename(columns={'B00001_001E':'Pop'})
group_period = '7D' $ cc_df = cc_df.resample(group_period).mean() $ cc_df.head()
plt.figure(0) $ source_counts = df['sourceurl'].value_counts().head(10) $ source_counts.plot.bar()
fixed_bonus_points.insert(0, "sep", 0) $ fixed_bonus_points
df_low_temps = pd.concat(low_temps)
df.printSchema()
data = pd.merge(data, pickup_demand, left_on= ['floor_10min', 'pickup_cluster'], right_on=['ceil_10min', 'pickup_cluster'], how='left') $ data = pd.merge(data, dropoff_demand, left_on= ['floor_10min', 'dropoff_cluster'], right_on=['ceil_10min', 'dropoff_cluster'], how='left') $ data = pd.merge(data, ride_demand, left_on= ['floor_10min', 'ride_cluster'], right_on=['ceil_10min', 'ride_cluster'], how='left')
autos[["price", "odometer_km"]].head()
df2[['treatment', 'control']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('treatment', axis=1) $ df2['intercept'] = 1 $ df2.head()
filtered_df[numeric_cols].dtypes
df2.drop(labels = 2862, axis = 0, inplace = True)
location
np.exp(0.0507)
rational_da.head()
len(interactions_deleted), len(red_inter_del)
plt.hist(p_diffs_alt) $ plt.grid() $ plt.axvline(p_diffs_alt.mean(), color='r', label='mean') $ plt.legend();
df[df.transaction_date > datetime.strptime('2017-02-01', '%Y-%m-%d')]
df_dummies.head()
X_train.shape
all_tables_df.iloc[:, 1]
!!from time import sleep $ for i in range(0, 100): $     print(i) $     sleep(0.1)
%matplotlib inline $ import matplotlib.pyplot as plt $ sns.set() # use seaborn styles $ births.pivot_table('births', index='year', columns='gender', aggfunc='sum').plot() $ plt.ylabel('total births per year')
train_data.head() $
plt.pie(total_ride, explode=explode, autopct="%1.1f%%", labels=labels, colors=colors, shadow=True, startangle=140) $ plt.show()
df2.drop_duplicates('user_id', inplace=True) $ df2.info()
df_transactions['not_auto_renew'] = df_transactions.is_auto_renew.apply(lambda x: 1 if x == 0 else 0)
lr = LogisticRegression() $ lr_grid = GridSearchCV(lr,params,scoring='recall')#'f1')
msft.dtypes
best_worst = data_df.loc[(stars==5) | (stars==1), :] $ best_worst.head()
print("Nr of months with a coin switch:", sum(fund_nr_coinswitches > 0), "out of", len(fund_nr_coinswitches)) $ print("Median coins switches during a month:", fund_nr_coinswitches.median())
(len(df2[(df2["landing_page"] == "new_page")]) / len(df2["landing_page"])) * 100
dd_df_fuelType.head()
ferrocarriles_caba = pd.read_csv('datasets/estaciones-de-ferrocarril.csv', sep=';', error_bad_lines=False, low_memory=False) $ ferrocarriles_caba.info()
max(dates['date'])
soup = BeautifulSoup(response.text,'lxml')
def chunks(list_, n): $
detailed_consumption = pd.read_csv("LA_detailed_consumption.csv", parse_dates =['heartbeat_end']) $ detailed_bc = detailed_consumption[detailed_consumption.subnet == 'BC'] $ detailed_bc.reset_index(inplace = True, drop = True) $ detailed_bc.head(5)
traindf['reviews'] = traindf['reviews'].apply(lambda x: preprocess_text(x)) $ evaldf['reviews'] = evaldf['reviews'].apply(lambda x: preprocess_text(x))
to_be_predicted_Day2 = 21.38782431 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
print stations.installation_date.min() $ print stations.installation_date.max()
duration_train_df.to_csv('./data/hours_training_data.csv')
tweet_archive_clean['text'] = tweet_archive_clean['text'].apply(lambda x: x.split('https')[0])
appointments['Specialty'].loc[appointments['Provider'].isin(dr_ID)]= 'doctor' $ appointments['Specialty'].loc[appointments['Provider'].isin(RNPA_ID)] = 'RN/PA' $ appointments['Specialty'].loc[appointments['Provider'].isin(ther_ID)] = 'therapist'
n_old = df2[df2['group']=='control']['user_id'].nunique() $ n_old
print(training_features.shape, training_target.shape, test_features.shape, test_target.shape)
unique_users = ab_data["user_id"].nunique() $ unique_users
df2['US_ab'] = np.multiply(df2['ab_page'],df2['US']) $ df2['UK_ab'] = np.multiply(df2['ab_page'],df2['UK']) $ df2.head()
twitter_archive_master.rating_numerator.plot(kind='hist');
cur.execute('SELECT EmpGradeRank, count(EmpGradeRank) FROM demotabl WHERE EmpGradeRank="SENIOR";') $ cur.fetchone()
donors['Donor Zip'].value_counts().head()
def html_wrap(content, element="span", attributes_str=None): $     element = f"h{level}" $     return html_wrap(content, element) $ def heading(*args, **kwargs): $     return HTML(make_heading(*args, **kwargs))
my_tweet_df.count()
merged.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount", ascending = False) $
archive_copy.info()
df2['FlightDate'].head()
conn.setsessopt(caslib='research')
pnew=df2.converted.mean() $ pnew
from sklearn.preprocessing import PolynomialFeatures $ poly15 = PolynomialFeatures(degree=15) $ x_15 = poly15.fit_transform(x)
states.mean(level='location')
vals1 = np.array([1, None, 3, 4]) $ vals1
logistic_countries = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US']]) $ results2 = logistic_countries.fit()
newParser = parser.SVPOLParser(newFormat) $ newParser
df_usa=df_usa.pivot(columns='Variable Name',values='Value') $ df_usa.head()
gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_
hours.head(10)
df_clean3.sample(5)
lm=smf.ols(formula='y ~ h0+h1+h2+h3+h4+h5+h6+h7+h8+h9+h10+h11+h12+h13+h14+h15+h16+h17+h18+h19+h20+h21+h22+h23+wet+low_vis',data=df_reg).fit() $ lm.summary()
def get_trip_data(x): $     return suspects_with_25_1.loc[:x].tail(2) $ map(get_trip_data, trip_index_25_1)
result = Geocoder.geocode("7250 South Tucson Boulevard, Tucson, AZ 85756")
df=json_normalize(data["dataset"], "data") $ df.columns = col_names $ df.head(3)
df_ad_airings_5.isnull().any()
d = rng[0] $ d
lq[('Category')] = lq[('Category')].astype(int)
y.name = "huhu" $ y = y.rename("jsdfjkdsfhsdfhdsfs") $ y
rhino_root = '/Volumes/RHINO/' $ report_db_location = "/Volumes/RHINO/scratch/report_database/"
plot = sb.boxplot(x=dfEPEXbase.index.hour, y='Price', data=dfEPEXbase) # plot distribution of hourly prices $ plot.set_title('Distribution of Hourly Prices')
senateAll[:1]
    temp_df =  spark.sql("select * from world_bank limit 2") $     print type(temp_df) $     print "*" * 20 $     print temp_df $
temp_series.plot(label="Period: 1 hour") $ temp_series_freq_15min.plot(label="Period: 15 minutes") $ plt.legend() $ plt.show()
df.source.value_counts()
y_pred = model.predict(X_test) $ predictions = y_pred.tolist()
(df_cprc.isnull()).sum()
from sklearn.linear_model import LogisticRegression $ from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
s1 + s2
ac['Status'].value_counts().plot(kind='bar')
van = pd.merge(comvandal,users,how='left',on='username') $ van.head()
df_daily[["PREV_DATE", "PREV_ENTRIES"]] = \ $     (df_daily.groupby(["C/A", "UNIT", "SCP", "STATION"])["DATE", "ENTRIES"] $             .transform(lambda grp: grp.shift(1))) $ df_daily.tail(5) $ df_daily.head(5)
tweet_archive.head(2)
fit_time = (end_fit - start_fit) $ print(fit_time/60.0)
archive_clean.head()
print(np.info(np.random.random)
df = df.drop(['seller','name'],axis=1) $ df.head(3)
autos['price'].value_counts().sort_index(ascending=True).head(5)
n_old=df2.query('landing_page=="old_page"').shape[0] $ n_old
from nltk.tokenize import sent_tokenize $ sentence = sent_tokenize(text) $ print(sentence)
keywords = ['earthquake', 'quake', 'magnitude', 'epicenter', 'magnitude', 'aftershock'] $ search_results = api.search(q=' OR '.join(keywords), count=100)
df2.query("landing_page == 'new_page'")['landing_page'].count()
np.random.seed(123456) $ dates = pd.date_range('2014-8-1',periods=10) $ s1 = pd.Series(np.random.randn(10), dates) $ s1[:5]
s.resample('30D').head(10)
prob_temp = df2.query("group == 'treatment'")["converted"].mean() * 100 $ print("Probability if user in treatment group: {}%".format(round(prob_temp, 2)))
colorless = all_cards.loc[all_cards.colors.isnull()] $ all_cards.loc[colorless.index, "colors"] = colorless.colors.apply(lambda x: [])
list(df)
archive_copy['source'].sample(10)
talks_train['speaker_gender']=gender
actual_diff = df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean() $ (p_diffs > actual_diff).mean()
with open('data/model2.pkl', 'wb') as f: $     pickle.dump(model2, f)
s = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e']) $ print(s)
from search_utils import tweet_time_2_epoch $ tweet_time_2_epoch('Sun Aug 27 14:57:15 +0000 2017')
t0 = time.time() $ model.evaluate(x = X_dev.reshape(shapeX),   y = Y_dev,   verbose=False) $ dt = time.time() - t0 $ print("Evaluation speed: {:.2f} traces/s ".format(len(X_dev) /dt)) $
df[df.index.month.isin([12,1,2])]['Complaint Type'].value_counts().head()
%matplotlib inline $ top10.plot()
new_page_converted =  np.random.binomial(1, p = p_new,size = n_new) $ new_page_converted
from test_package.print_hello_function_container import * 
pd.options.display.max_colwidth = 2000 $ data_df[data_df.nwords > 800]['clean_desc']
joined.country.value_counts()
df_ab_cntry.country.unique()
import statsmodels.api as sm $ logit_mod = sm.Logit(df2_dummy.converted, df2_dummy[['intercept','ab_page_new_page']]) $
changes['date'] = changes['date'].fillna(method = "ffill") $ changes.iloc[6:10]
cols = cols[1:] + cols[:1] $ cols $ tweet_json_df = tweet_json_df[cols] $ tweet_json_df.head(2)
warm.sum()
other_list = list(org_counts[org_counts >= 3].index) $ other_list.remove('unlisted') $ other_list.remove('unknown') $ other_list.remove('bae systems')
df_countries = pd.read_csv('countries.csv') $ df_joined = df_countries.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_joined.head()
res.summary()
mv_lens.head()
df.iat[1,1]
s1.iloc[1]
logodds.drop_duplicates().sort_values(by=['count']).plot(kind='barh')
Customers_df = Change_New[['Sales_in_CAD', 'New_or_Returning']] $ Customers_df.head()
np_cities = np.array(cities)
joblib.dump(full_data, 'pickles/full_data.pkl') $ joblib.dump(cleaned_synops, 'pickles/cleaned_synops.pkl') $ joblib.dump(cluster_names, 'pickles/cluster_names.pkl')
coins.loc[:, coins.mean() > .5].head()
to_be_predicted_Day4 = 81.95063916 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
model = Sequential() $ model.add(Dense(LATENT_DIM, activation="relu", input_shape=(2*T,))) $ model.add(Dense(HORIZON))
tweet_summary = tweet_df.groupby(["Tweet Source"], as_index = False)["Compound Score"].mean().round(3) $ tweet_summary
store1_open_data[['Customers']].plot()
loans_df.emp_length.replace(replace_dict, inplace=True)
((ab_data.group == "treatment") & (ab_data.landing_page != "new_page") \ $  | (ab_data.group != "treatment") & (ab_data.landing_page == "new_page")).sum()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print('The z_score is {} and the p value is {}.'.format(z_score,p_value))
train_topics_df.head()
fav_max = ... $ fav_tweet = ... $ print("The tweet with more likes is: \n{}".format(data['Tweets'][fav_tweet])) $ print("Number of likes: {}".format(fav_max)) $ print("{} characters.\n".format(data['len'][fav_tweet]))
df.columns.values
pd.get_dummies(df[cols], prefix=['colA','colB'])
df3 = df2[df2['group']=='control']
[x.text for x in html.find_all('a', {'class':'next '})]
import statsmodels.api as sm $ convert_old = df2.query('group=="control" & converted==1')['user_id'].count() $ convert_new = df2.query('group=="treatment" & converted==1')['user_id'].count() $ n_old = n_old $ n_new = n_new
train_sents, test_sents = get_train_test_sents(during['text'].str.lower().str.split().apply(lambda x: [item for item in x if item not in stop_words]) $ , split=0.8, shuffle=False) $ vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in utils.flatten(train_sents)), size=during_v) $ print "Train set vocabulary: %d words" % vocab.size
 model.doesnt_match("man woman child kitchen".split())
dfcounts.head()
data.plot() $ plt.ylabel('Hourly Bicycle Count');
qualConvpct = qualConvpct.filter(like='convertedQual').rename('qualConversionPercent').to_frame().reset_index()
df_archive.describe()
n_old = len(df2.query('landing_page=="old_page"')) $ n_old
run_augmented_Dickey_Fuller_test(therapist_duration, num_diffs=2)
int_ab_page = 1/np.exp(-0.0150) $ int_ab_page
impressions_by_algRef = pd.DataFrame(impressions.groupby(['algRef'])['session'].count()) $ impressions_by_algRef.columns.values[0] = 'impressions' $ impressions_by_algRef = impressions_by_algRef.sort_values(by='impressions', ascending=False) $ impressions_by_algRef
x_test = np.array(data)
df.zipcode = df.zipcode.apply(fill_zipcode)
status = client.experiments.get_status(experiment_run_uid)
unique_id = df_providers['name'].unique()
train[train.msno.isin(user_logs.msno)].shape
len(errors_index)
p_diffs=[] $ new_convert=np.random.binomial(Nnew, p_new, 10000)/Nnew $ old_convert=np.random.binomial(Nold, p_old, 10000)/Nold $ p_diffs=new_convert-old_convert
df_twitter_archive_copy.gender.dtype
train
dtest = xgb.DMatrix(test.drop("click_id", 1)[dtrain.feature_names])
p_diff_obs = prop_conv_treatment - prop_conv_control $ plt.hist(p_diffs); $ plt.axvline(p_diff_obs, color = 'r'); $ print('The sampling distribution of the mean of the differences looks normally distributed as expected.')
import pods $ from ipywidgets import IntSlider
archive_copy['timestamp'] = pd.to_datetime(archive_copy['timestamp']) $ archive_copy.info()
pf['created'] = pd.to_datetime(pf['created'],unit='s') $ q=pf.loc[:,['author_name','created','gilded','upvotes','downs','upvote_ratio','score','no_comments']] $ q
s1.shape, s1.size
countries.info()
import pickle $ output = open('mentioned_bills.pkl', 'wb') $ pickle.dump(mentioned_bills_all, output) $ output.close()
df['B']
df['ch_json'] = df.name.apply(search_org) $ df['ch_postcodes'] = df.ch_json.apply(ch_postcodes)
df[df.install_rate.isnull()] $
geo_countries = [] $ for item in sub_data["place"]: $     geo_countries.append(item["country_code"])
animals = pd.crosstab(df['Type'], df['Outcome']) $ rate = animals.div(animals.sum(1).astype(float), axis=0)*100 $ rate.plot(kind='barh',stacked=True) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
df2 = df2.drop_duplicates(subset='user_id', keep='first')
pd.get_option("display.max_rows")
print(stock_data.dtypes)
featureList = ['open', 'high', 'low', 'adj close','volume','sma5','sma10','sma20','macd','rsi_10', 'rsi_20','cci_20','atr','wr_14','kdjk','kdjd','obv','trix'] $ data1 = data[featureList]
df_train.max(axis=0)
conn_a.commit()
dot.attr('node', shape='rectangle', $          style='filled', color='black', $          fillcolor='lightgrey')
atdist_4x_count_prop_overall = compute_count_prop_overall(atdist_4x, 'emaResponse') $ atdist_4x_count_prop_overall
preds = pd.DataFrame(y_test) $ preds['knn'] = knn_pred $ preds['rf'] = rf_pred $ preds['lr'] = lr_pred
p_old = df2[df2['landing_page']=='old_page']['converted'].mean() $ print("Probability of conversion for old page (p_old) is", p_old)
sum(insertid_freq.values())
print((data["Time Stamp"][0] - data["Time Stamp"][200])) $ print((data["Time Stamp"][0] - data["Time Stamp"][200]).days) $ print((data["Time Stamp"][0] - data["Time Stamp"][200]).seconds)
for dataset in full_data: $     dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 $ print (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())
con = STSConnectionParameters(user = 'admin', pw = '', db = 'default') $ sts = STSClient(con)
some_numbers = [10,8,3,1,5,-5,2,-15,-4,5,-2,-1,-3,-5] $ pd.Series(some_numbers).replace(range(-5,-1,2),0)
weather_data = pd.concat([weather_data1, weather_data2, weather_data3], ignore_index= True); weather_data.head()
data.head(11).to_excel("data_10.xls")
pd.groupby(vlc, by=vlc.visited.dt.year).size() \ $ .plot(kind="bar", title="Volume of Members Visits per Year", color="green", figsize=(12,4))
df2 = df.join(countries.set_index('user_id'), on='user_id', how = 'inner', lsuffix = '_df', rsuffix = '_country') $ df2.head()
lst = data_after_subset_filter.SEA_AREA_NAME.unique() $ print('Waterbodies in subset:\n{}'.format('\n'.join(lst)))
df.query('group == "treatment" and landing_page != "new_page"').count()
new_col_names = ['date_crawled', 'name', 'seller', 'offer_type', 'price', 'ab_test', $        'vehicle_type', 'registration_year', 'gearbox', 'power_ps', 'model', $        'odometer', 'registration_month', 'fuel_type', 'brand', $        'unrepaired_damage', 'ad_created', 'nr_of_pictures', 'postal_code', $        'last_seen']
with open('mydata.csv', 'w') as f: $     f_csv = csv.DictWriter(f, headers) $     f_csv.writeheader() $     f_csv.writerows(rows)
df2['intercept'] = 1 $ df2[['control','treatment']] = pd.get_dummies(df2['group']) $ df2.head()
twitter_merged_data.hist(column='favorites', bins =60); $ plt.title('Favorites Histogram') $ plt.xlabel('Favorites (Bins=60)') $ plt.ylabel('Count');
len(members)
processed_working_exp.working_exp.plot.box(showmeans=True, figsize=(4,8))
writers
plt.plot(ticks, dataset)
t0 = time() $ model = MatrixFactorizationModel.load(sc, "lastfm_model.spark") $ t1 = time() $ print("finish loading model in %f secs" % (t1 - t0)) $
autos['registration_year'].describe()
archive_df_clean['clean_numerator'] = archive_df_clean.clean_numerator.astype(float)
df[df['Descriptor'] == 'Loud Music/Party']['Unique Key'].groupby(df[df['Descriptor'] == 'Loud Music/Party'].index.dayofweek).count().plot(kind='bar')
nodereader.head()
from pyspark.sql.functions import date_format $ df = df.withColumn("call_hour_of_day", date_format("starttime" , "H")).drop("starttime") $ df['call_hour_of_day', 'Created Date'].show(truncate = False)
df_final_edited.info()
plt.hist(p_diffs) $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('Histogram of p_diffs'); $
LogisticModel_ZeroFill = LogisticRegression()
tweetsDF = make_df(tweets_data)
df_clean = df.copy() $ pred_clean = pred.copy() $ tweet_df_clean = tweet_df.copy()
train_df['interest_level'].value_counts()*1.0/len(train_df)
wrQualified = nvidia.filter(lambda p: p["gfx"]["features"]["wrQualified"]["status"] == "available" ) $ wrQualified.count()
print('.text = ', tweet.text, '\n') $ print('.lang = ', tweet.lang, '\n') $ print('.favorite_count = ', tweet.favorite_count, '\n') $ print('.source_url = ', tweet.source_url, '\n')
data.shape
feature_matrix_duplicated.drop_duplicates().shape
gm_df.tail(15)
org_counts = org_name_counter(df) $ org_counts[(org_counts >= 5)]
writers.groupby('Country').last()
precipitation_measurement_df = pd.DataFrame(precipitation_year[:], columns=['Date','prcp',]) $ precipitation_measurement_df['Date'] =  pd.to_datetime(precipitation_measurement_df['Date']) $ precipitation_measurement_df.set_index('Date', inplace=True) $ precipitation_measurement_df.head()
import pandas as pd $ import json $ from pandas.io.json import json_normalize $ import datetime as dt $
sql="SELECT * FROM %s.%s LIMIT 3" % (schema, data_table) $ HTML(hc.sql(sql).toPandas().to_html())
pd.Timestamp(2018, 1, 1)
df2.nunique() $ df2.describe()
from scipy.stats import norm $ print(norm.cdf(z_score)) # Tells us how significant our z-score is $ print(norm.ppf(1-(0.05))) # Tells us what our critical value at 95% confidence is
df3.country.value_counts()
prediction_clean['p1'][:10]
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True).hist()
print(tweet_archive[tweet_archive.tweet_id.isnull()]) $ print(tweet_archive[tweet_archive.tweet_id.duplicated()])
titanic.groupby('sex')['survived'].mean()
obs_diff = p_new-p_old $ obs_diff
pd.Series(data=predicted5).hist()
from sklearn.feature_extraction.text import TfidfVectorizer
cashflows_act_investor_20150430_repaid = replace_repaid(cashflows_act_investor_20150430,cashflows_plan_investor_all) $ cashflows_act_arrears_latest_paid_investor_repaid = replace_repaid( cashflows_act_arrears_latest_paid_investor,cashflows_plan_investor_all) $ cashflows_act_origpd_latest_paid_investor_repaid = replace_repaid( cashflows_act_origpd_latest_paid_investor,cashflows_plan_investor_all) $
cvec = CountVectorizer(stop_words = 'english') $ lr = LogisticRegression() $ lr_pipe = Pipeline([('cvec', cvec), ('lr', lr)])
bp.rename(columns ={"value1num": "systolic", "value2num": "diastolic"}, inplace = True)
tweet_full_df.info()
df_concat["date_series"] = pd.to_datetime(df_concat["created_time"]) $ df_concat["date_series"].head()
executable_path = {'executable_path': '/usr/local/bin/chromedriver'} $ browser = Browser('chrome', **executable_path, headless=False) $ url = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars" $ browser.visit(url) $ hemisphere_image_urls = []
df.head(2)
def get_net_polarity(tweet): $
beginning = pd.Series(data=start, index=dates) $ beginning
total = df['user_id'].unique().shape[0] $ total
plate_appearances['pitcher_throws_left'] = np.where(plate_appearances['p_throws'] == 'L', 1, 0) $ plate_appearances['left_handed_batter'] = np.where(plate_appearances['stand'] == 'L', 1, 0)
rng = pd.date_range('1/1/2011', periods=72, freq='H') $ type(rng)
model_sm.params.sort_values(ascending=False)
p_new_null = conversion_prob $ p_old_null = conversion_prob $ p_old_null, p_new_null
frequency_list = california_house_dataframe.iloc[:, 2].value_counts() $ print(type(frequency_list)) # Series Type $ print(sorted(california_house_dataframe.iloc[:, 2].unique())) $ frequency_list.hist() $ pd.DataFrame(california_house_dataframe.iloc[:, 2]).hist() $
n_new, n_old = df2['landing_page'].value_counts() $ print("n_new:", n_new, "\nn_old:", n_old)
url='https://api.twitter.com/1.1/trends/place.json?id=2459115' $ parameters={'q':'Trump'} $ topics=requests.get(url,auth=auth,params=parameters)
print(DataSet_sorted['tweetText'].iloc[-3])
for topic in df.topic: $     for k, v in replace_topics.items(): $         if topic in v: $             df.topic[df.topic == topic] = k
prediction_modeling2 = lr_best_model.transform(hashed_modeling2).cache() $ prediction_modeling2.show(1, truncate=False)
%%R $ dim(flightsDB)
fs_X = pd.DataFrame(fs_ds, columns=fs_ds.columns) $ fs_y = pd.DataFrame(df['state_labels']) $ X_new = SelectKBest(chi2, k=5) $ X_new.fit_transform(fs_X, fs_y)
hawaii_measurement_df = pd.read_csv(r"\Users\Josue\Desktop\\hawaii_measurements.csv")
df
twitter_archive.shape
df2 = df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) & ((df['group'] == 'control') == (df['landing_page'] == 'old_page'))]
%pylab inline
num_row = df.shape[0] $ num_row
authors.sort_values('count', ascending=False).head()
count_vect = CountVectorizer(vocabulary=unique_feature_list, tokenizer=lambda x: x.split(','))
print("{}'s most favorited tweet is: \n {}".format(target_user,data['tweets'][fav])) $ print("Number of favorites: {}\n\n".format(fav_max)) $ print("{}'s most retweeted tweet is: \n {}".format(target_user, data['tweets'][rt])) $ print("Number of retweets: {}".format(rt_max))
newdf.describe()
tmdb_movies['day_of_week'].head()
df_new[["CA", "UK", "US"]] = pd.get_dummies(df_new["country"]) $ df_new.head()
df_train['id'].unique()
Trump_week_total = Trump_week.groupby(by=["year","week"]).sum() $ Trump_week_total = Trump_week_total.reset_index() $ Trump_week_total.shape
df_train.head()
com_grp[['Age','Salary']].sum()
from sklearn.model_selection import StratifiedKFold , cross_val_score , GridSearchCV $ skf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=42 )
train_size = 11538 #use 80% of data for training $ train_df = tweets_df[:train_size].copy() $ test_df = tweets_df[train_size:].copy()
eth = pd.read_csv('data/eth-price.csv', parse_dates=True) $ print(eth.dtypes) $ eth.head()
df_copy['timestamp']=pd.to_datetime(df_copy['timestamp'])
dot.attr('node', fillcolor='lightgreen', shape='oval')
def get_month_year(df): $     df['month'] = df.date.apply(lambda x: x.split('-')[1]) $     df['year'] = df.date.apply(lambda x: x.split('-')[0]) $     return df $ get_month_year(pd_train_filtered);
Base.classes.keys()
print('x, y, z coords:', keto_coord.values[0]) $ distances = pmol.distance(keto_coord.values[0])
import matplotlib $ import matplotlib.pyplot as plt $ %matplotlib inline
len(corn)
tweet_data = pd.read_json(twitter_json) $ tweet_data.set_index('created_at', drop=True, inplace= True) $ pd.to_datetime(tweet_data.index)
cc.low.describe()
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
mask = df_journey['name'] == (line['stop']) $ index = df_journey[mask].index[0] $ df_journey = df_journey.loc[index:] $ df_journey['arrDate'].iloc[0] = np.NaN $ df_journey['arrTime'].iloc[0] = np.NaN $
print(train_y, val_y)
data = res.json()
import pickle
calculated_average_df = calculated_average_df[['Country', 'Month', $                                                'Death monthly average', 'Case monthly average']] $ calculated_average_df.head()
sum(image_predictions.jpg_url.duplicated())
fileIn = '/media/dedsresende/BackUpDeds/Github_UB/CapstoneProject/Feelings/kagle_capstone_project/presentacion/data/tweets_public_decode2.csv' $ fileOut = '/media/dedsresende/BackUpDeds/Github_UB/CapstoneProject/Feelings/kagle_capstone_project/presentacion/data/tweets_public_clean.csv'
data_households = {} $ for household_name, household_dict in households.items(): $     data_households[household_name] = pd.read_pickle('raw_'+household_dict['dir']+'.pickle')
joined = join_df(joined, googletrend, ["State","Year", "Week"]) $ joined_test = join_df(joined_test, googletrend, ["State","Year", "Week"]) $ len(joined[joined.trend.isnull()]),len(joined_test[joined_test.trend.isnull()])
pd.concat([s1, s2, s3], axis=0)
df.isnull().sum().any()
red = [child['data'] for child in data['data']['children']] $ red = pd.DataFrame(red) $ time = pd.Timestamp.utcnow() $ red['time fetched'] = time $ red.head()
%%timeit $ for i in range(10000): $     if pattern.search(the_str): $         b = the_str.replace('AA', 'BB')
print(len(set(equipment.INSTANCE_ID) & set(intervention_train.INSTANCE_ID))) $ print(len(set(equipment.INSTANCE_ID) & set(intervention_test.INSTANCE_ID))) $ print(len(set(equipment.INSTANCE_ID) & set(intervention_history.INSTANCE_ID)))
x = pdp['lat'] $ y = pdp['lon']
about.attrs
loan_stats["loan_status"].table()
print ("Proportion Greater : {}".format((np.array(p_diffs) > (p_t-p_c)).mean()), end = "\r")
test_portfolio.tail()
joined['dcoilwtico']=joined['dcoilwtico'].astype(np.int8)
cityID = '1c69a67ad480e1b1' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Houston.append(tweet) 
c['code'].unique()
set_option("display.max_colwidth",280) $
srp_url = 'https://pythonprogramming.net/parsememcparseface/' $ src = urllib.request.urlopen(srp_url).read() $ soup = bs.BeautifulSoup(src,'lxml') $ for para in soup.find_all('p'): $     print(para.text) $
countries_f = pd.read_csv('./countries.csv') $ f_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ f_new.head()
RunSQL(sql_query) $ actor = pd.read_sql_query(sql_query, engine) $ actor.head()
s3 = pd.Series(['A','B']) $ s4 = pd.Series(['C','D']) $ pd.DataFrame([s3,s4])
another_list = [1.0, 1, 'two', 2, 2.0]
if save_n_load_df(joined, 'joined_roll_tran.pkl'): del(tr_roll)
data = data.dropna(); data
converted = df.converted.value_counts()[1] $ total = df.converted.shape[0] $ proportion_converted = converted/total $ proportion_converted * 100
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $
math_data = results_dict['math_summaries'][0] $ print("Data is saved as a {} object.\nSubject answer {}% math problems correctly".format(type(math_data), $                                                                                           math_data.percent_correct))
num_unique_users = len(df.user_id.unique()) $ print("Number of unique users: {}".format(num_unique_users))
high_rev_acc_opps_net.drop([' DandB Revenue ', 'DandB Total Employees'], axis=1, inplace=True)
movie_df.head()
df_predictions_clean = df_predictions.copy()
precip_df = history_df['precipitation'] $ precip_df.describe()
warm = weather_all['Temp (deg C)'] > 25 $ warm
known_places = list(processed_tweets[["location", "lat", "lon"]].drop_duplicates('location')['location'])
df_vow['2017']
Ralston = weather.loc[weather.NAME == 'RALSTON RESERVOIR, CO US'] $ Ralston.head() $
images_clean.info()
data_for_model = pd.read_pickle('data_for_model') $ df_modeling = data_for_model.copy(deep=True)
for v in squares.iteritems(): $     print(v)
tail = df.copy() $
df['harvey160'] = df.text.apply(lambda text: pd.Series([x in text for x in HARVEY_WORDS_160]).any()) $ df['harvey92']  = df.text.apply(lambda text: pd.Series([x in text for x in HARVEY_WORDS_92]).any())
confidence  = clf.score(X_test, y_test) $ print("Confidence our SVR classifier is: ", confidence)
temp_df = filter_series(daily_df, station='TIMES SQ-42 ST', control_area='R148', unit='R033', device_address='01-00-01') $ plt.figure(figsize=(12,4)) $ plt.plot(temp_df['DATE'].tolist(),temp_df['D_ENTRIES'].tolist());
jail_census = pd.concat([january_jail_census, $                          feburary_jail_census, $                          march_jail_census]) $ jail_census
p_new = df2.query('converted == 1').user_id.count()/df2.user_id.count() $ p_new
import warnings $ warnings.simplefilter('ignore', FutureWarning) $ from pandas import * $ beijing = read_csv('Beijing_PEK_2014.csv', skipinitialspace=True)
raw_data = raw_data.sort_values(by=["datetime", "campaign_id", "campaign_spend"]) $ part1_flt = raw_data.loc[raw_data.campaign_id.map(lambda x: x in [1,2])] $ part1_flt[:5]
data = pd.read_csv("train-parking.csv", parse_dates={"datetime" : ["Date", "Time"]}) $ test = pd.read_csv("test-no-labels-with-id.csv", parse_dates={"datetime" : ["Date", "Time"]})
tweets1 = pd.read_json(r'C:\Users\shampy\Desktop\project\RedCarpetUp\socialmediadata-tweets-of-congress-november\2017-11-01.json') $ tweets1.shape
appointments.shape
for tweet in tweepy.Cursor(api.home_timeline).items(10): $     print(tweet.text)
calls_df[calls_df["phone number"]==5419196969].head()
coefs.loc['age', :]
import pandas as pd $ import sentlex $ import sentlex.sentanalysis
tlen = pd.Series(data=datos['len'].values, index=datos['Creado']) $ tfav = pd.Series(data=datos['Likes'].values, index=datos['Creado']) $ tret = pd.Series(data=datos['RTs'].values, index=datos['Creado'])
df.info()
df_test = pd.read_csv('loan_test.csv') $ df_test.head()
coins = pd.DataFrame(np.random.randint(0, 2, (100, 10)), $                      columns=list('abcdefghij')) $ coins.head()
S = Simulation(hs_path+'/summaTestCases_2.x/settings/wrrPaperTestCases/figure07/summa_fileManager_riparianAspenSimpleResistance.txt')
weather.isnull().sum()
DA_power_df.to_excel(data_folder_path + '/temp/day_ahead_merge_power.xlsx', index = False)
planevisits=get_planevisits(start_t, end_t, end_t1)
vc_M.head()
import matplotlib.pyplot as plt $ fig = plt.figure(figsize=(11,8)) $ ax1 = fig.add_subplot(111) $ ax1.plot(crimes_by_yr_month['year'], crimes_by_yr_month['crime_count'], label=1)
mod=sm.Logit(df_new['converted'] ,df_new[['intercept','CA','US']] )
fraud_data_updated.head()
z_score, p_value = sm.stats.proportions_ztest((convert_new, convert_old), (n_new, n_old), alternative='larger') $ z_score, p_value
from scipy.stats import norm $ z_significance = 1 - norm.cdf(z_score) $ critical_value = norm.ppf(1-(0.05)) $ critical_value, z_significance
corr_df = pd.DataFrame(columns = ['user','date','score','tod']) $ corr_df['user'] = user $ corr_df['date'] = dates $ corr_df['score'] = score $ corr_df['tod'] = tod
gs_from_model_under.score(X_train, y_train_under)
autos['date_crawled'].str[:10].value_counts(normalize = True, dropna = False).sort_index()
master_df.name.isnull().sum()
for col in x: $     x[col] = x[col].astype('category') $ y = cat_outcomes['outcome_subtype'].astype('category')
frame.sort_values(by=['a', 'b'])
temp_wide_df = pd.concat([grid_df, temp_df], axis = 1) $ temp_wide_df.head()
dayofweek = pd.DatetimeIndex(pivoted.columns).dayofweek
model.most_similar(positive=['human', 'crime'], negative=['party'], topn=1)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.country.value_counts()
if(any(df.isnull())):        #any computes True if any value in the return structure is True $     print("Rows does have null value") $ else: $     print("Rows does not contain any null value")
df_q = pd.read_sql(query, conn, index_col='Date_ID') $ df_q.head(5)
cust_demo.columns
test.head()
simple_resistance_simulation_1 = sim_ET_Combine['simResist(Root Exp = 1.0)'] $ simple_resistance_simulation_0_5 = sim_ET_Combine['simResist(Root Exp = 0.5)'] $ simple_resistance_simulation_0_25 = sim_ET_Combine['simResist(Root Exp = 0.25)']
first_commit_timestamp = pd.to_datetime('2005-4-16 22:20:36') $ last_commit_timestamp = pd.to_datetime('today') $ boolIndex = (git_log['timestamp'] >= first_commit_timestamp) & (git_log['timestamp'] < last_commit_timestamp) $ corrected_log = git_log[boolIndex] $ corrected_log['timestamp'].describe()
df5 = pd.DataFrame({'group': ['Accounting', 'Accounting', $                               'Engineering', 'Engineering', 'HR', 'HR', 'Library'], $                     'skills': ['math', 'spreadsheets', 'coding', 'linux', $                                'spreadsheets', 'organization', 'nunchucks']}) $ df5
clean_rates.sample()
df = read_311_data('nyc_311_data_subset-2.csv') $ df.info()
def Find_Missing_Values(dataframe): $     return dataframe.columns[dataframe.isnull().any()]
xmlData['waterfront'].replace({'':'0', 'Yes':'1'}, inplace = True) $ xmlData['view'].replace({'':'0'}, inplace = True) $ xmlData['waterfront'] = pd.to_numeric(xmlData['waterfront'], errors = 'raise') $ xmlData['view'] = pd.to_numeric(xmlData['view'], errors = 'raise')
extract_nondeduped_cmp = extract_all[f_remove_extract_fields(extract_all.sample()).columns.values].copy()
date= plt.plot_date(data=tweets_df, x="created_at", y="retweet_count", fmt="go") $ plt.xticks(rotation=60) $
p_val = (p_diffs > observation_mean).mean() $ p_val
noloc_df = noloc_df.append(df[(df.city.str.lower() == 'yyy') | (df.city.str.lower() == 'yyyy')]) $ df = df[~((df.city.str.lower() == 'yyy') | (df.city.str.lower() == 'yyyy'))]
t_open_resol[45]
rows = session.query(Adultdb).filter_by(education="9th").all() $ print("-"*100) $ print("Count of rows having education as '9th' before delete: ",len(rows)) $ print("-"*100)
del siim['Unnamed: 0']
df_countries = pd.read_csv('./countries.csv') $ df_countries.groupby('country').count()
contribs[contribs.calaccess_committee_id==1371855].info()
df_A.groupby('Gender').get_group('M')
full_data = pd.concat([train_small_sample[usecols], val_small_sample[usecols], test[usecols]])
mars_weather = Weather_soup.find('p',class_="TweetTextSize").text $ print(mars_weather)
data_features = compute_features(data, n_customers=11730)
titanic.groupby(['sex', 'class'])['survived'].aggregate('mean').unstack()
sum(df2.duplicated(['user_id'])) $ df2['is_duplicated'] = df2.duplicated(['user_id']) $ print(df2.query('is_duplicated'))
predictions_pdf = predictions.select("prediction", "predictedLabel", "GENDER", "AGE", "PROFESSION", "MARITAL_STATUS").toPandas()
np.mean(p_diffs > obs_diff)
AFX_url_str = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?' $ AFX_date_str = 'start_date=2017-06-08&end_date=2017-06-08' $ AFX_APIkey_str = '&api_key='+API_KEY $ AFX_X_06082017_r = requests.get(AFX_url_str+AFX_date_str+AFX_APIkey_str)
sample = pd.read_csv('../../../data/talking/train_sample.csv')
fi = rf_feat_importance(m, df)
print("Number of rows and features", tipsDF.shape)
tweet_archive_enhanced_clean.loc[200]
response = requests.get(url2)
df=pd.read_csv('data/df_new_comma_2.csv') $ df.head()
df2_countries['country'].unique()
sess.get_data('gbp curncy', 'fwd curve', index='Settlement Date')
df_clean.sample(10)
def cython1(s): $     output = np.empty(len(s),dtype='float64') $     _cython(s.values, com, output) $     return Series(output)
df_weekly['polarity_average'].describe()
df.to_csv('ab_updated.csv', index=False)
fuel_type = autos.groupby("fuel_type").mean() $ fuel_type['price_$']
df2 = df[((df.landing_page=='new_page')&(df.group=='treatment')) $         |((df.landing_page=='old_page')&(df.group=='control'))] $ df2.head()
df2 = df.loc[~((df.group == 'control')&(df.landing_page == 'new_page')),:].copy() $ df2 = df2.loc[~((df2.group == 'treatment')&(df2.landing_page == 'old_page')),:] $ df2.shape
session.query(Measurement.station,func.count(Measurement.station)).\ $         group_by(Measurement.station).\ $         order_by(func.count(Measurement.station).desc()).all()
trends_per_year_avg = defaultdict(float) $ for key,value in trends_per_year.items(): $     tr = trends_per_year[key] $     trends_per_year_avg[key] = sum(tr)/ float(len(tr))
import pandas as pd $ countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new['country'].value_counts()
dbCunha.head()
session_df['delta_recall'] = session_df['session_summary'].apply(normalized_item_delta_recall) $ session_df.head()
dc['YearWeek'] = dc['created_at'].apply(lambda x: "%d/%s" % (x.year, str(x.week).zfill(2))) $ tm['YearWeek'] = tm['created_at'].apply(lambda x: "%d/%s" % (x.year, str(x.week).zfill(2)))
retweets = lead[~lead["retweeted_screen_name"].isnull()] $ retweet_pairs = retweets[["id","screen_name","retweeted_screen_name"]].groupby(["screen_name","retweeted_screen_name"]).agg({"id":"count"}).rename(columns={"id":"Weight"}) $ retweet_pairs.reset_index(inplace=True) $ retweet_pairs.head()
old_page_converted = np.random.choice([0,1], size = n_old, p = [1-p_old, p_old]) $ print(old_page_converted)
serieschallenge5 = data.groupby(['KEY2','DATE'])['EntriesDifference'].agg(pd.np.sum) $
autos=autos.drop('offerType',1)
p_val = (p_diffs > p_diff_obs).mean() $ p_val
%matplotlib inline
stop_words_update = list(pipe_cv.get_stop_words()) $ stop_words_update.append('pron') $ stop_words_update.append('aa')
samp311.agency.value_counts()
plt.hist(p_diffs) $ plt.xlabel('Difference') $ plt.ylabel('Frequency') $ plt.title('Simulated difference of New Page and Old Page');
first_status = re_json.get('statuses', [{}])[0] $ print first_status['text']
pd.DataFrame(data2.groupby('name').joy.mean())
test_process('ADV19000601-V02-06-page13.txt')
test_rows = no_of_rows_in_the_data_set - no_line_up_cases_new_page_treatment $ print("test rows = {}".format(test_rows))
import pandas as pd $ csv_file = zip_file.open(csv.filename) $ df = pd.read_csv(csv_file, low_memory=False) $ csv_file.close() $ zip_file.close()
train_data.loc[(train_data.gearbox == 'manuell') & (train_data.vehicleType.isnull()), 'vehicleType'] = 'kleinwagen'
logregmodel = 'logregmodel.sav' $ pickle.dump(pipeline, open(logregmodel, 'wb'))
temp_fine = interp_spline(grid_lat, grid_lon)
csvFile = open('ua.csv', 'a')
import pickle $ filename = 'automl_feat.sav' $ pickle.dump(automl_feat, open(filename, 'wb'))
from PyCMLib import *
print(metrics.classification_report(testy, predict_y))
other = [1375, 10225] $ mask = df['other'].isin(other) $ df[(df.company == True) & (df.other == True) & (df.other[~mask])]
X_reduced = X[coefs_sorted.head(best_number_of_variables).index] $ X_reduced.shape $ X_reduced['health (real)'] = X['health (real)'] $ X_final_test_3 = X_final_test_2[[c for c in X_final_test_2.columns if "foreign born" not in c]]
customer_emails['Days Between'].describe()
df.info()
X_mice.shape
S_1dRichards.decision_obj.hc_profile.options, S_1dRichards.decision_obj.hc_profile.value
df.shape
from astropy.coordinates import EarthLocation, AltAz $ paris = EarthLocation(lat=48.8567 * u.deg, lon=2.3508 * u.deg ) $ crab_altaz = c2.transform_to(AltAz(obstime=now, location=paris)) $ print(crab_altaz)
type(conditions_counts)
plt.figure(figsize=(12, 6)) $ plt.plot(x, x ** 2) $ plt.plot(x, -1 * (x ** 2)) $ plt.title('My Nice Plot')
df2.head(5)
train.created.dtype
sumTable.index
tlen.plot(figsize=(16,4), color='r')
z_score, p_value
pd.value_counts(ac['Filer']).head(20)
pdf = pd.DataFrame(port_perf, index=returns.index, columns=[dwld_key + '_optimized']) $ pdf[-1:]
def read_tweet(status_object): $     import pprint $     pprint.pprint(vars(status_object)) $ read_tweet(clinton_tweets[0])
database = "pidata"                # e.g. "pidata" $ hostname = "172.20.101.81"         # e.g.: "mydbinstance.xyz.us-east-1.rds.amazonaws.com" $ port = 3306                        # e.g. 3306 $ uid = "pilogger"                   # e.g. "user1" $ pwd = "foobar"                     # e.g. "Password123"
Raw_Forecast["ID"] = Raw_Forecast.Date_Monday.astype( $     str)[:] + "/" + Raw_Forecast.Product_Motor + "/" + Raw_Forecast.Part_Number.str[:] $ Raw_Forecast.head(20)
df2.drop( 2893,axis=0,inplace=True)
df['is_application'] = df.application_date.apply(lambda x: 'No Application' if x == None else 'Application') $ df.head(5)
filepath
engine = create_engine("sqlite:///measurement.sqlite", echo=False)
import statsmodels.api as sm $ convert_old = df2.query('converted == 1 and landing_page=="old_page"').count()[0] $ convert_new = df2.query('converted == 1 and landing_page=="new_page"').count()[0] $ n_old = df2[df2['landing_page']=="old_page"].count()[0] $ n_new = df2[df2['landing_page']=="new_page"].count()[0]
inter = sm.Logit(df3['converted'], df3[['intercept', 'ab_page','US', 'UK','ab_US','ab_UK']]) $ inter_results = inter.fit() $ inter_results.summary()
soup_p = soup.find_all('p') $ soup_p[2].get_text()
df_json_tweets.head()
users_unique = df.user_id.nunique() $ users_unique
(p_diffs > obs_diff).sum()/len(p_diffs)
df.loc[dates[0],'A']
plt.scatter(house_data['sqft_living'], house_data['price'])
df = data.groupby('Date').sum()
line_up1=df.query('group=="treatment" & landing_page=="old_page"')['user_id'].count() $ line_up2=df.query('group=="control" & landing_page=="new_page"')['user_id'].count() $ line_up=line_up1+line_up2 $ print("The number of times the new_page and treatment don't line up is ="+str(line_up))
df_atr = pd.read_csv('features_column.csv')
df.iloc[4]
full_globe_temp
internet=pd.read_csv('internet.csv' ,  dtype = {'dates': str,'percentage': float})
df1['PCT_Change']=(df1['Adj. Close']-df1['Adj. Open'])/df1['Adj. Open'] $ df1['PCT_Change'].head()
uber = cb.organization('uber')
df = pd.read_csv('weather_data_austin_2010 (1).csv') $ cols = ['Temperature','DewPoint','Pressure'] $ df = df[cols]
def random_coord(): $     xrange = 50 $     cities_df["Lat"] = [np.random.uniform(-90,90) for x in range(xrange)] $     cities_df["Lon"] = [np.random.uniform(-180,180) for x in range(xrange)]
def train_classifier(X_train, y_train, X_test, y_test, classifier): $     train_features = CountVectorizer(tokenizer=tokenize_and_stem).fit_transform(X_train) $     classifier.fit(train_features_tokenized, y_train) $     return classifier
Aust_result.to_csv("Australia_Tweets.csv")
new_sample = pd.concat([new_sample,pd.get_dummies(new_sample['created_date'].apply(lambda x: x.month))],axis=1)
print(mnb_gd.best_params_, mnb_gd.best_score_)
from test_package import print_hello_function_container
pred.p2_dog.value_counts()
company = pd.read_csv('data/company.csv') $ company.head()
autos['date_crawled'] = autos['dateCrawled'].str[:10]
metadata['epsg'] = int(refl['Metadata']['Coordinate_System']['EPSG Code'].value) $ metadata['epsg']
posts_questions_df.head()
roc_auc_score(y_test, y_pred_lgr)
eth = pd.read_csv('data/eth-price.csv') $ eth.head()
getDictionary().head()
Train = TrainData_ForLogistic.copy() $ Test = TestData_ForLogistic.copy()
to_be_predicted_Day2 = 82.26004376 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
epsilon = 0.3 $ minimumSamples = 7 $ db = DBSCAN(eps=epsilon, min_samples=minimumSamples).fit(X) $ labels = db.labels_ $ labels
with open('data/chefkoch_03.json') as data_file:    $     chef03 = json.load(data_file) $ clean_new(chef03) $ chef03df = convert(chef03) $ chef03df.info()
new_page_converted.mean()
dfEtiquetas.info()
input_edge_types_DF = pd.read_csv(input_edge_types_file, sep = ' ') $ input_edge_types_DF
%%sql $ ALTER TABLE facts $ DROP COLUMN sr_type_code, $ DROP COLUMN sr_description, $ DROP COLUMN owning_department;
ab_data.shape
trigram_sentences = LineSentence(trigram_sentences_filepath)
sherpa = current.loc[df["By Name"] ==  "Sherpa "] $ sherpa
df = pd.read_sql_query('SELECT ComplaintType, Descriptor, Agency ' $                        'FROM data ' $                        'LIMIT 10', disk_engine) $ df
baseball_swing_action_type = 1 $ url = form_url(f'actionTypes/{baseball_swing_action_type}') $ response = requests.get(url, headers=headers) $ print_body(response)
flight = spark.read.json(jsonl_file_name) $ flight.printSchema()
import pickle $ filename = 'finalized_automl.sav' $ pickle.dump(automl, open(filename, 'wb'))
df2[df2.duplicated(subset="user_id", keep=False)] #duplicated() returns the index of of the cuplicate columns
df2.groupby(df2['landing_page']=='old_page').size().reset_index()[0].iloc[1]/df2['landing_page'].count()
feature_df = data_df.copy()
all_complaints.info()
trump['est_time'] = ( $     trump['time'].dt.tz_localize("UTC") # Setting initial timezone to UTC $                  .dt.tz_convert("EST") # Convert to Eastern Time $ ) $ trump.head()
poverty.tail(10)
data
orig_ct = len(dfd) $ dfd = dfd.query('in_pwr_47F_min >= 0.05 and in_pwr_47F_min <= 2.0') $ print(len(dfd) - orig_ct, 'eliminated')
transactions.head()
temp = ['low','high','medium','high','high','low','medium','medium','high'] $ temp_cat = pd.Categorical(temp, categories=['low','medium','high'], ordered=True) $ temp_cat
DataSet.head()
df_all_repaid_latest=pd.concat(dfs,keys=reporting_dates) $ df_all_repaid_latest.columns=['irr'] $ df_all_repaid_latest.to_clipboard() $
df2['ab_page'] = np.where(df2['group']=="control", 0,1) $ df2.head()
percip_stats = df_sorted['precipitation'].describe() $ df = pd.DataFrame(percip_stats) $ df $
print('tweets_df: ', len(tweets_df))
df.plot();
answer1_quandl = quandl.get('FSE/AFX_X', start_date='2017-01-01', end_date='2017-12-31') $ answer1_quandl.head()
df = pd.read_sql('SELECT * FROM actor WHERE actor_id = 172', con=conn) $ df
s.str[1]  # return char-1 (second char) of every item
offseason08["InorOff"] = "Offseason"
import nltk $ from nltk.util import ngrams $ from pylab import * $ from nltk.corpus import stopwords $ import re
loan_requests_indebtedness_web.to_clipboard()
df = arcgis.features.SpatialDataFrame().from_featureclass(collisions_path)
df_prep11 = df_prep(df11) $ df_prep11_ = pd.DataFrame({'date':df_prep11.index, 'values':df_prep11.values}, index=pd.to_datetime(df_prep11.index))
venues_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/venues.csv')['Body'].read().decode('utf-8') $ venues = pd.read_csv(StringIO(venues_csv_string), header=0)
pd.set_option('display.max_colwidth',100)
PADDING = 20 $ N_CLASSES = 6
materials_file = openmc.Materials([inf_medium]) $ materials_file.export_to_xml()
parcel_in = parcels.apply(within_area,axis=1) $ parcels = parcels.loc[parcel_in] $ print "Number of parcels wuthin area",sum(parcel_in),"out of",len(parcel_in) $ del parcel_ins $ parcels["grid"] = parcels.apply(build_grid_index,axis=1)
df['TUPLEKEY'] = zip(df['C/A'], df['UNIT'], df['SCP'], df['STATION'])
twtter_count1.columns = ['base_twitter_count'] $ twtter_count2.columns = ['base_twitter_count'] $ twtter_count3.columns = ['base_twitter_count']
csv_df[csv_df['timestamp'] == csv_df['timestamp'].min()]['url'].values[0]
cust_data.iloc[0:100:10,:]
iris.groupby('Species')['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm'].describe()
from rgf.sklearn import RGFClassifier, FastRGFClassifier $ clf_RGF_tf = RGFClassifier(max_leaf=240, $                            algorithm="RGF_Sib", $                            test_interval=100, $                            verbose=False,).fit(X_traincv_tf, y_traincv_tf)
exceldmh = dmh.Excel('http://www.fsa.usda.gov/Internet/' +  $                      'FSA_File/disaster_cty_list_ytd_14.xls')
df2.drop(1899, axis=0, inplace = True) $ df2.query('user_id == 773192').index.get_values()
pd.Series.loc?
twitter_archive_df[twitter_archive_df['tweet_id'].duplicated()]
df2.query('user_id =="773192"')
pd.isnull(pd.read_csv("../data/microbiome/microbiome_missing.csv")).head(20)
df.rename(columns={'CMPLNT_FR_DATE_YEAR':'YEAR', $  'CMPLNT_FR_DATE_MONTH':'MONTH', $  'LAW_CAT_CD':'CRIME_CAT', $  'OFNS_DESC':'OFFENSE', $  'CMPLNT_NUM_count':'COUNT'}, inplace = True)
converted_old=  df2.query('landing_page == "old_page"')['converted'].sum() $ converted_new= df2.query('landing_page == "new_page"')['converted'].sum() $ n_old = df2.query('landing_page == "old_page"').shape[0] $ n_new = df2.query('landing_page == "new_page"').shape[0]
ax = df.plot(title="Adj Close",fontsize=2) $ ax.set_xlabel('Time') $ ax.set_ylabel('Price') $ plt.show()
data = ['peter', 'Paul', 'MARY', 'gUIDO'] $ for s in data: $     print(s.capitalize())
sim_ret = pd.DataFrame(sigma*np.random.randn(ndays,nscen)+r, index=dates) $ sim_ret
from sklearn.cluster import AffinityPropagation, MeanShift, KMeans $ from sklearn.manifold import TSNE
old_page_converted=np.random.choice([0,1],n_old,p=[1-p_old,p_old])
model.doesnt_match("paris berlin london austria".split())
df2.tail() $
answer2={} $ for lis in answer1: $     answer2[answer1.index(lis)]=lis
overdue.shape
sgd = SGDClassifier() $ sgd.fit(X_train, Y_train) $ Y_pred = sgd.predict(X_test) $ acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2) $ acc_sgd
flight_cancels = flight_cancels.reindex(taxi_hourly_df.index) $ flight_cancels.fillna(0, inplace=True)
wed11 = facts_metrics[facts_metrics['dimensions_date_id']==(datetime.date(2018, 4, 14))]
def filter_images_and_links(text): $     return re.sub('!?\[[-a-zA-Z0-9?@: %._\+~#=/()]*\]\([-a-zA-Z0-9?@:%._\+~#=/()]+\)', '', text) $ filter_images_and_links('Lookat ![j kjds](wehwjrkjewrk.de), yes [iii](jlkajddjsla), and ' $                         '![images (17).jpg](https://steemitimages.com/DQmQF5BxHtPdPu1yKipV67GpnRdzemPpEFCqB59kVXC6Ahy/images%20(17).jpg)')
s4.value_counts()
count_bldg_opps = opportunities.groupby(['Account ID','On Zayo Network Status'])['Building ID'].count() $ count_bldg_opps = count_bldg_opps.reset_index()
trains_fe= pd.merge(trains, prods, on='product_id') $ trains_fe.head()
dtypes={'date':np.str,'dcoilqtico': np.float64} $ parse_dates=['date'] $ oil = pd.read_csv('oil.csv', dtype=dtypes, parse_dates=parse_dates) # opens the csv file $ print("Rows and columns:",oil.shape) $ pd.DataFrame.head(oil)
df = pd.DataFrame(np.random.randint(10,size=(10,4)), $ index = pd.date_range('1/1/2000', periods=10), $ columns = ['A', 'B', 'C', 'D']) $ df
archive_clean = pd.read_csv('archive_clean.csv') $ images_clean = pd.read_csv('images_clean.csv') $ popularity_clean = pd.read_csv('popularity_clean.csv')
from gensim.models import word2vec $ print("Training model...") $ model = word2vec.Word2Vec(sentences, workers=num_workers, \ $             size=num_features, min_count = min_word_count, \ $             window = context, sample = downsampling)
pd.Series(bnb.first_affiliate_tracked).isnull().any()
telecom3 = telecom2.drop(['mobile_number', 'churn'], axis=1) $ plt.figure(figsize = (20,20))        # Size of the figure $ sns.heatmap(telecom3.corr())
df_mes = df_mes[df_mes['improvement_surcharge']==0.3] $ df_mes.shape[0]
gain_df = pd.DataFrame() $ gain_df['infy'] = infy_df.gain $ gain_df['glaxo'] = glaxo_df.gain $ gain_df['beml'] = beml_df.gain $ gain_df['unitech'] = unitech_df.gain
saved_model.meta.available_props()
from sklearn import preprocessing $ le = preprocessing.LabelEncoder()
mt_df = hi_df[(hi_df['created_at'] >= datetime(2018, 1, 13, 18, 7, 0)) & $               (hi_df['created_at'] <= datetime(2018, 1, 13, 18, 21, 0))].sort_values(by=['created_at']) $ print("HI tweets between 18:07AM-18:21AM 2019-01-13: {}".format(len(mt_df))) $ mt_df.head()
recipes.iloc[135598]
test1.to_csv('../data/negTweetsFinal.csv')
daily_averages['2014'].head()
df.query('full_sq>10 and full_sq<1500')
print(jsonl_file_name)
tweets['fixed'] = [ftfy.fix_text(tweet) for tweet in tweets['text'].values.tolist()]
df["new_customer"] = (df["new_customer"] == 1).astype(int)
tw_clean = tw_clean.drop(tw_clean[tw_clean.expanded_urls.isnull()].index)
print pd.pivot_table(data=df, $                      index='date', $                      columns='item', $                      values='status', $                      aggfunc='sum')
for i in twitter_archive_rows: $     if i in list(twitter_archive_clean.index): $         print("Failed, {} still in dataframe".format(i)) $         break
df['intercept']=1 $ df[['control', 'treatment']] = pd.get_dummies(df['group']) $ df.head()
df_geo_unique.shape
!pip install wordcloud $
auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) $ auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET) $ api = tweepy.API(auth) $ public_tweets = api.home_timeline() $ data = pd.DataFrame(data=[tweet.text for tweet in public_tweets], columns=['Tweets'])
from sklearn.model_selection import train_test_split $ trip_data_sub = trip_data.ix[trip_data.Total_amount >0,:] $ trip_data_sub["tip_percentage"] = trip_data_sub.apply(lambda x: float(x["Tip_amount"])/float(x["Total_amount"]), axis = 1)
z_values, labels = vae_latent(nn_vae, mnist_train_loader) $ plt.plot(z_values[:,0], z_values[:,1], ".")
df_temperature = pd.read_csv('temperature_nyc.csv') $ df_precipitation = pd.read_csv('precipitation_nyc.csv')
data.plot()
c.execute('SELECT * FROM cities') $ print(c.fetchall()) $ conn.commit()
len(askreddit)
aldf = indeed1.append(tia1, ignore_index=True)
autos = pd.read_csv('autos.csv', encoding='Latin-1') $ autos = pd.read_csv('autos.csv', encoding='Windows-1251') $
for tz in pytz.all_timezones: $     print(tz)
cassession.columninfo('HMEQ_SCORED_GB')
tw_clean[tw_clean.text.str.startswith('RT @')].shape[0]
sns.distplot(questions_scores[:int(len(questions_scores)*0.99)])
df3 = df2.query('group == "control"') $ len(df3.query('converted == 1'))/len(df3) $
batting_and_salary = batting_df2.join(salary_df2)
html = browser.html $ jpl_soup = bs(html, 'html.parser') $ image_url = jpl_soup.find('a', {'id': 'full_image', 'data-fancybox-href': True}).get('data-fancybox-href') $ image_url
df['created_at']=pd.to_datetime(df['created_at'],format='%Y-%m-%d %H:%M:%S')
two_zips_df = cfs_df[(cfs_df.Zip==70117) | (cfs_df.Zip==70119)]
twitter_archive_clean['timestamp']= twitter_archive_clean['timestamp'].apply(convert_to_datetime)
df2.query("user_id == {}".format(duplicated_user.iloc[0]["user_id"])) $
obs_diff_newpage = df2.query('landing_page == "new_page"')['user_id'].count() / df2.shape[0] $ obs_diff_newpage
end = df_ad_airings_4['end_time'][0] $ print(end)
one_row = date_df.iloc[0] $ print(one_row) $ print(type(one_row['count'])) $ print(type(one_row['date'])) $ print(type(one_row['duration']))
df2_clean = df2_clean[~(df2_clean.jpg_url.duplicated())]
compound_final = compound_sub1.append(compound_sub2)
actuals = pd.DataFrame(y_test.data.numpy(), columns=['tavg_norm']) $ actuals['series'] = 'actual'
data.rename(columns = {'Long':'long', 'Lat':'lat'}, inplace = True)
bruins_pregame.to_csv('../../../../../CS 171/cs171-gameday/data/bruins_pregame.csv',   index=False) $ celtics_pregame.to_csv('../../../../../CS 171/cs171-gameday/data/celtics_pregame.csv', index=False) $ sox_pregame.to_csv('../../../../../CS 171/cs171-gameday/data/sox_pregame.csv',         index=False)
p_diffs = np.array(p_diffs) $ p_diffs
df_list = pd.read_html('https://www.bloomberg.com/markets/currencies') $ print ('Total Table(s) Found : ', len(df_list)) $ df = df_list[0] $ print (df)
stat, p, med, tbl = scipy.stats.median_test(df2["tripduration"], df3["tripduration"]) $ print p $ print tbl
from scipy.stats import norm $ print('Critical value:'+str(norm.ppf(1-(0.05)))) $
pd.period_range('2015-07', periods=8, freq='M')
df.head()
MATTHEWKW.head()
hp.get_sensors('water')[:3]
df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp']) $ df_clean['date'] = df_clean['timestamp'].apply(lambda time: time.strftime('%m-%d-%Y')) $ df_clean['time'] = df_clean['timestamp'].apply(lambda time: time.strftime('%H:%M'))
predictions_clean.info()
taxi_hourly_df.loc[index_missin_hrafter6_before2016, :] 
for row in selfharmm_topic_names_df.iloc[2]: $     print(row)
payment_plans_combined[(payment_plans_combined.fk_loan==34) & (payment_plans_combined.fk_user_investor==38)].to_clipboard()
from IPython.display import Image $ from IPython.core.display import HTML $ Image(url= "https://image.slidesharecdn.com/20151125hiratewebdb2015-151127073513-lva1-app6891/95/recommender-system-with-distributed-representation-4-638.jpg?cb=1448609942")
print 'Total amount for the unknown state excluding outliers: ', df[(df.state == 'YY') & (df.amount < 45000)].amount.sum() $ print 'Total amount for the unknown state: ', df[(df.state == 'YY')].amount.sum() $ print 'Total amount: ', df.amount.sum()
plate_appearances['Month'] = plate_appearances['game_date'].dt.month $ dummies = pd.get_dummies(plate_appearances['Month']).rename(columns=lambda x: 'Month_' + str(x)) $ plate_appearances = pd.concat([plate_appearances, dummies], axis=1) $ dummies = pd.get_dummies(plate_appearances['game_year']).rename(columns=lambda x: 'Year_' + str(x)) $ plate_appearances = pd.concat([plate_appearances, dummies], axis=1)
deep_learning_tweet1 = api.get_status(str(deep_learning_tweet_id)) $ type(deep_learning_tweet1)
gbm_model.varimp_plot() $ print(gbm_model.confusion_matrix(valid = True))
treatment_cr = df2.query('group == "treatment"').converted.mean() $ treatment_cr
coefs = pd.DataFrame(logreg.coef_[0], index = X.columns, columns = ['coef']) $ coefs['coef'] = np.exp(coefs['coef']) $ coefs.sort_values(by='coef', ascending = False, inplace=True) $ coefs.head(10)
run txt2pdf.py -o "NEW YORK HOSPITAL MEDICAL CENTER OF QUEENS  Sepsis.pdf"   "NEW YORK HOSPITAL MEDICAL CENTER OF QUEENS  Sepsis.txt"
from nltk.corpus import stopwords # Import the stop word list $ print (stopwords.words("english")) 
dtrain, dval, evals = xgb_md.get_train_eval_ds()
p_new = df2['converted'].value_counts()[1]/len(df2) $ p_new
filled = summed.fillna(summed.mean()) $ filled
shows3 = pd.read_csv('scraped_data5.csv')
df2.head()
d.head()
p_new = df2['converted'].mean() $ print (p_new)
deployment_details = client.deployments.create(name="k_flwr_test3", model_uid=model_guid)
apple.asfreq('D',method = 'pad') $ apple.head()
urls = GetPositionLinks.Position('passing').player_links(2017)
profits = pd.concat([co_buildings_latlong, tx_buildings_latlong, ga_buildings_latlong])
raw.location.values[0], raw.location.values[17]
print('Predicted bin = ' + str(rf.predict(new_df)[0]))
gender = records['Gender'].value_counts() $ gender['Missing'] = len(records) - gender['Male'] - gender['Female'] $ gender
kw_nfl = [ $     'fantasy football', $     ]
autos["gear_box"].unique()
mod_model = ModifyModel(run_config='config/run_config.yml', model='MESSAGE_GHD', scen='hospitals baseline', $                         xls_dir='scen2xls', $                         file_name='data.xlsx', verbose=False)
vip_crosstab_percentage.plot() $ plt.ylabel('Proportion') $ ax = plt.gca() $ ax.set_title("Finish Type by Proportion Over Time") $ plt.show()
bnbAx
feature_names = ('sepal length', 'sepal width', 'petal length', 'petal width') $ sfs1 = sfs1.fit(X, y, custom_feature_names=feature_names) $ sfs1.subsets_
(english_df.groupBy('hashtag') $            .count() $            .sort('count', ascending=False) $            .show(5) $ )
search = api.GetSearch("bitcoin") # Replace happy with your search $ for tweet in search: $     print(tweet.id, tweet.text)
df['np_counts'] = df['clean_text'].apply(lambda x: get_np_counts(x, df['profile']))
store_items.fillna(method='backfill', axis=0)
df['message_vec'] = vec2.fit_transform(df['message']) $
chambers.reset_index(inplace = True)
merged_portfolio.reset_index(inplace=True)
csvData['date'] = pd.to_datetime(csvData['date'], format = "%Y%m%dT%H%M%S", errors = 'raise')
shannon_petropavlovsk = - sum(np.log2(petropavlovsk_freq.values) * petropavlovsk_freq.values) $ shannon_petropavlovsk
df2.groupby('group').mean()
cat_outcomes['outcome_subtype'].value_counts()
dummy_categories = pd.read_csv('./data/category_dummies.csv')
irradiance_clear_df = pd.read_csv(irradiance_clear_file_path, index_col='Date', parse_dates=True, encoding='latin1') $ irradiance_cloudy_df = pd.read_csv(irradiance_cloudy_file_path, index_col='Date', parse_dates=True, encoding='latin1')
df5 = df4.set_index(pd.DatetimeIndex(df4['Date'])) $ df5 = df5[['BG']].copy() $ df5
pres_df['split_location_tmp'] = pres_df['location'].map(lambda x: x.split(',')) $ pres_df.head(2)
validation.analysis(observation_data, simple_resistance_simulation_0_5)
temp=convert_index_most_liked(tweep_most_likes_df) $ most_likes_df=pd.DataFrame(temp) $ most_liked_tweets=most_likes_df.sort_values(by='number_of_likes') $ most_liked_tweets.tail(5) $
def expand_counts(source_counts): $     return np.repeat(np.arange(len(source_counts)), source_counts) $
materials = load_openmc_mgxs_lib(mgxs_lib, openmoc_geometry)
df3 = pd.get_dummies(df2, columns=['landing_page']) \ $         .rename(columns={'landing_page_new_page': 'ab_page'}) \ $         .drop(columns='landing_page_old_page') $ df3['intercept'] = 1 $ df3.head()
df_users_6=df_users_5
year_string = 'extracted_data/*.csv'
print("{} is the proportion of users converted.".format(df['converted'].mean()))
train.head()
B2_NTOT_WINTER_SETTINGS = lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_ref_settings['ntot_winter'] $ lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_ref_settings['ntot_winter'].allowed_variables $ lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_ref_settings['ntot_winter'].settings.get_value('EK G/M', 22) $
converted_rate = df2['converted'].mean() $ print('converted_rate: ', converted_rate)
train_data['price'], Lambda = stat.boxcox(train_data['price'].clip(1475,13000)) $ Lambda
df.hist(column='income', bins=50)
d = datetime.datetime(2018, 11, 12, 12) $ for post in posts.find({"date": {"$lt": d}}).sort("reinsurer"): $     pprint.pprint(post)
p = df2.converted.mean() $ p
factors = web.DataReader("Global_Factors", "famafrench") $ factors
store_items.count()
autos = autos[autos["price"].between(1, 350000)]
Results_kNN500d.to_csv('soln_kNN500_dist.csv', index=False)
Remaining_columns
iowa['state_bottle_cost'] = iowa['state_bottle_cost'].str.replace('$', '').astype('float64') $ iowa['state_bottle_retail'] = iowa['state_bottle_retail'].str.replace('$', '').astype('float64') $ iowa['sale_dollars'] = iowa['sale_dollars'].str.replace('$', '').astype('float64')
unnormalized_q_i_j = np.exp(-pairwise_squared_distances_i_j) $ q_i_j = unnormalized_q_i_j / unnormalized_q_i_j.sum(axis=1) $ f, (left, right) = plt.subplots(1, 2, figsize=(8, 8)) $ plot_matrix(p_ij, ax=left, title='$P_{j|i}$', vmin=0, vmax=1) $ plot_matrix(q_i_j, ax=right, title='$Q_{j|i}$', vmin=0, vmax=1)
cust_count.head(2)
page.is_filepage()
loans[loans.fk_loan==loan_test].payback_state
df.groupby("used_promo")["cancelled"].mean()
x_axis = ('a','b','c','d','e','f','g','h','i','j')
print(len(countdf['user'].unique())) $ print(len(countdf['user'].unique())-len(count1df['user'].unique())) $ print(len(countdf['user'].unique())-len(count6df['user'].unique()))
match_results = afl_data_cleaning_v2.get_cleaned_match_results()
user
!wget https://www.hydroshare.org/django_irods/download/2474e3c1f33b4dc58e0dfc0824c72a84/data/contents/ogh_meta.json $ !wget https://www.hydroshare.org/django_irods/download/2474e3c1f33b4dc58e0dfc0824c72a84/data/contents/ogh.py
hp.search_sites(inhabitants=5)
def to_skycoord(coords): $     coord = str_merge(*coords) $     c = SkyCoord(coord, unit=(u.hourangle, u.deg)) $     return c
dir(tweet)
train=train.sort_values(ascending=[True],by=['air_store_id']) $ train.head()
ct = pd.crosstab(titanic['survived'],titanic['sex']) $ ct
df_goog.Date = pd.to_datetime(df_goog.Date)
from pywikibot import pagegenerators
df_tot.sample(10)
n_new, n_old
errors = model.anomaly(mtcars_filtered) $ errors.describe()
df_train.hist(figsize=(5,3),color = 'y') $ plt.xlabel('is_Churn') $ plt.ylabel('Count') $ plt.show()
sum(mask_H)
cov = cov.x $ cov[cov==0] = np.NaN $ plt.imshow(cov)
rdf.loc[pd.isna(rdf.speed_limit),'speed_limit'] = np.median(rdf.speed_limit) $ rdf.loc[pd.isna(rdf.aadt),'aadt'] = 0.0 # AKA, unknown, zero, etc. This will help differentiate major/minor roadways
for i, column_label in enumerate(df.columns): $     if len(df[column_label].apply(lambda x: type(x)).value_counts()) > 1: $         print(i, column_label) $         print(df[column_label].apply(lambda x: type(x)).value_counts()) $         print()
val = val.join(street_freq, on="Block", rsuffix='_fre')
day_of_week15.to_excel(writer, index=True, sheet_name="2015")
token_send_add_receiveAvg_month.columns = ["ID","sendReceiveCntAvg_mon"]
twitter_df = pd.read_csv('twitter-archive-enhanced.csv')
x = store_items.isnull().sum() $ print(x)
with open('./data/processed/X_train.pkl', 'rb') as picklefile: $     X_train = pickle.load(picklefile)
for key in new_words.keys(): $     new_words[key] = get_weighting(new_words[key])
gb.agg(['sum', 'count'])    $
df_breed.info()
joined=join_df(joined, stores_df, 'store_nbr')
type2017.head()
len(scowl)
tweets['fromStart'] = tweets['created_at'] - startDay
a = 10 $ print(f"2 x {a} = {2*a}")
df["DATE_TIME"] = pd.to_datetime(df.DATE + " " + df.TIME, format = "%m/%d/%Y %H:%M:%S") $ df
during.head()
trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn]) $ val_clas = np.array([[stoi[o] for o in p] for p in tok_val])
train_shifted.head(3)
clean_predictions.info()
tz_tmp = timezone_df.groupby('zone_id').agg({'gmt_hour':{'gmt_hour_avg':'mean'}}) $ tz_tmp.columns = tz_tmp.columns.get_level_values(1) $ tz_tmp.reset_index(inplace=True)
df2[['CA', 'UK','US']]=pd.get_dummies(df2['country']) $ df2.head(1)
pbls = subs.problem_id.unique() $ print "%d resolved problems" % len(pbls)
label = "Attrition" $ label
df2 = df.groupby(['month','account_id']).count() $ df2 = df2.reset_index('account_id') $
vaderAnalyzer = SentimentIntensityAnalyzer()
ratings_matrix = df_ratings.pivot_table(index=['user_id'], columns=['video_id'], values=['overall_rating_value']) $ ratings_matrix                                  
rent_db2 = rent_db[rent_db.price < 1000000] $ rent_db2.head()
train_b, valid_b, test_b = df_b.split_frame([0.7, 0.15], seed=1234) $ valid_b.summary()
ts = dfs[name][dfs[name].index > startDate][['v_sentiment','f_sentiment']] $ ts_count = ts['v_sentiment'].resample('D', how='count').rename('count').resample(sampling, how='mean')
print('Number of unique users in the dataset :: ',df2['user_id'].nunique())
cluster = model.transform(feature_sel).cache() $ cluster_pd = cluster.toPandas()
df["current_state"].value_counts()
more_grades = final_grades_clean.stack().reset_index() $ more_grades
jobs = pd.merge(jobs, fairshare[['Group', 'MAXCPUS']], how = 'left')
import pandas as pd $ pd.DataFrame.from_dict(sfs.get_metric_dict()).T
df.groupby('episode_id').size().head()
n_new = df2.query('group == "treatment"').count()[0] $ print(n_new)
pMean = np.mean([pnew, pold]) $ NewPage = np.random.choice([1, 0], size=new,p=[pMean, (1-pMean)]) $ new_avg = NewPage.mean() $ print(new_avg)
reviews_w_sentiment = pd.merge(reviews, pd.DataFrame(sentiments, columns=['sentiment_score']),left_index=True, right_index=True) $ reviews_w_sentiment['date']  = pd.to_datetime(reviews_w_sentiment['date']) $ len(reviews_w_sentiment)
df_con=pd.concat([df_1, Xt], axis=1)
import nltk.data $
df4 = df3.merge(df_countries, on = 'user_id')
d = {'a' : dates, 'b': increment, 'c': score}
training_active_listing_dummy.shape
plt.figure(figsize=(8, 5)) $ plt.hist(train_df.comments_lognorm); $ plt.title('The distribution of the property comments_lognorm');
count_liberia = grouped_months_liberia.count() $ count_liberia=count_liberia.rename(columns = {'National':'count_v_T'}) $
import pandas as pd $ import numpy as np
cust_demo.count() # no of non null values per column
testcols = ['site_name', 'is_mobile', 'is_package', 'channel', 'srch_adults_cnt', $                 'srch_children_cnt', 'srch_destination_type_id', 'hotel_continent'] $ dftest = pd.read_csv(dataurl+'test.csv.gz', sep=',', compression='gzip')
rf = RandomForestRegressor(n_estimators=100, random_state=1) $ rf.fit(X_train, y_train)
from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler $ from pyspark.ml.classification import RandomForestClassifier $ from pyspark.ml.evaluation import MulticlassClassificationEvaluator $ from pyspark.ml import Pipeline, Model
df_full[['CA', 'UK', 'US']] = pd.get_dummies(df_full['country']) $ logit_mod = sm.Logit(df_full['converted'], df_full[['intercept', 'ab_page', 'US', 'CA' ]]) $ results = logit_mod.fit() $ results.summary()
dfleavetimes.info()
df_2013.dropna(inplace=True) $ df_2013
n_new = df2.query("landing_page == 'new_page'").shape[0] $ n_new
lr.fit(X_train, y_train)
weather_warm.shape
autos[["price","odometer"]].head()
df2.query("group=='control'").converted.mean()
groceries / 2
uber_15["month"] = uber_15["Pickup_date"].apply(lambda x: int(x[5:7])) $ uber_15["day_of_month"] = uber_15["Pickup_date"].apply(lambda x: int(x[8:10])) $ uber_15["day_of_year"] = uber_15["Pickup_date"].apply(lambda x: x.split(" ")[0]) $ uber_15.head()
pd.date_range(start, periods=10, freq='2h20min')
px = px.asfreq('B').fillna(method='pad') $ rets = px.pct_change() $ cumulative_returns = ((1 + rets).cumprod() - 1)
df2.loc[:,df2.any()]
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_axial_chord_saddle[0][0])) $ perr = np.sqrt(np.diag(pcov_axial_chord_saddle[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
prs = pd.DataFrame(prs).set_index(['org', 'repo', 'number']).sortlevel() $ pr_comments = pd.DataFrame(pr_comments).set_index(['org', 'repo', 'number', 'date']).sortlevel() $ issues = pd.DataFrame(issues).set_index(['org', 'repo', 'number']).sortlevel() $ issue_comments = pd.DataFrame(issue_comments).set_index(['org', 'repo', 'number', 'date']).sortlevel()
%pprint
%matplotlib inline $ s1.plot.bar()
adopted_cats.loc[adopted_cats['Color']=='Brown Tabby/Tortie','Color'] = 'Brown Tabby Tortie' $ adopted_cats.loc[adopted_cats['Color']=='Calico/Brown','Color'] = 'Calico Brown' $ adopted_cats.loc[adopted_cats['Color']=='Brown Tabby/Brown','Color'] = 'Brown Tabby' $ adopted_cats.loc[adopted_cats['Color']=='Calico/Brown Tabby','Color'] = 'Calico Brown Tabby' $ adopted_cats.loc[adopted_cats['Color']=='Agouti/Brown Tabby','Color'] = 'Brown Tabby'
autos['price'].value_counts().sort_index(ascending=False).head(15)
data['Date'] = [x.created_at for x in tweets] $ data
S_distributedTopmodel.forcing_list.filename
prev1week_index_price = (df_byzone.time_stamp_local >= pd.datetime(2017, 12, 25)) & \ $                         (df_byzone.time_stamp_local <= pd.datetime(2017, 12, 31, 23)) $ df_byzone = df_byzone.loc[prev1week_index_price, :]
people.index.is_unique
from sklearn.model_selection import GridSearchCV $ gs = GridSearchCV(LogisticRegression(), {}, cv=cv5_idx, verbose=3).fit(X, y) 
L = [0, 1, 0, 1, 2, 0] $ display('df', 'df.groupby(L).sum()')
_ = ok.grade('q03b') $ _ = ok.backup()
twitter_master1.shape
df2.query("user_id==773192")
df2[df2.duplicated(subset = ['user_id']) == True]['user_id']
df_archive_clean[["timestamp","retweeted_status_timestamp"]]
education_dummies = pd.get_dummies(df.education) $ purpose_dummies = pd.get_dummies(df.purpose) $ data = pd.concat([df.submitted,education_dummies,purpose_dummies],axis=1)
lesson_date
adj_close = all_data[['Adj Close']].reset_index() $ adj_close.head()
def time_between(times): $     sorted_times = times.sort_values() $     time_between = sorted_times - sorted_times.shift(1) $     return time_between
start_date = "2017-10-01 00:00:00" $ end_date = "2017-12-31 23:59:59"
Google_stock.tail()
from sklearn.ensemble import RandomForestRegressor $ model = RandomForestRegressor() $ print ('Random forest') $ reg_analysis(model,X_train, X_test, y_train, y_test)
Google_stock = pd.read_csv('~/workspace/udacity-jupyter/GOOG.csv') $ print('Google_stock is of type:', type(Google_stock)) $ print('Google_stock has shape:', Google_stock.shape) $
print('Vader averages for Tesla Oct 16 tweets: ' + '\n' $       'Vader compound: ' + str(vader_compound_average) + '\n' $       'Vader negative: ' + str(vader_neg_average) + '\n' $       'Vader neutral:  ' + str(vader_neutral_average) + '\n' $       'Vader positive: ' + str(vader_positive_average) + '\n')  
sessions
try: $     cur_a.execute('DELETE FROM hotel WHERE hotel_id=1') $ except Exception as e: $     print('Exception: ', e)
vect = TfidfVectorizer(ngram_range=(2,4), stop_words='english') $ summaries = "".join(amanda_tweets['text']) $ ngrams_summaries = vect.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(20)
overweight_threshold = 30 $ people.eval("overweight = body_mass_index > @overweight_threshold", inplace=True) $ people
landing_a
data.tail()
chk = joined.loc[joined['state_hol']==1] $ holidays_df.loc[holidays_df['date']==chk.head()['date'].iloc[0]].head()
pnew = df2['converted'].mean() $ print(pnew)
sample = msftAC[:2] $ sample
s = pd.Series([1,2,3,4,5,4]) $ s
df["created"] = pd.to_datetime(df["created"]) $ df["last_event"] = pd.to_datetime(df["last_event"]) $ df.head()
logs.columns
len(joined_samp)
"University of Pretoria".split()
yhat_SVM = clf.predict(X_test)
(loan_requests.postcheck_data!='null').sum()
saveToFile = os.path.join(PROCESSED_PATH, 'Opportunities_with_Current_High_Revenue_Accounts_On_Net.csv') $ high_rev_acc_opps_net.to_csv(saveToFile, index = False)
'my string my'.find('x')
df3['US_new_page'] = df3['new_page']*df3['US'] $ df3['UK_new_page'] = df3['new_page']*df3['UK']
df2.query('user_id=="773192"')
words, values = list(zip(*new_words.items())) $ data = {'word':words, 'weight1':values} $ new_df = pd.DataFrame(data, columns=['word', 'weight1']) $ new_df.head(5)
number_of_commits = len(git_log.index) $ number_of_authors = git_log.author.nunique() $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
print(train.shape) $ print(test.shape)
fb_vec.todense()
df2[['ab_page', 'treatment']] = pd.get_dummies(df2['group']) $ df2.head()
wikis["enwiki"].plot();
zf = zipfile.ZipFile(path) $ df = pd.read_excel(zf.open('Sample_Superstore_Sales.xlsx')) $ df.head(5)
old_page_converted = np.random.choice(a=[0, 1], $                                       p=[1-p_old, p_old], $                                       size=n_old) $ old_page_converted
season11["InorOff"] = "In-Season"
datetimes = pd.date_range(DATA_STARTTIME, DATA_ENDTIME, freq='min') $ datetimes[0:10]
df2[df2.duplicated(['user_id'],keep=False)]
session = Session(engine)
[media['media_url'] for media in tweets[0]._json['entities']['media'] if media['type'] == 'photo']
%%bash $ grep -A 50 "build_estimator" taxifare/trainer/model.py
import io $ import requests $ import pandas as pd $ from zipfile import ZipFile
climate_df.describe() $
tcat_df.to_csv('cat_tweets.csv') $ tdog_df.to_csv('dog_tweets.csv')
therm_abs_rate = openmc.Tally(name='therm. abs. rate') $ therm_abs_rate.scores = ['absorption'] $ therm_abs_rate.filters = [openmc.EnergyFilter([0., 0.625])] $ tallies_file.append(therm_abs_rate)
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ WHERE twitter_day = 8 \ $ GROUP BY tag_text ORDER BY count DESC LIMIT 1;
["string"]  # <-- converts inteeger to a list
topic_geo_infomation = pd.DataFrame(l, columns=['date', 'placeId', 'place_value'])
nodereader = sample_df[['series_or_movie_name','offer_group_desc','first_genre','entity_type','content_age']]
pd.set_option('display.max_columns', None)  
dfstationmeanbreak = station_mean(dfbreakfast) $ dfstationmeanbreak.head(2)
df2['intercept'] = 1 $ df2[['to_drop', 'ab_page']] = pd.get_dummies(df2['group']) $ df2.drop(['to_drop'], axis=1, inplace=True) $ df2.head()
def get_historical_price_hour(coin, to_curr=CURR, exchange=EXCHANGE, limit=168, **kwargs): $     return get_historical_price_hour(*args, **kwargs, limit=1)
cig_data['tar'].value_counts()
print("The probability of converted in control group:", df2.query('group == "control"')['converted'].mean())
lq.head(1)
pd_data.payChannel = pd_data.payChannel.map(lambda x: x.split(','))
df.loc[df['offerType'] != 'Angebot'].shape
zc.head()
autos['odometer'] = autos['odometer'].str.replace("km","").str.replace(",","").astype(int) $ autos.rename({"odometer": "odometer_km"}, axis=1, inplace=True) $ autos["odometer_km"].head()
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA', 'US']]
ls_other_columns = df_with_metac_with_onc.loc[:, ls_both].columns
y_pred = svm_clf.predict(X_train_scaled) $ accuracy_score(y_train, y_pred)
filter_df = all_df.copy() $ filter_df.head(2)
mod = sm.tsa.statespace.SARIMAX(stock.close, trend='n', order=(1,1,3), seasonal_order=(0,1,1,80)) $ results = mod.fit() $ results.summary()
!!from time import sleep $ for i in range(0, 100): $     print(i) $     sleep(0.1) $ do_something()
dfSPY=pd.read_csv("data/SPY.csv") $ print(dfSPY.head())
plt.show()
df.sum(1)
cashflows_plan_origpd_noshift_all[(cashflows_plan_origpd_noshift_all.id_loan==210)&(cashflows_plan_origpd_noshift_all.fk_user_investor==25151)].to_clipboard()
df_h1b = df_data[df_data.visa_class=='H-1B'].drop('visa_class',axis=1)
labels.iloc[20:30] $ cust.iloc[-4:, 6:]
import findspark $ findspark.init() $ from pyspark.sql import SparkSession $ spark = SparkSession.builder.master("local[*]").getOrCreate()
labels=f'{PATH}train_v2.csv' $ n=len(list(open(labels)))-1 $ val_idxs=get_cv_idxs(n)
if 'Discharge' in dat.columns: $     daily_dat['Discharge']=wx.aggregate_time_with_threshold(dat['Discharge'], 'D', steps_in_period=96, func='sum', threshold=0.6)
knn_10.fit(X_train, y_train)
df.head(5)
intervention_train_extract = intervention_train.loc[telephone_or_auto, :]
station_count = session.query(func.distinct(Measurement.station)).count() $ station_count $
taxiData.head(5)
transit_df = transit_df.rename(columns=lambda x: x.strip())
columns_with_nan = intervention_train.isnull().any()
x.dropna()
import pandas as pd $ from datetime import datetime $ pd.set_option('display.max_colwidth', -1) $ additional_june_noms = 18
for i in cpi_all['Adjustment Type'].cat.categories.tolist(): $     print i    
countries.info()
csv_filename = "../data/scenarios.csv"
store_items.insert(4, 'shoes', [8,5,0]) $ store_items
df_new[['UK', 'US']] = pd.get_dummies(df_new['country'])[['UK', 'US']] $ df_new.tail(10)
ts
mismatched = ab.query('(group == "treatment" & landing_page == "old_page") | (group == "control" & landing_page == "new_page")')
shopping_carts.values
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new.groupby('country')['converted'].mean()
pd.merge(left,right, on='key')
dfrecent = dfdaycounts[dfdaycounts['created_date']> pd.to_datetime('2012-12-31')]
df = df.loc[(df.created_at >= "2017-12-04") & (df.created_at <= "2018-06-30")]
rf_v1
twitter_archive_master.name.value_counts()
df1.rating_denominator.value_counts()
n_old = df2[ df2['landing_page'] == 'old_page' ]['user_id'].count() $ n_old
import pandas as pd $ col_names = ['seqid', 'source', 'type', 'start', 'end', 'score', 'strand', 'phase', 'attributes'] $ df = pd.read_csv('downloads/Homo_sapiens.GRCH38.85.gff3.gz', sep='\t', header=None, names=col_names, compression='gzip', comment='#', low_memory=False)
df_gateways = pd.read_csv(read_inserted_table(dumpfile, tablename),delimiter=",",error_bad_lines=False) $ df_gateways.head(10)
del df1[ 'Debt Ratio']
sns.distplot(titanic.age)
lda_nra = lda_model(df_tweets, 'nra', 4, 4) $ print(lda_nra)
df_potholes = df[df['Descriptor'] == 'Pothole'] $ df_potholes.groupby(df_potholes.index.hour).apply(lambda x: len(x)).plot()
lv_workspace.load_indicators(subset = 'B')
df_tsv['tweet_id'] = df_tsv['tweet_id'].astype(str) $ df_tsv.info()
df['5/3/2014':'5/4/2014']
len(resdatasep)
random.seed(1234) $ new_page_converted = np.random.choice([1, 0], size = n_new, p = [p_mean, (1-p_mean)])
year3 = driver.find_elements_by_class_name('yr-button')[2] $ year3.click()
import statsmodels.api as sm
archive_df_clean['tweet_id'] = archive_df_clean.tweet_id.apply(str)
by_day_by_hour14 = uber_14.groupby(["day_of_week", "hour_of_day"]).size() $ by_day_by_hour14.head()
reviews.rename_axis('wines',axis='rows') $
building_pa_prc.describe(include='all')
df_DST=pd.DataFrame() $ df_DST['Datetime']=pd.to_datetime(datetime) $
archive_df_clean.sample(10)
utils.serialize_data(data)
bad_replies
class Motorcycle(Vehicle): $         return "motorcycle"
clients.to_csv('data/toggl-clients.csv')
def create_validation(df, start_date): $     return df.loc[(df['date'] >= pd.to_datetime(start_date) - relativedelta(days=0)) & \ $                   (df['date'] <  pd.to_datetime(start_date) + relativedelta(months=6))].index, \ $            df.loc[(df['date'] >= pd.to_datetime(start_date) + relativedelta(months=6)) & \ $                   (df['date'] <  pd.to_datetime(start_date) + relativedelta(months=12))].index
df_agg_bounces_rand.head()
pos_vectors = embeddings.loc[pos_words].dropna() $ neg_vectors = embeddings.loc[neg_words].dropna()
!wget -O LoanPaymentsDataClean.csv https://ibm.box.com/shared/static/dzbwzjdu2kqje8qkwso3bzxmggluaiu5.csv
session.head()
plt.hist(p_diffs); $ plt.axvline(x=calc_diff, color = 'red');
res_noise = df[df['Complaint Type'] == 'Noise - Residential'] $ res_noise['Descriptor'].value_counts()
X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.3, random_state=62)
 date_retweets = tweets_df[(tweets_df.created_at < "2018-04-23") & (~tweets_df.retweet_count.isnull())]
print(data['class'].mean()) $ print(data.shape)
spark.version
submissionxg =  pd.DataFrame(  modelXg.predict_proba(test_X  )[:,1], index = test['comment_id'] , columns = ["is_fake"] ) $ submissionxg.to_csv('submissionxg_Xg.csv')
twitter_df_clean.loc[191]
obs_old = df2.query('group == "control" and converted == 1').count()/df2[df2['group'] == "control"].count() $ obs_old[0]
sorted(LSI_model.trigram_phrases.vocab.items(), key=operator.itemgetter(1), reverse=True)[:100]
aci_service.get_logs()
autos["odometer_km"].head()
df = pd.DataFrame(rng.rand(1000,3), columns=['A', 'B', 'C']) $ df.head()
df2 = df2[df2['timestamp'] != '2017-01-09 05:37:58.781806']
founddocs = fs.find({"$text": { "$search": "Cundall" }}) $ print founddocs.count()
treatment_df = df2.query('group=="treatment"') $ converted_treatment = treatment_df.query('converted == 1') $ p_new_act = float(converted_treatment.user_id.nunique()/treatment_df.user_id.nunique()) $ print('Prob. new page actual {}'.format(p_new_act))
localFile = open("loan_test.csv", "wb") $ localFile.write(rawdata) $ localFile.close()
precip_df = pd.DataFrame(precip, columns=['date', 'prcp']) $ precip_df.set_index('date', inplace=True, ) $ precip_df.head()
idx = df_providers[  df_providers['id_num']==payments_total_yrs.loc[0,'id_num'] ].index.tolist() $ len(idx) $ df_small = df_providers.loc[idx,:] $ df_small = df_small.sort_values(['discharges'], ascending=[False]) $ df_small = df_small.reset_index(drop=True)
from six import iteritems
df.query("group == 'treatment' & landing_page != 'new_page'").count()[0]
X.shape
centroids_order3 = kmeans3.cluster_centers_.argsort()[:, ::-1] $ for i in range(n_clusters): $     print('\n\nTop terms in cluster {}:\n'.format(i)) $     for idx in centroids_order[i, :10]: $         print('\t{}'.format(features3[idx]))
from sklearn.model_selection import train_test_split $ X_train,X_test,y_train,y_test = train_test_split(X,y, random_state=42)
n_old=df[df['group']=='control'].shape[0] $ n_old
import datetime $ import matplotlib.pyplot as plt $ import matplotlib.dates as dates $ %matplotlib inline
forecast_data = (forecast_data $                  .merge(test_data) $                 ) $ forecast_data.head()
INQ2018.Create_Date.dt.month.value_counts().sort_index()
pd.merge(df1, df3, left_on="employee", right_on="name").drop('name', axis=1)
week14 = week13.rename(columns={98:'98'}) $ stocks = stocks.rename(columns={'Week 13':'Week 14','91':'98'}) $ week14 = pd.merge(stocks,week14,on=['98','Tickers']) $ week14.drop_duplicates(subset='Link',inplace=True)
df.head()
all_noms[all_noms["agency"] != "Foreign Service"]["nom_count"].sum() + additional_june_noms
file_wb = 'https://assets.datacamp.com/production/course_1531/datasets/world_ind_pop_data.csv'
d = np.stack([a, b]); d  #Concatenate along a new axis (e.g., vectors to matrix).
print(airquality_pivot.head())
nmf_cv, nmf_cv_data, nmf_tfidf, nmf_tfidf_data, lsa_cv, lsa_cv_data, lsa_tfidf, lsa_tfidf_data, lda_cv, lda_cv_data, lda_tfidf, lda_tfidf_data, combo_models_list = mf.gen_vectorizer_model_combos(cv_fitted, cv_data, tfidf_fitted, tfidf_data, n_topics=7)
Y, W = dmatrices('Y ~ W', data=df) $ mod = sm.OLS(Y, W) $ res = mod.fit() $ res.summary2()
n_new = df2.query('landing_page == "new_page"').group.count() $ n_new
S.decision_obj.simulStart.value = "2007-07-01 00:00" $ S.decision_obj.simulFinsh.value = "2007-08-20 00:00"
exploration_titanic.findupcol() $
from nltk.corpus import stopwords $ en_stopwords = stopwords.words('english') $ print('* Parsing texts...') $ spacy_docs = [doc for doc in nlp.pipe(texts, batch_size=1024, n_threads=8)]
X = df.drop('revenue', axis=1) $ y = df.revenue $ categorical_features = [column for column in X.columns if X[column].dtype == 'object'] $ X = multi_label_binarize(X)
altitude_only = search_df[search_df.altitude != ''].copy()
df[['name','year1']]
precipitation_df.describe()
df2[df2.duplicated(subset='user_id')]['user_id']
from sklearn.model_selection import train_test_split
pd.Series(pd.Categorical(iris["Species"])).sample(5)
m = Prophet(interval_width=0.95) $ m.fit(df);
df.message_likes_dummy.value_counts()
dates
elon.nlp_text = elon.nlp_text + ' ' $ elon = elon.groupby('date')['nlp_text'].sum() $ elon = pd.DataFrame(elon) $ elon.columns = ['elon_tweet']
h2o.init(min_mem_size="8G")
len(distinct_uname) $ asu=distinct_uname[0:180]
autos['abtest'].value_counts()
planets = ["Alderaan", "Chandrila", "Corellia", "Coruscant", "Duro", "Hosnian Prime", "Kuat"]
tweets_clean.to_csv('tweets_clean.csv', index=False)
lm.pvalues
ffr_recentM = ffr_recent.resample("M").first() $ vc_M = vc.resample("M").pad()
factors
with open(os.path.join(outputs, 'train_test_data_features.pkl'),'wb') as f: $     pickle.dump((train_data_features_tf, $                  test_data_features_tf, $                  train_data_features_tfidf, $                  test_data_features_tfidf),f)
converted_users = df.query('converted == 1').user_id.nunique() $ print(converted_users) $ proportion = converted_users / number_of_users $ print(proportion)
df['full_text'] = df['full_text'].apply(lambda x: x.replace('\r', ' ')) $ df['full_text'] = df['full_text'].apply(lambda x: x.replace('\n', ' ')) $ df['full_text'] = df['full_text'].apply(lambda x: x.replace('\t', ' '))
%time $ ddf_test5 = thoughts_seperateRollingAndConditionalIntoTwoDaskProcesses(test_5,curves,windows)
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US', 'ab_page']]) $ results =lm.fit() $ results.summary() $
shelter_cleaned_df.loc[shelter_cleaned_df['AnimalType'] == 'Cat']
nnew = df2.query('landing_page == "new_page"').count()[0] $ print ("The population of Newpage is : {}".format(nnew))
X_train[:3]
df.to_csv('/tmp/coindesk.csv')
weather_df.is_copy = False $ weather_df["Time of retrieval"] = [datetime.fromtimestamp(d) for d in weather_df["Time of retrieval"]] $ weather_df.head()
add_datepart(weather, "Date", drop=False) $ add_datepart(googletrend, "Date", drop=False) $ add_datepart(train, "Date", drop=False) $ add_datepart(test, "Date", drop=False)
df.corr()['ratio_win_home_team'].abs().sort_values(ascending = False)
len(pres_df['subjects'][0].split())
for e in sumToRG['deposit'].iteritems(): $     (f, t), v = e $     dot.edge(t, f, taillabel=str(v))
for key, grp in part1_flt.groupby('campaign_id'): $     print "{0: <20}{1}".format(key, percent_charged(grp.charged.values))
a_col = df['A'] $ df.sub(a_col, axis=0)
!mkdir data
pnew = df2.converted.mean() $ print(pnew)
hn.shape[0]
print("The minimum value of userID:") $ userArtistDF.agg(min("userID")).show()
io2.shape
temps_df.Missoula > 82
df['text_stemming'][25]
template_vars = {'chart' : figdata_png, $                  'chart_title' : 'Professores', $                  'professores_q_avaliaram' : 10, $                  'professores_q_nao_avaliaram' : 5, $                 }
d = datetime.datetime(2018, 11, 12, 12) $ for treaty in treaties.find({"date": {"$lt": d}}).sort("reinsurer"): $     pprint.pprint(treaty)
%%time $ df["sentences"] = df["sentences"].str.split()
Base = automap_base()
def drop_table(cur, table_name): $     cur.execute(sql)
options_frame.head()
grouped
df_track.track_name.nunique()
taxi_hourly_df.head()
temps_df.iloc[[1, 3, 5]].Difference
train['feature_list'] = train['features'].map(lambda x: ','.join(x)).str.lower() $ test['feature_list'] = test['features'].map(lambda x: ','.join(x)).str.lower()
data_l2_end = tmpdf.index[tmpdf[tmpdf.isin(DATA_SUM1_KEYS)].notnull().any(axis=1)].tolist() $ data_l2_end
NYT_train_raw = pd.read_csv("NYTimesBlogTrain.csv") $ NYT_test_raw = pd.read_csv("NYTimesBlogTest.csv")
minute_return.head()
df[ $     (df['datetime'] > datetime(2016, 1, 1)) & $     (df['datetime'] < datetime(2017, 1, 1)) $ ].set_index('datetime').resample('1w').aggregate('count').plot()
data_final.head(2)
y=df['destination'] $ y
import random $ L=[random.random() for i in range(100000)] $ %timeit L.sort()
orgs.hist(column = 'founded_year')
apple.sort_index(inplace=True) $ apple.head()
minute_return = bars.close / bars.open - 1 $ minute_return.describe()
year_ago_ppt = dt.date.today() - dt.timedelta(days =365) $ year_ago_ppt $
countries = pd.read_csv('countries.csv') $ countries.head()
hru_rootDistExp['rootDistExp']
df = pd.read_sql_query('SELECT Agency, Descriptor FROM data LIMIT 3', disk_engine) $ df.head()
import pandas as pd $ values=list() $ for looped in range(0,100): $     for i in range(0,len(y)): $         values.append(y[i]);
n_old=df2.query('landing_page == "old_page"').shape[0] $ n_old
words_hash_sp = [term for term in words_sp if term.startswith('#')] $ corpus_tweets_streamed_profile.append(('hashtags', len(words_hash_sp))) # update corpus comparison $ print('List and total number of hashtags: ', len(words_hash_sp)) #, set(terms_hash_stream))
temps_df['Difference'] = temps_df.Missouri - temps_df.Philadelphia $ temps_df
actual_payments.iso_date.unique()
new_page_rows = len(df2.query("landing_page == 'new_page'")) $ total_rows = df2.shape[0] $ print('New page probability :: ',new_page_rows/total_rows)
raw_full_df.building_id_iszero.value_counts()
geocoded_df.loc[idx,'Case.Duration'] = geocoded_df.loc[idx,'Judgment.Date']-geocoded_df.loc[idx,'Case.File.Date']
w.get_step_object(step = 2, subset = subset_uuid).indicator_data_filter_settings
cluster["Cluster_Labels_GaussianMixture"] = cluster_labels
wrd_api.info()
mypath = '/Users/Lucy/Google Drive/MSDS/2016Fall/DSGA1006/Data/csv_export' $ comps = pd.read_csv(mypath + '/competitors.csv')
data['suma'] = data['category_0_count'] + data['category_1_count'] + data['category_2_count'] + data['category_3_count'] + data['category_4_count']
total.build_hdf5_store(filename='mgxs', append=True) $ absorption.build_hdf5_store(filename='mgxs', append=True) $ scattering.build_hdf5_store(filename='mgxs', append=True)
updated_df = multi_col_lvl_df.copy() $ df2 = updated_df.xs('Beer', level='Category', drop_level=False).copy()  # .copy() is to avoid SettingwithCopyWarning $ df2[idx['Dollars', 'Store 1']] = "We ALSO changed this" $ updated_df.update(df2, join="left", overwrite=True, filter_func=None, raise_conflict=False) $ updated_df.head(10)
google_stock.head()
ac.dtypes
from sklearn.svm import SVR $ model = SVR(kernel='rbf', C=1e3, gamma=0.1) $ print ('SVR') $
ALLbyseasons.shape # This matches the height of our previous sample, confirming that we didn't lose any transactions.
y,X = dmatrices('Lottery ~ Literacy + Wealth + Region', data=df, return_type='dataframe') $ mod = sm.OLS(y, X)    # Describe model $ res = mod.fit()       # Fit model $ res.summary()         # Summarize model
image_pred.shape 
plt.scatter(mario_game.NA_Sales, mario_game.JP_Sales) $ print('The pearson correlation between the NA sales and JP sales is {}' \ $       .format(pearsonr(mario_game.NA_Sales, mario_game.JP_Sales)[0]))
print('The Best parameter set: %s' % gs_lr_tfidf.best_params_) $
db_grade
from newspaper import Article
years_string = "2015 was a good year, but 2016 will be better!" $ re.findall("[1-2][0-9]{3}", years_string)
df_arch.info() $
data['title_len'].value_counts()
nperiods = 3
df_predictions['img_num'].value_counts()
total_time = 14 * 24 * 60 * 60 $ n_iterations = int(total_time / dt) $ n_iterations
from sklearn.utils import resample $ np.random.seed(1) $ err = np.std([model.fit(*resample(X, y)).coef_ $               for i in range(1000)], 0)
customer = rename_df(customer, feature_map, 'PERSON_INFO') $ product = rename_df(product, feature_map, 'LC_PRODUCT_INFO') $ transaction = rename_df(transaction, feature_map,'LC_TX_INFO')
last_12_precip = session.query(Measurement.date, Measurement.prcp).\ $ filter(Measurement.date >= '2016-08-24').filter(Measurement.date <= '2017-08-23').order_by(Measurement.date).all() $ last_12_precip
control_df = df2.query('group=="control"') $ converted_control = control_df.query('converted == 1') $ p_old_act = float(converted_control.user_id.nunique()/control_df.user_id.nunique()) $ print('Prob. old page actual {}'.format(p_old_act))
def reassemble_plots(list_of_words):    $     bag_of_words = '' $     for i in list_of_words: $         bag_of_words += (i + " ") $     return bag_of_words
%run returns.py
submit = pd.DataFrame(test.id, index=test.index) $ submit['click'] = 0.0 $ submit['id'] = submit['id'].astype(np.uint64) $ submit.dtypes
df_anomalies = pd.read_csv('anomalies_vw.csv') $ df_anomalies = df_anomalies.drop('Unnamed: 0', 1) $ df_anomalies['period_start'] = pd.to_datetime(df_anomalies['period_start']) $ display(df_anomalies.head())
id_range = range(baseball.index.values.min(), baseball.index.values.max()) $ baseball.reindex(id_range).head()
sunspots.info()
parch = df_titanic['parch'] $ print(parch.describe()) $ parch.value_counts()
pd.timedelta_range(0, periods=9, freq="2H30T")
df_max = df.groupby('date').head(1) $ df_count = df.groupby(['date'] ,as_index=False).count() $ df_mean = df.groupby(['date'], as_index=False).mean()
id_dense = np.reshape(df_cat_stat.main_cat_id.tolist(), (-1, 1)) $ goal_dense = np.reshape(df_cat_stat.goal.tolist(), (-1, 1)) $ duration_dense = np.reshape(df_cat_stat["duration"].tolist(), (-1, 1)) $ print(id_dense.shape)
helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)
first_movie.strong
df3 = df2.merge(df_countries) $ df3.head()
tweet_json.info()
os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $ os.getcwd()
ds_complete_temp_CTD_1988['date'] = ds_complete_temp_CTD_1988.date.astype(int)
sn.distplot(train_binary_dummy['unique_action'])
%matplotlib inline
schiaparelli_dict = {} $ schiaparelli_dict["title"] = schiaparelli_title $ schiaparelli_dict["img_url"] = schiaparelli_full_img $ print(schiaparelli_dict) $ hemisphere_image_urls.append(dict(schiaparelli_dict)) $
df["past_percent_cancelled"] = df["past_percent_cancelled"].fillna(df["cancelled"].mean())
twitter.head(4)
cp = c1.sum()/c1.index.size $ cp
df_comment['datetime']=df_comment.created_at.apply(datetime.fromtimestamp) $ df_comment['datedatetime']=df_comment.datetime.dt.date
print(new_page_converted - old_page_converted)
dfFull['BsmtFinSF1Norm'] = dfFull['BsmtFinSF1']/dfFull['BsmtFinSF1'].max()
tlen.plot(figsize=(16,4), color='r');
X_test.columns
%config InlineBackend.figure_format='svg' $ plt.plot(x, np.sin(x)/x)
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key='+API_KEY\ $ +'&start_date=2018-05-18&end_date=2018-05-18' $ r = requests.get(url)
REPO_PATH = %pwd $ DATA_PATH = REPO_PATH + "/data/" $ DATA_STARTTIME = '2017-01-01 00:00:00' $ DATA_ENDTIME   = '2018-03-15 23:59:59'
data1.keys()
p_old = p_new $ print(p_old)
builder.where(F.col('endcustomerlinefixed_data')=="HERC RENTALS").select(builder.endcustomerlinefixed_data,builder.decision_date_time_data,builder.sales_acct_id_data,builder.sales_acct_id_savm).count()
loans_df.term.value_counts()
%load "solutions/sol_2_29.py"
jeff = Customer('Jeff Knupp', 1000.0)    #jeff is the object, which is an iinstance of the *Customer* class
bnbAx_lang = pd.get_dummies(bnbAx.language, prefix='lang_') $ bnbAx_lang.shape
temp_fine = np.zeros((13, 26, 59)) $ for i in range(13): $     temp_mon = temp[i] $     interp_spline = interpolate.RectBivariateSpline(sorted(lat), lon, temp_mon) $     temp_fine[i] = interp_spline(grid_lat, grid_lon)
df = frame_masher() $ df = df.loc[df['job_id'] == '2572']
BroncosBillsPct.loc[25441]['text30']
match = pattern.search('<a href="http://twitter.com/download/iphone" rel="nofollow">Twitter for iPhone</a>')
data_2 =pd.read_excel
df2.head(5)
mlb_national_west_league_id = 9 $ url = form_url(f'leagues/{mlb_national_west_league_id}/teams', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
for index, row in df.iterrows(): $     df.loc[index,'is_oc_company'] = row.postcodes in row.oc_postcodes
import pandas as pd $ df = pd.read_csv('purchases.csv') $ df
s3 = pd.Series({'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}) $ s3
df = df.dropna().copy()
conn_helloDB = create_engine("mysql+pymysql://{user}:{pw}@localhost/{db}".format(user="few", pw="123456", db="HelloDB"))
sum(df.query('group=="treatment"').landing_page == 'old_page') + \ $ sum(df.query('group=="control"').landing_page == 'new_page')
_ = ok.grade('q04b') $ _ = ok.backup()
donors[donors['Donor Zip'] == '606' ]['Donor State'].value_counts().head()
sqlContext.sql("select * from pcs").show()
y_test_advanced, x_test_advanced = for_analysis(main[main.season == main.season.max()], basic_tree) $ y_train_advanced, x_train_advanced = for_analysis(main[(main.season < main.season.max())], basic_tree)
X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.2, random_state = 2)
u = np.array([1, 2, 3])  #u has shape (3,). $ v = np.array([4, 5, 6, 7])  #v has shape (4,). $ w = u[:, np.newaxis]  #w has shape (3, 1); a matrix with 3 rows and one column. $ w*v  #(3, 1) x (4,); starting from the back, 4 and 1 are compatible, and 3 and 'missing' are too -> (3, 4).
dataset=pd.read_csv("yelp_reviews.csv")
print(a[0])  # Implemented by __getitem__ $ a[0] = "t"  # No can do; strings are immutable.
mini = stock.cummin()
trips_data['duration'].plot() # pandas will interact with matplotlib  - default is linechart
df2['intercept']=1 $ df2[['control','treatment']]=pd.get_dummies(df2['group'])
tweets_df.possibly_sensitive.describe()
file_name='precios/AAPL.csv' $ aapl = pd.read_csv(file_name) $ aapl
investment_dates = pd.merge(investments,rounds[['funding_round_uuid','announced_on']], on = 'funding_round_uuid')
df2['FlightDate'].head()
frames = [k_var, k_var1] $ k_var_concat = pd.concat(frames) $ g = k_var_concat.groupby(['blurb']) $ size = g.size() $ size[size > 1].head()
df.loc["a"] #using labels 
df.info()
conv_prob = df2.converted.mean() $ print('The probability of an individual converting regardless of the page they receive is {}.'.format(conv_prob))
data = pd.read_csv('../valencia-data-projects/valenbisi/vb_data/data.csv')
links_filtered=links.loc[ (links['value'] > 0.1) & (links['User1'] != links['User2']) ] $ links_filtered
df.isnull().sum()
control = df2[df2['group']=="control"]
data.loc[(100, slice(None), 'put'),:].iloc[0:5, 0:5]
count_vect = CountVectorizer(analyzer=clean_text) $ X_counts = count_vect.fit_transform(tweets_1['text'])
measure.dtypes
avgPurchU = train.groupby(by='User_ID')['Purchase'].mean().reset_index().rename(columns={'Purchase': 'AvgPurchaseU'}) $ train = train.merge(avgPurchU, on='User_ID', how='left') $ test = test.merge(avgPurchU, on= 'User_ID', how='left')
autos[['date_crawled','ad_created','last_seen']][0:5]
new_n = df2.query('landing_page == "new_page"').shape[0] $ new_p = df2.converted.mean() $ new_page_converted = np.random.choice([0, 1], size=new_n, p=[1-new_p, new_p])
Trump_week_android = Trump_week[Trump_week["source"] == "Twitter for Android"] $ Trump_week_android.shape
list_Media_ID = the_posts.keys()
ad_group_performance['Clicks'].sum()
df2.drop_duplicates(subset = 'user_id', inplace = True)
city_eco["economy"].cat.categories = ["Finance", "Energy", "Tourism"] $ city_eco
with open("data/nouns_twitter.json", 'w') as fh: $     json.dump(nouns, fh) $ with open("data/adjectives_twitter.json", 'w') as fh: $     json.dump(adjectives, fh)
sl_data = data[['goodbad','TRADES',  'AGE_groups']]
oppstage = segmentData[['lead_mql_status', 'opportunity_month_year', 'opportunity_stage']].pivot_table( $         index=['lead_mql_status', 'opportunity_month_year', 'opportunity_stage'], aggfunc=len, fill_value=0).reset_index()
df_schools.columns.tolist()
tweetsDf
age.loc['Bob':'Peggy']
df2['intercept'] = 1 $ df2[['ab_page','old_page']] = pd.get_dummies(df['landing_page']) $ df2 = df2.drop('old_page',axis=1) $ df2.head()
np.random.seed(1) $ model = models.LdaMulticore(corpus, id2word=dictionary, num_topics=10, workers=5,passes=200, eval_every = 1)
auto.info()
df.main_category.unique()
df_prod_wide = df_prod.pivot(columns='Cantons').resample('1 d').sum() $ df_prod_wide.head()
pd.Period('2012-05', freq='D')
engine = create_engine("sqlite:///hawaii.sqlite") $ conn = engine.connect()
options = {'Company': 'name', 'Person': 'name', 'Fund': 'name', 'Office': 'description'} $ draw(objects, options, physics = True)
df2.drop_duplicates(subset='user_id', keep='first', inplace=True)
display(df.head(),(df.dtypes))
results.summary() #Summary of our test results.
season_groups.groups
df_clean.drop(['doggo','floofer','pupper','puppo','expanded_urls', 'source','name','in_reply_to_status_id','in_reply_to_user_id','retweeted_status_id', $                'retweeted_status_user_id','retweeted_status_timestamp'], axis=1,inplace=True) $
model.wv.similarity('king', 'man')
autos["odometer"] = autos["odometer"].astype("int") $ autos = autos.rename(index=str, columns={"odometer": "odometer_km"})
mean_mileage = {} $ for brand in top_5_percent.index: $     mileage_mean = autos_pr[autos_pr['brand'] == brand]['odometer_km'].mean() $     mean_mileage[brand] = mileage_mean $ print(mean_mileage)
autos['registration_year'].sort_values().head(10)
df[0:1000]['AQI'].mean()
reg_df = df2.copy() $ reg_df['intercept'] = 1 $ reg_df['ab_page'] = (reg_df['group'] == 'treatment')*1 $ reg_df.head()
sp.str[-1]
set(votes)
df_clean3.loc[2038, 'text']
negTrend=pd.value_counts(negative.tweet_created_at).reset_index() $ negTrend=negTrend.sort_values("index") $ posTrend=pd.value_counts(positive.tweet_created_at).reset_index() $ posTrend=posTrend.sort_values("index")
autos["brand"].unique()
LUM.plot_time_diff(all_lum_binned,subject="VP3")
emb_szs
mask = data['listing_id'].apply(lambda x : 1 if str(x) in house_id else 0)
%%time $ params = {'n_clusters' : 13, 'random_state' : 4444,  'init' : 'k-means++', 'n_jobs' : -1} $ km = KMeans(**params) $ km.fit(tfidf_matrix)
tweets_clean.to_csv('twitter_archive_master.csv') $ images_clean.to_csv('images_archive_master.csv')
Gc=df_usa['GDP']/df_usa['Population'] $ df_usa['GDP/capita']=Gc $ df_usa.head()
df1.index.values
cityID = '52445186970bafb3' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chandler.append(tweet) 
pd.read_csv('twitter_archive_master.csv').head()
tweet_archive_clean.head()
fraq_fund_volume_m.columns = 'Fund'
data = fat.add_bollinger_bands(data, 'Close')
pkl_file = open('speeches_cleaned.pkl', 'rb') $ speeches_cleaned = pickle.load(pkl_file) $ unique_ids = list(speeches_cleaned['id'].unique())
check_wait_corr().groupby('wait').var()
df = table[0] $ df.columns = ['Parameter', 'Values'] $ df.head()
data_month = data_nonan_temp.groupby(['Month']) $ data_month.describe()
TEXT_DIR_LIST = ["T1", "T2", "T3", "T4"] $ LIBRARY_PATH = "/Users/seddont/Dropbox/Tom/MIDS/W266_work/w266_project/" $
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret)
29509*50
offseason07["InorOff"] = "Offseason" # This assigns an identifier as to whether this is inseason or offseason.
%%sql $ UPDATE facts $ SET Status_Change_Date_key = hour.hour_key $ FROM hour $ WHERE TO_CHAR(facts.status_change_date, 'YYYY-MM-DD HH24:00:00') = hour.hour; $
twitter_merged_data.hist(column='rating', bins=30); $ plt.title('Rating Histogram') $ plt.xlabel('Rating (Bins=30)') $ plt.ylabel('Count');
from sklearn.preprocessing import StandardScaler $
df_comb.head()
freqs = [(word, X_train_feature_counts.getcol(idx).sum()) for word, idx in count_vect.vocabulary_.items()] $ print sorted(freqs, key = lambda x: -x[1])[:20]
session.query(func.count(station.station)).all()
df_concat.columns #Prints a list of all column names.
from sklearn.model_selection import KFold $ cv = KFold(n_splits=10, random_state=None, shuffle=True) $ estimator = Ridge(alpha=2500) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
eta = therm_fiss_rate / fuel_therm_abs_rate $ eta.get_pandas_dataframe()
tweets_clean.floofer.value_counts()
pd.merge(total, total2, how='inner', on=['name'])
data = pd.read_csv('grantland.csv', encoding='windows-1252', index_col=0)
clf.predict(digits.data[-1:]) # Hoping to get 8 here...
p_actual_new = df2.query("landing_page == 'new_page'")['converted'].mean() $ p_actual_new
marsdf = pd.read_html(url) $ marsdf
s_n_s_epb_one.Date = s_n_s_epb_one.Date.str.replace('-',"")
S_distributedTopmodel.decision_obj.groundwatr.options, S_distributedTopmodel.decision_obj.groundwatr.value
import matplotlib.cm as cm $ dots_c, line_c, *_ = cm.Paired.colors
sm = morning_rush.iloc[:5][['longitude', 'latitude']] $ sm
likes['date'] = likes['timestamp'].apply( lambda x : dt.datetime.fromtimestamp(x))
data['a':'b']
df = pd.read_csv('ab_data.csv') $ df.head()
logodds.drop_duplicates().sort_values(by=['count']).head(10) $ logodds.drop_duplicates().sort_values(by=['count']).tail()
ppt = session.query(Measurement.date,Measurement.prcp).filter(Measurement.date.between ('2016-08-23','2017-08-23')).order_by(Measurement.date).all() $ ppt $
df.loc[df['blurb'].isnull(), 'blurb'] = '' $ df.loc[df['name'].isnull(), 'name'] = '' $ df['blurb_wlen'] = df['blurb'].str.split().apply(len) $ df['name_wlen'] = df['name'].str.split().apply(len)
dt_features['launched_at'] = pd.to_datetime(dt_features['launched_at'],unit='s')
verify_response
from bs4 import BeautifulSoup as soup ##BeautifulSoup$ $ from urllib.request import urlopen as uReq $ import requests
S_1dRichards.decision_obj.bcLowrSoiH.options, S_1dRichards.decision_obj.bcLowrSoiH.value
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_key, access_secret) $ api = tweepy.API(auth, $                  wait_on_rate_limit=True, $                  wait_on_rate_limit_notify=True)
%matplotlib inline $ %reload_ext autoreload $ %autoreload 2
df.head(2)
Pop_df['Year'] = 0.0 $ for i in range(26): $     Pop_df.Year[i] = Pop_df.Date[i] $
lq2015_combined.head(5)
L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error, $            name="L")
test_data.head()
gatecount_game_stations = gatecount[gatecount.name.isin(fenway_stations+td_stations)].copy() $ gatecount_game_stations['team'] = 'sox' $ gatecount_game_stations.ix[gatecount_game_stations.name.isin(td_stations),'team'] = 'bc' $ gatecount_game_stations = gatecount_game_stations[['team','service_day','day_of_week','service_time','service_datetime','entries']] $ gatecount_game_stations = gatecount_game_stations.groupby(['team','service_day','day_of_week','service_time','service_datetime']).sum().reset_index(drop=False)
df.game_type.value_counts()
import matplotlib.pyplot as plt $ import matplotlib.image as mpimg $ img=mpimg.imread('C://Files//man.png') $ plt.axis("off") $ imgplot = plt.imshow(img) $
%run billboard.py $ billboard = pd.DataFrame(billboard) $ billboard.columns = ['year','artist','track','time','date.entered','week','rank'] $ billboard.info()
abtest_list = list(set(train_data.abtest))
same_destination = trips[trips['start_station_name'] == trips['end_station_name']] $ same_destination = same_destination[same_destination['duration'] > 120] $ same_destination = same_destination[same_destination['duration'] < 3600] $ same_destination.sort_values(by='duration',ascending=True)
categorical = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', $                 'affiliate_provider','first_affiliate_tracked','signup_app','first_device_type', $                'first_browser']                           
Jams_data = [] $ Irregularities_data = []
posts.find_one({"_id": post_id_as_str}) # No result
%matplotlib inline $ counts_compare.sort_values('Count_Diff', ascending=False)
train.cust_id.value_counts().unique(),test.cust_id.value_counts().unique()
gps__interestlevel_df.to_csv('gps_coords_w_interestlevel_df.csv',index=False)
from sklearn.neighbors import KNeighborsClassifier $ knn_model = KNeighborsClassifier(n_neighbors=6) $ knn_model.fit(X_train, y_train)
twitter_archive_enhanced_clean.info()
df2.query('landing_page == "old_page"').user_id.nunique()
dates = pd.date_range('1950-01', '2013-03', freq='M'); dates
print(testing_active_listing_dummy[0:5],testing_active_listing_dummy.mean())
so_hashlist = pd.read_csv('email_hash_list_so_2012.csv', header=None)
df13 = pd.read_csv('2013.csv')
import kchart
np.exp(0.0099), np.exp(-0.0408)
grouped_dpt.groups # groups 
ExponentStories
df_drug_counts.AMANTADINE.plot(kind = 'bar', color = 'b')
print(date_price[0])
adopted_cats.loc[((adopted_cats['Tabby']==1) & (adopted_cats['Tortie']==1)),'Torbie']=1 
print("Variables not in test but in train : ", set(train_data.columns).difference(set(test_data.columns)))
twitter_archive_clean.loc[twitter_archive_clean['tweet_id']==666287406224695296,'rating_numerator']=9 $ twitter_archive_clean.loc[twitter_archive_clean['tweet_id']==666287406224695296,'rating_denominator']=10
len(coins_infund)
result.summary()
replies.columns
print(trump.favorite_count.sum()) $ print(" ") $ print(trump.retweet_count.sum())
df.head()
test_df.info()
bm2 = list(map(int, benchmark21)) $ print('benchmark2 score is: {}'.format(np.around(f1_score(actual1, bm2, average='weighted'), decimals=5))) $
df.drop(['OBJECTID', 'QUALIFIED', 'USECODE', 'LANDAREA', 'GIS_LAST_MOD_DTTM'], axis=1, inplace=True)
import re
import pandas as pd $ import numpy as np
%%time $ pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv')
archive_copy['text'][archive_copy['text'].str.contains('&amp;')]
%matplotlib inline
X.head() #to check the columns name of X featuers
data_final = data_final[data_final['countPublications'] > 1] $ data_final = data_final[data_final['countCollaborators'] > 1]
list(zip(features,(np.std(train_features_arr)*logisticRegr.coef_[0])))
model_data = df_users.copy()
print(dir(tweets[0]))
org_counts = org_name_counter(df) $ org_counts[(org_counts >= 3) & (org_counts < 5)]
portal_json_1 = "bigcz-cuahsi-water-slcretbuttecrk-2017-11-15.json"
del(df['Time']) $ df.head()
ab_df.user_id.nunique()
df_joined['intercept'] = 1 $ logit_mod = sm.Logit(df_joined['converted'],df_joined[['intercept','UK','CA']]) $ results_2 = logit_mod.fit() $ results_2.summary()
top_10_authors = git_log.author.value_counts().nlargest(10) $ top_10_authors
from preprocess import build_ds_meta
cc['loglow'] = np.log(cc['low']) $ plt.hist(cc['loglow']) $ plt.show()
print('Unique user id in df2: {}'.format(df2.user_id.nunique()))
with open("cache_{}.pkl".format(date), "wb") as f: $     logging.info("Saving query cache.") $     pickle.dump(query_cache, f, pickle.HIGHEST_PROTOCOL)
X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.3, random_state=3)
X2.shape $ X2 = X2.values.reshape([785024,3]) $ print(X2.shape)
from pytz import common_timezones $ common_timezones[:5]
X_mice_dummify, _ = custom_dummify(X_mice, 0.01)
from src.pipeline import pipeline_json $ pj = pipeline_json(my_json) $ X = pj.convert_to_df(scaling=True, filtered=True) $ y = pj.output_labelarray()
afl_data = outlier_eliminator(merged_df)
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ r= requests.get(url) $
future_df = pd.concat([dfs,future_dates_df])
first_result = results[0] $ first_result
!ls -lah | grep github_issues.csv
import requests $ import quandl $ import json $ stock = 'AFX_X' $
archive = pd.read_csv('twitter-archive-enhanced.csv')
mydict = {'date': date, 'open': open_price, 'high': high, 'low': low, 'close': close, 'change': change, 'volume': volume}
price = pd.DataFrame({ticker: data['Adj Close'] for ticker, data in all_data.items()})
data[1:3]
df2[df2['user_id'].duplicated()].index
df.rename(columns={'Indicator':'Indicator_ID'},inplace=True) $ df.info()
energy[energy.index < valid_start_dt][['load']].rename(columns={'load':'original load'}).plot.hist(bins=100, fontsize=12) $ train.rename(columns={'load':'scaled load'}).plot.hist(bins=100, fontsize=12) $ plt.show()
import datetime $ austin['started_on'] = pd.to_datetime(austin['started_on']) $ dayOfWeek={0:'Mon', 1:'Tue', 2:'Wed', 3:'Thu', 4:'Fri', 5:'Sat', 6:'Sun'} $ austin['weekday'] = austin['started_on'].dt.dayofweek.map(dayOfWeek)
import pandas as pd $ import numpy as np $ import datetime $ import matplotlib.pyplot as plt
df.diff()
print("Original m was", m_star, "and original c was", c_star) $ learn_rate = 0.01 $ c_star = c_star - learn_rate*c_grad $ m_star = m_star - learn_rate*m_grad $ print("New m is", m_star, "and new c is", c_star)
plt.figure(figsize=(10,5)) $ sns.countplot(auto_new.Country)
exploration_airbnb.structure()
df_archive_clean["dog_stage"] = df_archive_clean["doggo"].add(df_archive_clean["floofer"], $                               fill_value="").add(df_archive_clean["pupper"], $                                                  fill_value="").add(df_archive_clean["puppo"],fill_value="")
train_df = monthly_sales.pivot_table(index=['shop_id','item_id'], columns='date_block_num', values='item_cnt_month').fillna(0) $ train_df.reset_index(inplace=True) $ train_df.head()
5485578+2467347
cols_to_export = ["epoch","src","trg","src_str","src_screen_str","trg_str","trg_screen_str","lang","text"] $ mentions_df.to_csv("/mnt/idms/fberes/network/usopen/data/uso17_mentions_with_names_and_text.csv",columns=cols_to_export,sep="|",index=False)
mod = pd.get_dummies(machines['model']) $ machines['model'] = tuple(zip(mod.model1,mod.model2,mod.model3,mod.model4))
df_schema.query('table_name == "country"')
definition_1_details = client.repository.store_definition(training_filename, model_definition_1_metadata) $ definition_1_url = client.repository.get_definition_url(definition_1_details) $ definition_1_uid = client.repository.get_definition_uid(definition_1_details) $ print(definition_1_url)
sb.pairplot(cats_df[cats_df.columns[:]].dropna())
final_ = sample.merge(b_df,on="Date",how="outer")
learner.fit(3e-3, 4, wds=1e-6, cycle_len=10, cycle_save_name='adam3_10')
pd.DataFrame(list(zip(tx,ty))).plot.scatter( $     x=0, y=1, alpha=.4);
len(df2.query("landing_page == 'new_page'")) / df2.shape[0]
jobs.groupby('MAXCPUS').JobID.count().sort_values(ascending = False)
h5.close()
print(g_test.shape) $ print(p_test.shape) $ merged_test = pd.merge(left=g_test,right=p_test,how='inner',left_on='customer',right_on='customer',copy=True) $ print(merged_test.shape) $ merged_test.head()
print(store_items.dropna(axis=0)) $ print(store_items.dropna(axis=1)) $
import pandas as pd $ git_log = pd.read_csv('datasets/git_log.gz', sep='#', encoding='latin1', header=None, names=['timestamp','author']) $ print(git_log.head(5))
score_group_by_id = ins_named.groupby('business_id') $ score_group_by_id.agg(lambda x: max(x)-min(x))['score'].sort_values(ascending=False).head(3)
baseball.hr.cov(baseball.X2b)
df = df_list[0] $ print(df)
priors_product = pd.merge(priors, products, on='product_id') $ priors_product.head()
countries_df = pd.read_csv('./countries.csv') #reading countries data $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
d=soup.find('li').get_text() $ des
posts_by_sampled_authors_saved.schema
check_int('2016-11-06')[check_int('2016-11-06').number_repeat >= 7]
base_dict_by_place.head()
slices_filename = "SLICES-2018-03-23.json"
rent_db2.boxplot(column='price')
df = pd.DataFrame({'one' : [1., 2., 3.], 'two' : [3., 2., 1.]}); df
df.loc['Mon']
describe(officer_citations, 'badge_#')
pytrends.get_historical_interest(kw_list, year_start=2018, month_start=1, day_start=1, hour_start=0, year_end=2018, month_end=2, day_end=1, hour_end=0, cat=0, geo='', gprop='', sleep=10)
colArr=df['WHO Region'].get_values() $ colArr
from pyproj import Proj, transform $ inProj = Proj(init='epsg:4326') $ outProj = Proj(init='epsg:3857') $ geo_db['x'], geo_db['y'] = transform(inProj, outProj, list(geo_db['latitude']), list(geo_db['longitude']))
twitter_ar = twitter_ar[pd.isnull(twitter_ar['retweeted_status_id'])] $ twitter_ar = twitter_ar[pd.isnull(twitter_ar['in_reply_to_status_id'])] $ twitter_ar = twitter_ar[pd.isnull(twitter_ar['in_reply_to_user_id'])] $ twitter_ar = twitter_ar[pd.isnull(twitter_ar['retweeted_status_user_id'])] 
updated_experiment_details = client.repository.update_experiment(experiment_uid, experiment_metadata)
sum(treatment - control<p_diffs)/len(p_diffs)
import CSVTable $ import json $ import copy
dateCounts = df['Created Date'].value_counts().reset_index() $ dateCounts.columns = ['dates','countsOnDate'] $ dateCounts.head()
td_alpha = td ** (1/3) $ td_alpha = td_alpha / td_alpha.max().max()
pivot = (pd.pivot_table(data_predict,index=['store','item'],columns=[freq],values=['sales', 'predict', 'variance'],aggfunc=np.sum,margins=True, margins_name='Total') $          .swaplevel(axis=1) $          .sortlevel(0, axis=1, sort_remaining=False) $         ) $ pivot
(autos['date_crawled'] $  .str[:10] $  .value_counts(normalize=True,dropna=False) $  .sort_index() $ )
data_donald_replies['hashtag'] = alignments
weather.head(10)
df, y, nas, mapper = proc_df(joined_samp, 'Sales', do_scale=True) $ yl = np.log(y)
for the_year in range(2011,2016): $     How_many_DRGs_between_LOW_and_HIGH(the_year,1,8) $     print('\n'*2 ) $
field_details(df_SP)
residuals_linear = y - y_pred $ sns.distplot(residuals_linear)
bnbAx['target'] = np.where(bnbAx['country_destination']=='US', 1, 0)
conn = pymysql.connect(host='localhost', port=3306, user='root', password='pythonetl', db='pythonetl')
(null_vals>obs_diff).mean()
predictions = knn.predict(test[['property_type', 'lat', 'lon','surface_covered_in_m2']])
tuple_lst = [] $ for k, v in data_dict.items(): $     if v != None: $         for text in v: $             tuple_lst.append((k, text)) $
y_size.describe()
fraud_df.shape
df.info()
df.select(a == 0).show(5)
changes = html_tables("http://en.wikipedia.org/wiki/List_of_S%26P_500_companies").read()[1] $ changes.columns = ['date', 'ticker_add', 'security_add', 'ticker_rem', 'security_rem', 'reason'] $ changes = changes.iloc[2:] $ changes.iloc[6:10]
train_body_raw = traindf.body.tolist() $ train_title_raw = traindf.issue_title.tolist() $ train_body_raw[0]
!$TARGETCALIBPATH/generate_ped -h
locations = session.query(Measurement).group_by(Measurement.station).count() $ print(f"There are",locations,"stations.")
sent.columns = ['news_compound', 'news_neg', 'news_neu', 'news_pos'] $ stock = pd.merge(stock, sent, how='left', left_index=True, right_index=True)
print(autos.columns)  $ autos.columns = autos.columns.str.replace('([a-z0-9])([A-Z])', r'\1_\2').str.lower()
my_movie = sc.parallelize([(0, 500)]) # Quiz Show (1994) $ individual_movie_rating_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD) $ individual_movie_rating_RDD.take(1)
print "DAU for {}: {:,}".format(D0.isoformat(), dau)
cursor.execute(" select * from ticket_issue ")
df_new['intercept'] = 1 $ df_new[['CA','UK','US']]=pd.get_dummies(df_new.country) $ df_new[['new_page','old_page']]=pd.get_dummies(df_new.landing_page) $ df_new.head()
test_df.dropna(inplace=True)
Y_ols = pymc.Normal(name='Y_ols', mu=y_hat_ols, tau=np.sqrt(train_std), value=y_train, observed=True)
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 4) & (jobs.GPU == 0)].groupby('Memory').Wait.agg(['mean','median', 'count'])
df.head()
StockData.loc[StockData['Date-Fri'] == 1].head()
RF = RandomForestRegressor() $ RF.fit(team_names[predictor_cols],team_names.regular_occurrences)
len(df_characters), len(df.groupby('raw_character_text')['episode_id'].nunique().reset_index())
P_new=df2['converted'].mean() $ P_new $
agg_trips_data.head()
properati['zone'].value_counts(dropna=False)
stn_tempobs = session.query(Measurement.station, Measurement.tobs).\ $     filter(Measurement.station == busiest_stn).\ $     filter(Measurement.date >= data_oneyear).all() $ stn_tempobs
weather.TMAX.mean()
image_predictions.tail()
df_ec2 = df_cols[df_cols.ProductName == 'Amazon Elastic Compute Cloud'] # narrow down to EC2 charges $ df_ec2_instance = df_ec2[df_ec2.UsageType.str.contains('BoxUsage:')] #narrow down to instance charges $ df_tte = df_ec2_instance[df_ec2_instance['LinkedAccountName'] == target_account] $
train.pivot_table(values = 'Fare', index = 'From-To', aggfunc=np.mean)
turnaround_planes_df.count()
df = pd.read_json('twitter_store.json', lines=True) $ df = df.assign(user_location = df.user.apply(lambda d: d['location']))
loans_df['credit_line_days'] = (loans_df['issue_d'] - loans_df['earliest_cr_line']).dt.days
journalist_mention_gender_summary(journalists_mention_df[journalists_mention_df.gender == 'F'])
df2.query('group == "treatment"').converted.mean()
sum(df_h1b_mv_ft.pw_1*df_h1b_mv_ft.total_workers)/sum(df_h1b_mv_ft.total_workers)
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
active_users.agg([np.mean])
example1_df.registerTempTable("world_bank")
plt.plot(ds_ossm['time'],ds_ossm['met_salsurf_qc_executed'],'b.') $ plt.title('CP04OSSM, OOI QC Executed SSS') $ plt.ylabel('Salinity') $ plt.xlabel('Time') $ plt.show()
df.ix['2010-01-07']
random_integers.max()
kmeans = KMeans(n_clusters=2)  $ kmeans.fit(X)  
f0 = w.get_data_filter_object(step=0) $ f0.include_list_filter
df.head()
hate_pos = df_hate[df_hate['Polarity'] >= 0] $ hate_pos['Polarity'].count()
order_date = 'Order Date' $ df = pd.merge(Users, Orders, left_on = ['id'], right_on = ['id_user']) $ df['Order Date'] = pd.to_datetime(df['Order Date']).dt.date $ df['Reg_date'] = pd.to_datetime(df['Reg_date']).dt.date
print('Before:', end='') $ print(X[0:10, 0]) $ print('After:', end='') $ print(X_transform[0:10, 3])
variables = ['Date', 'TeamName', 'FG', 'FGA', 'FT', 'FTA', 'ScoreDiff', 'PF'] $ GameStats = get_season_game_stats()[variables] $ MSUStats = GameStats[GameStats['TeamName'] == 'Michigan State'] $ MSUStats = MSUStats.set_index(['Date', 'TeamName']) $ MSUStats.head()
df2['landing_page'].value_counts()["new_page"]/len(df2)
tweet_data.info()
def pandas_df(data): $     return pd.DataFrame(data) $ celgene_df = pandas_df(celgene_tweet_data) $ celgene_df_clean = celgene_df.copy()
np.ones(5)
def combine_body_and_title(df): $     return df.title.apply(lambda x: x.lower()) +' ' + df.filtered_body.apply(lambda x: x.lower())
!ls crossref-by-doi/*.json | wc -l
import xgboost as xgb $ from xgboost.sklearn import XGBClassifier $ XGB_model = xgb.XGBClassifier(objective='multi:softprob')
ab_file.drop(ab_file.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True) $ ab_file.drop(ab_file.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
test_data_scores(24, "gbm_max_depth_4", $                  model=gbm, model_specs = basic_tree, $                  x=x_train_advanced, y=y_train_advanced, $                  x_=x_test_advanced, y_=y_test_advanced)
import numpy as np $ print(np.corrcoef(ages,weights)) $ print(np.corrcoef(ages,weights)[0,1])
lmdict[lmdict.Positive != 0].head()
train.drop('Date of Birth', axis = 1, inplace = True) $ test.drop('Date of Birth', axis = 1, inplace = True)
iris.groupby('Species')['Species'].count()
with pd.option_context('display.max_rows', 150): $     print(news_period_df.groupby(['news_collected_time']).size())
convert_old,convert_new,n_old,n_new
df.info() 
df['converted'].mean() $ print("{:.0%}".format(df['converted'].mean()))
backers = databackers.pivot_table(index = 'backers', columns='successful',aggfunc=np.size) $ backers = backers.rename(columns= {1: 'Successful', 0:'Failed'}) $ backers[backers['Successful'] == backers['Successful'].max()]
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM__NearTop_CurveF_20180726 = test5result $ pickle.dump(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM__NearTop_CurveF_20180726, open( "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p", "wb" ) )
tweets_df.place.describe()
print(tfidf_svd_v2.explained_variance_ratio_.sum())
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
plt.barh(ds['title'], ds['quantity'], align='center') $ plt.show()
cohorts_size = cohorts['TotalUsers'].groupby(level = 'CohortGroup').first() $ cohorts_size.head()
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?" + \ $       "&start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY $ req = requests.get(url)
access_logs_df = access_logs_df.cache()
for i in forcast_set:    # iterating through the forcast set $     next_date = datetime.datetime.fromtimestamp(next_unix) # find the next date for the data $     next_unix+=one_day  #increment for finding next date $     df1.loc[next_date] =[np.nan for item in range(len(df1.columns)-1)]+[i]  #settinf up the date as a $
start = datetime.now() $ print((datetime.now() - start).seconds)
df.describe()
%time df.to_hdf(target, '/data')
df=pd.read_csv('C:\\Users\\Christopher\\Google Drive\\TailDemography\\outputFiles\\mapped-data-all_18-01-08_post_openrefine.csv')
xml_in[xml_in['venueName'].isnull()].count()
Labels = pd.read_hdf('//FS2.smpp.local\RTO\CIS-PD MUSC\decoded_forms\\form509.h5') $ watch_df = Labels[['SubjectCode','Q146_UTC']]
x[x==0]
tt_json.shape
url = "https://raw.githubusercontent.com/miga101/course-DSML-101/master/pandas_class/TSLA.csv" $ tesla = pd.read_csv(url, index_col=0, parse_dates=True) # index_col = 0, means that we want date to be our index $ tesla
x
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn; seaborn.set()
linear_model, error, short_dict = train_and_evaluate(1)
import numpy as np $ import tensorflow as tf
_,ax=plt.subplots(2,1,figsize=(20,20)) $ sns.countplot(calls_df["user"],ax=ax[0]) $ sns.countplot(calls_df["status"],ax=ax[1]) $
from sklearn.model_selection import cross_validate $ from sklearn.model_selection import cross_val_score
brand_list = list(set(train_data.brand))
extract_deduped_with_elms.loc[(~extract_deduped_with_elms.ACCOUNT_ID.isnull()) $                              &(extract_deduped_with_elms.LOAN_AMOUNT.isnull())].shape
s4 = Series([42, 1, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 4, 5, 6]) $
testdf = getTextFromThread(urls_df.iloc[2,0], urls_df.iloc[2,1]) $ testdf.head() $
integratedData.sort_values(by = 'date', ascending = 0, inplace = True) $ integratedData.reset_index(inplace = True) $ integratedData.drop('index', axis = 1, inplace = True) $ integratedData.head()
fname = "C://Users/dirkm/Meditationszeiten_ab_Okt_2017.csv" $ f1name = path.expanduser(fname) $ df = pd.read_csv(f1name) $ df.head()
for df in (joined, joined_test): $   df.loc[df['CompetitionDaysOpen']<0,'CompetitionDaysOpen']=0 $   df.loc[df['CompetitionOpenSinceYear']<1900,'CompetitionDaysOpen']=0
nb_classifier = nltk.classify.NaiveBayesClassifier $ nb_class = nb_classifier.train(v_train) $ print ("Accuracy of the model = ", nltk.classify.accuracy(nb_class, v_validation))
from pyspark.ml import Pipeline $ from pyspark.ml.classification import DecisionTreeClassifier $ from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer
for index in df_wrong_rating.index: $     df_enhanced['rating_10_scaled'][index] =  df_wrong_rating['rating_10_scaled'][index]
Daily_MET_1915_2011 = ogh.getDailyMET_livneh2013(homedir, mappingfile)
train = pd.read_csv('train.csv', sep=',') $ print(type(train)) $ train.head(15)
titanic.groupby(['sex', 'class'])['survived'].mean().unstack()
twitter_archive_clean.info()
dataCSV = urllib.request.urlopen("https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2015-09.csv").read() $ with open("green_tripdata_2015-09_1.csv", 'wb') as greenTaxiData: # remember to change the file name for saving later on $     greenTaxiData.write(dataCSV)
props
for i in compare[-1]: $     print('element: ', i) $     print('pre: ', data.view_count[i]) $     data.view_count[i] = data.suma[i] $     print('post: ', data.view_count[i])
lr_pipe.fit(X_train, y_train) $ lr_pipe.score(X_test, y_test)
sns.set(color_codes=True) $ for column in ['bottles_sold','volume_sold_lt']: $     plt.figure(figsize=(4,4)) $     sns.regplot(x=column, y='sale_dollars', data=df_2015)
s = pd.Series(todays_datetimes) $ s.diff().mean()
cursor = db_connection.cursor() $ cursor.execute(update_1_query) $ db_connection.commit() $ cursor.close() $ db_connection.close()
print(type(by_hour)) $ print(list(by_hour)[10]) $ print(list(by_hour)[0], list(by_hour)[0][1].shape)
df1 = tier1_df.reset_index() $ df1 = df1.rename(columns={'Date':'ds', 'Incidents':'y'})
col_names = ['Number', 'Number_TD', 'Temperature', 'Pressure', 'Order'] $ cdata = pd.read_csv(data_file, delim_whitespace=True, $                     index_col=False, names = col_names) $ cdata.sample(5)
data.datetime.min(), data.datetime.max()
psy_hx = pd.read_table("phx01.txt", skiprows = [1], na_values= ["-3","-5","-7","-9", $                                                                "-999","-888", $                                                                "-9999"])
!./flow --model cfg/tiny-yolo-voc-3c.cfg --train --dataset "../darkflow_data/MY20173c/JPEGImages" --annotation "../darkflow_data/MY20173c/Annotations" --epoch 5 --trainer adam --load bin/tiny-yolo-voc.weights --gpu 0.85
pgh_311_data.resample("Q").mean()
num_unique_user_ids = df2.user_id.nunique() $ num_unique_user_ids
Measurements = Base.classes.Measurements $ Station = Base.classes.Station $
features.shape
linked[linked=='44d0dc437936b13f7cea2f77053806bd']
import numpy as np $ import pandas as pd $ import statsmodels.api as sm $ from scipy import stats
df.loc[sq_index,'life_sq'] = np.NaN
file = 'https://raw.githubusercontent.com/mwentzWW/petrolpy/master/Zeus/Sample_Production/Sample_Prod_Data.xlsx' $ well_data = pd.read_excel(file) $ well_data.columns = [c.lower() for c in well_data.columns] $ well_data.info()
cashflows_act_investor_20150430_def=generate_cashflows_act_investor(actual_payments_combined,loan_fundings1,'2015-04-30',payment_plans_combined)
data = res.json()   
scores = cross_val_score(knn5, Xs, y, cv=skf) $ print 'Cross-validated KNN5 scores based on words:', scores $ print 'Mean score:', scores.mean()
linkNYC['lonlat'] = zip(linkNYC.longitude,linkNYC.latitude) $ linkNYC['geometry'] = linkNYC['lonlat'].apply(lambda x:shapely.geometry.Point(x))
list(df_.dropna(thresh=int(df_.shape[0] * .9), axis=1).columns) #set threshold to drop NAs
import datetime as dt
y = np.ravel(y) $ y
twitter_ar['rating_deno'] = 10 $ twitter_ar = twitter_ar[(twitter_ar.rating_denominator != 0)] 
AFX_close = [item[4] for item in AFX_X_2017['dataset_data']['data']] $ AFX_diff = [abs(x - AFX_close[i - 1]) for i, x in enumerate(AFX_close)][1:] $ Max_close_diff = max(AFX_diff) $ Max_close_diff = str(Max_close_diff) $ print("The maximum change between two consecutive days was $" + Max_close_diff + ".")
to_be_predicted_Day5 = 38.49420904 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
P_treatment = 17264/145310 $ print ("Probability that individual was in the treatment group,and they converted: %0.4f" % P_treatment)
new_converted = np.random.choice([1, 0], size=nnew, p=[pmean, (1-pmean)]) $ new_converted.mean()
df2['intercept']=1 $
data.isnull().any()
lr_df[['control', 'ab_page']] = pd.get_dummies(lr_df['group']) $ lr_df = lr_df.drop('control', axis = 1) $ lr_df['intercept'] = 1.0 $ lr_df.head()
abc3 =prediction['vova']*0.5 + prediction['rf']*0.2 + prediction['lgb_s']*0.3
g20 = g_all.loc[520:,'2000':'2016'] $ g20 = g20.reset_index(drop=True) $ g2000s = pd.concat([country, g20], axis = 1) $ print(g2000s.shape) $ print(g2000s.head())
len([prenatalScn for prenatalScn in SCN_BDAY_qthis.scn_age if prenatalScn < 0])
m = model_attention_applied_after_lstm(Tx = 20, n_x = 2)
testDataVecs = getAvgFeatureVecs(test_labeled_sentences, model, num_features)
np.mean(y_test > 0)
url1 = "https://api.propublica.org/congress/v1/members/R000435/votes.json"
df3["weekday"]=df3["Monday"]+df3["Tuesday"]+df3["Wednesday"]+df3["Thursday"]+df3["Friday"] $ df3["weekend"]=df3["Saturday"]+df3["Sunday"]
AAPL.shape  # returns 858 rows and 6 columns
portalcuahsi_json_pth = os.path.join(portal_pth, portal_json_1) $ with open(portalcuahsi_json_pth, 'r') as fj: $     portalcuahsi_json = json.load(fj)
print('Row zero, column zero:', data.iloc[0,0])
fcst_trips=fcst_trips.set_index('day') #set idex to be date $ for k in range(0,len(list(fcst_trips.columns))): $     fcst_trips.loc['2015-01-01':date_from, list(fcst_trips.columns)[k]] = np.nan
df2 = df.loc[(df["group"]!="treatment") & (df["landing_page"]=="new_page") ,  ["group","landing_page"]] $ df3 = df.loc[(df["group"]=="treatment") & (df["landing_page"]!="new_page") ,  ["group","landing_page"]] $ df2+df3
(d + pd.tseries.offsets.Week(weekday=4)).weekday()
len(train_data[train_data.fuelType == 'lpg'])
autos['price'].value_counts().sort_index(ascending=False).head(5)
data = spark.read.csv('sensor_data.csv',header=True)
ix = ((iris.Species == "setosa") | (iris.Species == "versicolor")) & (iris["Sepal.Length"]>6.5) $ iris.loc[ix.values,iris.columns[:2]]
percent_null_columns = [(c,df_train[c].count()/len(df_train)*100) for c in df_train.columns if df_train[c].isnull().any()] $ for t in percent_null_columns: $     print (t)
web_stats = {'Day':[1,2,3,4,5,6], $             'Visitors':[43,44,56,75,34,23], $             'Bounce_Rate':[67,45,56,76,78,56]}
sns.lmplot(x="age", y="happy", data=training, x_estimator=np.mean, order=1)
(autos['last_seen'] $          .str[:10] $          .value_counts(normalize=True, dropna=False) $          .sort_index() $         )   
short_urls = df_congress[ $     df_congress['link_url_long'].apply(ux.is_short) $ ]['link_url_long'].unique() $ len(short_urls)
import sys $ !conda install --yes --prefix {sys.prefix} pandas-datareader
def rank_performance(stock_price): $     if stock_price > 12: $         return 'Good' $     else: $         return 'NoBad'
df_loudparties = df[df['Descriptor'] == 'Loud Music/Party'] $ df_loudparties.groupby(df_loudparties.index.weekday).apply(lambda x: len(x)).plot(kind='bar')
%sql \ $ SELECT twitter.url,twitter.heats, twitter.user_id \ $ FROM twitter \ $ ORDER BY twitter.heats DESC LIMIT 1;
x = len(df2.query("landing_page == 'new_page'")) / df2.shape[0] $ print("{:.2%}".format(x))
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['acc'])
ax = grouped.sum().plot(kind = 'bar') $ ax.set_ylabel('total likes')
df.head(5)
len(idx_inst)
from preprocessing_pipeline import preprocessing $ with open("model/{}/word_embedder_500.pickle".format(version), "rb") as file: $     word_embedder = pickle.load(file)
archive_clean['name'].replace(archive_clean[archive_clean.name.str.islower()].name ,np.nan, inplace=True) $
df2 = df2.drop_duplicates(subset='user_id', keep='first') $ df2[df2.duplicated(subset='user_id')==True]
res=AgglomerativeClustering(linkage='ward', n_clusters=3).fit(crosstab_transformed)
matrix1 = np.array([(2,2,2),(2,2,2),(2,2,2)],dtype='float32') $ matrix2 = np.array([(1,1,1),(1,1,1),(1,1,1)],dtype='float32')
autos["date_crawled"].str[:10].value_counts(normalize=True, $                                    dropna = False).sort_index()
df_filtered = df[df["Rank"]==1]
sum(twitter_df_clean.rating_denominator > 10)
join_c.select('party_id_orig','aggregated_prediction','predicted').show(50)
df2['converted'].sum()/df2.shape[0]
set(df[df['author'] == 'send_nasty_stuff'].subreddit)
DataDescription
rejected.shape, approved.shape
print 'Total walking distance covered during the trip: %.2f km \nTotal steps covered during the trip: \t\t%d steps' % ( $     sum(walking_df['ttl_wal_distance']),sum(walking_df['ttl_steps']))
df
df2.query('group=="control"').converted.sum()/df2.query('group=="control"').count()[0]
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'],unit='s') $ git_log.describe()
rolling_mean = data['Close'].rolling(signal_lookback).mean() $ rolling_std = data['Close'].rolling(signal_lookback).std() $ data['Rolling Mean'] = rolling_mean $ data['Bollinger High'] = rolling_mean + (rolling_std * no_of_std) $ data['Bollinger Low'] = rolling_mean - (rolling_std * no_of_std)
sum(intervention_test.ORIGINE_INCIDENT.isnull())
fig=plt.figure(figsize=[5,8]) $ ax = fig.add_subplot(111) $ ax.bar(injuries_hour.date_time[:200], injuries_hour.injuries[:200],width=0.1) $ ax.xaxis_date()
filter_df['start_time'].min(), filter_df['start_time'].max() $
clean_vol = vol.between_time('10:00', '15:59') $ dev = clean_vol - df.reindex(clean_vol.index, method='pad') # be careful over day boundaries $ dev
datetime(-1200, 1, 1).toordinal()
pd.DataFrame(list(user_politics.values())).to_csv("outputs/user_politics-2017.01-07_2017.11.csv")
for x in range(0,9): $         hi_temps[x] = hi_temps[x].drop(hi_temps[x][(hi_temps[x]['Hi Temperature'] < 21.3) $                                                                | (hi_temps[x]['Hi Temperature'] > 23.3)].index) $         print (hi_temps[x].head(5))
samp = pd.DataFrame(samp, columns=['tweet_id','favorite_count','retweet_count']) $ samp.to_csv('tweet_json.txt')
unique_domains.query('num_authors >= 10').sort_values('num_authors', ascending=False)[['domain', 'num_authors']][0:50]
tbl2[['250beta','250alpha']]['2012':].plot()
df_twitter_copy.info()
df = pd.read_csv('upwork_df.csv', index_col=0) $ df['time'] = df['time'].apply(pd.to_datetime) $ df = df.dropna(subset=['title', 'desc']).reset_index(drop=True)
cleaned_df.drop('Unnamed: 0', axis=1, inplace=True)
os.chdir("C:/Users/mudmoham/Documents/pr/Call Records") $ calls_df=pd.read_excel("Call Records.xlsx",sheetname="Call Records",na_values=["NULL",""," "],keep_default_na=False)
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK_ind_ab_page', "UK","ab_page"]]) $ results = log_mod.fit() $ results.summary()
df_hi_temps.head()
x_values = df2.reset_index()[['intercept', 'ab_page']].values.tolist() $ X = df2[['intercept', 'ab_page']] 
print str(len(df.district_id[(df.district_size == 0) & (df.accounts_provisioned <> 0)].unique())) + ' districts w/ no size but w accounts' $ print str(len(df.district_id[(df.district_size == 0) & (df.accounts_provisioned == 0)].unique())) + ' districts w/ neither'
len(INQ2018)
total_control = df2[(df2['landing_page'] == "old_page")].count() $ print(total_control)
replies = twitter_archive_clean[twitter_archive_clean['in_reply_to_status_id'].isnull()==False] $ twitter_archive_clean.drop(replies.index, inplace=True) $ twitter_archive_clean.reset_index(inplace=True, drop=True)
infinity.head(10)
import matplotlib.pyplot as plt $ %matplotlib inline $ plt.plot(k_range, k_scores) $ plt.xlabel('Value of K for KNN') $ plt.ylabel('Cross-Validated Accuracy')
plt.figure() $ plt.plot(1.5, 1.5, 'o') $ plt.plot(2, 2, '*') $ plt.plot(2.5, 2.5, '*')
df.head()
rent_db.sort_values(by='price',ascending=False, inplace=True) $ rent_db.head(20)
df = pd.merge(train.to_frame(), stock["rise_in_next_day"].to_frame(), left_index=True, right_index=True)
new_page_converted = np.random.choice([1, 0], size=n_new, p=[page_new, (1-page_new)])
volumes = df[1::2].transpose().stack() $ volumes.index = index $ volumes.plot()
%matplotlib inline $ import pandas as pd $ !ls
stock['predict_grow'] = stock[['forecast', 'next_day_open']].apply(lambda x: 1 if x[0] - x[1] >= 0 else 0, axis=1) $ stock['true_grow'] = stock[['target', 'next_day_open']].apply(lambda x: 1 if x[0] - x[1] >= 0 else 0, axis=1)
assortativity = nx.degree_pearson_correlation_coefficient(multiG) $ print assortativity
latest_timelog.tail()
df[df['Complaint Type'] == 'Homeless Encampment'].index.month.value_counts().sort_index().plot()
df = df.drop(['mintempm', 'maxtempm'], axis=1) $ X = df[[col for col in df.columns if col != 'meantempm']] $ y = df['meantempm']
dftemp1 = dftemp[dftemp['Variable Name'].isin(['National Rainfall Index (NRI)']).where(dftemp['Value']> 950, dftemp['Value']<900)] $ dftemp2 = dftemp1[-6:] $ dftemp2['Year']     #printing the years for the given condition
names = d[d.name != 'None']
df.shape
data = pd.read_csv('2016-donations.csv', index_col=False)
processed_tweets_with_obs = processed_tweets_with_obs[processed_tweets_with_obs['obs'] != 'FAILED'] $ processed_tweets_with_obs = processed_tweets_with_obs.drop(processed_tweets_with_obs.index[[201]])
c.find_one(result.inserted_id)
frame
reddit_comments_data.groupBy('author').agg({'subjectivity':'mean'}).orderBy('avg(sentiment)').show()
num_rows = len(df)   $ print("The number of rows in the dataset is: {}".format(num_rows))
glm_binom_feat_2.accuracy(valid=True)
y_pred = lin_clf.predict(X_train_scaled) $ accuracy_score(y_train, y_pred)
def strip_insertintologs(text): $     try: $         return int(text.replace('INSERT INTO `logs` VALUES (',"")) $     except AttributeError: $         return text 
import patsy as pts $ y, x = pts.dmatrices('label ~ total_bill + tip + size + C(sex) + C(day) + C(time)', $                      data=tdf, return_type='dataframe') $ y = np.ravel(y) $ x.sample(5)
df_new[['US','UK','CA']] = pd.get_dummies(df_new['country']) $ df_new.head()
drive_time= [] #empty list of drive times $ for row in range(len(df)): $     element = driving_time(df.start_station_name[row], df.end_station_name[row]) #iterate through dataframe $     drive_time.append(element) #append to empty list $ df['drive_time'] = drive_time #create data series called walk_time $
convert_old,convert_new,n_old,n_new
plt.rcParams['axes.unicode_minus'] = False $ dta_6201.plot(figsize=(15,5)) $ plt.show()
pd_aux2.describe()
def get_gender(name): $     urlprefix = 'https://api.genderize.io/?name='  # Define url prefix $     r = requests.get(urlprefix + name.lower()) $     return r.json()['gender']
df_twitter_extract = pd.read_csv('data/df_twitter_extract.csv') $ df_twitter_extract_copy = df_twitter_extract.copy() $ df_twitter_extract_copy.head()
Imagenes_data
received_new_page = len(df2.query('landing_page == "new_page"')) $ received_new_page
%%time $ lmlist, c_v = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=train_texts, limit=10)
RF.score(X_train,Y_train)
major_cities_l1_features[0].attributes
msft_cum_ret = (msftAC / msftAC.shift()).cumprod() $ msft_cum_ret
df.groupby('author')['body'].count().sort_values(ascending=False)[:10]
print('\nMinimum Close value:', google_stock['Close'].min())
dataAnio.head()
m3 = np.around(m3,2) $ print("m3: ", m3)
time_to_take_a_break(start_time, scrape_time)
import numpy as np $ from sklearn.linear_model import LinearRegression $ from sklearn import linear_model $ x = hours[['hour']] $ y = hours.start
destination=table.find(text='Destination').find_next('td').text $ destination
frame2.values
len(nullCity2017)
trips.duration.describe()
client.repository.list_models()
tt_final.shape
from sklearn.ensemble import RandomForestClassifier $ from sklearn.linear_model import LogisticRegression
portfolio = pq.Portfolio() $ portfolio.add_strategy('bb_strategy', strategy) $ portfolio.run()
print(twitter)
new_model = gensim.models.Word2Vec(min_count=1)  # an empty model, no training $ new_model.build_vocab(sentences)                 # can be a non-repeatable, 1-pass generator     $ new_model.train(sentences, total_examples=new_model.corpus_count, epochs=new_model.iter)                       $
df.describe()
train_data.head().T
names.str.capitalize()
data = data.iloc[:,:7]
spark.table("orders").show(10)
archive_clean.info()
dataset.head(2)
df_group_by = pd.merge(df_group_by,df_members, on = 'msno', how = 'left')
iso_gdf_2.plot();
pulledTweets_df.sentiment_predicted_lr.value_counts().plot(kind='bar', $                                                            title = 'Classification using Logistic Regression model') $ plt.savefig('data/images/Pulled_Tweets/'+'LR_class_hist.png')
pd.isnull(twitter_archive_master).sum()
pd.get_dummies(tokens)
dictionary = {"a":1, "b":2} $ for key, value in dictionary.items(): $     print (key) $     print (value)
preds = model.predict(x_test)
fashion['predicted']
g_sorted1.head(10)
test.Page.value_counts().shape
sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $
tlen.plot(figsize = (16,4), color = 'r')
df['text_no_urls'] = remove_by_regex(df_1, re.compile(r"http.?://[^\s]+[\s]?"))['text']
ab_data = pd.read_csv('ab_data.csv') $ ab_data.head()
autos['last_seen'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
pred_probas_under = log_reg_under.predict_proba(X_test)
pandas.DataFrame( $     get_child_column_data(observations_node) + $     get_child_column_data(observations_ext_node) $ )
data.tail(10)
global batch_sz $ print(batch_sz) $ modelrest = trainModel(model, trX.reshape(-1,24,1), trY.reshape(-1,1), 1, batch_sz,2)
daily_returns.hist(bins=20) $ plt.axvline(mean, color='w', linestyle='dashed', linewidth=2) $ plt.axvline(std, color='r', linestyle='dashed', linewidth=2) $ plt.axvline(-std, color='r', linestyle='dashed', linewidth=2) $ plt.show()
get_freq(df=raw, colname="run")
stock['daily_gain'] = stock.close - stock.open $ stock['daily_change'] = stock.daily_gain / stock.open
y_pred = gnb.predict(X_clf)
feature_imp[:15]
active_creation_ratio =  [x/len(clean_users[clean_users['active']==1]) for x in list(clean_users[clean_users['active']==1]['creation_source'].value_counts())] $ active_creation_ratio
p_diff=p_new-p_old
passenger_ratings = np.concatenate(( $     stats.norm.rvs(loc=4.5, scale=0.3, size=500), $     stats.skewnorm.rvs(a=-1, loc=4, scale=1, size=500) $ )).clip(0.01, 4.99) $ df_passengers = pd.DataFrame({"rating": passenger_ratings, "id": range(len(passenger_ratings))})
start = "2005-04-01" $ end = "2018-06-31" $ trading_days = fk.get_monthly_last_trading_days(start=start, end=end) $ trading_days = sorted(trading_days)
z_score, p_value = sm.stats.proportions_ztest([17489, 17264], [145274, 145310],alternative='smaller') $ z_score, p_value
for name in invalid_names: $     twitter_archive_clean.loc[twitter_archive_clean.name == name ] = np.NaN
session = Session(engine) $ my_table = Table('Measurements', Base.metadata, $     Column("station", String, ForeignKey("Stations.station")), $     autoload=True,autoload_with=engine)
usb = CustomBusinessDay(calendar = USFederalHolidayCalendar()) $ usb
feat_image_url = "https://"+jpl_link+image_url $ print(feat_image_url)
devices_df['state'].unique()
tfv = TfidfVectorizer(ngram_range=(1,5), max_features=2000) $ X2 = tfv.fit_transform(clean_text2).todense() $ print X2.shape
selection1=df.loc[(df.group=='treatment')&(df.landing_page!='new_page')].count()+df.loc[(df.group!='treatment')&(df.landing_page=='new_page')].count() $ selection1[0]
control_mean = (df2.query('group == "control"')['converted']==1).mean() $ control_mean
from multiprocessing.dummy import Pool as ThreadPool 
crimes_df.T
df_ad_airings_2.dtypes $
data_sample['value'] = [i.replace(',', '') for i in data_sample['value']] $ data_sample['value'] = pd.to_numeric(data_sample['value'])
percent_quarter = (qtrclosePrice / qtrclosePrice.shift(4)) - 1
df_dummies = pd.get_dummies(df_uro[ls_other_columns]) $ dummy_colnames = df_dummies.columns $ dummy_colnames = [clean_string(colname) for colname in dummy_colnames] $ df_dummies.columns = dummy_colnames $ df_dummies.head()
stations = session.query(Measurement).group_by(Measurement.station).count() $ print(stations)
print('Original Grocery List:\n', groceries) $ print() $ print('We remove apples (out of place):\n', groceries.drop('apples')) $ print() $ print('Grocery List after removing apples out of place:\n', groceries)
pd.read_excel(cfg_fnames[0]).head()
own_star[own_star.duplicated('uniqueID', keep=False)].shape
len(column_datasets['train'].y)
full_data = full_data.replace(np.nan, 'null') $ one_hot = pd.get_dummies(full_data['gender']) $ full_data = full_data.drop('gender', axis =1) $ full_data = full_data.join(one_hot)
title_df = pd.DataFrame(vector_title.todense()) $ body_df = pd.DataFrame(vector_body.todense()) $ df = pd.concat([df, title_df, body_df], axis=1) $ df.head(2)
media_user_results_df = pd.DataFrame.from_dict(results_list) $ media_user_results_df.head(10)
autos.rename({'odometer': 'odometer_km'}, axis= 1, inplace=True)
geometry = [Point(xy) for xy in zip(g['X Coordinate (State Plane)'], g['Y Coordinate (State Plane)'])] $ crs = {'init': 'epsg:2263'} $ g_geo = gpd.GeoDataFrame(g, crs=crs, geometry=geometry)
df.shape
pca.explained_variance_ratio_
NB.fit(X_train.f, y_train.f) $ y_predict_bayes = NB.predict(X_test.f)
intervention_test['CRE_DATE_GZL'].min(), intervention_test['CRE_DATE_GZL'].max()
display(data.head(10))
fig1, axes1 = plt.subplots(2, 2, figsize=(12, 4), sharex=True, sharey=True) $ for i in range(2): $     for j in range(2): $         axes1[i, j].hist(np.random.randn(500), bins=15, alpha=0.4, color='c') $ plt.subplots_adjust(wspace=0.1, hspace=0.1) $
data = pd.DataFrame(session.query(Measurement.date, Measurement.prcp).filter(Measurement.date > '2016-08-23').all()) $ data.head()
n_new = df2.query('group == "treatment"').user_id.nunique() $ n_new
%matplotlib inline $ fig, ax =viewer.draw(nrn) 
!grep -A 20 "INPUT_COLUMNS =" taxifare/trainer/model.py
%%time $ M_NB_model.fit(X_train_term, y_train)
p_all = df2['converted'].mean() $ print('The probability of an individual converting is {}.'.format(round(p_all,4)))
metadata['bad_band_window1'] = refl.attrs['Band_Window_1_Nanometers'] $ metadata['bad_band_window1']
props.prop_name.value_counts()
obs_diff = df2[df2['landing_page']=='new_page'].converted.mean() - df2[df2['landing_page']=='old_page'].converted.mean() $ plt.hist(p_diffs); $ plt.axvline(obs_diff, c='red');
liquor2016_fy = liquor2016.SaleDollars.groupby(liquor.StoreNumber).agg(['sum']) $ liquor2016_fy.columns = ['Total_Sales'] $ liquor2016_fy.tail()
people_with_one_or_zero_collab['authorId'].nunique()
van_final[ben_final['pagetitle'].str.contains('/')]
group_by_kmeans = Cluster_df.groupby('KMeansLabels').mean() $ group_by_kmeans.head()
import pandas as pd $ import numpy as np $ surveys_df = pd.read_csv("data/surveys.csv") $ species_df = pd.read_csv("data/species.csv")
wkrng = pd.date_range('2012-10-25', periods=3, freq='W') $ wkrng
filtered_active_sample_sizes = active_sample_sizes.filter( $     active_sample_sizes.sample_size_1 > 10).persist()
ts_split_under = TimeSeriesSplit(n_splits=3).split(X_train)
baseball_h = baseball.set_index(['year', 'team', 'player']) $ baseball_h.head(10)
our_nb_classifier.last_probability
api.retweeters(id=981924371647393793,count = 10)
prods_user2=prods_user2.fillna(0) $ test_orders_prod3=pd.merge(test_orders,prods_user2, on='user_id') $ test_orders_prod3.head()
new_page_converted = np.random.binomial(1, p, n_new)
print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)
new_eng = pd.merge(df_user_engagement,df_eng_mult_visits, on="user_id") $
data[1:3]
graf_counts['AFFGEOID'].dtypes
p_old = df2['converted'].sum() / df2.shape[0] $ p_old
def calc_temps(start_date, end_date): $     return session.query(func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)).\ $     filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all() $ print(calc_temps('2017-03-04', '2017-03-11'))
dd.plot(sample_field='Subject',gui='jupyter')
print(count_2010, count_2011, count_2012, count_2013, count_2014, count_2015, count_2016, count_2017)
predictions_clean['p1'] = predictions_clean['p1'].str.replace('_', ' ') $ predictions_clean['p2'] = predictions_clean['p2'].str.replace('_', ' ') $ predictions_clean['p3'] = predictions_clean['p3'].str.replace('_', ' ')
maxFVWordLength = 300 $ maxSentenceWordLength = 30
accuracy_eval = pipeline.predict(X_eval) $ accuracy = np.mean(np.array(accuracy_eval['PredictedLabel'].astype(int)) == y_eval) $ print('evaluation accuracy: ' + str(accuracy))
logit_co = sm.Logit(df_reg2['converted'], df_reg2[['country_UK', 'country_US', 'intercept']]) $ result_co = logit_co.fit()
from scipy.stats import norm $ norm.cdf(z_score), norm.ppf(1-(0.05))# Tells us what our critical value at 95% confidence is
head = pd.Timestamp('20150617') $ tail = pd.Timestamp('20150618') $ df=sensor.get_data(head,tail,diff=True, unit='W') $ charts.plot(df, stock=True, show='inline')
order_with_data = pd.merge(order_item,product, how='left', left_on=['MATNR'], right_on =['MATNR']) $ paid_success_with_data = order_with_data[order_with_data['STATUS'] == "PAID_SUCCESS"] $ paid_success_with_data.head()
base_dict = collections.defaultdict(int, {k: d1[k] + d2[k] + d3[k] for k in set(d1) | set(d2)| set(d3)})
events.where( col("user_id").isNull() ).show()
support_or_not["text"][11]
print(iris_mat[20,0:2])
country.name.unique()
taxi_hourly_df.index.max()
( temp_cat.min(), temp_cat.max(), temp_cat.mode() )
df_new.head(5)
f_lr_hash_modeling2 = (prediction_modeling2 $                        .withColumn('f_lr_hash_inter2_2p18_noip', get_pred_udf(col('probability'))) $                        .select('id', 'f_lr_hash_inter2_2p18_noip')) $ f_lr_hash_modeling2.show(3)
df_columns[df_columns['Complaint Type'].str.contains('Firework')]['Day in the year'].value_counts().head() $
pd.Series([1,2,3,np.nan]).mean()
obs_diff = df2['converted'][df2['group'] == 'treatment'].mean() - df2['converted'][df2['group'] == 'control'].mean()
pred_probas_over
gdax_trans_btc['Balance'] = gdax_trans_btc['Trade_amount'].cumsum();
sp['close_chg'] = ((sp.day_ago_close - sp.week_ago_close)/sp.week_ago_close) $ sp['open_chg'] = ((sp.day_ago_open - sp.week_ago_open)/sp.week_ago_open) $ sp['high_chg'] = ((sp.day_ago_high - sp.week_ago_high)/sp.week_ago_high) $ sp['low_chg'] = ((sp.day_ago_low - sp.week_ago_low)/sp.week_ago_low) $ sp['vol_chg'] = ((sp.day_ago_vol - sp.week_ago_vol)/sp.week_ago_vol)
mean_mileage = {} $ for brand in top_brands.index: $     mean_mileage[brand] = autos.loc[autos['brand'] == brand, 'odometer_km'].mean() $ print(mean_mileage)
df2_dup = df2[df2.user_id.duplicated(keep= False)] $ df2_dup
sns.boxplot(x = 'loan_status', y = 'loan_amnt', data=train);
small_f = urllib.request.urlretrieve (ml_1m_url, ml_1m_path)
active_station_temps = session.query(func.max(Measurement.tobs), \ $                                      func.min(Measurement.tobs), func.avg(Measurement.tobs)).\ $                                     filter(Measurement.station == 'USC00519281').all() $ active_station_temps
os.mkdir('code')
df_likes = pd.merge(df_likes,df_authors,left_on='author_id',right_on='authorId') $ df_likes.drop('authorId',inplace=True,axis=1) $ df_likes.head()
hdf['Age'].mean(level=[0, 1]).head()
RNPA_new = RNPA[RNPA['ReasonForVisitDescription'].str.contains('New')] $ RNPA_existing = RNPA[~RNPA['ReasonForVisitDescription'].str.contains('New')]
url = "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv" $ response = requests.get(url) $ with open(url.split("/")[-1], mode="wb") as file: $     file.write(response.content)
baseball_newind.sort_index(ascending=False).head()
alpha = 1.0/summary.mean()[0] $ type(alpha)
temp_df2.info()
first_movie.a
cv = CountVectorizer(stop_words='english')
import gensim $ import numpy as np $ numpy_matrix = np.random.randint(10, size=[5,2]) $ corpus = gensim.matutils.Dense2Corpus(numpy_matrix) $ numpy_matrix_dense = gensim.matutils.corpus2dense(corpus, num_terms=10)
df1
top_tracks = pd.DataFrame(columns=[i.replace(".txt","") for i in files])
from selenium import webdriver $ from selenium.webdriver.support.ui import Select $ from selenium.webdriver.common.by import By $ from selenium.webdriver.support.ui import WebDriverWait $ from selenium.webdriver.support import expected_conditions as EC
train_col.plot_lr()
fulldf.to_csv('FullResults.csv', encoding='utf-8', index=False)
shows.isnull().sum()
print(dt.date(2012,12,21) + dt.timedelta(30,12,0))
data_archie = data_archie.drop(['fullname','longitude','latitude','source'] , 1)
alg2 = RandomForestClassifier() $ alg2.fit(X_train, y_train) $ probs = alg2.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
import gcsfs $ import google.datalab.bigquery as bq $ import pandas as pd
df_twitter_clean = df_twitter.filter(['id', 'favorite_count', 'retweet_count'], axis=1)
autos["price"].unique().shape $ autos["price"].describe() $ autos['price'].value_counts().head(30)
x = np.random.randn(10) $ df9 = pd.DataFrame(index = range(100), columns = range(10)) $ for i in range(df9.shape[0]): $     df9.iloc[i] = x $ df9
classify_df['Male'] = 0 $ classify_df.loc[classify_df['Female']=='Male','Male'] = 1
ab_diffs = df2[df2['group'] == 'treatment']['converted'].mean() -  df2[df2['group'] == 'control']['converted'].mean() $ p_diffs = np.array(p_diffs) $ (ab_diffs < p_diffs).mean()
links_lst=[] $ for k in users_retweeted_dict: $     if users_retweeted_dict[k]: $         for u in users_retweeted_dict[k]: $             links_lst.append({"source":k,"target":u}) $
df2 = df2.rename(columns ={"control":"ab_page"})
d = datetime.date(1492, 10, 12) $ d.strftime('%A')
train_data = prepare_data(intervention_train_df).drop(drop_columns, axis=1)
url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vRlXVQ6c3fKWvtQlFRSRUs5TI3soU7EghlypcptOM8paKXcUH8HjYv90VoJBncuEKYIZGLq477xE58C/pub?gid=0&single=true&output=csv' $ df_hourly = pd.read_csv(url,parse_dates = ['time'],infer_datetime_format = True,usecols = [0,3]) $ df_hourly.head()
pnew = df2['converted'].mean() $ pnew
%load "solutions/sol_2_38.py"
start = datetime.now() $ modelxg_lr1 = xgb.XGBClassifier(max_depth=15, learning_rate=.1, n_estimators=250, n_jobs=-1) $ modelxg_lr1.fit(Xtr.toarray(), ytr) $ print(modelxg_lr1.score(Xte.toarray(), yte)) $ print((datetime.now() - start).seconds)
df['prob'] = df / df.sum()
open_issues = Issues(github_index).is_open().get_cardinality("id_in_repo").by_period() $ print("Trend for month: ", get_trend(get_timeseries(open_issues))) $ open_issues = Issues(github_index).is_open().get_cardinality("id_in_repo").by_period(period="quarter") $ print("Trend for quarter: ", get_trend(get_timeseries(open_issues)))
twitter_archive_full[['retweet_count','favorite_count']].info()
sl_data3=sl_data3.fillna(0)
new_dems.newDate.tail()
def strip_parenthesisandsemicolon(text): $     try: $         return text.replace(');',"") $     except AttributeError: $         return text 
model.userFeatures().first() $
resp = r.json() $ print(resp)
predictions = knn.predict(test[['property_type', 'lat', 'lon','surface_total_in_m2']])
pd.set_option('display.max_columns', None)
rf_reg.score(x_test,y_test)
df.groupby('episode_id')['id'].nunique().agg(['min', 'mean', 'max'])
email_string = formatEmails(df['Recipient Email Address']) $ context = rdb.query(ppdb, query.format(email_string)) $ context.head()
!docker kill mylocalservice
df['closed_at'].head()
pst = mfp_boss.pst
print("Probability of control group converting:", df2[df2['group']=='control']['converted'].mean())
n_cols = 3 $ layout = (n_cols, 1+ int( (1+states.shape[1])/n_cols)) $ states.columns = ['ofi', 'vol_mb', 'macd', 'mid_std', 'possession', 'cap_pct'] $ states.plot(subplots=True, figsize=(15,10), layout=layout);
ethPrice['close'].plot(figsize=(16, 12))
plot = song_tracker[["rockstar","Bad and Boujee (feat. Lil Uzi Vert)"]].astype(float).plot(figsize=(20,15)) $ plot.set_xlabel("Date") $ plot.set_ylabel("Chart Position")
soup.find_all('div', attrs={'class':'art_title'})
som = MiniSom(20, 30, 8292, sigma=0.3, learning_rate=0.5) # initialization of 6x6 SOM $ print("Training...") $ som.train_random(data.values, 100) # trains the SOM with 100 iterations $ print("...ready!") $
today_date = dt.datetime.today().strftime('%Y-%m-%d') $ today_date
total_newpage = df2[df2.landing_page == 'new_page'].count()['user_id'] $ total_pages = df2.shape[0] $ total_newpage / total_pages #0.50006194422266881 $
prcp_df.count()
(df2.landing_page == 'new_page').mean()
def save_tweets(tweets, path): $     out_file = open(path, "w+") $     json.dump(tweets, out_file) $
example['hour (sin)'] = np.sin((2. * example['hour'] *  np.pi / 24)) $ example['hour (cos)'] = np.cos((2. * example['hour'] *  np.pi / 24))
tt = pd.get_dummies(test_df['pred'],prefix='pred')
ts.resample('D').sum().plot()
import test_package.print_hello_function_container #import module $ test_package.print_hello_function_container.print_hello_function() # call function from the imported copy of library $ import test_package.print_hello_function_container $ print_hello = test_package.print_hello_function_container.print_hello_function $ print_hello()
acc.find(amount=1500)
dfList = dfList.rename(columns={'ZIP':'ZIPCODE', 'EST':'EST', 'YEAR':'YEAR'})
plt.scatter(x,y, color = 'lightgreen') $ plt.plot(x, np.dot(x3, ridge1.coef_) + ridge1.intercept_, color ='darkorchid', linewidth = '5') $ plt.plot(x, np.dot(x3, model3.coef_) + model3.intercept_, color = 'dimgrey', linewidth = '3.5')
score, acc = model.evaluate(x_test, $                             y_test, $                             batch_size=batch_size) $ print('Test score:', score) $ print('Test accuracy:', acc)
reg.fit(X_train,Y_train)
df.groupby('Team').groups
input_col = ['msno','date'] $ df = df[input_col].groupby('msno').apply(t_diff,2) $ gc.collect()
treatment_convert_rate= df2.query('group == "treatment"').converted.mean() $ print(' Given that an individual was in the treatment group, what is the probability they converted is {}'. $       format(treatment_convert_rate))
type2017 = type2017.dropna() 
df_merged.hist(column='favorite_count',bins=100);
best_ids_df = topics_data[topics_data.id.isin(best_ids)] $ display(best_ids_df)
print(pred[:10]) $ lgb_xgb_combined_pred_pickle_file_name = "pickle/zillowAbhishek_lgb_xgb_combined_pred.pickle" $ make_pickle(lgb_xgb_combined_pred_pickle_file_name, pred)
df=pd.read_csv('ab_data.csv') $ df.head()
data = bson.Binary(open(filename).read()) $ fs.put(filename=filename, data=data, text=text, tags=tags, username='dvader')
asf_agg_by_gender_and_proj_df.select("*").show()
df_pageviews_mobile_app.head()
weather.Humidity_avg_percent =pd.to_numeric(weather.Humidity_avg_percent,errors = "coerce")
df.shape  # tuple
xml_in.head(5)
averaged_data = dry_ndvi.groupby('time.month').mean(dim='time')
qbfile = open("/home/midhu/DataScience/nlp/kaggle/all_question_pairs_train.csv","r") $ for aline in qbfile.readlines(): $     line=aline.strip().split(',') $     tq1.append(line[3]) $     tq2.append(line[4]) $
import pandas as pd $ pd.set_option("display.max_colwidth", 80) $ pd.set_option("display.max_columns", 100) $ pd.set_option("display.max_rows", 6)
import pandas as pd $ import psycopg2 $ pd.options.display.max_columns = 100
df.iloc[[1,2,4],[0,2]]
no_conv, yes_conv = df2.query('group == "treatment"').converted.value_counts() $ yes_conv/(no_conv + yes_conv)
df.index.hour.value_counts().head()
stores.toPandas().head()
number_of_topics = 150 $ posts_df["Tags"] = posts_df["Tags"].map(lambda x: [] if (x is np.nan) else x.replace("<","").replace(">",",").strip(",").split(",")) $ tags = reduce(lambda x,y: x+y,posts_df["Tags"]) $ tags_freq = Counter(tags) $ print(tags_freq.most_common(number_of_topics))
convert_rate_p_new = df2.converted.mean() $ print('Convert rate of p_new under the null is:{}'.format(convert_rate_p_new))
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old],alternative='larger') $ z_score, p_value
data4.crs= "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
m3.fit(lrs, 7, metrics=[accuracy], cycle_len=2, cycle_save_name='imdb2')
metrics, predictions = pipeline.test(X_valid, y_valid, output_scores=True) $ print("Performance metrics on validation set: ") $ display(metrics) $ print("Individual scores: ")
ncTest.variables['tvar']
sns.heatmap(shows.corr())
df_clean3.loc[1779, 'text']
df[["PREV_DATE", "PREV_ENTRIES"]] = (df $                                                        .groupby(["C/A", "UNIT", "SCP", "STATION"])["DATE", "ENTRIES"] $                                                        .transform(lambda grp: grp.shift(1))) $ df.head()
StockData.tail()  # pandas.DataFrame.tail(n) shows the last n rows
nb2 = MultinomialNB() $ %time nb2 = nb2.fit(train_4_reduced, y_train) $ %time y_hat_nb2 = nb2.predict(train_4_reduced) $ print(roc_auc_score(y_train, y_hat_nb2)) $ print(log_loss(y_train, y_hat_nb2))
no_conv, yes_conv = df2.query('group == "control"').converted.value_counts() $ yes_conv/(no_conv + yes_conv)
x = store_items.isnull() $ print(x) $
df = pd.read_csv('ab_data.csv') $ df.head()
indeed.shape
nar5.to_clipboard()
http://localhost:4040/
table3= table3.drop(['device_id','source','browser','sex','signup_time','purchase_time'], axis=1)
df2['Change'].abs()  
pd.read_html(html_string)[0]
print(df_test.shape) $ print(y_final.shape) $ print(X_final.shape) $ print(Feature_test.shape)
libraries_df.tail()
z_values, _ = vae_latent(nn_vae, mnist_test_loader) $ print(z_values[:,0].mean(), z_values[:,0].std()) $ print(z_values[:,1].mean(), z_values[:,1].std()) $ recon_x = vae_recon(nn_vae, z_values) $ plot_mnist_sample(recon_x)
df_2015.shape, df_2016.shape
license_words = ['license', 'apply', 'certificate', 'certification', 'permit', 'business', 'work']
dff = dff.reset_index().set_index('Datetime') $ dff = dff[dff['Incident_number'].index.notnull()] $ dff.loc[:, 'index'] = 1
unsorted_AMD_data = pd.read_csv(AMD_data_file) $ unsorted_AMD_data.head()
srctake = sourcetake.stack().to_frame() $ srctake.rename(columns={'0':'Use (ac-ft)'},inplace=True) $ srctake.reset_index(inplace=True) $ srctake['Year'] = pd.to_numeric(srctake['Year'],errors='coerce') $ srctake = srctake[(srctake['Year']<=datetime.today().year)&(srctake['Year']>=1000)]
autos['price'].value_counts().sort_index(ascending=False).head(10)
dfRegMet.to_pickle("dfRegMet.p")
print('Number of unique symbols: {}'.format(len(news.symbol.unique()))) $ print('Average number of news per symbol: {}'.format(np.mean(news.symbol.value_counts())))
acc.find(agent='agent 4')
dfgts = pd.DataFrame() $ for i in range(len(datafiles)): $   dftmp = pd.read_csv(datafiles[i]) $   dfgts = dfgts.append(dftmp)
sn.distplot(train_binary_dummy['avg_time'])
databreach_2017['Ticker_symbol']= data_ticker['Ticker_Symbol']
plt.show()
def filter_stopwords(tokens): $     return [t for t in tokens if len(t) > 2 and t not in stopwords]
df['key'] = 0 $ df
logit_model = sm.Logit(df3['converted'],df3[['intercept','CA','UK']]) $ results = logit_model.fit() $ results.summary()
session = Session(bind=engine)
austin[austin['distance_travelled'].isnull()].index.tolist()
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=38000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
for drink in Beverage_name: $     new_data[drink] = 0 $     new_data.loc[new_data['drink1'].str.contains(drink), drink] = 1 $     new_data.loc[new_data['drink2'].str.contains(drink), drink] = 1 $     new_data.loc[new_data['drink3'].str.contains(drink), drink] = 1
autos["registration_year"].describe() $
y_pred = regr.predict(X_test)
contract_history['NUM_CAMPAGNE'] = contract_history['NUM_CAMPAGNE'].map(lambda x: float(x) if x not in ['N', ''] else np.nan)
d = pd.date_range('11-Sep-2017', '17-Sep-2017', freq='2D') $ d + pd.Timedelta('1 days 2 hours')
autos.info()
df2.query("group=='control' & converted==1").count()[0]/df2.query("group=='control'").count()[0]
data_rec = data.view(np.recarray) $ data_rec.age
print(reverse_id_dict.get('cisuabn14','Unknown'))
start = pd.to_datetime('2017-01-01') $ end = pd.to_datetime('2017-08-02') $ df = data.get_data_google('CSRA', start, end) $ df
tst_lat_lon_df[985:990]
import word2vec
flat = TextBlob("The Earth is flat") $ flat.sentiment
for col in df.select_dtypes(include='datetime64').columns: $     print_time_range(col)
d311_gb.head()
autos['odometer_km'].describe()
joined['onpromotion'].value_counts()
twitter_key = os.environ.get('TWITTER_CONSUMER_KEY') $ twitter_secret = os.environ.get('TWITTER_CONSUMER_SECRET') $ twitter_token = os.environ.get('TWITTER_ACCESS_TOKEN') $ twitter_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')
coin_data.columns
dum = pd.get_dummies(TrainData, sparse=True, drop_first=True, dummy_na=True, $                     columns=['Gender', 'City_Category', 'Employer_Category1', 'Primary_Bank_Type', 'Contacted', $                             'Source_Category', 'Employer_Category2', 'Var1']) $ dum.head()
dfOld= dfOther[dfOther['Name'].apply(lambda x: x.split(' ')[0])=="HOMESENSE"] $ dfOld.head()
x = x.rename({"A2": "A_sq"}, axis=1) $ x
df_tweets.head()
from sklearn.decomposition import TruncatedSVD $ tfidf_svd_v2 = TruncatedSVD(n_components=n_comp, n_iter=100, random_state=2) $ review_reduced_v2 = tfidf_svd_v2.fit_transform(tfidf_vecs)  $ print(tfidf_svd_v2.explained_variance_ratio_) $ print(tfidf_svd_v2.singular_values_)
training_data = [doc_duration, RN_PA_duration, therapist_duration]
dark_sky = temperature_sensors_df[ $     temperature_sensors_df['entity_id']=='sensor.darksky_sensor_temperature'] $ dark_sky = dark_sky.set_index('last_changed') $ dark_sky.head()
tweet_archive_enhanced_clean['doggo'].value_counts()
from pandas_datareader.famafrench import get_available_datasets $ import pandas_datareader.data as web $ len(get_available_datasets()) $ ds = web.DataReader("5_Industry_Portfolios", "famafrench") $ print(ds['DESCR'])
weather_data = pd.read_csv('Historical_Weather_Data.csv',index_col=0,parse_dates=True) $ weather_data = weather_data.fillna(0) $ weather_data.index = pd.to_datetime(weather_data.index, unit='s') $ weather_data = weather_data.resample('H').mean()
    prices['perc_change']=(prices['close'] - prices['close'].shift(1))/prices['close'].shift(1)  # Getting the daily percentage change $     prices['5dayvol'] = prices['perc_change'].rolling(window=5, center=False).std()  # 5 day volatility $     prices['5_day_target'] = prices['close'].shift(-5) / prices['close']  # target of X days $     prices.head()
validation_features = spark.read.csv(os.path.join(mungepath,"model_data/20180504/rf_lr_lasso_inter2_noip/validation_features/*"), header=True) $ print("Number of observations in validation :", validation_features.count())
x_train = labeled_news[:400][1] $ y_train = labeled_news[:400][0] $ x_test = labeled_news[401:][1] $ y_test = labeled_news[401:][0]
twitter_archive_clean=twitter_archive_clean[twitter_archive_clean.rating_denominator==10]
tfidf = models.TfidfModel(terms_matrix) $ corpus_tfidf = tfidf[terms_matrix] $ corpus_lda = ldamodel[corpus_tfidf] $ total_topics = 5
observed_mean_treatment = df2.query('group=="treatment"')['converted'].mean() $ print(observed_mean_treatment)
unique_users_count = df.user_id.nunique() $ print(unique_users_count)
group_one = df[(df['group'] == 'treatment') & (df['landing_page'] == 'old_page')] $ group_two = df[(df['group'] == 'control') & (df['landing_page'] == 'new_page')] $ print("Number of times 'treatment' group don't line up with 'old_page' - {}".format(len(group_one))) $ print("Number of times 'control' group don't line up with 'new_age' - {}".format(len(group_two))) $ print("Number total of times the 'new_page' and 'treatment' don't line up - {}".format((len(group_one)+len(group_two))))
df_new.query('group == "treatment" and country =="US"')['converted'].mean()
treatment = df2['group'] == 'treatment' $ obs_conv_new = df2[treatment]['converted'].mean() $ print(obs_conv_new)
df.describe(percentiles=[0.9,0.3,0.2,0.1])
model.evaluate(X_train, Y_train2, verbose=0)
mlp_pc = mlp_df.pct_change() $ mlp_pc.head()
appointments = pd.read_csv('./data/AppointmentsSince2015.csv')
gamma.E_gsf_line_plot(vect=[0.5, 0.5, -1], length_unit=length_unit, $                       energyperarea_unit=energyperarea_unit) $ plt.show()
import statsmodels.api as sm $ reg1 = sm.OLS(endog=df1['logpgp95'], exog=df1[['const', 'avexpr']], missing='drop') $ type(reg1)
posts = data["author"].value_counts().reset_index() $ posts.rename(columns = {"index" : "author", "author": "links"}, inplace=True) $ posts[0:9]
url_mars_facts = "https://space-facts.com/mars/" $ browser.visit(url_mars_facts)
del df['genre'] $
df_final_edited_10.plot(x='rating_numerator', y='fav_count', kind='scatter');
titanic.pivot_table('survived', index='class', columns='sex')
df_goog
datetime.datetime(2018, 2, 18)
occurrences = np.asarray(vectorized_text_labeled.sum(axis=0)).ravel() $ terms = ( ... ) $ counts_df = pd.DataFrame({'terms': terms, 'occurrences': occurrences}).sort_values('occurrences', ascending=False) $ counts_df
retweets = pd.read_csv('LaManada_new/tblretweets.csv',sep=SEP) $ retweets.shape
ds_temp_casts_CTD_1988.Project.dtype
def value_of_price(price_mention): $     if price_mention == 'price': return 1 $     return 0 $ def price_score(review):    $     return sum ([value_of_price(tag) for sentence in review for token in sentence for tag in token[2]])
msft = pd.read_csv("../../data/msft.csv", dtype={'Volume': np.float64}) $ msft.dtypes
import statsmodels.api as sm $ import scipy.stats as stats $ logit = sm.Logit(df2['converted'],df2[['intercept' ,'ab_page']]) $ results = logit.fit()
print("p_new under the null is: %.4f\np_old under the null is: %.4f" %(p_new, p_old))
sale2_table = sale_prod_table.groupby(['Product', 'Country']).SalePrice.mean() $ sale2_table
old_page_converted = np.random.normal(0, p_old, n_old)
retweet_df['retweet_user_id'].unique().size
top10 = git_blame[git_blame.knowing].author.value_counts().head(10) $ top10
prcp_results_df.describe()
Grouping_Year_DRG_discharges_payments.head()
c = d6tstack.convert_xls.XLStoCSVMultiSheet(cfg_fnames[0],output_dir = 'test-data/output',logger=PrintLogger()) $ c.convert_all(header_xls_range="B2:B2") $
calls_df.describe()
engine.execute('SELECT * FROM station LIMIT 5').fetchall()
results = pd.read_csv('datasets/results.csv')
len(df_bkk)
freq_titles = final_data.groupby(['clean_titles']).size().reset_index(name='counts').sort_values('counts', ascending=False).head(200) $ freq_titles
cur.execute("select * from sqlite_master where type == 'table';").fetchall()
import pandas as pd $ df = pd.read_csv('movie_data.csv', encoding='utf-8') $ df.head(3)
tweet_df.iloc[:,1:15].sample(5)
filename1 = 'expr_3_qi_nmax_32_nth_1.0_g_91+25_04-13_opt_qcb_L-BFGS-B.csv' $ df1 = pd.read_csv('../output/data/expr_3/' + filename1, comment='#') $ filename2 = 'expr_3_qi_nmax_32_nth_1.0_g_101_04-17_opt_etgl_L-BFGS-B.csv' $ df2 = pd.read_csv('../output/data/expr_3/' + filename2, comment='#')
brands = autos['brand'].value_counts().head(20) $ brands = brands.index.tolist() $ print(brands)
tweet_df_clean = tweet_df.rename(columns={'id':'tweet_id'})
drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15]) * 0.7
df_os.head()
News_outlets_df = all_pd.rename(columns={"Username": "Media Sources"}) $ News_outlets_df["Media Sources"].replace(to_replace=["nytimes"], value="New York Times", inplace=True) $ News_outlets_df["Media Sources"].replace(to_replace=["FoxNews"], value="Fox News", inplace=True) $ News_outlets_df
df_weather['HOURLYPrecip'].fillna(value=0,inplace=True) $ df_weather['HOURLYVISIBILITY'].fillna(df_weather['HOURLYVISIBILITY'].mean(),inplace=True)
df2=df.copy() $ print(id(df),sep='\n') $ print(id(df2),sep='\n')
print(multi.shape) $ print(pp.shape)
gnb.__dict__
def prefiltering_of_neighbors(old_similar_table, thershold): $     new_similar_table = old_similar_table.copy() $     new_similar_table[new_similar_table < thershold] = 0 $     return new_similar_table
from rl_portfolio_management.callbacks.tensorforce import EpisodeFinishedTQDM, EpisodeFinished $ from rl_portfolio_management.util import MDD, sharpe
df_wrong_rating = df_enhanced[df_enhanced.text.str.contains(r"(\d+\.\d*\/\d+)")] $ l = [] $ for item in  df_wrong_rating['text']: $     l.append(item.split('/')[0].split()[-1]) $ print(l)
model.wv.most_similar("cantonese")
cust_demo.describe()
sentiment_pipeline = joblib.load("../data/other_data/subtweets_classifier.pkl")
p=df2['converted'].mean() $ p
simband_time_df = pd.DataFrame.from_records(simband_time_date) $ simband_time_df.columns = ['subject_id', 'simband_start_time', 'simband_stop_time']
df.isnull().sum(axis=1)
index = similarities.MatrixSimilarity(lsi[corpus]) # transform corpus to LSI space and index it
tensorboard_pid = ml.TensorBoard.start('./train')
df = titanic3[['cabin', 'pclass']].dropna() $ df['deck'] = df.apply(lambda row: ord(row.cabin[0]) -64, axis=1) $ sns.regplot(x=df["pclass"], y=df["deck"])
train_df = pd.read_csv('data/howpop_train.csv') $ test_df  = pd.read_csv('data/howpop_test.csv')
LabelsReviewedByDate = wrangled_issues_df.groupby(['Priority','DetectionPhase']).created_at.count() $ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue','yellow', 'purple', 'red', 'green'], grid=False)
import gensim, logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) $ sentences = [x.split(" ") for x in tt] $
connection = sqlite3.connect("projectTwit.db") $ cursor = connection.cursor() $ cursor2= connection.cursor()
df2['datetime'].min(), df2['datetime'].max()
archive_clean.info()
auto = pd.read_excel("Auto.am_Final.xlsx")
for col in data.columns: $   print(col, ' : ' , data[col].dtype, desc.loc[desc['Column name'] == col]['Type'].values)
def merge(leftdf, rightdf): $     return pd.merge(leftdf,rightdf, how='inner', left_index=True, right_index=True) $ mergeltc = merge(priceltc, ltc) $ mergeeth = merge(priceeth, eth) $ mergexrp = merge(pricexrp, xrp)
r.json()
uniq_churned_plans_counts
url=("/Users/maggiewest/Projects/detroit_gdp.csv") $ gdp_df = pd.read_csv(url)
image_pred_df = pd.read_csv('image-predictions.tsv', sep='\t') $ image_pred_df.head(2)
temperature_2016_df["tobs"].hist(bins=12, color="darkblue") $ plt.title("Histogram: Temperature from Station with highest observation") $ plt.savefig("Histogram: Temperature from Station with highest observation") $ plt.show()
df2 = df2.drop(df.index[2893])
images_df.breed = images_df.breed.str.replace('_', ' ').str.title()
print ("The original dtype of dateCrawled is " + str(rawautodf.dateCrawled.dtype)) # checked datatype $
test_collection.count()
import pandas as pd $ the_dict = pd.read_clipboard().to_dict('records') $ the_dict
from sklearn.model_selection import GridSearchCV
df_vow['Date'] $ type(df_vow['Date'].loc[0])
cutoff_times = generate_labels('/7/KMLZlMBnmWtb9NNkm3bYMQHWrt0C1BChb62EiQLM=',  trans, $                                label_type = 'MS', churn_period = 30) $ cutoff_times[cutoff_times['churn'] == 1].head()
df
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=16000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
r = 0.0162/360
twitter_archive_master = pd.merge(df_clean4, df_image_tweet2, on='tweet_id', how='left') $ twitter_archive_master.head()
wb.search('cell').iloc[:5,:2]
forked = forked[forked.repo_id != 0] $ forked.info() $ forked.head()
series1.cov?
df = pd.read_csv("/Users/peterjost/Downloads/12-classwork/classwork-12-311/data/311_Service_Requests_from_2010_to_Present.csv", nrows=50000) $ df.head()
suspects_with_1T_27 = suspects_with_1T[suspects_with_1T['event_type'] == 27]
salesdec['Standard_Plat'] = salesdec['Platform'] $ salesdec['Standard_Plat'].isnull().any() $
with open('valid.json', 'w') as file: $     file.write(df_valid.to_json(orient='records'))
os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.chdir(str(today)) $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END)
df = df.reset_index()
ttDailyClean = (ttAll $                 .groupby(mainkey+['DATE'])['Corrected_Entry'] $                 .sum().reset_index()) $ ttDailyClean.head()
my_df_free1.iloc[100:110,0:3]
stamp_name = '4. timestamp'
close_month.loc[month].nlargest(2)
twitter_archive_clean.loc[twitter_archive_clean['tweet_id']==835246439529840640]
X_conv_train = np.expand_dims(X_conv_train, axis=2) $ X_conv_test = np.expand_dims(X_conv_test, axis=2)
df_columns['Complaint Type'].value_counts() # Homeless $ df_columns[df_columns['Complaint Type'].str.contains('Homeless')].index.month.value_counts().sort_index().plot() $
pd.read_csv?
daily_returns=compute_daily_returns(df) $ plot_data(daily_returns,title="Daily Returns")
excelDF['Ship Mode'].count()
rng = pd.bdate_range(start, end)
cityID = 'cb74aaf709812e0f' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Tulsa.append(tweet) 
datAll['Street_name'] = np.where(~pd.isnull(datAll['Street Name']),datAll['Street Name'],np.nan) $ datAll['Street_name'] = np.where(~pd.isnull(datAll['StreetName']),datAll['StreetName'],datAll['Street_name'])
import pandas_datareader.data as web  #Not 'import pandas.io.data as web' as in the book.
def rmse(x,y): return math.sqrt(((x-y)**2).mean()) $ def print_score(m): $     res = [m.score(X_train, y_train), m.score(X_valid, y_valid)] $     if hasattr(m, 'oob_score_'): res.append(m.oob_score_) $     print(res)
df_students1 = df_students.rename(columns={'school':'school_name'}) $ df_students1.head()
features, feature_names = ft.dfs(entityset = es, target_entity = 'clients', $                                  agg_primitives = ['mean', 'max', 'percent_true', 'last'], $                                  trans_primitives = ['years', 'month', 'subtract', 'divide'])
assists_df
cols = ['RequestType', 'RequestSource', 'Address', 'Latitude' , 'Longitude', 'CD', 'Created Date', 'Created Time', 'Week Number'] $ data = pd.read_csv("311_week_number.csv", usecols = cols, low_memory = False) $ data
def convert(x): $     return x.encode('utf8')
b_cal_q1.columns
new_reps.Cruz.astype("float64").describe()
logreg = LogisticRegression() $ logreg.fit(X_train, Y_train) $ Y_pred = logreg.predict(X_test) $ acc_log = round(logreg.score(X_train, Y_train) * 100, 2) $ acc_log
cur = conn.cursor() $ cur.execute('UPDATE actor SET first_name = CASE WHEN first_name = \'HARPO\' THEN \'GROUCHO\' ELSE \'MUCHO GROUCHO\' END WHERE actor_id = 172;') $
X.head()
from datetime import datetime $ from datetime import timedelta $ import pandas as pd
print(1/np.exp(-0.0408),1/np.exp(0.0099))
membership['new_date'] = membership['Created at'].apply(lambda x:x.split(' ')[0])
temp_df['reorder_interval_group'] = temp_df['reorder_interval_group'].astype(float)
import pandas as pd 
file_path = "
df[df['type']!='User']
tweet_archive_enhanced_clean['text'][2335]
from sklearn.preprocessing import StandardScaler
cd ../
len(other_inter_recr)
plt.figure(figsize=(16, 16)) $ sns.heatmap(corr, annot=True) $ plt.title('Correlation in Heat Map')
df.index    
tweet_json['id'][29]
print(click_condition_meta.platform.unique()) # one unique value for the entire dataframe $ click_condition_meta.drop(['platform'], axis = 1, inplace = True)
df2.landing_page.value_counts()[0] / df2.shape[0]
sns.lmplot('yearOfRegistration','price', data= cars) $
print('       Sponsored Jobs: ' + str(len(dedup[dedup.job_type == 'Sponsored']))) $ print('Unique Sponsored Jobs: ' + str(dedup[dedup.job_type == 'Sponsored'].hash.nunique()) + '\n') $ print('         Organic Jobs: ' + str(len(dedup[dedup.job_type == 'Organic']))) $ print('  Unique Organic Jobs: ' + str(dedup[dedup.job_type == 'Organic'].hash.nunique()))
df[df.hash.duplicated(keep=False)]
stackedCloses = closes.stack() $ stackedCloses
analyze_set=pd.read_csv('Twitter_archive_master.csv')
df_new.country.unique()
ratings_df.head()
df_clean[['retweeted_status_id', 'retweeted_status_user_id', $          'retweeted_status_timestamp']].notnull().sum()
con = sqlite3.connect('microbiome.sqlite3') $ con.execute(query) $ con.commit()
learn.save('clas_0')
tweet_data_copy['date_time']=pd.to_datetime(tweet_data_copy['date_time'])
df2.query('group == "treatment" and converted == 1').user_id.count() / df2.query('group == "treatment"').user_id.count() $
people.groupby(lambda x:GroupColFunc(people,x,'a')).groups
col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]
style.use('ggplot')
print('Accuracy Metrics for Random Forest') $ score[['is_false_positive', 'is_false_negative', 'is_true_positive', 'is_true_negative']].mean()
df_amznnews_2tick = df_amznnews[['publish_time','textblob_sent', 'vs_compound']] $ df_amznnews_2tick = df_amznnews.set_index('publish_time') $ df_amznnews_2tick = df_amznnews_2tick[['textblob_sent', 'vs_compound']].resample('1D').mean() $ print(df_amznnews_2tick) $
import requests
type(1)
from IPython.display import display
df_con.to_csv('df_con', index=False)
(len(df[df['converted']==1]))/df.shape[0]
df_weather = df_weather[['STATION_NAME','DATE','HOURLYVISIBILITY','HOURLYDRYBULBTEMPC','HOURLYWindSpeed','HOURLYPrecip']].copy()
raw_full_df.building_id.value_counts()[:10]
df.head()
import pandas as pd $
(len(df.query('converted == "1"')) / len(df.query('converted'))) * 100
to_be_predicted_Day1 = 17.78 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
words_only_sp = [term for term in words_sp if not term.startswith('#') and not term.startswith('@')] $ corpus_tweets_streamed_profile.append(('words', len(words_only_sp))) # update corpus comparison $ print('The number of words only (no hashtags, no mentions): ', len(words_only_sp))
data['temp']=data['Date'].astype(basestring)+'-1999' $ data.head(5) $
excel_data = pd.read_excel("Superstore.xlsx", sheet_name = 'Orders') $ print(excel_data)
import statsmodels.api as sm $ convert_old = control_converted.shape[0] $ convert_new = treatment_converted.shape[0] $ n_old = df2.query('landing_page=="old_page"').shape[0] $ n_new = df2.query('landing_page=="new_page"').shape[0]
d = {'Name':pd.Series(['Tom','James','Ricky','Vin','Steve','Smith','Jack']), $    'Age':pd.Series([25,26,25,23,30,29,23]), $    'Rating':pd.Series([4.23,3.24,3.98,2.56,3.20,4.6,3.8])} $ df = pd.DataFrame(d)
train['PassengerId'].describe()
autos.head()
slFullDf = pd.concat(slFrameList,axis=0) $ slFullDf.head()
df_2015['bank_name'] = df_2015.bank_name.str.split(",").str[0] $
df.loc[0:2, ['A', 'C']]
x_test, y_test = x[int(0.6*len(x)):], y[int(0.6*y.size):]
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])
df[df.index.month.isin([12,1,2])]['Complaint Type'].value_counts().head()
autos["odometer_km"].value_counts()
sociologistParagraphsDF['source'] = ['wikipedia_sociologists'] * len(sociologistParagraphsDF['paragraph-text']) $ sociologistParagraphsDF['paragraph-number'] = range(len(sociologistParagraphsDF['paragraph-text'])) $ sociologistParagraphsDF['source-paragraph-number'] = [None] * len(sociologistParagraphsDF['paragraph-text']) $ sociologistParagraphsDF['source-paragraph-text'] = [None] * len(sociologistParagraphsDF['paragraph-text']) $ sociologistParagraphsDF[:10]
df_clean.rating_denominator.value_counts()
print("Number of Techniques in Enterprise ATT&CK") $ print(len(all_enterprise['techniques'])) $ df = all_enterprise['techniques'] $ df = json_normalize(df) $ df.reindex(['matrix', 'tactic', 'technique', 'technique_id', 'data_sources'], axis=1)[0:5]
vectorized_text_labeled
print(store_info.CompetitionDistance.describe()) $ store_info.CompetitionDistance.plot.hist();
dfRegMet2016 = dfRegMet[dfRegMet.index.year == 2016]
from IPython.display import Image $ Image(url=csv_df[csv_df['timestamp'] == csv_df['timestamp'].min()]['url'].values[0], width=600)
np.array([1, 2, 3, 4]) + 2
print type(_html)
date_price = b_cal_q1.groupby('date').mean()['price']
mmbb_series = pd.Series(mileage_mean_by_brand) $ mmbb_series
df_prep17_.index
dd_df['fuelType'].fillna('fueltype_unknown', inplace=True)
train['Hour'] = train.apply(lambda row: hour_bins(row[12]), axis = 1) $ test['Hour'] = test.apply(lambda row: hour_bins(row[11]), axis = 1)
breakfastlunchdinner.sort_values(['lunch + brexits'], ascending=False)
df2.columns
print (df.loc[ [101,103], ['name','year1'] ], '\n')  # by list of row label and column names $ print (df.loc[  101:104 ,  'name':'year1'  ], '\n')  # by range of row label and column names
print(bag.toarray())
twitter_names = df[~df['Twitter_Name'].isnull()]['Twitter_Name'].unique()
df_train['totals.bounces'] = df_train['totals.bounces'].fillna('0') $ df_test['totals.bounces'] = df_test['totals.bounces'].fillna('0')
pd.DataFrame({'Bob': ['I liked it.', 'It was awful.'], 'Sue': ['Pretty good.', 'Bland.']})
df2 = df.copy() $ df2['one'] = 0 $ df2
from helper_code import iris as hi $ test_frac = 0.4 $ show_plot = True $ d_train_sc, d_test_sc, l_train, l_test = \ $     hi.get_iris_data(test_frac, show_plot)
print('The maximum change between two days is {}'.format(max(change)))
(df_final[df_final['Scorepoints'] == 200]).head()
with open(meritocracy_save, mode='w', encoding='utf-8') as f: $     f.write(wikiMeritRequest.text)
move_1_23st = sale_lost(breakfastlunchdinner.iloc[1, 1], 20) $ move_2_23st = sale_lost(breakfastlunchdinner.iloc[3, 2], 20) $ adjustment_1 = move_1_23st + move_2_23st $ print('Adjusted total for route: ' + str(move_34p23s34p - adjustment_1))
sum(~twitter_archive['retweeted_status_id'].isnull())
pysqldf("select * from best where tot_len > 4000000 order by N asc")
p_diffs = np.array(p_diffs) $ (p_diffs > obs_diff).mean()
dd_df_gearbox = pd.get_dummies(dd_df['gearbox']) $ dd_df_notRepairedDamage = pd.get_dummies(dd_df['notRepairedDamage']) $ dd_df_fuelType = pd.get_dummies(dd_df['fuelType'])
import pandas as pd $ import numpy as np $ import re $ import scipy
sel_df[sel_df.BlockRange.isnull()].head()
df[('2016-09-01' < df['Observation Date']) & (df['Observation Date'] < '2017-08-31')]['AQI'].mean()
result = tdf.join(rate, on=['currency', 'rtdate'], how='left')
(autos["registration_year"].between(1900,2016)).sum() / autos.shape[0]
print(f'Second element of a is {a[1]}') $ print(f'Last element of a is {a[-1]}') $ print(f'Middle two elements of a are {a[1:3]}')
categories = df['main_category'].value_counts() $ categories.plot.bar(title='Number of projects by main category'); $
plt.hist(p_diffs) $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('10K simulated p_diffs');
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
print("Saving dictionary to elasticsearch - please be patient") $ es.saveToEs(df_dict,index=es_dictindex,doctype=es_dicttype)
session.query(func.count(distinct(Measurement.station))).all()
url = "https://data.cityofnewyork.us/download/i8iw-xf4u/application%2Fzip" $ urllib.request.urlretrieve(url, 'nyc.zip') $ os.system("unzip -d %s nyc.zip"%(os.getenv("PUIDATA"))) $ nycshp = gpd.GeoDataFrame.from_file((os.getenv("PUIDATA") + "/ZIP_CODE_040114.shp"))
df_providers_pared = df_providers_pared.sort_values(['id_num'], ascending=[True]) $ df_providers_pared = df_providers_pared.reset_index(drop=True) $ df_providers_pared = df_providers_pared.drop('Unnamed: 0', axis=1) $ df_providers_pared.head()
import pandas as pd $ review_df = pd.read_json('Amazon_reviews/Clothing_Shoes_and_Jewelry_5.json', orient='records', lines=True)
to_be_predicted_Day1 = 55.05 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
print ('Polynomial Mean:\n\n',training_pending_ratio.mean())
stations
def wiki_table_to_df(end_url, month_range, base_url=base_url): $
sp['day_ago_high'] = sp.High.shift(periods = 1) $ sp['week_ago_high'] = sp.High.shift(periods = 7)
p_new = df2.query('converted == 1').user_id.count()/df2.user_id.count() $ print('p new convert rate: ', p_new)
titanic.deck.value_counts()
frame2
x = datetime.strptime(inner_list[0][0], '%Y-%m-%d') $ type(x.year)
items2 = [{'bikes': 20, 'pants': 30, 'watches': 35, 'shirts': 15, 'shoes':8, 'suits':45}, $ {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5, 'shirts': 2, 'shoes':5, 'suits':7}, $ {'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4, 'shoes':10}] $ store_items = pd.DataFrame(items2, index = ['store 1', 'store 2', 'store 3']) $ store_items
dr.shape, RNPA.shape, ther.shape
n_old = unique() - n_new $ n_old
ccc = td.columns
for i in Train_extra.columns: $     if i not in Test_extra.columns: $         print i
print(prop.info())
tweets_df.to_csv('StockVibes.csv')
autos["registration_year"].value_counts().sort_index()
train_df.describe(include=['O'])
twitter_Archive.to_csv('twitter_archive_master.csv', index=False)
grid_svc.cv_results_['mean_test_score']
df1=df.toPandas()
df2['ab_page'] = pd.Series(np.zeros(len(df2)), index=df2.index); $ trt_ind = df2[df2['group']=='treatment'].index; $ df2.set_value(index=trt_ind, col='ab_page', value=1); $ df2['ab_page'] = df2['ab_page'].astype(int); $ df2.head()
inter1 = pd.read_pickle(folderData + 'interactions1Data.pkl') $ inter1.shape
def get_covariance(returns, weighted_index_returns): $     assert returns.index.equals(weighted_index_returns.index) $     assert returns.columns.equals(weighted_index_returns.columns) $     return None, None $ project_tests.test_get_covariance(get_covariance)
got_data.head(10)
DataSet = DataSet[DataSet.userTimezone.notnull()] $ len(DataSet)
final_data["clean_titles_cat"] = final_data["clean_titles"].astype('category').cat.codes $ final_data.head()
movies.describe()
groupmean = tokendata.drop("ID",axis=1).groupby("level").agg(mean_except_outlier).reset_index()
prop.info()
country_with_least_expectancy = le_data.idxmin(axis=0) $ country_with_least_expectancy
diff_user_cols = ['favourites_count', 'friends_count', 'lang', 'listed_count', 'name', 'time_zone'] $ expected_user_cols = [e for e in user_cols if e not in diff_user_cols] $ assert sorted(pol_users.columns) == sorted(expected_user_cols)
data_year=data_nonan_temp.groupby(['Year']) $ data_year['Temperature(C)'].mean().plot()
result2.summary2()
autos["registration_year"].describe()
twitter_archive_full = pd.merge(twitter_archive_with_json, image_prediction_clean, how='left', on=['tweet_id'])
newdf.head()
df.select(functions.lower(df.hashtag)).show()
nodes = pd.read_csv("nodes.csv",parse_dates=['timestamp']) $ ways = pd.read_csv("ways.csv",parse_dates=['timestamp']) $ nodes_tags = pd.read_csv("nodes_tags.csv") $ ways_tags = pd.read_csv("ways_tags.csv")
df2['high_windchillf'] = wind_chillf(df2['high_tempf'], df2['wind_mph']) $ df2['low_windchillf'] = wind_chillf(df2['low_tempf'], df2['wind_mph']) $ df2
df_parsed = pd.read_csv('./data/FB.csv', parse_dates=True, index_col='Date')
t1 - t2
df2[df2['user_id'].duplicated()]
data[['returns', 'strategy']].mean() * 252
from collections import Counter $ x = analyze_set['name'] $ count = Counter(x) $ count.most_common(5)
journalists_mentioned_by_female_summary_df = journalist_mention_summary(journalists_mention_df[journalists_mention_df.gender == 'F']) $ journalists_mentioned_by_female_summary_df.to_csv('output/journalists_mentioned_by_female_journalists.csv') $ journalists_mentioned_by_female_summary_df[journalist_mention_summary_fields].head(25)
gbrcolumns = ['CHSubscriberCount', 'CHVideoCount', 'CHViewCount','CHAge','PComments', $               'PDislike', 'PLike', 'PView','LikeCount','Duration'] $ X_dfgrb = df[gbrcolumns] $ y_dfgrb = df['logViewsPercentChange']
sqlContext.sql("select * from example2").toPandas()
s.ix[:3].resample('W', fill_method='ffill')
np.exp(fruits)
pred_CSCO
deletes
px = pd.read_csv(dwld_key + '-hold-pricing.csv', index_col='Date', parse_dates=True)
submit.to_csv("properati_dataset_sample_submision2.csv", index = False)
text = re.sub(r'[^a-zA-Z0-9]', ' ', text) $ print(text)
autos["brand"].value_counts(normalize = True).head(10)
obs_mean = treatment_convert - control_convert $ obs_mean
for i in tqdm(range(0,23)): $     mol = molecules.loc[i, 'molecule'] $     m2 = Chem.AddHs(mol) $     if AllChem.EmbedMolecule(m2) < 0: $         print 'oh no {}'.format(i)
clean_rates.info()
test = pd.read_csv('Input/test.csv') $ train  = pd.read_csv('Input/train.csv')
pd.DataFrame({'population': population, 'area': area})
tweet_df["tweet_source"].unique()
imdb_df = pd.read_html("http://www.imdb.com/chart/top?ref_=nv_wl_img_3")[0] $ imdb_df.head()
xgb_learner.params
dfd = dfd[~dupes_to_delete].copy() $ len(dfd)
tweets_clean.rating_numerator.value_counts()
data.describe()
import pandas as pd $ with pd.option_context("max.rows", 10): $     print(dta.results.value_counts())
prop_new_page = df2[df2['landing_page'] == 'new_page'].shape[0] / df2.shape[0] $ prop_new_page
wellness_visits.limit(10).toPandas()
import pickle $ output = open('think_tanks.pkl', 'wb') $ pickle.dump(think_tank_party_dict, output) $ output.close()
dftopcomplaint = dfjoined.groupby('created_date').first().reset_index()
x_train.head() $
df_bill_data = pd.merge(df_bill_id, df_bill, on='bill_id') $ df_bill_data[df_bill_data['patient_id'] == sample_repeat]
df2['intercept'] = 1 $ df2[['ab_page2', 'ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('ab_page2', axis = 1) $ df2.head()
plt.hist(p_diffs) $ plt.axvline(actual_diff, c='r');
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt
page.oldest_revision['timestamp'].strftime("%Y-%m-%d")
res_json['posts']
df_prep6 = df_prep(df6) $ df_prep6_ = pd.DataFrame({'date':df_prep6.index, 'values':df_prep6.values}, index=pd.to_datetime(df_prep6.index))
ab_df2 = ab_df2.drop(labels=2893)
plt.hist(p_diffs); $ plt.axvline(x=p_diff_actual, color='r');
small_train.to_csv('small_train.csv', index=False) 
autos.price.value_counts().sort_index().head(10)
clf = LogisticRegression(class_weight="balanced", solver="newton-cg", tol = 1e-3) $ clf.fit(X, y) $ predictions = clf.predict(test_data_features) $ output = pd.DataFrame( data={"id":test["id"], "rating":predictions} ) $ output.to_csv( "regression.csv", index=False, quoting=3 )
df3['month'] = df3.activity_date_time_c.dt.to_period('M')
merge['index'] = pd.to_datetime(merge['index']) $ merge_ = merge[(~merge['ratio'].isnull()) & $                (merge['index'] >= datetime.datetime(2016,7,17))]
df_clean3.loc[2091, 'text']
x = tf.constant(1, name='x') $ y = tf.Variable(x+10, name='y') $ print(y)
hist_cats['DaysInShelter'].hist(bins=10,color='c') $ plt.axvline(hist_cats.DaysInShelter.mean(), color='b', linestyle='dashed', linewidth=2)
df.sort_values(['operator', 'part'], inplace=True)
print(lr.intercept_)
lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = lm.fit() $ results.summary()
IncBC = (delta(df_new, df_old, ['BCDemandMW','BC_TP'], Aggregate=True, ConvertToMWH=True) - df_new['RealPrice'] * delta(df_new, df_old, ['BCDemandMW'], ConvertToMWH=True)).rename('IncBC') $ IncSwaps = (df_new['RealPrice'] * delta(df_new, df_old, ['SwapMW'], ConvertToMWH=True) - delta(df_new, df_old, ['SwapMW','VWSwapStrike'], Aggregate=True, ConvertToMWH=True)).rename('IncSwap') $ IncSwaptions = (df_new['RealPrice'] * delta(df_new, df_old, ['ManExercOptionsMW'], ConvertToMWH=True) - delta(df_new, df_old, ['ManExercOptionsMW','IntrinsicStrike'], Aggregate=True, ConvertToMWH=True)).rename('IncSwaption') $ IncCaps = (df_new['CapCurve'] * delta(df_new, df_old, ['CapMW'], ConvertToMWH=True) + delta(df_new, df_old, ['CapPremium'])).rename('IncCaps')
merkmale.Merkmal_typ.unique()
festivals.head(5)
sns.distplot(master_list[master_list['Count'] >= 5]['Count'])
agg_stats.tail()
intersections[for_normalized_columns].head()
df_new[['CA', 'UK','US']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop('CA', axis=1) $ df_new.head()
points.loc[:, 'c1']
autos.rename(index=str, columns={"odometer":"odometer_km"})
thecmd = 'curl -v -F file=@'+curDir+'/'+dataDir+'input/beh_nyc_walkability.csv "https://'+USERNAME+'.carto.com/api/v1/imports/?api_key="'+APIKEY $ os.system(thecmd) #run the command to curl the input file, this should work as its not GDAL/OGR
voc = vect.vocabulary_
elms_all_0604 = pd.read_excel(cwd+'\\ELMS-DE backup\\elms_all_0604.xlsx') $ elms_all_0604['ORIG_DATE'] = [datetime.date(int(str(x)[0:4]),int(str(x)[5:7]),int(str(x)[8:10])) $                              for x in elms_all_0604.ORIG_DATE.values]
p_diffs = np.random.binomial(n_new, CRnew, 10000)/n_new - np.random.binomial(n_old, CRold, 10000)/n_old $
plt.figure(figsize=(8, 5)) $ plt.scatter(prepared_train.favs_lognorm, prepared_train.views); $ plt.title('The distribution of the favs_lognorm and number of the views without outliers');
p_new = df2['converted'].mean() $ p_new
file = path + "/ks-projects-201612.csv" $ ks_projects = pd.read_csv(file, encoding = 'latin1') $
test_df.isnull().sum()
pd.DataFrame ([{"name":"Yong", "id":1},{"name":"Gan","id":2}], columns=("MyID","MyName"))
print(reviews['comments'][2975])
df.loc[df['waiting_days']>=0]['waiting_days'].describe()
wrd.info()
crimes.head()
station_most_active = session.query(Measurement.station, Station.name).group_by(Measurement.station).\ $                         order_by(func.count(Measurement.tobs).desc()).first() $ station_most_active
ioDF.shape
age_new = pd.cut(age, [0,10,20,30,40,50,60,70,80]) #discrete decade intervals $ age_new.value_counts().sort_index().plot(kind='bar') $ age_new.value_counts()
fb.head()
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ GROUP BY twitter.tag_text \ $ ORDER BY count DESC LIMIT 10;
evaluator = full_text_classifier.evaluate(df_test) $ print(evaluator.get_metrics("ALL")) $ evaluator.plot_confusion_matrix() $ evaluator.get_metrics("macro_f1")
    def simple_function(v): $         return int(v * 10) $     print simple_function(3)
B2.print_all_paths() $
df_test.groupby(['Gender'])['loan_status'].value_counts(normalize=True)
posts.plot(kind='scatter',x=list(posts.index),y=posts['postsCount'],subplots=True,figsize=(15,10)) $
df2 = pd.DataFrame(np.random.randn(7,3),columns=['col1','col2','col3']) $ df2
df_cs.head(2)
df['Created Date'].resample('MS').count()
pickle.dump(lda_cv_data, open('iteration1_files/epoch3/lda_cv_data.pkl', 'wb'))
cust_demo.sample(n=700, replace=False).duplicated().value_counts()
x.iloc[:3,:]
df.replace('\n', ' ', inplace=True, regex=True) $ df.lic_date = pd.to_datetime(df.lic_date, errors='coerce') $ df = df.sort_values('lic_date', ascending=False)
hp.get_devices()[:4]
q_multipoint = c.retrieve_query(url='https://v3.pto.mami-project.eu/query/9d0a16b4fbb090b661e33140c847b999a8907a49058f21795ab68cdeb6784c95') $ multipoint_df = q_multipoint.results() $ multipoint_df.columns = ["count","condition","time"] $ multipoint_df["time"] = pd.to_datetime(multipoint_df["time"]) $ q_multipoint.metadata()
feature_matrix = compute_features(features, trips[['id', 'pickup_datetime']]) $ preview(feature_matrix, 5)
from sklearn.linear_model import LogisticRegression $ from sklearn.model_selection import cross_val_score $ from sklearn.metrics import confusion_matrix,classification_report,recall_score
import swat $ conn = swat.CAS('server-name.mycompany.com', 5570, 'username', 'password')
df.index.values
print(tweet_archive.name.value_counts())
weights_path = 'wikigrader/data/nn_weights.hdf5' $ model_path = 'wikigrader/data/nn_model.hdf5' $ checkpointer = ModelCheckpoint(filepath=weights_path, save_best_only=True) $ model.fit(X_train.as_matrix(), y_train.as_matrix(), batch_size=128, $           epochs=1000, validation_split=0.2, callbacks=[checkpointer])
import numpy as np $ import pandas as pd $ from sklearn.tree import DecisionTreeClassifier
station_distance['Sex'] = station_distance.Gender.map({0:'unknown', 1:'male', 2:'female'})
rnn_g , rnn_op = rnn_graph() $ runtime(name = "rnn1" ,datalist =[rtrain_x  ,rtrain_y, rvalid_x, rvalid_y], op_list = rnn_op, g = rnn_g)
histogram=active_station.hist(column='tobs', bins=12) $ plt.ylabel("Frequency") $ plt.tight_layout() $ plt.show()
date_str = bird_data.date_time[0] $ print(type(date_str))
import glob $ filenames = {data_dir[:data_dir.find('_')]: glob.glob('../data/ebola/{0}/*.csv'.format(data_dir)) for data_dir in ebola_dirs[1:]}
male = crime.loc[crime['Sex']=='M'] $ male.head(3)
files1= files1.loc[files1['Shortlisted']==1] $ files1.shape
%timeit df.groupby('key').value.sum()
p_old = df2[df2['landing_page']=='old_page']['converted'].mean() $ p_old
auto_new = auto.dropna()
(autos['ad_created'] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index() $ )
%load "solutions/sol_2_31.py"
rng = pd.date_range('1/1/2018',periods=100, freq='M')  # it can also be 'M' $ rng
df[['polarity', 'subjectivity']] = df['text'].apply(lambda text: pd.Series(TextBlob(text).sentiment)) $ df['SA'] = np.array([ analize_sentiment(tweet) for tweet in df['text'] ])
dfETHVol.corr()
test_df.tail(5)
U_B_df = query_df(U_B_URL) $ U_B_df.head()
df2 = df.copy() $ df2.iloc[3] = 0 $ df2
modern_pings = Dataset.from_source("telemetry") \ $     .where(docType='main') \ $     .where(submissionDate="20170716") \ $     .records(sc, sample=0.01)
hp.search_sensors(type='electricity', direction='Import')
n_new = df2.query('group == "treatment"')['user_id'].nunique() $ n_new
df = pd.DataFrame({'tmin': trip[0][0], 'tmax': trip[1][0], 'avg': trip[2][0]}, columns=['tmin','avg','tmax']) $ df
df_variables = df_variables.loc[~(df_variables["_merge"] == "left_only")] $ df_variables = df_variables.loc[:,df_variables.columns != "_merge"] $ print("Data entries variables: " + str(len(df_variables.index)))
data = {"text":clean_en_test_df["text"], $         "created_at":pd.to_datetime(clean_en_test_df["created_at"]) $        } $ for column in column_dict.keys(): $     data[column] = train_and_predict(df_train = clean_train_df, df_test = clean_en_test_df, column = column)
y_predicted = fit3.predict(X3) $ plt.plot(y3, y_predicted, 'b.') $ plt.title('Actual Gross Outcome vs. Budget') $ plt.xlabel('Budget') $ plt.ylabel('Domestic Gross Total')
preprocessor.infer_subtypes() # this function tries to indentify different subtypes of data
def addtwo(elem): $     mytime = datetime.strptime(elem, "%Y-%m-%d %H:%M:%S") $     mytime += timedelta(hours=2) $     return mytime.strftime("%Y.%m.%d %H:%M:%S")
m.reset_index() $ m.head()
twelve_months_prcp.plot(figsize = (12, 8), rot = 45, fontsize=20, use_index = True, legend=False) $ plt.ylabel('Precipation', fontsize=20) $ plt.xlabel('Date', fontsize=20) $ plt.title("Precipition in Hawaii from %s to %s" % (twelve_months_prcp.index.min(),twelve_months_prcp.index.max()), fontsize=24) $ plt.show()
archive_df_clean['rating_numerator'] = archive_df_clean.rating_numerator.astype(float)
kmf.median_
data.describe()
p = pd.Period('2014-07', freq='M')
temps_df[['Philadelphia', 'Missoula']]
modelXg.fit(X,y)
naimp.data_isna.corr()
INT.loc[:,'MONTH'] = INT['Create_Date'].apply(lambda x: "%d" % (x.month))
na_df
pd.DataFrame(data.target_names)
to_be_predicted_Day1 = 50.85 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
data = session.query(hi_measurement).first() $ data.__dict__
pold = df2[df2['landing_page']=='old_page']['converted'].mean() $ pold
df2.drop('drop_me', axis = 1, inplace = True) $ df2.head()
pred8 = nba_pred_modelv1.predict(g8) $ prob8 = nba_pred_modelv1.predict_proba(g8) $ print(pred8) $ print(prob8)
control_group = df2.query('group == "control"') $ print(control_group.shape[0])
import statsmodels.api as sm $ logm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])
communities=[int(i[0:i.index('.')]) for i in labels]
df_data.SOLUCAO.value_counts()
ssh_host = '192.168.31.56' $ ssh_username = 'itouchtv' $ ssh_password = 'itouchtv'
def yup(text): $     text =[word for word in text.split(',')] $     text =[word.strip() for word in text] $     return(text)
df_goog.sort_values('Date', inplace=True)    # This is a good idea to sort our values so the indexes ultimately line up $ df_goog.set_index('Date', inplace=True)      # also df_goog.index = df_goog['Date'] works well here $ df_goog.index = df_goog.index.to_datetime()  # Convert to datetime
lr = LogisticRegressionCV(n_jobs=3) $ lr.fit(X_train[['avg_shifted_against', 'def_shift_pct']], y_train)
sleep = pd.DataFrame({ $     'extra': [0.7, -1.6, -0.2, -1.2, -0.1, 3.4, 3.7, 0.8, 0, 2, 1.9, 0.8, 1.1, 0.1, -0.1, 4.4, 5.5, 1.6, 4.6, 3.4], $     'group': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], $     'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] $ })
plt.figure(figsize=(8, 5)) $ train_df.groupby('flow').favs.median().plot.bar() $ plt.title('Median of the #favs by flow'); $ plt.xticks(rotation='horizontal');
data.sort_values(by='Date',ascending=False,inplace=True) $ data.head()
df_events['prefecture'] = df_events.prefecture.map(prefecture_dict) $ df_users['prefecture'] = df_users.prefecture.map(prefecture_dict) $ df_users['gender'] = df_users.gender.map(gender_dict) $ df_log['payment_method'] = df_log.payment_method.map(payment_method_dict) $ df_events['interest'] = df_events.interest.map(interest_dict) $
temp_df.groupby('reorder_interval_group')['Order_Qty'].mean()
logistic = sm.Logit(df2['converted'], df2[['intercept', 'treatment']]) $ model = logistic.fit()
stacked
shows[['primary_genre','secondary_genre']] = shows[['primary_genre','secondary_genre']].apply(lambda x: x.astype('category')) $ shows.to_pickle("ismyshowcancelled_final.pkl") $ shows.to_csv("ismyshowcancelled_final.csv")
data.head(10)
ac['Compliance Review End'].groupby([ac['Compliance Review End'].dt.year]).agg('count')
df.isnull().sum()
plt.hist(p_diffs) $ plt.axvline(x=-0.001576, color='r') $ plt.title('Distribution of p_diffs')
df = pd.DataFrame({'ImageId': imageids, 'Label': predCategories}) $ print (df.shape) $ print (df.head()) $ print (df.tail())
maxitems = 10 $ print "Brighton tweets retrieve testing" $ print '----------------------------------' $ for tweet in tweepy.Cursor(api.search, q="place:%s" % place_id_B).items(maxitems): $     print tweet.text
print(odometer_high['price'].mean()) $ print(odometer_mid['price'].mean()) $ print(odometer_low['price'].mean())
df_master.drop(['Unnamed: 0'],axis=1,inplace=True) $ df_master.tweet_id = df_master.tweet_id.astype(str)
df2['converted'].mean()    #convertion rate of people regardless of the page they receive
damd = pd.read_csv("20170718 hashtag_damd uncleaned.csv") $ damd.columns
sp500.iat[0, 1]
archive_copy.puppo.unique()
learner.save_encoder('adam3_10_enc') $
n_new=145310 $ new_page_converted=np.random.binomial(1, mean_new, n_new) $
with rio.open(local_orig) as src: $     data = src.read(indexes=1) $     pyplot.imshow(data)
df = pd.read_sql_query(q , conn) $ df # giving error $
table = df[['text', 'timestamp_ms', 'coordinates', 'id', 'user_location']].set_index('id')
col = ["num_25", "num_50", "num_75", "num_985", "num_100", "num_unq", "total_secs"] # remove date# ignore time dimension $ d = user_logs.groupby(["msno"], as_index=False)[col].sum()    $ d[d.msno == '29V0Jm3Xli1dy9UFeEL/BH2EMOr62DgeGLeKAKfE07k=']
endpoint = "https://api.twitter.com/1.1/statuses/oembed.json?id={tid}&omit_script=0&maxwidth=500" $ res = requests.get(endpoint.format(tid=df_tweets.index[0])) $ res
print(actual_value_second_measure[actual_value_second_measure==2]) $ holdout_results.loc[holdout_results.wpdx_id == ('wpdx-00063550') ]
tweet_counts_by_hour = pd.concat([tweet_counts_by_hour.groupby('timestamp').sum(), $                                   tweet_counts_by_hour.groupby('timestamp').count()['favorite_count'].rename('tweet_count')], $                                  axis = 1) $ tweet_counts_by_hour.index.names = ['hour of day'] $ tweet_counts_by_hour.plot()
P_Converted = (df2.converted==1).mean() $ P_Converted
del df_final['name_islower']
ct_df = pd.read_csv('countries.csv')
df_A.loc[df_A.index.str.endswith(('1','4'))] 
hr_total[hr_total.notnull()]
df_293 = pd.read_sql(sql_293,conn_laurel) $ df_293.groupby(['accepted','paid','preview_clicked','preview_watched','preview_finished'])['applicant_id'].count().unstack().fillna(0)
log_mod=LogisticRegression() $ log_mod.fit(x_train,y_train) $ y_preds=log_mod.predict(x_test)
total_users = df2['user_id'].nunique() $ total_users
np.mean(df.query('group == "treatment"')['converted'])
a * b
final=final[(final.dec<30)&((final.ra<195.)|(final.ra>330.))] $ print final.shape, final.groupby('priority',axis=0).get_group('A').shape, final.groupby('priority',axis=0).get_group('B').shape,\ $ final.groupby('priority',axis=0).get_group('C').shape, final.groupby('priority',axis=0).get_group('D').shape $
reddit_data
df.head()
driver = webdriver.Chrome(executable_path="./chromedriver")
allqueryDF.head()
np.cov(Xs.T)
combined_df3.shape
DataDescriptionFileName = "DataDescription.json" $ with open(DataDescriptionFileName, 'w') as f: $     json.dump(DataDescription, f, indent=4) $ print("Saved data description to {}".format(DataDescriptionFileName))
datatmp.pivot_table(index=datatmp.index,values="Total Gallons",aggfunc=sum)#.plot(legend=False) $
df[['name', 'age']]
len(fda_drug_names)
newdf.head()
bigdata = ['Big Data Class', 'Is Really Fun', 'I Love Python'] $ for i in [1, 2]: $     print(bigdata[i])
from sklearn.naive_bayes import MultinomialNB $ nb = MultinomialNB() $ nb.fit(X_train, y_train)
sites_on_net.rename(columns={'Building ID': '# Buildings on Net'}, inplace=True) $ sites_no_net.rename(columns={'Building ID': '# Buildings not on Net'}, inplace=True)
SST=((y.mean() - test.readingScore)**2).sum() $ print SST
plt.figure(figsize = (6,6)) $ kmeans.fit_predict(X_std) $ plt.scatter(X_std[:,0],X_std[:,1], c=kmeans.labels_, cmap='rainbow')  
fact_url = "http://space-facts.com/mars/"
df.tail(2) # prints the last 2 rows
len(df_enhanced[df_enhanced['dog_name'] != 'NaN'])
pd.value_counts(ac['Sector']).head()
blob = TextBlob(text.encode('ascii','ignore'))
probs
requests.get(wikipedia_meritocracy)
df3 = df.copy() $ df3 ['E'] = ['q', 'w', 'e', 'r', 't', 'y', 'g', 'j', 'k', 'l', 'f', 's'] $ df3
intForMaxTS=interactionsData[(interactionsData['interaction_type']>0) & (interactionsData['interaction_type']!=4)] $ intForMaxTS=intForMaxTS.groupby(['user_id','item_id'])['created_at'].max() $ intForMaxTS=intForMaxTS.to_frame()
df_img_algo_clean.head()
table = pd.crosstab(df["grade"], df["loan_status"], normalize=True) $
precipitation_df = pd.DataFrame(sq.prec_last_12_months())
pieces = [df[:3], df[3:7], df[7:]] $ pieces
comm_merge.head()
circle_companies = pd.DataFrame(graph.run("MATCH p=(c1:Company)<-[:CONTROLS*1..]-(c1:Company)\ $ RETURN DISTINCT (c1.company_number)").data()) $ active_companies[active_companies.CompanyNumber.isin(circle_companies['(c1.company_number)'])].to_csv('data/for_further_investigation/circular_ownership.csv') $ len(circle_companies)
np_tr = ab_data[(ab_data['group']=='treatment') & (ab_data['landing_page']=='new_page') ].count()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ z_score, p_value # but here, P_value is for two_tailed test..we need one tailed test. 
(call.iloc[-1].Prima-2.6*sim_closes.iloc[-1].std()*np.exp(-r*ndays)/np.sqrt(nscen),call.iloc[-1].Prima+2.6*sim_closes.iloc[-1].std()*np.exp(-r*ndays)/np.sqrt(nscen))
mini_df.groupby(['wait', 'month']).var()
fig = acc.plot_history(startdate='01.08.2016', enddate='23.09.2016', what='cumsum')
corrplot(fin_r_monthly.loc[start_date:end_date].corr(), annot=True) $ plt.title('Correlation matrix - monthly data \n from ' + start_date + ' to ' + end_date) $ plt.show()
plt.figure(figsize=(10,5)) $ plt.plot(df['datetime'], df['distance']) $ plt.xlabel('time') $ plt.ylabel('distance') $ plt.show()
min(SCN_BDAY_qthis.start_date)
LATENT_DIM = 5 # number of units in the dense layer $ BATCH_SIZE = 32 # number of samples per mini-batch $ EPOCHS = 50 # maximum number of times the training algorithm will cycle through all samples
df.name.value_counts()[0:19].plot(kind='bar');
seaborn.countplot(company_vacancies.weekday)
print(train_data.shape) $ print(test_data.shape)
hn = pd.read_csv('../data/HN_posts_year_to_Sep_26_2016.csv', index_col='id', parse_dates=['created_at']) $ hn.dtypes
vip_reason.columns = ['VIP_'+str(col) for col in vip_reason.columns]
charge_counts = df.groupby('hash_id')['cause_num'].count()
missing_map(df_airbnb, nmax=200)
df[df['status_type']=='video'].groupby('dayofweek').status_id.count()
xgb = XGBClassifier(objective='binary:logistic') $ xgb.fit(X_train, y_train) $ test_predictions = xgb.predict(X_test) $ eval_sklearn_model(y_test, test_predictions, model=xgb, X=X_test)
station_counts = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).\ $                 order_by(func.count(Measurement.tobs).desc()).all() $ for station, count in station_counts: $     print(f"Station: {station}, Observations: {count}")
bnbAx[bnbAx['gender_female']==1]
greater_than_10 = twitter_archive[twitter_archive['rating_denominator']>10] $ for idx in range(len(greater_than_10)): $     observation = greater_than_10.iloc[idx] $     print("tweet id: {}\n rating: {}/{}\n text: {}".format(observation['tweet_id'],observation['rating_numerator'],observation['rating_denominator'],observation['text']))
stringlike_instance
h = plt.hist(tag_degrees.values(), 100) #Display histogram of node degrees in 100 bins $ plt.loglog(h[1][1:],h[0]) #Plot same histogram in log-log space
pp.pprint(r.json())
stopword_list = stopwords.words("german")   #saves German stop words in a list $ print(len(stopword_list),"stop words in the list.")   #Prints number (len()) of elements in a list. $
data[data.title == 'Collaborative Filtering and Embeddings']
df_never_moved['Long'].value_counts().head(5)
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key="+API_KEY)
autos.columns = new_cols
print faa[-20:]
print(autos['odometer_km'].unique().shape) $ print(autos['odometer_km'].describe())
prop['bedroomcnt'].nunique(dropna=False)
df_counts = df2.user_id.value_counts().to_frame('counts') $ df_counts[df_counts.counts > 1]
msftAV = msft[['Adj Close', 'Volume']] $ aaplAV = msft[['Adj Close', 'Volume']] $ pd.concat([msftAV, aaplAV])
df_h1b_mv = df_h1b_mv.drop([123031])
airbnb_df['room_type'].value_counts(dropna=False)
train_df.info
cityID = 'af2a75dbeb10500' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Lincoln.append(tweet) 
import pandas as pd $ %matplotlib inline
for row in qres: $     print("%s is an structured name %s is one of it's names" % row)
print(len(clinton_tweets)) $ clinton_tweets[0]
df_merge = pd.merge(archive_clean, tweet_clean, on = 'tweet_id', how = 'inner' )
autos["odometer_km"].value_counts()
df_image_tweet2.head()
google_stock.corr()
dat = pd.read_csv('./311_clean.csv',parse_dates=[0,2]) $ dat.head(2)
with open(os.path.expanduser('~/.secrets/twitter_thebestcolor.yaml')) as f: $     creds =  yaml.load(f)
os.chdir(folder_source_of_files) $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.chdir('../') $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $
f=pd.DataFrame(f) $ f=f.reset_index() $ f=f.set_index(keys='Date') $ f=f.drop('Symbol',axis=1) $ f.head()
sns.barplot(df['Month'], df['Sale (Dollars)'], estimator = sum)
station_count = session.query(Stations.id).count() $ print ("Total Number of Stations are: "+ str(station_count))
control_num = df2.query('group=="control"').shape[0] $ control_num
model.get_params()
pd.Series(pnl_array).hist(figsize=[15,5])
session, train, test, age_gender, country, sample_submission = data_reader()
autos.info() $
df.head(2)
temp_cat = pd.Categorical(temp, categories=['low','medium','high']) $ temp_cat
exploration_titanic.dfquantiles(nb_quantiles=10)
temp_pred.dtypes
dates = pd.date_range('2018-03-22',periods=ndays).astype('str') $ sim_ret_hist = pd.DataFrame(np.random.choice(values[1:], (ndays,nscen), p = prob), index=dates) $ sim_closes_hist = (closes_aapl.iloc[-1].AAPL)*np.exp(sim_ret_hist.cumsum())
signals[signals.positions > 0]
if not os.path.isdir('output'): $     os.makedirs('output')
flight6.filter((flight6.trip==1) & \ $                (flight6.start_date=='2017-10-01') & \ $                (flight6.company=='AirAsiaX')) \ $     .select('duration', 'search_date', 'price', 'future_min_price', 'price_will_drop') \ $     .show(100)
conditions.nunique()
for row in example1_df.take(2): $         print row $         print "*" * 20
ncTest.ncattrs()
print("LOCATION") $ DataSet["userLocation"].value_counts() $ print("TIMEZONE") $ DataSet["userTimezone"].value_counts()
sessions_summary = pd.DataFrame(data=sessions_sample.groupby(["user_id"])["action"].apply(list))
total_Visits_Convs_month_byMC.loc[total_Visits_Convs_month_byMC.conversion_rate > 1]  # outlier - this data may be incorrect $
pd.DatetimeIndex(pivoted.columns).dayofweek
df_predictions['p3_dog'].value_counts()
my_list_of_dicts = [] $ for each_json_tweet in list_of_tweets: $     my_list_of_dicts.append(each_json_tweet._json)
Results_kNN100 = Results_kNN100[['ID', 'Approved']] $ Results_kNN100.head()
plt.subplots(figsize=(6, 4)) $ sn.barplot(train_session_v2['isNDF'],train_session_v2['search_results'])
cens_key = open(os.getenv('PUIDATA')+'/census_key.txt', 'r+') $ myAPI = cens_key.readlines()[0] $
days_of_week = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']
df2['abtest'].value_counts().plot(kind='bar') $ print(df2['abtest'].value_counts())
with open('../data/categories.json', 'r') as f: $     video_categories = json.load(f)
from pysumma.Simulation import Simulation
class Review_Item(scrapy.Item): $     author = scrapy.Field() #author of review $     date = scrapy.Field() #date published $     rating = scrapy.Field() #rating (1-5) $     text = scrapy.Field() #content of review
date_cols = [u'Request Status', u'Created Date', u'Assigned Date', u'Sub Assigned Date', \ $              u'Escaltion Date', u'Due Date', u'Complete Date', u'Close Date', u'Service Location', \ $              u'Actual Close Datetime', u'Hold Date', u'Total Hold Time (In Minutes)', u'Reopen Count']
lm = sm.Logit(df2['converted'], df2[['intercept', 'treatment']])
c = news.comments.fillna(0) $ plt.figure(figsize=(16, 5)); $ c.hist(bins=len(np.unique(c))); $ plt.title('Distributon of the number of comments per news article');
dsi_me_1_df.head(3)
cityID = '9531d4e3bbafc09d' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Oklahoma_City.append(tweet) 
print ("Filtered records for query 3 : ", len(final_location_ll))
cohort_retention_df = cohort_retention_df.append(cohort_retention_sum)
data = np.array(['a','b','c','d']) $ ser2 = pd.Series(data,index=[101,102,103,104]) #manually passing the index of the array $ ser2
df2[df2['country'].unique()] = pd.get_dummies(df2['country']) $ df2.head()
np.exp(-2.0397)
df2
Xfinal = X_main.iloc[holdout_idx:, :][cols_final] $ yfinal = y_main[holdout_idx:]
p_diffs_in_ab_data = p_treatment_converted - p_control_converted $ num_p_diffs_greater = len(p_diffs[p_diffs > p_diffs_in_ab_data]) $ num_p_diffs_greater / len(p_diffs)
df_pol.pol_id.value_counts()
for k in range(len(list_of_py)): $     print(os.path.abspath(list_of_py[k])) $     shutil.copy(os.path.abspath(list_of_py[k]), str(holding_file_name)) # target filename is /dst/dir/file.ext $
avg_arrests = stadium_arr.pivot_table(index='season', values='arrests', aggfunc='mean') $ avg_arrests
labeled_features['failure'].value_counts()
np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)
snow.select("select count(distinct claim_number) from st_pharma_cohort_a")
joined = joined.merge(trend_de, 'left', ["Year", "Week"], suffixes=('', '_DE')) $ joined_test = joined_test.merge(trend_de, 'left', ["Year", "Week"], suffixes=('', '_DE')) $ len(joined[joined.trend_DE.isnull()]),len(joined_test[joined_test.trend_DE.isnull()])
print('Features matrix first row: {}'.format(get_numpy_data(sales, ['sqft_living'], 'price')[0][0, :])) $ print('Prices vector first value: {}'.format(get_numpy_data(sales, ['sqft_living'], 'price')[1][0]))
df = pd.read_csv('twitter_archive_master.csv') $ df['timestamp'] = pd.to_datetime(df['timestamp']) $ df.set_index('timestamp', inplace=True)
rec_spend = budg[(budg['Department Code'] == 'REC') & (budg['Revenue or Spending'] == 'Spending')] $ rec_spend.groupby(['Fiscal Year'])[['Amount']].sum().plot(kind='bar');
df_new.groupby('country')['converted'].mean()
handler = newHandler(path) $ print(handler)
data.iloc[1]
tree_features_df['p_hash'].describe()
df_unique_providers.head()
finals[finals.PLAYER_NAME.str.contains("Durant")]
df2[df2.duplicated(['user_id'])]['user_id']
s < 0
ctc = ctc.fillna(0)
df_new.country.unique()
question1_word_sequences[0]
IndianaNews
results.summary2()
rf = pickle.load(open('../data/model_data/size_mod.sav', 'rb'))
df_ab_converted_portion = df_ab_raw.groupby(['converted']).count() $ df_ab_converted_portion.reset_index(inplace = True) $ df_ab_converted_portion['porportion'] = df_ab_converted_portion['user_id'] / df_ab_converted_portion['user_id'].sum()
local_sea_level_stations.columns = [name.strip().replace(".", "") $                                     for name in local_sea_level_stations.columns] $ local_sea_level_stations.columns
from pymongo import MongoClient $ client = MongoClient() $ bitcoin = client.test_database.bitcoin
from pyspark.sql.functions import lit, concat $ bad_content_size_df.select(concat(bad_content_size_df['value'], lit('*'))).show(truncate=False)
np.unique(raw_train_y)
df_new.query('group == "treatment" and country =="CA"')['converted'].mean()
season07 = ALL[(ALL.index >= '2007-09-06') & (ALL.index <= '2008-02-03')] # This means every transaction between 9-6-07 and $
data_set.to_csv("14nov17-17.30.csv")
conv_mean = df2['converted'].mean() $ conv_mean
df_transactions['discount'].unique()
dfD.to_sql('distances', conn_aws)
train_df = stats_diff(train_df) $ print(train_df.head(5))
m2 =m[:,[1,2]] $ m=np.delete(m,[1,2],1) $ print(m.shape) $ print(m2.shape) $ print("m2: ", m2)
target_google = people_person[people_person['channel'] == "Google"] $ target_google.head()
bb.plot()
for df in reader.iterate_over_events(): $     break $ df
today = datetime.now() $ print(today)
robust_cov_matrix_b= pd.DataFrame(np.insert((np.insert(skcov.ShrunkCovariance().fit(daily_ret).covariance_,len(tickers),0,axis=0)),len(tickers),0,axis=1) $ ,columns=daily_ret_b.columns,index=daily_ret_b.columns) $ robust_cov_matrix_b
targetUserItemInt['label']=[((1.0 if i1>0 else 0.0)+(5.0 if (i2>0 or i3>0) else 0.0)+(20.0 if i5>0 else 0.0)+\ $                              (-10.0 if i1==0 and i2==0 and i3==0 and i4>0 and i5==0 else 0.0) $                             )*(2.0 if pr==1 else 1.0) $                             for i1,i2,i3,i4,i5,pr in zip(targetUserItemInt['1'],targetUserItemInt['2'],targetUserItemInt['3'],\ $                                                       targetUserItemInt['4'],targetUserItemInt['5'],targetUserItemInt['premium'])] $
import random
data.whitelist_status.unique()
dat_wkd.vgplot.line(value_name='Hospital mortality rate')
len(df_all_wells_wKNN)
daily[['Total', 'predicted']].plot(alpha=0.5);
inspector = inspect(engine) $ inspector.get_table_names()
df2 = df.drop(index = control_new_page.index) $ df2 = df2.drop(index = treatment_old_page.index)
control['converted'].sum() / control.shape[0]
df2 = df.query("(group == 'control' and landing_page == 'old_page') or (group == 'treatment' and landing_page == 'new_page')")
p_new = df2[df2['converted'] == 1].user_id.nunique()/df2.user_id.count() $ p_new
station_distance['Start Coordinates'] = station_distance['Start Station Latitude'].astype(str) \ $     + " , " + station_distance['Start Station Longitude'].astype(str) $ station_distance['End Coordinates'] = station_distance['End Station Latitude'].astype(str) \ $     + " , " + station_distance['End Station Longitude'].astype(str)
df.head()
f.all_filters
x["sum"]=x[list(x.columns)].sum() $ x
cov = pyemu.Cov.from_ascii(os.path.join(new_model_ws,m.name+".pst.prior.cov"))
itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq] $ itos.insert(0, '_pad_') $ itos.insert(0, '_unk_')
tree_features_df['filename'].isin(manager.image_df['filename']).describe() $
train[simple_features].head(1)
clinton_pivoted.plot.barh()
businesses = df_training.select('business_id').distinct() $ df_test = df_test.join(businesses, on='business_id')
print classification_report(etc_pred, y_test) $ print 'ROC AUC Score: ', roc_auc_score(etc_pred, y_test)
uso17_coll, db = au.get_coll("usopen17") $ uso17_qual_coll, _ = au.get_coll("usopen17_qual")
consumer_key = "" # Use your own key. To get a key https://apps.twitter.com/ $ consumer_secret = "" $ auth = tweepy.OAuthHandler(consumer_key=consumer_key, consumer_secret=consumer_secret) $ api = tweepy.API(auth)
df.info()
both_dfs = pd.concat(list_of_df)
import datetime as dt $ from datetime import datetime $ from dateutil.relativedelta import relativedelta $ import numpy as np $
vip_df = pd.read_csv("/Users/erikgregorywebb/Downloads/VIPKID_Raw.csv") $ vip_df.head()
prediction_dataframe=pd.DataFrame(prediction_data,columns=(predictions['fields'])) $ prediction_dataframe.iloc[:,0:1].shape
git_log = git_log.sort_index() $ git_log[git_log['author'] == 'Linus Torvalds'].head(10)
engine.execute('SELECT * FROM measurement LIMIT 10').fetchall()
import shapefile $ import numpy as np # Note: numpy is a part of Pandas pack: you can access numpy via pandas.np or pd.np $ sakhalin_shp = shapefile.Reader("shapefiles/sakhalin.shp")
import math $ math.ceil(37670294/1000000)
r.html.absolute_links
df_twitter_copy = df_twitter_copy.drop(['doggo', 'floofer', 'pupper', 'puppo'], axis = 1)
df.tail()
future = m.make_future_dataframe(periods=52*3, freq='w') $ future.tail()
r.headers['content-type']
to_be_predicted_Day3 = 36.48931367 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
num = it_df.isnull().sum() $ den = len(it_df) $ a = round(num/den, 2) $ a.sort_values()
for o in ['Before', 'After']: $     for p in columns: $         a = o+p $         df[a] = df[a].fillna(0).astype(int)
def get_date(created): $     return datetime.fromtimestamp(created) $ topics_data = topics_data.assign(timestamp = topics_data["created"].apply(get_date)) $ topics_data = topics_data.drop('created', axis=1) # remove the 'created' column
obs_diff = treatment_conversion - control_conversion
df_clean.to_csv('twitter_archive_master.csv', encoding='utf-8', index=False) $
plt.plot(times)
os.system("mv %s %s"%("departureZIP.csv", os.getenv("PUIDATA"))) $ os.system("mv %s %s"%("destinationZIP.csv", os.getenv("PUIDATA"))) $ departureZip = pd.read_csv('http://cosmo.nyu.edu/~fb55/data/departureZIP.csv') $ destinationZip = pd.read_csv('http://cosmo.nyu.edu/~fb55/data/destinationZIP.csv')
errors['datetime'] = pd.to_datetime(errors['datetime'], format="%Y-%m-%d %H:%M:%S") #string to date time format.V:Wasn't datetime already in the ymd-hms format?? $ errors.count()
linkNYC = gpd.GeoDataFrame(linkNYC,geometry='geometry') $ linkNYC.head()
import pandas as pd $ import requests $ from fuzzywuzzy import fuzz $ from collections import Counter $ from sklearn import preprocessing
data.head(10)
archive_copy['id_str'] = archive_copy['tweet_id'].astype(str)
(p_diffs>obs_diff).mean()
l=read_cvs_by_pands(path_database,'lesk_20.csv',None,0) $ l
cryptos.loc[0]
len(train_data[train_data.fuelType == 'elektro'])
X_train = train.drop(axis=1, labels=['loan_status', 'default']) $ y_train = train.loan_status
df_new['intercept'] = 1 $ log_mod = sm.Logit(df_new.converted,df_new[['intercept','CA','UK','new_page']]) $ results = log_mod.fit() $ results.summary()
print("The maximum value of artistID:") $ userArtistDF.agg(max("artistID")).show()
import requests $ import json $ from pprint import pprint $ import pickle
daily_df.reset_index(inplace=True)
openmc.plot_geometry(output=False)
news_title = soup.find('div', class_='content_title').text.strip() $ news_title
df_twitter_copy = df_twitter_copy.drop(['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'], axis = 1)
m3 = md2.get_model(opt_fn, 1500, bptt, emb_sz=em_sz, n_hid=nh, n_layers=nl, $            dropout=0.1, dropouti=0.4, wdrop=0.5, dropoute=0.05, dropouth=0.3) $ m3.reg_fn = partial(seq2seq_reg, alpha=2, beta=1) $ m3.load_encoder(f'adam3_20_enc')
print len(stations.name.unique()) $ print stations.shape
x = df_final.img_num $ y = df_final.rt_count $ data = [y[x == i] for i in range(1, 5)]
baseball_df.info()
wrd_full.query('favorite > 9447')['hour'].value_counts()
model = RandomForestRegressor(n_estimators=200, max_leaf_nodes=1000, $                               max_features=0.1, n_jobs=-1, verbose=1, $                               random_state=42)
apple_tweets2.to_pickle('../data/apple2.pkl')
autos['registration_year'].describe()
df1 = df[df['Title'].str.contains(search_terms)] $ df1.shape
lm=sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ r=lm.fit()
st_streams
pickle.dump(nmf_tfidf, open('iteration1_files/epoch3/nmf_tfidf.pkl', 'wb'))
dfMonth.set_index('Date').to_csv('PipelineInventorySales_Monthly.csv')
autos["price"].describe().apply(lambda x: format(x, 'f')) #Found this lambda to better view info.
df['Miles'] = df.Miles[df.Miles > 0] $ df.head(10)
"cost is: {}".format(cost)
negGroups = list(neg_tweets.group_id_x) $ num_convos = len(set(negGroups)) $ print(f'Working with {num_convos} conversations') $ companyNeg = filtered[filtered.group_id.isin(negGroups)] $
import pandas $ from pandas.io import gbq $ df = gbq.read_gbq(query=query, dialect='standard', project_id=os.environ['PROJECT'], verbose=False) $ df.head()
merged.shape
x = cat_outcomes.filter(items=['sex_upon_outcome', 'breed', 'color', 'coat_pattern', $                                'domestic_breed', 'dob_month', 'age_group', 'outcome_month', $                                'outcome_weekday', 'outcome_hour', 'Cat/Kitten (outcome)'])
import nltk $ nltk.download('stopwords')
train_holiday_oil_store_transaction_item_test_004 = train_holiday_oil_store_transaction_item_test_004.drop('city', 'state', 'store_type', 'cluster') $
the_list
df_new['day_part']= df_new.strfrmt.apply(lambda x: day_part(int(x.hour)))
df['Long'] = df['2017-09-07 23:00:02'].apply(split1) $ df['Lat'] = df['2017-09-07 23:00:02'].apply(split2) $ df['Timestamp'] = '2017-09-07 23:00:02' $ del df['2017-09-07 23:00:02']
utility_patents_subset_df['figure_density'] = utility_patents_subset_df['number-of-figures'] / utility_patents_subset_df['number-of-drawing-sheets'] $ utility_patents_subset_df['figure_density'].describe()
print(financial_crisis)
df_2015.dropna(inplace=True) $ df_2015
%%R $ numericDB <- select(flightsDB, -c(CRS_DEP_TIME, CRS_ARR_TIME)) $ write.csv(numericDB, 'FinalFlightsNumeric.csv')
pop_con = len(df2[df2.group == "control"]) $ pop_con
def one_year_csv_writer(this_year, all_data, path, name): $     surveys_year = all_data[all_data.year == this_year] $     filename = path + name + str(this_year) + '.csv' $     surveys_year.to_csv(filename) $ one_year_csv_writer(1997, surveys_df, './data/', 'function_surveys')
for family in ['academia', 'industry']: $     family_accounts = [account for account in enabled_accounts_in_selected_period $                        if account['family'] == family] $     print(f"in family {family}, {len(family_accounts)} new enabled accounts")
import requests
p_mean = (pnew + pold) /2 $ p_mean
karma_per_comment = (karma_per_user / comments_per_user) $ karma_per_comment.sort_values(ascending=False)
df_min_max.head()
import pandas 
respond = requests.get("https://en.wikipedia.org/wiki/List_of_best-selling_music_artists") $ soup = BeautifulSoup(respond.text) $ l = soup.find_all('tr') $ pprint(l[1].text)
1/np.exp(-0.0507)
!ls
reddit_comments_data.groupBy('author').agg({'subjectivity':'mean'}).orderBy('avg(sentiment)', ascending = False).show()
result4 = sm.ols(formula="HPr_RF ~ Mkt_RF + SMB + HML", data=tbl3).fit() $ result4.summary()
simdata=(closes.loc['2016-12-30',:].AAPL)*np.exp(simret.cumsum()) $ simdata
print(trump_tweets[0]._json['created_at'])
dog = shelter_cleaned_df.loc[shelter_cleaned_df['AnimalType'] == 'Dog'] $ dog.describe()
gmap.heatmap(data['Latitude'],data['Longitude'])
simple_cv(train_df,test_df)
experiment_details = client.repository.store_experiment(meta_props=experiment_metadata) $ experiment_uid = client.repository.get_experiment_uid(experiment_details) $ print("Experiment UID: "+ experiment_uid)
mar = pd.read_excel(mar_file)
temp = combined_salaries.groupby('cleaned_job_title').count().salary.reset_index() $ jobs_to_model = temp[temp.salary >= parameters['min_salary_records']] $ combined_salaries = combined_salaries[combined_salaries.cleaned_job_title\ $                                                        .fillna('').isin(jobs_to_model.cleaned_job_title)] $ print("Number of jobs with "+str(parameters['min_salary_records'])+"+ salary records:", jobs_to_model.cleaned_job_title.count())
sgm_malthus_run(T=801, figure_title = "Demographic Transition", $     h=0.0000805, phi=0.01, ybar = 1500)
df_test_index = pd.merge(df_test_index[event_list['event_start_at'] > df_test_user['created_on']], $                             log_user1[event_list['event_start_at'] > df_test_user['created_on']], on='event_id', how='left') $ df_test_index
scaled_df = scale_df(source_df, ['date']) $ scaled_df.tail() $ scaled_df.pop('date')
type(twitter_archive_full.timestamp[0])
df['rating_numerator'].value_counts()
train.info()
PROCESSED_DATA_DIR = 'processed_data' $ clean_appt_df = pd.read_csv(PROCESSED_DATA_DIR + "/clean_appt_df.csv", $                            parse_dates=['AppointmentDay', 'ScheduledDay']) $ clean_appt_df['No-show'].value_counts() / len(clean_appt_df)
print finalGoodTargetUserItemInt.shape $ finalGoodTargetUserItemInt.head()
vid_list = ['411799','22786099','20728529','2122212','12312312312312'] $
df_new['US_ind_ab_page'] = df_new['US']*df_new['ab_page'] $ df_new['CA_ind_ab_page'] = df_new['CA']*df_new['ab_page'] $ logit_h = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'US', 'CA', 'US_ind_ab_page', 'CA_ind_ab_page']]) $ results = logit_h.fit() $ results.summary()
dfn['goal_log'] = np.log10(dfn['goal'].values)
df_joy.describe()
iter_visits = pd.read_csv(visits_path, iterator=True, chunksize=1000000) $ users_visits = Users.assign(visits=lambda x: pd.Timedelta(0, unit='d')) $ users_visits = last_visit(iter_visits, users_visits, 0)
np.exp(df3_results.params)
trend = pd.merge(df.reset_index(), topic_geo_infomation, on='date')
std_for_each_weekday['day_order'] = pd.Series(lst, index=std_for_each_weekday.index) $ std_for_each_weekday
df.to_csv("test_df_to_csv.csv",index=False)
df_prep1 = df_prep(df1) $ df_prep1_ = pd.DataFrame({'date':df_prep1.index, 'values':df_prep1.values}, index=pd.to_datetime(df_prep1.index))
rhum_df = pd.DataFrame(data = us_rhum) $ rhum_df.columns = ts.dt.date $ rhum_wide_df = pd.concat([grid_df, rhum_df], axis = 1) $ rhum_wide_df.head()
twitter_archive_clean['dog_stage'].value_counts()
import statsmodels.api as sm $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = df2.query("group == 'control'")['converted'].count() $ n_new = df2.query("group == 'treatment'")['converted'].count()
first_result.find('strong')
data.dropna(axis=1)
get_url = 'https://staging.app.wikiwatershed.org/api/jobs/{job}/'.format
df.fillna(0.0,inplace=True) $ dfg = df.groupby("date").mean() $ dfg.rename(columns={"prcp":"precipitation"},inplace=True) $ dfg.head(5)
df['count'] = temp['count']
country.head()
pd.DataFrame(features['MEAN(payments.payment_amount)'].head())
df2[['CA','UK','US']] = pd.get_dummies(df2['country']) $ df2.head()
from unidecode import unidecode $ def get_entitie(obj, key, name): $     return [unidecode(entity.get(key, u'').lower()) for entity in obj[name]] $ hashtags_series = df.entities.apply(lambda obj: get_entitie(obj, 'text', 'hashtags')).dropna() $ mentions_series = df.entities.apply(lambda obj: get_entitie(obj, 'screen_name', 'user_mentions')).dropna()
archive_clean[archive_clean['tweet_id'] == 883482846933004288].rating_numerator
norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True)) $ normalized_embeddings = embeddings / norm $ valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset) $ similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True) $
import pandas as pd $ import numpy as np
full.groupby(['Dx2'])['<=30Days'].agg({'sum':'sum','count':'count','mean':'mean'}).sort_values(by='mean',ascending=False)
filtered_words[:5] $ for i in range(len(filtered_words)): $     filtered_words[i] = filtered_words[i].lower() $ filtered_words[:5]
calc_temps('2016-02-01', '2016-02-11')
users.to_pickle('data/pickled/new_subset_users.pkl')
festivals.index
df_agg_op_rand.head()
filepath = 'C:\\Users\\jennifer.bosch\\Documents\\Metrics\\NDBC_GTS_metrics_2016.xlsx' $ df = pd.read_excel(filepath,0, index_col='Month') $ df.dtypes $
archive_copy.info()
STD_reorder_stats.describe()
count_pre_strategy_google = len(pre_strategy_google) $ count_pre_strategy_google
gbm_v1.score_history() $
twitter_archive.info()
data_archie = data_archie[data_archie['user_id'].notnull()] $
df.query('breed == "Not identified"').shape[0]
trump.dtypes
help(elasticsearch)
np.set_printoptions(precision=2, suppress=True)
print(len(df2.user_id.unique()))
station.isnull().any()
session.query(Measurement.id, func.min(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
train['business_day'] = train.date.isin(business_days) $ train['holiday'] = train.date.isin(holidays)
for col in time_cols: $     df[col] = df[col].astype('datetime64[s]')
run txt2pdf.py -o"2018-06-19 2015 UNIVERSITY HOSPITALS OF CLEVELAND Sorted by discharges.pdf"  "2018-06-19 2015 UNIVERSITY HOSPITALS OF CLEVELAND Sorted by discharges.txt"
yf.pdr_override()
model3 = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK','ab_page']]) $ model3.fit().summary()
cfs_df.TimeArrive = pd.to_datetime(cfs_df.TimeArrive) $ cfs_df.TimeCreate= pd.to_datetime(cfs_df.TimeCreate) $ cfs_df.TimeClosed = pd.to_datetime(cfs_df.TimeClosed) $ cfs_df.TimeDispatch = pd.to_datetime(cfs_df.TimeDispatch)
analyze_set['rating_numerator'].value_counts()
!head -5 /home/ubuntu/data/restaurant.csv
delta=datetime(2020, 9, 15)-datetime(2019, 9, 12, 6, 10) $ delta
sentiments_pd = pd.DataFrame(sentiments) $ sentiments_pd.head()
df.to_csv('alexa_clean.csv')
intersections_final_for_update_no_dupe.info()
merged.amount.sum()
print(archive_copy[archive_copy.new_rating_numerator == 'NaN']['text'][342]) $ print(archive_copy[archive_copy.new_rating_numerator == 'NaN']['text'][516]) $ print(archive_copy[archive_copy.new_rating_numerator == 'NaN']['text'][1663]) $
df['Media URL'].sample(10, random_state=42)
df_h1b_ft_US_Y = df_h1b_ft_US[df_h1b_ft_US.pw_unit_1=='Year']
TWEETS = get_tweets(EVENTS['matthew-geo'], None)
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline
from scipy.stats import norm $ norm.ppf(1-(0.05/2))
df2 = df2.drop_duplicates()
mars_hemispheres
daily_deltas = (hits_df.hits - hits_df.hits.shift()).fillna(0)
df.donation_date = df.donation_date.apply(pd.to_datetime) $ df.charitable = df.charitable.apply(bool) $ df.amount = df.amount.apply(int)
model_df.news_text = model_df.news_text.fillna('') $ model_df.tesla_tweet = model_df.tesla_tweet.fillna('') $ model_df.elon_tweet = model_df.elon_tweet.fillna('')
tableWebExtract = restaurantsExcelFile.parse(sheetname="WebExtract");
print(train_data.gearbox.isnull().sum()) $ print(test_data.gearbox.isnull().sum())
xgb_model.fit(X_train, y_train)
plt.figure(figsize=(20, 5)) $ distplot = sns.distplot(df.user_answer, bins=101, color='darkblue', kde=False)
weather.dtypes
scoresdf = pd.DataFrame(tree_scores) $ scoresdf.columns = ['n_estimators', 'time_taken', 'num_cols', 'score']
import pods $ from ipywidgets import IntSlider
df_merge.info()
parsed.pprint()
df3 = df_tweet_json_clean.copy() $ df3.sort_values(by = ['created_at']) $
df = pp.create_plotdata(results) $ tecs = ['fluorescent_tube', 'bulb', 'halogen_bulb', 'led_lamp']
lb = aml.leaderboard $ lb
countries_df = pd.read_csv('/Users/pra/Desktop/AnalyzeABTestResults 2/countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
births_by_date = births.pivot_table('births', $                                    [births.index.month, births.index.day]) $ births_by_date.head()
merged_data.drop(redundant_features, axis=1, inplace=True)
import statsmodels.api as sm $ logit_model=sm.Logit(y_all,X) $ result=logit_model.fit() $ print(result.summary2())
df3[['new', 'old']] = pd.get_dummies(df3['landing_page']) $ df3.head()
df
data.head()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative="smaller") $ z_score, p_value
with open('../data/faa.txt') as f: $     data = f.read()
df = pd.read_sql('SELECT a.address_id, a.city_id, c.city FROM address a JOIN city c ON a.city_id = c.city_id ORDER BY a.address_id DESC;', con=conn) $ df
token_sendreceiveCnt = pd.merge(token_sendcnt,token_receivecnt,on=["ID","month"],how="outer")
check_cols(v_item_hub, v_item_hub_dropper)
log_mod1 = sm.Logit(df3['converted'], df3[['intercept','ab_page' ,'country_UK','country_US']]) $ results1 = log_mod1.fit()
nypd_unspecified = data[(data['Borough']=='Unspecified')& $                        (data['Agency']=='NYPD')]['Borough'].count() $ nypd_unspecified
data.head()
pd.DataFrame(population, columns=['population'])
donors_c.iloc[2097169, 4]
df_temperature = pd.melt(df_temperature, id_vars=['Year'], var_name='Month', value_name='Temperature')
df = pd.DataFrame([tweet.text for tweet in tweets], columns=['Tweets'])
df_click.info()
df2['converted'].mean() $ print("{:.2%}".format(df2['converted'].mean()))
soup = BeautifulSoup(page, 'html5lib') $ print soup.prettify()[:1000]
df2['intercept'] = 1 $ df2['ab_page'] = 0 $ df2.loc[df2['group']=='treatment',['ab_page']]=1
diff_actual=df2.query('group == "treatment"')['converted'].mean()-df2.query('group == "control"')['converted'].mean() $ (p_diffs>diff_actual).mean()
print cust_data[cust_data.duplicated()] $
ax = sns.barplot(x='Single Name', y='Amount',data=dfmean.reset_index(),estimator=sum)
ip['p3_conf'].describe()
import datetime as dt $ df_concat["day"] = pd.to_datetime(df_concat["date"]) $ df_concat["day"] = df_concat["day"].map(dt.datetime.toordinal) $ df_concat["day"].head()
g = sns.lmplot(x="Wins", y="regular_occurrences", data=team_names,fit_reg = True, legend = True) $ g.set_axis_labels(x_var = "Number of Wins", y_var = "Mentions") $ g.axes[0,0].set_xlim(0,82) $ plt.show()
df.describe()
path = r'X:\CIS-PD Videos\timestamp' $ fname = 'video_utc_timestamp.csv' $ filename = os.path.join(path, fname) $ with open(filename,'wb') as f: $     result.to_csv(filename, sep=',')
wic.to_excel('wic.xlsx', sheet_name='Total Women') # This didn't work in code as openpyxl was supposedly missing!
index_values=df.query('group=="treatment" and landing_page!="new_page" or group=="control" and landing_page!="old_page"').index.values $ index_values
print('{} outliers were removed from the training dataset'.format(train_df.shape[0] - prepared_train.shape[0]))
train_trees = [conlltags2tree(sent) for sent in train_labels] $ valid_trees = [conlltags2tree(sent) for sent in valid_labels] $ test_trees = [conlltags2tree(sent) for sent in test_labels]
print(tabulate(mostcommon[:6],headers=('Topic','Times Tweeted')))
import pandas as pd $ df = pd.DataFrame(all_records, columns=['date', 'lie', 'explanation', 'url'])  $
def Cube(x): $     return x ** 3 $ df_n = df.iloc[:,1:] $ df_n.apply(Cube)
autos = autos[autos["registration_year"].between(1900,2016)]
(autos['ad_created'] $         .str[:10] $         .value_counts(normalize = True, dropna = False) $         .sort_index() $         )
train.info()
treatment_gp = df2.query('group == "treatment"') $ prob_treatment_converted = treatment_gp[treatment_gp["converted"] == 1].count()  / (treatment_gp[treatment_gp["converted"] == 0].count() + treatment_gp[treatment_gp["converted"] == 1].count()) $ prob_treatment_converted = prob_treatment_converted[0] $ prob_treatment_converted
rdd = sc.parallelize([random() for _ in range(10)]) $ rdd.collect()
con = sqlite3.connect('db.sqlite') $ print(pd.read_sql_query("SELECT * FROM temp_table LIMIT 2", con)) $ con.close()
del (reduced_df, chunks_list) $ gc.collect()
fig = plt.figure(figsize=(10,10)) $ ax = fig.add_subplot(111) $ ax.axis('off') $ pumashp.plot(edgecolor='black',color='white',linewidth=2.0,ax=ax)
daily_df_subset = daily_df[['Time','Company','Day Prediction','Price_Change']] $ daily_df_subset.head()
fin_p.index
toy['encrypted_customer_id'].unique()[0:10]
data["engagement"] = np.where(data["comms_num"]>500, 1, 0)
usage_400hz_filter.head()
df.tail()
result = df2.groupby('group') $ result.describe()
import os
ac['Compliance Review Start'].describe()
source_paths = { $     'medical_conditions_questionnaire': 'https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/MCQ_D.XPT', $     'weight_history_questionnaire': 'https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/WHQ_D.XPT', $     'demographics_with_sample_weights': 'https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/DEMO_D.XPT' $ }
s.str.replace('@','$')
load2017['date'] = load2017['time'].str.slice(0,10) $ load2017['date'] = pd.to_datetime(load2017['date']) $ load2017['date'].head()
s[s > 10].head()
afl_data.tail(3)
import pandas as pd $ import numpy as np $ import requests as rq $ import matplotlib.pyplot as plt $
myzip.filename
retweets = cleanedData['text'].str.startswith("RT @") $ print("Num retweets: ", sum(retweets)) $ cleanedDataNoRetweets = cleanedData[~retweets]
display(data.head()) $ display(pd.DataFrame(testdata).head())
dummies = pd.get_dummies(plate_appearances['bb_type']).rename(columns=lambda x: 'bb_type_' + str(x)) $ plate_appearances = pd.concat([plate_appearances, dummies], axis=1) $ plate_appearances.drop(['bb_type'], inplace=True, axis=1)
bucket.upload_dir('data/wx/tmy3/proc/', 'wx/tmy3/proc', clear_dest_dir=True)
def cnull(x): return x.isnull().sum()
def constructDF(user_account): $     df = pd.DataFrame(sentiment_results[user_account], $                       columns=["Compound", "Positive", "Neutral", "Negative", "Date", "Tweets Ago"]) $     return df
hour_distributedTopmodel = calc_total_et(results_distributedTopmodel)
talks_train.columns
plt.figure(figsize=(10,10)) $ sns.distplot(df_nd101_d[df_nd101_d['ud120']>0].ud120)
highest_obs_temps = session.query(Measurement.station, Measurement.date, Measurement.tobs).\ $     filter(Measurement.date > '2016-08-023').\ $     filter(Measurement.station == highestobs_station).\ $     order_by(Measurement.date).all() $ highest_obs_temps
time_vals = ['scheduled_departure', 'departure_time', 'wheels_off', 'wheels_on', 'scheduled_arrival', 'arrival_time']
events_per_day = events_df[['event_day','event_id']].groupby('event_day').count() $ events_per_day.rename(columns={'event_id':'count_event_day'},inplace=True) $ events_per_day.reset_index(inplace=True)
print("Probability an individual recieved new page:", $       ab_file2['landing_page'].value_counts()[0]/len(ab_file2))
train_df = pd.read_csv('train_2016_v2.csv',parse_dates = ['transactiondate'])
Grouping_Year_DRG_discharges_payments, Year_levels, DRG_levels =\ $ Generate_Grouping_Year_DRG_discharges_payments(False) $
s1 = np.random.normal(0, 1, 2000) $ s2 = np.random.normal(9, 2, 2000) $ v = pd.Series(np.concatenate([s1, s2])) $ v.hist(bins=100, alpha=0.4, color='B', normed=True) $ v.plot(kind='kde', style='k--')
s = type.__new__(type,'MetaClass',(),{"a":1})
data = pd.Series(["quick", None, "fox"], name="Fox") $ data
specs=[x for x in list(set(list(building_pa_specs['Column name'].values))) if x.lower().find('street')>=0] $ raw_data=[x for x in list(set(building_pa.columns)) if x.lower().find('street')>=0];    $ print(specs) $ print(raw_data)
dfn.head(10)
df.index.values   # underlying values are numpy.ndarray
rng_dateutil.tz
w = ekos.get_workspace(user_id, unique_id = workspace_uuid, alias = 'lena_newdata')
assert 1 == 2
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0)
convert_old, convert_new, n_old, n_new
data = pd.concat([data1, data2, data3, data4, data5, data6, data7, data8, data9, data10, \ $                  data11, data12, data13, data14, data15, data16, data17, data18, data19, data20, \ $                  data21, data22, data23])
active_users = users[users[['Asked', 'Answered', 'Comments']].notnull().any(axis = 1)] $ active_users.head()
df['dealowner'] = df['dealowner'].str.split(expand = True)[0]
train.head()
poverty.drop([386, 387, 388, 389, 390], inplace=True)
new_discover_sale_transaction = post_discover_sales[post_discover_sales['Email'].isin(new_customers_test['Post Launch Emails'].unique())] $ new_discover_sale_transaction['Total'].mean()
plt.title('GC_MARK_MS') $ (aggregated_content + aggregated_parent).plot(kind='bar', figsize=(15, 7))
not_creditworthy_user.shape
main.wl_ta.value_counts(normalize=True)
knn.fit(train[['property_type', 'lat', 'lon','surface_covered_in_m2']], train['covered/total'])
evaluator.get_metrics('entity_level_results')
items2 = [{'bikes': 20, 'pants': 30, 'watches': 35}, $           {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5}] $ store_items = pd.DataFrame(items2) $ store_items
tweet_df.head()
kickstarter = pd.read_csv("/data/royzawadzki/kemical/kickstarter-projects/ks-projects-201801.csv", encoding = "latin1", usecols = np.arange(0,13), parse_dates = [5,7])                      
df = pd.read_csv('tweets_mentioning_candidates.csv')
print('Largest change bewteen any two days (consecutive) is:',np.diff(closep).max())
C = pd.merge(A,B, on = 'team', how = 'left').drop_duplicates(['pts','ast']) $ C[C.team.duplicated()]
delimited_hourly['text']=delimited_twitter_df.groupby([pd.Grouper(freq="H"), 'company'])['text'].apply(lambda x: ' '.join(x)) $ delimited_hourly['Number_of_Users'] = delimited_twitter_df.groupby([pd.Grouper(freq="H"), 'company'])['user_name'].nunique() $ delimited_hourly = delimited_hourly.reindex(delimited_hourly.index.rename(['Time', 'Company'])) $ delimited_hourly.head()
total.print_xs()
modern_combos.cache()
results = pp.get_results()
sp = openmc.StatePoint('statepoint.50.h5')
col_names = data.columns.values[6:] $ col_names
f1 =(df['year'] >= 2000) $ f2 = (df['teamid'].isin(['BOS','NYA'])) $ interesting_teams = teams_sorted_by_wins[f1 & f2] $ interesting_teams.head(20)
df2 = df.drop(df[(df.landing_page == "new_page") & (df.group != "treatment")].index) $ df2 = df2.drop(df[(df.landing_page == "old_page") & (df.group != "control")].index) $
diff=new_page_converted.mean()-old_page_converted.mean() $ diff
ratings.sample(5)
df2[(df2['landing_page']=='new_page') & (df2['converted']==1)].count()[0]
aapl['open'].plot(color='b', style='.-', legend=True) $ aapl['close'].plot(color='r', style='.', legend=True) $ plt.axis(('2001','2002',0, 100)) $ plt.show()
responsesJsonList = entities.apply(getJson)
percent_of_total_list[:first_value_greater_50]
pd.Timestamp('2014-01-01 23:00:00')
percentage_loc = sum(df['Picture_Location'].isnull())/len(df) $ print("Percentage of non-empty location: " + "{0:.2f}%".format((1-percentage_loc) * 100))
history = import_all(data_repo + 'intervention_history.csv', history=True)
def get_action_counts(frame,action_list): $     action_subset = frame[frame['action'].isin(action_list)] $     action_counts = pd.crosstab(action_subset['user_id'],action_subset['action']) $     return action_counts
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ mydata = pd.read_csv(path, sep ='\s+', header=None) $ mydata.head(5)
hawaii_station_df.head(20)
df2 = df2.drop_duplicates(subset=['user_id'], keep='first') $ df2[(df2.user_id == 773192)]
lesson_date
airlines = [h for h in heap if h.company in ['Delta', 'AmericanAir', 'British_Airways']]
group_by_catgory = review['rating'].groupby(review['Category']) $ for name, group in group_by_catgory: $     print name $     print group.mean() $     print group.median()
len(df.query('group=="treatment" and landing_page !="new_page"')) + len(df.query('group!="treatment" and landing_page =="new_page"'))
plt.scatter(sing_fam.baths.values, sing_fam.rp1lndval.values);
full_clean_df.to_csv('twitter_archive_master.csv', index=False)
sns.factorplot(data=tweets_df, x="created_at", y="retweet_count", kind="box")
text = 'My email address is mailto:neal.caren@gmail.com so be sure to send me notes.'
df_transactions.hist(figsize=(15,12),color = 'y') $ plt.show()
active_station_data = session.query(Measurement.station, func.count(Measurement.id)).\ $     filter(Measurement.station == Station.station).\ $     group_by(Measurement.station).order_by(func.count(Measurement.id).desc()).all() $ active_station_data
import statsmodels.api as sm $ df2.head()
df_cities = pd.read_csv('cities.csv') $ df_cities
xgb_predictor = xgb.deploy(initial_instance_count=1, $                            instance_type='ml.m4.xlarge')
def date_only(dt): $     day = dt.split('T')[0] $     return day
run txt2pdf.py -o"2018-06-14 2148 OROVILLE HOSPITAL Sorted by Discharges.pdf"  "2018-06-14 2148 OROVILLE HOSPITAL Sorted by Discharges.txt"
bands = questions['bands'].str.get_dummies(sep="'")
finals[finals.pts_l==1].sort_values("PTS", ascending=False).head()
lgb_cv.fit(X_reduced.values,y_train.values.flatten())
reddit_master['Class_comments'].value_counts()/reddit_master.shape[0]
count_non_null(geocoded_df, 'Case.File.Date')
loans.loan_nr[loans.id_loan==815]
! wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-US,en;q=0.9" "https://storage.googleapis.com/kaggle-competitions-data/kaggle/10038/99000/train.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1538843579&Signature=CMMknLvJRFdZOeLNVz20xx84XMeuMgwdpJ9sZmDpZPTIy2%2BUsg0KfFz8fZx15CyMQt1FJNNt35Asqz8w96H%2BgDb2apGOiXkTCIMQAebwSmcT82A%2Bn8mNIBA7UVb4vsLnokTIy39%2FtzLAAaIgDfOBk0224DGt0Qzve1Yridk33aOCBYZD1C7NGmG%2FMOzdLjJp3%2FkqS08PVcDlSg6ZIKrziIdg%2BMAhILrQtenrAOGXuKWON2mE7S5%2BY379ubviVYXltB3vDgwMPIA%2BQmGQLvokne4k7uPI9OQf4MZBStN74Ndmldcifn6Pi72wDXIYbEqnbsFq0geTRZBDzZQm9VsE8A%3D%3D" -O "data/train.csv.zip" -c $ ! wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-US,en;q=0.9" "https://storage.googleapis.com/kaggle-competitions-data/kaggle/10038/99000/test.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1538843531&Signature=ScKXJBDHKlCtQw1iWjzD7AF%2FRPlr3zC3szdm%2B6MTX5WUyYfVZDelYXGfpGFHSsO1io2rmo5wzseYxZHQWg5zlaAY8TWtRMnTRUTjEJQHwrrF8amHsKwE%2FabUh%2B%2BIxfmrQygH36MMPy4UUBs1UGKRaby9R9LsqxkSf5WhBZrIrBpYfPoOFishYdIqfMd6QStEPyjZg5ciEyd1nQYuUP7jY%2BMfSsLjw7XS8jNUq8qXh1cb86D07jjyEwCtumIxTMPClh6Wwp1ljFXe1pOlxKHRQwWecPe6yfJXRqrOcptnUSI71NPMPBcXw9%2Fbx8dFK0%2BZsGXgNb7GZh1wH5eNaVrMWQ%3D%3D" -O "data/test.csv.zip" -c $ ! wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-US,en;q=0.9" "https://storage.googleapis.com/kaggle-competitions-data/kaggle/10038/99000/sample_submission.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1538843600&Signature=WQ1mnLPbKV0sob7l2PIFWW3gXXNkWUcsVks0njs82Pvq4BeV3t3K5sOXiYyIsP68TieXXfDIQRAn4bFfCIAJZr11tt5T6Va7tsD0lvjXBKzpTzMaDK%2BBQagUul2nul25r1gPg4ajwbIzts%2BRoAK7y%2FWiRXNwWNwBAGJo1RbHTVTFMvFLqJpcj28o5zFPM6h%2BgT%2FPAtN6zPpuCFw6epDne8v1qhEtJYBiYUoDJVyJg1Fdsq9QCun%2F2MmH0T9dOu2OWWcDM73RewFs1QVbIGPlAFF8V0yiH4GEX2srF%2Bk96Gb6jeLAav4TnZ2MPOLwXDfTUQO0uucKbhFiKqUc8C3DeA%3D%3D" -O "data/sample_submission.csv.zip" -c $
traffic_df_rsmpld = traffic_df_byday.reset_index().groupby('LINK_ID').apply(lambda x: x.set_index('DATETIME').resample('1M').median()).swaplevel(1,0) $ traffic_df_rsmpld.info() $ traffic_df_rsmpld.head()
df2_treatment = df2.query("group == 'treatment'")
calories_df.head()
post = {"author": "Renato", $         "text": "My first blog post!", $         "tags": ["mongodb", "python", "pymongo"], $         "date": datetime.datetime.utcnow()}
x = pandas.read_csv("data1.csv")
df['topicmax'] = topicmax
y_tscv_s2=df_test[['id','visitors']] $ y_tscv_s2['visitors']=lgbmrstcv.predict(df_test[col].values) $ preds_s2=np.expm1(y_tscv_s2['visitors'].values) $ df_test['visitors']=preds_s2 $ df_test.sort_index(inplace=True)
test_rank = resturaunt_orders_count(new_test_data,38)
no_specialty = no_specialty[['Provider', 'Specialty', 'AppointmentDate', 'AppointmentCreated',\ $         'AppointmentDuration', 'ReasonForVisitId', 'ReasonForVisitName', $        'ReasonForVisitDescription','MeetingStatusId', 'MeetingStatusName', 'MeetingStatusDescription', \ $     'OfficeId',  'OfficeName']]
df_total = df_total[ls_columns_reordered]
print(ozzy.name + ' is ' + str(ozzy.age) + 'year(s) old.')
dominique = relevant_data[relevant_data['User Name'] == 'Dominique Luna'] $ df = dominique['Event Type Name'].value_counts() $ df_dominique = pd.Series.to_frame(df) $ df_dominique.columns = ['Dominique L'] $ df_dominique
df_questionable[df_questionable['unreliable'] == 1]['link.domain_resolved'].value_counts(25).head(25)
soup.img['src']
plt.hist([comment['first.comment.week.diff'] for comment in eligible_comments]) $ plt.title("How many weeks previously was this comment posted") $ plt.show()
service_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], alternative='smaller', $                                               nobs=[n_new, n_old]) $ print("z-score:", z_score,"\np-value:", p_value)
rate_change['rating'].sort_values(ascending=False)[0:2]
measurement_df.describe()
df2[['ab_page', 'ab_page_old']] = pd.get_dummies(df2['landing_page']) $ df2 = df2.drop('ab_page_old',axis = 1) $ df2.head()
len(nullCity)
weather_yvr.plot(subplots=True, figsize=(10, 10))
index = pd.date_range(start='2016-09-01-00:00',periods=30*24,freq='h') $ index
sites_on_net.head()
twitter_archive_full = twitter_archive_full[twitter_archive_full.in_reply_to_status_id_x.isna()].copy() $ twitter_archive_full = twitter_archive_full[twitter_archive_full.retweeted_status.isna()].copy() $
 myCViterator
pst.npar,pst.nobs
import pandas as pd
print(returns_vol_tbl(start='2017', assets=assets_))
df.drop("water_year2",axis='columns',inplace=True)
LOC[LOC.Merkmalcode=='KG']
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=5000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
all_df = pd.read_csv('data/political_ads.csv', parse_dates=['start_time', 'end_time', 'date_created'])
strategy.df_pnl().head()
hour = pd.tseries.offsets.Hour()
timecat_df['tweetFavoriteCt'].max() $ timecat_df.index[timecat_df['tweetFavoriteCt'] == timecat_df['tweetFavoriteCt'].max()].tolist() $ timecat_df['userLocation'][3489] #this one is form somewhere in the US $ timecat_df['Hashtag'][3489]      #hashtag: cat
data.info()
%matplotlib inline
image_predictions_df[image_predictions_df.p1_dog == False][0:5]
Z = np.diag(1+np.arange(4),k=-1) $ print(Z)
random_crashes_df['Crash Recorded'] = 1 $ random_non_crashes_df['Crash Recorded'] = 0
new_page_converted = np.random.choice([0,1],size = n_new, p = [1-p_new,p_new]) $ new_page_converted
yhat = clf.predict(X_test) $ yhat [0:5]
trips_data['percentile'] = trips_data['duration'].rank(pct=True) # using rank method of pandas to assign percentile to each duration values $ pct_95_trips_data= trips_data[trips_data['percentile']<=0.95] # to remove outliers, say, we accept to consider only 95% of the data observed
submission['proba'] = preds[:,1]
display(Markdown(q8a_answer))
run_augmented_Dickey_Fuller_test(series=dr_existing_hours, num_diffs=2)
control_convert = df2.query('group == "control"').converted.mean() $ control_convert
new_page_converted = np.random.binomial(n_new,convert_rate_p_new)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old]) $ norm_ppf = norm.ppf(1-(0.05/2)) $ print('p-value (two-sided): {}'.format(p_value)) $ print('z-score: {} \nPercent point function: {}'\ $      .format(z_score, norm_ppf)) 
Measurement_df = pd.read_sql("SELECT * FROM measurement", conn) $ Measurement_df.head(10)
vectorizer, bow = make_review_bow(amazon_review) $ bow_df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())#TODO $ bow_df
categorical = ideas[['Authors','Link','Tickers','Strategy','Title']]
bigdf['comment_body'] = bigdf['comment_body'].replace('/r/r',' ')
df_cluster = pd.DataFrame({"cluster" : cluster_range, "cluster_error" : cluster_errors}) $ df_cluster
s.index
twitter_df.info()
jobs_data3 = json_normalize(json_data3['page']) $ jobs_data3.head(5)
serious_start_count = 0 $ for row in data: $     if re.search("^[\[\(][Ss]erious[\]\)]", row[0]): $         serious_start_count = serious_start_count + 1 $ print("[1] serious_start_count: " + str(serious_start_count))
auto.info()
percent_quarter.plot()
%pylab inline $ pandas_ds["time2close"].plot(kind="hist")
df[df.state.isin(invalid_states)].state.value_counts().sort_index()
stand_err = std_w/np.sqrt(len(hm_data)) $ z_score = (mean_w - 85.)/stand_err $ print stand_err, z_score
autos["price"].head(3)
joined_train_df.to_pickle(slowdata+'joined_train_df.pkl') $ joined_test_df.to_pickle(slowdata+'joined_test_df.pkl')
for ele in r.json().keys(): $     print(ele)
raw_df.shape
date + np.arange(12)
df.is_shift.value_counts()
df_con1 = df2.query("converted=='1'") $ x_ = df_con1["user_id"].count() $ x_ $
df.index[7]
df.head()
df1.head()
plt.scatter(X2[:, 0], X2[:, 1], c=dayofweek, cmap='rainbow') $ plt.colorbar();
pos_tweets = [ tweet for index, tweet in enumerate(data['tweets']) if data['SA'][index] > 0] $ neu_tweets = [ tweet for index, tweet in enumerate(data['tweets']) if data['SA'][index] == 0] $ neg_tweets = [ tweet for index, tweet in enumerate(data['tweets']) if data['SA'][index] < 0]
nufission = xs_library[fuel_cell.id]['nu-fission'] $ nufission.print_xs(xs_type='micro', nuclides=['U235', 'U238'])
hasGun = [' gun ' in tweet for tweet in allData.text] $ pd.options.display.max_colwidth = 280 $ pprint(allData[hasGun].text) $ pd.options.display.max_colwidth = 50
winpct['text'][7]
train_holiday_oil = train_holiday.join(oil_spark, 'date', 'left_outer') $ train_holiday_oil.show()
print("The following is the row information for the repeat user_id:  ") $ df3.head()
import os $ with open(os.path.join('genesis.txt'), 'r') as f: $     text = f.read() $     print(text)
intersections_irr['isWeekend'] = [ 0 if  datetime.datetime.strptime( dateStr,'%Y-%m-%d %H:%M:%S').weekday()<5 else 1 for dateStr in intersections_irr['updateTimeStamp'] ]
diabetes_risk = high_hba1c.select('person_ref') \ $                           .union(diabetes_conditions.select('person_ref')) \ $                           .distinct() $ diabetes_risk.limit(10).toPandas()
from sklearn.ensemble import AdaBoostClassifier
Base = automap_base() $ Base.prepare(engine, reflect=True)
format8 = lambda x: '%.2f' %x $ print df.applymap(format8) $ print df.apply(lambda x: x.max() - x.min())
cust_count = data.cust_id.value_counts().reset_index() $ cust_count.columns = ['cust_id','cust_count']
import pandas as pd $ tweets_df = pd.read_json('Twitter_SCRAPING/profile_tweets.json', lines=True) $ tweets_df.info()
tsla_30_pd.tsla.hist(bins=range(T0, 180, 1), normed=1)
tweetVolume(boston) $ tweetVolume(obama) $ tweetVolume(gop)
df2 = df2.drop(duplicate_row_indexes[1])
df_pageviews_desktop.head()
def format_time(x): $     return datetime.strptime(x, '%m/%d/%Y').strftime('%Y-%m-%d')
tweets_per_hour_df.plot.line(x='hour',y='number_of_tweets',figsize=(15,8),lw=1)
df.tail()
time.mktime(jdfs.pushed_at[0].timetuple())
df.head()
print (df2[df['user_id'] == 773192])
autos["last_seen"].str[:10].value_counts(normalize=True, dropna=False).sort_index()
trans = pd.read_csv("raw/translationsbackup.csv", encoding="utf-8", index_col=0) $ trans.head() $
own_star['uniqueID'] = own_star.repo_id.astype(str) + own_star.user_id.astype(str)
twitter_archive_master[twitter_archive_master.rating_numerator < 10].head()
therm_abs_rate = sp.get_tally(name='therm. abs. rate') $ thermal_leak = sp.get_tally(name='thermal leakage') $ thermal_leak = thermal_leak.summation(filter_type=openmc.MeshSurfaceFilter, remove_filter=True) $ res_esc = (therm_abs_rate + thermal_leak) / (abs_rate + thermal_leak) $ res_esc.get_pandas_dataframe()
print len( niners_td[niners_td['Jimmy'] == 'no'] ) $ print len( niners_td[niners_td['Jimmy'] == 'yes'] )
sns.distplot(temp_df[temp_df.total_companies > 100].proportion_no_psc)
sample_size_new_page = df2.query('landing_page == "new_page"').shape[0] $ print('Sample size new_page: {}'.format(sample_size_new_page))
df_users_6.shape
corr_df = factor_ts.rolling(250).corr().dropna() $ corr_df.head(7)
print("The probability of individual in the control group converting is: {}".format(df2[df2['group'] == 'control']['converted'].mean()))
closed_issues = Issues(github_index).is_closed().get_cardinality("id_in_repo").by_period(field="closed_at") $ print("Trend for month: ", get_trend(get_timeseries(closed_issues))) $ closed_issues = Issues(github_index).is_closed().get_cardinality("id_in_repo").by_period(period="quarter") $ print("Trend for quarter: ", get_trend(get_timeseries(closed_issues)))
print("Probability of control group converting:", $       df2[df2['group']=='control']['converted'].mean())
df =  pd.DataFrame(list(sorted_posts))
train['feature_list'] = train['features'].map(lambda x: ','.join(x)).str.lower()
(df.doggo.value_counts(), df.floofer.value_counts(), df.pupper.value_counts(), df.puppo.value_counts())
df = pd.DataFrame ( $     {'A': ['A1', 'A2', 'A3','A1','A3','A1'], $      'B': ['B1','B2','B3','B1','B1','B3'], $      'C': ['C1','C2','C3','C1',np.nan,np.nan]}) $ df
k
print(train.columns.values)
from patsy import dmatrices $ from statsmodels.stats.outliers_influence import variance_inflation_factor
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y, test_size=.20, random_state=42) $ X1_train.shape , y1_train.shape,  X1_test.shape, y1_test.shape
df.dtypes # Tells us what data type each column is!
run txt2pdf.py -o"2018-06-14-1513 FLORIDA HOSPITAL - 2013 Percentiles.pdf"  "2018-06-14-1513 FLORIDA HOSPITAL - 2013 Percentiles.txt"
df_joined['intercept'] = 1 $ df_joined[['US','UK']] = pd.get_dummies(df_joined['country'])[['US','UK']]
recipes.iloc[0]
class A: $     pass $ a = A() $ print(f'a.__class__ = {a.__class__}') $ print(f'A.__class__ = {A.__class__}')
plt.hist(p_diffs);
dataset['content_tokens'] = content_tokens
trump.index
xmlData['price'] = pd.to_numeric(xmlData['price'], errors = 'raise')
targettraffic = dfs_morning.merge(census_zip, left_on=['STATION'], right_on=['station'], how='left') $ targettraffic['targettraffic'] = targettraffic['ENTRIES_MORNING'] * targettraffic['betw150kand200k']/100
p_diffs =np.array(p_diffs) $ null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size)
archive_clean.loc[archive_clean['tweet_id'].isin(remove_list),:]
pred.info()
pd.to_datetime('today')
df_new = df2.query('landing_page == "new_page"') $ n_new = df_new.shape[0] $ n_new
import statsmodels.api as sm $ convert_old = 0.119659 $ convert_new = 0.119659 $ n_old = 145274 $ n_new = 145310
f_ip_app_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_app_clicks"), header=True) $ print('Found %d observations.' %f_ip_app_clicks.count())
(df_merged['p1'].iloc[1150], df_merged['p1_conf'].iloc[1150],df_merged['p1'].iloc[602],df_merged['p1_conf'].iloc[602],df_merged['p1'].iloc[201],df_merged['p1_conf'].iloc[201])
%%read_sql df_test4_promotions -c engine $ SELECT * from PromotionRuleInstance $ where promotionID in ({promotionID_test4}) $ and activityTimestamp < '{endDate}' $
row_slice = (slice(None), slice(None), 'Bob')  # all years, all visits, of Bob $ health_data_row.loc[row_slice, 'HR'] $
def row_split(line, cols, index): $     label, *vector = line.split() $     index.append(label) $     for col_num, new_value in enumerate(vector): $         cols[col_num].append(new_value) $
shelter_pd
df.query('group == "treatment" and landing_page != "new_page"').count()[0]
credentials_file = 'oauth.json' $ yfs = YahooFantasySports(credentials_file)
df2[['control','ab_page']]=pd.get_dummies(df["group"]) $ df2=df2.drop('control',axis=1) $ df2["intercept"]=1 $ df2.head()
dtrain = xgb.DMatrix(X_train, np.log(y_train + 1)) $ dvalid = xgb.DMatrix(X_validation, np.log(y_validation + 1)) $ watchlist = [(dvalid, 'eval'), (dtrain, 'train')] $ model = xgb.train(params, dtrain, 300, evals = watchlist, early_stopping_rounds = 30, feval = rmspe_xg, verbose_eval = 10)
df['treatments'] = df['tokens'].apply(find_treatments)
msftAC.tail(5), shifted_forward.tail(5)
df_ind_site.head()
data = df[base_col].values $ training_split_cut = int(split_pct*len(df))
df.fillna(method='pad')
%%time $ binary_sensors_df['robin'] = binary_sensors_df['last_changed'].apply( $     lambda x: get_device_state(parsedDF, 'device_tracker.robins_iphone', x))
df.converted.describe()
ctr = pd.read_csv("ctr_graph.csv")
scores[scores.IMDB == max_IMDB]
df['Complaint Type'] [df['Agency'] =='NYPD'].value_counts().head()
conn = sqlite3.connect("geo.db") $
tm_week_df = df_table.groupby(df_table['Datetime'].dt.weekday_name).count() $ print(tm_week_df['Incident_number'])
result['cycle'] = result.videoname.str.replace('6_part1', '6') $ result['cycle'] = result.cycle.str.replace('6_part2', '7') $ result['cycle'] = result.cycle.apply(reversetextsplitter,sep='cycle') $ result['cycle'] = result.cycle.apply(textsplitter,sep='_') $ result['cycle'] = result.cycle.apply(textsplitter,sep='.')
R=Meter1.ErrorReadAct(); print(R)
df2 = df2.drop_duplicates(subset = 'user_id')
dotenv_path = find_dotenv() $ load_dotenv(dotenv_path)
loan_stats["issue_d"].types
pulledTweets_df.columns
live_weights.value_counts()
df2.head()
len(df.groupby(['week','year','date'],as_index=False).max())
print(autos["price"].unique().shape) $ print(autos["price"].describe()) $ autos["price"].value_counts().head(20)
autos.describe(include='all')
n_old = df2.query('landing_page == "old_page"').group.count() $ n_old
tweet_data = pd.read_csv('tweet_json.txt', encoding = 'utf-8')
fe.bs.bootshow(256, poparr2, repeat=3) $
new_df = con_df.set_index('user_id').join(df2.set_index('user_id'),how = 'inner') $ new_df.head()
df_img_algo_clean['prediction_1'] = df_img_algo_clean['prediction_1'].str.lower() $ df_img_algo_clean['prediction_2'] = df_img_algo_clean['prediction_2'].str.lower() $ df_img_algo_clean['prediction_3'] = df_img_algo_clean['prediction_3'].str.lower()
rolled_returns = (prices.pct_change() * weights).sum(1)
type('abc')
df_Sessions = populate_tweet_df(tweets) $ df_Sessions = df_Sessions.drop(['source','user_mentions','user'],axis=1)
import sklearn.linear_model as lm
start_idx
diffs = new_page_converted.mean() - old_page_converted.mean() $ print("Difference between mean of each scenarios probability:",diffs)
Today = pd.to_datetime('now') $ YrFromToday = Today - dt.timedelta(days=365) $ print(YrFromToday)
df_clean.drop(['doggo','floofer', 'pupper','puppo'], axis=1, inplace= True)
ghana['WindDirDegrees'] = ghana['WindDirDegrees'].str.rstrip('<br />')
test_url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-08-21&end_date=2018-08-21&api_key=' + API_KEY $ test_data = requests.get(url)
gps_df.to_csv('gps_coords_df.csv',index=False)
materials_list = ("EVA", "PMMC") $ cur.execute('SELECT alpha, beta FROM materials WHERE material_id IN (?, ?)', materials_list) $ [(mat, cur.fetchone()) for mat in materials_list]  # use the cursor fetchone() method to get next item
lda_model.show_topics()
train_geo.columns
df.shape
url=("/Users/maggiewest/Projects/detroit_census.csv") $ detroit_census = pd.read_csv(url)
print(pd.DataFrame(test_matrix).head())
precipitation = pd.read_csv('../clean_data/hourly_precipitation.csv')
git_blame.info(memory_usage='deep')
a.alias('sepal_width')
import seaborn as sn $ from scipy import stats
bg_df2 = pd.DataFrame(bg3) # create new variable explicitly for it being a DataFrame in pandas $ bg_df2 $
tweet_archive_df.info()
train.head(3)
shows.dtypes
sales_df.groupby('Country').sum()['Quantity'].sort_values(ascending=False)
data_archie.isnull().sum()
ival = 17239871 $ ival ** 6
doglist['weight'].fillna(50, inplace = True) $ doglist
cityID = '60e2c37980197297' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         St_Paul.append(tweet) 
DOT_df = df[df['Agency'] == 'DOT'] $ DOT_df['Complaint Type'].value_counts().head() $
df.head(2)
clf.fit(X_train, y_train)
pd.to_datetime(eth['UnixTimeStamp']).head()
df['cowbell'] = df['body'].apply(lambda x: len([x for x in x.split() if '(((' in x]))
reddit_comments_data.select(min("score")).show(truncate=False)
tmi['weekFriendliness'].max()
countries_df = pd.read_csv('countries.csv') $ countries_df.head()
twitter_archive_df_clean.info()
unique_urls.sort_values('total_payout', ascending=False)[0:50][['url', 'total_payout']]
csvData['yr_built'] = pd.to_datetime(csvData['yr_built'], format = '%Y', errors = 'raise') $ csvData['yr_renovated'] = pd.to_datetime(csvData['yr_renovated'], format = '%Y.0', errors = 'coerce') $ csvData['yr_built'] = [d.strftime('%Y') if not pd.isnull(d) else '' for d in csvData['yr_built']] $ csvData['yr_renovated'] = [d.strftime('%Y') if not pd.isnull(d) else '' for d in csvData['yr_renovated']]
df.to_csv('twitter_archive_master.csv', index=False)
wget.download('https://cernbox.cern.ch/index.php/s/ibtnI2ESaFjIgSi/download')
rfmTable.head()
avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_reviewText, model=model, num_features=5000) $ print (np.round(avg_word_vec_features, 3))
%matplotlib inline $ import matplotlib $ import matplotlib.pyplot as plt $ matplotlib.rcParams['figure.figsize'] = (15.0, 8.0) $ bt.plot_equity()
df_2016['bank_name'] = df_2016.bank_name.str.split(",").str[0] $
Events.head(3)
train_label[0]
df.drop(df.query("landing_page == 'new_page' and group == 'control'").index, inplace=True) $ df.drop(df.query("landing_page == 'old_page' and group == 'treatment'").index, inplace=True) $ df.info()
daily = hourly.asfreq('D') $ daily
try: $     result = db.tweets.drop() $     print ("analytics tweets dropped") $ except: $     pass
df2.query('group == "treatment" and converted == 1').count()['user_id']/df2.query('group == "treatment"').count()['user_id']
df_centered.select('user_id','stars','date','seq', 'numOfReviews', 'ratings_centered').orderBy('user_id', 'seq').limit(20).toPandas()
reddit.head(50)
cities_list = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose', 'Detroit', 'Jacksonville', 'Indianapolis', 'San Francisco', 'Columbus', 'Austin', 'Memphis', 'Fort Worth', 'Baltimore', 'Charlotte', 'El Paso', 'Boston', 'Seattle', 'Washington', 'Milwaukee', 'Denver', 'Louisville', 'Las Vegas', 'Nashville', 'Oklahoma City', 'Portland', 'Tucson', 'Albuquerque', 'Atlanta', 'Long Beach', 'Fresno', 'Sacramento', 'Mesa', 'Kansas City', 'Cleveland', 'Virginia Beach', 'Omaha', 'Miami', 'Oakland', 'Tulsa', 'Honolulu', 'Minneapolis', 'Colorado Springs', 'Arlington', 'Wichita', 'Raleigh', 'St. Louis', 'Santa Ana', 'Anaheim', 'Tampa', 'Cincinnati', 'Pittsburgh', 'Bakersfield', 'Aurora', 'Toledo', 'Riverside', 'Stockton', 'Corpus Christi', 'Newark', 'Anchorage', 'Buffalo', 'St. Paul', 'Lexington-Fayette', 'Plano', 'Fort Wayne', 'St. Petersburg', 'Glendale', 'Jersey City', 'Lincoln', 'Henderson', 'Chandler', 'Greensboro', 'Scottsdale', 'Baton Rouge', 'Birmingham', 'Norfolk', 'Madison', 'New Orleans', 'Chesapeake', 'Orlando', 'Garland', 'Hialeah', 'Laredo', 'Chula Vista', 'Lubbock', 'Reno', 'Akron', 'Durham', 'Rochester', 'Modesto', 'Montgomery', 'Fremont', 'Shreveport', 'Arlington', 'Glendale']
data_2017_subset = dd.read_csv('data/output-*.csv', usecols=data_columns, dtype=str) $ data_2017_subset = data_2017_subset.compute() $ data_2017_subset = data_2017_subset.reset_index(drop=True) $ data_2017_subset.to_csv('data/data_2017_subset.csv')
stfvect.get_feature_names()
store.list_collections()
r = pd.DataFrame(q, columns = ['cat','score']) $ r.head()
df_ad_state_metro_1['candidates'].value_counts()
Bug(legs=8)
data.groupby('category').position.mean()
people_person.channel.value_counts()
cust_demo.age.plot(kind='hist', bins=50, color='R');
df_A=pd.DataFrame({"Student_height":heights_A,"Student_weight":weights_A})
df.sort_index(axis=1,ascending=False)
import numpy as np
len(combined_item_df)
train = train.astype({'From':'category', 'To':'category', 'Title':'category',\ $                         'From-To':'category', 'Hour':'category', 'Age_bin':'category'}) $ test = test.astype({'From':'category', 'To':'category', 'Title':'category',\ $                        'From-To':'category', 'Hour':'category', 'Age_bin':'category'})
bg3 # let's see what current DataFrame looks like
df[df['group']=='treatment'].groupby('landing_page').count()
print ('The number of time slices at this location is '+ str(all_ndvi_sorted.shape[0])) 
import re $ no_urls = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-f][0-9a-f]))+','',no_html) $ print(no_urls)
model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, compute_loss=True, hs=0, sg=1, seed=42) $ training_loss = model_with_loss.get_latest_training_loss() $ print(training_loss)
data.dtypes
print("Trips shape:") $ dftrips.shape
recv_npage = (df2.landing_page == 'new_page').mean() $ print("The probability that an individual received the new page is {0: .4} ".format(recv_npage))
for el in joined_amanda[joined_amanda['Proba_Amanda']==min(joined_amanda['Proba_Amanda'])]['text']: $     print el
["{} {}".format(x, len(weather_word_search(processed_tweets_with_obs, x))) for x in top_words]
football=pd.read_csv("results.csv") $ football.head()
ex2.sort_values(ascending = False, inplace = True) $ ex2
first_row = session.query(Station).first() $ first_row.__dict__
%%time $ df = pd.read_csv("data/311_Service_Requests_from_2010_to_Present.csv", usecols=["Agency", "Complaint Type", "Descriptor", "Closed Date", "Created Date"]) $
import numpy as np $ dist = np.sum(train_data_features, axis=0) $ for tag, count in zip(vocab, dist): $     print(count, tag)
creations = pd.read_table( $     "2016-10_enwiki_article_creations.tsv", $     parse_dates = [2, 8]) $ creations.head()
1-0.190/2
news_title_docs_high_freq_words_df = pd.read_pickle(news_title_docs_high_freq_words_df_pkl) $ with pd.option_context('display.max_colwidth', 100): $     display(news_title_docs_high_freq_words_df)
details.dropna(subset=['Genres', 'Released'], inplace = True)
print('RMSE LGBMRegressor: ', RMSLE(np.log1p(train['visitors'].values), lgbmrscv.predict(train[col])))
df_grouped_turnstile = mta.groupby([ 'C/A', 'UNIT', 'STATION', mta.DATE_TIME.dt.date])['NEW_ENTRIES']\ $     .sum() $ df_grouped_turnstile.head(10)
sensor.key
df_ml_701 = df.copy() $ df_ml_701.index.rename('date', inplace=True) $ df_ml_701_01=df_ml_701.copy()
tweets_list = df['processed']
retweets = pd.read_sql_query(query, conn) $ retweets.head()
df.head()
from sklearn import svm
import matplotlib.pyplot as plt $ plt.scatter(rets.FSLR,rets.TAN)
urls
df_trips.head(3)
ext_1_train = df_train['EXT_SOURCE_1'] $ ext_1_test = df_test['EXT_SOURCE_1']
new_page_converted = np.random.binomial(1, p_new, n_new)
vow.describe()
mask = y_test.index $ t_flag = y_test == 1 $ p_flag = pred == 0
posts.count()
data = data.sort_values(by=['time'])
data_donald_replies.to_csv("ddr_following_and_hashtag.csv")
content_input_count_hist = cached.map(lambda x: x[1]['content_input']).histogram(range(20)) $ draw_histogram("Content Input Hangs Distribution", content_input_count_hist)
df_twitter_archive_master[df_twitter_archive_master['rating_numerator']==1776]
day_later_timestamps = len(timestamp_left_df[timestamp_left_df['time_delta_seconds'] < -86400]) $ print("There are %d objects in which the 'timestamp' are 1 day later than events were created." % day_later_timestamps)
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
((df['converted'].mean()))
liberiaCases = liberiaCasesSuspected + liberiaCasesProbable + liberiaCasesConfirmed $ liberiaCases.head()
start_date = '2015-01-19' # b/c first row of data was differenced away $ end_date = '2018-04-30' $ end_pred = '2018-09-30'
new_page_samples = np.random.binomial(n=n_new, p=p_new, size=10000) $ new_page_samples = new_page_samples / n_new $ old_page_samples = np.random.binomial(n=n_old, p=p_old, size=10000)/n_old $ p_diffs = new_page_samples - old_page_samples $
energy_cpi_with_id = abs_to_df(response_json,name_only=False) $ energy_cpi_with_id
df = pd.read_csv(input_path) $ df.dropna(subset=['Recipient Email Address'], inplace=True) $ df.ix[:5, :5]
for i in cpi_all['Index'].cat.categories.tolist(): $     print i    
df['user_id'].unique().shape[0]
prob =df2.converted.mean() $ print('The probality of an individual converting is {}'.format(prob))
df_final.last_trx.max()
fraud_df = fraud_df.merge(device_tmp, on='device_id',how='left').merge(ip_tmp,on='ip_address',how='left')
df2.head()
talks_train.head()
len(data_2017_subset[data_2017_subset.incident_zip.str.match('([^0-9])', na=False)])
data_x, data_y, newFeature = data_preprocessing(training_data, target_col='target', timestamp='last_pymnt_d')
to_be_predicted_Day4 = 83.28399652 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
cercanasA1_11_14Entre75Y100mts = cercanasA1_11_14.loc[(cercanasA1_11_14['surface_total_in_m2'] >= 75) & (cercanasA1_11_14['surface_total_in_m2'] < 100)] $ cercanasA1_11_14Entre75Y100mts.loc[:, 'Distancia a 1-11-14'] = cercanasA1_11_14Entre75Y100mts.apply(descripcionDistancia, axis = 1) $ cercanasA1_11_14Entre75Y100mts.loc[:, ['price', 'Distancia a 1-11-14']].groupby('Distancia a 1-11-14').agg(np.mean)
test_df = us_companies.copy()
nnew = df2.query('group == "treatment"').shape[0] $ print(nnew)
df_t_not_n = df[(df['group'] == 'treatment') & (df['landing_page'] == 'old_page')] $ df_not_t_n = df[(df['group'] == 'control') & (df['landing_page'] == 'new_page')] $ mismatch= len(df_t_not_n) + len(df_not_t_n) $ mismatch_df = pd.concat([df_t_not_n, df_not_t_n]) $ mismatch
predictions_clean.sample(10)
average_polarity.reset_index(inplace=True) $ count_polarity.reset_index(inplace=True)
print(full_df.columns)
sns.boxplot(autodf.powerPS)
people
modeling2.show(3)
mydata.describe()
add_plane_data = pk_planes.join(grouped, ['pk_id'], 'left_outer')
df.groupby('STNAME').agg({'CENSUS2010POP': np.average})
df[df['Agency'] == 'NYPD'].resample('M').count().plot(y='Created Date')
LinkNYCpLag = ps.lag_spatial(qW, pumashplc['linkNYCppcBB'])
new_page_convert_rate=df2.query('converted==1 and landing_page=="new_page"').user_id.nunique()/n_new $ old_page_convert_rate=df2.query('converted==1 and landing_page=="old_page"').user_id.nunique()/n_old $ obs_diff=new_page_convert_rate-old_page_convert_rate $ print(obs_diff)
id_difs = set(b_rev['listing_id']) - set(b_cal['listing_id']) $ id_difs2 = set(b_rev['listing_id']) - set(b_list['id']) $ print(id_difs) $ print(id_difs2)
gender_top = top_gender(general_info) $ gender_top
df_users = pd.read_csv('takehome_users.csv', encoding='latin-1')
np.exp(	-0.0140), 1/np.exp(	-0.0140)
plt.xlabel('p_diff value') $ plt.ylabel('Frequency') $ plt.title('Plot of Simulated p_diffs'); $ plt.hist(p_diffs) $
tag_df.values
frame2.ix['three']
all_noms[(all_noms["agency"] == "Foreign Service") & (all_noms["confirmed"] == "yes")]["nom_count"].sum()
s.ix[3:6].mean()
df_train['totals.newVisits'] = df_train['totals.newVisits'].fillna('0') $ df_test['totals.newVisits'] = df_test['totals.newVisits'].fillna('0') $
df.tail(50)
best_params = dict(ind_params) $ best_params.update(optimized_XGB.best_params_) $ print(best_params)
food["created_date"]=food["created_datetime"].apply(date)
df_new.groupby(['country', 'ab_page', 'converted']).describe().iloc[:,:1]
w = 'kike' $ model.wv.most_similar (positive = w)
cols = ['chanel', 'ROI_0', 'ROI_6', 'ROI_14', 'ROI_30'] $ ROI = ROI[cols] $ ROI_cols = [ 'ROI_0', 'ROI_6', 'ROI_14', 'ROI_30'] $ ROI = ROI[ROI_cols].fillna(0)
print len(test_encodedlist) $ print len(test_encodedlist[0]) $ print len(test_encodedlist[0][0])
pd.to_datetime([1, 3.14], unit='s')
df3 = gbq.read_gbq(querry, project_id = project_id, dialect = "standard") $ df3.head()
R_weather.info()
arima11= ARIMA(dta_713,[1,1,0],freq='Q').fit() $ arima11.summary()
query = pgh_311_data_merged['Category'] == "Road/Street Issues" $ pgh_311_data_merged[query]['Issue'].value_counts()
import nltk $ nltk.download('stopwords')
session.query(User).filter(User.name.in_(['ed', 'fakeuser'])).all()
print(len(distance_list)) $ station_distance.shape
print('How many bikes and pants are in each store:\n', store_items[['bikes', 'pants']])
df.head(2)
sp
print(avi_data.describe()) $ print('\nThere are %d quantitative variables of 31.\n\n' %(avi_data.describe().shape[1])) $ print(avi_data.describe(include=['O'])) $ print('\n There are %d qualitative variables of 31.' %(avi_data.describe(include=['O']).shape[1]))
df_ml_53 = df.copy() $ df_ml_53.index.rename('date', inplace=True) $ df_ml_53_01=df_ml_53.copy()
df[df.topicmax == 2].subreddit.value_counts()[:10]
count = np.array([convert_new, convert_old]) $ nobs = np.array([n_new, n_old]) $ z_score, p_value = sm.stats.proportions_ztest(count,nobs, alternative='larger') $ z_score, p_value
training_data,holdout = train_test_split(lq2015_combined,test_size=0.10,random_state=123)
len(names)
df.fillna(method="ffill",inplace=True) $ df.fillna(method="bfill",inplace=True)
    from pathlib import Path $     import deathbeds $     from pandas import DataFrame, Series, Index, to_datetime $     from IPython.display import Markdown, display $     loader = __import__('importnb').Execute(display=True);
df1=pd.DataFrame(dict(id=range(4),age=np.random.randint(18,31,size=4))) $ df2=pd.DataFrame(dict(id=list(range(3))+list(range(3)),score=np.random.random(size=6))) $ print(df1) $ df2
autos["brand"].unique() $ top20=autos["brand"].value_counts(normalize=True, dropna=False).head(20) $ top20
percentage_click_by_no_emails = train.groupby('no_of_emails')['is_click'].agg(np.mean) $ percentage_click_by_no_emails
train.groupby('campaign_id')['day_of_week'].unique()
ghana['GMT'] = to_datetime(ghana['GMT'])
import numpy as np $ import pandas as pd $ from datetime import date, datetime
month_max_loc = df.month.value_counts().sort_values(ascending=False).index[0] $ month_max = df.month.value_counts().sort_values(ascending=False).values[0] $ month_max_loc = df.created_at[df.month == month_max_loc].iloc[0] $ month_string = month_max_loc.strftime('%B %Y') $ print('I drank the most beers during the month of {} with {} total beers.'.format(month_string, month_max))
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_mean, (1-p_mean)])
print(y_train.value_counts()) $ print(y_val.value_counts())
df_methods.head()
pd.unique(survey["question"])
mike[match]
kochdf = pd.merge(kochdf, koch11df,  how='left', left_on=['name', 'user'], $                   right_on = ['name', 'user'], suffixes=('','_11')) $ kochdf.info()
time_hour_for_file_name
root = ET.fromstring(xml_page) $ cdtags = root.xpath('//CD/TITLE') $ for cd in cdtags: $     print(cd.text)
from ramutils.classifier.utils import reload_classifier $ classifier_container = reload_classifier('R1387E', 'catFR5', 1, mount_point='/Volumes/RHINO/') $ classifier_container.features.shape # n_events x n_features power matrix
[sample_pivot_table,test] = split_data(order,70,200)
print(soup.prettify())
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM $ pickle.dump(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724, open( "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p", "wb" ) )
addOne = cylHPData.mapValues(lambda x: (x, 1)) $ print (addOne.collect()) $
percentage_open_by_no_emails = train.groupby('no_of_emails')['is_open'].agg(np.mean) $ percentage_open_by_no_emails
df3['us_new'] = df3['country_US']*df3['ab_page'] $ df3['uk_new'] = df3['country_UK']*df3['ab_page'] $ df3.head(3)
len(mdate),len(mname),len(mpoint)
df_ml_59_01.tail(5)
db_names = [ $ ] $ for db_name in db_names: $     DataAPI.schema.show_db_info(db_name)
tx, ty = tsne[:,0], tsne[:,1] $ tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx)) $ ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty)) $ pd.DataFrame(list(zip(tx,ty))).plot.scatter( $     x=0, y=1, alpha = .4)
new_page_converted = np.random.choice([1,0], size = n_new, p = [p_new, 1-p_new]);
(df_final[df_final['R'] == 10]).head()
subset = date_series[3:7] $ subset
s_mean_df = s_mean_df.set_index('date') $ print(len(s_mean_df.index)) $ s_mean_df.info() $ s_mean_df.head(5)
Test.SetFlowStats(Ch1State=True, Ch2State=False, Ch3State=True)
datatest['expenses'] = datatest['expenses'].apply(lambda x : float(x))
df_copy['rating_numerator'].value_counts()
honeypot_df = pd.concat([honeypot_df,pd.DataFrame(honeypot_df['Time stamp'].str.split("mhn").tolist(), columns = ['time_stamp1','time_stamp2','time_stamp3'])],axis = 1) $ honeypot_df = pd.concat([honeypot_df,pd.DataFrame(honeypot_df['time_stamp3'].str.split("T").tolist(), columns = ['date','time'])], axis = 1)
contractor_clean['updated_date'].head() $
1 / df1
breed_predict_df.head(1)
unique_feature_list[:10]
from_api.info()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.TAB.txt" $ mydata = pd.read_table(path, sep= '\t') $ mydata.head(5)
import pandas as pd
pattern = r'^[a-z]' $ mask = df.name.str.contains(pattern) $ mask.value_counts()
df.loc['1975-01-01']
df.query('landing_page == "new_page"').shape[0] / df.shape[0]
df = pd.read_csv('/Users/yennanliu/Desktop/used-cars-database/autos.csv' ,encoding = "ISO-8859-1" )
np.sum(df["past_percent_cancelled"].isnull())
daily_window = Window.partitionBy('day').orderBy(functions.desc('count'))
ph_str_gpd = gpd.read_file('phillydata/Street_Centerline_qgis3.geojson') $ ph_str_bfr_gpd = ph_str_gpd[['STCL2_ID','SEG_ID','geometry']].copy() $ ph_str_bfr_gpd.geometry = ph_str_bfr_gpd.buffer(0.0001) $ ph_str_gpd.head().T
df_loantoVal = df[(df.Loan_Amount != 0) & (df.Property_Value != 0)] $ df_loantoVal['LoanToValue'] = (df_loantoVal.Loan_Amount/df_loantoVal.Property_Value)
invoice_hub_dropper = ['deleted_at','fk_s_change_context_id_cr', 'fk_s_change_context_id_dl','fk_x_billing_account_hub_id', 'fk_x_subscription_hub_id', 'sk_id'] $ for col in invoice_hub_dropper: $     print invoice_hub[col].value_counts()
prob_control = df2.query('group == "control"').converted.mean() $
df.isnull().sum()
categorised_df = pd.read_csv('/s3/three-word-weather/hack/categorised_words.csv', header=0, names=['word', 'weight', 'category'], na_values=' ')
q_multi = c.submit_query(PTOQuerySpec().time("2018-06-10", "2018-06-12").set_id(0xc) $                                        .condition("ecn.multipoint.*") $                                        .group_by_condition())
santos_tweets = pbptweets.loc[pbptweets['text'].apply(lambda x: any(re.findall('Santos',x)))][['date','screen_name','text']] $ santos_winpct = winpct.loc[winpct['text'].apply(lambda x: any(re.findall('Santos',x)))][['playId','homeWinPercentage','playtext','date']]
r6s.shape
plt.hist(p_diffs) $ plt.xlabel('p_diffs') $ plt.ylabel('Freq') $ plt.title('10 k  simulate'); $ plt.axvline(x=(pnew-pold), color='r'); $
df_usa['PD2'] = np.around(df_usa['Total population']/df_usa['Area'], decimals = 2) $ df_usa
station_activity = df.groupby('station').count().sort_values(['id'], ascending=False) $ station_activity
df_res['total_score'] = df_res['score_rep'] + df_res['score_ps'] + df_res['score_rank'] + df_res['score_activity'] + df_res['score_followers'] + df_res['score_contributions']
data.info()
USvideos.describe() $ USvideos.head()
r.html.search('Python is a {} language')[0]
df.head(2)
len([premiePair for premiePair in BDAY_PAIR_qthis.pair_age if premiePair < 0])/BDAY_PAIR_qthis.pair_age.count()
processed_tweets_with_obs['t_ob'] = processed_tweets_with_obs['obs'].apply(lambda x: json.loads(x[2:-1])['currently']['temperature']) $ processed_tweets_with_obs['flt_ob'] = processed_tweets_with_obs['obs'].apply(lambda x: json.loads(x[2:-1])['currently']['apparentTemperature'])
from sklearn.model_selection import RandomizedSearchCV $ from scipy.stats import reciprocal, uniform $ param_distributions = {"gamma": reciprocal(0.001, 0.1), "C": uniform(1, 10)} $ rnd_search_cv = RandomizedSearchCV(svm_clf, param_distributions, n_iter=10, verbose=2) $ rnd_search_cv.fit(X_train_scaled[:1000], y_train[:1000])
df.boxplot('MeanFlow_cms');
dollars_per_unit = multi_col_lvl_df['Dollars'] / multi_col_lvl_df['Units'] $ dollars_per_unit.sample(10)
symbol = 'XLE' $ df = make_data([symbol]) $ df.xs(symbol,level='symbol')['2011':].so.rename('shares_outstanding').plot(figsize=(10,4),legend=True) $ df.xs(symbol,level='symbol')['2011':].close_adj.rename('price').plot(title='{}: Shares Outstanding vs Price'.format(symbol),figsize=(8,4),legend=True,secondary_y=True, )
plt.hist(null_vals) $ plt.axvline(x=act_diff, color='red');
df.isnull().sum()[df.isnull().sum() > 0]
autos[['price', 'odometer_km']].corr()
page.status
scaled = scaled.join(vol, how='outer')
head = pd.Timestamp('20150101') $ tail = pd.Timestamp('20160101') $ df = hp.get_data(sensortype='water', head=head, tail=tail, diff=True, resample='min', unit='l/min') $
tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['rating_denominator']==50]
consumer_key = "r13GARnSlIYpn4rT8Cky7lggU" $ consumer_secret = "jNzYyOlz2cKvN5hqTetJGpa9C2HxeTGrq6FLb6J1VlZRwkFMg8" $ access_token = "365121943-MtrTOA6FQ7ZXgkDM70f4fbrvmjI5vtucwok4nd8U" $ access_token_secret = "7LiSAbsWOyLjpagesrBduFvL82NcP4rrdodlMfzCM9Q0r"
monte.str.extract('([A-Za-z]+)', expand=False)
s = pd.Series(np.random.randint(0,7,size=10))  #low,high,size $ print(s,'\n') $ s.value_counts()
source_dummy = pd.get_dummies(fraud_data_updated['source']) $ fraud_data_updated = pd.concat([fraud_data_updated,source_dummy],axis=1) $
autos["price"].value_counts().sort_index(ascending=False)
import datetime as dt
austin['yr_mo'] = austin['started_on'].apply(lambda x:x.strftime('%Y-%m'))
mention_pairs.sort_values("Weight",ascending=False).head()
print("CNN data: ", len(cnn_data)) $ print("Fox News data: ", len(fox_data))
stats_str = json.dumps(stats, indent=2) $ print(stats_str)
s1 - s2
row_df.head()
pd.concat(pieces)
dtm = vectorizer.fit_transform(df['body'])
adopted_cats.head(5)
serial_corp[0], serial_corp[1000]
contractor_merge.rename(index=str, columns={"state_abbrev" :"state_code"}, inplace =True)
X = np.hstack((np.ones_like(x), x)) $ print(X)
import ffn $ df2_portfolio_value =  data['Strategy Return'].cumsum() $ perf2 = df2_portfolio_value.calc_stats() $ print perf2.display()
%%timeit -n 100 $ summary =0 $ for i in ser7: $   summary +=i
lm = sm.OLS(df_new['converted'],df_new[['a/b_page','us_intercept']]) $ result = lm.fit() $ result.summary()
df_tot.Income = df_tot.Income.str.replace('+', '').str.replace(',', '')
import pandas as pd
df_brown
twitter_archive_master = twitter_archive_master.drop(['doggo','floofer','pupper','puppo'],axis = 1)
VXcnn = np.expand_dims(VX, axis=2) $ VXcnn $ scores = model.evaluate(VXcnn, $                         y_label_test_OneHot, verbose=0) $ scores[1]
dir(np) 
opt_fn = partial(optim.Adam, betas=(0.7, 0.99))
dates = ["date_crawled","month_of_registration","year_of_registration","ad_created","last_seen"] $ autos[dates].info() $
ratings = ['review_scores_rating','review_scores_accuracy','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_location','review_scores_value'] $ filtered_df.dropna(axis=0, subset=[ratings], inplace=True)
df_CLEAN1C.head()
%matplotlib inline
pop['California']
df.fillna(method='backfill')
df_max.columns = [dict_names[x] for x in df_max.columns]
print('Number of rows', reviewsDF.shape)
df.head(12)
dat[dat.zip.isnull()]
df.groupby(['Team','Year']).groups
mask = (df['tweet_created'] > "Thu Dec 14 00:00:00 +0000 2017") & (df['tweet_created'] <= "Thu Dec 14 23:59:59 +0000 2017") $ data_2017_12_14 = df.loc[mask]
import pandas as pd $ data = pd.read_csv('data/data_mixed.csv') $ print(data)
df_Outter_trans_users = pd.merge(transactions,users,how="outer",on="UserID") $ df_Outter_trans_users
P = type.__new__(LetsGetMeta,'S',(),{}) $ P.__class__.__class__
df.drop(['text_split'], axis=1, inplace=True)
rows = session.query(Adultdb).filter_by(education="9th").all() $ print("-"*100) $ print("Count of rows having education as '9th' after delete: ",len(rows)) $ print("-"*100)
data2=data.iloc[1:4] $ data2
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2.head()
df2_dummy['intercept']=1
len(df.index)
most_confident_predictions.head()
pd.value_counts(ac['External ID'].values, sort=True, ascending=False) $
import pandas as pd $ Google_stock = pd.read_csv('./goog-1.csv') $ print('Google_stock is of type:', type(Google_stock)) $ print('Google_stock has shape:', Google_stock.shape)
prediction_and_counts=image_predictions_clean.merge(tweet_scores_clean, how='left',on='tweet_id')
transactions.merge(transactions,on="UserID")
to_be_predicted_Day4 = 48.73986814 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
learner.load_encoder('adam1_20_enc')
df1[40:].head(5)
df_mes[df_mes['PULocationID'].astype('int64') >= 266]
f = np.dot(X, w) # np.dot does matrix multiplication in python
plt.figure(figsize=(8, 5)) $ plt.scatter(train_df.favs_lognorm, train_df.comments); $ plt.title('The distribution of the favs_lognorm and number of the comments'); $
twitter_archive.rating_denominator.describe()
sns.set(font_scale=1.5) $ serfeatures = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=True) $ _ = serfeatures.plot(kind='barh') $ _ = plt.title('Relative Feature Importance')
cat_pizza = reviews_without_rare_words(cat_pizza,'reviews_token',rare_wrds)
duplicates_en_emb = compute_sentence_embeddings(duplicates_en, en_embeddings, "en") $ duplicates_fr_emb = compute_sentence_embeddings(duplicates_fr, fr_embeddings, "fr") $ similarity_matrix_emb = cosine_similarity(duplicates_en_emb, duplicates_fr_emb) $ acc_embedding = evaluate_similarity_matrix(similarity_matrix_emb) $ print("Embedding accuracy:", acc_embedding)
a = df.groupby('district_id').count().app_id.value_counts() $ print a/float(a.sum()) $ b = df_nona.groupby('district_id').count().app_id.value_counts() $ print b/float(b.sum()) $
df_precep_dates_12mo = pd.read_sql_query("select date, prcp from measurement where date > '2016-08-22';", engine) $ df_precep_dates_12mo
import pickle $ filename = 'models/sentiment_model.sav' $ pickle.dump(sentiment_model, open(filename, 'wb'))
os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $ Copy_a_file_to_put_in_a_directory(str(today),'txt2pdf.py') $ print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.chdir('../') $ print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END)
df2[df2['group'] == 'treatment']['converted'].mean()
sites = pd.read_csv('../data/station.csv', $                    dtype={'HUCEightDigitCode':'str'})
rf_v1.cross_validation_metrics_summary()
categories = pd.cut(df_titanic_temp.age, [0,age_median,80], labels=['category1','category2']) $ print(categories.value_counts())
df_ms = df_measures[df_measures['ms']==1] $ df_hc = df_measures[df_measures['ms']==0]
LUM.plot_time_all(all_lum_binned)
import pandas as pd
history = import_all(data_repo + 'intervention_history.csv', history=True)
f_ip_device_hour_clicks.show(1)
pd.Timestamp('2017-02-13') - pd.Timestamp('2017-02-12')
df2 [['control', 'ab_page']]= pd.get_dummies(df2['group']) $ df2.head()
print((p_diffs > obs_diff ).mean()) $ print(((p_diffs < obs_diff ).mean()) * 2)
if (10001 % 1000) == 0: $     print('a') $ else : $     print('b')
traindf, testdf = train_test_split(pd.read_csv('github_issues.csv').sample(n=2000000), $                                    test_size=.10) $ print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns') $ print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns') $ traindf.head(3)
erfurt.info()
print len(com311) $ com311 = pd.merge(com311, df, how='outer', on ='Unique Key') $ print len(com311)
df1 = pd.DataFrame({"A":["A1", "A2"], $                     "B":["B1","B2"]},index=[1,2]) $ df2 = pd.DataFrame({"A":["A3", "A4"], $                     "B":["B3","B4"]},index=[3,4]) $ pd.concat([df1,df2])
df.describe()
term_tops = lda.get_term_topics(rand_word, minimum_probability=.0001) $ term_tops, get_topic_desig(term_tops)
df_goog[['Open', 'Close', 'High', 'Low', 'Adj Close']].plot()
df2.query('group == "control"').user_id.size
to_be_predicted_Day4 = 14.85340501 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
complete_df = complete_df.join(pd.get_dummies(complete_df['country'])) $ complete_df.head()
type(historic_data)
old_page_converted = np.random.binomial(1, p, n_old)
cfModel = Sequential() $ merge = Merge([userModel, poiModel], mode='dot') $ cfModel.add(merge)
sinplot() $ sns.set_style("ticks") $ sns.despine(offset=10, trim=True)
fouls_df = scrape_group_stats(base_url, 'fouls')
autos.isnull().sum()
df.to_csv('twitter_archive_master.csv', encoding = 'utf-8')
final.isnull().any()
(active_marketing - inactive_marketing) / SD_marketing
s_pnew-s_pold
raw_data.head()
def mean_grouping(df): $     return df.set_index('datetime').groupby(pd.TimeGrouper('D')).mean().dropna() $ ltccomment = mean_grouping(ltc) $ xrpcomment = mean_grouping(xrp) $ ethcomment = mean_grouping(eth) $
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ print("z_score:",z_score,"p_value:", p_value)
x_normalized = pd.DataFrame(x_scaled)
np.count_nonzero(df.isnull().values)
train["hotel_cluster"].value_counts(1)
tesla.tail()
tfidf_vect = TfidfVectorizer(analyzer = clean_text) $ X_tfidf = tfidf_vect.fit_transform(tweets_1['text']) $ Y_tfidf = tfidf_vect.transform(tweet_table['text'])
calls_df["length_in_sec"].isnull().sum()
autos["odometer_km"].unique().shape
print('The number of rows in the dataset is {}'.format(df.shape[0]))
autos['date_crawled'].str[:10].head()
xmlData['state'].value_counts() $ xmlData['state'].replace({' Washington':'WA'}, inplace = True)
stat_info_st[[0,1]]
%matplotlib inline $ ventas_por_mes.plot('bar', title = 'Ventas por mes por anio', figsize=(15,10))
spire_event = pd.read_csv("spire_eventV3.csv")
Train.head()
y = ['gdp'] $ x = ['overnight','cpi','usd','klci','allocation','year','quarter']
df2.drop(df2.index[2893], inplace = True)
autos["date_crawled"].str[:10].\ $ value_counts(normalize=True,dropna=False).\ $ sort_index(ascending=True)
x = [str(i) for i in range(10)] $ x $ starts_with_digit = tuple(x)
results=results.to_frame().reset_index() $ results.columns=['item_id','users'] $ results.head()
full.groupby(['PrimaryDx'])['<=30Days'].agg({'sum':'sum','count':'count','mean':'mean'}).sort_values(by='mean',ascending=False) $
tweet_df = pd.read_json('tweet_json.txt',orient='index') $
dfJobs
df_new['ab*CA'], df_new['ab*UK'] = df_new['ab_page']*df_new['country_CA'], df_new['ab_page']*df_new['country_UK'] $ df_new.head()
bnbAx.language.value_counts().plot.bar()
data = api.concatFields(data, ['keyphrases']) $ pd.DataFrame(data).head()
df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'], $                    'supervisor': ['Carly', 'Guido', 'Steve']}) $ display('df3', 'df4', 'pd.merge(df3, df4)')
fname = iris.sample_data_path('air_temp.pp') $ temperature_cube = iris.load_cube(fname) $ brewer_cmap = mpl_cm.get_cmap('brewer_OrRd_09')
datastore[0].shape, datastore[1].shape
company_count = {} $ for item in twitter_data.itertuples(): $     for company in item[1]: $         company_count[company] = company_count.get(company,0)+1
tw.name.value_counts()
test_data_df.plot()
iris.head().iloc[:,0]
y_fit_proba = rf.predict_proba(test_data_features)
station_distance.head() $ station_distance.shape
df.dtypes
len(most_freq.imei.unique())
city_pop2 = city_pop.copy() $ city_pop2.columns = ["population", "name", "state"] $ pd.merge(left=city_loc, right=city_pop2, left_on="city", right_on="name")
Zn1_post 
meal_addon_counts_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/meal_addon_counts.csv')['Body'].read().decode('utf-8') $ meal_addon_counts = pd.read_csv(StringIO(meal_addon_counts_csv_string), header=0, delimiter='|')
df = df[df.powerPS != 0] $ df.dropna(subset=['powerPS'],axis=0, how='any', thresh=None, inplace=False) $ df.shape
sum(pd.isnull(newdf['score']))
live_kd915_df = pd.concat([live_df, kd915], axis = 0)
ttTimeEntry.head()
df.drop(treatment_mismatch.index, inplace=True) $ df.drop(control_mismatch.index, inplace=True) $ df.to_csv('AB_test_edited.csv', index=False) $ df2 = pd.read_csv('AB_test_edited.csv', index_col='timestamp', parse_dates=True) $ df2.head(3)
stringlike_instance_3.content = 'changed content'
df2.query('converted == 1').user_id.nunique() / df2.user_id.nunique()
sum(tweet_image_clean.duplicated())
to_be_predicted_Day2 = 48.33846849 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
fields_frame = get_child_frame(fields_node, data)[crop.notnull()]
exploration_titanic.count_unique()
df_wna = df_selparams.dropna()
channel_id = 'UCaLfMkkHhSA_LaCta0BzyhQ'
df3 = df2.merge(df_countries, on='user_id', how='left') $ df3.head()
closeSeriesR.pct_change().nlargest(5)
df2.drop('zeros',axis=1,inplace=True)  # dropping a particular column thats not required
filtered_brewery[['beer_name', 'brewery_name', 'rating_score']][filtered_brewery.brewery_name == top_three[0][0]]
datelist = ['14-Sep-2017', '9-Sep-2017'] $ search_dates=pd.to_datetime(datelist) $ print(search_dates)
pd.Timestamp(d)
df.dropna().shape == df.shape, df.shape
d = [1, 2, 3] $ df * d $
reliableData.describe(include = 'all')
twitter_ar = twitter_ar[twitter_ar['text'] != ' ']
test_train = np.random.random(word_vecs.shape[0]) < .8 $ Xtr = tf_idf_matrix[test_train] $ Xte = tf_idf_matrix[~test_train] $ ytr = y[test_train] $ yte = y[~test_train]
df.loc[df.userLocation == 'Youngstown, Ohio', :]
nnew = len(df2.loc[(df2['landing_page'] == 'new_page')])#updated $ nnew
ts.index.dtype
access_logs_raw = spark.sparkContext.textFile('data/apache.log')
sessions
!ls -l $REPO/taxi_trained/export/Servo
climate_df = pd.DataFrame(climate_data, columns=['date', 'Precipitation']) $ climate_df = climate_df.sort_values('date', ascending=True) $ climate_df.set_index('date', inplace=True) $ climate_df.head()
mid = sample_submission['ID'] $ result.insert(0, 'ID', mid) $ result.head()
p
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page' and converted==1 ").converted.count() $ convert_new = df2.query("landing_page == 'new_page' and converted==1 ").converted.count() $ n_old = df2.query("landing_page == 'old_page'").converted.count() $ n_new = df2.query("landing_page == 'new_page'").converted.count()
n_new = df2['group'].value_counts()[0] $ n_new $
f_counts_week_app.show(1)
test_cleaned
from bmtk.builder.networks import NetworkBuilder $ net = NetworkBuilder('V1')
transactions.head()
measure_avg_prcp_year_df = data_year_df.groupby('Date', as_index=False, sort=False)['Prcp'].mean() $ measure_avg_prcp_year_df.rename(columns={'Prcp':'Avg Prcp'}, inplace=True) $ measure_avg_prcp_year_df.head()
my_gempro.blast_seqs_to_pdb(all_genes=True, seq_ident_cutoff=.7, evalue=0.00001) $ my_gempro.df_pdb_blast.head(2)
df2.orderBy('count', ascending=False).show()
dropoff_kmeans = KMeans(n_clusters=num_dropoff_clusters, random_state=1).fit(dropoff_coords)
df_clean.info()
active = gbq.read_gbq(query1+query2, project_id=project_id,dialect = "standard")
xgb_learner.initialize()
features_df = afl_feature_creation_v2.prepare_afl_features(afl_data=afl_data, match_results=match_results, odds=odds)
guineaFullDf.index.get_level_values(0).value_counts()
femalemoon = pd.concat([moon, femalebydatenew], axis=1) $ femalemoon.head(3)
mypath = '/Users/Lucy/Google Drive/MSDS/2016Fall/DSGA1006/Data/unsupervised' $ df = pd.read_csv(mypath + '/clustering_data.csv',index_col = 0) $ comps = pd.read_csv(mypath + '/competitors.csv')
ogXfinaltemptf= ogXfinaltf.copy() $ ogXfinaltemptf = ogXfinaltemptf.rename(columns={'fit': 'fit_feat'})
dsg['station'][:]
station_count = session.query(Station.id).count()  $ print(f'There are {station_count} weather stations in Hawaii.')
Beverage_name = ["Pepsi","Tropicana Lemonade","Water","Sobe Yumberry Pomegranate","Aquafina Sparkling","Tropicana Pink Lemonade","Diet Pepsi","Diet Sierra Mist","Brisk Raspberry Iced Tea","Sierra Mist","Mountain Dew","Rug Root Beer","Sweet Tea","Diet","LMN","CHR","STW","VAN"] $ new_data['drink2'] = new_data['drink2'].fillna("no") $ new_data['drink3'] = new_data['drink3'].fillna("no")
results.summary()
a_df.index = a_df.TimeCreate $ a_df.rename(columns={0:"Crime Count"},inplace=True) $
data['cat'] = [1 if 'cat' in x else 0 for x in data['title']] $ data['funny'] = [1 if 'funny' in x else 0 for x in data['title']] $ data['love'] = [1 if 'love' in x else 0 for x in data['title']] $
timeFilter = 10 $ sorted_stays = removeNonStays(timeFilter) $ removeNonStays(timeFilter).head()
log_mod4 = sm.Logit(df3['converted'], df3[['intercept','ab_page','evening','eve_new']]) $ results4 = log_mod4.fit() $ results4.summary()
max_user_id = max(data_train['user_id'].max(), data_test['user_id'].max()) $ max_item_id = max(data_train['item_id'].max(), data_test['item_id'].max()) $ n_users = max_user_id + 1 $ n_items = max_item_id + 1 $ print('n_users=%d, n_items=%d' % (n_users, n_items))
tia.head()
print(clf.feature_importances_)
v_item_sat = reindexer(v_item_sat) $ item_sat = reindexer(item_sat)
BLINK.plot_count(blink)
import pandas as pd $ import numpy as np $ df=pd.read_csv('talks.csv') $ df.head()
merged1.drop('Id', axis=1, inplace=True)
df_user_engagement.head()
cvec = CountVectorizer(stop_words='english') $ nb = MultinomialNB() $ nb_pipe_2 = Pipeline([('cvec', cvec), ('nb', nb)])
USres.summary()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'larger') $ z_score, p_value
autos.price.value_counts().sort_index(ascending=False).tail(100)
segmentData.opportunity_source.value_counts()
logit = sm.Logit(data['Churn'], data[train_cols]) $ result = logit.fit()
df_DOT['Complaint Type'].value_counts().head(1)
implied_therapy = ['Therapy', 'New Patient Therapy', ] $ implied_doctor = ['Therapy Telepsychiatry','Follow up Telepsychiatry', 'New Patient Therapy Telepsychiatry',\ $                   'New Patient MD Adult', 'New Patient MD Adult Telepsychiatry'] $ merged1['Specialty'].loc[merged1['ReasonForVisitName'].isin(implied_therapy)] = 'therapist' $ merged1['Specialty'].loc[merged1['ReasonForVisitName'].isin(implied_doctor)] = 'doctor'
station_distance.loc[:, ['Start Station Name', 'End Station Name', $                          'Start Coordinates', 'End Coordinates']].head(2)
rng = pd.date_range('4/23/2018', periods=100, freq='S') # range of 100 seconds $ ts = pd.Series(np.arange(len(rng)), index = rng) # fill in values of 0 - 100 $ ts.head()
group = sdf.groupBy('Cantons', window("_c0", "1 day")).agg(sum("Production").alias('Sum Production')) $ sdf_resampled = group.select(group.window.start.alias("Start"), group.window.end.alias("End"), "Cantons", "Sum Production").orderBy('Start', ascending=True) $ sdf_resampled.printSchema() $ sdf_resampled.show()
monthly_tweets = read_monthly(tweets) $ monthly_tweets.head()
condo_6.describe()
to_be_predicted_Day1 = 81.95 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new
repos.info() $ print "-----------------------------------" $ users.info()
logit_mod3 = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'US', 'UK', 'ab_page_US', 'ab_page_UK']]) $ results3 = logit_mod3.fit() $ results3.summary()
vocabulary_expression['component_6'].sort_values(ascending=False).head(7) $
df2[df2['user_id']==773192] $
data.plot() $ plt.ylabel('Hourly Bicycle Count')
df_country = pd.read_csv("countries.csv") $ df_country.head(3) $ df3 = df2.merge(df_country,how='left',on='user_id') $ df3_country_dummies = pd.get_dummies(df3.country,prefix = 'country').iloc[:,1:] $ df3 = df3.join(df3_country_dummies)
abc = df.groupby(['drg3','year']).describe()
one_week_in_seconds = 60*60*24*7 $ current_time = datetime.datetime.utcnow() $ for guest_id, guest in ama_guests.items(): $     guest['ama.week.diff'] = int((current_time - guest['date']).total_seconds() / $                                              one_week_in_seconds) $
x_normalized = intersections[for_normalized_columns].values.astype(float)
!tail -n 10 p32cfr_results.txt
import json $ import requests
cityID = '5c2b5e46ab891f07' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Las_Vegas.append(tweet) 
tmp = data_df.copy() $ tmp.columns = [x if re.search("size", x) else "data_{}".format(x) for x in tmp.columns]
instrument_1 = 'AUD_USD'
df2.query("user_id == 773192")
model.predict_proba(X_train)[:,0]>0.5
from datetime import datetime, timezone                                      # modules from datetime library
recs = [] $ coef = [] $ specs = [] $ sp_rec = [] $ t = list(range(40, 61, 1))
sentiments[:10]
datetime_now = datetime.now().strftime("%B_%d_%Y") $ output_path = "/home/bukalapak/nietzsche/warehouse/datadump_{}.csv".format(datetime_now) $
statuses = api.GetUserTimeline(screen_name='TIME') $ print([s.text for s in statuses])
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs);
hm_clean.count()
errors_index
titanic_dummy_df = pd.get_dummies(titanic_df, columns=['pclass', 'sex', 'embarked','cabin_floor'])
dates_by_tweet_count
df_concensus_uaa['latest_consensus_created_date'] = pd.to_datetime(df_concensus_uaa['latest_consensus_created_date'], dayfirst=True)
bg2 = pd.read_csv('Libre2018-01-03.txt') # when saved locally $ print(bg2) $ type(bg2) # at bottom we see it's a DataFrame $
df_columns[df_columns.index.month.isin([11,12,1])]['Complaint Type'].value_counts().head() $
finals.loc[(finals["pts_l"] == 1) & (finals["ast_l"] == 0) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'pure_scorers'
new_page_converted=np.random.binomial(1,pnew,nnew) $ s_pnew=new_page_converted.mean()
df_ad_airings_5.shape
transactions.merge(users, how='inner', on='UserID')
wsj_df = getNews(domain_list[0]) $ wsj_df['source'] = "The Wall Street Journal"
state_renames = {'Ny': 'NY', 'IO': 'IA', 'Ca' : 'CA', 'Co' : 'CO', 'CF' : 'FL', 'ja' : 'FL'} $ df.replace({'state': state_renames}, inplace=True)
('threshold, F1 score : {0}').format(gbm_model.F1(valid=True)[0])
import json $ r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=apikey') $
df['First Date'] = df['min'] $ df['Last Date'] = df['max']
corpus_tf = [dictionary.doc2bow(sentence) for sentence in all_sentences] $ print('corpus tf done') $ tfidf = models.TfidfModel(corpus_tf) $ corpus_tfidf = tfidf[corpus_tf] $ print('corpus tfidf done')
X = data['X']  $ y = data['y']  $ X.shape, y.shape
df.head(10)
mileages_by_brand = {} $ for b in brands: $     mileages_by_brand[b] = autos.loc[autos['brand']==b, 'kilometer'].mean() $ mileages_by_brand
d_utc = d.tz_localize('UTC') $ d_utc
knn = KNeighborsClassifier(n_neighbors=5) $ knn.fit(X_train, y_train) $ knn.score(X_test, y_test)
store_items.pop('new watches') $ store_items
grouped_by_plot = merged_data.groupby('plot_id')
df.isnull().sum()
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05)))#considering 95 percent confidance interval
df_user[df_user['user.name'].str.contains('marco rubio', case=False)]
tweet_data_copy.info()
pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'], [1, 2, 1, 2]])
wrd_clean['pupper'].value_counts()[:10]
(v_invoice_link.loc[:, invoice_link.columns] == invoice_link).sum()
clf = decomposition.NMF(n_components=n_topics, random_state=1)
df_json_tweets.drop(['date_timestamp'], axis=1,inplace=True) $ df_json_tweets.info()
target_vs_created = history.loc[~history.target.isnull(), ['target', 'target_test']]
mentioned_bills_all['votes_roll_call'].value_counts().sum()
users.to_csv('users_cleaned.csv')
dic1 = {"col1":[1,2,3], "col2":["practical","data","science"],"col3":["a","b","c"]} $ ex3 = pd.DataFrame(dic1) $ ex3
loans.payback_state[loans.fk_loan.isin(filtered_de_payments)]
df.head() $ df.info()
x = pd.merge(ign, salesdec, on=['Name','Platform'], suffixes=['_ign','_vg'], how='outer', indicator=True) $ x.head()
nf.set_index(rng2, inplace = True) $ nf.head(5)
b_threeish = df['b'].between(2.8, 3.2)
properati.country_name = properati.place_with_parent_names.apply(lambda x : x.split('|')[1])
topUsers_df = pivotTab_toDF(dataset = youtube_df, value="views" \ $                             , indices = "channel_title", aggregFunct = np.sum)[:10] $ print("Top 10 Youtube Channels:\n") $ topUsers_df
for key, sheet in sheets.items(): $     print(f"{key}, {type(sheet)!r}")
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0]
s_n_s_epb_one.index = s_n_s_epb_one.Date $ del s_n_s_epb_one['Date'] $
df3['eve_new'] = df3['evening']*df3['ab_page'] $
alldata.sample(frac = 0.1).head(10)
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_3 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_2.drop(['Neighbors_Obj'], axis=1) $
sample_groups_sorted_by_members = sample_groups.sort_values([ 'MSA_CODE', 'members' ], ascending=False) $ sample_groups_sorted_by_members.groupby([ 'MSA_NAME']).apply(lambda x: x.head(5))
df_chapters_read.head(3)
team_accronym = "PHI" $ team_city = "Philadelphia" $ team_name = "76ers"
apple_tweets.head()
coefs = pd.DataFrame(gs.best_estimator_.coef_[0], index = X_train_total.columns, columns = ['coef']) $ coefs['coef'] = np.exp(coefs['coef']) $ coefs.sort_values(by='coef', ascending = False, inplace=True) $ coefs.head(10)
df.fillna(value = 0) 
payments_all_yrs = (df_providers.groupby(['id_num','name','year'])[['disc_times_pay']].sum()) $ payments_all_yrs = payments_all_yrs.reset_index() $ payments_all_yrs = payments_all_yrs.sort_values(['disc_times_pay'], ascending=[False]) $ payments_all_yrs = payments_all_yrs.reset_index(drop=True) $ payments_all_yrs.head()
kNN100.fit(X, y)
def date_index(df, coin): $     return df[df['symbol'] == coin].set_index('date') $ pricesLTC = date_index(prices, 'LTC') $ pricesETH = date_index(prices, 'ETH') $ pricesXRP = date_index(prices, 'XRP')
df2_new = df2
unigrams_fd = nltk.FreqDist() $ unigram_words = wordsX $ unigrams_fd.update(unigram_words)
df.query('converted == 1').count()[0] / df.shape[0]
df_archive_clean.info()
aapl = pd.read_csv('aapl.csv', index_col='date',parse_dates=True)  # parse_dates makes better for graph $ aapl = aapl.drop(['Unnamed: 0'], axis=1) $ aapl.head() $
Nold = df2.query('group == "control"')['user_id'].nunique() $ print (Nold)
tweets.head() $ tweets.dtypes
appleNegs.groupby('group_id').tweet_id.count().reset_index() $
summed
%%time $ amts = f_loan_amt(these_accounts=these_accts,pconn=pconn)
bnbAx[bnbAx['language']!='en'].country_destination.value_counts().plot.bar()
exogx = np.column_stack([df[x]])
df_ab_raw.info()
df_eng_mult_visits.info()
e_p_b_two = cfs_df[cfs_df.Beat=='5G02'] $ e_p_b_two = e_p_b_two.groupby(e_p_b_two.TimeCreate).size().reset_index()
sns.distplot(train.loan_amnt);
learn.unfreeze()
df_25year=df[df['date']>'1901-02-24'] #last 25 years $ df_25year.groupby('origin')['description'].agg('count').sort_values(ascending=False).head(1)
results.summary()
education_data.head()
sort_by_duation = r.sort_values(by='duration',ascending=False) $ outliers = sort_by_duation.loc[sort_by_duation['duration'] > 3000] $ outliers
df_repub_2016 = df_repub[(df_repub['created_at'] > '2015-12-31') & (df_repub['created_at'] < '2017-01-01')]
samples_query.display_records(10)
oppose.sort_values("amount", ascending=False).head()
pumashp.crs
Counter(sample_value)
index_of_non_unique_user_id = df2[df2['user_id']==duplicated_user_id].index.values.astype(int)[0] $ index_of_non_unique_user_id $ df2.drop(index_of_non_unique_user_id, inplace=True) $ df2.query("user_id == @duplicated_user_id") $ df2.shape[0] #test - this value has to be 290584, which is one less than (df2_size = 290585)
predict_final = reg_final.predict(X) $ print mean_squared_error(y, predict_final) $ print r2_score(y, predict_final)
df2.drop_duplicates(subset=['user_id'], inplace=True) $ print(df2[df2.duplicated(['user_id'])])
latest_date
topics_by_time(period=topics_data.timestamp.dt.weekday, period_str='Weekday (0= Monday, 6=Sunday)')
data.shape
example = pd.DataFrame(index=pd.date_range(start='February 1 2018', end='February 2 2018', freq='H')) $ example.head(5)
scr_retention_df.head()
df2[df2['group']=='control'].converted.mean()
df = json_normalize(j['datatable'], 'data') $ df.columns = col_names $ df.head()
df['rating'].mean()
data.keys()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
df.head()
resolved_links
week48 = week47.rename(columns={336:'336'}) $ stocks = stocks.rename(columns={'Week 47':'Week 48','329':'336'}) $ week48 = pd.merge(stocks,week48,on=['336','Tickers']) $ week48.drop_duplicates(subset='Link',inplace=True)
titanic.describe()
ice_df.head(10)
for n in toNodes: $     dot.node(n)
twitter_archive_master.loc[(twitter_archive_master.name == 'None'),'name'] = 'Unknown'
kimanalysis.queryitems('test', driver='ClusterEnergyAndForces__TD_000043093022_001')
traindf['inspection_result'].value_counts()
lr_cv.fit(X_train, y_train)
p_control = df2.query('group == "control"').count()['user_id']/df2.count()['user_id'] $ p_control_converted = df2.query('group == "control" & converted == 1').count()['user_id']/df2.query('group == "control"').count()['user_id'] $ print('The probability of someone converting given that an individual was in the control group, disregaring the page he/she visited is {:2f}'.format(p_control_converted))
%matplotlib inline $ import seaborn; seaborn.set()
techniques[0]
import pandas as pd $ from datetime import timedelta $ %matplotlib inline $ unemp = pd.read_csv('../10.02-intro_to_timeseries-lesson/datasets/seasonally-adjusted-quarterly-us.csv')
lemmatizer = WordNetLemmatizer()
all_cards.loc["Umezawa's Jitte"]
date = pd.to_datetime("4th of July, 2017") $ date
Base.classes.keys()
Col={'Flavor':["Strawberry","Vanilla","Chocolate"],'Price':[3.50,3.00,4.25]} $ icecream=pd.DataFrame(Col,columns=['Flavor','Price']) $ print(icecream)
print(contrib_state.sort_values(['amount'], ascending=False).set_index('contributor_state'))
df['open'] = df[b'closed_date'] - df[b'created_date']
pickle.dump(lsa_cv_df, open('iteration1_files/epoch3/lsa_cv_df.pkl', 'wb'))
print(df.shape) $ df.tail()
unique_urls[unique_urls.domain == domain].head()
for key, value in g8_groups: $     pprint(value)
df_playlist_videos.head(2)
merged2['Hours_Spent'] = merged2['AppointmentDuration'] /60
r = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2016-12-31&end_date=2018-01-01',auth = ('User','Key')) $ r_limit = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?limit=1',auth = ('User','Key'))
import pandas as pd
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ z_score, p_value
data01 = qr.execute('warehouse', query01) #run query $ data02 = qr.execute('warehouse', query02) #run query
S_distributedTopmodel.initial_cond.filename
actual_next_order = find_next_order_date(all_data,most_recent,84,38) $ display(actual_next_order)
QUIDS["week"].value_counts()
display(data.head(10))
document_term_matrix = tfidf_vectorizer.fit_transform(important_tweets.text)
df = pd.read_csv('../nba-enhanced-stats/2016-17_teamBoxScore.csv') $ df.head()
df.sub(s,axis='index')
plt.xlim(0, 1.0) $ _ = plt.barh(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.values()), align='center') $ _ = plt.yticks(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.keys()))
df.head(50)
shiftlog_entries_df.head()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, $                                                     random_state = 321)
zipcodesdetail = zipcodesdetail[zipcodesdetail.country == 'US'][['zip', 'primary_city', 'county', 'state', 'timezone', 'latitude', 'longitude']] $ zipcodesdetail = zipcodesdetail.rename(columns = {'zip':'zipcode', 'primary_city': 'city'})
model_df = topics_data.drop(['body', 'comms_num', 'id', 'title'], axis=1)
close_series = aapl['close'] $ type(close_series)
import brunel $ df = data.toPandas() $ %brunel data('df') bar x(CHURN) y(EstIncome) mean(EstIncome) color(LocalBilltype) stack tooltip(EstIncome) | x(LongDistance) y(Usage) point color(Paymethod) tooltip(LongDistance, Usage) :: width=1100, height=400 
B=b'spam' # make a bytes object (8-bit bytes) $ S='eggs'  # make a str object (unicode chracters,8-bit or wider) $ type(B),type(S)
from pandas import read_csv $ df = read_csv("tweets.csv")
ps["post patch class"].value_counts()
pd.DataFrame(raw_data['data'])
from sklearn.metrics import classification_report $ print(classification_report(y_test,y_pred_lgr))
 new_page_converted = new_page_converted[:145274] $ p_diff = (new_page_converted/n_new) - (old_page_converted/n_old)
model.add(Dense(16, input_shape=(4,))) $ model.add(Activation('sigmoid'))
filing_ts_df.running_total.plot() $ plt.title("Cumulative Total Filings Over Time") $ plt.ylabel("Cumulative Filings") $ plt.show()
yc_new.head()
autos.columns
def MakeYahooStockURL(Ticker, Start, End): $
print 'Pandas will unexpectedly throw an errorif it can\'t parse a value in to_datetime()' $ print 'To avoid that, set errors = \'coerce\'' $ pd.to_datetime(['Aug 1, 2014', 'foo'], errors='coerce')
min_val = df_noblends[df_noblends[y_col] == 'pinot noir'].shape[0] $ print(min_val) $ is_pinot = df_noblends[df_noblends[y_col] == 'pinot noir'].sample(min_val) $ not_pinot = df_noblends[df_noblends[y_col] != 'pinot noir'].sample(min_val) $ ndf = pd.concat([is_pinot, not_pinot])
df.text[279]
with open("train_df_dict.json", "r") as fd: $     train_df_dict = json.load(fd) $ with open("test_df_dict.json", "r") as fd: $     test_df_dict = json.load(fd)
len(global_sea_level.year) == len(northern_sea_level.year)
df2_controlconverted = df2.query('group=="control"').converted.mean() $ df2_controlconverted
n_old = len(df2.query("group == 'control'")) $ print('The n_old is: {}.'.format(n_old))
print(norm.cdf(z_score)) #value to test significance $ print(norm.ppf(1-(0.05))) # critical value for one sided test $
remove_list = df_errors.tweet_id $ archive_clean = archive_clean.loc[~archive_clean['tweet_id'].isin(remove_list),:]
!head '../data/image_log_20180205.csv'
print('textrank:') $ print(deut5_tr)
walmart.end_time
df = pd.read_csv('df.csv',index_col=0,low_memory=False) $ df.shape
Y = 'label' $ dogscats_h2o[Y] = dogscats_h2o[Y].asfactor() $
mean_sea_level = pd.DataFrame({"northern_hem": northern_sea_level["msl_ib(mm)"], $                                "southern_hem": southern_sea_level["msl_ib(mm)"], $                                "date": northern_sea_level.year}) $ mean_sea_level
sns.lmplot(x="totalAssets", y="totalRevenue", data=fin_df) $ ax = plt.gca() $ ax.set_title("Relationship between Assets and Revenue") $ plt.show()
dfChile = dfChile[dfChile["longitude"] < -66.000000]
print(news_titles_sr.iloc[0]) $
df_r['intercept']=1 $ df_r[['control', 'treatment']] = pd.get_dummies(df['group'])
sentiments_pd = pd.DataFrame(sentiments) $ sentiments_pd.head()
autos = autos.drop(index = price_outliers)
df_local_website.sample(3, random_state=303)
print(ng_m1.wv.similarity('men', 'women')) $ print(ng_m3.wv.similarity('men', 'women')) $ print(ng_m4.wv.similarity('men', 'women')) $ print(ng_m5.wv.similarity('men', 'women')) $ print(ng_m6.wv.similarity('men', 'women'))
promo_df.sort_values(['item_nbr','store_nbr', 'date'], ascending=[True, True, False], inplace=True)
lsi_tf.print_topics(10)
def range_calc(x): $     return x.max() - x.min()
unique_urls[unique_urls.domain == domain].head()
texts = all_df['text'] + ' ' + all_df['url_content']
variables = chosen_user_vars + chosen_vars $ df = pd.DataFrame(data, columns=variables) $ df.head()
y = auto_data[['MPG']] $ x = auto_data[['Displacement', 'Weight', 'Acceleration']] $ x['Horsepower'] = auto_data.Horsepower.map(lambda x : np.nan if '?' in x else float(x)) $ x[x['Horsepower'].isnull()]
all_text.sum().sort_values(ascending=False).head(50)
print(len(coins_infund), "different coins has been in top 10. Out of these,", $       len(intersection),"are now outside top 50." )
changediff = left_change - right_change $ print("Mean Difference: " + str(np.nanmean(changediff))) $ print("Min Difference: " + str(np.nanmin(changediff))) $ print("Max Difference: " + str(np.nanmax(changediff))) $ print("Standard Deviation: " + str(np.nanstd(changediff)))
validation.analysis(observation_data, richard_simulation)
workspaces.to_csv('data/toggl-workspaces.csv')
rollcorr_daily = pd.rolling_corr(r_top10_mat.iloc[:,0], r_top10_mat.iloc[:,1:], window=250) $ print("Monthly vs daily, in size: ", rollcorr.shape, rollcorr_daily.shape)
full = pd.merge(admit,claims,on=['Patient','AdmitDate']) $ full['AdmitDate'] = pd.to_datetime(full['AdmitDate']) $ full.head()
from json import dumps,loads $ tweet = loads(mach) $ print(type(tweet))
rng_eastern = rng_utc.tz_convert('US/Eastern')
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer         # import vader for sentiment analysis $ analyzer = SentimentIntensityAnalyzer()
pd.set_option("display.max_rows",80) $ pd.get_option("display.max_rows")
n_old = (df2['landing_page'] == 'old_page').sum() $ print(n_old)
total_sales.head()
from matplotlib.pyplot import figure $ figure(num=None, figsize=(14, 3), dpi=80, facecolor='w', edgecolor='k') $ names = list(age_hist.age) $ values = list(age_hist.age_freq) $ plt.plot(names, values, linewidth=1.0, marker='o', linestyle='-')
data.to_csv('myfile.csv')  #To_excel exists as well.
prop.head()
file = 'https://assets.datacamp.com/production/course_2023/datasets/dob_job_application_filings_subset.csv' $ df = pd.read_csv(file) $ print(df.head())
samples_query.execute_sql('SELECT ID, Name, DOB, SSN FROM Sample.Person')
num_old = len(df2[df2['landing_page'] == 'old_page']) $ print(num_old)
tomerge = vio2016.groupby(["business_id","new_date"]).count()["date"].to_frame().reset_index() $ tomerge = tomerge.rename(columns = {"date" : "num_vio"}) $ ins2016 = pd.merge(ins2016, tomerge) $
obs_diff = new_page_converted.mean() - old_page_converted.mean() $ obs_diff
df['dog_description'].value_counts()
plot = data.plot(**plot_params) $ _ = plot.set_ylabel("")
df2.columns[df2.columns.str.upper().str.contains('ORIGIN')]
df['Picture_Location'].replace('NA', np.NaN, inplace = True)
cityID = '629f4a26fed69cd3' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Hialeah.append(tweet) 
df_html_extract.info()
store_a_b_left = pd.merge(store_a, store_b, how='left')
df_tweets.sample(10)
X_train.shape
tesla.plot(y='Adj Close') # plotting by indicating which column we want the values from... $ plt.show()
n = 10 $ p = 3 $ Z = np.zeros((n,n)) $ np.put(Z, np.random.choice(range(n*n), p, replace=False),1) $ print(Z)
df2.loc[df2.duplicated('user_id') == True]['user_id']
taxiData.Trip_distance.size
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=58000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
train['hour'] = train.created_at.dt.hour $ train.groupby('hour').popular.mean().plot()
df['intercept'] = 1 $ df[['treatment', 'control']] = pd.get_dummies(df['group'])
top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=25).takeOrdered(25, key=lambda x: -x[1]) $ print ('TOP recommended movies (with more than 25 reviews):\n%s' % $         '\n'.join(map(str, top_movies)))
print ("Probability that individual was in the control group,and they converted: %0.5f" % (df2.query('converted == 1 and group == "control"').shape[0]/df2.query('group == "control"').shape[0]))
n_inputs = 28*28  # MNIST $ n_hidden1 = 300 $ n_hidden2 = 100 $ n_outputs = 10
ab = AdaBoostRegressor(base_estimator=GradientBoostingRegressor(n_estimators = 2000, \ $                                                                 max_depth = 5, min_samples_split = 3), \ $                         n_estimators=50, learning_rate=0.1, loss='exponential', random_state=3)
most_retweets = yxe_tweets.sort_values('retweet count', ascending=False) $ most_retweets.head(10)[['userHandle', 'text', 'retweet count']]
ds[4].head() $
df_gt.head()
costs = 0.1 $ df_customers['profits'] = (df_customers['price']-costs)*df_customers['number of customers']
autos['gearbox'].unique()
logit = sm.Logit(df3['converted'],df3[['intercept' ,'treatment']])
fromId = dfUsers['userFromId'].unique() $ dfChat = dfUsers[dfUsers['userToId'].apply(lambda x: x in fromId)]
total_ridership = most_traffic['TOTAL TRAFFIC'] $ plt.hist(total_ridership)
display('df8', 'df9', 'pd.merge(df8, df9, on="name", suffixes=["_L", "_R"])')
twitter_final = pd.read_csv('twitter_archive_master.csv',encoding='latin-1')
dfSF = pd.read_csv("Case_Data_from_San_Francisco_311__SF311_.csv")
df.isnull().sum()
top_supporters.head(10)
autos['odometer'].head(10)
df2.query('group == "control"')['converted'].mean()
next_day_pf.df
top_2_violations = violations_by_type.nlargest(2).sum() $ total = df['PRIMARY VIOLATION'].count() $ print('Lead content and tracking labels account for {0:.2f}% of violations in this dataset.'\ $       .format((top_2_violations / total) * 100))
test_df.head(15)
n=13 $ df['Close'].pct_change(n).mean()
ebola_melt.head()
image_soup = BeautifulSoup(html, 'html.parser') $ print(image_soup.prettify())
lots=calc_positions_two_lookbacks(10,80,mkts,df,FundAUM).dropna(how='all')
full[full['Patient'] == '5e982461-47d7-4ae9-ac75-d1f31acaee81']
plt = sns.boxplot(data=df, x="race_desc", y="log_time_detained", hue="race_desc", dodge=False) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) $ plt.set_xticklabels(plt.get_xticklabels(), rotation=45)
conn = MySQLdb.connect( host=hostname, user=uid, passwd=pwd, db=database ) $ cur = conn.cursor()
new_page_converted = np.random.binomial(1, p_new, Nnew) $ print(new_page_converted)
top_songs['Month'] = top_songs['Date'].dt.month
df3_month = df3[['Donation Received Month-Year', 'Donor ID', 'Donation Amount']].groupby(by='Donation Received Month-Year', as_index=False).agg({'Donation Amount': ['min', 'max', 'mean', 'sum', 'count'], 'Donor ID': pd.Series.nunique}) $ df3_month.columns = [' '.join(col).strip() for col in df3_month.columns.values] $ df3_month.rename(columns = {'Donor ID nunique': 'Count of Unique Donors', 'Donation Amount min': 'Minimum Donation', 'Donation Amount max': 'Maximum Donation', 'Donation Amount mean': 'Mean Donation', 'Donation Amount sum': 'Total Donations', 'Donation Amount count': 'Count of Donations'}, inplace=True)
df.converted.sum()/len(df)
n = p_diffs.mean()
!du -h 'test.csv'
n_new = len(df2.query('landing_page=="new_page"')) $ n_new   #displaying the number of individuals receiving new page       
new_page_converted = new_page_converted[:nold]
df4 = df3.copy() $ df4 = df4.drop('DAY',1) $ df4 = df4.drop('WEEK',1)
ng_m6 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 30, size=100, sg=1)
TopCompanies
most_active_station_tobs = session.query(Measurement.tobs).\ $ filter(Measurement.station == most_active_station, Measurement.station == Station.station,\ $        Measurement.date >="2017-08-01", Measurement.date <="2018-07-31").all()
RK=match_id(merkmale, merkmale.Merkmalcode.isin(['RK'])) $ RK.Merkmalcode.unique()
df_opc = df2[(df2['landing_page'] == 'old_page')] $ conv = df_opc['converted'] $ old_page_converted = np.random.choice(conv, nold)
import numpy as np $ import pandas as pd $ pd.set_option('display.max_colwidth', -1)
df.loc[:, 'name']
df2['intercept']=1 $ df2['ab_page']=pd.get_dummies(df2['group'])['treatment'] $ df2.head()
plt.rcParams['figure.figsize'] = [16,4] $ plt.plot(pd.to_datetime(mydf1.datetime),mydf1.fuelVoltage); $ plt.ylim(200,400);
to_be_predicted_Day1 = 47.75 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
df_users['churned'].value_counts().plot('bar')
polarity = pd.DataFrame(df.groupby(['polarityFeeling', 'profile']).size().rename('counts')).astype(int) $ polarity['percent'] = polarity['counts'].apply(lambda x: x/polarity.sum()) $ polarity $
split_index = df.index[df['Date'] == split_date].tolist()[0]
print 'Python Version ' + sys.version $ print 'Pandas Version ' + pd.__version__
samples_query.skip_records(20) $ samples_query.display_records(10)
df3['intercept'] = pd.Series(np.zeros(len(df3)), index=df3.index) $ df3['ab_page'] = pd.Series(np.zeros(len(df3)), index=df3.index)
autos = autos[autos['price'] < 500000] $ autos.price.describe()
df.head()
merged.groupby(["contributor_firstname", "contributor_lastname", "committee_position"]).amount.sum().reset_index().sort_values("amount" , ascending=False)
lda = models.LdaModel(corpus=corpus, num_topics=15, id2word=id2word, minimum_probability=.03, passes=10)
np.eye(3)
print_stats(df)
df['ab_test_group'] = df['fitness_test_date'].apply(lambda x: "B" if pd.isnull(x) else "A") $ df.head(5) $
len(df2.query('landing_page=="new_page"'))/len(df2)
med_dist = reddit['Above_Below_Median'].value_counts()/len(reddit) $ med_dist
df_twitter['tweet_id'] = df_twitter['tweet_id'].astype(str)
Q3.plot()
csv_df['uploader'].value_counts()[:12]
mallet_path = 'Data/mallet-2.0.8/bin/mallet' $ ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=7, id2word=dictionary)
closingPrices.nlargest(1)
sims = index[tfidf[vec_test]] $ print(list(enumerate(sims)))
import numpy as np $ data = np.array(['a','b','c','d']) $ ser1 = pd.Series(data) $ print(ser1)
bnbA.age.plot.hist()
test_kyo1['ex_lat'][0]
tweet_json.describe()
props.head()
reviews.region_1.fillna('Unknown').value_counts()
ac[:1]
import pytz $ mountain_tz = pytz.timezone("US/Mountain") $ eastern_tz = pytz.timezone("US/Eastern") $ mountain_tz.localize(now),eastern_tz.localize(now)
x = ['monetary', 'frequency', 'recency','total_cust', 'join_days', 'total_profit'] $ data = df_final[x] $ data.head()
walkmin = walk.resample("1Min")
authors = metadata['author_info'] $ stats['deep_review_authors'] = len(authors)
pold = df2['converted'].mean() $ print(pold)
s.values
import ogh $ homedir = ogh.mapContentFolder(str(os.environ["HS_RES_ID"])) $ print('Data will be loaded from and save to:'+homedir)
import sqlalchemy.engine
nps = shapefilereader(downloadLink)
'issid' in df_protest.columns
hdf = df.set_index(['AgeBins', 'Industry']) $ hdf.loc[('adult', 'Cosmetics'), :].head()
movies.describe()
hours['AppointmentDate'] = pd.to_datetime(hours.index,format='%Y-%m-%d')
nfl['Jimmy'] = np.where( nfl['Date']>=pd.datetime(2017,12,3), 'yes', 'no')
processed_tweets_with_obs = processed_tweets_with_obs[processed_tweets_with_obs['obs'] != 'FAILED']
df2['intercept']=1 $ df2[['old_page','ab_page']] = pd.get_dummies(df2['group']) $ df2.head()
stackedCloses.loc['2012-01-03', 'AAPL']
m3.save('split_model_v1')
autos.describe(include='all')
trainheadlines = train["text"].values $ basicvectorizer = CountVectorizer() $ basictrain = basicvectorizer.fit_transform(trainheadlines) $ print(basictrain.shape)
index_to_change = df[df['group']=='treatment'].index $
D2[0:5]
 df.describe(include='all')
subred_num_tot = reddit[['subreddit','num_comments']].groupby(by='subreddit', sort=True, as_index=False).sum().sort_values(by='num_comments',ascending=False)
t = splits[0].examples[0]
df3.head()
df_new.head(10)
subs = subs.sort_values(by='time_out')
df2['intercept'] = 1 $ df2['ab_page'] = [1 if x == 'treatment' else 0 for x in df2['group']] $ df2.head()
for df in list_to_change: $     df["cumsum_"] = df.diff_.cumsum() $ addicks.head() # to see the cumsum_ column.
fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat']) $ for word in sorted(fdist): $     print(word, '->', fdist[word], end='; ')
troll_users.head()
df_boost_csv = df_boost.to_csv('model1.csv') 
scores = cross_val_score(model, X_train, y_train, cv=5)  # cross Validation $ np.mean(scores), np.std(scores) #  scoring the performance of training data
(-1 * close_month).loc[month].nlargest(2)
df['Website'] = df['Website'].str.rstrip('/')
brand_counts = autos['brand'].value_counts(normalize=True) $ brand_counts
df.head(10) $ df.sort_values(by=['Year']).head(10)
puppos = df1_clean[df1_clean.type == 'puppo'].count()[0] $ puppers = df1_clean[df1_clean.type == 'pupper'].count()[0] $ doggos = df1_clean[df1_clean.type == 'doggo'].count()[0] $ floofers = df1_clean[df1_clean.type== 'floofer'].count()[0] $ puppos, puppers, doggos, floofers
new_page_converted = np.random.choice([1, 0], size=n_new, p=[convert_rate_new, (1-convert_rate_new)]) $ new_page_converted.mean()
pd.value_counts(no_specialty['ReasonForVisitName'])
print("X_train:",len(X_train)) $ print("y_train: ",len(y_train)) $ print("X_test: ",len(X_test)) $ print("y_test: ",len(y_test))
old_page_converted = np.random.choice([1,0],size=nold,p=[pmean, (1-pmean)]) $ old_page_converted.mean()
list(map(range, a.shape))
df = pd.read_csv('C:/Users/Minkun/Desktop/classes_1/NanoDeg/1.Data_AN/L4/project 4/AnalyzeABTestResults 2/ab_data.csv') $ df.head(2)
train_df.head()
%matplotlib inline $ import pandas as pd
df2= df.copy() $ df2.head()
from sklearn.externals import joblib $ joblib.dump(pca, '../models/pca_20kinput_6858comp.pkl')
gDateProject_all = itemTable.groupby([pd.Grouper(freq='1W', key='Date'),'Project'], as_index=True) $ gDateProject_all.groups $ gDateProject_all.count() $ gDateProject_content = gDateProject_all['Content'] $ gDateProject_content = gDateProject_content.count() $
1/_
conditions_clean.value_counts(dropna=False)
cats_out = outcome.loc[outcome['Animal Type']=='Cat'] $ cats_out.shape
billtargs = dfp[['bill_id', 'congress', 'bill_type', 'number', 'chamber', 'committee_ids', 'introduced_on', 'last_action_at', 'last_vote_at', 'official_title', 'urls']]
grouped_ct = events.groupBy("geography_id").agg( count("geography_id") ) $ grouped_ct = grouped_ct.withColumnRenamed( "count(geography_id)", "count" ) $ sorted_ct = grouped_ct.sort( grouped_ct['count'].desc() )
flights2.loc[[1950, 1951]]
from sklearn.ensemble import RandomForestRegressor $ start_fit = time.time() $ reg1 = RandomForestRegressor(random_state=rs, n_estimators=200, n_jobs=-1) $ reg1.fit(X_train, y_train.ravel()); $ end_fit = time.time()
import datetime
converted = (df2.query('converted == 1').count()[0]) $ total = df2.shape[0] $ print ("The number of user converted is {}".format(float(converted) /  float(total)))
df_unit_after.head()
pickle.dump(comments, open('data/comments_with_post_ids.dat', 'wb'))
final_elo = compute_elo_table(my_elo_df) $ final_elo.head(7) $
df=df.dropna(axis=1,how='all')
df['language'] = [np.nan if l == 'C' else l for l in df.language]
setup = people.set_index('Id').loc[ids.keys(), ['Street', 'SetId', 'StartTimeStamp', 'EndTimeStamp']]
twitter_Archive.drop(twitter_Archive[twitter_Archive['retweeted_status_id'].notnull()== True].index,inplace=True) $ twitter_Archive.info() $
twitter_archive[twitter_archive.tweet_id==782021823840026624] $
df2.drop_duplicates('user_id',keep = 'first', inplace = True) $ df2.shape[0]
pd.Period('2005', 'AS')
accts[accts['parent_id'].isnull()].to_sql('account', engine, chunksize=1000, if_exists='append', index=False)
df = pd.DataFrame(data) $ df['keyword'] = searchQuery[1:] $ print(df.head()) $ df.to_csv('Tweets_ss_24.csv', index = False)
!rm world_bank.json.gz -f $ !wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz
data.head(5)
tw.tail()
json.dumps({"data": test_df.to_json(orient='records')})
n_old = df2.query('landing_page == "old_page"').user_id.count() $ n_old
df.sort_values(by=['userid', 'price'])
re.findall('title="(.*?)"', slices[2])
prob_treat = x_treat/y_treat $ prob_treat $
df_log = df_log.fillna(9999) $ df_users = df_users.fillna(9999) $ df_events = df_events.fillna(9999)
closeSeriesP.nlargest(5)
import pandas as pd $ pets = [('Rover', 100, 'german shepard', 'dog'), ('Rex', 20, 'labrador', 'dog'), ('Polly', 5, None, 'bird'), ('Putin', 15, 'siamese', 'cat')] $ pets = pd.DataFrame(pets, columns = ['names', 'weight', 'breed', 'species']) $ pets
json_data['dataset_data']['column_names']
wrong_name_list = ['an', 'the', 'officially','quite']
variance(New_df.Sales_in_CAD, len(New_df.Sales_in_CAD))
X = std_scaler.fit_transform(nuevo_df.values)
cust_data2=cust_data1.set_index("ID") $
susp = pd.read_pickle('C:\\Users\\Andrew\\Documents\\jupyter studies\\reddit\\suspicion.pkl')
import neon_aop_hyperspectral as neon_hs
dup_id = df2[df2.user_id.duplicated()].user_id.values
k1.head()
order
X_train, X_test, y_train, y_test = data.get_X_y(data.simple_features, transformed_df)
joined['intercept'] = 1 $ joined[['CA', 'US']] = pd.get_dummies(joined['country'])[['CA', 'US']] $ joined.head(10)
from nltk.tokenize import word_tokenize #Import word tokenize funktion from NLTK.
files = os.listdir(os.getenv('PUIDATA') + '/archives')
vec1 = CountVectorizer() $ vec1
plt.hist(null_vals);
n_old = df2.query('landing_page == "old_page"').landing_page.count() $ n_old
postsM.reset_index(inplace=True)
knn5 = KNeighborsClassifier(n_neighbors=5, weights='uniform')
kick_projects[['goal', 'pledged', 'backers','usd_pledged','usd_pledged_real','usd_goal_real','state']].corr()
shows = pd.concat([shows,dummies],axis=1) $ shows.head()
pca=decomposition.PCA() $ stocks_pca_t= pca.fit_transform(stocks_pca_m) $
openaccess_df = pd.read_csv('oa_file_list.csv') $ print(openaccess_df.shape) $ openaccess_df['PMCID'] = openaccess_df['Accession ID'].str[3:] $ print(openaccess_df.count(axis=0)) $ openaccess_df.head()
twitter_Archive.describe()
print(type(valid_set[0]), type(valid_set[1]), type(valid_set[2])) $ print(len(valid_set[0]), len(valid_set[1]), len(valid_set[2])) $ print() $ print(type(test_set[0]), type(test_set[1]), type(test_set[2])) $ print(len(test_set[0]), len(test_set[1]), len(test_set[2]))
y_ls.shape
df_blacklist = pd.read_csv(read_inserted_table(dumpfile, tablename),delimiter=",",error_bad_lines=False) $ df_blacklist.rename(inplace=True, columns={'ID':'id', $                                           'IP':'ip'}) $ df_blacklist.head(10)
var_lon = str(-118.159953387143) $ var_lat = str(33.8719625444075)
merged = df2.merge(dfCountry, on='user_id') $ merged.head()
s = pd.Series(['A', 'asdDAr', 'dioD', 'sp', 'MelVille']) $ s.str.lower()
for row in RandomOneDF.take(2): $     print row.ID, row.VAL1, row.VAL2
is_wrong_timestamp = blame.timestamp < initial_commit $ wrong_timestamps = blame[is_wrong_timestamp] $ blame.timestamp = blame.timestamp.clip(initial_commit) $ len(wrong_timestamps)
to_be_predicted_Day2 = 55.11573689 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
z_score, p_value = sm.stats.proportions_ztest(count = [convert_new,convert_old], nobs = [n_new,n_old], alternative ='larger' ) $ print ("z_score:",z_score) $ print("p_value:",p_value)
df_old = df2.query('landing_page == "old_page"') $ p_old = df2['converted'].mean() $ print(p_old)
df.head()
df2.shape
df3.sort_values('timestamp').head(2), df3.sort_values('timestamp').tail(2)
from scipy.stats import norm $ norm.cdf(z_score)
user_profiles = [ $     {'user_id': 218, 'reinsurer': 'Partner Re'}, $     {'user_id': 219, 'reinsurer': 'General Re'}] $ result = db.profiles.insert_many(user_profiles) $
with open("Events_eps0.7_5days_500topics","rb") as fp: $     Events = pickle.load(fp)
csvDF.head()
so[(so['score'] >= 5) & (so['ans_name'] == 'Scott Boston')]
contribs.head()
print(All_tweet_data_v2.rating_numerator.median(),All_tweet_data_v2.rating_numerator.mean())
actual_diff = df2.converted[df2.group == 'treatment'].mean() - df2.converted[df2.group == 'control'].mean() $ (actual_diff < p_diffs).mean()
archive[archive['rating_denominator'] != 10].sample(5)
sample = pd.read_csv('data/sample_submission.csv')
df['Close'].pct_change() #One timeperiod percent change
py.iplot({'data': traces, 'layout': Layout(barmode='stack', xaxis={'tickangle': 40}, margin={'b': 150})}, filename='311/complaints by city stacked')
a.capitalize()
oldest_date = pd.to_datetime(pd.Series([t["created_at"] for t in trump_tweets])).min() $ oldest_date
plt.figure(figsize=(16,8)) $ vT_gb = sns.countplot(x='vehicleType', hue='gearbox', data=df2) $ vg = vT_gb.set_xticklabels(vT_gb.get_xticklabels(), rotation=90)
index = pyndri.Index('../index/')
automl = autosklearn.regression.AutoSklearnRegressor( $ )
train['late_night'] = ((train.hour <= 5) | (train.hour >= 23)).astype(int) $ train.groupby('late_night').popular.mean()
df['OpenData Unique Key'] = df['OpenData Unique Key'].str.replace(',', '') $ df['Agency'] = 'HPD' $ df['Complaint Type'] = 'HEAT/HOT WATER'
ctd_df.dtypes
vecs = pd.concat([dfv, df['subreddit']], axis=1)
page.contributors()
tree.fit(X_train, y_train)
all_tweet_polarity = [] $ for tweet in tweets: $     all_tweet_polarity.append(...) $ all_tweet_polarity
twitter_archive_enhanced[twitter_archive_enhanced.duplicated('tweet_id')]
crimes.columns = crimes.columns.str.replace('__', '_') $
msft.insert(0, 'Symbol', 'MSFT') $ aapl.insert(0, 'Symbol', 'AAPL') $ combined = pd.concat([msft, aapl]).sort_index()
investors_df = pd.merge(investors_df,most_recent_investment, on = 'uuid')
print(str(len(outputList)) + " API requests were made and stored in outputList")
age_up70.head()
country_df = pd.read_csv('countries.csv') $ country_df.head() $ country_df.country.unique()
df = pd.DataFrame() $ for tweet in tweets: $     df['text'] = tweet.text $     df['created_at'] = tweet.created_at $     df['author'] = tweet.author
y_cat = aldf['category'] $ y_cat.shape
pd.Period(pd.datetime.today(), 'A') - pd.Period('1/1/1970', 'A')
borough_group = data.groupby('Borough') $ borough_group.size().plot(kind='bar')
class_test['thanks'].value_counts(normalize=True)
w=(Window.partitionBy('user_id').orderBy(col('score').desc()).rowsBetween(Window.unboundedPreceding, Window.currentRow)) $ predictions_train=predictions_train.withColumn('reccomendations',F.count('user_id').over(w))
save_obj('translated_bkk_tweet', df_bkk)
%run returns.py
norm.ppf(1-(0.05))
bars = ticks.Price.resample('1min', how='ohlc') $ bars
theft.sort_values('DATE_OF_OCCURRENCE', inplace=True, ascending=False) $ theft.head()
for s in re.finditer(r'\d{1,2}\.(mp3|wav)', haystack): $     print(haystack[s.start():s.end()]) $     print('  [file {}]'.format(s.group(1)))
search['trip_duration'] = (search['trip_end_date'] - search['trip_start_date']).dt.days
df_sites = (df_providers.groupby(['id_num','name','year'])[['discharges']].sum()) $ df_sites.head()
temp = temp.loc[temp.tweet_id >= 10,:] $ temp.sort_values(['grade'], ascending=False).iloc[:10,]
new_page_df = df2.query('landing_page == "new_page"')
daily_feature = pd.concat([daily_feature, sentiment_votes_avg_byday(data_demo, "vader")], axis=1) $ daily_feature
teama_merge = my_elo_df.merge(final_elo, how='left', left_on=['Date', 'Team A'], right_on=['Date', 'Team']) $ teama_merge[teama_merge['Team A'] == 'Cloud9'].tail(7)
writer.save()
autos["brand"].value_counts(normalize=True)
missing_val_df=pd.DataFrame(pd.isnull(calls_df).sum()).reset_index() $ missing_val_df=missing_val_df.rename(columns={"index":"Columns",0:"Missing Percentage"}) $ missing_val_df["Missing Percentage"]=missing_val_df["Missing Percentage"]/len(calls_df)*100 $ missing_val_df=missing_val_df.sort_values("Missing Percentage",ascending=False).reset_index(drop=True) $ missing_val_df.head(10)
import nltk $ parser = nltk.ChartParser(my_grammar) $ sentence = word_tokenize("I shot an elephant in my pajamas") $ for tree in parser.parse(sentence): $     print(tree)
ids = df2['user_id'] $ (df2[ids.isin(ids[ids.duplicated()])])
autos['odometer_km'].sort_index(ascending=True).head()
mentions_df.head()
print((X_train_all.select_dtypes(include=['O']).columns.values))
params = result.params # get coefficients $ print(np.exp(params)) # odds ratio of the coefficients
cov_matrix_b = daily_ret_b.cov() $ cov_matrix_b
menu_about_vectorizer = CountVectorizer(tokenizer=custom_tokenizer, ngram_range=(1,1))
total_driver=pd.DataFrame(dfa_1) $ driver_count=total_driver.reset_index() $ driver_count
promo_df['onpromotion']=promo_df['onpromotion'].astype(np.int8)
pd.to_datetime(['04-01-2012 10:00'], dayfirst=False)
targetUsersRank=targetUserItemInt.groupby(['user_id'])['label'].sum().sort_values(ascending=False).to_frame() $ print targetUsersRank.shape $ targetUsersRank.head()
all_tables_df.iloc[0]
latest_df.annotations.head(1)
im.head()
first_name = first_movie.h3.a.text $ first_name
vectorizer = TfidfVectorizer(sublinear_tf=True, $                                  stop_words='english',analyzer='word') $ tf = vectorizer.fit_transform(posts_df["processed_text"]) $ posts_df["tf_idf"] = [x for x in tf.toarray()]
rng = pd.date_range(px.index[0], periods=len(px) + N, freq='B')
lda = np.linspace(10, 170, 500) $ dist = [f(a) for a in lda] $ plt.figure(figsize=(10, 5)) $ _ = plt.plot(lda, dist)
autos['registration_month'].value_counts()
import numpy as np $ A = np.ones((5,5,3)) $ B = 2*np.ones((5,5)) $ print(A * B[:,:,None])
print_models(models)
properati.shape
df['name'].apply(capitalizer)
combined_df.head(2)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=["accuracy"]) $
random_normal = norm.rvs(size=10000) $ plt.hist(random_normal); $ plt.axvline(x=z_score, color='red'); $ plt.axvline(x=critical_value, color='black');
sum(hpd['StatusDescription'].isnull())
transactions.head()
cat_outcomes = pd.read_csv('../data/shelter cats/cats_df_clean.csv')
! cat readme.html | wc -l
pd.DataFrame(df_comment[df_comment.authorName==u'The Zeus']['date'].value_counts()).head()
historicalPriceEFX[:5]
data_archie[data_archie['cur_sell_price'] == 0.0].head()
pd.concat([d1, d3], join='inner')
excelDF.groupby(['Ship Mode','Region']).Sales.mean()
def get_duration_career(input_): $     return max(input_) - min(input_) $ grouped_publications_by_author['duration_career'] = grouped_publications_by_author['publicationDates'].apply(get_duration_career) $
search_booking = search1[search1.booking == 1] $ search2 = search1.append([search_booking]*4,ignore_index=True)
autos.odometer.unique()
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation
tweet_df.columns
pd.read_csv("../../data/msft.csv",nrows=3)
df2.user_id.duplicated().sum()
av = lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_data_filter_settings['ntot_winter'].allowed_variables $ lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_data_filter_settings['ntot_winter'].settings.df[av]
giss_temp.index
PredClass.df_raw.shape
corr = dd_df.corr()
import pandas as pd $ url = 'https://raw.githubusercontent.com/chrisalbon/simulated_datasets/master/data.json' $ df = pd.read_json(url, orient='columns') $ df.head(2) $
z, p = sm.stats.proportions_ztest(count = [convert_new, convert_old], nobs=[n_new, n_old], alternative='smaller') $ print('z-score:', z, $       '\np-value:', p)
model.summary()
ab_df2.query('group == "control"')['converted'].mean()
import pandas as pd $ houses_train = pd.read_csv('../Data/encoded_houses_train.csv') $ houses_test = pd.read_csv('../Data/encoded_houses_test.csv')
!ls -l columncache/acs2015_5year_tract2010/B08006_002.float32
np.array(actual_payments.iso_date,'datetime64[D]')[0:10]
len(nullCity2016)
prog_lang = df[df.Year != 'Year'] $ prog_lang
print(train["review"][0])
autos=autos.drop('abtest',1)
print(data["Ganhadores_Sena"].sum()) $ print(data["Ganhadores_Quina"].sum()) $ print(data["Ganhadores_Quadra"].sum())
import seaborn as sns
data = data.drop(['Name', 'Year_of_Release', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales', 'Critic_Count', 'User_Count', 'Developer'], axis=1)
df['favorite_count'].describe()
tweets_raw["date"] = tweets_raw["date"].apply(lambda d: parse(d))
all_simband_data['group_type'] = all_simband_data['subject_id'].apply(lambda x: subject_id_to_group[x])
all_tables_df.loc[[2, 3, 4, 10, 2000], ['OBJECT_NAME', 'OBJECT_TYPE']]
titanic.groupby(['sex','embark_town'])['survived'].mean()
df_lm.filter(regex='q_lvl_0|last_month|q_lvl_0_c').boxplot(by='last_month', figsize=(10,10),showfliers=False)
%%time $ df['created_at'] = pd.to_datetime(df['Created Date'], format='%m/%d/%Y %X %p')
data = ['peter', 'Paul', 'MARY', 'gUDIO'] $ [s.capitalize() for s in data]
y_t.size
train.MARQUE_LIB.value_counts()
sns.heatmap(rhum_us)
df_img_algo_clean.columns
df4.head()
breaches
import pandas as pd
df['Visitors']
user_retention[['2016-12', '2017-01', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07' ]].plot(figsize = (10, 5)) $ plt.title('Cohorts: User Retention') $ plt.xticks(np.arange(1,12.1, 1)) $ plt.xlim(1,12) $ plt.ylabel('% of Cohort Purchasing');
df.loc[df['edition']=='NBC']
display(data.head(20))
p_diff = p_new - p_old $ p_diff
model.wv["flower"][0:10]
X_test = data3[302:].drop('sales', axis = 1)
print(pd.read_csv("loans_2007.csv", parse_dates=["issue_d","earliest_cr_line","last_credit_pull_d"], nrows=5))
print(s1.head(3)) $ print() $ print(s1.tail())
faa_data_minor_damage_pandas['AIRPORT'].value_counts()
us_grid_id = (pd.read_csv('../data/model_data/us_grid_id.csv', index_col = 0) $               .groupby('grid_id').count().reset_index()) $
df_wm = pd.read_csv("walmart_all.csv", encoding="latin-1")
import pandas as pd $ import numpy as np
tables
set_themes.head()
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $
prop.head()
print(df2.converted.value_counts()) $ print(df2.landing_page.value_counts())
mention_pairs[["FromType","FromName","Edge","ToType","ToName","Weight"]][mention_pairs["Weight"]>1].to_csv("For_Graph_Commons2.csv",index=False)
day = lambda x: x.split(',') $ emotion_big_df['date']=emotion_big_df['date'].apply(day)
SVC = SklearnClassifier(SVC()) $ SVC.train(train_set)
tweet_en = tweet_df[tweet_df.lang=='en'] $ tweet_th = tweet_df[tweet_df.lang=='th']
df_group=df2.groupby('group') $ df_group.describe() $
tweet_text = result.p.text $ tweet_text
appointments.info()
fig, ax = plt.subplots() $ weekday_agg.plot(kind='scatter',x='end_station_id', y= 'duration', c='weekday', s= 50,cmap=plt.cm.Blues, ax=ax) $ plt.show() $
np.exp(-0.0149), 1/(np.exp(-0.0149))
dict_photo = json.loads(df.loc[0,'photo']) $ pprint(dict_photo)
ac.groupby(['IAM', 'Bank'])['Complaint Name'].count()
active_stations = session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).\ $                                 order_by(func.count(Measurement.station).desc()).all() $ active_stations $
actor = pd.read_sql_query('select first_name, last_name from actor', engine) $ actor.head()
new_page_converted = np.random.binomial(1, 0.1196, 145310)
rcn_in_proj = h2020_proj["projectRCN"].unique() $ missing_rcn = h2020_part.loc[~h2020_part["projectRCN"].isin(rcn_in_proj),"projectRCN"]
df_meta.loc[df_meta.user_id == '379881d5-32d7-49f4-bf5b-81fefbc5fcce'].head(3)
df[df['Agency'] == 'DOT'].resample('M').count().plot(y='Created Date')
data["Age (Years)"].mean()
xml_in[xml_in['publicationKey'].isnull()].count()
df_goog.Open.resample('M').plot()
df2.drop(index = df2.query('landing_page =="old_page" and group=="treatment"').index,inplace = True, axis = 0)
train_data = pd.read_feather('data/train_data') $ test_data = pd.read_feather('data/test_data')
df_twitter_archive.name.value_counts()
suspects_with_25_1['is_trip'] = suspects_with_25_1['timestamp'] - suspects_with_25_1['timestamp'].shift(1) > "12:00:00"
n_new = df2.query("group == 'treatment'").shape[0] $ n_new
df.head()
train_centroids = np.zeros((train["review"].size, num_clusters), dtype="float32") $ counter = 0 $ for review in clean_train_reviews: $     train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map) $     counter += 1
h2020_part.loc[h2020_part.projectRCN.eq(208306)] =\ $     h2020_proj.loc[h2020_proj.projectRCN.eq(208306)].projectAcronym[0] $ h2020_part.loc[h2020_part.projectRCN.eq(194607)] =\ $     h2020_proj.loc[h2020_proj.projectRCN.eq(194607)].projectAcronym[0]
import kipoi $ model_name = "DeepSEA/variantEffects" $ model = kipoi.get_model(model_name) $ Dataloader = kipoi.get_dataloader_factory(model_name)
Probas2 = pd.DataFrame(estimator.predict_proba(X2), columns=["Proba_Josh", "Proba_Matt"]) $ joined2 = pd.merge(tweets2, Probas2, left_index=True, right_index=True)
tag_pairs["FromType"] = "Person" $ tag_pairs["ToType"] = "Hashtag" $ tag_pairs["Edge"] = "Mentioned" $ tag_pairs.rename(columns={"screen_name":"FromName","hashtag":"ToName"},inplace=True) $ tag_pairs.head()
(p_diffs > act_diff ).mean()
grouped_df = df.groupby('Date')
twitter_archive.loc[(twitter_archive['name'].str.islower())]
not_in_misk = not_in_misk.copy().sort_values('created_at')
import pandas as pd
df.head()
get_freq(df=raw, colname="progress")
com_grp.get_group(('C1','D3'))
url = "https://api.nytimes.com/svc/books/v3/lists/overview.json?api-key=e096672c9fd940b0a45e031e17e5f002" $ request = urllib.request.Request(url) $ response = urllib.request.urlopen(request) $ rs = response.read().decode('utf-8')
tweet_json.retweeted_status.value_counts()
df3 = tier1_df.reset_index() $ df3 = df3.rename(columns={'Date':'ds', 'Incidents':'y'})
hp = houseprint.load_houseprint_from_file('new_houseprint.pkl')
automl_feat = pickle.load(open(filename, 'rb'))
print(plan['plan']['itineraries'][0]['legs'][0].keys())
query = 'SELECT count(*) FROM ways' $ c.execute(query) $ results = c.fetchall() $ print results[0][0]
r = requests.get('https://en.wikipedia.org/wiki/Python_(programming_language)') $ pd.read_html(r.text, header=0)[1]
gender_counts = nobel['Sex'].value_counts() $ female_pct = round(gender_counts['Female'] * 100.0 / len(nobel), 2) $ "{}% of winners are female".format(female_pct)
!rm score.py myenv.yml
train_data['visitStartTime'] = pd.to_datetime(train_data['visitStartTime'],unit='s') $ train_data['date'] = pd.to_datetime(train_data['date'].astype('str')) $ test_data['visitStartTime'] = pd.to_datetime(test_data['visitStartTime'],unit='s') $ test_data['date'] = pd.to_datetime(test_data['date'].astype('str'))
res = sm.tsa.seasonal_decompose(events_per_day['count_event_day'].values,freq=7,model='multiplicative')
df4 = pd.read_csv('countries.csv') $ df4.head()
max_fwers = df.loc[df.followers.idxmax()] $ name = max_fwers['name'] if max_fwers['name'] is not None else max_fwers['login'] $
weather_yvr = pd.read_csv('data/weather_yvr.csv')
%matplotlib inline $ df['log_AAPL'].plot(figsize=(12,8));
pd_data['year']=pd_data.createdAt.map(lambda x:time.strftime("%Y",time.localtime(int(x)/1000)) if x else x) $ pd_data['month']=pd_data.createdAt.map(lambda x:time.strftime("%m",time.localtime(int(x)/1000)) if x else x) $ pd_data['week']=pd_data.createdAt.map(lambda x:time.strftime("%w",time.localtime(int(x)/1000)) if x else x) $ pd_data['day']=pd_data.createdAt.map(lambda x:time.strftime("%d",time.localtime(int(x)/1000)) if x else x) $ pd_data['pay']=pd_data.status.map(lambda x: 0 if x in ['0','1','2'] else 1)
image_predictions.head()
pets = pd.read_csv('pets.csv', index_col = 0) $ pets
df
predict_actual_df.columns
df.donation_date = pd.to_datetime(df.donation_date) $ df.charitable = df.charitable.astype('bool') $ df['zipcode'] = df.zipcode_initial.str[0:5]
print('Number of rows in customers = {}'.format(customers.CustomerID.count())) $ print('Number of rows in trips = {}'.format(transactions.CustomerID.count()))
noaa_month = noaa_data.loc["2018-07-01":"2018-08-01"] $ noaa_month.loc[:,'AIR_TEMPERATURE'].groupby(noaa_month.index.hour).mean()
zstats,pvalue=sm.stats.proportions_ztest([convert_new,convert_old],[n_new,n_old],alternative="larger") $ print("Zstats",zstats) $ print("pvalue",pvalue) $
bg2 = pd.read_csv('Libre2018-01-03.txt', sep='\t', nrows=100) # local $ print(bg2) $ print(type(bg2))
Test.ReadFlows() $
np.where([min(BID_PLANS_df.iloc[i]['scns_created']) != BID_PLANS_df.iloc[i]['scns_created'][0] for i in range(len(BID_PLANS_df))])
query = pgh_311_data_merged['Category'] == "Road/Street Issues" $ pgh_311_data_merged[query]['Issue'].value_counts(ascending=True).plot.barh(figsize=(10,10))
youthUser1 = user.loc[user['type']=='youth'] $ youthUser1 = youthUser1[['_id','creationDate','cityId','namefirst','namelast','contactInfoemail','contactInfophone', $  'demographicInfodob','demographicInfoethnicity','demographicInfogender','demographicInfozipcode', $  'loginDetailslastLoginTimestamp1','orgInfo']].copy()
trainheadlines = train["text"].values $ advancedvectorizer = CountVectorizer(ngram_range=(2,2)) $ advancedtrain = advancedvectorizer.fit_transform(trainheadlines)
df_concensus.info()
print(tweet_archive[tweet_archive.rating_numerator == 0])
df2
def sumthis(a, b): $     return a+b $ list(map(lambda x, y: sumthis(x, y), [i for i in df.a], [j for j in df.b]))
by_day_by_hour15 = uber_15.groupby(["day_of_week", "hour_of_day"]).size() $ by_day_by_hour15.head()
geo_irregularities.head()
tweet_archive_clean['url'] = tweet_archive_clean.text.str.extract('(?P<url>https://.*/+[a-zA-Z0-9]+)', expand=True)
len(image_predictions_df[image_predictions_df.p1_dog == False])
df_final[features2].describe()
type(df.date[0])
fig = plt.figure(figsize=(10,4)) $ ax = fig.add_subplot(111) $ ax = resid_6201.plot(ax=ax);
from sklearn.model_selection import train_test_split
df_master = pd.read_csv("../01_data preprocessing/data new/masterdata.csv",encoding="utf-8",sep=",", $                         parse_dates=["ValidityDate"],usecols=[0,1,11],dtype={"CardID":object}) $ df_master1 = df_master.sort_values(["CustID","ValidityDate"]).loc[~df_master.duplicated(subset=["CustID"],keep="first")] $ df_master1 = df_master1.loc[:,["CustID","ValidityDate"]]
plt.plot(x, x ** 2)
number_of_commits = git_log['timestamp'].count() $ number_of_authors = git_log["author"].nunique(dropna = True) $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
df2 = df[['FlightDate', 'DayOfWeek', 'Carrier', 'TailNum', 'FlightNum', 'Origin', $          'OriginCityName', 'OriginStateName', 'Dest', 'DestCityName', 'DestStateName', $          'DepTime', 'DepDelay', 'AirTime', 'Distance']] $ df2.sample(5)
condos.head()
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = sm.graphics.tsa.plot_acf(resid_6203.values.squeeze(), lags=40, ax=ax1) $ ax2 = fig.add_subplot(212) $ fig = sm.graphics.tsa.plot_pacf(resid_6203, lags=40, ax=ax2)
%pwd
n_old = len(df2.query("group == 'control'" )) $ n_old
reg.score(X_train,Y_train)
df2[((df2['group'] == 'treatment') & (df2['converted'] == 1))].user_id.count() / df2.user_id.count()
X_c=X_d.drop(['title','text'],axis=1)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=vq_k_-sidSPNHLeBVV8a')
plt.savefig('News Mood Analysis.png')
df2.head()
data.columns
df3 = df3.join(dummy) $ df3.head()
autos["brand"].value_counts(normalize = True)
for obj in cos.Bucket(buckets[0]).objects.all(): $     print('Object key: {}'.format(obj.key)) $     print('Object size (kb): {}'.format(obj.size/1024))
g = sns.barplot(count_categories.index, count_categories) $ for item in g.get_xticklabels(): $     item.set_rotation(90)
df_user_product_ids=pd.merge(left=df_userIds,right=df_productIds,how="outer",on="Key")[["UserID","ProductID"]]
with open(saem_women_save, mode='w', encoding='utf-8') as f: $     f.write(SAEMRequest.text)
filterdf = result.query("0 <= best <= 1 and fileType == 'csv' and teamCount < 1000") $ filterdf.corr()['best'].sort_values()
pd.Series([2,4,6])
autos['registration_year'].describe()
conn = hive.Connection(host=cred['host_hive'], database="default", port=10000, auth="LDAP", username=cred['username'], password=cred['password_hive'])
cdata = {"author":authorflat,"publishdate":publishdateformatted,"title":titleflat,"description":descriptionflat,"upvote":upvoteflat} $ df = pd.DataFrame(data = cdata) $ df.to_csv("c:\\users\\ssalahuddin\\documents\\datacamp130818.csv", header=True, index=False)
print('Under the null hypothesis p_new is the same conversion rate regardless of the page: {}.'.format(conversion_rate_all_pages))
trans_data = pd.read_csv('transaction_data/btc_transaction_data.csv')
selfharmmm_final_df = mf.compile_combo_dfs(epoch3_df, 'cleaned_text', selfharmmm_dictionary, nmf_cv_df, nmf_tfidf_df, lsa_cv_df, lsa_tfidf_df, lda_cv_df, lda_tfidf_df)
ufos_df2 = spark_df.withColumn("year", extract_year(spark_df['Reports']))
data.treatment
twitter_archive_clean.loc[twitter_archive_clean['tweet_id']==810984652412424192]
X_train, X_test, y_train, y_test = train_test_split(X_dfgrb, y_dfgrb, test_size=0.2) $ params = {'n_estimators': 1000, 'max_depth': 4, 'min_samples_split': 2, $           'learning_rate': 0.01, 'loss': 'ls','random_state':42 } $ grbreg = GradientBoostingRegressor(**params) $ grbreg.fit(X_train, y_train) $
le = preprocessing.LabelEncoder()
filtered.ix['2011-11-03':'2011-11-04'].head(20)
predCBoE=model_CBoE.predict()
top_supporters.amount.plot.barh().set_yticklabels(top_supporters.contributor_fullname)
cmask = hdf.index=='child' $ hdf.loc[cmask, 'Age'].head()
df['Unique Key'].resample('M').count().plot()
points[0]
us_grid = np.array(np.meshgrid(grid_lon, grid_lat)).reshape(2, -1).T $ np.shape(us_grid)
df_all_wells_basic.astype(bool).sum(axis=0)
data['Sales'].diff(periods=1).head()
(df2.query("group == 'treatment'")['converted'] == 1).mean()
lm2 = sm.Logit(ab_df_new['converted'], ab_df_new[['intercept','CA','UK']]) $ result2 = lm2.fit() $ result2.summary2()
%cpaste
y_pred = rf.predict(X_test)
for c in ccc: $     vhd[c] = vhd[vhd.columns[vhd.columns.str.contains(c)==True]].sum(axis=1)
df[['product_type','price_doc']].groupby('product_type').aggregate(np.median)
df['log_CBoE']=np.log(df['NASDAQ.CBOE']) $ CBoE_array=df["log_CBoE"].dropna().as_matrix() $ df['diff_log_CBoE']= df["log_CBoE"]-df["log_CBoE"].shift(periods=-1) $ model_CBoE = ARIMA(CBoE_array, (2,2,1)).fit() $ predCBoE=model_CBoE.predict()
df_clean.duplicated('tweet_id').value_counts()
from sklearn.ensemble import RandomForestClassifier
(p_diffs > act_diffs).mean()
tweets_predictions_all.info()
errors['datetime'] = pd.to_datetime(errors['datetime'], format="%Y-%m-%d %H:%M:%S") $ errors['errorID'] = errors['errorID'].astype('category') $ print("Total number of error records: %d" % len(errors.index)) $ errors.head()
cpi_sdmx.translate_expression('1..40055+115524+97558.10')
population.loc['California':'Illinois']
clm = ['store_x', 'Sub_store', 'Brand','stage', 'url_x', 'rpc', 'product_description', $        'review_id', 'review_date', 'username', 'review_text', 'review_rating', $        'variant_1', 'variant_2','variants','image_url']
logout_form = get_logout_form(members_page) $ logout_form $ logged_out = s.post(login_url, data=logout_form, headers=headers) $ logged_out.status_code
iso_join.head()
df_clean3.loc[189, 'text']
initial_date #Printing the initial date
df['Vendor Number'].nunique() #115 $
train['Hours'] = train.apply(lambda row: row[4].split(":")[0] , axis = 1) $ test['Hours'] = test.apply(lambda row: row[4].split(":")[0] , axis = 1) $ train.drop('Flight Time', axis = 1, inplace = True) $ test.drop('Flight Time', axis = 1, inplace = True)
compound_df = compound_df.reset_index()
to_be_predicted_Day2 = 25.06214181 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
n_new = treatment_df.shape[0] $ n_new
with open(join(DATA_FOLDER, 'gold.json'), 'r') as f: $     gold = json.load(f)
new_fp = 'data/brain2body_headers.txt' $ b2b_df.to_csv(new_fp, index=False) # Setting index to False will drop the index integers, which is ok in this case
merged.committee_position.value_counts().reset_index()
tfa_train=train.timestamp_first_active.value_counts() $ tfa_test=test.timestamp_first_active.value_counts() $ tfa_train.unique()
store_items.pop('glasses') $ store_items
print(type(df_final['created_time'][0])) $ pd.__version__
percipitation_measurement_df.plot() $ plt.show()
from sklearn.pipeline import Pipeline $ text_clf = Pipeline([('vect', CountVectorizer()), $                      ('tfidf', TfidfTransformer()), $                      ('clf', MultinomialNB()), $ ])
plt.hist(null_vals); #Here is the sampling distribution of the difference under the null $ plt.ylabel('Frequency', fontsize = 18); $ plt.xlabel(' Difference in Mean', fontsize = 18); $ plt.title('Null Values Plot', fontsize = 18); $
data = data.dropna()
tr, _ = acc.get_month('2016-10') $ tr
aggDF.shape
df_new['intercept'] = pd.Series(np.zeros(len(df_new)), index=df_new.index) $ df_new['ab_page'] = pd.Series(np.zeros(len(df_new)), index=df_new.index)
top_10_authors = git_log.author.value_counts(dropna=True).head(10) $ top_10_authors
features_scores ={} $ for c, v in zip(logreg_sentiment.fit(X, y).coef_[0].round(3), ['Negative sentiment','Positive Sentiment','Neutral','Compound','Length']): $     features_scores[v] = c $ sent_coef = pd.DataFrame(features_scores, index=['Coefficient']).T.sort_values(by='Coefficient', ascending=False) $ sent_coef
'NORDITROPIN' in fda_drugs.DrugName.values
top_supports.head(5)
add_datepart(googletrend, "Date", drop=False) $ add_datepart(train, "Date", drop=False) $ add_datepart(test, "Date", drop=False)
df.head()
print(imgp_clean[(imgp_clean.p1_dog == True) | (imgp_clean.p2_dog == True) | (imgp_clean.p3_dog == True)].shape) $ print(imgp_clean.shape)
joined = pd.merge(tweets1, Probas, left_index=True, right_index=True)
my_result
measure_val_2014_to_2017 = measure_nan[(measure_nan['date'] > '2014-1-1') & (measure_nan['date'] <= '2017-12-31')]
from IPython.display import Image
Meter1.ModeSet('DC_V')
lm = sm.Logit(new_df2['converted'], new_df2[['intercept','ab_page']]) $ reg_lm = lm.fit()
write_acs_5year_description(process_year)
autos["registration_year"].value_counts()
1/np.exp(results.params[1])
submission = reddit.submission(url='https://www.reddit.com/r/CryptoCurrency/comments/7upe01/daily_general_discussion_february_2_2018/')
from pysumma.Simulation import Simulation
upper_region = (p_diffs > full_diff).mean() $ p_value = upper_region $ p_value
p_diffs = [] $ for i in range(10000): $     new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, 1-p_new]) $     old_page_converted = np.random.choice([1,0], size=n_old, p=[p_old, 1-p_old]) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean()) $
second_comb.head(5)
metadata['data_ignore_value'] = float(refldata.attrs['Data_Ignore_Value']) $ metadata
tweet_archive_clean = tweet_archive_clean[tweet_archive_clean.tweet_id != 855862651834028034]
features_std = [35, 36, 37] $ for f in features_std: $     feature_sc = StandardScaler() $     X[:,f] = feature_sc.fit_transform(X[:,f].reshape(-1, 1)).reshape(X.shape[0])
df3['intercept'] = 1
model = clf.fit(train_data, train_labels)
def plot_num_appointments(df, group_col, count_col, plot_name, colormap='Dark2'): $
liberiaFullDf = pd.concat(liberiaFrameList)
quarters + 3
outcomes = [0,1] ### 0 is not converted and 1 is converted $ probs = [1-c_rate_null,c_rate_null] ### probability of 0 is one minus conversion rate at null, and probability of 1 is c_rate_null $ new_page_converted = np.random.choice(outcomes, size= nnew,replace = True, p = probs) $ new_page_converted
weather_mean = weather_all.groupby('Station Name').mean() $ weather_mean.head()
so.loc[so['viewcount'] > 500000, col_bools]
autos = autos[autos["registration_year"].between(1970,2016)] $ autos["registration_year"].describe()
pd.get_dummies(df.C)
print "tortie" , adopted_cats.Tortie.sum() $ print "tabby" , adopted_cats.Tabby.sum() $ print "torbie" , adopted_cats.Torbie.sum() $ print "calico" , adopted_cats.Calico.sum() $ print "point" , adopted_cats.Point.sum()
with open('file_2', 'w') as f: $     writer = csv.writer(f) $     reader = csv.reader(r.text.splitlines()) $     for row in reader: $             writer.writerow(row) $
X = stock.iloc[915:-1].drop(['target', 'news_sources', 'news_text', 'tesla_tweet', 'elon_tweet'], 1) $ y = stock.iloc[915:-1].target
rm_SPY = df['SPY'].rolling(window=20).mean()
predictors = titanic.drop('survived', axis = 1).values $ predictors
mean_w = np.mean(hm_data.weight) $ std_w = np.std(hm_data.weight) $ print 'Mean weight: {}'.format(mean_w) $ print 'StDev weight: {}'.format(std_w)
y_pred=knn3.predict(X_test)
ben_fin = ben_dummy.groupby('userid').nth(1)
ndvi_nc = Dataset("../data/ndvi/AVHRR-Land_v004-preliminary_AVH13C1_NOAA-19_20180604_c20180605095501.nc")
speeches_cleaned = (speeches_cleaned[['mentions_think_tank_Republican_no', 'mentions_think_tank_Democrat_no', $                                     'mentions_think_tank_Non-partisan_no', 'mentions_non_partisan_agency_no', $                                     'uses_statistics', 'id','speaker_bioguide', 'date', 'index']])
raw['hash'] = raw['title'] + raw['company'] + raw['location'] + raw['parse_date'].dt.strftime('%Y%U')
id_list2 = df_clean3[df_clean3.tweet_id.isin(df_image_tweet.tweet_id)].tweet_id $ len(id_list)
twitter_archive_master['favorite_count'].corr(twitter_archive_master['retweet_count'])
learner.load_encoder('adam1_enc')
%matplotlib inline $ plt.plot(data.sepal_length, data.sepal_width, ls ='', marker='o', label='sepal'); $ plt.plot(data.petal_length, data.petal_width, ls ='', marker='o', label='petal')
print(store['prealn/queue'].shape[0]) $ add_table(store, 'prealn/queue', data=problems) $ print(store['prealn/queue'].shape[0])
brand_mileage = {} $ for brand in brands: $     mean_mileage = autos['odometer_km'][autos['brand']==brand].mean() $     brand_mileage[brand] = int(mean_mileage) $ brand_mileage
data = pd.read_csv('bitstampUSD_1-min_data_2012-01-01_to_2018-06-27.csv') $ data.isnull().values.any()
pd.Timestamp('2018-01')
df = df.loc[df['statement_type']!='Flip']
df['Market'].head(1)
df2['converted'].sum()/290584
sum(df2.converted == 1) / 290584
df = pd.DataFrame(d)  # convert data to a pandas data frame
feature_df.to_csv('feature_df.csv',index=False)
import pandas $ pandas.read_csv('Movies.csv')
temp_nc = Dataset("../data/nc/air.mon.mean.nc")
from requests_html import HTMLSession $ session=HTMLSession()
archive_clean.drop(['expanded_urls'], axis=1, inplace = True) $ archive_clean.drop(['doggo'], axis=1, inplace = True) $ archive_clean.drop(['floofer'], axis=1, inplace = True) $ archive_clean.drop(['pupper'], axis=1, inplace = True) $ archive_clean.drop(['puppo'], axis=1, inplace = True)
df.filter(~a_sevenPointSeven).count()
plt.hist(diffs);
hits_df = hits_df.reindex(til_today)
all_preds.shape
df[df['Complaint Type'] == 'Illegal Fireworks'].index.dayofyear.value_counts().sort_index().plot()
autos.head()
ah = list(t.follower_ids(user="ahernandez85b")) $ rl = list(t.follower_ids(user="rlyor")) $ ma = list(t.follower_ids(user="mandersonhare1")) $ print(len(ah),len(rl),len(ma))
import matplotlib.pyplot as plt $ df[['Date','GasPrice']].set_index('Date').plot()
ex=x.mean() $ ey=y.mean() $ print(ex) $ print(ey)
nonc =  df[df['converted']==0].converted.count() $ nonc
likes.shape
IQR = scores_thirdq - scores_firstq $ print('The IQR of the ratings is {}.'.format(IQR))
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth)
pgh_311_data_merged['Category'].value_counts(ascending=True).plot.barh(figsize=(10,10))
df.sort_values(by='B', ascending=False)
import json $ def checkCityExistence(name): $
sns.distplot(titanic.fare)
def a(): $     return 5,6 $ b,c = a() $ print(c)
learn.metrics = [accuracy, auc]
apple['2017-07']['Close'].mean()
print('No missing values')
agg_trips_data = trips_data.groupby('start_station_id').agg({'id':'count','duration':['mean','sum']}).reset_index() # aggregatig data $ agg_trips_data.columns= ['start_station_id','count','mean_duration','total_duration'] #to overwrite multi-column indexe with single index
s = pd.Series(np.random.randn(4)) $ s
merged2 = merged2['2018-02-28':]
rs = pd.Series(cs[0]).sort_values(ascending=0) $ top5 = rs.iloc[0:5] $ top5
tweet_data
t1= twitter.copy() $ t2 = pred.copy() $ t3 = tweet_df.copy()
station_count = session.query(Stations.name).count() $ print(station_count)
ttarc_clean = ttarc_clean[ttarc_clean.rating_denominator == 10] $ ttarc_clean.shape
dfWeek = dfWeek.groupby(['Date', 'Project Name',   $                          'Estimated Start', 'Estimated End', 'Estimated Duration (Days)',  $                           'Sales Stage', 'Likelihood of Win', 'Likelihood Percent', 'Contract Value', $                          'JTBD', 'Big Bet', 'Client Type: Direct Impact', 'Client Type: Funder', 'New/Repeat Client', $                          'Contract Band Size'])[['Weighted Value', 'Contract Value (Daily)']].sum().reset_index()
temp.shape
df2.groupby('group')['converted'].mean()
df = pd.read_csv('twitter_archive_master.csv')
from sklearn.feature_extraction.text import TfidfVectorizer $ from sklearn.cluster import KMeans $ from sklearn.metrics import silhouette_samples, silhouette_score
df2.query('landing_page == "new_page"')['user_id'].count() / df2['user_id'].count()
aru_df = pd.read_csv(aru_file)
p_diffs = np.array(p_diffs)
df['pct_chg_opencls'] = ((df['dollar_chg_opencls']/df['open_price']))
autos['price'].value_counts().sort_index(ascending = True).head(20)
%%bash $ wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_11_06_2017.tar.gz $ tar xvzf ssd_mobilenet_v1_coco_11_06_2017.tar.gz
df.set_index("fundedDate")
df_ca[df_ca['ab_page'] == 0]['converted'].mean()
print len(df) $ print len(df['Master SR #'].unique()) $ print len(df['SR #'].unique())
valid_scores.groupby('tx_TacticId').apply(slice_metrics_2)
words_hash_scrape = [term for term in words_scrape if term.startswith('#')] $ corpus_tweets_scraped.append(('hashtags', len(words_hash_scrape))) # update corpus comparison $ print('Total number of hashtags: ', len(words_hash_scrape)) #, set(terms_hash_stream))
items.old[items.old.code_ogr.isin(obsolete_items)].tail()
book_ratings
es.indices.refresh(index="test-index*")
df.groupby(('C/A', 'UNIT', 'SCP', 'STATION', 'DATE')).sum() $ df
with open("keys/key") as f: $     key = f.read().strip() $ with open("keys/iv") as f: $     iv = f.read().strip() $ obj = AES.new(key, AES.MODE_CFB, iv)
df = df.drop_duplicates(subset='id')
file = 'https://assets.datacamp.com/production/course_2023/datasets/airquality.csv' $ airquality = pd.read_csv(file) $ airquality.info()
df = pd.merge(df, zipcodesdetail, on=['state', 'city', 'zipcode'], how='inner', suffixes=('_x', ''))[newcols]
import pandas as pd
grid_pr_size.describe()
cat 1st_flask_app_2/templates/hello.html
import pandas as pd $ import numpy as np $ url = 'small_train.csv' $ small_train = pd.read_csv(url, parse_dates=[0, 11, 12])
tweets.head()
print('There are {} datasets'.format(len(dsdf)))
import pymysql $ conn = pymysql.connect(host='localhost', port=3306, user='root', password='pythonetl', db='pythonetl') $ cur = conn.cursor()
classes = ['AAPL','FB','005930.KS','TSLA','MSFT','SNE','NOK','AMZN'] $ comp_names = ['apple','facebook','samsung','tesla','microsoft','sony','nokia','amazon']
df_ab_cntry.head()
autos['offer_type'].value_counts()
df2.head(5)
grouped_dpt_city.size()
cancer_data = pd.read_csv('data/small_Endometrium_Uterus.csv', sep=",")  # load data $ X = cancer_data.drop(['ID_REF', 'Tissue'], axis=1).values $ y = pd.get_dummies(cancer_data['Tissue']).values[:,1] $ print X.shape, y.shape
to_be_predicted_Day2 = 43.14093082 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
last_date = df.iloc[-1].name $ last_unix = last_date.timestamp() $ one_day = 86400 $ next_unix = last_unix + one_day
df
prec_fine = np.zeros((844, 26, 59)) $ for i in range(844): $     prec_mon = prec_us_full[i] $     interp_spline = interpolate.RectBivariateSpline(sorted(lat_us), lon_us, prec_mon) $     prec_fine[i] = interp_spline(grid_lat, grid_lon)
tweet_data = pd.read_csv('tweet_json.txt', encoding = 'utf-8')
data2[['Inspired O2 (Ventilator)']]
s2 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd']) $ s2
type(df.Date[0])
type(f)
print("Number of observations in train set :", train1.count()) $ print("Number of observations in dev set :", dev1.count()) $ print("Number of observations for next model training:", modeling2.count())
np.sum(x < 6)
df2 = df.query('group == "treatment" & landing_page =="new_page" | group == "control" & landing_page =="old_page"' )
sq = Square((0, 0), 10) $ print(f"Square size: {sq.x, sq.y}") $ print(f"Square corner: {sq.corner}")
def clean_text(text): $     text = "".join([char for char in text if char not in string.punctuation]) $     tokens = re.split('\W+', text) $     text = [ps.stem(word) for word in tokens if word not in stopword] $     return text
import statsmodels.api as sm $ convert_old = len(df2[(df2['landing_page']=='old_page')&(df2['converted'])]) $ convert_new = len(df2[(df2['landing_page']=='new_page')&(df2['converted'])]) $ print(f"convert_old: {convert_old}\nconvert_new: {convert_new}\nn_old: {n_old}\nn_new: {n_new}")
aqi.boxplot();
from scipy import stats $ chi2,p,dof,expected = stats.chi2_contingency(obs) $ p
to_be_predicted_Day3 = 34.57440189 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
target_pf.head()
confusion_matrix(y_test, y_pred)
points_df.to_csv('all_gps_from_garmin.csv')
sns.jointplot(x="age", y="fare", data=titanic)
import numpy as np $ rng = np.random.RandomState(42) $ x = rng.rand(1000000) $ y = rng.rand(1000000) $ %timeit x + y
url_table = 'https://space-facts.com/mars/' $ table =  pd.read_html(url_table)
print(api.VerifyCredentials())
trends_per_year = defaultdict(list) $ for key,value in deaths_per_year.items(): $     df = df_valid[(df_valid.Died >= datetime.strptime(str(key), "%Y")) & (df_valid.Died < datetime.strptime(str(key + 1), "%Y"))] $     trends_per_year[key] = df.Trends.tolist()
p_new = new_page_converted.mean() $ p_old = old_page_converted.mean() $ actual_diff = p_new - p_old $ actual_diff
df_mes.shape[0]
conv_all = (df2['converted'] == 1).sum() / len(df2.index) $ print(conv_all)
testObj.outDF.head()
bus.head(10) $ ins.head(10) $ vio.head(10) $
from datetime import datetime,date,time $ print(datetime.now()) $ datetime.now().day $ time(2,24)
def join_df(left, right, left_on, right_on=None, suffix='_y'): $     if right_on is None: right_on = left_on $     return left.merge(right, how='left', left_on=left_on, right_on=right_on, $                       suffixes=("", suffix))
temperature.head()
stocks_info_df.tail()
data["Council District"] = data["Council District"].astype("int")
import pandas as pd $ import numpy as np $ from matplotlib import pyplot as plt
db_file = '/home-assistant_v2.db' $ DB_URL = 'sqlite:////' + os.getcwd() + db_file $ DB_URL
prob_conv = len(df2.query('converted == 1')) / len(df2['converted']) $ print('The  probability of an individual converting regardless of the page they receive is {}'.format(prob_conv))
user_logs.head()
j = pd.read_sql_query(QUERY, conn) $ j 
autos['price'].unique().shape
df.head()
measure.info()
tweet['created_at']
ind[1] = 0
tweet_archive_enhanced_clean['rating_denominator'].value_counts()
tmp = pd.Series.from_array(y_test)
pd.Period("2018-04-23") # inferred as Day
df2['intercept'] = 1 $ df2[['ab_page', 'control_page']] = pd.get_dummies(df2['landing_page']) $ df2.head()
df_weather['DATE'] = pd.to_datetime(df_weather['DATE']) $ df_weather['YEAR']= df_weather['DATE'].apply(lambda time: time.year) $ df_weather['MONTH']= df_weather['DATE'].apply(lambda time: time.month) $ df_weather['DAY_OF_MONTH']= df_weather['DATE'].apply(lambda time: time.day) $ df_weather['HOUR']= df_weather['DATE'].apply(lambda time: time.hour)
pickle.dump(lda_tfidf_data, open('iteration1_files/epoch3/lda_tfidf_data.pkl', 'wb'))
subway5 = subway3_df.groupby(['STATION','LINENAME','datetime'])[['Hourly_Entries','Hourly_Exits']].agg('mean')
df.unstack()
x = [str(i) for i in range(10)] $ x $ starts_with_digit = tuple(x)
df2.head(2)
news.head()
np.concatenate([np.random.random(5), np.random.random(5)])
from sklearn.svm import SVC $ from sklearn import svm $ estimator = SVC(kernel='linear') $ from sklearn.model_selection import GridSearchCV
for t in tables: display(t.head())
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
end_date = [USER_PLANS_df.loc[cid,'canceled_at'][np.argmax(USER_PLANS_df.loc[cid,'scns_created'])] if USER_PLANS_df.loc[cid,'cancel_at_period_end'][np.argmax(USER_PLANS_df.loc[cid,'scns_created'])] == False else USER_PLANS_df.loc[cid,'current_period_end'][np.argmax(USER_PLANS_df.loc[cid,'scns_created'])] if USER_PLANS_df.loc[cid,'cancel_at_period_end'][np.argmax(USER_PLANS_df.loc[cid,'scns_created'])]==True else None for cid in churned_ix]
y, X = dmatrices('converted ~ ab_page + CA +UK' , df2_new, return_type='dataframe') $ vif = pd.DataFrame() $ vif["VIF Factor"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])] $ vif["features"] = X.columns $ vif
df2 = df2[['date', 'cust_avail_v3', 'css_count', 'css_score', 'orders', 'new_conv', 'new_sales', 'new_sales_perc', $           'sales', 'calls_per_day']] $ df2.head(1)
plt.plot(df.index, df['Price'])
investors_df.hist(column = 'most_recent_active_year',bins = 20, figsize = (20,8))
train['discussion_hn'] = ((train.url.isnull()) & (train.title.str.contains(' HN: '))).astype(int) $ train.groupby('discussion_hn').popular.mean()
print('There are {} patients readmitted in the 30 day window'.format(np.sum(full['Patient'].value_counts()>1))) $
properati[properati['zone'] == ""]['place_name'].value_counts(dropna=False)
dfRegMet2016.to_pickle(data+"dfRegMet2016Sentences.p")
jsummaries = jcomplete_profile['summaries'] $ recent = pd.DataFrame.from_dict(jsummaries) $ print(recent[['start_date','type','number_of_active_accounts', 'log_ins_market_downturn']][-5:])
del_id = list(df_never_moved['id']) $ df['Timestamp +2'] = df['Timestamp'].apply(addtwo)
day_of_month15 = uber_15["day_of_month"].value_counts().sort_index().to_frame() $ day_of_month15.head()
print("{} is unique user_id are in dataset df2.".format(df2['user_id'].nunique()))
print WorldBankdf.printSchema()
ts.shift(3, freq='D')
tb['All']   # row total, return a Series
fig, axarr = plt.subplots(1, 2, figsize=(12, 6)) $ _ = pd.DataFrame(df_trips["passenger"].value_counts()).plot(kind="hist", bins=11, ax=axarr[0]) $ _ = axarr[0].set_title("Distribution of Passenger Trips Taken") $ _ = pd.DataFrame(df_trips["pilot"].value_counts()).plot(kind="hist", bins=11, ax=axarr[1]) $ _ = axarr[1].set_title("Distribution of Pilot Trips Given")
to_be_predicted_Day3 = 31.23086009 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
df_combined.head()
twitter_archive_full[ $     (twitter_archive_full.stage != 'None')][ $     ['rating_numerator','rating_denominator']].describe()
logs.fm_ip.unique()
p_new_sim - p_old_sim
ed = ['continuing', 'education', 'school'] $ school = wk_output[wk_output.explain.str.contains('|'.join(ed))] $ school.shape $ school.to_csv('school_feedback.csv')
%%time $ df['created_at'] = pd.to_datetime(df['Created Date'], format='%m/%d/%Y %X %p') $ df['closed_at'] = pd.to_datetime(df['Closed Date'], format='%m/%d/%Y %X %p')
nt = pd.read_csv(path+"datas/new_territories.csv", index_col=0)
weather.info()
tweet_archive_enhanced_clean.loc[277,'text']
train_set.columns = ['tweet_id', 'tweetText', 'polarity_value', 'polarity_type', 'topic','set']
zone = 'HUDVL' #'GENESE'#'DUNWOD'#'CENTRL' #'CAPITL' $ df_byzone = pd.read_csv('nyiso_' + zone + '_price.csv', parse_dates=['time_stamp', 'time_stamp_local'])
url_authors = grouped['author'].agg({'mentions': 'count', 'num_authors': 'nunique','authors': lambda x: np.unique(x)})
np.shape(temp_us_full)
call.iloc[-1]
new
df_tsv.head()
from pandas_datareader import data $ goog = data.DataReader('GOOG', start='2004', end='2016', data_source='morningstar') $ goog.head()
breaches = pd.read_csv('breaches.csv', parse_dates=['BreachDate', 'AddedDate', 'ModifiedDate']) $ breaches.head()
investors_df.groupby('investor_type').size().plot(kind = 'bar',figsize = (20,8))
results.params
df.iloc[1]
temp_df2['titles'] = temp_df2['titles'].astype(str)
del train['index']
model.wv.most_similar(positive=['human', 'crime'], negative=['party'], topn=1)
dt_features['created_at'] = pd.to_datetime(dt_features['created_at'],unit='s')
lv_workspace.cfg['indicators']
plt.hist(weather.Precipitation.values, range=(0, 1.4)) #, bins = np.arange(0, 1.4, 0.1) $ plt.show()
with open('image_predictions.tsv', 'wb') as file: $     file.write(response_tweet_image.content)
df = pd.read_csv('./data/train.csv') $ cats = df.project_subject_categories $ cats.sample(10)
ip.info()
results_df.plot(label='prcp', xticks=[]) $
year_tobs_df = pd.DataFrame(year_tobs, columns=['date', 'tobs'])
df_archive_clean.info()
e_neg = df_elect[df_elect['Polarity'] < 0] $ e_neg.shape
tweet_data.info()
model.fit(train_Features, train_species_ohe, epochs=100, batch_size=1, verbose=0);
print(len(new_page_converted) == n_new, len(old_page_converted) == n_old)
for i in c.find().limit(10): $     print i
lv_workspace.get_data_filter_info(step=1, subset='A') 
logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit_mod.fit()
import pandas as pd $ pd.crosstab(np.where(test_set[1] == 0, 1, 0), predictions, rownames=['actuals'], colnames=['predictions'])
targetarticles_files_list = glob.glob('/home/dave/datapubmed/targetarticles/PMC*.nxml') $ targetarticles_files_list = [i[40:] for i  in targetarticles_files_list  ] $ targetarticles_files_list = [i[:-5] for i  in targetarticles_files_list  ] $ print(targetarticles_files_list[:5]) $ print(len(targetarticles_files_list))
p_range = max(v.high-v.low for v in data.values()) $ p_range_date = [v for v in data.values() if v.high-v.low == p_range][0][0] $ print('The largest trading range in 2017 was: {:.2f} on {}'.format(p_range, p_range_date)) $
finaldf.head()
rec_budg = budg[budg['Department Code'] == 'REC'] $ rec_budg.groupby(['Fiscal Year'])[['Amount']].sum()
dfs['Seasonal Difference'] = dfs['Close'] - dfs['Close'].shift(9) $ dfs['Seasonal Difference'].plot()
df_treatment = df2[(df2['group'] == 'treatment')] # create new DF with only treatment group $ df_treatment['converted'].mean() # for the treatment group, probablility of conversion is 0.11880806551510564
twitter_df_clean['timestamp'].sample(5)
list_of_secretly_controlled_companies = secret_corporate_pscs.company_number.unique().tolist()
df_protest.loc[:, df_protest.columns[df_protest.dtypes=='object'].tolist()]
from IPython.core.interactiveshell import InteractiveShell $ InteractiveShell.ast_node_interactivity = "all"
station_activity[0]   
y_pred = rfmodel.predict(X_test)
df_cs['blobw'] = df_cs['cleaned_text'].apply(TextBlob)
print(q6b_answer)
p_new = df2.converted.mean() $ p_new $
f,a = plt.subplots() $ a.bar(num_fli_by_month.index, num_fli_by_month) $ a.set_xticks(num_fli_by_month.index) $ a.set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sept','Oct','Nov','Dec'])
dr.shape
pilot_ratings = np.concatenate(( $     stats.norm.rvs(loc=4.5, scale=0.3, size=70), $     stats.skewnorm.rvs(a=-1, loc=4, scale=1, size=30) $ )).clip(0.01, 4.99) $ df_pilots = pd.DataFrame({"rating" : pilot_ratings, "id": range(len(pilot_ratings))})
df.shape
data['age'][data['age'] < 0] = 365+data['age'] $ data.iloc[140:170,] $
pd.DataFrame( $     np.array(user_status_keys + [""]).reshape(5, 5) $ )
df2[df2.user_id.duplicated(keep=False)]
criteria_1 = so['ans_name'].isin(['Scott Boston', 'Ted Petrou', 'MaxU', 'unutbu']) $ criteria_2 = so['score'] > 30 $ criteria_all = criteria_1 & criteria_2 $ so[criteria_all].tail()
random_forest.fit(train, train_labels) $ feature_importance_values = random_forest.feature_importances_ $ feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values}) $ predictions = random_forest.predict(test)
ldl_values.limit(10).toPandas()
from collections import namedtuple $ fields = ("tag", "count" ) $ Tweet = namedtuple( 'Tweet', fields )
url='https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' $ url_api=url+API_KEY $ r = requests.get(url_api)
p_new - p_old
plots.frequency_of_attack("DAY")
total_order_per_user = prior_orders.groupby('user_id').size().reset_index(name='total_order_per_user') $ total_order_per_user.head()
print('The current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $ df = pd.read_csv('DRG Categorical Payments by YEAR.csv') $ df.head()
pumashp = pumashp.merge(pumaBB[['public use microdata area','pcBB']],left_on='puma',right_on='public use microdata area') $ pumashp.head(1)
df_iberia = df.loc[df.iberia_o_no == "iberia"] $ df_NOT_iberia = df.loc[df.iberia_o_no != "iberia"]
countries = pd.read_csv('countries.csv') $ countries.head()
s1.loc['Tues']
van.revtime = ben_final.revtime.str[:19]
date_info.head()
df_ser_dict.pop("Four") $ df_ser_dict
X, Y = np.meshgrid(np.linspace(-1,1,10), np.linspace(-1,1,10)) $ D = np.sqrt(X*X+Y*Y) $ sigma, mu = 1.0, 0.0 $ G = np.exp(-( (D-mu)**2 / ( 2.0 * sigma**2 ) ) ) $ print(G)
df.mean()
train.drop(['Class', 'Gender'], axis = 1).corr()
plt.hist(commits.log_commits, bins=50) $ plt.xlim((1,8)) $ plt.title('Distribution of Commits per Repo') $ plt.xlabel('Log(commits per repo)') $ plt.ylabel('Count') $
tweets.head()
componentTable
print len(json_df)
my_df["company_create"] = df_user.groupby('nweek_create')['company_id'].nunique() $ my_df["company_active"]   = df_user.groupby('nweek_active')['company_id'].nunique()
%run indebtedness.py
total_budget = sum(df_schools['budget']) $ total_budget
reddit.shape 
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['rowsToEdge'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['closTopBotDist']/0.25 $ df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['rowsToEdge'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['rowsToEdge'].astype(int)
autos["price"].unique().shape[0]
traffic_df = df_traffic.dropna() $ traffic_df_byday = traffic_df.loc[traffic_df.index.weekday==weekday] $ traffic_df_byday.info() $ traffic_df_byday.head()
channel_ids = ['UCaLfMkkHhSA_LaCta0BzyhQ', 'UC6MFZAOHXlKK1FI7V0XQVeA']
ab_file2['intercept'] = 1 #creating an intercept column with all values 1 $ ab_file2[['ab_page']] = pd.get_dummies(ab_file2['group'])[['treatment']] # a dummy variable column
bad_iv_post = options_frame[np.isnan(options_frame['ImpliedVolatilityMid'])]
pd.date_range('2017-12-30', periods=4)  # 4 values using the default frequency of day
df2.head()
df.head()
sNew = pd.Series([1,2,3,4,5], $                  index=pd.date_range('20180102', periods=5)) $ df['F'] = sNew $ df
import time $ time.clock()
df2['intercept'] = 1  #adding intercept column to the dataset $ dummy = pd.get_dummies(df['group']) $ df2['ab_page'] = dummy['treatment'] $ df2.head()
iris.loc[:,"Species"] = iris.loc[:,"Species"].astype("category")
df_final.head()
converted_users2 = float(df2.query('converted == 1')['user_id'].nunique()) $ p2 = converted_users2/float(df2.shape[0]) $ print("The probability of an individual converting regardless of the page they receive is {0:.4}".format(p2)) $
items2 = [{'bikes': 20, 'pants': 30, 'watches': 35}, $           {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5}] $ store_items = pd.DataFrame(items2) $ store_items
from scipy import stats $ chi2, p, dof, expected = stats.chi2_contingency(ct.values) $ p
df_l_s["loc"][:1]
pd.to_datetime(pd.Series(['Jul 31, 2009', '2010-01-10', None]))
n_bins = 10 $ bin_fxn = lambda y: pd.qcut(y,q=n_bins,labels = range(1,n_bins+1)) $ features['f15'] = prices.volume.groupby(level='symbol').apply(bin_fxn) $
single_patient.columns
hdf.loc[(slice('adult', 'child'), slice('Alcoholic Beverage', 'Choc/Cocoa Prod')), :].head()
review_body.head()
popups = soup.findAll('a', {'aria-haspopup': 'true'})
connection = sqlite3.connect("stocks.sqlite") $ stocks = pd.io.sql.read_sql("SELECT * FROM STOCK_DATA;", $                              connection, index_col="index") $ connection.close() $ stocks.head()
def vectorize_string(txt): $   vec = proc.transform([txt])[:,1:] $   emb = np.mean(embedding_model.predict(vec), axis=1) $   return emb
train_data.isnull().sum()
df2.country.value_counts()
cercanasA1_11_14Entre25Y50mts = cercanasA1_11_14.loc[(cercanasA1_11_14['surface_total_in_m2'] >= 25) & (cercanasA1_11_14['surface_total_in_m2'] < 50)] $ cercanasA1_11_14Entre25Y50mts.loc[:, 'Distancia a 1-11-14'] = cercanasA1_11_14Entre25Y50mts.apply(descripcionDistancia, axis = 1) $ cercanasA1_11_14Entre25Y50mts.loc[:, ['price', 'Distancia a 1-11-14']].groupby('Distancia a 1-11-14').agg(np.mean)
len(prcp_data)
van_out = van15_fin.drop(['diffs','pagetitle','stiki_mean','cluebot_mode'],axis=1)
def count_paragraphs(text): $     return text.count('\n\n') + 1 $ count_paragraphs('Hello \n\n World \n\n\n !')
SeriesJota = pd.Series(ListJota) $ SeriesJota
merged_DA_power_df = pd.read_excel(data_folder_path + '/temp/day_ahead_merge_power.xlsx')
writers.groupby('Country').all()
 j = pd.concat([a, b], axis=1, join_axes=[a.index])
_ = ok.grade('q05c') $ _ = ok.backup()
(df2.landing_page == 'new_page').sum() / df2.user_id.count()
dates[0]
score_c.shape[0] / score.shape[0]
tag(content='testing',name='img')
time_df = pd.DataFrame(en_test_df[['created_at']]) $ time_df = time_df.reset_index(drop=True) $ time_df.head()
filtered_greater_100 = english_df.groupBy('hashtag').count().filter('count > 100')
data['Created Date'] = data['Created Date'].apply(lambda x: datetime.datetime. $                                                   strptime(x,'%m/%d/%Y %H:%M'))
y_q.value_counts()
len(df[df['converted']==1]['user_id'].unique())/ (1.0*n_unique_users)
new_page_converted = np.random.choice([1, 0], n_new, (p_new, 1-p_new))
!pip3 uninstall -y Pillow
aapl = pd.read_csv(file_name, index_col='Date') $ aapl
del train_df
games_2017 = nba_df.loc[(nba_df.index.year == 2017), ] $ GSW_2017 = games_2017.loc[games_2017.loc[:, "Team"] == "GSW", ] $ rest_2017 = games_2017.loc[games_2017.loc[:, "Team"] != "GSW", ]
top5_days.plot(kind='barh')
s1 = pd.Series([0, 1, 2], index=[0, 1, 2]) $ s2 = pd.Series([3, 4, 5], index=['0', '1', '2']) $ s1 + s2
pokemon.drop(["Total"],inplace=True,axis=1) $ plt.figure(figsize=(10,6)) #manage the size of the plot $ sns.heatmap(pokemon.corr(),annot=True) #df.corr() makes a correlation matrix and sns.heatmap is used to show the correlations heatmap $ plt.show()
datAll = datAll[pd.notnull(datAll['zip'])]
p
plt.hist(p_diffs); $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('simulated 10,000 values in p_diffs'); $
print(train_df["description"].head()) $ tfidf = CountVectorizer(stop_words='english', max_features=100) $ tr_sparse1 = tfidf.fit_transform(train_df["description"]) $ te_sparse1 = tfidf.transform(test_df["description"])
odds = vis["Trophies"].plot(kind = "bar", figsize = (25, 5), title = "Number of trophies won in the world cup", rot = 90, legend = True) $ odds.set_ylabel("Trophies", fontsize = 15) $ plt.show()
High_temp_station = session.query(func.min(measurements.tobs), $                                   func.max(measurements.tobs), $                                   func.avg(measurements.tobs)).\ $                     filter(measurements.station == active_stations[0][0]).all() $ High_temp_station
df2.query('landing_page=="old_page"')['user_id'].count()
! ls -1 ./data/raw-news_tweets-original/
n_new = len(df2.query("group == 'treatment'")) $ (n_new)
neg_columns = [word for word in columns if word in negative_words] $ dtm_neg = dtm_df[neg_columns] $ dtm_neg['neg_count'] = dtm_neg.sum(axis=1) $ dtm_neg['neg_count']
a+'.'
test_prediction = clf.predict_proba(test_df[features])
theft_bool = crimes['PRIMARY_DESCRIPTION']=='THEFT' $ theft_bool
print("Mean squared error: %.2f"% mean_squared_error(y_test, y_pred))
df2_con=df2[df2['group']=="control"] $ prob1 = df2_con[df2_con['converted']==1].shape[0]/df2_con.shape[0] $ prob1
df = pd.read_csv('Tweets_retweet_count.csv') $ df.head()
X_train
top10.plot.pie();
clean_appt_df = train_set.copy() $ clean_appt_df.shape
data.to_csv('time_series_manu.csv')
department_df
df.rename(columns={'Indicator':'Indicator_id'}).head(2)
df5 = df4.set_index("user_id").join(df3.set_index("user_id"), how="inner") $ df5.head()
pconversion = df2['converted'].mean() $ pconversion
events_df[events_df['event_day']==events_df['event_day'].min()]
stamp.strftime('%d/%b/%y')
hand = pd.get_dummies(auto_new.Hand_Drive) $ hand.head()
n_high/total
combined_df2.fin_year.value_counts()
cnf_matrix = confusion_matrix(y_test, yhat_knn, labels=['PAIDOFF','COLLECTION']) $ np.set_printoptions(precision=2) $ plt.figure() $ plot_confusion_matrix(cnf_matrix, classes=['PAIDOFF','COLLECTION'],normalize= False,  title='Confusion matrix')
data_compare.plot()
import pandas as pd $ tweets = pd.read_csv ("./twitter.csv", header=None) $ tweets.head()
merged2['DurationHours'] = merged2['AppointmentDuration'] /60
clicking_conditions.info()
archive_copy['name'].loc[archive_copy['name'].str.islower()] = 'None'
top_20['retweet_count'].sort_values().plot.barh(figsize=(10, 8));
mb = pd.read_csv("../data/microbiome/microbiome.csv") $ mb.head()
time2close_2015 = merged_tickets_2015["time2close"]
non_null_counts = df.count() $ non_null_counts[non_null_counts < 1000]
plt.figure(1) $ actor_counts = df['Actor1Name'].value_counts().head(10) $ actor_counts.plot.bar()
trips_sorted_pilot.head(10)
seen_and_click = pd.merge(seen, click,  how='outer', left_on=['article_id','user_type','user_id','project_id'], right_on = ['article_id','user_type','user_id','project_id'])
import requests $ import pprint $ pp = pprint.PrettyPrinter(indent=4) $ from collections import OrderedDict, defaultdict, namedtuple
Z = np.random.random(10) $ Z[Z.argmax()] = 0 $ print(Z)
import pandas as pd $ import numpy as np $ import re $ autos = pd.read_csv('autos.csv', encoding = 'Latin-1') $ autos.head()
data = data.loc[(data.place_with_parent_names.str.contains('Capital Federal')) | \ $                 (data.place_with_parent_names.str.contains('G.B.A.'))]
nba_df.tail(10)
mean1 = df[df['dataset_location'] == 'MICU2-6FL-B1'].groupby(['dataset_location'])['ABPm'].rolling('900s').mean().reset_index() $ max1 = df[df['dataset_location'] == 'MICU2-6FL-B1'].groupby(['dataset_location'])['ABPm'].rolling('900s').max().reset_index() $ min1 = df[df['dataset_location'] == 'MICU2-6FL-B1'].groupby(['dataset_location'])['ABPm'].rolling('900s').min().reset_index() $ mock_data = pd.merge(mean1, max1, how = 'left', on = ['dataset_datetime', 'dataset_location']) $ mock_data = pd.merge(mock_data, min1, how = 'left', on = ['dataset_datetime', 'dataset_location'])
df2['intercept']  = 1
breed_conf = breed_conf.sort_values(ascending = False)
df_clean3[df_clean3.rating_denominator != 10].sample(5)
df['user_id'].nunique()
plt.hist(taxiData.Trip_distance, bins = 100, range = [taxiData.Trip_distance.min(),taxiData.Trip_distance.max()]) $ plt.xlabel('Traveled Trip Distance') $ plt.ylabel('Counts of occurrences') $ plt.title('Histogram of Trip_distance') $ plt.grid(True)
df['ts_year'] = df['timestamp'].dt.year $ df['ts_dayofweek'] = df['timestamp'].dt.dayofweek
import datetime $ import time
subway = subway3_df[(subway3_df['SCP'] == '02-00-00') & (subway3_df['UNIT'] == 'R051')]
df3 = pd.read_csv('countries.csv') $ df4 = df3.set_index('user_id').join(df2.set_index('user_id')) $ df4.head()
print final['priority'].value_counts() $ print 'Sum of total:', np.sum(final['priority'].value_counts())
print (r.json())
import numpy as np
movie_ratings.loc[:, 'year'] = movie_ratings['year'].str[-5:-1].astype(int)
spark.sql('create database if not exists my_analysis_work') $ spark.sql('drop table if exists my_analysis_work.example_timeseries') $ joined_patient.write.saveAsTable('my_analysis_work.example_timeseries')
df[df['D'] == 10.72]
p_diffs = [] $ new_sim = np.random.binomial(n_new,p_new,10000)/n_new $ old_sim = np.random.binomial(n_old,p_old,10000)/n_old $ p_diffs = (new_sim - old_sim) $ p_diffs = np.array(p_diffs)
b.find_by_xpath('//*[@id="day-section"]/div/div[3]/div[8]/div[2]/ul/li[4]/button').click()
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=56000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
print('{}index/agencies/{}'.format(base_url, '1')) $ print(json.loads(requests.get('{}index/agencies/{}'.format(base_url, '1')).text))
print "The column names in df1 are " + str((df1.columns.values)) $ df1
stat_info[0]
stock['target_class'] = stock.target.apply(lambda x: 1 if x >= 0 else 0)
df_archive_clean.text[1]
print(page_soup.prettify())
autos['price'] = autos['price'].str.replace('\$', '') $ autos['price'] = pd.to_numeric(autos['price'].str.replace(',', '')) $ autos['odometer'] = (autos['odometer'].str.replace(',', '')) $ autos['odometer'] = pd.to_numeric(autos['odometer'].str.replace('km', '')) $ autos = autos.rename(index = str, columns = {'odometer' : 'odometer_km'})
styles = df.groupby('simple_style').size().sort_values(ascending=False) $ styles.head(5)
top_membership = df.sort_values("members", ascending=False).reset_index(drop=True) $ top_membership.head(5)
df[df['Created At'] > df['Shipped At']].shape
data1.dropna(inplace=True)
df_ml_54_01.tail(5)
df_customers[['number of customers','MA_3','MA_6']].plot()
map_train = split_cols(cat_map_train) + [contin_map_train] $ map_valid = split_cols(cat_map_valid) + [contin_map_valid] $ map_test = split_cols(cat_map_test) + [contin_map_test]
theta = opt.fmin_bfgs(cost_computation, theta_0, fprime=grad_computation, args=(X_train_1, y_train))
print(np.isfinite(trainDataVecs).all())
titanic.groupby('sex')[['survived']].mean()
df1 = pd.DataFrame(np.arange(9.0).reshape((3, 3)), columns=list('bcd'), index=['Ohio', 'Texas', 'Colorado'])
poverty.drop(rows_todrop_1, inplace=True)
p_mean = np.mean([p_new, p_old]) $ p_mean $ print('Probability of conversion under the null hypothesis ',p_mean )
tweet_df_clean.info()
consumerKey= 'XXXXXXXXXXXXXXX' $ consumerSecret='XXXXXXXXXXXXXXXX' $ auth = tweepy.OAuthHandler(consumer_key=consumerKey, $                            consumer_secret=consumerSecret) $ api=tweepy.API(auth)
p_convert_obs_diff = p_convert_treatment - p_convert_control $ p_convert_obs_diff
df2['duplicate'] = df2['user_id'].duplicated() $ user_id_duplicate = df2['user_id'].loc[df2['duplicate'] == True] $ print("The one user_id repeated in df2 is : {} ".format(user_id_duplicate.iloc[0]))
All_tweet_data_v2=All_tweet_data_v2[All_tweet_data_v2.rating_numerator<=30]
data.plot(title='AAPL stock price | 50 & 200 days SMAs', figsize=(10, 6))
temps_df.iloc[1]
is_unpaid.plot.scatter(x='score', y='paid_status', alpha=0.3, figsize=(20, 10))
print(f'The value of variable b plus variable a is {b + a}.')
It appears there are a few stores that have significantly more sales than the rest
print "Mean time for closing a ticket in 2013: %f hours" % (time2close_2013.mean()/3600.0) $ print "Median time for closing a ticket in 2013: %f hours" % (time2close_2013.median()/3600.0) $ print "Quantiles: " $ print time2close_2013.quantile([0.25, 0.5, 0.75])
users_visits = users_visits.assign(user=1) $ users_visits = pd.merge(users_visits, Relations, how='outer', on=['name', 'id_partner']) $ users_visits = users_visits[['visits', 'user', 'chanel']] $ users_visits.head()
merged_df.plot(x='state', y='Percent margin',  kind='bar') $ merged_df.plot(x='state', y='ad_duration_secs',  kind='bar')
import twitterUtils
top10.plot.pie( $     figsize=[5,5], $     title="Top 10 Authors", $     label="");
df2_new.head()
sox['date'] = sox.START_DATE.str.slice(0,10) $ sox.rename(columns={'OPPONENT':'opponent'}, inplace=True) $ sox['playoff'] = 0 $ sox['late'] = 0 $ sox['day_of_week'] = pd.DatetimeIndex(sox.date).weekday
df7.plot(x='Date', y='BG') $ plt.title('Blood Glucose over 600 Values starting Dec. 22, 2017', $           color='Green') $ plt.show()
from datetime import datetime
model = sm.tsa.ARIMA(train, (1, 1, 0)).fit() $ predictions = model.predict(dynamic=True) $ print("Mean absolute error: ", mean_absolute_error(test, predictions)) $ model.summary()
import sqlite3 $ import pandas as pd
croppedFrame.describe()
S_1dRichards = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure09/summa_fileManager_1dRichards.txt')
get_response('My spririt animal is a menacing cat. What is yours?')
def print_rmse(model, name, input_fn): $   metrics = model.evaluate(input_fn=input_fn, steps=1) $   print 'RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['loss'])) $ print_rmse(model, 'validation', make_input_fn(df_valid))
preds_df
df_mysql.head()
%%time $ grid_svc.fit(X, y)
a1 = np.array([1, 2, 3, 4]) $ a2 = np.array([4, 3, 2, 1]) $ a1 + a2
j = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&start_date=2014-01-01&end_date=2014-01-02&api_key=' + API_KEY) $
n_old = df2.query('group == "control"').shape[0] $ n_old
journalist_retweet_gender_summary(journalists_retweet_df[journalists_retweet_df.gender == 'M'])
input_hash_tag
time2close_2014 = merged_tickets_2014["time2close"]
grouped.columns
images.info()
fig = m.plot_components(forecast);
donations['Donation Amount'].value_counts().head(10)
len(X)/2
for tweet in tweets: $     print(tweet.author.name) $     print(tweet.text)
test_df.head().T
site.get_sensors('electricity')
df_new.head()
weather_features.iloc[:6]
pageviews_tags.info()
geocoded_df.loc[idx,'Case.Duration'].dt.days.plot()
print(clean_madrid.info()) $ clean_madrid.head()
print("Service URL: {}".format(web_service._service_url)) $ print("Service Key: {}".format(web_service._api_key))
dc['energy'].mean() $ dc['energy'].var() $ dc['friendliness'].mean() $ dc['friendliness'].var()
dfHawWQ = dfSubSites[dfSubSites['MonitoringLocationIdentifier'] == 'USGS-0209699999']
season16["InorOff"] = "In-Season"
sunspots.info()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
TEXT.vocab.stoi['the']
images.tail()
logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit_mod.fit() $ results.summary()
s.reset_index()
df.groupby("PredictedIntent").agg({'Notes':'count'})
owns.to_pickle('data/pickled/new_subset_owns.pkl')
new_page_converted = np.random.choice([0,1],n_new,p_new); $ new_page_converted
pold
heights_B=pd.Series(np.random.normal(170,25,5),index=['s1','s2','s3','s4','s5']) #pd.Series(170+25*np.random.randn(5),index=['s1','s2','s3','s4','s5']) $ weights_B=pd.Series(np.random.normal(75,12,5),index=['s1','s2','s3','s4','s5']) #pd.Series(75+12*np.random.randn(5),index=['s1','s2','s3','s4','s5'])
SVM_yhat = SVM.predict(test_X) $ print("SVM Jaccard index: %.2f" % jaccard_similarity_score(test_y, SVM_yhat)) $ print("SVM F1-score: %.2f" % f1_score(test_y, SVM_yhat, average='weighted') )
month_initial = s3.get_object(Bucket=bucket, Key=f'{prefix}/bitcoinmonth.csv') $ raw_data = month_initial $ df_all_columns = pd.read_csv(io.BytesIO(raw_data['Body'].read()), encoding="ISO-8859-1", low_memory=False)
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key={}".format(API_KEY)) $ print(r.json()['dataset']['data'][0])
pandas_list_2d_rename = pandas_list_2d.rename(columns={0 : 'Name', 1: 'ID', 2 : 'State'}) $ print(pandas_list_2d_rename)
model.predict_proba(np.array([1,50]))
pd.groupby(vlc, by=[vlc.visited.dt.year,vlc.visited.dt.month]) \ $ .size() \ $ .plot(kind="bar", title="Visits distributed by Month", color="green", figsize=(12,4))
incl_Ss.shape
import re $ letters_only = re.sub('[^a-zA-Z]',              # the pattern to search for $                              " ",                          # the pattern to replace it with $                              example1.get_text())   # the text to search $ print(letters_only[:100])  
a = list(topic_list.values()) $ a.extend(usrfeat + docfeat) $ zipped = zip(a, np.ravel(coefs)) $ coef = sorted(list(zipped), key=lambda tup: abs(tup[1]), reverse=True) $ coef
df_positive = data[data['SA']>0].copy() $ df_positive.sort_values(by='Likes',ascending=False,inplace=True) $ df_positive.head()
tweets_kyoto_filter.reset_index(inplace=True)
df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])
popt_axial_chord_saddle, pcov_axial_chord_saddle = fit(d_axial_chord_saddle)
def _weekday(date): $     return datetime.strptime(date, DATETIME_PARSE_PATTERN).weekday() $ weekday2 = udf(_weekday, IntegerType()) $
df_img_algo.head()
print(newdf['Area'].unique()) $ print(newdf['Year'].unique()) $ print(newdf['Variable Name'].unique())
df[df.amount_initial == '$0.00'].groupby(['fund', 'appeal'])['donor_id'].count()
pct.head()
Fireworks['Complaint Type'].groupby(by= Fireworks.index.date).count().sort_values(ascending= False).head(1)
import matplotlib.pyplot as plt $ plt.style.use('ggplot')
active_ordered = ordered_df.loc[~churned_ord] $
oil_prices = joined[['date', 'dcoilwtico']].copy().drop_duplicates(['date']).reset_index(drop=True) $ oil_prices['oil_30_day_change'] = oil_prices['dcoilwtico'].pct_change(periods=30).fillna(method='bfill') $ oil_prices[['date', 'oil_30_day_change']].plot()
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"').converted.sum() $ convert_new = df2.query('landing_page == "new_page"').converted.sum() $ n_old = df2.query('landing_page == "old_page"').converted.count() $ n_new = df2.query('landing_page == "new_page"').converted.count()
df.titles.shape
capa2017offshore.head()
with tb.open_file(filename='data/file1.h5', mode='w') as f: $     f.create_array(where='/',  name='array1', obj=[0, 1, 2, 3])
tweet_archive_clean.timestamp.head()
null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size) $ plt.hist(null_vals) $ plt.axvline(x=-0.001576, color='r')
print(rscale, numpeo) # Making sure the list compresion and the list declaration worked
print(type(number)) $ character_factor = str(number) $ print(type(character_factor)) $
val_utf8.decode('utf-8')
chinadata.plot?
month.columns = ['MONTH_'+str(col) for col in month.columns]
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']] $ df_new['country'].astype(str).value_counts()
txns[txns['token']=='DAI'].amount.apply(lambda x: np.clip(np.log(x), -5,15)).hist()
returns, positions, transactions, gross_lev = pf.utils.extract_rets_pos_txn_from_zipline(results) $ pf.plot_drawdown_periods(returns, top=5).set_xlabel('Date')
trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt) $ val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt) $ md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)
print(metrics.classification_report(y_test, y_pred_class))
le_data_all.index.levels[0]
train.pivot_table(values = 'Fare', index = 'Age_bin', aggfunc=np.mean)
tc = TwitterClient()
%matplotlib inline $ %reload_ext autoreload $ %autoreload 2
test.isnull().sum()
twitter_archive_master[twitter_archive_master['rating_denominator']==50]['full_text']
niners = nfl[ (nfl["HomeTeam"] == 'SF') | (nfl["AwayTeam"] == 'SF') ]
final_annotations_df.young_present.value_counts()
train['is_holiday'] = train['start_timestamp'].map(lambda x: 1 if str(x.date()) in us_holidays else 0) $ test['is_holiday'] = test['start_timestamp'].map(lambda x: 1 if str(x.date()) in us_holidays else 0)
tsne_input = word_vectors.drop(spacy.en.STOPWORDS, errors=u'ignore') $ tsne_input = tsne_input.head(5000)
print(automl.show_models())
status_keys = list(status._json.keys()) $ len(status_keys)
from bs4 import BeautifulSoup as bs $ import requests $ from splinter import Browser $ import pandas as pd
typesub2017['Solar'] = typesub2017['Solar'].multiply(4) $ typesub2017['Wind Offshore'] = typesub2017['Wind Offshore'].multiply(4) $ typesub2017['Wind Onshore'] = typesub2017['Wind Onshore'].multiply(4)
df_members['bd_c'] = pd.cut( df_members['bd'] , age_bins, labels=age_groups)
fb = cb.organization('facebook')
min_train, max_train = pd_train_filtered['unit_sales'].min(), pd_train_filtered['unit_sales'].max()
import pandas as pd $ autos = pd.read_csv("autos.csv", encoding="Latin-1")
df_new[['c1','c2','c3']] = pd.get_dummies(df_new['country'])
descrip_ct =  pd.crosstab(combined_df5['vo_propdescrip'],combined_df5['bin_label']) $ descrip_ct['neg_pctg']=descrip_ct[1]/(descrip_ct[0]+descrip_ct[1])*100 $ descrip_ct.sort_values('neg_pctg',ascending=False)
obj
filepath = os.path.join('input', 'input_plant-list_NO.csv') $ data_NO = pd.read_csv(filepath, encoding='utf-8', header=0, index_col=None) $ data_NO.head()
high_null_columns = [c for c in df_train.columns if df_train[c].count()<len(df_train) * 0.5] $ print ("total high missing vars = "+str(len(high_null_columns)))
df2_treatmentconverted = df2.query('group=="treatment"').converted.mean() $ df2_treatmentconverted
!cat crossref-by-doi/*.json | jq -r .message.DOI | grep ';' | wc -l
bigram_model = prep.get_bigram_model(from_scratch=False):
df2[df2.user_id.duplicated() == True].index
test = pd.read_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/ratings.csv', sep = ',', $                    encoding='utf-8', low_memory=False)
bbc_df = constructDF("@BBC") $ display(constructDF("@BBC").head())
datelist = pd.date_range(start=pd.datetime.today(), periods=n, normalize=True).tolist() $ my_data2 = pd.DataFrame({'date': datelist, 'input1': x, 'input2': z, 'input3': u1, 'input4': u2, 'target': y}) $ my_data2 = my_data2.drop(0) $ my_data2.head(5)
DateTimeFeature(train_user,"date_first_booking", '%Y-%m-%d')
np.count_nonzero(nba_df.isnull())
del vol['Volume'] $ vol.head()
cog_simband_times = pd.concat([simband_time_df, all_test_times_dates], axis = 1)
df_birth.boxplot(column='population', by = 'Continent') $ plt.show()
df_prep10 = df_prep(df10) $ df_prep10_ = pd.DataFrame({'date':df_prep10.index, 'values':df_prep10.values}, index=pd.to_datetime(df_prep10.index))
for row in nocachedf.itertuples(): $     print(row)
df.isnull().any()
model.predict_proba(np.array([2,50]))
extract_all.app_id_short.unique()
contribs.groupby("committee_name")
results = session.query(Measurement.tobs).all() $ tobs_values = list(np.ravel(results)) $ tobs_values
bigdata = ["Big Data Class", "Is Really Fun", "I Love Python"] $ print bigdata[1:]
c = watershed.unary_union.centroid # GeoPandas heavy lifting $ m = folium.Map(location=[c.y, c.x], tiles='CartoDB positron', zoom_start=12)
test_df.columns[test_df.isnull().any()].tolist()
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)]) $ print(old_page_converted)
split_date = '2017-06-01' $ window_len = 10 $ norm_cols = ['trans','volatil', 'transpm']
valid_scores = valid_scores[valid_scores['home team 41 game win%'].notnull() & $                 valid_scores['away team 41 game win%'].notnull()] $ print(valid_scores.shape) $ valid_scores.head()
zipped = list(zip(list_keys, list_values)) $ print(zipped) $ data = dict(zipped) $ df = pd.DataFrame(data) $ print(df)
df2['intercept'] = 1 $ df2['ab_page'] = np.where(df2['group'] == 'treatment', 1,0) $ df2.head()
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == 1').shape[0] $ convert_new = df2.query('landing_page == "new_page" and converted == 1').shape[0] $ n_old = df2.query('group == "control"').shape[0] $ n_new = df2.query('group == "treatment"').shape[0]
import _pickle as cPickle $ with open('tuned_crf_classifier.pkl', 'rb') as fid: $     crf = cPickle.load(fid)
df_transactions['amount_per_day'].unique()
temp_df.groupby('reorder_interval_group')['Order_Qty'].var()
hashtags["hashtag"].str.lower().value_counts()
stemmer = PorterStemmer()
prcp_df.describe()
donations = pd.read_csv('../input/Donations.csv') $ donations.columns
hours = bikes.groupby('hour_of_day').agg('count') $ hours['hour'] = hours.index $ hours.start.plot() $
cleaned_asf_people_human_df_saved = non_blocking_df_save_or_load( $     cleaned_asf_people_human_df, $     "{0}/human_data_cleaned/asf_people_cleaned".format(fs_prefix)) 
import matplotlib $ matplotlib.style.use('ggplot')
weather_yvr_dt.plot(subplots=True, figsize=(10, 10));
df_model = pd.concat([df_h1b_ft_US_Y.pw_1,df_sector,df_subsector,df_industry,df_decision_days,df_employment_length_days,df_states],axis=1)
logging.info("Connecting to mediaflux.") $ con.open() $ result = con.execute("server.version")
reg = sm.Logit(df_joined['converted'], df_joined[['intercept', 'US', 'UK']]) $ reg_fit = reg.fit() $ reg_fit.summary()
score_variable = 'correct.answers'
df_delta = df[df['Difference'] >= pd.Timedelta(seconds=1)].dropna()
frequent_authors.head()
vocab = vectorizer.get_feature_names() $ print(vocab)
data.resample('T').mean().plot()
control_conversion = df2[df2['group'] == 'control']['converted'].mean() $ print(control_conversion) $ control_rounded = round(control_conversion, 4) $ print(control_rounded)
datetime.now().time()
y_test_over[k150_bets_over].sum()
y2 = tweets2['handle'].map(lambda x: 1 if x == 'mattjpfmcdonald' else 0).values $ print max(pd.Series(y2).value_counts(normalize=True))
week10 = week9.rename(columns={70:'70'}) $ stocks = stocks.rename(columns={'Week 9':'Week 10','63':'70'}) $ week10 = pd.merge(stocks,week10,on=['70','Tickers']) $ week10.drop_duplicates(subset='Link',inplace=True)
train.business_day = train.business_day.map(lambda x: 1 if x == True else 0) $ train.holiday = train.holiday.map(lambda x: 1 if x == True else 0)
tweets_possible_bots = tweets.query("snsuserid.isin(@possible_bots.snsuserid.values)") $ tweets_possible_bots.shape
containers[0].find("li", {"class":"paid-amount"}).span.contents[-1].split()[2]
tweets_df["created_at"]
keepColumns = [0,2,3,7,8,11,12] $ sites = sites.iloc[:,keepColumns] $ sites.columns
columns = ['SchoolHoliday', 'StateHoliday', 'Promo']
vocabulary_expression['component_4'].sort_values(ascending=False).head(7) $
so[so['score'] >= 10].head()
ls -n data.*
plt.figure(figsize=(10,5)) $ plt.plot(df_cluster.cluster, df_cluster.cluster_error) $ plt.xlabel('Number of cluster') $ plt.ylabel('cluster errors')
rdd.map(lambda x: x**2 + really_large_dataset).collect()
excelDF['Order Date'] = excelDF['Order Date'].apply(lambda x: x.strftime('%Y-%m-%d'))
r=requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=7up7a3-xNp8abz4VJGsq&start_date=2018-07-22')
y_pred_clf =clf.predict(X_test) $ y_train_pred_clf=clf.predict(X_train) $ print("Accuracy of logistic regression classifier on on test set: {:0.5f}".format(clf.score(X_test, y_test)))
print(crypto) $ crypto.to_csv('WSentiment--03.csv', sep=',', encoding='utf-8')
import pandas as pd $ import numpy as np
print(rf_gd.best_params_, rf_gd.best_score_)
autos["vehicle_type"].unique() $
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0]
p_control = df2.query("group == 'control'")['converted'].mean() $ p_control
d.tz
dset
joined_patient = patients \ $                  .join(observations, 'patient_id', 'left_outer') \ $                  .join(conditions, ['patient_id', 'year', 'month'], 'left_outer') \ $                  .where(col('year').isNotNull())
len(topUserItemDocs['item_id'].unique())
Growth_Rate_1000BC_to_1000AD = $ Growth_Rate_1400_to_1600 = $ Growth_Rate_1900_to_1950 =
cnt = 1 $ while cnt < 100: $     if (cnt % 10) == 0: $         print(cnt) $     cnt = cnt + 1 $
y_pred = model.predict(x_test, batch_size=1024, verbose=1)
s1[:3].dot(s2)
maxTemp = beirut['Max TemperatureC'].plot(grid=True, figsize=(10,5), title="Daily Maximum Temperature in Beirut, 2015") $ maxTemp.set(xlabel="", ylabel="Temperature, C")
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt
PTrueNegative = (PNegativeTrue * PTrue) / PNegative $ "%.2f" % (PTrueNegative * 100) + '%'
scaler = preprocessing.MinMaxScaler()
energy_cpi_with_id.loc[:,'Frequency'] = energy_cpi_with_id.loc[:,'Frequency'].map(lambda i: i.split(':')[0]) $ energy_cpi_with_id
from datetime import datetime
tweets_df.describe()
ax1.set_xticks([5, 15, 25, 35, 45]) $ ax1.set_title('This is a placeholder Plot Title') $ ax1.set_xlabel('Values of X') $ ax1.set_ylabel('Values of Y') $ f
z_score, p_value = sm.stats.proportions_ztest(count=[convert_old, convert_new],nobs=[n_old, n_new], alternative='smaller') $ print(z_score, p_value)
r = requests.post('https://stream.twitter.com/1.1/statuses/filter.json', $                  params = {'track': '#data'}, $                  auth=auth, $                  stream=True) # important
control_diff = (df2[df2.group == 'control'].converted).mean()
n = tweets_df.favorite_count.hist() $ n.set(yscale="log")
df.drop(df['lang']!='en',inplace = True)
for col in b_list.columns: $     print(f'{col}...{len(b_list[col].unique())}')
df_selparams.to_csv('../../data/raw/booth_sc.csv')
df.columns
df2['intercept'] = 1 $ df2[['control', 'treatment']] = pd.get_dummies(df['group']) $ df2['ab_page'] = df['treatment'] $ df2.drop(['control', 'treatment'], axis = 1)
dfa = pd.read_excel(open('/Users/taposh/Documents/python_boot_camp/2017_python_teaching/data/M3C.xls','rb'), sheetname='M3Year')
autos_p['odometer_km'].value_counts().sort_index(ascending = False)
parties = df[df['Descriptor'] == 'Loud Music/Party'] $
from biopandas.mol2 import PandasMol2 $ pmol = PandasMol2().read_mol2('./data/1b5e_1.mol2')
df=data $ print(df.isnull().sum())
pricePublication = list(zip(*it_df.finalPrice.apply(getfinalPriceANDpublicationDate))) $ it_df["price"] = pricePublication[0] $ it_df["publicationDate"] = pricePublication[1]
prob1_kNN100 = pd.Series(x[1] for x in prob_kNN100) $ Results_kNN100 = pd.DataFrame({'ID': Test.index, 'Approved': prob1_kNN100})
merged.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount" , ascending=False)
treatment_notnew = df.query('group == "treatment" & landing_page !="new_page"').nunique()['user_id'] $ nottreatment_new = df.query('group != "treatment" & landing_page =="new_page"').nunique()['user_id'] $ diff = treatment_notnew + treatment_notnew $ prop = diff/df.nunique()['user_id'] $ print("Looks like {},representing {:2f} % of our entire dataset, times new_page and treatment don't line up!".format(diff,prop))
df2 = get_odm2rest_resultvalues_df('Phosphate', export_df=True)
!wget --load-cookies /tmp/cookies.txt "https://drive.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?id=1LAiELvs_FQhZuldX7JPMutfX00NzhonF&export=download' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1LAiELvs_FQhZuldX7JPMutfX00NzhonF" -O train.zip && rm -rf /tmp/cookies.txt
data['Created Date'] = pd.to_datetime(data['Created Date'], errors='coerce') $ data['Created Year'] = data['Created Date'].dt.year
df['DAY_OF_WEEK'] = df['DATE'].dt.dayofweek
add_basic_features(test_df)
trump_df['text'] = new_trump_df
cercanasAfuerteApacheEntre100Y125mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 100) & (cercanasAfuerteApache['surface_total_in_m2'] < 125)] $ cercanasAfuerteApacheEntre100Y125mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre100Y125mts.apply(descripcionDistancia2, axis = 1) $ cercanasAfuerteApacheEntre100Y125mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
sc = SparkContext()
items.head()
stationCount = func.count(Measurement.station) $ session.query(Measurement.station, stationCount).group_by(Measurement.station).order_by(stationCount.desc()).all() $
tweets_clean.index
autos["price"].value_counts().sort_index(ascending=False)
df_mes = df_mes[df_mes['trip_distance']>0] $ df_mes.shape[0]
data_2018
df[df['Complaint Type'] == 'Homeless Encampment']['Unique Key'].resample('M').count().plot()
df3 = df2.set_index('user_id').join(df_countries.set_index('user_id')) $ df3.head()
archive_copy.expanded_urls.isnull().sum()
odds = vis["Points"].plot(kind = "bar", figsize = (25, 5), title = "Total FIFA Points by Country", rot = 90, legend = True) $ odds.set_ylabel("FIFA Official Ranking Points", fontsize = 15) $ plt.show()
x = search.cv_results_["param_max_depth"].data $ score = search.cv_results_["mean_test_score"] $ yerr = search.cv_results_["std_test_score"] $ fig, ax = plt.subplots(figsize=(12,8)) $ ax.errorbar(x, score, yerr = yerr);
most_active_station = record[0][0] $ most_active_station_name = session.query(Station.name).filter(Station.station == record[0][0]).first()[0] $ most_active_station_name
import pandas as pd $ import numpy as np $ import re $ from IPython.display import display
old_page_converted= np.random.binomial(1, p_old, n_old)
df_AA=df_A.append(s) $ print(df_AA) $ df_AA.index
grouped_dpt.last() # last row of each group 
df_converted = df.converted.mean() $ print("The proportion of users converted is {}%.".format(round(df_converted * 100)))
print('Original Grocery List:\n', groceries) $ groceries['eggs'] = 2 $ print() $ print('Modified Grocery List:\n', groceries)
mean_mileages = pd.Series(top_brand_mean_mileage).sort_values(ascending=False) $ mean_prices = pd.Series(top_brand_mean_price).sort_values(ascending=False) $ brand_info = pd.DataFrame(mean_mileages,columns=['mean_mileage']) $ brand_info['mean_price'] = mean_prices $ brand_info
plot_autocorrelation(series=dr_num_new_patients.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr number of new patients')) $ plot_autocorrelation(series=dr_num_existing_patients.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr number of existing patients'))
model_info = kipoi_veff.ModelInfoExtractor(model, Dataloader) $ vcf_to_region = kipoi_veff.SnvCenteredRg(model_info)
jobs.head()
b.find_by_xpath('//*[@id="day-section"]/div/div[3]/div[8]/div[2]/ul/li[6]/button').click()
tesla['nlp_text'] = tesla.text.apply(lambda x: tokenizer.tokenize(x.lower())) $ tesla.nlp_text = tesla.nlp_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x]) $ tesla.nlp_text = tesla.nlp_text.apply(lambda x: ' '.join(x))
autos["brand"].value_counts()
df2.converted.mean()
df3_new.groupby('country')['converted'].mean()
display('x', 'y', 'pd.concat([x, y], ignore_index=True)')
food=pd.read_csv("en.openfoodfacts.org.products.tsv", sep="\t", nrows=20000)
import sys $ print(sys.version)
tier1 = getcrimesby_tier(store1,Quality) $ tier1_df = tier1.copy()
cs.describe()
least = pd.DataFrame(data.groupby('tasker_id').hired.sum()) $ least.loc[least['hired']==0]
data = engine.execute("SELECT * FROM measurement")._metadata.keys $ print(data) $ data2 = engine.execute("SELECT * FROM station")._metadata.keys $ print(data2)
test.info()
pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])
processed_tweets['processed_tweet_features'] = processed_tweets.tweetText.apply(extract_features)
df_new = df.query('landing_page == "new_page"') $ p_new = df_new[df_new['converted'] == 1]['converted'].count() / df_new[['converted']].count() $ print (p_new)
crime_wea=pd.read_csv('data/crime/crimes_weather.csv')
clinton_tweetDeck = clinton_df[clinton_df['source'] == 'TweetDeck'] $ clinton_webClint = clinton_df[clinton_df['source'] == 'Twitter Web Client'] $
nr_rev ='the masala was great very yummy indeed, i wish the naan was not cold but the paneer and garlic rice' $ vectorizer.transform([nr_rev]) $ M_NB_model.predict_proba(vectorizer.transform([nr_rev]))
from scipy import stats
sb.distplot(cats_df['age at death'].dropna()) $ plt.show()
lg=sm.Logit(df2['converted'], df2[['intercept', 'ab_page' ]]) $ results = lg.fit() $ results.summary()
logit = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']]) $ result=logit.fit()
def getstatus(x): $     for K in disposition_dict.keys(): $         if x in disposition_dict[K]: $             return K $ calls_df["call_type"]=calls_df["status"].apply(lambda x:getstatus(x))
print("Number of Relationships in Mobile ATT&CK") $ print(len(all_mobile['relationships'])) $ df = all_mobile['relationships'] $ df = json_normalize(df) $ df.reindex(['object id','relationship', 'relationship_description','source_object', 'target_object'], axis=1)[0:5]
df_only_headline = df_tweets[(df_tweets["sub-headline"].str.len() == 0) & (df_tweets["tags"].apply( $     lambda x: len(set(["misc", "learning", "tool", "dataviz", "research"]).intersection(set(x))) == 0 $ ))] $ headline_date_count = df_only_headline.groupby(["headline", "date"]).size() $ headline_date_count[headline_date_count > 1]
price2017.dtypes
df.num_comments.max()
jobs = find_rome_dataset_by_name(rome_data, 'referentiel_appellation') $ new_jobs = set(jobs.new.code_ogr) - set(jobs.old.code_ogr) $ obsolete_jobs = set(jobs.old.code_ogr) - set(jobs.new.code_ogr) $ stable_jobs = set(jobs.new.code_ogr) & set(jobs.old.code_ogr) $ matplotlib_venn.venn2((len(obsolete_jobs), len(new_jobs), len(stable_jobs)), (OLD_VERSION, NEW_VERSION));
tweets_clean.drop(index = 754011816964026368, inplace = True) $ images_clean.drop(index = 754011816964026368, inplace = True)
fullDf[pd.isnull(fullDf.level)].location.value_counts()
inputPath2 = "graphdata/imagelabels.csv" $ edges = sqlContext.read.options(header='true', inferSchema='true').csv(inputPath2) $ edges.show(5)
df2[df2['group'] == 'control']['converted'].mean() #going directly by mean unlike below step
df.shape
post_id  ##output is an object
no_newpage_treatment.isnull().sum().sum()
twitter_archive_master[twitter_archive_master.rating_numerator > 15].head()
SEA = SEA[["Date", "# Seats", "Seating Area", "Section", "Row", "Total Price"]]
van_final.apply(lambda x: sum(x.isna()))
pd.concat([msftA[:3], aaplA[:3]], ignore_index=True)
df[(df.company == True) & (df.other == True)]
maint['comp'].value_counts().plot(kind='bar') $ plt.ylabel('Number of replacements') $ plt.show()
infinity.head(10)
norm.ppf(1-(0.05/2)) $
p_values = (p_diffs > obs_diff).mean() $ p_values
tweet1.created_at
aussie_search = api.search(q='%23Aussie') $ len(aussie_search)
autos.head()
df.columns
dsDir = "data/" $ bus = pd.read_csv("data/businesses.csv", encoding='ISO-8859-1') $ ins = pd.read_csv("data/inspections.csv") $ vio = pd.read_csv("data/violations.csv")
train = pickle.load( open("../Data/train.csv", "rb") )
k_fold = KFold(n_splits=10, shuffle=True, random_state=0) $ score = cross_val_score(classifier, xy, z, cv=k_fold, n_jobs=1, scoring='accuracy') $ print (score) $ round(np.mean(score)*100, 2)
a = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 9]]) $ c = np.array([[1], [2], [3]]) $ print(a) $ print(c)
grades_cat = df['Grades'].astype(dtype= 'category') # Series object $ grades_cat
h = Hart85() $ h.train(mains,cols=[('power','active')]) $
Inspection_duplicates = data_FCInspevnt_latest.groupby(['brkey'])[['Inspection_number']].sum() $ Inspection_duplicates = Inspection_duplicates.loc[Inspection_duplicates['Inspection_number'] > 1] $ Inspection_duplicates
twitter_archive_clean['dog_type'] = twitter_archive_clean['text'].str.extract('(puppo|pupper|floofer|doggo)', expand=True)
model2=sm.Logit(df_new['converted'],df_new[['intercept','treatment','CA','UK']]) $ result_2=model2.fit() $ result_2.summary()
temperature = session.query(Measurement.station, Measurement.date, Measurement.tobs).\ $     filter(Measurement.station == most_active).\ $     filter(Measurement.date > last_year).\ $     order_by(Measurement.date).all()
tweets['isRetweet'].value_counts(dropna=False) $ tweets['truncated'].value_counts(dropna=False)
display('df1a', 'df2a', "pd.merge(df1a, df2a, left_index=True, right_index=True)")
joined_patient.corr('avg_ldl', 'max_triglycerides')
gender_mappings = get_mappings("gender", [1, 2])
with open('image-predictions.tsv', 'w', encoding='utf-8') as file: $     file.write(str(r.text))
subwaydf.iloc[117329:117333] # this high & correseponding low number seems to be because entries&exits messes us.
store = pystore.store('mydatastore') $ store
p_diffs=[] $ for _ in range(10000): $     new_page_converted=np.random.binomial(n_new,p_new) $     old_page_converted=np.random.binomial(n_old,p_old) $     p_diffs.append((new_page_converted/n_new)-(old_page_converted/n_old))
tlen = pd.Series(data['len'].values, index=data['Date']) $ tfav = pd.Series(data['Likes'].values, index=data['Date']) $ tret = pd.Series(data['RTs'].values, index=data['Date'])
pixel, charge = reader.select_columns(['pixel', 'charge']) $ charge = charge.values # Convert from Pandas Series to numpy array $ pixel = pixel.values # Convert from Pandas Series to numpy array $ print('charge = ', charge) $ print('pixel = ', pixel)
df2.query("landing_page=='new_page'").count()[0]/len(df2)
comunas
sentiments_pd.to_csv("Twitter_News_Mood.csv", index=False)
df.T
df2['WT'] = (df2['help_time'] - df2['entry_time']).astype('timedelta64[s]').astype(int) $ df2['HT'] = (df2['exit_time'] - df2['help_time']).astype('timedelta64[s]').astype(int)
from sklearn.metrics import log_loss $ log_loss(y_test, yhat_prob)
logit_mod2=sm.Logit(df_new['converted'],df_new[['intercept','UK','US']]) $ results2=logit_mod2.fit() $ results2.summary2()
df = df.drop(['Unnamed: 0','Unnamed: 1'],axis=1) $ df.head()
from PutIDFiles import put_team_ids, put_game_ids, put_all_matchups $ Teams = put_team_ids() $ Teams.head() $
from nltk.sentiment.vader import SentimentIntensityAnalyzer
state_party_df.shape
plt.plot(running_total) $ plt.title('Simulation of betting strategy for 2018 season') $ plt.ylabel('Total bankroll') $ plt.xlabel('# of games bet');
from scipy.stats import norm $ print(norm.cdf(z_score)) #significance of z-score $ print(norm.ppf(1-(0.05/2))) #critical value at 95% confidence
df_new[['US', 'CA']] = pd.get_dummies(df_new['country'])[['US', 'CA']] $ df_new.head()
autos['odometer_km'].value_counts().sort_index()
for i in list(data): $     if data[i].dtype == "int64" or data[i].dtype == "float64": $         if(data[i].max()!=0): $             data[i] = (data[i]-data[i].min())/(data[i].max()-data[i].min()) $ data = data.fillna(0)
word_count = Counter() $ for sent in df_links[df_links['link.domain'] == 'amzn.to']['tweet.text']: $     word_count.update([w for w in sent.split() if w not in stopwords.words('English')]) $ word_count.most_common(10)
v = voters[['E1_110816', 'E2_060716', 'E3_110414','E4_060314', 'E5_110612', 'E6_060512']] $ v.columns = pd.MultiIndex.from_product([v.columns, ['vote']]) $ v = v.stack(0) $ v.index.set_names('election', level=1, inplace=True)
test_data = data1[data1['dataset'] == 'test'] $ test_data = test_data.drop('dataset', axis = 1) $ print('Number of Country destination (Checking):', test_data['country_destination'].unique()) $ test_data.head(3)
merged1.shape
lda_tf.save(os.path.join(outputs, 'model_tf.lda')) $ corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lda_tf.mm'), corpus_lda_tf) $ lda_tfidf.save(os.path.join(outputs, 'model_tfidf.lda')) $ corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lda_tfidf.mm'), corpus_lda_tfidf)
tweet_archive_clean['new'] = tweet_archive_clean.text.str.extract('(?P<new>(\d+).(\d+)+/\d+)', expand=False)['new']
os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $ Generate_code_to_generate_PDF_files_from_TXT()
Base.classes.keys() $
model.evaluate_word_pairs(test_data_dir + 'wordsim353.tsv')
baseball_newind.loc['wickmbo012007']
pickle.dump(selfharmm_topic_names_df, open('iteration1_files/epoch3/selfharmm_topic_names_df', 'wb'))
pgh_311_data['REQUEST_TYPE'].value_counts(ascending=True).plot.barh(figsize=(10,50))
logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','US','CA','US_ab','CA_ab']]) $ results = logit_mod.fit() $ results.summary()
sqlContext.sql("select * from pcs where count > 1").show()
iso_gdf.intersects(iso_gdf_2)
data[["Class","V1"]].groupby(["Class"]).count()
%matplotlib inline $ pandas_df.plot.bar()
data.info()
print('Best score for data:', svm_classifier.best_score_) $ print('Best C:',svm_classifier.best_estimator_.C) $ print('Best Kernel:',svm_classifier.best_estimator_.kernel) $ print('Best Gamma:',svm_classifier.best_estimator_.gamma)
hdf5_file.close
pd.cut(df['CTYNAME'],bins= 5,include_lowest= True).sort_values() # series object
Y=dataset.iloc[:,2:].values
demand = model.get_parameter(par='demand') $ list(set(demand.commodity)) $ demand.head()
print "The probability of an individual converting regardless of the page they received is {0:.4f}%".format(float((treat_con[0] + control_con[0]))/df2.shape[0]*100)
cond_1 = (df['group'] == 'treatment') & (df['landing_page'] == 'new_page'); $ cond_2 = (df['group'] == 'control') & (df['landing_page'] == 'old_page'); $ df2 = pd.DataFrame(df[cond_1|cond_2]);
row_count = df.shape[0] $ print ("The number of rows in dataset is {}".format(row_count))
df = df[df["status"] == "active"] $ df.drop(["status"], axis = 1, inplace = True) $ df.shape
df2['intercept'] = 1 $ df2[['new_page', 'old_page']] = pd.get_dummies(df['landing_page']) $ df2['ab_page'] = pd.get_dummies(df['group']) ['treatment'] $ df2.head()
for col in giss_temp.columns: $     giss_temp.loc[:, col] = giss_temp[col].astype(np.float32)
df.loc[df['edition']=='PunditFact'].head()
test.shape
%%bash $ mkdir sample $ gsutil cp "gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*" sample/train.csv $ gsutil cp "gs://$BUCKET/taxifare/ch4/taxi_preproc/valid.csv-00000-of-*" sample/valid.csv
top20 = autos['brand'].value_counts().head(20).index
len(STD_customer_order_intervals)
userArtistDF.agg(min("userID"),max("userID")).show()
tweets_predictions_all = pd.merge(tweets_prediction, popularity_clean, how='left', on='tweet_id')
rng2 = pd.date_range(start = '07/01/2017', end = '07/21/2017', freq = usb) $ rng2[0:3]
y.end_time # Tells us the end of the period
ts.asfreq(pd.tseries.offsets.BDay())
models_series = pd.Series(represented_models) $ models_series
df_twitter_archive.rating_denominator.value_counts()
pd.datetime(2014,8,1)
average_compound = sentiments_pd.groupby("User")["Compound"].mean() $ print(average_compound) $ print ("") $ print(len(average_compound))
data['StdW'] = data[odds_W].std(axis=1) $ data['StdL'] = data[odds_L].std(axis=1) $ display(data.StdW.head()) $ display(data.StdL.head())
c_ctrl = df2.query('group == "control"')['converted'].mean() $ c_ctrl
df2 = df2.drop(1899) $ df2.info()
conn = 'mongodb://localhost:27017' $ client = pymongo.MongoClient(conn) $
twitter_archive_full.to_csv(data_folder+'twitter_archive_master.csv', index=False)
df = df[df['Views-PercentChange'] < 100]
quarterly_index.tail()
pd.DataFrame(data).head()
ax=datatmp.plot(kind="box",title="Graph 2-Water consumption in Austin by Consumer Class",sharex=True) $ plt.ylabel("Gallons") $ plt.xlabel("Consumer Class") $ plt.show()
s.loc['b':'d']
utils.serialize_data(data)
convert_p_new = convert_p_old = probability $ print ("The convert rate for  p_new  under the null is: {:.4f}".format(convert_p_new))
autos['price'].value_counts().sort_index(ascending=False).head(50)
raw_data.drop(null_desc.index, axis=0, inplace=True)
cs = Custom_Seq()
providers_schedules.head()
cityID =  '5a110d312052166f' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Francisco.append(tweet) 
ins_named = pd.merge(ins,bus.loc[:,['business_id','name','address']],right_on="business_id",left_on="business_id",how = 'left')
release_titles = [r["title"] for r in releases] $ print("\nALL TITLES:") $ for t in release_titles: $         print(t)
sns.regplot(x=final["BTC Volume"], y=final['Crypto Compound'], fit_reg=False) $ sns.regplot(x=final["BTC Volume"], y=final['Crypto Negative'], fit_reg=False, color = 'r') $ sns.regplot(x=final["BTC Volume"], y=final['Crypto Positive'], fit_reg=False, color = 'g')
df_users_5.head()
dfTemp=transactions.merge(users, how='inner',left_on='UserID',right_on='UserID') $ dfTemp
type(target_pf['date'].iloc[0])
mtcars.head()
def ngrams(input_list,n): $     ngrams = list(zip(*[input_list[i:] for i in range(n)])) $     return [*map('_'.join, ngrams)]
tweet_archive_clean.loc[2436]
list(rain_period.index)
preds = nb.predict(X_test)
df
most_common_registered_addresses_slps = active_companies[active_companies['company_type'] == 'Limited Partnership for Scotland'].groupby(['first_and_postcode'])['CompanyNumber']\ $     .agg(lambda x: len(x.unique())).sort_values(ascending=False).head(20) $ most_common_registered_addresses_slps.to_excel('data/for_further_investigation/top_20_slp_addresses.xlsx') $ most_common_registered_addresses_slps
oembeds = Parallel(n_jobs=8)(delayed(collect_oembed)(tid) for tid in tqdm_notebook(df_tweets.index.tolist()))
cohort_retention_df.fillna(0,inplace=True)
print (df2.set_index('user_id').index.get_duplicates())
model.add(Dense(1024, activation='relu')) $ model.add(Dropout(rate=0.25))
print(norm.ppf(1-(0.05)))
merge_DA_imb_power_df.to_excel(data_folder_path + '/temp/merged_DA_imb_power_df.xlsx', index = False)
df_twitter_copy['retweet_count'] = df_twitter_copy['tweet_id'].map(df_twitter_extract_copy.set_index('tweet_id')['retweet_count']) $ df_twitter_copy['favorite_count'] = df_twitter_copy['tweet_id'].map(df_twitter_extract_copy.set_index('tweet_id')['favorite_count'])
van15_fin['stiki_percent']=van15_fin['stiki_mean']<0.1
from sklearn.linear_model import LogisticRegression $ model = LogisticRegression() $ model = model.fit(X, y) $ model.score(X, y) 
corM['is_fake']
new_page_converted = np.random.binomial(1,Pnew,Nnew) $ new_page_converted
dr = hours[hours['Specialty'] == 'doctor'] $ ther = hours[hours['Specialty'] == 'therapist'] $ RNPA = hours[hours['Specialty'] == 'RN/PA']
df2.drop_duplicates(subset='user_id', keep='first', inplace=True) $ df2.set_index('user_id').index.get_duplicates()
psy_hx.drop_duplicates(subset=None, keep='first', inplace=True) $ psy_hx.shape
measurement_df.groupby(['station']).count().sort_values(by='id', ascending=False) $
train.tail()
0.1196
sorted(airlines, key=lambda x:x[0][-1])
mentions_count = mentions.value_counts().head(10) $ mentions_count
trimmed = prune_below_degree(tweetnet, 100) $ nx.info(trimmed)
print(users['Registered'].dtype) $ print(users['Cancelled'].dtype) $ print(sessions['SessionDate'].dtype) $ print(transactions['TransactionDate'].dtype)
old_page_converted = np.random.binomial(1,p_old, n_old)
df_archive.sample(15)
baseball.loc[89521, "player"]
df['Funding %'] = df['pledged'] / df['goal']
to_be_predicted_Day3 = 56.17900512 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
np.random.shuffle(whitelistedArticles) $ np.random.shuffle(whitelistedArticles)
k1 = data[['cust_id','total_spend']].groupby(['cust_id']).agg('count').reset_index() $ k1.columns = ['cust_id','cust_id_count'] $ train = train.merge(k1,on=['cust_id'],how='left') $ test = test.merge(k1,on=['cust_id'],how='left')
df_train['totals.pageviews'] = df_train['totals.pageviews'].fillna('0') $ df_test['totals.pageviews'] = df_test['totals.pageviews'].fillna('0')
positive_topic_dataframe.to_csv("positive_topics.csv")
sel = [Measurements.tobs] $ temperature_data = session.query(*sel).\ $     filter(Measurements.date >= last_year_date).\ $     filter(Measurements.station == highest_station).all()
!pip install gensim
old_mean = df2.query('group == "control"').converted.mean() $ new_mean = df2.query('group == "treatment"').converted.mean() $ diffs = new_mean - old_mean $ p_diffs = np.array(p_diffs) $ (p_diffs > diffs).mean()
a1.b=0
autos["price"].value_counts()
lm.rsquared
import statsmodels.api as sm $ logit   = sm.Logit(ab_file2['converted'], ab_file2[['intercept','ab_page']]) # creating logistics for our query $ results = logit.fit()
y = df.cylinders.tolist()
Ralston.loc[:,"T_DELTA"] = Ralston.loc[:,"TMAX"] - Ralston.loc[:,"TMIN"] $ Ralston.head()
b.d
miss_loans=set(loans.id_loan) ^ set(payment_plans.fk_loan) # symmetric difference
import textblob $ import re
start_value = 10000 $ actual_returns = (r.cumsum() + 1) * start_value $ end_value = actual_returns[-1] $ fig = actual_returns.plot(figsize=(10,4), title="Portfolio Vaue ($)") $ print("start_value:\t{}\nend_value:\t{}".format(start_value, end_value))
p_old = p_undernull $ p_old
p0
image_clean.dog_breed.value_counts()
df.state_retail.mean()
import pickle $ with open('new_reddit_topic_comments.pkl', 'rb') as pikkle: $     sub_comments = pickle.load(pikkle)
collection.create_snapshot('snapshot_name') $ collection.list_snapshots()
viz = cdf[['CYLINDERS','ENGINESIZE','CO2EMISSIONS','FUELCONSUMPTION_COMB']] $ viz.hist() $ plt.show()
import dask.dataframe as dd $ import pandas as pd
to_be_predicted_Day1 = 26.40 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
print("{} games in January of 2016".format(len(nba_df.loc[(nba_df.index.year == 2016) & (nba_df.index.month == 1), ]))) $ print("{} games in January of 2017".format(len(nba_df.loc[(nba_df.index.year == 2017) & (nba_df.index.month == 1), ])))
reddit['Date Only'] = reddit['Time'].apply(lambda x:x.date().strftime('%m-%d-%y'))
twitter_archive_master['rating_denominator'][794] = 10 $ twitter_archive_master['rating_numerator'][794] = 14
scenarios_rdd.takeSample(False, 1,0)
twitter_merged_data.plot('stage', 'rating', kind='bar'); $ plt.title('Stage vs Rating Bar Plot') $ plt.xlabel('Stage') $ plt.ylabel('Rating');
dump["country"] = dump["country"].map(coumap) # Applying thee mapping dictionary to the column 
dfX_hist['payout_win']=dfX_hist['final_tote_odds']+1
!ls -alrt
print(df_data.PERIDOOCORRENCIA_TIPO.value_counts()) $ df_data.PERIDOOCORRENCIA_TIPO.hist(bins=[0,1,2,3,4,5])
filename_2018
daily_listings_intrMode.plot(kind='bar', figsize = (20,12))
raw.event.values[0], raw.event.values[25], raw.event.values[17]
classifier = OneVsRestClassifier(LinearSVC(random_state=42)) $ classifier.fit(train_matrix, train_labels_bin) $ y_pred = classifier.predict(test_matrix) $ y_pred
df.head(10)
r2_score(y_,y_pred)
precipitation_df.describe()
sizes = [0.0,] $ for i in range(0,len(newdf)-1): $     size = 0.4 / newdf['Ret_stdev'][i] $     sizes.append(size) $
grouped.describe()
import datetime as dt
df2.query('group=="control" & converted==1').user_id.count() / df2.query('group=="control"').user_id.count()
con = sqlite3.connect('db.sqlite') $ df=pd.read_sql_query("SELECT * from tbl", con) $ con.close() $ df
import matplotlib.pyplot as plt $ plt.boxplot(y) $ plt.show()
fulldf.describe()
merged_df_cut = merged_df.dropna(subset = ["weight1"])
twitter_df.to_csv("news_mood_df.csv", encoding="utf-8")
p_new = df2['converted'].mean() $ print('p_new: ',p_new)
ther_trend = fit_linear_trend(therapist_duration) $ ther_detrended = therapist_duration - ther_trend
def tweet_extend (tweet_id): $     try: $         return api.get_status(tweet_id, tweet_mode='extended')._json['full_text'] $     except: $         return "ERROR" $
s_1 = "SELECT * FROM paudm.photoz_bcnz as bcnz " $ s_2 = "JOIN paudm.cosmos " $ s_3 = "ON bcnz.ref_id = cosmos.paudm_id " $ s = s_1 + s_2 + s_3 $ data = pd.read_sql(s,engine)
conv_old = df2.query('landing_page == "old_page"')['converted'].mean() $ conv_new = df2.query('landing_page == "new_page"')['converted'].mean() $ (p_diffs>(conv_new-conv_old)).mean()
gdp_df=gdp_df.rename(index=str, columns={"TIME_PERIOD": "Year", "All industry total [Detroit-Warren-Dearborn MI (Metropolitan Statistical Area)]": "Detroit GDP"})
pbptweets['count'] = 1
autos_pr = autos_p[autos_p['registration_year'].between(1910, 2016)]
df_schools.head()
X = train[feature_cols] $ y = train.popular
tvec = TfidfVectorizer(max_features = 2000,ngram_range=(1,3)) $ tvec.fit(X_train,y_train)
bg_df2.index # list all the row labels
df2['user_id'].duplicated().sum()
df_schools = pd.read_csv(file1) $ df_students = pd.read_csv(file2)
pd.set_option('display.max_colwidth', 200)  $ pd.set_option('display.max_rows', 50) $ yxe_tweets
df_new['country'].unique()
for columns in DummyDataframe[["Positiv", "Negativ"]]: $     basic_plot_generator(columns, "Graphing Dummy Data" ,DummyDataframe.index, DummyDataframe)
jobs_data.to_pickle('D:/CAPSTONE_NEW/jobs_data_clean.pkl') $ import gc $ gc.collect()
unique_desc=class_merged_hol['description'].unique() $ print (unique_desc)
p_new = len(df2.query('converted == 1'))/len(df2) $ p_new
gnb.fit(X_clf, y_clf)
max_IMDB = scores.IMDB.max() $ max_IMDB
cbg['COUNTYFP'] = cbg['COUNTYFP'].astype(str)
trump['num_punc']= trump['num_punc'].apply(lambda x: np.log(x))
profile_ids
users.head(2) 
archive_df_clean = archive_df.copy()
print(airquality_pivot.index)
X_words.head(1)
appl.head()
dt = DecisionTreeRegressor(max_depth = 10, min_samples_split = 3)
p_diff = new_page_converted/n_new - old_page_converted/n_old $ p_diff
jobs_data['clean_description'].replace(to_replace=r"\s([a-z])\1+", value="", regex=True, inplace=True)
from sklearn.naive_bayes import GaussianNB
for file in os.listdir(PATH): $     if not file.endswith('zip'): $         continue $     !unzip -q -d {PATH} {PATH}{file}
df_archive_clean["retweeted_status_id"] = df_archive_clean["retweeted_status_id"].astype(str) $ row_drop = df_archive_clean[df_archive_clean["retweeted_status_id"]!="nan"]["tweet_id"].index $ df_archive_clean.drop(row_drop, inplace=True) $ df_archive_clean["retweeted_status_id"].replace("nan", np.nan, inplace=True)
rep_rows = rep.index $ rep_row = rep_rows[0] $ print ("elemento a ser retirado: " + str(rep_row)) $
df_enhanced[['dog_name']].groupby(['dog_name'])['dog_name'].size()
clf = decomposition.NMF(n_components=n_topics, random_state=1)
path_to_array1 = '{f1path}:/array1'.format(f1path=os.path.abspath(os.path.join(data_dir, 'file1.h5'))) $ path_to_array2 = '{f2path}:/array2'.format(f2path=os.path.abspath(os.path.join(data_dir, 'file2.h5'))) $ print(path_to_array1) $ print(path_to_array2)
print(mars_hemispheres.prettify())
pd.DataFrame({'features': X.columns, 'LasoCoefficients': lasso.coef_})
controlg = df2.query('group == "control" & converted ==1').user_id.count() $ controlttl = df2.query('group == "control"').user_id.count() $ control_convert = controlg/controlttl $ control_convert
def analyze_sentiment(tweet): $     analysis = TextBlob(clean_tweet(tweet)) $     tweet_sentiment = round(analysis.sentiment.polarity,4) $     return tweet_sentiment
np.identity(5)
p_diffs = np.random.binomial(NewPage, p_new, 10000)/NewPage - np.random.binomial(OldPage, p_old, 10000)/OldPage
ds = tf.data.TFRecordDataset(train_path) $ ds = ds.map(_parse_function) $ ds
cur.description   # Carries info regarding the tables. With index 0 containining headers
merged_df.tail(3)
expectancy_for_least_country = le_data.min(axis=0) $ expectancy_for_least_country
[i for i in train.columns if i not in test.columns],[i for i in test.columns if i not in train.columns]
DT_yhat = DT_model.predict(test_X) $ print("DT Jaccard index: %.2f" % jaccard_similarity_score(test_y, DT_yhat)) $ print("DT F1-score: %.2f" % f1_score(test_y, DT_yhat, average='weighted') )
tweets.to_csv("../output/clean_data_4_18_14_40.csv",encoding="utf-8")
type(linear_predictor)
frame
np.vstack((a, a))[1::,:] # through the end. 
X_train_valid, X_test, y_train_valid, y_test = train_test_split(pm_final.drop('status', axis = 1) $                                                     ,pm_final[['status']], $                                                     test_size=0.20)
classifier_mybag = train_classifier(X_train_mybag, y_train) $ classifier_tfidf = train_classifier(X_train_tfidf, y_train)
df_users_mvp.shape
import pandas as pd $ import numpy as np $ from google.colab import files as filess
energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12) $ plt.xlabel('timestamp', fontsize=12) $ plt.ylabel('load', fontsize=12) $ plt.show()
df[df['converted']==1].shape[0]/df.user_id.nunique()
! tar -xzvf wikipedia_devtst.tgz
import sqlalchemy $ import psycopg2 $ import pandas as pd
air_rsrv.rename(columns={'reserve_visitors': 'air_rsrv_visitors'}, inplace=True) $ hpg_rsrv.rename(columns={'reserve_visitors': 'hpg_rsrv_visitors'}, inplace=True)
members = [] $ dict_temp = year_count(df_ny, 'joined') $ for key in tqdm(year): $     members.append(dict_temp[key]) $ members_count.append(members)
bytes_to_write = combined_preds.to_csv(None).encode() $ fs = s3fs.S3FileSystem() $ with fs.open(filepath, 'wb') as f: $     f.write(bytes_to_write)
import tweepy $ trump_tweets = get_tweets_with_cache("realDonaldTrump", "keys.json") $ trump_df = make_dataframe(trump_tweets)
def twitter_setup(): $     auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) $     auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET) $     api = tweepy.API(auth) $     return api
df_clean.info()
1/np.exp(-0.0149),1/np.exp(-0.0408),np.exp(0.0099)
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.binomial(1, p_new, n_new) $     old_page_converted= np.random.binomial(1, p_old, n_old) $     p_diffs.append(np.average(old_page_converted)-np.average(new_page_converted)) $
results_df = pd.DataFrame(returns) $ results_df.columns = ['requestor','doi','title','abstract_background','journal_name','publication_year','publication_date'] $ results_df.head()
corn.groups
new_dems.shape[0] # this is how many rows we have...
cbg = gpd.read_file('cb_2015_36_bg_500k.shp')
for dataset in combine: $     dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 $ train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)
avg_annual_days_traded = years.median() $ avg_annual_days_traded
pd.Timestamp('2018-01-01') + Hour(3) + (5 * Minute())
df3 = df2.copy() # Take a copy for the new regression approach. $ df3.head()
data.loc[data.floor.isnull(),'floor'] = data['floor*price']/data['price_aprox_usd'] $ del data['floor*price']
def join_df(left, right, left_on, right_on=None, suffix='_y'): $     if right_on is None: right_on = left_on $     return left.merge(right, how='left', left_on=left_on, right_on=right_on, $                       suffixes=("", suffix))
df.head(5)
m =np.hstack((a,b)) $ m=m.reshape(5,4) $ print("m: ", m)
print ("Train Accuracy :: ", accuracy_score(train_dep[response], trained_model_RF.predict(train_ind[features]))) $ print ("Test Accuracy  :: ", accuracy_score(test_dep[response], trained_model_RF.predict(test_ind[features]))) $ print ("Complete Accuracy  :: ", accuracy_score(kick_projects_ip[response], trained_model_RF.predict(kick_projects_ip_scaled_ftrs))) $ print (" Confusion matrix of complete data is", confusion_matrix(kick_projects_ip[response],kick_projects_ip["Pred_state_RF"]))
total_sales[['2015_q1_sales', '2016_q1_sales', 'bottles_total', '2015_q1_sales_bottlenorm']].corr()
mean_mileage = pd.Series(brand_mean_mileage).sort_values(ascending = False) $ mean_price = pd.Series(brands_mean_price).sort_values(ascending = False)
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05))) $
my_query = "SELECT * FROM specimens LIMIT 1" $ my_result = limsquery(my_query) $ first_element = my_result[0] $ print first_element.keys() $
speeches_cleaned['index'] = speeches_cleaned.index
autos['odometer'] = autos['odometer'].str.replace("km","").str.replace(',', '') $ autos['odometer'] = autos['odometer'].astype(int) $ autos.rename({"odometer": "odometer_km"}, axis=1, inplace=True) $ autos["odometer_km"].head()
df2[['control', 'ab_page']] = pd.get_dummies(df2['group'])
breakfastlunchdinner.plot(x="STATION", y=["breakfast", "lunch + brexits", "dinner"], $                           figsize = (20,9), kind="line", rot = 30, fontsize=13, ylim = (0,90000), $                           title="Average Daily Weekday Traffic for Top 10 Stations at B/L/D"); $ plt.xticks([0, 1, 2, 3,4, 5, 6, 7, 8, 9], ['GRND CNTRL-42ST', '34ST-HERALD SQ', '34ST-PENN STA', '14ST-UNION SQ', '23ST', 'TIMES SQ-42ST', '47-50 STS ROCK','86ST', '59 ST COLUMBUS','59ST', ]); $
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, 1-p_new]) $ new_page_converted.mean()
df_ml_55_01.tail(5)
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)
os.getcwd()
grouped.get_group(2011).shape
kd915 = pd.read_csv('/Users/auroraleport/Documents/LePort_git/9_15_full')
df.shape 
pwd
rng
for item in test_data: $     index_to_datetime(item)
train_norm = train_norm.join(train[['duration', 'is_holiday', 'is_weekend', 'is_rushhour'] + dummy_features_test]) $ test_norm = test_norm.join(test[['row_id', 'is_holiday', 'is_weekend', 'is_rushhour'] + dummy_features_test])
model1 = linear_model.LinearRegression() $ model1.fit(x1, y) $ (model1.coef_, model1.intercept_)
df2 = pd.read_csv('ab_edited.csv')
autos = autos[autos['price'].between(1,351000)] $ print(autos['price'].describe()) $
fish
properati['state_name'].value_counts(dropna=False)
trump_tweets = get_tweets_with_cache("realdonaldtrump", key_file) $ print("Number of tweets downloaded:", len(trump_tweets))
autos.info()
print('Change the column Indicator to Indicator_id permanently') $ df.rename(columns = {'Indicator':'Indicator_id'},inplace=True) $ df.head(2)
t_conv = gb.loc['treatment', 1][0] $ t_no_conv = gb.loc['treatment', 0][0] $ p = t_conv/(t_conv + t_no_conv) $ p
archive_clean.info()
from sklearn.decomposition import PCA $ pca = PCA(random_state=100) $ pca.fit(X_train)
iowa.shape
trn_lm[:10]
import pyspark.sql.functions as func $ hashed_test.groupBy().agg(func.min(col('id'))).show()
data.sample(4)
features_rolling_averages = create_exp_weighted_avgs(afl_data, span=10)
week_day_frequency = my_df["week_day"].value_counts() $ ax = plt.subplot(111) $ ax.bar(week_day_frequency.index, week_day_frequency.data) $ plt.show()
import datetime $ data['yyyymm'] = data['Created Date'].apply(lambda x:datetime.datetime.strftime(x,'%Y%m'))
xmlData.rename(columns = { $             'living_area': 'sqm.living_area', $             'lot_area': 'sqm.lot_area', $             'upper_area': 'sqm.upper_area', $             'basement_area': 'sqm.basement_area'}, inplace = True)
multi_index=pd.MultiIndex.from_tuples([(i, j, k) for i , j, k in zip(label_index, $                                                                      community_index, poverty_data['Category'].tolist())])
hp.dtypes
r['A'].aggregate([np.sum,np.mean])
print(len(fdist.hapaxes()))
flatten_plot_df = flatten_df(plot_df)
tmpdata = raw_large_grid_df.groupby(['eyetracker','subject','posx', 'posy'],as_index=False).mean()
a[ind]
row_num = df.shape[0] $ print("Number of rows is: {}".format(row_num))
logit_control_2 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page','UK_int_ab_page', 'US_int_ab_page']]) $ result_control_2 = logit_control_2.fit()
df.head()
text1 = textract.process(datapath3 / 'textimage.png').decode('utf-8') $ print(text1[0:900])
payments_all_yrs.to_csv('Line_item_Site_Year.csv',index=False)
ts['year'] = ts.index.year
df_2018.isnull().sum()
table.to_csv(report_path+'\%s.csv' % day.strftime('%Y-%m-%d'), encoding='utf-8', index=False)
import nltk $ nltk.download('punkt') $ from nltk.tokenize import word_tokenize
df3[df3['STATION']=='103 ST'].groupby('WEEK').sum()
pp[0:5]
inspector = inspect(engine) $ columns = inspector.get_columns('station') $ for c in columns: $     print(c['name'], c["type"])
len(lifetime)
print(X_train.info())
def download_historical_prices_for_instrument( ticker ): $
data = r.json()
customer_visitors['Yearcol']=customer_visitors.DateCol.dt.year
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = sm.graphics.tsa.plot_acf(resid_701.values.squeeze(), lags=40, ax=ax1) $ ax2 = fig.add_subplot(212) $ fig = sm.graphics.tsa.plot_pacf(resid_701, lags=40, ax=ax2)
len(df2[df2.duplicated(['user_id'],keep=False)])
sakhalin_freq = sakhalin_filtered.genus.value_counts() / len(sakhalin_filtered) $ sakhalin_freq
trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn]) $ val_lm = np.array([[stoi[o] for o in p] for p in tok_val])
twitter_Archive.info()
tfidf_vect = TfidfVectorizer(analyzer = clean_text) $ X_tfidf = tfidf_vect.fit_transform(tweets_1['text']) $ print(X_tfidf.shape)
import haversine
s1 = pd.Series([4,3,5,2,5], index=['Mon', 'Tues', 'Wed', 'Thur', 'Fri'], name='Test1')
DummyDataframe2 = DummyDataframe[["Tokens","Token_Count"]].copy() $ DummyDataframe2 = DummyDataframe2[["Tokens","Token_Count"]].groupby('Date').agg({'Tokens': 'sum', 'Token_Count': 'sum'}) $ DummyDataframe2
unigram_sentences = LineSentence(unigram_sentences_filepath)
def clean_mentions(text): $     return re.sub(r"@\w+", "", text)
cityID =  '4b25aded08900fd8' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Reno.append(tweet) 
metadata_df = pd.read_html(DATA_FOLDER+'/titanic.html', header=0)
doctors = duration_df[duration_df['Specialty'] == 'doctor'] $ therapists = duration_df[duration_df['Specialty'] == 'therapist'] $ RN_PA = duration_df[duration_df['Specialty'] == 'RN/PA']
df_new['intercept'] = 1 $ logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'CA']]) $ results = logit_mod.fit() $ results.summary() $
word = "best" $ try: $     print(model.most_similar(word)) $ except: $     print(word, "not in the vocab")
!wget -nv -O /resources/data/PierceCricketData.csv https://ibm.box.com/shared/static/reyjo1hk43m2x79nreywwfwcdd5yi8zu.csv $ df = pd.read_csv("/resources/data/PierceCricketData.csv") $ df.head()
tweet_image_clean = tweet_image_clean[['tweet_id', 'jpg_url', 'img_num', 'prediction','confidence']]
results_sim_rootDistExp, output_sim_rootDistExp = S.execute(run_suffix="sim_rootDistExp", run_option = 'local') $
plot_fi(fi)
fulldata_copy['Returns'] = np.log(fulldata_copy['AUDUSD'] / fulldata_copy['AUDUSD'].shift(1))
print("Type of data in cell (row 3, col 2):"), $ print(sheet.cell_type(3, 2))
import statsmodels.api as sm $ convert_old = df2.query('landing_page=="old_page" and converted==1').count()[0] $ convert_old $
data['results']['companies'][0]
df.drop_duplicates(inplace=True)
tweet.user.location $ tweet.user.time_zone $ tweet.created_at $ tweet.user.name $ tweet.user.screen_name
soup.find_all('div', {'class': 'thing'})['data-fullname']
results = session.query(Station.station, Station.name).count() $ print(f"There are {results} stations.")
results =df[['itapudid', 'max1stdetectwssc', 'max1stdetectwssd', 'max1stdetectwsse', 'max1stdetectwssf', 'eventtime']]
index = pd.date_range('2018-01-01 20:30:40', periods=10, freq='2S') $ index
%matplotlib inline $ import matplotlib $ import numpy as np $ import matplotlib.pyplot as plt
len(df_sched)
matthew = pd.DataFrame(TWEETS) $ matthew['date'] = matthew.created_at.apply(lambda x: x.date()) $ print(len(df))
rng = pd.date_range('2005', '2012', freq='M') $ rng
print df.shape[0] + noloc_df.shape[0]
con = pd.concat([tasks,emails,site_landings],axis=0)
X_train_df = pd.DataFrame(X_train_matrix.todense(), $                          columns=tvec.get_feature_names(), $                          index=X_train.index)
def lm(x): $     return x/2
newstudents = pd.DataFrame([(150, 62, 'M'), (170, 65, 'F')], columns = ['weight', 'height', 'gender'], index = ['Matt', 'Jen']) $ print(newstudents)
csv_fn=f'{PATH}tmp/sub.csv'
autos['last_seen'].describe()
query_result1.fields
df.head()
shoppingCart = pd.Series(["apple", "cherry", "banana", "cucumber", "apple"]) $ priceCategory = {"apple": "$3", "cherry": "$20", "banana": "$1.5", "cucumber": "$3"}
students.weight.values
r_test = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-04-30&end_date=2018-04-30&api_key=YOURAPIKEY') $
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/Sample_Superstore_Sales.xlsx" $ df = pd.read_excel(path) $ df.head(5)
bg_df2[2:5] # select rows with a slice, row 2 up to but not including row 5
pd.options.display.max_colwidth = 2000 $ jobs.new[jobs.new.code_ogr.isin(new_jobs)][['code_ogr', 'libelle_appellation_long', 'code_rome']]
from IPython.display import display $ import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
save_model('model_svm_v1.mod', svc_grid)
japandata.Yen2USD = japandata.Yen2USD.shift(periods = -1)
small_movies_data = remove_header(small_movies_raw_data,small_movies_raw_data_header,2)
run txt2pdf.py -o "2018-06-18 1525 2015 872 discharges.pdf"  "2018-06-18 1525 2015 872 discharges.txt"
reviews[(reviews.points>=90) & reviews.country.isin(['France','Italy'])].country
crimes.DATE__OF_OCCURRENCE = pd.to_datetime(crimes.DATE_OF_OCCURRENCE)
fld = 'SchoolHoliday' $ df = df.sort_values(['Store', 'Date']) $ get_elapsed(fld, 'After') $ df = df.sort_values(['Store', 'Date'], ascending=[True, False]) $ get_elapsed(fld, 'Before')
btc_price_df.index = btc_price_df.index.map(lambda x: x.replace(second=0)) $ btc_forum_df.index = btc_forum_df.index.map(lambda x: x.replace(second=0))
np.exp(results.params)
(val_texts $  .groupby('SDG') $  .count())
js
pd.DataFrame.from_dict(sbs.get_metric_dict()).T
pnls = df.map( lambda r: {'date': r.date, $                           'neutral': r.neutral, $                           'var': float(var(r.scenarios.array, neutral_scenario=r.neutral))})
master = df.merge(context, how='left', left_on='Recipient Email Address', right_on='email') $ master.head()
import statsmodels.api as sm $ convert_old = df2[(df2.group == 'control')&(df2.converted == 1)].shape[0] $ convert_new = df2[(df2.group == 'treatment')&(df2.converted == 1)].shape[0] $ n_old = df2[(df2.group == 'control')].shape[0] $ n_new = df2[(df2.group == 'treatment')].shape[0]
df_pivot_days_b = pd.pivot_table(df_convert, values='delta_to_convert_b', index=['user_id','nd_key_formatted'],columns=['course_key'], aggfunc=np.sum).fillna(0).reset_index()
print('Largest change (based on closing prices) =', '{:.2f}'.format(max(diff_close))) $ print('Largest absolute change (based on closing prices) =', '{:.2f}'.format(max(map(abs,diff_close))))
senate[:1]
score['is_false_positive'] = np.where((score['pred_rf']==1) & (score['is_shift']==0), 1, 0) $ score['is_false_negative'] = np.where((score['pred_rf']==0) & (score['is_shift']==1), 1, 0) $ score['is_true_positive'] = np.where((score['pred_rf']==1) & (score['is_shift']==1), 1, 0) $ score['is_true_negative'] = np.where((score['pred_rf']==0) & (score['is_shift']==0), 1, 0)
weather_mean.columns
data.head()
trump_bow = vect_bow.fit_transform(df_train) $ trump_cleaned_bow = vect_cleaned_bow.fit_transform(df_cleaned_train) $ trump_stemmed_bow = vect_stemmed_bow.fit_transform(df_stemmed_train)
def remove_stopwords(tokenized_list): $     text = [word for word in tokenized_list if word not in stopword] $     return text $ infinity['text_nostop'] = infinity['text_tokenized'].apply(lambda x: remove_stopwords(x))
big_df_count.head()
url_1day='https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=TaaJzC_if3-1gX5Wty4D&start_date=2017-04-10&end_date=2017-04-10' $ response=requests.get(url_1day)
tweet_archive_df[tweet_archive_df.rating_numerator == 0].sort_values(by=['rating_numerator'],  ascending=False)[['tweet_id','rating_numerator', 'rating_denominator', 'text']][0:10]
stats = collections.OrderedDict() $ stats['creation_time_utc'] = datetime.datetime.utcnow().isoformat()
df2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']) $ df2
df.head(10)
shows[['release_date', 'first_year']].isnull().sum()
tfidf_vect = TfidfVectorizer(analyzer = clean_text) $ X_tfidf = tfidf_vect.fit_transform(tweets_1['text']) $ Y_tfidf = tfidf_vect.transform(infinity['text'])
df.head()
t2
hashed_train_sample.show(5, truncate=False)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ z_score, p_value
nypd=agency[(agency['Agency'] == 'NYPD')] $ dot=agency[(agency['Agency'] == 'DOT')] $ dpr=agency[(agency['Agency'] == 'DPR')] $ hpd=agency[(agency['Agency'] == 'HPD')] $ dohmh=agency[(agency['Agency'] == 'DOHMH')]
from sklearn.datasets import fetch_20newsgroups $ data = fetch_20newsgroups() $ text = ' '.join(data.data).lower() $ text[100:350]
df2.drop_duplicates('user_id', keep = 'first', inplace = True)
company_page.content
np.array([1,2,3,4,5]) $
!ls _talks
days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'] $ for i in range(7): $     daily[days[i]] = (daily.index.dayofweek == i).astype(float)
train.head()
author_data.reset_index(inplace=True)
historicFollowers = pd.read_csv('Data/todaysFollowers_all.csv',sep = ';') #load historic Followers
sqlContext.sql("select count(person) from pcs").show()
session.query(User).from_statement( $ ...                     text("SELECT * FROM users where name=:name")).\ $ ...                     params(name='ed').all()
calls.info()
print(sum(filter_reg_met))
fNames
engine = create_engine("sqlite:///hawaii.sqlite")
np.exp(results.params)
_, pval_2_sided = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='two-sided') $ pval_2_sided
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
stores_tran_nulls['date'].unique()
start = datetime.now() $ modelxg3 = xgb.XGBClassifier(max_depth=3, learning_rate=.05, n_estimators=250, n_jobs=-1) $ modelxg3.fit(Xtr.toarray(), ytr) $ print(modelxg3.score(Xte.toarray(), yte)) $ print((datetime.now() - start).seconds)
sns.distplot(df['age'], kde= False) $ plt.axvline(df['age'].mean(), color='k', linestyle='dashed', linewidth=1) $ plt.xlabel("Post age by minutes") $ plt.ylabel("Frequency");
%matplotlib inline $ pandas_df.plot.bar()
tweets = tweets[['user', 'favorite_count', 'retweet_count']]
cashflows_act_investor=generate_cashflows_act_investor(actual_payments_combined,loan_fundings1,'2015-04-30')
df2['ab_page'] = df2['old_page'] $ df2 = df2.drop(['old_page', 'new_page'], axis=1) $ df2.head()
cust_data = pd.read_csv("DataSets/Cust_data.csv") $ cust_demo = pd.read_csv("DataSets/Cust_demo.csv") $ cust_new  = pd.read_csv("DataSets/cust_new.csv")
print(type(data.values)) $ data.values
new_page_converted = np.random.choice([0, 1], num_new, p=[1 - null_p_new, null_p_new]) $ print(new_page_converted[:10])
train_user.loc[pd.notnull(train_user.date_first_booking)].groupby(["country_destination"]).size()
trumpTweets=api.GetUserTimeline('25073877', include_rts=False, count=200) $ print(trumpTweets[0]) $ print(len(trumpTweets))
results = query_by_name(artist_url, query_type["simple"], "Nirvana") $ pretty_print(results)
df3[0] = df3[0] - time_utc[0]
np.std(old_means)
print "Probability of control group converting:",df2.groupby(['group'], as_index = False)['converted'].mean().iloc[0,1]
app_pivot = df[['is_application','ab_test_group', 'email']].groupby(['ab_test_group','is_application']).count().reset_index() $ app_pivot
df2gdb(driller,fileloc,'driller')
len(machin[(machin.user_id == 1881755) & (machin.item_id == 1928479) & (machin.interaction_type == 5)])
p = data_s[data_s['isvalid']>0].groupby('customer')['date'].agg({"first": lambda x: x.min(),"last": lambda x: x.max()}).reset_index()
df = pd.DataFrame({'ImageId': imageids, 'Label': category}) $ print (df.shape) $ print (df.head()) $ print (df.tail())
try: $     stream.filter(track=["python"]) $ except Exception as e: $     print(e)
df2.drop(index = df2.query('landing_page =="new_page" and group=="control"').index,inplace = True, axis = 0)
test_bow.shape, test_tfidf.shape
df.dtypes
prob_control = df2[df2['group'] == 'control']['converted'].mean() $ prob_control
diff = p_control - p_treatment $ actual_diff = diff.user_id $ actual_diff
from sklearn.naive_bayes import MultinomialNB $ nb = MultinomialNB()
dt.day
df_A.to_csv("classA.csv")
frames = [nuc, part_nuc, out_out, noSeal, entirecell] $ ps = pd.concat(frames) $ ps.head(10)
model2.summary()
df.to_excel("msft.xls")
df2.head()
predicted_table_T = predicted_table.T
numeric_df = compute_new_columns(joined_with_project_info)
df_r.head()
df.isnull().sum()
%timeit StockData = pd.read_csv(StockDataFile + '.gz', index_col=['Date'], parse_dates=['Date'])
df_state_victory_margins
number_of_users = df['user_id'].nunique() $ print(number_of_users)
df_norm = (df - df.mean()) / df.std()
from sklearn.model_selection import KFold $ cv = KFold(n_splits=100, random_state=None, shuffle=True) $ estimator = Ridge(alpha=30000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
mean_expr = np.mean(df1_subset['avexpr']) $ mean_expr
train_Y = train['Fare'] $ train_X = train.drop('Fare', axis = 1) $ test_X = test
df = df[df['Created At'] <= df['Shipped At']]
data.groupby('bandit')['expid', 'visibility'].mean()
import matplotlib.pyplot as plt $ from matplotlib import style $ style.use('ggplot') $ Y=df1[forcast_col] $ x=range(len(df1[forcast_col]))
from h2o.estimators.glm import H2OGeneralizedLinearEstimator $ help(H2OGeneralizedLinearEstimator) $ help(h2o.import_file)
breakdown = df_questionable[media_classes].sum(axis=0) $ breakdown
pd.DataFrame(zip(X.columns, np.transpose(model.coef_)))
twitter_archive.rating_numerator.describe()
df1.shape
aml.leader
hdf = df.set_index(['AgeBins', 'Industry']) $ hdf.head()
cust_demo.Location.value_counts().plot(kind='barh', color='R', alpha=0.5)
df3 = make_df('AB', [0, 1]) $ df4 = make_df('CD', [0, 1]) $ display('df3', 'df4', "pd.concat([df3, df4], axis=1)")
testObj.buildOutDF(tst_lat_lon_df)  ## some tests not shown performed ahead of this run $
contractor_clean['address2'].value_counts()
data['Sales'].dtypes
Measurement = Base.classes.measurements
len(af)
s.dt.time   # extract time as time Object
df.describe()
from sklearn.model_selection import StratifiedKFold, cross_val_score $ from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier $ from sklearn.linear_model import LogisticRegression $ from sklearn.pipeline import Pipeline
import awrams.utils.wiski.interface as W $ wcds = W.get_station_list("Water Course Discharge") # Get discharge volume (m3/sec) from wiski for gauging stations $ stns = W.find_id_in_station_list('105001',wcds) $ stns  #['105001B', '105001A']
df_archive_clean[["timestamp", "retweeted_status_timestamp"]] = df_archive_clean[[ "timestamp", "retweeted_status_timestamp"]].replace("\+0000","",regex = True)
import urllib3 $ dir(urllib3.exceptions)
df_questionable_3[df_questionable_3['state_CA'] == 1]['link.domain_resolved'].value_counts()
to_be_predicted_Day2 = 34.15678836 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
df_ad_airings_5.drop('state',inplace=True, axis=1)
tweet_archive_enhanced_clean['retweeted_status_id'].value_counts()
df_new[['US','UK']] = pd.get_dummies(df_new['country'])[['US', 'UK']]                        
df_final.to_csv('twitter_archive_master.csv', header=True, sep=',', encoding='utf-8')
diff_weekly_mean.show(10)
p_diffs = pd.DataFrame({'col': p_diffs}) $ p_diffs.plot(kind='hist')
overal_topic(date_df=df[df.placeId == place], total_base_dict=total_base_dict_by_place.loc[place]['dict'], $              total_N=total_base_dict_by_place.loc[place]['base_twitter_count'])
dbini = '/home/bkrull/.fireworks/metatlas.ini' $ lpad = create_launchpad(dbini) $ data = lpad.get_fw_dict_by_id(108317) $ pprint(data)
train_df[["SibSp", "Survived"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)
full['WillBe<=30Days'].value_counts()#.mean()
c.execute('SELECT COUNT(*) FROM CITIES WHERE name LIKE "San%" and state="CA"') $ print(c.fetchall())
import statsmodels.api as sm $ from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)#workaround for issues with logistics regression in statsmodel.api $ logit_mod=sm.Logit(df2['converted'],df2[['ab_page','intercept']]) $ results=logit_mod.fit()
converted = df['converted'].value_counts()[1] $ total =df['converted'].count() $ total_converted = (converted/ total)* 100 $ print('The total amount converted: {} \n' $      'The percentage : {}%'.format(converted, total_converted))
autos['registration_year'].describe()
so.loc[so['viewcount'] > 20000, ['creationdate', 'viewcount', 'ans_name']].head(10)
Q96P20=Graph().parse(format='ttl', $                      data=turtle) $
!head data/age_gender_bkts.csv -n 25
idx = df_providers [(df_providers['drg3']==470) | (df_providers['state'] == 'CA')].index.tolist() $ len(idx)
pd.date_range(start = '8/01/2018', end = '8/31/2018', freq = sm)
print(cfs)
prob_conv_control = df2[df2['group']=='control'].sum()['converted'] / len(df2[df2['group']=='control']) $ prob_conv_control
result_new.summary2()
pv1 = pd.pivot_table(df_cod, values="Age at death", index="Death year") $ pv1
data['data'].keys()    # getting the data Keys
w.apply_data_filter(subset = subset_uuid, step = 1)
plt.show()
df.head()
countries_df = pd.read_csv('./countries.csv') $ countries_df.head() $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
All_tweet_data_v2.timestamp=pd.to_datetime(All_tweet_data_v2.timestamp) $ All_tweet_data_v2.timestamp.head()
df_sms[df_sms['group']=='T4'].groupby(['ShopperID'])['ID'].count().reset_index().groupby('ID').count().plot.bar()
df2[['no_ab_page','ab_page']] = pd.get_dummies(df2.group) $ df2 = df2.drop('no_ab_page', axis=1) $ df2.head()
with pd.option_context('display.max_colwidth', 100): $     display(news_period_df[['news_collected_time', 'news_title']])
result['modifiedBy'].nunique()
df4["intercept"] = 1 $ logit_mod = sm.Logit(df4["converted"], df4[["intercept","UK","CA","old_page"]]) $ results = logit_mod.fit() $ results.summary()
indeed.isnull().sum()
plot_corr(input_data)
train[['StateHoliday_0', 'StateHoliday_a', 'StateHoliday_b', 'StateHoliday_c']] = pd.get_dummies(train['StateHoliday']) $ test[['StateHoliday_0', 'StateHoliday_a']] = pd.get_dummies(test['StateHoliday']) $ train.drop('StateHoliday', axis=1, inplace=True) $ test.drop('StateHoliday', axis=1, inplace=True)
yt.get_channel_metadata(channel_id, key)
from pyspark.sql.functions import monotonically_increasing_id $ test = (test $         .withColumn('id', monotonically_increasing_id()) $         .drop('click_time')) $ print("test size: ", test.count())
df.groupby(['group','landing_page']).size()
information_ratio['Manager_A'] + 2
unique_labels = set(labels) $ unique_labels
df_img_predictions.tweet_id.nunique()
dump = dump.dropna() # Drop all null values
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=8000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
p_diffs_actual =-0.1203 + 0.1188 $ greater_than_diff = [i for i in p_diffs if i > (p_diffs_actual)] $ print('Proportion greater than actual difference:', len(greater_than_diff)/len(p_diffs))
learning_rate = 0.01 $ with tf.name_scope("train"): $     optimizer = tf.train.GradientDescentOptimizer(learning_rate) $     training_op = optimizer.minimize(loss)
!hdfs dfs -cat {HDFS_DIR}/p32cf-output/part-0000* > p32cf_results.txt
X_train_dtm = pd.DataFrame(cvec.fit_transform(X_train.title).todense(), columns=cvec.get_feature_names())
from pandas import ExcelWriter $ with ExcelWriter("multisheets.xls") as writer: $     df.to_excel(writer, sheet_name="sheet1") $     df.to_excel(writer, sheet_name="sheet2")
df.head(5)
df['Predicted'] = y_pred
Y_train_lab = le.fit_transform(Y_train) $ Y_valid_lab = le.transform(Y_valid)
weather.events
pd.Timestamp(datetime(2018, 1, 1))
future_pd_30360_origpd_all[(future_pd_30360_origpd_all.fk_loan==574) & (future_pd_30360_origpd_all.fk_user_investor==31192)].to_clipboard()
logit_country = sm.Logit(df_new['converted'], $                            df_new[['country_UK', 'country_US', 'intercept']]) $ result_country = logit_country.fit()
d_education=detroit_census2.drop(detroit_census2.index[36:]) $ d_education=d_education.drop(d_education.index[:34]) $ d_education.head()
data_sets = {} $ data_sets['1min'] = pd.read_pickle('fixed_1min.pickle') $ data_sets['3min'] = pd.read_pickle('fixed_3min.pickle')
df['user_id'].nunique() $
x=numpy.memmap('columncache/acs2015_5year_tract2010/B00001_001.float32', dtype=numpy.float32, mode='r')
(details['Runtime'] == 0).value_counts()
df.loc[row,'blurb']
df2.to_csv("msft_temp.csv", index_label='date')
actual = test_df['completed_by'] $ pd.value_counts(actual).plot.bar()
bd.index.name
def load_data(filename): $     return pd.read_csv(filename,  $                        parse_dates = True)
twitter_archive_clean=twitter_archive_clean[twitter_archive_clean['retweeted_status_id'].isnull()]
plt.scatter("obs_date","status", data=pm_final,marker='o', alpha = 0.25) $ plt.xlabel("Cumulative date for readings") $ plt.ylabel("Status") $ plt.title('Failure over time') $ plt.show()
charge = reader.select_column('charge', start=0, stop=100) $ charge = charge.values # Convert from Pandas Series to numpy array $ charge
r = requests.get('https://httpbin.org/basic-auth/myuser/mypasswd', auth=('myuser', 'mypasswd')) $ r.status_code
type(R16df)
print(r.text)
facts_metrics.dimensions_item_id.nunique()
news = Get_News(1000) $ news.shape
journalists_retweet_df = retweet_df.join(user_summary_df['gender'], how='inner', on='retweet_user_id', rsuffix='_retweet') $ journalists_retweet_df.rename(columns = {'gender_retweet': 'retweet_gender'}, inplace=True) $ journalists_retweet_df.count()
train_frame += 1
consumer_key = "HqmbSQzqMOeQI5U6zaIS42Pja" $ consumer_secret = "EfDRnSYxvdBXE4Tj5rwAZyNC5gJXOxg7GL32Vf6QNqb1K45Xaw" $ access_token = "35740765-Z8VkREBVuYzEnoKwfKU9NqbJp1FRJEEUZ1VKQ5yNY" $ access_token_secret = "N3VeKhoiOgFfrW65uFJflIfAxhDT24MHR2NVkFH5vd0XR"
print 'Examine the contents of the data structure' $ for p in mp2013: $     print "%s %s %s %s" % (p, p.freq, p.start_time, p.end_time)
df_tweet_clean.info()
df2gdb(systemuseData,fileloc,'systemuse')
len(nyc_census_tracts)
bbpc.head()
df_2010 = pd.DataFrame(rows)
stops_map = folium.Map(location=[39.0836, -77.1483], zoom_start=11) $ marker_cluster = plugins.MarkerCluster().add_to(stops_map) $ for name, row in morning_rush.iloc[:1000].iterrows(): $     folium.Marker([row["longitude"], row["latitude"]], popup=row["description"]).add_to(marker_cluster) $ stops_map.render()
data_file = 'https://alfresco.oceanobservatories.org/alfresco/d/d/workspace/SpacesStore/0ddd2680-e35d-46bc-ac1a-d350da4f409d/ar24011.asc'
state_DataFrames['OH_R_con'].head(30)
id_of_tweet = 932626561966247936 $ tweet = (api.get_status(id_of_tweet, tweet_mode='extended')._json['full_text']) $ print(tweet)
data_year = session.query(Measurement.date, Measurement.station, Measurement.prcp).\ $                 filter(Measurement.date > '2016-08-23').\ $                 filter(Measurement.date <= '2017-08-23').\ $                 order_by((Measurement.date).asc()).all()
reviews_sample.shape
temp = pd.to_datetime(df_all['created'], unit='s') $ temp = pd.DatetimeIndex(temp) $ print(temp[0:2])
df_station = df.groupby(('STATION', 'DATE')).sum().reset_index() $ df_station
from sklearn import tree
print(df["created_at"].min()) $ print(df["created_at"].max())
df2 = df2.drop('control',axis = 1) $ df2 = df2.rename(columns={'treatment': 'ab_page'}) $ df2.head()
gdax_trans_btc.loc[(gdax_trans_btc['Type'] == 'match') & (gdax_trans_btc['Trade_amount']>0),'Type'] = 'Buy' $ gdax_trans_btc.loc[(gdax_trans_btc['Type'] == 'match') & (gdax_trans_btc['Trade_amount']<0),'Type'] = 'Sell' $ gdax_trans_btc.loc[(gdax_trans_btc['Type'] == 'transfer') & (gdax_trans_btc['Trade_amount']>0),'Type'] = 'Fund' $ gdax_trans_btc.loc[(gdax_trans_btc['Type'] == 'transfer') & (gdax_trans_btc['Trade_amount']<0),'Type'] = 'Withdraw'
autos["odometer_km"].unique().shape
nnew=df2[df2['group']=='treatment'].count()[0] $ nnew
pax_raw['minute_of_day'] = pax_raw.paxhour*60 + pax_raw.paxminut $ pax_raw.head()
precip_data_df1=precip_data_df.copy() $ precip_data_df1.reset_index(inplace=True,drop=False) $ precip_data_df1.head(5)
import statsmodels.api as sm $ convert_old = sum(df2[df2.group == 'control'].converted) $ convert_new = sum(df2[df2.group == 'treatment'].converted) $ n_old = df2[df2.group == 'control'].user_id.count() $ n_new = df2[df2.group == 'treatment'].user_id.count()
pathModel
show_in_notebook('./rendered.html')
articles = response.get('items') $ body_txt = articles[0]['body'] $ title = articles[0]['title'] $ print(body_txt)
df_nuevo = df
table_name = "Nathan_Carto_SQL_API_test" $ columns_and_data_types = ", ".join([col + " " + cols_and_types[col] for col in cols_and_types]) $ res = sql_api(carto_url, create_table_sql, carto_api_token) $ print(res.text)
if 0 == 1: $     news_titles_csv_file = os.path.join(config.HR_DIR, 'news_titles.csv') $     news_period_df = pd.read_pickle(config.NEWS_PERIOD_DF_PKL) $     news_period_df.to_csv(path_or_buf=news_titles_csv_file, columns=['news_collected_time', 'news_title'], sep='\t', header=True, index=True)
plt.figure(figsize=(8,8)); $ plt.boxplot(tt_final['rating_numerator'], labels=['rating_numerator']);
input_data = input_data[input_data.year >= 2010] $ input_data.shape
pandas.read_csv('Movies_awards.csv') $
%%time $ filename = keys_today_[0][0] $ extract_all = f_import_extract_and_dedup(filename, zipped=True)
legos['inventory_parts'].shape, lego_color_cat.shape
type(data)
logistic_mod_page = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results_page = logistic_mod_page.fit()
pd.DataFrame(data = np.random.rand(50, 4), columns=['a', 'b', 'c', 'd']) $ plot_test.plot(kind='scatter', x='a', y='b')
volt_prof_before.set_index('Bus', inplace=True) $ volt_prof_after.set_index('Bus', inplace=True)
tables = [pd.read_csv(f'{PATH}{fname}.csv', low_memory=False) for fname in table_names]
dates = [i[0] for i in prcp_results] $ precipitation = [(i[1]) for i in prcp_results] $ print(dates[:5],'\n', precipitation[:5])
countries_df = pd.read_csv('countries.csv') $ countries_df.head()
states = ('TMSS', 'PS', 'PSA', 'PCS') $ df01 = calc_chernoff_data(df_one, 0.1, states, 3, 6, 50) $ df1 = calc_chernoff_data(df_one, 1.0, states, 3, 6, 50) $ df2 = calc_chernoff_data(df_one, 2, states, 3, 6, 50)
_GoogOut_forgbatmn00.count()
manager.image_df[manager.image_df['filename'] == 'image_picea_sitchensis_in_winter_11.png'] $
(df2.query("group == 'treatment'")['converted'] == 1).mean()
pd.merge(df_a, df_b, on='mathdad_id', how='outer')
import numpy as np $ print 'Originally there were ', len(df.columns) $ _, i = np.unique(df.columns, return_index=True) $ print 'Now there are ', len(np.unique(df.columns)), ' columns' $
data = [row["_source"] for row in res["hits"]["hits"]] $ df = pd.DataFrame(data)#, columns=["phenomenonTime", "Datastream.@iot.id", "result"]) $ df.index = pd.to_datetime(df["phenomenonTime"]) $ print(df.shape) $ df.head()
full.duplicated().sum()
clients = pd.DataFrame() $ for i in list(range(0, len(user_clients))): $     clients_df_temp = pd.DataFrame.from_dict(user_clients) $     clients = pd.concat([clients_df_temp, clients])
xlfile = os.path.join(DATADIR,DATAFILE) $ xl = pd.ExcelFile(xlfile)                $ tmpdf = xl.parse(xl.sheet_names[0])       
new_page_converted.sum()/len(new_page_converted) - old_page_converted.sum()/len(old_page_converted) $
df.loc['a']>0
van15_fin['reverted_mode'].value_counts()
today = dt.date.today
stocks_pca_m=stocks_pca_m[~np.isnan(stocks_pca_m).any(axis=1)]
sub1.shape
df_clean.head(1)
df_max.T.to_csv('../outputs/CSFV2_outlook_monthly_90th_per_summary_table_from_{:%Y%m%d}.csv'.format(dates_range[0]))
against = merged[merged.committee_position == 'OPPOSE'] $ support = merged[merged.committee_position == 'SUPPORT']
df2 = pd.read_csv('clean_data_df2.csv') $ df2.head()
country = pd.read_sql_query('select * from country', engine) $ country.head()
consumer_key = 'pBpEQD7eWF7BBRmYscIp3CzBJ' $ consumer_secret = 'aAHqk4U0GFXcTcjXbrRmGLCTcTnVwsfBOynweKH67iRxYClqR2' $ access_token = '1012202171700408322-dr6srgFzYzCKnNAK1UfbuN3CCxRB85' $ access_secret = '6UR4x6s7MlprW3zbqoFRmTE8o6ox27zbjQZP7XNcwwJ8D'
datAll['half_year'].value_counts(dropna=False)
df_merge.head()
input_nodes_DF=nodes_table('network/source_input/nodes.h5', 'inputNetwork') $ input_nodes_DF[:5]
import statsmodels.api as sm $ lm = sm.Logit(df['converted'], df[['intercept', 'ab_page']]) $ results = lm.fit()
from gensim.models import Word2Vec $ model = Word2Vec.load("300features_40minwords_10context")
first_result = records[0] $ print(first_result)
print(df_tweets.reset_index().drop_duplicates("tid").shape[0]) $ df_tweets.shape[0]
ttest(cool, fresh)
autos["vehicle_type"].value_counts()
import pandas as pd $ from pyspark.sql import SparkSession $ import numpy as np
tx_sum = TX_profit['Profit Including Build Cost'].sum() $ buildings_tx = len(TX_profit)
year = pd.get_dummies(auto_new.CarYear) $ year = year.ix[:, ["2014", "2011","2010", "2012", "2013", "2015", "2016", "2008", "2009", "2007", "2017"]] $ year.head()
plt.figure(figsize=(15, 5)); $ plt.plot(np.sin(np.arange(50)), 'c-*');
os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $ Copy_a_file_to_put_in_a_directory(os.getcwd(),'test_file.py')
autos.columns = columns
df_new['intercept'] =1 $ df_new = df_new.join(pd.get_dummies(df_new.country)) $ df_new = df_new.join(pd.get_dummies(df_new.landing_page))
forecast = prophet_model.predict(future_dates)
v_invoice_sat.shape
automl_feat.fit(X_train, y_train, $            dataset_name='psy_native', $            feat_type=feat_type)
save_filepath = '/media/sf_pysumma' $ hs_path = save_filepath+'/a0105d479c334764ba84633c5b9c1c01/a0105d479c334764ba84633c5b9c1c01/data/contents' $ S = Simulation(hs_path+'/summaTestCases_2.x/settings/wrrPaperTestCases/figure07/summa_fileManager_riparianAspenSimpleResistance.txt')
data[(data.value>1000) & (data.phylum.str.endswith('bacteria'))]
Google_stock['Open'].min()
row = 0 $ df.loc[row,:]
list_of_tables = pd.read_html("https://simple.wikipedia.org/wiki/List_of_U.S._state_capitals") $ states_df = list_of_tables[0] $ states_df.head()
np.sum(x < 6, axis=1)
getUnique(df_train)
no_psc_top_of_chain = pd.DataFrame(graph.run("MATCH p=(c1:Company)<-[:CONTROLS*0..]-(c2:Company)-[:STATES]-(s:Statement)\ $ WHERE s.statement CONTAINS 'no-individual-or-entity-with-signficant-control'\ $ RETURN DISTINCT (c1.company_number)").data()) $ print(len(no_psc_top_of_chain)) $ print("Additional companies due to network effect: " + str(len(no_psc_top_of_chain) - len(active_psc_statements[active_psc_statements.statement.str.contains('no-individual-or-entity-with-signficant-control')].company_number.unique())))
store_items[['bikes', 'glasses']]
run txt2pdf.py -o"2018-06-18  2015 871 disc_times_pay.pdf"  "2018-06-18  2015 871 disc_times_pay.txt"
for x in tweets_clean.dog_class.unique(): $     print('Mean rating numerator for ' + str(x) + ' class:' + str(tweets_clean[tweets_clean.dog_class == x].rating_numerator.mean()))
data = data[['last', 'volume']]
session.query(measurement.date).\ $     filter(measurement.station == 'USC00519281').\ $     order_by(measurement.date).first()
cityID = 'b49b3053b5c25bf5' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Denver.append(tweet) 
p_old = sum(df2.converted == 1) / 290584 $ p_old
(p_diffs > actual_diff).mean()
url_customers = 'https://ibm.box.com/shared/static/gwak77ibs1zy7i5foza03f9u7zcdgn6j.csv' $ url_transactions = 'https://ibm.box.com/shared/static/zjx66wjdtl02gi9rr2nmal2po4eqaxtc.csv'
print("P(converted) = %.4f" %df2.converted.mean())
error_as_pandas_df = errors.as_data_frame(use_pandas = True) $ print(error_as_pandas_df.head()) $ %matplotlib inline $ import matplotlib.pyplot as plt $ plt.plot(error_as_pandas_df, 'ro')
to_be_predicted_Day2 = 83.14924533 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
from sklearn import svm
df_pol_d=pd.get_dummies(df_pol_t, columns=['domain_d'], drop_first=True)
rf = RandomForestClassifier(labelCol="label", featuresCol="features")
v = Vehicle(4, 0, 'Honda', 'Accord', 2014, None)
tags = pd.read_csv('tag_categories.csv')
y_pred[:10]
lr = search.best_estimator_
com_grp.head(2)
animals = pd.crosstab(df['Is_Fixed'], df['Outcome']) $ rate = animals.div(animals.sum(1).astype(float), axis=0)*100 $ rate.plot(kind='barh', stacked=True) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) $ plt.show()
intersections_final_for_update = intersections_final[['Segment ID','speed','level','avg_traffic_flow','updateTimeStamp','isWeekend','date_time_hour','estimated_number_vehicles','CO2_grams']]
print(SbItem)
import xarray as xr  # a toolset for reading netcdf files $ %matplotlib inline   
access_logs_df.select(min("contentSize"), avg("contentSize"), max("contentSize")).show()
df_twitter_copy['rating_numerator'] = df_twitter_copy['rating_numerator'].astype(float) $ df_twitter_copy['rating_denominator'] = df_twitter_copy['rating_denominator'].astype(float)
print(len(terror)) $ terror.head()
flight_delays = flight_delays.reindex(taxi_hourly_df.index) $ flight_delays.fillna(0, inplace=True)
df.set_index('Day') # this returns the new data frame with index set as 'Day' but does not modify the existing df dataframe $ df.head()
(trainingData, testData) = data.randomSplit([0.7, 0.3]) $
from sklearn.linear_model import LogisticRegression $ log_model = LogisticRegression() $ log_model = log_model.fit(X=X_train, y=y_train) $ y_pred = log_model.predict(X_test) $ print(classification_report(y_test, y_pred)) $
import spacy $ nlp = spacy.load('en')
to_be_predicted_Day3 = 25.10910355 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
df = pd.read_pickle('data/df.pkl')
breakdown[breakdown != 0].sort_values().plot( $     kind='bar', title='Russian Trolls Number of Links per Topic' $ );
people_person = pd.read_csv('drive-download-20180422T230536Z-001/people_person.csv') $ people_person.head()
logit_mod = sm.Logit(df_US['converted'],df_US[['intercept','noon','evening']]) $ results = logit_mod.fit() $ results.summary() $
df.describe(include=['object','number'])
var_num = ['Shop_Products_Count','Shop_Bo_Connections_Ratio','Shop_Orders_Count_Ratio', $            'Shop_Gross_Sales_Ratio','Shop_Shipping_Methods_Count'] $ var_cat = ['Merchant_Country','Merchant_Language','Shop_Status']                                                      
df2.columns
autos['registration_month'].value_counts()
vis = sercRefl[:,:,57] $ nir = sercRefl[:,:,89] $ ndvi = np.divide((nir-vis),(nir+vis))
temp = train.groupby("DOW")["any_spot"].mean().sort_values(ascending=True) $ DOW_rank = pd.DataFrame({'DOW': temp.reset_index(drop=False).DOW.values, 'DOW_ranking':range(7)}).set_index('DOW') $ train = train.join(DOW_rank, on="DOW", rsuffix='_ranking') $ val = val.join(DOW_rank, on="DOW", rsuffix='_ranking')
Chart1 = crimes[['Violent Crimes', 'Property Crimes']].sum() $ fig, ax = plt.subplots(figsize=(6,6)) $ Chart1.plot(kind='pie',autopct='%.1f%%', fontsize=13, subplots=True , ax=ax) $ ax.set_title('Ratio of Violent Crimes and Property Crimes', fontsize=15, weight='bold') $ plt.show()
sql = SparkSession.builder.master("local").appName("wm").getOrCreate()
Results_kNN1000 = Results_kNN1000[['ID', 'Approved']] $ Results_kNN1000.head()
g = data_s[data_s['isvalid']>0].groupby('customer').apply(lambda x: x['isvalid'].sum()).reset_index()
with tf.name_scope("loss"): $     xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, $                                                               logits=logits) $     loss = tf.reduce_mean(xentropy, name="loss")
pd.merge(master,transaction,on='nameid')
festivals.reindex(festivals.index.drop('Fest'))
cols = ['A','B'] $ pd.get_dummies(df[cols])
df_transactions['discount'] = df_transactions['plan_list_price'] - df_transactions['actual_amount_paid']
user_project = pd.read_csv('data_old/user_project.csv') $ print(user_project.shape) $ user_project.head()
actual_obs_diff = p_treatment_converted - p_control_converted $ actual_obs_diff
Stemmed = [stemmer.stem(word) for word in word_freq_df.Word] $ word_freq_df['Word_stem'] = Stemmed
full_history.columns = ["key", "date", "value"] $ full_history = full_history.reindex(columns=["key", "value", "date"]) $ full_history.to_csv(FILENAME_PREFIX + "modifications" + FILENAME_SUFFIX + ".csv", index=False) $ full_history.head()
xgb = XGBRegressor(gamma=0, learning_rate=0.02, max_depth=3, n_estimators=100)
df_all = pd.merge(df2, df_countries, on='user_id') $ df_all.head()
to_be_predicted_Day2 = 26.32 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
staff['Number'] = staff.index $ staff
lst = [5, 1, 6, 7, 4, 2, 3] $ mean_for_each_weekday['day_order'] = pd.Series(lst, index=mean_for_each_weekday.index) $ mean_for_each_weekday
analyzer.polarity_scores("The book was not good.")
points_df = pd.DataFrame(all_points, columns = ['Time', 'Lat', 'Lon']) $ points_df.set_index('Time', inplace = True) $ points_df.head()
train.head(3)
%matplotlib inline
highest_confidence = stacked_image_predictions.groupby(['tweet_id'])['confidence'].max().reset_index()
name = ['Alice', 'Bob', 'Cathy', 'Doug'] $ age = [25, 45, 37, 19] $ weight = [55.0, 85.5, 68.0, 61.5]
pd.Timestamp('9/1/2016 10:05AM')
sum(data.c1_date.isnull())
obs_diff = new_page_converted.mean() - old_page_converted.mean() $ obs_diff
print("Column names before change") $ df.head(2)
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','US','UK','ab_page']]) $ results=logit_mod.fit() $ results.summary()
ryanair = df[df["text_3"].str.contains("ryanair", case = False)] $ ryanair.text_3.str.split(expand=True).stack().value_counts().head(10).reset_index()
tweets_df.sample(3)
df.query('converted == 1')['user_id'].nunique() / df['user_id'].nunique()
classifier.fit(X_train, y_train) # fitting the model using train set
average_polarity.reset_index(inplace=True)
oecd['year'] = pd.to_datetime(oecd.Date).apply(lambda x: x.year) $ oecd_year = oecd.set_index(oecd.Country.str.title())['year'].dropna() $ oecd_year
df_clean.info()
np.mean(high_pdiff)
print("Probability an individual recieved new page is", $       df2['landing_page'].value_counts()[0]/len(df2))
clean_users[clean_users['active']==1]['account_life'].mean()
learn.unfreeze() $ learn.fit(lrs,3,cycle_len=1,cycle_mult=2)
df = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']) $ df
df_test.dtypes $
with open('posts2.pkl', 'rb') as fh: $     data = pickle.load(fh) $ del data['block_nums']
Y=file278[target] $ X=file278[train] $ X=X.fillna(0) $ np.nan_to_num(Y)
months = pd.concat([nov, dec]) $ print nov.shape, dec.shape, months.shape
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=10)
filter_df['start_time'].min(), filter_df['start_time'].max() 
search_key_words = 'electric cars' $ num_articles = 10 
autos["price"].value_counts().sort_index(ascending = True)
blocks.head()
import chakin $ chakin.search(lang='English')
input_edge_types_DF = pd.read_csv('network/source_input/edge_types.csv', sep = ' ') $ input_edge_types_DF
data_mean = health_data.mean(level='year') $ data_mean
ad_groups_zero_impr.groupby( $     ['CampaignName', 'Date'], $     as_index=False $ ).count()
sns.factorplot('brand',kind='count',data=autodf,size=50)
price_to_num = lambda x: float(x.replace("$","")) $ df['Price'] = df['Price'].map(price_to_num)
df[['Gross Sales','Units Sold']].iloc[[0,2],[0,1]]
upper_band.plot(label="Upper Band",ax=ax) $ lower_band.plot(label="Lower Band",ax=ax)
file_name='2018-07-09-2015-871 - SEPTICEMIA OR SEVERE SEPSIS Without MV 96+ HOURS W MCC' $ file_name='2018-07-09-2015-872 - SEPTICEMIA OR SEVERE SEPSIS Without MV 96+ HOURS Without MCC'
bikes['hour_of_day'] = (bikes.start.dt.hour + (bikes.start.dt.minute/60).round(2)) $ (bikes['hour_of_day']).head(10)
model.add(Conv1D(30, kernel_size = 30, $                  padding='same', activation='relu')) $ model.add(MaxPooling1D(pool_size=(30))) $ model.add(Dropout(rate=0.25))
tokens = nltk.regexp_tokenize(text.lower(), '[a-z]+') $ tokens[:10]
r_col = ['835152434251116546', '746906459439529985', '670842764863651840', '855862651834028034', '810984652412424192'] $ archive_df.drop(index=r_col, inplace=True, errors='ignore')
df2.query('user_id == {}'.format(dup_id)).head()
y.name
c.execute(query) $ results=c.fetchall() $ print 'Number of contributors who made 10 or less contributions:', results[0][0]
wkrng[0].dayofweek
print(psy_hx.shape) $ psy_hx = psy_hx[psy_hx["subjectkey"].isin(incl_Ss)] $ psy_hx.shape $
df.size
pd.set_option('display.max_colwidth', 150) $ clinton_df.head()
if committee_df.iloc[0]['party_full'] is not None: $     print(committee_df.iloc[0]['party_full'] ) $ else: $     print('yes')
print('old', n_old, '\nnew', n_new)
talks.text.shape
df.groupby("cancelled")["weekend_pickup"].mean()
parking_planes_df=parking_planes.toPandas() $ parking_planes_df
df3.head()
root_cell = openmc.Cell(name='root cell') $ root_cell.region = +min_x & -max_x & +min_y & -max_y $ root_cell.fill = pin_cell_universe $ root_universe = openmc.Universe(universe_id=0, name='root universe') $ root_universe.add_cell(root_cell)
users = users[ users["mobile_registered"] == False ] $ revs = revs[ revs["mobile_registered"] == False ]
twitter_archive_master.info()
kmeans = KMeans(n_clusters=5, random_state=0).fit(tfidf_matrix)
ben_fin["stiki_mean"] = ben_scores.groupby('userid')['stiki_score'].mean()
data.dropna().describe()
archive.pupper.value_counts()
tm_2040 = pd.read_csv('input/data/trans_2040_m.csv', encoding='utf8', index_col=0)
pd.to_datetime(1)
!head data/goog.csv $
a = codebuddy("spectrum").split(" ") $ a = pd.DataFrame(a) $ a = a.transpose()
model_2 = graphlab.linear_regression.create(train_data, target = 'price', $                                                   features = model_2_features, $                                                   validation_set = None)
max_meetup_events = 800
print(reddit.user.me())
import psutil $ psutil.virtual_memory()
user_alias = aliased(User, name='user_alias')
train['date'] = pd.to_datetime(train['date'])
df_vimeo_selected=df_vimeo_selected[['uri','course_name','name','loads','plays','finishes', 'downloads', $         'duration', 'created_time', 'sizes', 'mean_percent']]
Base.metadata.create_all(engine)
sex.value_counts().sort_index().plot(kind='bar') $ sex.value_counts()
july = summer.ix[datetime(2014,7,2) : datetime(2014,8,28)] $ july[['Mean TemperatureC', 'Precipitationmm']].plot(grid=True, figsize=(10,5))
(~autos["registration_year"].between(1900,2016)).sum() / autos.shape[0] $ autos = autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].value_counts(normalize=True).head()
df.sort_values('Year').head(5)
col = col.dropna(subset = ['LOCATION'])
df_full = pd.DataFrame() $ df_list = []
random_series.cumsum().plot()
%%time $ X_df.iloc[:, 5] = X_df['text'].apply( lambda rev: re.sub(r'(\d+)', '', rev) )
tweets_df.id.describe()
final_df = en_df.dropna() $ print('Final data set has {} entries'.format(len(final_df))) $ final_df.head()
num_check(num_closures)
import pandas as pd $ import numpy as np $ autos = pd.read_csv("autos.csv", encoding = "Latin-1" ) #Unicode failed the fist time (UTF-8) $ autos                                                   #Changed to Latin-1
pokemon_train = pokemon[~pokemon['Name'].isin(pokemon_test['Name'])]
replace_dict = {'10+ years':10, '2 years':2, '< 1 year':0, '3 years':3, '1 year':1, '5 years':5, '4 years':4, 'n/a':0, $                 '7 years':7, '8 years':8, '6 years':6, '9 years':9}
print(df[df.B>0],'\n') $ print(df[df>0],'\n')
n_new=df2.query('group == "treatment"').shape[0] $ n_new
df['room_size'] = df ['life_sq']/df['num_room'] $ df['room_size']
lr = LogisticRegressionCV() $ lr.fit(train_Features, train_species)
xgb = XGBRegressor(gamma=0, learning_rate=0.02, max_depth=3, n_estimators=100) $ xgb.fit(X_train, y_train)
old_page_converted = np.random.binomial(1, p=p_old, size=n_old)
item1={'c1':('A','B','C','D'),'c2':np.random.randint(0,4,4)} $ item2={'c1':np.random.randint(0,4,5),'c2':np.random.randint(0,4,5)} $ pd.Panel({'one':item1,'two':item2})
path = "/Users/ekdlsjubilee/Downloads/kickstarter-projects"
weight = live.birthwgt_lb
plot_spirals(models, m_names)
students.sort_index(axis=0)
*test1, target1 = next(iter(column_dataloaders['train']))
area=df_usa['Area']*10 $ df_usa['Area']=area $ df_usa.head()
history = model.fit([nn_X_train], nn_y_train, initial_epoch=0, epochs=25, batch_size=100, validation_data=([nn_X_test],nn_y_test), shuffle=True)
tweet_df.count()
punctuations = list(string.punctuation) $ df['body_tokens'] = df['body_tokens'].apply(lambda x: [word for word in x if word not in punctuations]) $ print(df['body_tokens'])
df.head() $
train=pd.read_csv('pisa2009train.csv') $ test=pd.read_csv('pisa2009test.csv') $ train.shape
import pandas as pd
billstargs.drop(([408], [416], [456], [718], [757], [811], [928], [968], [1075], [1204], [1463], [1464], [1466], [1467], [1468], [1469], [1470], [1497], [1552], [1555], [1576], [1577], [1655], [1801], [2125], [2126], [2127], [2128], [2129], [2130], [2131], [2260], [2428], [2599]), inplace=True) $
es = elasticsearch.Elasticsearch(es_url)
df_unit.head(2)
twitter_archive_master = twitter_archive_master.drop(twitter_archive_master[twitter_archive_master['rating_numerator'] <= 10].index)
top_10_authors = git_log['author'].value_counts().head(10).to_frame() $ top_10_authors
df2 = df.drop(df[(df['group'] == 'treatment') & (df['landing_page'] != 'new_page')].index) $ df2 = df2.drop(df2[(df2['group'] != 'treatment') & (df2['landing_page'] == 'new_page')].index) $ df2.head()
s = pd.Series() $ s
df['Duration'].plot.hist()
len(test_data)
print('Shifts with a pause: {}/{}'.format(len(df['pause'][df['pause'].astype('timedelta64[m]') != 0]), len(df)))
df_group = df2["group"] $ prob_new_page = (df_group == "treatment").mean() $ prob_new_page
d
weather_missing.sum()
station_bounds[station_bounds.station_id == choice].index[0]
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False]["user_id"].count()
df_archive_clean["source"] = df_archive_clean["source"].replace('<a href="https://about.twitter.com/products/tweetdeck" rel="nofollow">TweetDeck</a>', $                                                                "TweetDeck")
df.mean(1)
df_test3_promotions.head().T
X_train_all.shape
sum((austin['driver_rating']== austin['rider_rating']))
from sklearn.neural_network import MLPRegressor $ n_net = MLPRegressor(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(1000, 500, 250, 100, 50), $                       random_state=1, verbose = True) $ n_net.fit(x_train,y_train)
HMM('GOOG', datetime.datetime(2016,1,1), datetime.datetime(2016,5,1),True)
df2.query("user_id=='773192'")
autos = pd.read_csv("autos.csv", encoding='Latin-1') $ autos
df.dtypes
num_newpage = df2[df2['landing_page'] == 'new_page']['user_id'].count() $ num_total = df2['user_id'].count() $ p_newpage = num_newpage / num_total $ print('The probability of an individual receiving the new page: ', p_newpage)
grid.best_score_
t_cont_prob = df2[df2['group']=='treatment']['converted'].mean() * 100 $ output = round(t_cont_prob, 2) $ print("The probability of the treatment group individual converting regardless of the page they receive is: {}%".format(output))
countries_df = pd.read_csv('./countries.csv') $ countries_df.head() $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
unique_users_cnt=len(ab_data.user_id.unique()) $ unique_users_cnt
new_trump_df = trump_df.text.apply(clean_tweet)
custom = pd.get_dummies(auto_new.Custom) $ custom.head()
df_predictions_clean.p1_conf = df_predictions_clean.p1_conf*100 $ df_predictions_clean.p2_conf = df_predictions_clean.p2_conf*100 $ df_predictions_clean.p3_conf = df_predictions_clean.p3_conf*100
Genres=",".join(Genres).join(("",""))
import statsmodels.api as sm $ df2['intercept']=1.0 $ df2['ab_page']=pd.get_dummies(df2['group'])['treatment'] $ df2.head()
ethereum_github_issues_df[['title', 'html_url', 'created_at','updated_at']].head()
pvt = pvt.drop(['ga:dimension2', 'customerId'], axis=1) $ pvt = pvt[['ga:transactionId', 'ga:date', 'customerName', 'productAndQuantity']] $ pvt
plt.plot(ds_cnsm['time'],ds_cnsm['met_salsurf_qc_executed'],'b.') $ plt.title('CP01CNSM, OOI QC Executed SSS') $ plt.ylabel('Salinity') $ plt.xlabel('Time') $ plt.show()
goog.head()
logm2 = sm.Logit(df_con['converted'], df_con[['intercept', 'US', 'UK']]) $ result = logm2.fit() $ result.summary2()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ from scipy.stats import norm $ z_score,1-p_value/2 $
autos.rename(column_name,axis=1,inplace=True)
titanic.isnull().any()
new_page_converted/n_new-old_page_converted/n_old
charge = reader.select_column('charge') $ charge = charge.values # Convert from Pandas Series to numpy array $ charge
sum((df.group=='control') & (df.landing_page=='new_page')) + \ $ sum((df.group=='treatment') & (df.landing_page=='old_page'))
df2[df2['user_id']==773192] # checking whether dropping worked
model.wv.vectors.shape
df.isnull().any().any()
plt.hist(taxiData.Trip_distance, bins = 20, range = [50,150]) $ plt.xlabel('Traveled Trip Distance') $ plt.ylabel('Counts of occurrences') $ plt.title('Histogram of Trip_distance') $ plt.grid(True)
about=r.html.find('#about',first=True)
active_df = pd.read_sql("SELECT s.station, count(m.station) as station_count FROM measurements m, stations s WHERE m.station=s.station GROUP BY m.station", con=engine, columns=[["station"],["station_count"]])
points.name="WorldCup" $ points.index.name="Previous Points" $ points $
TestData_ForLogistic = dum.drop(['City_Code', 'Employer_Code', 'Customer_Existing_Primary_Bank_Code', 'Source', $                                  'DOB', 'Lead_Creation_Date'], axis=1)
pd.set_option('display.max_colwidth', 100)
df_members.hist(figsize=(15,12),color = 'y') $ plt.show()
op_add_comms['thanks'] = op_add_comms['body'].apply(lambda x: any(substring in x.lower() for substring in thanks))
two_day_sample.set_index('timestamp', inplace=True)
tweet_model = models.Word2Vec(sentences=tweet_hour['tweet_text'].apply(tweet_tokenizer.tokenize))
trip_data_sub["lpep_pickup_datetime"] = trip_data_sub.lpep_pickup_datetime.apply(lambda x: (pd.to_datetime(x) -  pd.datetime(2015, 9, 1))/ np.timedelta64(1, 's')) $ trip_data_sub["Lpep_dropoff_datetime"] = trip_data_sub.Lpep_dropoff_datetime.apply(lambda x: (pd.to_datetime(x) -  pd.datetime(2015, 9, 1))/ np.timedelta64(1, 's'))
rng[5].tz_localize('Asia/Shanghai')
os.remove(checkpoint_epoch_path)
uber_14.head()
df.drop(1899)
predictSVR = svr_rbf.predict(np.concatenate((X_train,X_validation,X_test),axis=0)) $ predictANN = ANNModel.predict(np.concatenate((X_train,X_validation,X_test),axis=0))#one can ignore this line if ANN is $
result['sourceId'].nunique()
time.gmtime()
df3.head(3)
import matplotlib.pyplot as plt $ %matplotlib inline
c.execute("SELECT state FROM cities") $ print(c.fetchall())
df.Date.dtype
players = player_p1.append(player_p2)
import numpy as np
model.train(documents, total_examples=len(df['body']), epochs=10)
for dfr in [roll_17, roll_90, roll_360]: $     dfr.drop('store_nbr', 1, inplace=True) $     dfr.reset_index(inplace=True) 
dfp.groupby("congress").bill_type.count().plot(kind='bar') $ plt.xlabel('Bills per Session')
pred10 = nba_pred_modelv1.predict(g10) $ prob10 = nba_pred_modelv1.predict_proba(g10) $ print(pred10) $ print(prob10)
tmp = data['deadline'] - data['launched_at'] $ data['duration'] = pd.to_timedelta(tmp, unit='D') $ data.head()
df_twitter_archive_copy.source.dtype
data['subreddit_other'] = [subred if (data['subreddit'] == subred).sum() > 3 else 'other' for subred in data['subreddit']]
X=df_con
grouped_by_origin = df3.groupby('Origin') $ type(grouped_by_origin)
df_npc = df2[(df2['landing_page'] == 'new_page')] $ conv = df_npc['converted'] $ new_page_converted = np.random.choice(conv, nnew)
from pandas.tools.plotting import scatter_matrix $ scatter_matrix(mean_sea_level, figsize=LARGE_FIGSIZE);
phone = "2004-959-559 # This is Phone Number" $ num = re.sub(r'#.*$', '', phone) $ print('Phone Num : {}'.format(num))
benchmark_n = 500000 $ from multiprocessing import cpu_count $ n_cores = cpu_count() $ print(f'benchmarking {benchmark_n:,} rows on {n_cores} cores.')
bnbAx[bnbAx['language']=='en'].country_destination.value_counts().plot.pie()
plt.figure(figsize = (6,6)) $ kmeans.fit_predict(X) $ plt.scatter(x_9d[:,0],x_9d[:,1], c = kmeans.labels_, cmap='rainbow')  $ plt.scatter(kmeans.cluster_centers_[:,1],kmeans.cluster_centers_[:,0], color='black')  
days
new_page_converted = np.random.binomial(n_new, p_new) $ new_page_converted
pub_bias = db.get_sql(sql) $ pub_bias.head()
from sklearn import linear_model $ from sklearn import datasets, linear_model, metrics $ from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict $ from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV $ lm = linear_model.LinearRegression()
train['Month']     = train["date"].dt.month $ train['Day']       = train["date"].dt.day $ train['DayOfWeek'] = train["date"].dt.dayofweek
df.info()
import eikon as ek
flight2.unpersist()
df_2008 = pd.DataFrame(rows)
test_bow, test_word_freq = get_bow(test_clean_token) $ train_bow, train_word_freq = get_bow(train_clean_token)
test_data = get_deepar_data("{}/test/test.json".format(s3_data_path)) $ pred_data = get_deepar_data("{}/train/train.json".format(s3_data_path))
p_diffs = [] $ for _ in range(10000): $     new_page_converted1 = np.random.binomial(1,P_new,n_new) $     old_page_converted1 = np.random.binomial(1, P_old, n_old) $     p_diffs.append(new_page_converted1.mean()-old_page_converted1.mean())
recs.ix['ibm us equity','best analyst recs bulk'].ix[:,'chg pct ytd'].mean()
import nltk $ from nltk import word_tokenize $ import pandas as pd $ import numpy as np $ from datetime import datetime
data[[name.endswith('bacteria') for name in data.phylum] & $     (data.value > 1000)] $ data
br_pr = pd.Series(brand_price) $ br_mi = pd.Series(brand_mileage)
print(bixi.head())
print('Features importances:') $ rfc_imp = rfc.feature_importances_ $ features_names = var_num + var_cat $ for i in range(0,len(rfc_imp)): $     print (features_names[i]+": %s" % "{0:.1}".format(rfc_imp[i]))
sensors_num_df = detective.NumericalSensors(db.master_df)
top_tracks.plot.bar() $ plt.xticks(np.array(range(0,5)),top_tracks.track_name ) $ plt.title('Top Tracks by Artist Followers')
most_yards.groupby("Jimmy").Jimmy.value_counts()
DataSet['userDesc']
rand_bg = np.random.uniform(low=2.5, high=19, size=(21,)) # create a data sample of random numbers $ rand_bg_s = pd.Series(rand_bg) $ bg_s = rand_bg_s.round(1) # reduce to one decimal places to it's a little easier to read $ bg_s # bg_s name confers 'bg' and 'Series', a nice short name that reminds you it's a Series
import seaborn as sns
z_score, p_value = sm.stats.proportions_ztest([convert_new,convert_old], [n_new,n_old],alternative='larger') $ z_score, p_value
date2= plt.plot_date(data=date_retweets2, x="created_at", y="retweet_count") $ plt.xticks(rotation=90) $ ax = plt.subplot() $ ax.xaxis.set_major_formatter(mdates.DateFormatter("%d-%m-%Y")) $
from IPython.core.interactiveshell import InteractiveShell $ InteractiveShell.ast_node_interactivity = "all"
cityID = '0e2242eb8691df96' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Henderson.append(tweet) 
df_new[['US', 'UK']] = pd.get_dummies(df_new['country'])[['US','UK']]
from sklearn.metrics import accuracy_score $ y_pred = rnd_clf.predict(X_test) $ accuracy_score(y_test, y_pred)
ip = ip_clean.copy()
m=learner.model $
pie_chart
bwd = df[['Store'] + columns].sort_index().groupby("Store").rolling(7, min_periods=1).sum()
x = df2[df2.user_id.duplicated()] $ x
first_year = first_year.text $ first_year
df_CLEAN1A.info()
tweetsDuringEvent = pd.read_sql("SELECT * FROM tweetCoords WHERE CREATED_AT >= DATE('2017-09-10') AND CREATED_AT <= DATE('2017-09-12')", Database().myDB) $ tweetsDuringEvent
print("The maximum change in one day is {}. The minimum change in one day is {}".format(max(change_values), min(change_values)))
df_subset.dtypes
import statsmodels.api as sm $ convert_old = df2.query('group == "control"')['converted'].sum() $ convert_new = df2.query('group == "treatment"')['converted'].sum() $ n_old = df2.query('group == "control"').shape[0] $ n_new = df2.query('group == "treatment"').shape[0]
date_df
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn; seaborn.set()
movies2000to2014['year']=pd.to_numeric(movies2000to2014['year'],errors='coerce')
(eug_cg_counts / eug_counts).unstack(fill_value=0).plot(kind='bar', stacked=True);
for x in prop.columns: $     print (x,prop[x].nunique(dropna = False))
url=("/Users/maggiewest/Projects/Portland_census.csv") $ portland_census = pd.read_csv(url) $ portland_census2=portland_census.drop("Fact Note", axis=1) $ portland_census2=portland_census2.drop("Value Note for Portland city, Oregon", axis=1)
import numpy as np $ import pandas as pd $ import os $ import h2o
list(zip(df_min.columns, [type(x) for x in df_min.ix[0,:]]))
NiceHist('age',df)
rural_ride_total = rural_type_df.groupby(["city"]).count()["ride_id"] $ rural_ride_total.head()
source_proc = processor(hueristic_pct_padding=.7, keep_n=15000) $ source_vecs = source_proc.fit_transform(source_docs)
treatment_converting = df2[df2['group'] == 'treatment']['converted'].mean() $ print('Probability of an individual in the treatment group converting:') $ print(treatment_converting)
risk(data,portfolio=True)
joined = joined[joined.priceClose!=0]
during['count'] = during.groupby('hashtags')['hashtags'].transform(pd.Series.value_counts) $ during.sort('count', ascending=False) $ during.hashtags.dropna().head()
len(pred_hacker_list)
query = 'SELECT count(distinct x.uid) FROM (SELECT uid FROM ways UNION ALL SELECT uid FROM nodes) as x' $ c.execute(query) $ results=c.fetchall() $ print results[0][0]
tbl['pr_rf'] = tbl['port_ret'] - tbl['rf'] $ tbl['mr_rf'] = tbl['mkt_ret'] - tbl['rf']
from sklearn.ensemble import RandomForestClassifier $ random_forests_params = { $         "randomforestclassifier__n_jobs" : [-1]} $ pipe = make_pipeline(IncidentPreprocessor(), RandomForestClassifier()) $ random_forests_grid = GridSearchCV(pipe, param_grid=random_forests_params, cv=3)
top_songs['Artist'].unique()
df.to_excel("msft_temp.xlsx")
free_data.describe()
from bs4 import BeautifulSoup $ r = requests.get('http://www.aflcio.org/Legislation-and-Politics/Legislative-Alerts') $ page = r.content $ page[:1000]
my_gempro.prep_itasser_modeling('~/software/I-TASSER4.4', '~/software/ITLIB/', runtype='local', all_genes=False)
df['user_id'].nunique()
new_page_converted = np.random.choice([0,1] ,size = n_new, p=[(1-p_new),p_new])
countries_df = pd.read_csv('countries.csv') $ countries_df.head(5)
df.loc[[11, 24,37], :]
gs.best_params_
df.query('converted == 1').user_id.nunique() / df['user_id'].nunique()
ls_not_numeric = [not pd.api.types.is_numeric_dtype(dtype) for dtype in df_onc_no_metac.dtypes] $ prog = re.compile('DATE[0-9]*$') $ ls_not_date = [not bool(prog.search(colname)) for colname in df_onc_no_metac.columns] $ ls_both = [num and date for num, date in zip(ls_not_numeric, ls_not_date)] $ df_onc_no_metac.loc[:,ls_both].nunique()
filename = processed_dir+'pulledTweetsProcessedAndClassified_df' $ gu.pickle_obj(filename,pulledTweets_df)
commits['log10_commits'] = np.log10(commits['commits'])
sess.get_data(['ibm us equity','aa us equity','vod ln equity'],['px last','px open'])
print(f"{urls[2]} returns:") $ ux.is_short(urls[2])
%%timeit $ scipy.optimize.broyden1(globals()[function_name], 2, f_tol=1e-14)
properati[properati['state_name'] == "Bs.As. G.B.A. Zona Norte"][['state_name','zone']]
obs_diff = df2.query('group == "treatment"')['converted'].mean() - df2.query('group == "control"')['converted'].mean() $ (obs_diff < p_diffs).mean()
knn = KNeighborsClassifier(n_neighbors = 3) $ knn.fit(X_train, Y_train) $ Y_pred = knn.predict(X_test) $ acc_knn = round(knn.score(X_train, Y_train) * 100, 2) $ acc_knn
time12MDict['health_bmiDF']
digits.target
corr = pandas.DataFrame(test.corr()) $ corr.to_csv(path_or_buf=path + '/fantasy_results_corr.csv')
docs_by_topic = tfidfnmf_topics.groupby('PrimaryTopic')
shannon_sakhalin = - sum(np.log2(sakhalin_freq.values) * sakhalin_freq.values) $ shannon_sakhalin
train_cols $
identify which 
doi_pid = pd.read_csv("crossref-pid-from-doi.csv", $                       dtype=str, $                       keep_default_na=False) $ pd.concat([doi_pid.head(), doi_pid.tail()])
X_test = count_vect.transform(df_test.text)
dfcounts['created_date'] = pd.to_datetime(dfcounts['created_date']) $ dfrecent['created_date'] = pd.to_datetime(dfrecent['created_date'])
inspector = inspect(engine) $ inspector.get_table_names()
rf = RandomForestClassifier(n_jobs = -1) $ k_fold = KFold(n_splits=5) $ cross_val_score(rf, X_features, tweets_1['sentiment'],cv = k_fold, scoring = 'accuracy', n_jobs=-1)
p_new=df2.query('converted==1').user_id.count()/df.shape[0] $ p_new
perturb = (np.random.randint(0, 20, size=N) - 10) * 0.25
tweet_json
from sklearn.model_selection import train_test_split
idxs = get_cv_idxs(n, val_pct=150000/n) $ joined_samp = joined.iloc[idxs].set_index("Date") $ samp_size = len(joined_samp); samp_size
for col_cells in ws.iter_cols(min_col=28, max_col=28): $     for cell in col_cells: $         cell.number_format = '0.00'
df['timestamp'] = pd.to_datetime(df['timestamp'])
airports_by_city = df2.groupby('OriginCityName')['Origin'].unique()
consumerKey = 'XXXXXXXXXXXXXX' $ consumerSecret = 'XXXXXXXXXXXXXX' $ auth = tweepy.OAuthHandler(consumer_key=consumerKey, consumer_secret=consumerSecret) $ api = tweepy.API(auth)
from sklearn.model_selection import KFold
offseason14["InorOff"] = "Offseason"
df3 = DataFrame([[1,2],[3,4]],columns=['c1','c2'],index=['r1','r2']) $ df3
df
most_common_registered_addresses.sort_values(ascending=False).head(10)
pandas.concat(sheets).info()
mb.head()
df.select((a == 0).alias('aIsZero')).show(5)
import statsmodels.api as sm $ from sklearn.metrics import mean_absolute_error
import gspread $ from oauth2client.service_account import ServiceAccountCredentials $ from operator import itemgetter
table_to_compare = pivot_discover_first $ source_table_to_compare = sales_data_clean
crs = {'init': 'epsg:4326'} $ geometry = df_TempJams['lineString'] $ geo_TempJams = gpd.GeoDataFrame(df_TempJams, crs=crs,geometry = geometry)
data.loc[data.surface_total_in_m2.isnull(), 'surface_total_in_m2'] = data['surface/price']*data['price_aprox_usd'] $ del data['surface/price']
output= "Update user SET following=50 where user_id='@Pratik'" $ cursor.execute(output) $
topics = 40
park['named_fda_drug'] = park.abstract.apply(named_fda_drug)
def Score(series): $     return series
print('\n'.join((f"{key}, {type(sheet)!r}" for key, sheet in sheets.items())))
np.exp(-0.0149), np.exp(-0.0408), np.exp(0.0099)
forecast_df['white_pop'] = 0 $ for ind, row in SANDAG_ethnicity_df[SANDAG_ethnicity_df['ETHNICITY'] == 'White'].iterrows(): $     forecast_df.loc[ind, 'white_pop'] = row['POPULATION'] $ forecast_df.head()
users = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/users.csv') $ sessions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/sessions.csv') $ products = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/products.csv') $ transactions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/transactions.csv')
relevant_pings.count()
... $ display(Markdown(fake_news_answer))
scr_activated_df = pd.DataFrame(index=daterange,columns=daterange)
set(tcga_target_gtex_labels.disease).intersection(treehouse_labels_pruned.disease)
!hdfs dfs -mkdir hw3
workspace_alias = 'lena_indicator'
tweets = pd.read_csv('https://raw.githubusercontent.com/organisciak/Scripting-Course/master/data/voldemort_tweets.csv') $ tweets.head()
options_frame.info()
pay_train = pd.read_csv('paiements_train.csv') $ print('Training data shape: ', pay_train.shape) $ pay_train.head(3)
fraud_df = pd.read_csv('Fraud_Data.csv') $ ip_df = pd.read_csv('IpAddress_to_Country.csv')
print(max(2, 3, 1))  # Multiple scalar args $ print(max([3, 2, 1])) # Single list arg $ print(max([3, 2, 1], 4))  # Not allowed
QUIDS_wide = QUIDS.pivot_table(index='subjectkey', columns = 'week', values='qstot') $ QUIDS_wide.reset_index(inplace=True) $ QUIDS_wide.columns $ QUIDS_wide.columns = ['subjectkey', 'qstot_0', 'qstot_12','qstot_14'] $
df_geo = pd.read_csv('https://www.aggdata.com/download_sample.php?file=nl_postal_codes.csv') $ df_geo.head()
df3['Donation Received Month'] = df3['Donation Received Date'].dt.month $ df3['Donation Received Year'] = df3['Donation Received Date'].dt.year $ df3['Donation Received Month-Year'] = df3['Donation Received Date'].dt.to_period('M') $ df3.sort_values(by='Donation Received Date', inplace=True, ascending= True)
(df['converted'] == 1).sum()/rows[0]
newgeo.sort_index(inplace=True)
most_informative_features_top_and_bottom(vectorizer=vectorizer, classifier=rdg, binary=False, n=15)
for df in (joined,joined_test): $     df.loc[(pd.to_timedelta(df.CompetitionDaysOpen).dt.days)<0, "CompetitionDaysOpen"] = 0 $     df.loc[df.CompetitionOpenSinceYear<1990, "CompetitionDaysOpen"] = 0 $ print(type(df.CompetitionOpenSinceYear)) $ print(type(df.CompetitionDaysOpen))
fail_df = input_df[df['Match']=='No_Match'].copy() $ fail_df['street'] = fail_df['street'].str.replace('\b(?:apt *)?(?:no|#) (?:- *)? *[a-z0-9-]','')
df_DST
session.query(Adultdb).filter_by(education="9th").delete(synchronize_session='fetch')
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],alternative = 'smaller') $ z_score,p_value
print(calc_temps('2017-05-10','2017-05-13'))
output.summary()
df
ss = StandardScaler() $ Xt = ss.fit_transform(X) $ type(Xt) $ Xt
dataset["article_publisher_id"].unique().size - 1 # -1 to remove the empty
p_diffs = np.array(p_diffs) $ (p_diffs > actual_diff).mean() $
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
bnbAx[bnbAx['language']=='fr'].country_destination.value_counts().plot.pie()
import textstat.textstat
top_5_beats = cfs_df['Beat'].groupby(cfs_df['Zip']).value_counts() $ top_5_beats.groupby(level=0).nlargest(5)
print(data.info())
logit_mod1 = sm.Logit(df_new['converted'], df_new[['intercept', 'CA','US']]) $ result1 = logit_mod1.fit() $ result1.summary()
project_gdb = r'utah.gdb' $ collisions_path = os.path.join(project_gdb,'collisions_joined') $ road_features_path = os.path.join(project_gdb,'static_features')
ann_ret_SP500[9].describe()
dr.get('https://ya.ru')
customer_visitors_new.index.levels[1]
tw_sample_df.created_at
!curl -I 'http://www.scielo.br/scielo.php?script=sci_arttext&pid=1807-57622016.0103&lng=en&nrm=iso&tlng=pt&debug=xml'
aaplA = aapl[['Adj Close']] $ pd.concat([msftAV, aaplA])
len(df)
df2.user_id.nunique(), df2.shape
crime_wea.loc[:, ['Arrest', 'Domestic']]=crime_wea.loc[:, ['Arrest', 'Domestic']].astype('bool')
df.query('converted == 1').count()[0]/df.count()[0]
import pickle $ filename = 'IMDB_model.sav' $ pickle.dump(gs_lr_tfidf, open(filename, 'wb')) $
df.head()
len(team_list)
df5[(df5.priority=='B') & ((df5.ra>315) | (df5.ra<195))].sort_values(by='ra')
doc_top_mat = pd.DataFrame(doc_top) $ doc_top_mat['TopicNum'] = doc_top_mat.apply(np.argmax, axis=1)
df_clusters.columns=['cluster']+list(df.columns)+['text'] $ df_clusters['cluster_cat']=pd.Categorical(df_clusters['cluster']) $ df_clusters.head()
my_stream_listener = PrintingStreamListener() $ my_stream = tweepy.Stream(auth = api.auth, listener=my_stream_listener)
print('Repeated user_id in df2 dataframe is:{}'.format(df2[df2.user_id.duplicated(keep=False)].iloc[0,0]))
station_count = df['station'].count() $ station_count
b_rev.shape
random_forests_grid.fit(X_train,y_train) $
cols
options = {'Company': 'name', 'Person': 'name', 'Fund': 'name'} $ draw(objects, options, physics = True)
classes.head(1)
sgd = SGDClassifier(loss="log", alpha=0.01, $                                          learning_rate='optimal')
df["booking_user_agent"].value_counts()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth)
for idx,file in enumerate(files): $     dat = pd.read_excel(dir+file) $     if idx == 0: $         datAll = dat $     datAll = pd.concat([datAll,dat])
df_similar_items = content_rec.get_similar_items().to_dataframe() $ df_similar_items.head(20)
for word in sorted(fdist): $     print('{0}->{1};'.format(word, fdist[word]), end=' ')
print("\nClassification Report:\n",classification_report(y_test, y_pred)) $
autos.describe(include="all")
[series.name for series in get_descendant_column_data(observations_node)]
s = so['score']
testdata = np.load(outputFile) $ data = pd.read_csv(inputFile, delimiter=',', header=None)
df.columns
list(Measurement.__table__.columns)
(df.query('group == "treatment" & landing_page != "new_page"').converted.count() + $  df.query('group == "control" & landing_page != "old_page"').converted.count())
twitter_Archive.duplicated()
df.head()
print('No. of rows in dataset:',df.shape[0]) 
raw.education.head()
grouped_df = surveys_df.groupby(['sex','year'])['wgt'].mean().unstack() $ grouped_df.plot(kind="bar") $ plt.xlabel("Sex") $ plt.ylabel("Average Weight") $ plt.show()
data_df = pd.read_csv('dropbox/github/thinkful/unit1/data/lecz-urban-rural-population-land-area-estimates_continent-90m.\ $ csv', encoding='windows-1252') 
train_data.columns
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=8000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
print fullDf.shape[0] $ fullDf.region.value_counts()
Pnew = df2['converted'].mean() $ Pnew
plt.clf()
sum_columns=result_mwu.weights.sum(axis=0) $ mask_weight=sum_columns>0 $ result_mwu.weights.loc[:,mask_weight]
fav_max = np.max(data['Likes']) $ rt_max = np.max(data['RTs']) $ fav = data[data.Likes == fav_max].index[0] $ rt  = data[data.RTs == rt_max].index[0]
new_page_converted = np.random.choice([0, 1], size=n_new, p=[(1-mean_of_p), mean_of_p])
woba_leaderboard = df.groupby(['batter_name', 'batter'])['woba_value'].agg(['mean', 'count']) $ woba_leaderboard.loc[woba_leaderboard['count']>100,].sort_values('mean', ascending=False).head()
for i in [lds.dataframes, lds.tables, lds.raw_data]: $     print(i, '\n')  # pprint does not workon lazy-loaded dicts
df_Left_trans_users = pd.merge(transactions,users,how="left",on="UserID") $ df_Left_trans_users
data = pd.read_csv('data_science_challenge_samp_18.csv',parse_dates=[1],keep_date_col = True)
c.execute("UPDATE cities SET state='Californ-I-A' WHERE state='CA'") $ conn.commit() $ print(c.fetchall())
price2017.head()
ser6=pd.Series([1,2,3,4,5,6]) #example of summation $
print('# of unique riders: ' + str(len(set(austin['rider_id']))) ) $ print('# of unique drivers: ') + str(len(set(austin['active_driver_id'])))
df2.query('landing_page == "new_page"').count
temps1 = Series([80, 82, 85, 90, 83, 87], $                 index = dates) $ temps1
best_150_cols = X_train.columns[gs_k150.best_estimator_.named_steps['kbest'].get_support()] $ k150_coef = gs_k150.best_estimator_.named_steps['logreg'].coef_ $ pd.DataFrame(k150_coef, columns=best_150_cols).T.sort_values(0, ascending=False).head()
AAPL.info()
to_be_predicted_Day4 = 52.53461933 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
pd.Series.iloc?
df2 = df.iloc[df.query('group == "treatment" and landing_page == "new_page"').index.values]
p_diffs=[] $ for _ in range(10000): $     s_new_t=np.random.binomial(1,pnew,nnew).mean() $     s_pold_t=np.random.binomial(1,pold,nold).mean() $     p_diffs.append(s_new_t-s_pold_t)
grp[['Age','Salary']].transform('sum')
autos["kilometer"] = autos["kilometer"].str.replace(",","") $ autos["kilometer"] = autos["kilometer"].str.replace("km","") $ autos["kilometer"] = autos["kilometer"].astype(int) $ autos["kilometer"].head(3)
columns_dailySummary = ['jamNum','irregularNum','timeStamp']
Station_data = session.query(Measurement).first() $ Station_data.__dict__
s.holidays()
autos.describe()
data_jsn=[x for x in tweets_data.find({}, $                      {'favorite_count':1,'retweet_count':1,'created_at':1,'entities.hashtags':1,'entities.urls':1, $                       'entities.media':1,'_id':0})]
forest = RandomForestClassifier(max_depth = 10, n_estimators=5, random_state=42) $ forest.fit(X_train, y_train) $ forest.score(X_test, y_test)    
lm_country = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US']]) $ reg_country = lm_country.fit()
countries.user_id.nunique()
sns.distplot(df['comms_num']); $
df_enhanced.head(1)
df_h1b_nyc_ft.groupby(['lca_case_employer_name'])['total_workers'].sum()
df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'],format='%Y-%m-%d %H:%M:%S')
cercanasA1_11_14Entre100Y125mts = cercanasA1_11_14.loc[(cercanasA1_11_14['surface_total_in_m2'] >= 100) & (cercanasA1_11_14['surface_total_in_m2'] < 125)] $ cercanasA1_11_14Entre100Y125mts.loc[:, 'Distancia a 1-11-14'] = cercanasA1_11_14Entre100Y125mts.apply(descripcionDistancia, axis = 1) $ cercanasA1_11_14Entre100Y125mts.loc[:, ['price', 'Distancia a 1-11-14']].groupby('Distancia a 1-11-14').agg(np.mean)
client.repository.list_experiments()
df2['ab_page'] = pd.get_dummies(df['group'])['treatment'] $ df2.head()
mb.sum(level='Taxon')
df_clean = df_clean.dropna()
from sklearn.metrics import jaccard_similarity_score $ from sklearn.metrics import log_loss
n_time_series = len(df.groupby(['unit', 'item'])) $ n_time_series
columns = inspector.get_columns('measurement') $ for c in columns: $     print(c['name'], c["type"])
expAndCoinByUser = firstWeekUserMerged.groupby('userid')["exp","coin"].sum().reset_index() $ print(expAndCoinByUser.head(5)) $ print(expAndCoinByUser.shape)
from sklearn.tree import DecisionTreeClassifier $ DT = DecisionTreeClassifier(criterion="entropy", max_depth = 4) $ DT.fit(X_train,y_train) $ DT
rng
class_merged=pd.merge(class_merged,stores,on=['store_nbr'],how='left') $ print("Rows and columns:",class_merged.shape) $ pd.DataFrame.head(class_merged)
df_ab_cntry[['UK','US','CA']] = pd.get_dummies(df_ab_cntry['country'])
df['2015-04-01']
plt.plot(spks[:, 0], spks[:, 1], '.k') $ plt.xlim(0, config_file['run']['tstop']) $ plt.ylim(-1, 7.5) $ plt.xlabel('Time (ms)') $ plt.ylabel('Neuron Number')
from sklearn.dummy import DummyClassifier
df_ml_58 = df.copy() $ df_ml_58.index.rename('date', inplace=True) $ df_ml_58_01=df_ml_58.copy()
p_new = df2.query("converted==1").user_id.nunique()/df2.user_id.nunique() $ p_new
age_gender.shape $ age_gender.head(5) $
x_min_max = pd.DataFrame({'x': [df['x'].min(), df['x'].max()]}) $ x_min_max
crimes.describe(include=['O'])
len(all_time_statements_and_records_company_numbers)
df.index[0]
df_hate = pandas_from_hashtag('hate', 10) $ df_hate.head()
dir(all_tweets[0])
base_url = "https://en.wikipedia.org" $ index_ref = "/wiki/List_of_accidents_and_incidents_involving_commercial_aircraft" $ index_html = urlopen(base_url + index_ref) $ soup = BeautifulSoup(index_html, "lxml")
df['created_at']=pd.to_datetime(df['Created Date'], format="%m/%d/%Y %I:%M:%S %p") $ df['created_at']
mini_df = data[['Order_Qty','wait','month']] $ mini_df['wait'] = pd.to_numeric(mini_df['wait'])
html = browser.html $ soup = bs(html,'html.parser') $ get_full_img = browser.find_by_xpath('//*[@id="fancybox-lock"]/div/div[2]/div/div[1]/a[2]').first.click() $ time.sleep(3)  #allow time for page to load $
df_max['date'] = df_max['date'].dt.date $ df_count['date'] = df_count['date'].dt.date $ df_mean['date'] = df_mean['date'].dt.date
df_students.columns
json_ex1 = json_df.iloc[0,:].to_json() $ json_ex2 = json_df.iloc[1,:].to_json() $ print json_ex1 $ print $ print json_ex2
org_id = '865de3da-27e1-4c88-8321-f47fa523a49a'
uber_14["hour_of_day"].value_counts()
xgb = XGBClassifier(objective='binary:logistic') $ xgb.fit(X_train, y_train) $ test_predictions = xgb.predict(X_test) $ eval_sklearn_model(y_test, test_predictions, model=xgb, X=X_test)
station_count = session.query(func.count(distinct(Measurement.station))).all() $ station_count $
from sklearn.svm import LinearSVR $ model = LinearSVR() $ print ('SVR') $ SVR_model = reg_analysis(model,X_train, X_test, y_train, y_test)
datetime_count_times = {turnstile: [[rows[i][0], $                                      rows[i+1][1] - rows[i][1], $                                      rows[i+1][0] - rows[i][0]] $                                     for i in range(len(rows) - 1)] $                         for turnstile, rows in datetime_cumulative.items()}
print bigram_text[10:20]
old_page_converted=np.random.binomial(1,Conversion_Rate,Conversion_No1)
misk_df.head(3)
prob_control = (df2.query('group=="control"')['converted']==1).mean() $ print('Probability converted given that individual in control group: ' + str(prob_control))
Find_Missing_Values(election_data)
from pandas_datareader import data as pdr
df_2017.dropna(inplace=True) $ df_2017
metadata['reflectance_scale_factor'] = float(refldata.attrs['Scale_Factor']) $ metadata
autos[autos.registration_year > 2016]
df_tweet_clean = df_tweet.copy() $ df_image_clean = df_image.copy() $ df_clean       = df.copy()
frames1 = [first_values, last_values, count] $ result1 = pd.concat(frames1, axis=1) $
crimes = pd.read_csv('../../data/chicago_past_year_crimes.csv')
data = pd.DataFrame(Rationales,columns=["TweetID","Rationales"])
cr_under_null = df2['converted'].mean()    # Same as above: under the null, use both groups. $ cr_under_null $
print (" The text: %s \n The grade in rating_numerator: %.1f \n" % (df_clean['text'].ix[695], df_clean['rating_numerator'].ix[695]))
ff3 = pd.read_csv('FF3Factors_Monthly.csv', parse_dates=['Date'])
sub_dataset.groupby(["NewsDesk", "Popular"]).size()
lo = sm.Logit(df2['converted'], df2[['intercept','ab_page']])
df_new.groupby(['country','landing_page']).converted.mean()
X = bow_transformer.transform(X)
df1 = df1.rename(columns={'date': 'Tweet_Count'})
mean2=df2['converted'].mean() $ mean2
rows_todrop_1=[i*5 for i in range(int(len(poverty)/5))]
genre_vectors.shape
with open('./data/out/xport_demo.xpt', 'wb') as f: $     xport.from_dataframe(_xport_dm2, f)
tweet_source_hist = pd.crosstab(index=tweets_df["tweet_source"], columns="count") $ tweet_source_hist['source_freq'] = tweet_source_hist['count'] * 100 / tweet_source_hist.sum()['count'] $ tweet_source_hist = tweet_source_hist.sort_values('source_freq', ascending=False) $ tweet_source_hist.head(15)
to_be_predicted_Day3 = 48.31596998 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
data = allocate_equities(allocs=[0,0,0.49185487,0.50814513]) $ data.plot() $ plt.show()
stream_measures.sort_values(by=['mean_occurances'], ascending=False).head(n=15)
converted = df.converted.value_counts() $ print("{:.2f}".format(converted[1] / df.user_id.nunique()))
df[df['Descriptor'] == 'Loud Music/Party'].index.hour.value_counts().head()
df.mean(1)
twitter_final.shape
all_patents_df.number_of_claims.describe()
plt.scatter(cdf.CYLINDERS, cdf.CO2EMISSIONS, color='blue') $ plt.xlabel("Cylinders") $ plt.ylabel("Emission") $ plt.show()
rfc = RandomForestClassifier(n_estimators= 600, max_depth = 8) $ rfc.fit(X, y) $ y_score_rfc = rfc.predict_proba(X)[:,1] $ metrics.roc_auc_score(y, y_score_rfc)
dfM.head(3)
! wc -l data/tweets.csv
data.describe()
J, grad = backprop(params, input_size, hidden_size, num_labels, X, y_onehot, learning_rate)  $ J, grad.shape $
df_more[df_more.Engineer != 0]
total_base_dict_by_place = base_dict_by_place.copy()
deaths=dropped.loc['Total deaths (confirmed + probables + suspects)'] $
tips_sentiment.head()
topics_by_time(period=topics_data.timestamp.dt.weekofyear, period_str='Week Number')
tweet_json = pd.read_json('tweet_json.txt',lines=True) $ tweet_json.to_csv('tweet_json.csv')
vectorizer = TfidfVectorizer(ngram_range=(1,3))
data[(data.phylum.str.endswith('bacteria')) & (data.value>1000)]
events.describe()
print 'A (no fillna() and values cut 18<=x<80):' $ print bnbA.shape $ print 'B (values cut 18<=x<80):' $ print bnbB.shape
donors.loc[donors['Donor Zip'].isnull(), 'Donor State'].value_counts(normalize=True).head()
db.collection_names(include_system_collections=False)
data2 = copy.deepcopy(df[['dates','sales']])
df_users_5=pd.merge(df_users_4,pt_unit_resume,left_on='uid',right_on='uid',how='left') $ df_users_5.loc[df_users_5['no_units_resume'].isnull(),'no_units_resume']=0 $ df_users_5.loc[df_users_5['units_resume_flag'].isnull(),'units_resume_flag']='Did not start any unit'
number = 100 $ factor = 1.1 $ text =  "hello world"
sp500.index
data.to_csv("csvs/datosAgregados.csv", index = False)
stocks
tokenizer = Tokenizer() $ tokenizer.fit_on_texts(message) $ sequences = tokenizer.texts_to_sequences(message)
df_unit.shape
g = sns.FacetGrid(bb_df, col="Pos", margin_titles=True) $ g.map(plt.hist, "Wt", color="steelblue") $ plt.subplots_adjust(top=0.5) $ g.fig.suptitle("Distribution of NBA Player Weight by Position") $ plt.show()
merged_data['invoices_creation_date'].min() # 4th of Jan, 2016 - start date
stage_summary.stage
p_old = df2['converted'].mean() $ print("Convert rate for p_old :", p_old)
temp_df2 = Ralston.TMAX $ for i in range(10): $     temp_df2= temp_df2 * 2 $     print ("Iteration",i+1,":", temp_df2.mean(), temp_df2.std())
pd.set_option('display.max_columns', 50) $ tt_final.head()
ET_Combine_Graph = ET_Combine.plot(color=['blue', 'green', 'orange']) $ ET_Combine_Graph.invert_yaxis() $ ET_Combine_Graph.scatter(xvals, df_gp_hr['Observation (aspen)'], color='black') $ ET_Combine_Graph.set(xlabel='Time of day (hr)', ylabel='Total evapotranspiration (mm h-1) ') $ ET_Combine_Graph.legend()
customer_emails.sort_values('Paid at', inplace=True) $ customer_emails['Previous Transaction'] = customer_emails.groupby(['Email'])['Paid at'].shift() $ customer_emails['Days Between'] = customer_emails['Paid at'] - customer_emails['Previous Transaction'] $ customer_emails['Days Between Int'] = (customer_emails['Days Between'].dropna()/ np.timedelta64(1, "D")).astype(int) $ customer_emails['Buy Count'] = customer_emails.groupby(['Email'])['Paid at'].cumcount()+1
t[np.argmax(sp_rec)]
df_user = df_user.set_index('user_id') $ df_churn = np.floor((pd.to_datetime(df_user['last_seen'])  - pd.to_datetime(df_user['registration_date'])).dt.total_seconds()/(3600*24*30)) $ df_churn = df_churn.rename('churn_leg') $ df_churn.head()
siteGroup = nitrodata.groupby('MonitoringLocationIdentifier')['Year'] $ siteInfo = siteGroup.agg(['min','max']) $ siteInfo.columns = ['StartYear','EndYear'] $ siteInfo['Range'] = siteInfo['EndYear'] - siteInfo['StartYear'] $ siteInfo
dfData.shape
df3['timestamp'] = pd.to_datetime(df3['timestamp']) $
reviews_w_sentiment['sentiment_score_scaled'] = reviews_w_sentiment['sentiment_score']*5 + 5 $ reviews_w_sentiment[['sentiment_score_scaled', 'sentiment_score']].describe() 
stars.created_at = pd.to_datetime(stars.created_at) $ stars['starred'] = 2
df[df.donor_id == '_1D50SWTKX'].sort_values(by='activity_date').tail()
autos = autos[autos.registration_year.between(1920,2016)] $ autos.shape
vectorizer2 = CountVectorizer(tokenizer=tokenize_and_stem)
emojis_db=pd.read_csv('emojis_db_csv.csv') $ emojis_db.head()
fuel_xs = fuel_rxn_rates / flux $ fuel_xs.get_pandas_dataframe()
convert_old_ct = df2.query("group=='control'and converted==1")['user_id'].count() $ print(convert_old_ct) $
country=pd.read_csv('countries.csv')
data.loc[1]
help(h2o)
display(df.columns, df.head())
FIGURE_PREFIX = '../figures/'
indices_of_interest = df_protest.loc[df_protest.TownCity_Name=='Cape Town'].head().index $ df_protest.loc[indices_of_interest]
np.arange(10, 50)
plt.scatter(X2[:, 0], X2[:, 1])
r.summary()
merge_event = df_event.merge(df_user, on="user_id", how = 'inner') $ merge_event = merge_event.loc[(merge_event.event_name == "login")]
mmileage_series = pd.Series(mean_mileage_by_brand).sort_values(ascending=False) $ mmileage_series
education.drop([702, 703, 704, 705, 706], inplace=True)
to_be_predicted_Day2 = 38.62079858 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
tipsDF.shape
right(np.array([0, 1]), np.array([1, 0]))
train.bathrooms.describe()
sm.stats.proportions_ztest([converted_new, converted_old],[n_new, n_old],alternative ='larger')
creations.groupby("creator_autoconfirmed").size()
Measurements = Base.classes.measurements $ Stations = Base.classes.stations
[[scores.loc[:,[row]].mean(),scores.loc[:,[row]].sum()]for row in scores]
tweet_archive.info()
path = os.path.join('Final emission factors.csv') $ ef = pd.read_csv(path, index_col=0)
group=merged.groupby(['date','store_nbr','class','family'],as_index=False) $ class_sales=pd.DataFrame(group['unit_sales'].agg('sum')) $ pd.DataFrame.head(class_sales)
fm.head()
sqlContext = SQLContext(sc) $ sdf = sqlContext.read.csv('production.csv', header=True, inferSchema=True) # requires spark 2.0 or later $ print ('Number of rows: ' , sdf.count()) $ sdf.printSchema() $ sdf.show()
import pandas as pd $ multiple_party_votes_all['votes_hour'] = [x.hour for x in pd.to_datetime(multiple_party_votes_all['time'], format = '%H:%M:%S')] $ bins = [0, 5, 12, 18, 22, 24] $ labels = ['early_morning', 'morning', 'afternoon', 'evening', 'night'] $ multiple_party_votes_all['votes_time_of_day'] = pd.cut(multiple_party_votes_all['votes_hour'], bins = bins, labels = labels)
z=recoveries #repaid_loans_cash $ z[z.fk_loan==loan_test].groupby(['dcf']).payment.sum()
sessions.action_type.value_counts()
past_year = dt.date(2017, 8, 23) - dt.timedelta(days=365) $ past_year
tweet_archive_clean['new'].unique()
datetime.now().date()
control_conv = df2[df2['group'] == 'control'].converted.mean() $ df_grp = df.groupby('group') $ df_grp.describe() $
df['time_detained'] = df['release_dt'] - df['booking_dt']
df.drop(['County', 'Category Name', 'Vendor Number', 'Item Description', 'Volume Sold (Gallons)'], axis=1, inplace=True)
train = pd.read_csv("./train-1.csv") $ train.head(10)
compound_df = pd.DataFrame(save_compound_list) $ compound_df = compound_df.transpose() $ compound_df.head()
sns.set_style("darkgrid") $ plt.figure(figsize=(8, 4)) $ maint['comp'].value_counts().plot(kind='bar') $ plt.ylabel('Count')
ser5.loc[["a","b"]]
proj_df['Project Subject Category Tree'].value_counts()
df_new1.shape[0]+df_new2.shape[0] $ print("The number of times the new_page and treatment don't line up are: {}".format(df_new1.shape[0]+df_new2.shape[0]))
df.to_csv("sentiment_dataframe.csv")
df['20170105':'20170111']
print(df['fault_code_type_3',].head(3)) $
a = df.groupby(['group', 'landing_page']).count() $ print(a) $ print('\n Total rows/items dont line up is {}'.format(a.iat[0,1] + a.iat[3,1]))
df2.query('group =="control"')['converted'].mean()
archive_copy.info()
rounds.hist(column = 'announced_year',bins = 10, figsize = (20,8))
eval("print('hello')") # converts string to object and runs it $
xgb_model = xgb.XGBRegressor(random_state=1)
pd.merge(df1, df3)
unique_domains = group_on_field_with_metrics('domain') $ unique_domains.head()
models_dict = visual_recognition.list_classifiers(verbose=True); models_dict
brands = map(brandDifferences, brand_names) $ models = map(modelDifferences, model_names)
print(final_results)
pd.DataFrame({'one' : pd.Series([1.]), 'two' : pd.Series([1., 2.], index=['a', 'b'])}, columns = ['one', 'two', 'three'])
active_ix = [a for a in USER_PLANS_df.index if a not in churned_ix]
data = pd.read_csv("BreastCancer.csv") $ data.head()
df_selected.cache()
new_page_converted = new_page_converted[:145310]
youtube_urls = {} # key will be title/artist and value will be YouTube URL $ for title, artist in unique_title_artist[:min(batch_size, len_unique_title_artist)]: $     youtubeurl = urllib.parse.quote_plus(YOUTUBE_URL_TEMPLATE.format(gc.search(title +' '+artist)), safe='/:?=') $     youtube_urls[str((title, artist))] = youtubeurl
df2['intercept']=1 $ df2[['control','ab_page']]=pd.get_dummies(df2['group']) $ df2 = df2.drop('control', axis = 1)
import qgrid $ qgrid.set_grid_option('forceFitColumns', False) $ qgrid.set_grid_option('editable', False)
lm=sm.OLS(df2_by_day['new_rate'], df2_by_day[['intercept', 'day']]) $ results=lm.fit() $ results.summary()
type2017.isnull().sum()
mask = (calls_nocontact_simp['ticket_closed_date_time'] > '2017-1-1') & (calls_nocontact_simp['ticket_closed_date_time'] <= '2017-12-31')
final.tail(3)
for item in all_objects['Contents']: $     print(item['Key'])
result_list
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True) $ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True) $ df.info()
oppstage = segmentData[['lead_mql_status', 'opportunity_month_year', 'opportunity_stage']].pivot_table( $                 index=['lead_mql_status', 'opportunity_month_year'], $                 columns='opportunity_stage', aggfunc=len, fill_value=0 $             )
new_reps.newDate[new_reps.Cruz.isnull()]
print(list(festivals.columns.values))
if cl_ca.loc[(cl_ca.duplicated('APP_APPLICATION_ID')==True)].shape[0]==0: $     ca_de_xml = ca_de.copy() $     ca_de_xml = ca_de_xml.merge(cl_ca, how='left', on='APP_APPLICATION_ID') $ else: $     print("Duplicated application id in XML data")
trump.head()
from scipy.stats import norm $ z_sig = norm.cdf(z_score) # Tells us how significant our z-score is $ crit_val = norm.ppf(1-(.05/2)) # Tells us what our critical value at 95% confidence is $ print(z_sig, crit_val)
remove_ms = lambda x:re.sub("\+\d+\s","",x) $ mk_dt = lambda x:datetime.strptime(remove_ms(x), "%a %b %d %H:%M:%S %Y") $ sentiment_df["Tweet date"] = [mk_dt(d) for d in sentiment_df["Tweet date"]] $ sentiment_df["Tweets ago"] = sentiment_df["Tweets ago"].astype(int)
np.median(investors_df.investment_count)
df_c.head()
my_string = "Hi world" $ re.sub("Hi", "Hello", my_string) $ print("[1] my_string is unchanged: " + str(my_string)) $ my_string = re.sub("Hi", "Hello", my_string) $ print("[2] my_string is changed after variable assignment: " + str(my_string))
LSI_model = LSI.train(corpus[:-100])
new_page_conversions = np.random.binomial(n_new,p_new,10000) $ old_page_conversions = np.random.binomial(n_old,p_old,10000) $ p_diffs = [(x/n_new) - (y/n_old) for x, y in zip(new_page_conversions, old_page_conversions)]
movies = pd.read_csv('../../data/raw/movies.csv',  encoding='latin-1') # Loading in the dataset
StockData = pd.read_csv(StockDataFile + '.gz', index_col=['Date'], parse_dates=['Date']) $ StockData.head(10)
highest = max(dataset["Open"]) $ lowest = min(dataset["Open"]) $ print("Highest Opening Price : " + str(highest) + "\n" + "Lowest Opening Price : " + str(lowest))
%matplotlib notebook $ df1 = var_per_portfolio.toPandas() $ df2 = df1.set_index(['date', 'portfolio']) $
quarters.asfreq("M")
import requests $ import collections $ mydata = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=8F4u-pRdtQsDCn-fuBZh")
train.shape
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) $ model = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2)
crop = fields_node['crop'][DATA]
len(label_rows)
pickle.dump( train, open("../Data/train.csv", "wb") )
print 'Percentage of amount for unknown (YY) state : {:.2f}'.format(100*df[df.state == 'YY'].amount.sum()/df.amount.sum())
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller') $ z_score, p_value
lst = [1, 2, np.nan, 3] $ for i in lst: $     print(i==i)
metrics.homogeneity_score(labels, km.labels_)
df_clean3.loc[1254, 'text']
old_page_converted =  np.random.binomial(1, p = p_old,size = n_old) $ old_page_converted
p_old = df2['converted'].mean() $ p_old
tfidf = TfidfModel(dictionary=dictionary) $
print(pd.Series.unique(df1['Area'])) $ print(pd.Series.unique(df1['Variable Name'])) $ print(pd.Series.unique(df1['Year']))
posts.groupby('PostTypeId').count()
df_concensus.head(20)
!pip3 install Pillow==4.0.0
with open('D:/CAPSTONE_NEW/indeed_com-jobs_us_deduped_n_merged_20170817_113515380318786.json', encoding="utf-8-sig") as data_file: $     json_data3 = j.load(data_file)
na_df.loc["a", "two"] = None # fills as NaN since replacing an int $ na_df.loc["a", "three"] = np.nan # fills as NaN $ na_df
list1 = list(df2_clean.p1.unique()) $ list2 = list(df2_clean.p2.unique()) $ list3 = list(df2_clean.p3.unique()) $ full_list = list1 + list2 + list3 $ any(word[0].islower() for word in full_list)
train_df.info()
df.loc['r1':'r2','B':'C']
print (df2.set_index('user_id').index.get_duplicates()) $ df2.count()
master_copy = archive_clean.copy()
joined = joined[joined['Monetary_score'] <=5]
vol = vol.fillna(0.) $ vol.head(20)
loans_plan_origpd_xirr[350]
len(full_df.listing_id.unique())
high_ranked_answers_threshold = np.percentile(answers_scores, 90) $ high_ranked_answers_threshold
ss.fit(X)
import pandas as pd $ import numpy as np
df.iloc[99,3]
new_query = "SELECT id, recording_date, stage1_reviewer_id, workflow_state FROM ephys_roi_results" $ err_df = get_lims_dataframe(new_query) $ err_df.head()
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ logres.summary()
students.weight
room_temp.__delattr__('temperature')
df1 = df.drop(a_mismatch.index) $ df2 = df1.drop(b_mismatch.index)
to_be_predicted_Day3 = 43.12236678 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
format = lambda x: x['E']+x['F'] $ df2.apply(format,axis=1)
frames = [df_prep17_, df_prep16_, df_prep15_, df_prep14_, df_prep13_, df_prep12_, df_prep11_, df_prep10_, $          df_prep9_, df_prep8_, df_prep7_, df_prep6_, df_prep5_, df_prep4_, df_prep3_, df_prep2_, df_prep1_, $          df_prep0_, df_prep99_, df_prep98_] $ df = pd.concat(frames)
lr2.score(new_x, new_y) # the result hasn't improved
saveTweetDataToCSV(input_hash_tag.value)
df_members['bd_c'] = le.transform(df_members['bd_c'])
bookings.columns
print("Training took {:.2f}s".format(t1 - t0)) $
from lxml import etree $ from io import StringIO, BytesIO
weather.hail.value_counts()
from pymer4.models import Lmer $ mres = Lmer("accuracy~1+et+(1+et|subject/block)",data=raw_large_grid_df) $ mres.fit()
pf_rdd = sc.parallelize([('P1', 'RF1', 1.), ('P1', 'RF2', 2.), ('P2', 'RF1', 0.2), ('P2', 'RF2', -0.8)]) $ dfpf = sql.createDataFrame(pf_rdd, ['portfolio', 'rf', 'qty'])
z_sc, p_val = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], value=0.0, alternative='larger') $ z_sc, p_val
equipment.columns
group_df = df.groupby('group') $ group_df.describe()
df.describe()
autos['odometer_km'].unique()
twitter_data.tweet_id.unique().size
data_sample.groupby('type').count()
sort_dict(xgb_learner.best_model.get_fscore())
df = pd.DataFrame.from_csv('../data/acs_chem_lead_joined.tsv', sep='\t')
pickling_on = open("df1.pickle","wb") $ pickle.dump(df, pickling_on) $ pickling_on.close()
df_station_group = mta.groupby(['STATION', mta.DATE_TIME.dt.date])['NEW_ENTRIES'].sum() $ df_station_group.tail(20)
player['event'] = np.where((player['events'] == 'home_run'), #| $                            1, 0) $ player['event'].sum()
precip_df.describe()
p_diffs = np.array(p_diffs) $ null_vals = np.random.normal(0,p_diffs.std(), p_diffs.size) $ plt.hist(null_vals) $ plt.axvline(x = p_obs_diff, color = 'red');
from nltk.stem.snowball import SnowballStemmer $ stemmer = SnowballStemmer('english')
random_numbers.groupby(by=[random_numbers.index.month]).mean()#here only mean values for each calender month with only month no.
df2['Open'].apply(lambda x:2*x/32)
df_total = pd.concat([df_no_cat, df_dummies], axis=1)
n_new = (df2['landing_page'] == 'new_page').sum() $ print(n_new)
old_conver_rate = df2.converted.mean() $ (old_conver_rate, new_conver_rate)
elnet = ElasticNet(alpha=0.0001, l1_ratio=0.25) $ elnet.fit(train_data, train_labels)
from sklearn import datasets $ digits = datasets.load_digits()
df_tweet_json_clean.info()
for i in df['questions_48542_why_is_this_your_preferred_brand']: $     correct_spelling(i)
pclass.value_counts().sort_index().plot(kind='bar') $ pclass.value_counts()
sc = SparkContext(appName="Spark Streaming mini project") $ stc = StreamingContext(sc, 10)
autos[['date_crawled','date_created','last_seen']][0:5]
date.strftime("%A")
x.mean(axis=0)
df_pol_t.info() $
ffr.resample("M").first().head()
from sklearn.preprocessing import StandardScaler $ scaler = StandardScaler().fit(X_train) $ X_train_scaled = scaler.transform(X_train) $ X_test_scaled = scaler.transform(X_test)
ebay.head()
from nltk.corpus import stopwords $ stop_words = stopwords.words(['english','danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish']) $ stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
run txt2pdf.py -o "MAIMONIDES MEDICAL CENTER  Sepsis.pdf"   "MAIMONIDES MEDICAL CENTER  Sepsis.txt"
totals = df.groupby(['omg_outcome','public']).count()['abuse_number'].unstack().reset_index()
print([x for x in df.columns])
c.find_one({'name.first': 'Michael', $              'name.last': 'Bowie'})
convert_old = df2[(df2['landing_page']=='old_page') & (df2['converted']==1)].count()[0] $ convert_old
proc = processor(hueristic_pct_padding=.7, keep_n=5000) $ vecs = proc.fit_transform(target_docs)
df2['intercept']=1 $ df2[['control', 'ab_page']]=pd.get_dummies(df2['group']) $ df2.drop(labels=['control'], axis=1, inplace=True) $ df2.head()
joined[['Frequency_score']] = joined[['Frequency_score']].apply(pd.to_numeric, errors='coerce')
content_wed11.shape
bres = opener.open(uri) $ tres = opener.open(uri + "?%s" % exportparams)
cols=pd.Series(tmp_df.columns, index=tmp_df.columns) $ tmp_df = tmp_df.drop(cols[cols.str.match(r'^(?:Second.)?(?:Defendant|Plaintiff)')], axis=1) $ tmp_df = tmp_df.drop(['Case.Type', 'Case.Subtype', 'Nature.of.Claim','Judgment.Against','Judgment.In.Favor.Of', $                       'Next.Hearing.Date', 'Next.Hearing.Desc', 'Next.Hearing.Time', 'Style.Of.Case' $                      ], axis=1)
df_mes = df_mes[df_mes['RatecodeID'] != '99']
zip_1_sns.Date = zip_1_sns.Date.str.replace('-',"") $ zip_1_sns.Date = pd.to_datetime(zip_1_sns.Date,format="%Y%m%d") $ zip_2_sns.Date = zip_2_sns.Date.str.replace('-',"") $ zip_2_sns.Date = pd.to_datetime(zip_2_sns.Date,format="%Y%m%d")
pgh_311_data.groupby('NEIGHBORHOOD').size()
search_eligible_members('Platinum Elite')
from sklearn.svm import SVC
k.dtypes
df.head()
edfrawtocsv(inputFile, os.path.join(edfDir, "pt1sz2"), True)
d_3 = d(theta[:,:3]) $ plt.hist(d_3, bins=50) $ plt.show()
acc_params_df
os.getcwd()
convert_mean = df2['converted'].mean() $ convert_mean
archive.name.value_counts()
conn.columninfo(table=dict(name='data.iris', caslib='casuser'))
irisDF.filter( irisDF["PetalWidth"] > 0.4).show()    
n_new =df2[df2['group'] == "treatment"].shape[0] $ print(n_new)
print('Number of rows in dataset: {}'.format(df.shape[0]))
result1 = (df['A'] + df['B']) / (df['C'] - 1) $ result2 = pd.eval("(df.A + df.B) / (df.C - 1)") $ np.allclose(result1, result2)
import praw $ import json
from scipy.stats import norm $ norm.cdf(z_score) $
items = {'Bob' : pd.Series(data = [245, 25, 55], index = ['bike', 'pants', 'watch']), $          'Alice' : pd.Series(data = [40, 110, 500, 45], index = ['book', 'glasses', 'bike', 'pants'])} $ print(type(items))
df = pd.DataFrame() $ variables = ['text','created_at','source','user'] $ df = pd.DataFrame([[getattr(i,j) for j in variables] for i in tweets], columns = variables)
df_prep15 = df_prep(df15) $ df_prep15_ = pd.DataFrame({'date':df_prep15.index, 'values':df_prep15.values}, index=pd.to_datetime(df_prep15.index))
csv = 'clean_tweet.csv' $ my_df = pd.read_csv(csv,index_col=0) $ my_df.head()
ny_df=pd.DataFrame.from_dict(foursquare_data_dict)
autos = autos.drop(['nr_of_pictures','seller','offer_type'],axis = 1)
X_test_desc_vect
p_converted_ca = df_new[df_new.country == 'CA'].converted.mean() $ p_converted_us = df_new[df_new.country == 'US'].converted.mean() $ p_converted_uk = df_new[df_new.country == 'UK'].converted.mean() $ print("CA: {0}, US: {1}, UK: {2}".format(p_converted_ca, p_converted_us, p_converted_uk))
X = pd.merge(X, meal_is_interactive[['meal_id','is_interactive']], on='meal_id', how='inner')
plt.hist(p_diffs) $ plt.axvline(actual_diff, color = 'r');
!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz  --directory-prefix=data
import seaborn as sns $ sns.regplot(x=np.round(predictions), y=test_data['lifetime_revenue'], color = 'thistle')
my_cols = [u'Request Status', u'Equipment Type', u'Created Date', u'Priority', u'Major Issue', u'Description', \ $            u'Assigned Date', u'Sub Assigned Date', u'Escaltion Date', u'Due Date', u'Complete Date', \ $            u'Close Date', u'Service Location', u'Repeated in last 48 hours', u'Resolution Code', \ $            u'Replacement Item', u'Replacement Quantity', u'Resolution Description', u'Service Location Type', \ $            u'Actual Close Datetime', u'Hold Date', u'Total Hold Time (In Minutes)', u'Reopen Count']
df = pd.read_sql('SELECT last_name, COUNT(*) FROM actor GROUP BY last_name ORDER BY last_name', con=conn) $ df
from sklearn.naive_bayes import GaussianNB
contribs = pd.read_csv("http://www.firstpythonnotebook.org/_static/contributions.csv")
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ z_score, p_value
df_twitter_copy['timestamp'] = pd.to_datetime(df_twitter_copy['timestamp'])
from __future__ import print_function $ import json $ from collections import namedtuple, OrderedDict $ import requests
fund_nr_coinswitches['2014':].plot() $ plt.title('Nr of coin switches in the fund') $ plt.ylabel('Coin switches') $ plt.show()
new_page_converted = np.random.binomial(1, p_new,n_new) $ new_page_converted.mean()
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $
train.shape
tweets.describe()
import datetime $ now = datetime.datetime.now() $ print now
finaldf.congress = finaldf.congress.astype(int) $ finaldf.head()
knnmodel.fit(X_train, y_train) $ knnmodel_predictions = knnmodel.predict(X_test) $ num_of_knnmodel_pred = collections.Counter(knnmodel_predictions) $ num_of_knnmodel_pred
reviews_recent20 = reviews_w_sentiment.groupby(['listing_id']).tail(20)
total_rows_in_treatment = (df2['group'] == 'treatment').sum() $ rows_converted = len((df2[(df2['group'] == 'treatment') & (df2['converted'] == 1)])) $ rows_converted/total_rows_in_treatment
df_clean = df_clean[df_clean['in_reply_to_status_id'].isnull()]
import math $ from keras.models import Sequential $ from keras.layers import LSTM, Merge, Activation, Dense, Dropout $ from keras import regularizers
[v for v in vessels.type.unique() if v.find('/')==-1] $
check_null = df.isnull().sum(axis=0).sort_values(ascending=False)/float(len(df)) $ print(np.sum(check_null.values) == 0.0)
uClient = uReq(my_url) $ uClient
df1.info()
app_label = train['signup_app'].unique() $ print(app_label)
house_data[house_data['bedrooms'] >= 11]
components
p_old = df2['converted'].mean() $ print(p_old)
df.tail()
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline
X_train, X_test, y_train, y_test = train_test_split(tweet_embedding, train['label'], test_size = 0.15, random_state=42)
cnt,bns=np.histogram(investor_fut_bucket_30360_xirr_orig, bins=np.linspace(-.5,.5,101)) $ a=pd.DataFrame({'bin':bns[:-1], 'count':cnt}) $ a.to_clipboard() $ a
project_human_df = non_blocking_df_save_or_load( $     rewrite_human_data(project_human_raw_df), $     "{0}/human_data_cleaned/projects".format(fs_prefix))                                  
date_now.toordinal()
test_data_features = vectorizer.transform(test.review) $ test_data_features = test_data_features.toarray() $ predictions = clf.predict(test_data_features)
cc.drop(columns=['logspread','logclose']) $ cc.head()
c="r,g,b,c,m,y,k,w".split(",") $ scatter_matrix(dfL, alpha=1, figsize=(10,10), s=100, c=df[predictColumn].apply(lambda x:c[x]));
twitter_archive_df_clean.drop(['rating_numerator', 'rating_denominator'], axis=1, inplace=True)
import sympy as sym $ from sympy import init_printing; init_printing(use_latex='mathjax') $ sym.var('t r') $ C = sym.Function('C')
corpusDF.head(2)
merged.head()
tlen_d.plot(figsize=(16,4), color='r');
def variance(columnname, n): $     mean = sum(columnname)/n                        # creating a variable for the mean of the list/array $     std_part_1 = sum([(num-mean)**2 for num in columnname]) # creating a variable for the first caclulation of the standard deviation $     var = std_part_1/(n-1)         # Completing the standard deviation calculation $     return(var) $
xmlData['build_year'].value_counts()
combined_df2.columns
def completed_vs_incompleted_songs(x): $     x['num_completed_songs'] = x.num_100 + x.num_985 $     x['num_incompleted_songs'] = x.num_25 + x.num_50 + x.num_75 $     return x $ user_logs = user_logs.apply(completed_vs_incompleted_songs, axis = 1)
!hdfs dfs -ls
FileLink(csv_fn)
tweets['created_at'] = tweets.apply( $     convert_timezone, $     axis=1, $     args=(ConvertTZArgs('created_at', 'US/Central'),) $ )
print(df['date'].min().month) $ print(df['date'].min().year) $ print(df['date'].min().day)
df['pb_prod'] = df['pledged'] * df['backers'] $ df['pb_avg'] = df[['pledged', 'backers']].mean(axis=1)
print(guinea_df)
con.execute('INSERT INTO samples VALUES(\'{}\',{},{},{})'.format(*few_recs.ix[0]))
new_array = np.concatenate((training_active_listing_dummy,training_pending_ratio),axis=1)
print(autos['odometer_km'].unique().shape) $ autos['odometer_km'].describe()
x = df2[df2['group']=='treatment']['converted'].mean() $ print("{:.2%}".format(x))
ranked_variance = (weekly_variance.select('week','hashtag', 'variance', weekly_rank) $             .filter('rank <= 5') $             .sort('week','rank') $             .cache()) $
ts['2018-01']
cont_probability = df2[df2['group'] == 'control']['converted'].mean() $ cont_probability
rating_and_retweet['score'].corr(rating_and_retweet['retweet_count'])
from planet import f2 $ metrics=[f2] $ f_model = resnet34
save_n_load_df(joined, 'joined_promo_bef_af.pkl')
pd.DataFrame.query?
df.query('group == "control" and landing_page == "new_page"').count()
accident_counts_per_segment = df.groupby('segment_id').size().reset_index(name='accident_counts').set_index('segment_id')
df
logit_countries = sm.Logit(df3['converted'], $                            df3[['country_UK', 'country_US', 'intercept']]) $ result1 = logit_countries.fit()
player_frame = baseball[baseball['team'].isin(['LAN', 'SFN'])] $ print(player_frame['player']) $ print('The number of records is: ',len(baseball[baseball['team'].isin(['LAN', 'SFN'])]))
data['moving_avg'] = pd.rolling_mean(data['sentiment'], 100)
data.drop(["link_flair", "ops_flair"], axis=1, inplace=True)
df = pd.DataFrame(np.random.rand(5,5), index=list('ABCDE'), columns=list('PQRST')) $ print df
df2.drop(labels = 'control',axis = 1, inplace= True)
df_wm.to_csv("walmart_senti_score.csv", encoding='utf-8', index=False) $
from sklearn import svm $ SVM_model = svm.SVC() $ SVM_model.fit(X_train, y_train) 
bag = vect.fit_transform(df['lemmas']).toarray() $ bag
ffr.rolling(window=7).max().head(10)
rf = RandomForestClassifier() $ rf.fit(X_train, y_train) $ rf.score(X_test, y_test)
plt.hist(p_diffs); $ plt.xlabel('Difference in conversion rates') $ plt.ylabel('Frequency') $ plt.title('Simulated differences between new and old conversion rates')
events.head()
pold = df2['converted'].mean() $ print(pold) #creating the mean converted values as pnew
df.info()
from datetime import datetime as dt $ from dateutil.relativedelta import relativedelta
patient_times.columns = ['patient', 'start_time', 'stop_time']
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
evaluator.get_metrics('tag_level_results')
clean_users[clean_users['active']==0]['creation_source'].value_counts()
tags = {} $ for i, el in rentals['description'].iteritems(): $     tags[i] = get_rental_concession(el) $ T = pd.Series(tags)
idx = df_providers[ (df_providers['id_num']==30137)].index.tolist() $ '${0:10,.0f}'.format(np.sum(df_providers.loc[idx,'disc_times_pay'])) $
autos.describe()
from newsapi import NewsApiClient $ my_api_key = 'INSERT YOUR API KEY' $ newsapi = NewsApiClient(api_key = my_api_key)
timeseries.rolling(50).mean().plot(label='50 day Rolling Mean', figsize = (12,8)) $ timeseries.rolling(50).std().plot(label='50 day Month Rolling Std') $ timeseries.plot() $ plt.legend()
df2_treatment = df2.query('group == "treatment"')
template = '%.2f %s are worth $%d'
store_items.toPandas().head()
draft_df.to_pickle("draft_df.pkl")
learner.save_encoder('adam1_10_enc')
(df2["converted"] == 1).mean() ### assumed that is  pnew  and  pold  are equal
import pandas as pd $ csv_url = "http://data.cocorahs.org/export/exportreports.aspx?ReportType=Daily&dtf=1&Format=CSV&State=NC&County=BC&ReportDateType=reportdate&StartDate=5/1/2018&EndDate=5/31/2018&TimesInGMT=True" $ daily_data = pd.read_csv(csv_url)
data_df.groupby('topic')['ticket_id'].nunique()
p_aux=[] $ for i in range(0,l2): $     if i not in seq: $         p_aux.append(priority_num[i]) $ col.append(np.array(p_aux))
print("Unique users:", len(df2.user_id.unique())) $ print("Non-unique users:", len(df2)-len(df2.user_id.unique()))
data_libraries_df.to_excel(writer, "data_libraries", index=False, na_rep="NA")
prcp_12monthsDf.set_index('date').describe()
F = df_forecast[names] $ y_forecast = exported_pipeline.predict(F) $ y_forecast
from collections import Counter $ c = Counter('letters') $ c
corrplot(fin_r.loc[start_date:end_date].corr(), annot=True) $ plt.title('Correlation matrix - daily data \n from ' + start_date + ' to ' + end_date) $ plt.show()
def hour_bins(hour): $
dashes = {'PA':[6,2], 'PAS':[6,2,1,2], 'PS':[3,2], 'PSA':[3,2,1,2], 'PCS':[1,1]} $ names = ('TMSS', 'PS', 'PA', 'PSA', 'PAS', 'PCS')
from scipy.stats import norm $ norm.cdf(z_score) #how significant our z_score is
newdf = pd.DataFrame({'yearmonth':sf.index, 'deger':sf.values})
users[users.age < 18].age = np.nan
treehouse_labels_pruned = treehouse_labels.filter(regex='\ATH|\ATHR', axis="index")
test['date'].min(), test['date'].max(), train['date'].min(), train['date'].max()
df2=df2.drop(df2.index[1899]) $ df2.user_id.nunique()
merged.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount", ascending=False)
print ("Filtered records for query 2 :  ", len(final_location_ll))
df.to_csv('new_edited.csv', index=False)
sconf = SparkConf() \ $     .setAppName("Word Count") \ $     .set("spark.ui.port","4141") $ sc = SparkContext(conf=sconf)
sampled_contirbutors_human_raw_df = session.read.format("csv").option("header", "true") \ $                 .option("inferSchema", "true").load( $     "{0}/human_data/sampled_contirbutors".format(fs_prefix))
s.str.count('m')
logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])
df['Datetime'].min()
output= "SELECT Distinct user_id  from tweet limit 10 " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['user_id'])
v_invoice_hub.columns[~v_invoice_hub.columns.isin(invoice_hub.columns)]
times.dt?
x_test.shape
df4[['ab_page', 'country_UK', 'country_US','intercept', 'converted']].astype(int) $ df4.head()
df_master.head()
predict_actual_df=pd.concat([prediction_dataframe.iloc[:,0:1], actuals_dataframe], axis=1) $ predict_actual_df.shape
aux = aux[~(aux.tweet_id.duplicated())] $ aux = aux.drop(['cuteness_x', 'cuteness_y'], axis=1) $ clean_rates = clean_rates.merge(aux, how='inner', on='tweet_id') $ clean_rates = clean_rates.drop(['doggo','floofer', 'pupper', 'puppo'], axis=1)
feat = ['categoryname', 'eventname', 'location'] $ for f in feat: $     PYR[f] = PYR[f].apply(clean_dataset)
frac = 0.9
df_clean['tweet_id'] = df_clean['tweet_id'].astype('str') $ image_clean['tweet_id'] = image_clean['tweet_id'].astype('str') $ tweet_clean['tweet_id'] = tweet_clean['tweet_id'].astype('str')
small_train
import pandas as pd $ import folium $ from matplotlib import pyplot as plt $ %matplotlib inline
dfcopy.sort_values('date').tail()
p_old = df2["converted"].mean() $ print("Convert rate for P-old is: {}".format(p_old))
conn = connect(host="localhost", port= 9091, user="mapd", password="HyperInteractive", dbname="mapd")
len(sample)
sns.barplot(data=df.groupby('purpose').agg({'applicant_id':lambda x:len(set(x))}).reset_index(), $             x='purpose',y='applicant_id')
building_pa.columns[building_pa.columns != building_pa.columns.str.extract(r'^(\w+)$')] 
 goals_df = scrape_group_stats(base_url, 'goals')
festivals['Index'] = range(1, len(festivals) + 1)
df['duration'].describe()
autos.price = (autos.price $                            .str.replace('$','') $                            .str.replace('.','') $                            .str.replace(',','') $                            .astype(float)) $
df.dtypes
print(dfx.fillna(value=-999.25),'\n') $ print(dfx) # original data remains unchanged. 
pos_lda = models.LdaModel(pos_bow, id2word=pos_dic, num_topics=3, chunksize=10000, passes=4, iterations=100) $ pos_lda.show_topics(formatted=False)
df.pop('Seconds')
df.drop(['rating_numerator', 'rating_denominator'], axis=1, inplace=True)
eval_RF_tfidf_tts = clf_RF_tfidf.score(X_testcv_tfidf, y_testcv_tfidf) $ print(eval_RF_tfidf_tts)
unsegmented_users['segmentation_reason'] = 'enrollment'
kimanalysis.getfile(result, 'results.edn', contentformat='edn')
BAL_analysis = team_analysis.get_group("BAL").groupby("Category") # Pulls only team transactions from sample, then groups $
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page','UK','US']]) $ results = logit_mod.fit() $ results.summary()
for txt in input_texts[:20]: $     print("Original txt:", txt) $     emb, summ = seq2seq_inf.generate_issue_title(txt) $     print("Summary:", summ) $     print("----") $
all_data = {ticker: web.get_data_yahoo(ticker) $            for ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']}
df_train.columns.values
df.drop(['in_reply_to_status_id', 'in_reply_to_user_id', 'retweeted_status_user_id', 'retweeted_status_id', 'retweeted_status_timestamp'], axis=1, inplace=True)
(obs_diff < p_diffs).mean()
VX.head(1)
session.query(Measurements.station,func.count(Measurements.date)) \ $              .group_by(Measurements.station).order_by(func.count(Measurements.date).desc()).all() $
rate.head()
freq = pd.Series(' '.join(df['body']).split()).value_counts()[-10:] $ freq
green_line = data[:training_split_cut] $ purple_line = [] $ for iter_x in np.arange(training_split_cut): $     purple_line.append(np.nan) $ purple_line = purple_line + list(reg1.predict(X_test))    
dcrime.head()
len(total_df)
df_A4=df_A3.dropna() $ print(df_A4) $ df_A.index
dt.date() $ dt.time()
tags.head(25)
df['Start Date'].head()
tweet_df.to_csv("Data/Tweetanalysis.csv",index=False, header=True)
base_df = sqlContext.read.text(log_file_path) $ base_df.printSchema()
df2.to_csv('./Data/changes.csv')
type(ts.index)
df2['UK_ab_page'] = df2['UK']*df2['ab_page'] $ df2['US_ab_page'] = df2['US']*df2['ab_page'] $ df2['CA_ab_page'] = df2['CA']*df2['ab_page']
TestRatio = 0.2
run txt2pdf.py -o '2012 Snapshot.pdf' '2012 Snapshot.txt' $
hours['pred'] = linreg.predict(X) $ plt.scatter(hours.hour, hours.start) $ plt.plot(hours.hour, hours.pred, color='red') $ plt.xlabel('hours') $ plt.ylabel('start') $
df2 = df2.drop('control', axis=1)
table.info('stats')
keys_0604 = pd.read_excel(cwd + '\\ELMS-DE backup\\keys_0604.xlsx') $ keys_0604['integration_date'] = [datetime.date(int(str(x)[0:4]),int(str(x)[5:7]),int(str(x)[8:10])) $                                 for x in keys_0604.integration_date.values]
n_old = len(df2[df2.group == 'control']) $ n_old
df2.query("group == 'treatment'")['user_id'].count()/df2.shape[0]
train_small_data.reset_index(drop=True, inplace=True) $ val_small_data.reset_index(drop=True, inplace=True)
85127 - 83349
_dm = pd.read_sas('./data/in/dm.xpt', format='xport') $ _dm.head()
def load_data (dataframe, sheetname): $
download = True
learner.fit(3e-3, 1, wds=1e-6, cycle_len=10)
multiple_party_votes_all.shape
logit_mod3 = sm.Logit(df_new['converted'], df_new[['intercept','treatment','UK','US','page_UK','page_US']]) $ results3 = logit_mod3 .fit() $ results3.summary()
X_train.info()
temp_series_paris_naive = temp_series_paris.tz_localize(None) $ temp_series_paris_naive
scraped_batch6_sec
y_log = np.log(y)
typesub2017.isnull().sum()
data.columns
(active_companies_psc_applies_unique_count - active_psc_records_unique_company_count) - active_psc_statements_unique_company_count
train = pd.read_csv("data/wikipedia_train3.csv") $ test = pd.read_csv("data/wikipedia_test3.csv")
features.head()
s.resample('Q').head()
cityID = '42e46bc3663a4b5f' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fort_Worth.append(tweet) 
enumeration = [{'id': 1, 'name': 'red'}, {'id': 10, 'name': 'green'}, {'id': 100, 'name': 'blue'}] $ print_enumeration(enumeration)
plt.plot(dfdaycounts['created_date'], dfdaycounts['count_n']) $ plt.xticks(rotation='vertical')
result = 'TE_320860761056_001-and-MO_800509458712_001-1525993487-tr' $ print(kimanalysis.shortcode(result)) $ print(kimanalysis.shortid(result)) $ print(kimanalysis.extendedid(result))
print(read.shape) $ read.head()
kickstarters_2017 = pd.read_csv("ks-projects-201801.csv") $ kickstarters_2017.head()
model = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US']]) $ results_2 = model.fit() $ results_2.summary()
pd_aux['permit_number_notM']=pd_aux['permit_creation_date'].apply(lambda x: str(x)[0:10].replace('-',''))+pd_aux['permit_number'].apply(lambda x: x[3:]) $ pd_aux.head(5)
archive_df.name.value_counts()
sql("show tables").show()
popC15[popC15.content == 'post'].sort_values(by='counts', ascending=False).head(10).plot(kind='bar')
import sys $ sys.version
df_with_predictions = pd.DataFrame(np.c_[X_test, y_test, y_pred]) $ df_with_predictions.columns = list(acs_df.columns) + ['homeval_pred'] $ df_with_predictions.head()
retweet_and_count = sns.factorplot(data=tweets_df, x="name", y="retweet_count", kind="box") $ retweet_and_count.set(yscale="log") $ plt.xticks(rotation=60)     # alwatan have above average number of retweets and alseyassah have below average number of retweets $
df_sample_day=df_sample[(df_sample["day"] == 1)] $ df_sample_night=df_sample[(df_sample["day"] == 0)] $ scipy.stats.ks_2samp(df_sample_day["tripduration"],df_sample_night["tripduration"])
tweets.head(3)
manager.image_df['p_hash'].isin(tree_features_df['p_hash']).head(20)
df_rand['ch_json'] = df_rand.name.apply(search_org) $ df_rand['ch_postcodes'] = df_rand.ch_json.apply(ch_postcodes)
shift_entries.head()
agency_borough.head()
baseline_temp = 291.483 $ weather_features['HDD'] = weather_data.temp.apply(lambda x: np.max([(baseline_temp-x)/24,0])) $ weather_features['CDD'] = weather_data.temp.apply(lambda x: np.max([(x-baseline_temp)/24,0])) $
class_perishables=pd.DataFrame(group['perishable'].agg('sum')) $ pd.DataFrame.head(class_perishables)
np.random.randn(6,4) $
scaled = data.div(data['Close'], axis=0) $ scaled.head()
closingPrices.head()
print([w for w in chat_words if re.search('^[A-Z][0-9]{3,5}$', w)])
sql("describe formatted orders").toPandas()
log_1 = pd.read_csv('log_1.csv') $ log_1.head()
exiftool -csv -createdate -modifydate cisuabd4/cisuabd4_cycle1.MP4 cisuabd4/cisuabd4_cycle2.MP4 cisuabd4/cisuabd4_cycle3.MP4 cisuabd4/cisuabd4_cycle4.MP4 cisuabd4/cisuabd4_cycle5.MP4 cisuabd4/cisuabd4_cycle6.MP4 > cisuabd4.csv
??convert_game_value
merged_df['DL_length_x'].fillna(0, inplace=True) $ merged_df['DL_length_y'].fillna(0, inplace=True) $ merged_df['DL_both_length'] = merged_df['DL_length_x'] + merged_df['DL_length_y']
import re $ letters_only = re.sub("[^a-zA-Z]",           # The pattern to search for $                       " ",                   # The pattern to replace it with $                       example1.get_text() )  # The text to search $ print(letters_only)
Y_test = data3[302:].sales
min_df = 0.0 # minimum frequency of words needed to be a part of the model $ max_df = 1.0 # max frequency of words to take into account as part of the model $ num_k = 10 # number of clusters $ data = train # use the train data set $ model_iteration(min_df, max_df, num_k, data)
posts_renamed = posts.rename(columns={col: 'post_' + col for col in posts.columns}) $ comments_renamed = comments.rename(columns={col: 'comment_' + col for col in comments.columns})
np.mean([len(h.tweets) for h in heap])
clean_train_df = pd.concat([clean_train_df, selected_train_df], axis=1)
print("Number of Groups in Enterprise ATT&CK") $ print(len(all_enterprise['groups'])) $ df = all_enterprise['groups'] $ df = json_normalize(df) $ df.reindex(['matrix', 'group', 'group_aliases', 'group_id', 'group_description'], axis=1)[0:5]
airbnb_df.groupby('property_type')['bed_type'].agg(lambda x: (x=='Real Bed').sum()*0.75)
plt.hist(taxiData.Trip_distance, bins = 100, range = [taxiData.Trip_distance.min(),50]) $ plt.xlabel('Traveled Trip Distance') $ plt.ylabel('Counts of occurrences') $ plt.title('Histogram of Trip_distance') $ plt.grid(True)
test_transf = sklearn_pca.fit_transform(X_test)
deaths.head()
trump[trump['is_retweet']==True].text[0:5]
cats_df = pd.read_csv('./data/cats.csv')
train['PassengerId'].max()
humans = TextBlob("I love humans") $ humans.sentiment
train_col.lr_find(steps=30)
stock["rise_in_next_day"] = stock["Close"].astype("float").shift(-1)/stock["Close"].astype("float") >=1
twitter_master2.shape 
unique, counts = np.unique(countries, return_counts=True)
support=merged[merged.committee_position=="SUPPORT"]
df[df.duplicated(['user_id'], keep=False)]
df_ad_state_metro_2.plot(x='state', y='ad_duration_secs', kind='bar')
wrd_tidy.head(3)
cur.execute('SELECT * from iris')
Games = put_game_ids() $ Games.head()
data2= data.assign(SYMBOL=SYMBOLs, SOTERM=SOTERMs, FXNCLASS=FXNCLASSs) $ forIdx = data2["V1"].str.split(":", expand=True).applymap(int).sort_values(by=[0, 1]) $ df = data2.reindex(forIdx.index) $ df.head() $ df.to_csv("outputs/allV_info.csv", sep='\t')
df3['country'].value_counts() $ dummy_country = pd.get_dummies(df3['country'], prefix='country') $ dummy_country.head()
autos["num_pictures"].value_counts()
repos
df.plot(y = 'Close')
df2_after_df1 = df1.append(df2) $ df2_after_df1
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print("Z-score: {}, p_value: {}".format(z_score, p_value))
mean_weekday = daily_averages.groupby('Weekday').mean()
paired_df_grouped['best_co_occurence'] = paired_df_grouped.apply(lambda x: x['all_co_occurence'][:rule_of_thumb(x['top10'])], axis=1) $ del paired_df_grouped['top10'] $
df["winter_pickup"] = (df["pickup_month"].isin([1, 12])).astype(int)
p_old = df2['converted'].mean()
v_invoice_link.drop(v_invoice_link_dropper, axis=1, inplace=True) $ invoice_link.drop(invoice_link_dropper, axis = 1, inplace=True)
theft.sort_values('DATE_OF_OCCURRENCE', inplace=True, ascending=True) $
print("The no of observations in dataset are",df.shape[0]) 
payload_online = {"name": "test", "description": "test", "type": "online"} $ response_online = requests.post(endpoint_deployments, json=payload_online, headers=header) $ print(response_online) $ print(response_online.text)
xbar_chart_title = 'Average control chart' $ xbar_chart_subtitle = 'Thickness average (mm)' $ xbar_chart_ylabel = 'Thickness average (mm)' $ xbar_chart_xlabel = 'Operator--Part'
bnbAx.language.value_counts()
cityID =  '4ec71fc3f2579572' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $          Shreveport.append(tweet) 
len(air_store[air_store.air_genre_name.isnull()])
df_h1b_nyc_ft.lca_case_employer_name.nunique()
y_pred # predicted 
weather_features = pd.DataFrame(index=weather_data.index)
master_df.to_csv('twitter_archive_master.csv')
FileLink("xgb_unconverged.csv")
len(unsolve_hacker_list)
df['text'] = [process_lem(text) for text in df['text']]
p_new = len(df2.query( 'converted==1'))/len(df2.index) $ p_new
df3.head(500)
xgb = XGBClassifier() $ xgb.fit(X_train, y_train)
df_merged.hist(column='rating_numerator',bins=[0,1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]);
monthly_sales_2015 = liquor_2015[['Sale (Dollars)','Month']].groupby(['Month']).sum() $ month_sales_15 = monthly_sales_2015.plot(kind='bar',figsize=(8,8)) $ month_sales_15.set_ylabel('Sale ($)') $ month_sales_15.set_title('Total Sales Per Month in 2015') $
trains_fe2_x= trains_fe1.drop(['days_since_prior_order','add_to_cart_order','reordered','eval_set'], axis=1) $ trains_fe2_x.head() #12 features as the predictor variables
result.summary()
day_of_week14 = uber_14["day_of_week"].value_counts().to_frame()
cols_to_drop = const_cols + col_to_remove + ["sessionId"] + ["fullVisitorId"] $ cols_to_drop = [x for x in cols_to_drop if x not in ["totals.transactionRevenue","date"]] $ cols_to_drop_test = [x for x in cols_to_drop if x not in ["trafficSource.campaignCode","totals.transactionRevenue","date"]] $ train_data = train_data.drop(cols_to_drop, axis=1) $ test_data = test_data.drop(cols_to_drop_test, axis=1)
archive_clean[archive_clean['tweet_id'] == 680494726643068929].rating_numerator
if 0 == go_no_go: $     all_top_vecs = [lda.get_document_topics(serial_corp[n], minimum_probability=0) \ $                     for n in range(len(serial_corp))]
from pyspark.sql.functions import col $ usage_400hz_filter = usage_400hz.where(col("pk_ramp").isin({'B15', 'B23', 'B31', 'E09','E20', 'E24', 'H02', 'H04', 'H06'}) == False) \ $ .where(col("pk_ramp").isin({'B16', 'B20','B24', 'B28', 'B32', 'B36'}) == False) \ $ .where(col("actype").isin({'DASH 8-400', 'FK50', 'EMB 145', 'EMB-145'}) == False) \ $ .where(col("pk_ramp").isin({'H02', 'E17', 'D55', 'E17', 'G02', 'D18', 'B35', 'D04', 'D04', 'D53', 'D53', 'D07', 'D07', 'E09', 'D24', 'H03', 'E09', 'G05', 'E20', 'E20', 'G05', 'G08', 'G08', 'F04', 'F04', 'F07', 'F07', 'D14', 'B15', 'B31', 'H04', 'E18', 'E18', 'E18', 'D41', 'E18', 'D02', 'D02', 'D51', 'D51', 'D08', 'D08', 'D57', 'D57', 'E07', 'E07', 'G03', 'G03', 'D10', 'G06', 'G06', 'D43', 'G09', 'E24', 'D26', 'D43', 'H05', 'E24', 'G09', 'G09', 'B27', 'G09', 'F05', 'F05', 'F08', 'F08', 'D16', 'B17', 'E19', 'E19', 'D03', 'D03', 'H06', 'H01', 'D22', 'D05', 'B23', 'E05', 'E05', 'E08', 'E08', 'G04', 'G04', 'E22', 'E22', 'G07', 'D12', 'D49', 'D47', 'B13', 'D47', 'F03', 'F03', 'D28', 'H07', 'F06', 'F06', 'F09', 'F09'}) == True) $
df_input.filter("`Park Borough` == '225841'").show()
folder = 'test' $ user_logs = utils.read_multiple_csv('../../feature/{}/user_lgos_listening_behavior'.format(folder)) $
plots = plot_analysis(reg_traffic_with_flags)
station_total = station_by_week.reset_index().groupby(['STATION'])['entries_diff'].sum().reset_index() $ station_total = station_total.sort('entries_diff', ascending = [0]) $ print "Top 10 Stations for Entry Volume: (8/29/2016 - 9/16/2016)" $ station_total.head(10).set_index('STATION')
print("Number of speeches of all kinds: ", speeches_df2.shape[0]) $ print(speeches_df2['kind'].value_counts()) $ speeches_df3 = speeches_df2[(speeches_df2['kind'] == 'speech') | (speeches_df2['kind'] == 'title')] $ print("Number of speeches (speech): ", speeches_df3.shape[0]) $ speeches_df3.reset_index(inplace = True)
df.to_csv('cleaned_vg_df')
def read_mongo(db, collection, query={}, host='localhost', port=27017, username=None, password=None, no_id=True): $
def percents( date_time , date_series , data_frame , look_back = 365 , quantiles = [25,75]): $     look_back = datetime.timedelta(look_back) $     mask = (-look_back < date_series - date_time) & (date_series - date_time <= datetime.timedelta(0)) $     nums = np.percentile( data_frame[mask],q = quantiles,axis = 0) $     return nums $
optimizer = tf.train.AdamOptimizer() $ training_op = optimizer.minimize(loss, name="training_op")
train = spark.read.csv(os.path.join(datapath,"train.csv"), header=True) $ print('Found %d observations in training set.' %train.count())
graf['DETAILS']=graf['DETAILS'].str.replace('\n', ' ')
df2.head()
faa_data_minor_damage_pandas = faa_data_pandas[faa_data_pandas['DAMAGE'] == "M"] $ print(faa_data_minor_damage_pandas.shape) $ faa_data_minor_damage_pandas.head()
userproduct.merge(transactions,how='outer')[['UserID','ProductID','Quantity']].fillna(0).astype('int64')
df=df.rename(columns = {'Parent?':'flag'}) $ d=len(df.com_id) $ print("Total Number of Comments Scraped: ") $ d
p_new = df2.query('landing_page == "new_page"').user_id.nunique()/df2.user_id.nunique() $ print('The probability that an individual received the new page is {}.'.format(round(p_new,4)))
data.drop(['Unnamed: 0'], axis=1, inplace=True) $ data = data.dropna()
result = grouper.dba_name.value_counts()
transactions.merge(users, how='left', on='UserID')
y.value_counts()
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer $ analyzer = SentimentIntensityAnalyzer()
plt.figure(figsize=(16,8)) $ plt.title('MODELS DISTRIBUTION FOR VOLKSWAGEN BRAND') $ model_count = sns.countplot(volkswagen_cars['model']) $ model = model_count.set_xticklabels(model_count.get_xticklabels(), rotation=90)
pca.explained_variance_ratio_
proj_df['Project Grade Level Category'].unique()
raw_full_df.head()
chart = top_supporters.head(5).amount.plot.bar()
(new_page_converted.mean()) - (old_page_converted.mean())
rcf_inference = rcf.deploy( $     initial_instance_count=2, $     instance_type='ml.c5.xlarge', $ )
print(member['duration_sec'].describe())
top_100_2016['genre_ids'] = [map(int, re.sub("[\[ \] ]", "", top_100_2016['genre_ids'][i]).split(',')) for i in range(len(top_100_2016))] $ data_count_genres = top_100_2016['genre_ids'].apply(collections.Counter) $ one_hot_encode_genres = pd.DataFrame.from_records(data_count_genres).fillna(value=int(0)) $ top_100_2016 = top_100_2016.join(one_hot_encode_genres) $ top_100_2016.head(4) $
print(np.shape(lats), np.shape(lons))
ax = pdf.plot() $ s_etf.plot(ax=ax, legend='right')
cand_date_df = pres_date_df.copy() $ cand_date_df.head()
schumer.head()
df[df['SchoolHoliday'] == 0].head()
titanic = sns.load_dataset('titanic')
df.set_value(306, 'yob',1998)
df_r1 = df_r.groupby(["CustID"], as_index=False).agg({"Recency (1)": np.min, $                                                       "Recency (3)": np.mean, $                                                       "Counter":np.max}) $ df_r1["Recency (3)"] = np.round(df_r1["Recency (3)"], decimals = 2)
to_be_predicted_Day3 = 52.42539737 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
tweet_df.rename(columns={'id':'tweet_id'}, inplace=True) $ final_df = pd.merge(df, img_df, how = 'left', on = ['tweet_id'] ) $ final_df = pd.merge(final_df, tweet_df, how = 'left', on = ['tweet_id']) $ final_df.info() $ fin_df=final_df.copy()
eval_df = create_evaluation_df(predictions, test_inputs, HORIZON, y_scaler) $ eval_df.head()
100*np.exp(fundret.loc['2017-03':].cumsum())
print("The total no of unique users are",df['user_id'].nunique())  #Many Users have same user id
test_df_01.to_pickle("dogscats_pred.pkl")
pd.set_option( 'display.notebook_repr_html', False)  # render Series and DataFrame as text, not HTML $ pd.set_option( 'display.max_column', 10)    # number of columns $ pd.set_option( 'display.max_rows', 10)     # number of rows $ pd.set_option( 'display.width', 80)        # number of characters per row
post_id = posts.insert_one(post) #insere dados $ post_id $
test_pl = tweet_en[tweet_en['text'].apply(lambda x: "I'm at" in x)] $ test_pl = tweet_en[tweet_en['ex_lat']!='-1']
weekly_variance.show(10)
Sales_per_customer= training.groupby(by='Store').Sales.mean()/training.groupby(by='Store').Customers.mean() $ store_info['Sales_per_cust']= Sales_per_customer.values
df.head()
store_items.fillna(method = 'ffill', axis = 0)
live = live.dropna(subset=['agepreg', 'totalwgt_lb']) $ ages = live.agepreg $ weights = live.totalwgt_lb
beirut.head()
iris_fromUrl.head(6)
y_test
len(df_users), len(df_users.drop_duplicates())
df = pd.read_csv('/Users/bellepeng/Desktop/Metis/sf18_ds11/challenges/challenges_data/mta.csv') $ df.reset_index(drop=True).head() $ df.drop('Unnamed: 0', axis=1, inplace=True)
len(df2[df2['landing_page']=='new_page'].user_id.unique())/len(df2['user_id'].unique())
import numpy as np $ import pandas as pd $ autos = pd.read_csv("autos.csv", encoding = "Latin-1")
excelDF[["Sales","Region"]]
tesla = pd.read_csv('tesla_tweets.csv')
tweets_df.name.describe()
rounds_df[(rounds_df.announced_on >= '2015-08-01')].shape[0]
df['tweet_location_coord'] = df['tweet_location'].map(ttloc) $ df['user_timezone_coord'] = df['user_timezone'].map(usrloc)
departures = df[['Start Station', 'Start Date', 'Duration']]
bad_dates_cnt = bad_dates.count()*100 $ print "{:,} users have a bad date format somewhere ({:.2%} of DAU)"\ $       .format(bad_dates_cnt, bad_dates_cnt*1./dau)
p_new = df2.query('landing_page == "new_page"')['converted'].mean() $ print('The convert rate for the new page:', p_new)
clf=GaussianNB()
df_goog['Date'].unique()
size_control= df2.query('group == "control"').user_id.nunique() $ size_treatment = df2.query('group == "treatment"').user_id.nunique() $
a[2, :] = np.array([[0,0,0]]) $ a
b.visit("https://www.skyscanner.net/transport/flights/tpet/cts/180118/180130?adults=1&children=0&adultsv2=1&childrenv2=&infants=0&cabinclass=economy&rtn=1&preferdirects=false&outboundaltsenabled=false&inboundaltsenabled=false&qp_prevProvider=ins_month&qp_prevCurrency=GBP&qp_prevPrice=178&pricesourceid=b3ms-SG1-2#results")
coins.shape
words_mention_scrape = [term for term in words_scrape if term.startswith('@')] $ corpus_tweets_scraped.append(('mentions', len(words_mention_scrape))) # update corpus comparison $ print('Total number of mentions: ', len(words_mention_scrape)) #, set(terms_mention_stream))
greater_than_diff = [i for i in p_diffs if i > p_diff]
diff_act=df2[df2['group'] == 'treatment']['converted'].mean()-df2[df2['group']=='control']['converted'].mean() $ (p_diffs > diff_act).mean()
TestData.head()
df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv')
df2.timestamp = pd.to_datetime(df2.timestamp) $ df2.sort_values(['timestamp'], inplace=True) $ df2.iloc[-1, 1] - df2.iloc[0,1]
sns.lmplot(x='tweet_lenght',y='favorite_count',data=df) $ sns.lmplot(x='tweet_lenght',y='retweet_count',data=df) $ plt.show()
typesub2017['Solar'].sum() 
tweet_summary_df = pd.DataFrame(tweet_df.sum(), columns=['tweet_count']).drop('total') $ tweet_summary_df['percentage'] = tweet_summary_df.tweet_count.div(tweet_df.count()).mul(100).round(1).astype(str) + '%' $ tweet_summary_df
tesla = pd.read_csv('twitter/Tesla_tweets.csv')
autos.rename({'yearOfRegistration' : 'registration_year','monthOfRegistration':'registration_month','notRepairedDamage':'unrepaired_damage','dateCreated':'ad_created','dateCrawled':'date_crawled','offerType':'offertype','vehicleType':'vehicletype','fuelType':'fueltype','nrOfPictures':'nrofpictures','postalCode':'postalcode','lastSeen':'last_seen'},axis=1,inplace=True) $
image_predictions_df[image_predictions_df.img_num != 1][0:5]
df['Phone'] = ['555-123-4567', '555-321-0000', '555-999-8765'] $ df
df2.drop_duplicates(['user_id'], inplace=True)
pold = df2.converted.mean() $ print(pold)
weather = join_df(weather, state_names, "file", "StateName")
print('Outliers are points below {} or above {}.'.format((scores_firstq - (1.5 * IQR)), (scores_thirdq + (1.5 * IQR))))
df2 = df2.drop_duplicates(['user_id'], keep = 'first')
from sklearn.model_selection import GridSearchCV $ param_grid = [{'weights': ["uniform", "distance"], 'n_neighbors': [3,5,7,9,11,13,15,17,19,21,23,25,27,29]}] $ knn_clf = KNeighborsClassifier() $ grid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3, n_jobs=-1) $ grid_search.fit(X_train, y_train)
sns.boxplot(autodf.price)
X_train.f = X.f[0:30892,] $ y_train.f = y.f[0:30892] $ X_test.f = X.f[30893:32315,]
reddit_comments_data.groupby('link_id').count().orderBy('count', ascending = False).show(100, truncate = False)
yc_sd.columns
bigdf.shape
leadDollarsClosedPerMonth = segmentData[['opportunity_won_amount', 'opportunity_month_year', 'lead_source']].pivot_table( $                                 values='opportunity_won_amount', $                                 index='opportunity_month_year', $                                 columns='lead_source', aggfunc='sum')
to_be_predicted_Day1 = 22.17 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
image_1 = np.expand_dims(x_test[0], axis=2) $ image_2 = np.expand_dims(x_test[1], axis=2)
results_lumpedTopmodel, output_LT = S_lumpedTopmodel.execute(run_suffix="lumpedTopmodel_hs", run_option = 'local')
from monkeylearn import MonkeyLearn $ ml = MonkeyLearn('5214f791698d7ea9b907d922cd6def91f05e1d39')
k_fold = KFold(n_splits=10, shuffle=True, random_state=0) $ score = cross_val_score(regressor, y, z, cv=k_fold, n_jobs=1, scoring='accuracy') $ print (score) $ round(np.mean(score)*100, 2)
sgdScore = SGDC.score(x_test, y_test) $ print ("Model Accuracy: %f" % sgdScore)
autos['odometer'].head()
raw_data = pd.read_csv('data/aligned_cmd.csv', index_col='DATES', usecols=['DATES', asset1, asset2]) $ df = raw_data.loc[bt_start_date:bt_end_date] $ bt_data = raw_data.loc[bt_end_date:] $ df.plot()
total.first_valid_index()
breed_predict_df_clean = breed_predict_df.copy()
jobPostDFSample = jobPostDFSample.drop("index", axis=1)
df = pd.read_sql("select a.item_id , a.order_item_size_id , sum(quantity) as quantity , b.title from order_details a inner join store_items_sizes b on a.order_item_size_id = b.order_item_size_id where a.item_id = 11 or a.item_id = 23 GROUP BY order_item_size_id, item_id",conn)
new_page_converted = np.random.choice(2,n_new,p=[0.8804,0.1196])
sample.dtypes
tz_dog.describe()
airbnb_df = airbnb_df[airbnb_df['city']=='Seattle'] $ airbnb_df['city'].value_counts(dropna=False)
pageRankPlot = prDf.head(33).plot.barh(title="PageRank score distribution", figsize=(14,7)) $ pageRankPlot = pageRankPlot.get_figure() $ pageRankPlot.savefig("plots/pageRankPlot.png")
iowa_file = '../../DSI-SF-3/datasets/iowa_liquor/Iowa_Liquor_sales_sample_10pct.csv' $ iowa = pd.read_csv(iowa_file) $ print "Dataframe is of size: " + str(iowa.values.nbytes / 10**6) + "MB"
fig, ax1 = plt.subplots(1,1, figsize=(8,3)) $ totals.T.plot(ax=ax1) $ ax1.set_xlim(1944,2018) $ fig.savefig(os.path.join(output_dir, 'Sox_totals.png'), dpi=200, bbox_inches='tight' )
sm_model = sm.Logit(df_new['converted'] , df_new[['intercept','ab_page_UK','ab_page_CA','CA','UK','ab_page']]) $ results_model=sm_model.fit() $ results_model.summary()
plt.hist(p_diffs) $ plt.axvline(x = real_diff, color='red')
len(df.index)
spp['DK'] = spp[spp.columns[spp.columns.str.contains('DK')==True]].sum(axis=1) $ spp['DE'] = spp[spp.columns[spp.columns.str.contains('DE')==True]].sum(axis=1)
connection = sqlite3.connect("../../data/stocks.sqlite") $ stocks = pd.io.sql.read_sql("SELECT * FROM STOCK_DATA;", connection, index_col="index") $ connection.close() $ stocks.head()
df_mas.timestamp = pd.to_datetime(df_mas.timestamp) $ df_mas.retweeted_status_timestamp = pd.to_datetime(df_mas.retweeted_status_timestamp)
sqlContext.sql("select * from pcs").show()
!pip install graphviz
p_new = df2.converted.mean() $ print(p_new)
feature_col = ibm_hr_final.columns $ feature_col
train_df['num_photos']=train_df['photos'].apply(len) $ num_photos = train_df['num_photos'].value_counts() $ x = num_photos.index $ y = num_photos.values $ sns.barplot(x, y )
uniques = pd.DataFrame(data=[train.columns.values, [len(train[col].unique()) for col in train.columns], [train[col].dtype for col in train.columns]]).T $ uniques = uniques.rename({0:'Column Name', 1:'Unique Values', 2:'Dtype'}, axis=1) $ uniques = uniques.sort_values(by='Unique Values', ascending=False).reset_index(drop=True) $ uniques
df_train = df[df['serial_number'].isin(train_disks_list)] $ df_test = df[df['serial_number'].isin(test_disks_list)]
type(api)
raw_df = pd.read_csv('./wild-wolf-watch-classifications.csv')
obj2
category = "men", "women", "mixed", "men", "women", "mixed" $ course = "long", "long", "long", "short", "short", "short" $ record_tables = [t for t in tables if "Event" in t.columns] $ len(record_tables)
client.experiments.get_status(experiment_run_uid)['state']
dates - dates[0]
df.ix['2015-09-03 11:00:00+01:00':'2015-09-03 12:00:00+01:00'].plot()# select a time range and plot it $
sns.boxplot(x=fraud_data_updated['class'],y=fraud_data_updated.purchase_value)
mgxs_lib.by_nuclide = True
MergeWeek.set_index('Date').to_csv('Sales_WeeklySummary.csv')
plt.plot(glons, glats, marker='.', color='k', linestyle='none') $ plt.show()
events = events[events.repo_id.isin(repos_ids.repos_ids)] $ events.shape
tbl = df[df.date_diff == 1].groupby('msno').date_diff.size().to_frame() $ tbl.columns = ['num_listen_music_in_a_row_count'] $ tbl['num_listen_music_in_a_row_ratio'] = tbl.num_listen_music_in_a_row_count / df.groupby('msno').date_diff.apply(len) $ tbl.reset_index(inplace = True) $ tbl
for f in zip(X_.columns, rfecv.support_): $     print(f)
selected=features[features.importance>0.0001] $ selected.sort_values(by="importance", ascending=False)
sex2NA=['adult','juv'] $ sex2m=['unm'] $ df.loc[df.sex.isin(sex2NA)==True] $ print(df.sex.loc[df.sex.isin(sex2NA)==True].count()) $ print(df.sex.loc[df.sex.isin(sex2m)==True].count())
num_users = df['user_id'].nunique() $ print('Number of unique users in dataset: ',num_users)
countries_df = pd.read_csv('countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
tables = [train_df, transactions_df, sample_sub_df, test_df, oil_df, holidays_df, stores_df, items_df]
tweets['id'].groupby(pandas.to_datetime(tweets['created_at']).dt.date).count().mean()
test_kyo1.describe()
df_merged.shape
inspector.get_table_names()
train_df["created_year"] = pd.to_datetime(train_df["created"], coerce=True).dt.year $ train_df["created_month"] = pd.to_datetime(train_df["created"], coerce=True).dt.month $ train_df["created_day"] = pd.to_datetime(train_df["created"], coerce=True).dt.day
(p_diffs > (p_new-p_old)).mean()
logit_mod_new = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','US', 'UK']]) $ results_new = logit_mod_new.fit() $ results_new.summary()
lm2 = sm.Logit(df_new['converted'],df_new[['ab_page','US','UK']]) $ results2=lm2.fit() $ results.summary()
ds_cnsm = xr.open_mfdataset(data_url2) $ ds_cnsm = ds_cnsm.swap_dims({'obs': 'time'}) $ ds_cnsm = ds_cnsm.chunk({'time': 100}) $ ds_cnsm = ds_cnsm.sortby('time') # data from different deployments can overlap so we want to sort all data by time stamp. $ ds_cnsm
logit_mod_joined2 = sm.Logit(df_joined_dummy.converted, \ $                            df_joined_dummy[['intercept', \ $                                             'ab_page_new_CA', 'ab_page_new_UK', 'ab_page_new_US']]) $ logit_mod_joined2_result = logit_mod_joined2.fit() $ logit_mod_joined2_result.summary()
df_A.loc[['s2','s5','s1']]
news_items = soup.find_all(class_='slide') $ news_title = news_items[0].find(class_='content_title').text $ print (news_title)
full_data.reset_index(drop=True, inplace=True)
cityID = '53b67b1d1cc81a51' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Birmingham.append(tweet) 
lon_us = lon[lon_li:lon_ui] $ lat_us = lat[lat_li:lat_ui] $ print(np.min(lon_us), np.max(lon_us), np.min(lat_us), np.max(lat_us))
dog_stage_stats['rating_numerator'].sort_values(ascending=False).plot(kind='bar', ylim=[10,13])
df['release_date'] = pd.to_datetime(df['release_date']) $ df['budget_adj'] = df['budget_adj'].astype(int) $ df['revenue_adj'] = df['revenue_adj'].astype(int) $ df['budget'] = df['budget'].astype(int) $ df['revenue'] = df['revenue'].astype(int)
save_n_load_df(ph, 'mergeable_holidays.pkl') $
etsamples_100hz.loc[etsamples_100hz.eyetracker=='el','pa_diff']=etsamples_100hz.query("eyetracker=='el'").pa_norm.values-etsamples_100hz.query("eyetracker=='pl'").pa_norm.values
T = 0 $ train = pd.read_csv('../../input/preprocessed_data/trainW-{0}.csv'.format(T))
df2.head(2)
goals_df.to_pickle('goals_df.pkl')
loan_requests[loan_requests.id_loan_request==122466]
didnt_convert=df.converted[df.converted==0].count() $ convert=df.converted[df.converted==1].count() $ portion_converted=convert/(didnt_convert+convert) $ print ('Portion converted is '+ str(portion_converted*100)+'%.')
test_data_scores(26, "Full_Mod_9+ratio_vars", $                  model=reg_logit, model_specs = model_9_ratio_vars, $                  x=x_train_advanced, y=y_train_advanced, $                  x_=x_test_advanced, y_=y_test_advanced)
test_df.head()
print(df2.query('landing_page == "new_page"').shape[0] / df2.shape[0]) $ obs_diff = treat_convert - cont_convert $ print(obs_diff)
print(model.params)
import pandas as pd $ iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)
from pandas.tseries.offsets import DateOffset
print(autos["odometer_km"].value_counts().sort_index()) $ print(autos["odometer_km"].unique().shape) $ print(autos["odometer_km"].describe())
help(pyodbc)
merged.info()
day_markers = np.arange(df1['DATETIME'].min(), df1['DATETIME'].max(), timedelta(days=1)) $ day_markers = np.append(day_markers, df1['DATETIME'].max()) $ day_markers
for tweet in collect.get_iterator(): $     print(json.dumps(tweet, indent=4)) $     break
ip.head(20)
df_new['intercept'] = 1
df= df[['created_at','display_text_range','favorite_count','full_text','retweet_count']] $ df['tweet_lenght']= df['display_text_range'].apply(lambda arr: arr[1]) $ x = df.drop(['display_text_range','retweet_count','favorite_count','full_text'], axis=1)   # Input Feature Matrix
coins_comp = ['Bitcoin', 'Ethereum', 'Litecoin', 'Ripple']
joined.reset_index(inplace=True) $ joined_test.reset_index(inplace=True)
print(twitter_archive_df.columns) $ twitter_archive_df.head()
needles = re.findall(r'\d{1,2}\.(mp3|wav)', haystack) $ print(needles)
obj4
from sklearn import svm $ clf = svm.SVC(kernel='rbf') $ clf.fit(X_train, y_train) 
df2.drop(2893, inplace = True) $ df2.query('user_id == 773192')
normed_score[:5]
df.dtypes
print(autos['price'].max()) $ print(autos['price'].min())
newdf.drop(newdf.columns[0],axis=1,inplace=True)
url = "https://api.census.gov/data/2016/acs/acs1/variables.json" $ resp = requests.request('GET', url) $ aff1y = json.loads(resp.text)
new_page = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $ print(len(new_page))
name_sentiments.ix[::25]
monthly_medication_df.first_month_adherence.mean()
df_input_final.show(10)
real_test_df = model_df.iloc[725:-1].copy() $ real_test_df['model_predict'] = np.NaN
from spectrum_tools import db_connect
crosstab = pd.crosstab(index=purchase_history.user_id, columns=purchase_history.product_id)
sql_usedb = 'use HelloDB;' $ cur_helloDB.execute(sql_usedb)
!head -n 5 p32c_results.txt | python frequencies_mapper3.2.Cfr.py | sort -k3,3n | python frequencies_reducer3.2.Cfr.py
few_recs.ix[0]
bs=64; bptt=70
df_concat['date'] = pd.DatetimeIndex(df_concat.date_series).normalize() $ df_concat.date.head()
data_2012=emotion_big_df[(emotion_big_df['createdAt']>='2012-01-01 00:00:00') & (emotion_big_df['createdAt']<'2013-01-01 00:00:00')]
req = urllib2.Request(googleurl+'?'+request) $ print req $ print req.get_full_url() $ print req.get_method
pprint(Counter( [d['PutRequest']['Item']['pmcid']['S']  for d in output_dict['demographics']]).most_common(40)) $ pprint(Counter( [d['PutRequest']['Item']['pmid']['S']  for d in output_dict['demographics']]).most_common(40)) $ pprint(Counter( [d['PutRequest']['Item']['date_processed']['S']  for d in output_dict['demographics']]).most_common(40)) $ pprint(Counter( [d['PutRequest']['Item']['title']['S']  for d in output_dict['demographics']]).most_common(40))
autos['odometer_km'].describe()
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page' and converted == 1").shape[0] $ convert_new = df2.query("landing_page == 'new_page' and converted == 1").shape[0] $ print(convert_old, convert_new, n_old, n_new)
pd.Timestamp('2014-12-15')
titanic.pivot_table('survived', index='sex', columns='class', aggfunc='sum')
archive_df.sample(5)
for i, correct in enumerate(correct[:9]): $     plt.subplot(3,3,i+1) $     plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none') $     plt.title("Predicted {}, Class {}".format(predicted_classes[correct], y_true[correct])) $     plt.tight_layout()
X_train, X_test, y_train, y_test = train_test_split(train_data_features, y, test_size=0.15, random_state=42)
firstWeekUserMerged["day_time_stamp2"] = firstWeekUserMerged.time_stamp2.dt.day
Conversion_No1=df2.loc[df2['landing_page']=="old_page",].shape[0] $ print("The no of observations regarding the old page equals",Conversion_No)
dummy_df = pd.get_dummies(data=countries_df, columns=['country']) $ df4 = dummy_df.merge(df3, on='user_id') $ df4 = df4[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'country_CA', 'country_UK', 'country_US', 'intercept', 'converted']] $ df4[['ab_page', 'country_CA', 'country_UK', 'country_US', 'intercept', 'converted']] = df4[['ab_page', 'country_CA', 'country_UK', 'country_US', 'intercept', 'converted']].astype(int) $ df4.head()
tokaise = read_csv("tokaise.csv") $ tokaise.head(50)
type(df_train_time.fulldate2[0])
active_station = str(session.query(Measurement.station).group_by(Measurement.station).\ $                        order_by(func.count(Measurement.id).desc()).first())
r = pd.read_csv('price.csv')
import crowdtruth
learn.lr_find(lrs/1000) $ learn.sched.plot()
new_set_two = fraud_data_updated[fraud_data_updated['class']==0]
model = sklearn.linear_model.LinearRegression() #use the scikit linear regression model
matthew.head()
df['date'] = pd.to_datetime(df['starttime'])
print("Test After", test['StateHoliday'].unique()) $ test.replace({"StateHoliday": {"0": 0}}, inplace=True) $ print("Test After", test['StateHoliday'].unique())
z_score,  p_value =  sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print('Z score - {} and p-value - {}'.format(z_score,p_value))
1/np.exp(-0.0783), 1/np.exp(-0.0314), 1/np.exp(-0.0118), 1/np.exp(0.0057)
constructor['price'] = price_series
git_blame.path.nunique()
week9 = week8.rename(columns={63:'63'}) $ stocks = stocks.rename(columns={'Week 8':'Week 9','56':'63'}) $ week9 = pd.merge(stocks,week9,on=['63','Tickers']) $ week9.drop_duplicates(subset='Link',inplace=True)
regr.fit(X_train, y_train)
df2[df2['converted'] == 1].count()[0]/df2.shape[0]
user_logs.sort_values(by = 'date').date.unique()
twitter_Archive['in_reply_to_status_id'] = twitter_Archive['in_reply_to_status_id'].astype(str) $ twitter_Archive['in_reply_to_user_id'] = twitter_Archive['in_reply_to_user_id'].astype(str) $ twitter_Archive['tweet_id'] = twitter_Archive['tweet_id'].astype(str) $ twitter_Archive.info()
sum(bet_under)
print("The no of Unique user_id in dataframe are",df2['user_id'].nunique())
data['Created Date'].tail()
data['title_len'].sort_values(ascending= False)
user["favourites_count"] = user["favourites_count"].apply(np.exp2) $ user.head(3)
titanic.drop('deck', axis=1, inplace=True)
df1.info()
survived = df_titanic['survived'] $ print(survived.describe()) $ df_titanic['survived'] = df_titanic.survived.astype('category')
raw_readings = {}    $ for row in rows:    $     raw_readings.setdefault(tuple(row[:4]), []).append(tuple(row[4:])) 
result['UTC_create_date'] = result['CreateDate'].apply(str_to_utc) $ result['UTC_modify_date'] = result['ModifyDate'].apply(str_to_utc)
len(train_data[train_data.vehicleType == 'kleinwagen'])
plt.figure(figsize=(8, 5)) $ train_df.flow.value_counts().plot.bar(); $ plt.title('Number of #hab by flow') $ plt.xticks(rotation='horizontal'); $
for res_key, df in data_sets.items(): $     logger.info(res_key + ': %s', df.shape) $ for res_key, df in entso_e.items(): $     logger.info('ENTSO-E ' + res_key + ': %s', df.shape)
import pandas as pd $ import numpy as np $ import matplotlib as plt $ %pylab inline
df1 = gbq.read_gbq(querry, project_id = project_id, dialect = "standard") $ df1.head()
jdfs.loc[~jdfs.fork]
result
df2[ids.isin(ids[ids.duplicated()])]
non_usa_states = ['ON', 'AP', 'VI', 'PR', '56', 'HY', 'BC', 'AB', 'UK', 'KA'] $ print 'Total amount for locations outside USA: ', sum(df[df.state.isin(non_usa_states)].amount) $
interact(plot_data, date=data.index)
data["Improvements"] = data["Improvements"].apply(filtering)
print(s) #print output
p_diff = p_new-p_old
!pttree -L 2 --use-si-units --sort-by 'size' 'data/my_pytables_file.h5'
support.amount.sum()/merged.amount.sum()
Boxscores = get_boxscores() $ Boxscores.head()
groups = openmc.mgxs.EnergyGroups() $ groups.group_edges = np.array([0., 0.625, 20.0e6])
train.head()
df2.sort_values('fatalities',ascending=False).head(5) #top 5 fatalities
import yahoo_finance $ yahoo_finance.pandas.options.display.max_rows = 10
sales_leads = df_leads[df_leads.Managed_By__c=='Direct Sales'] $ reseller_leads = df_leads[df_leads.Managed_By__c=='Reseller'] $ partner_leads = df_leads[df_leads.Managed_By__c=='Partner Org'] $ unmanaged_leads = df_leads[df_leads.Managed_By__c=='Unmanaged']
df_new['intercept'] = 1 $ df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']]
train.head()
from pyspark.sql.functions import monotonically_increasing_id $ modeling1 = (train $              .withColumn('id', monotonically_increasing_id()) $              .drop('click_time', 'attributed_time')) $ print("modeling1 size: ", modeling1.count())
df_clean['date'] = pd.to_datetime(df_clean['date']) $ df_clean['time'] = pd.to_datetime(df_clean['time'])
gdf.sample(10)
df2.converted.mean()
sel_df.info()
n_periods = 1 $ df_survival.loc[:, 'Churn Date'] = pd.to_datetime(obs_end_date)-df_survival.loc[:, 'Frequency of Donations']*n_periods
pca=decomposition.PCA() $ stocks_pca_t4= pca.fit_transform(stocks_pca_m_in)
skip_idx = random.sample(range(1, num_lines), num_lines - size)
num_unique_users = df['user_id'].nunique() $ num_unique_users
df3[['CA','UK','US']] = pd.get_dummies(df3['country']) $ df3.head()
d_bnbAx = pd.get_dummies(bnbAx['first_browser'], prefix = 'brow_')
sphere.subjectivity
df1 = df1.dropna()
offseason10["InorOff"] = "Offseason"
import statsmodels.api as sm $ x = sm.Logit(df2['converted'], df2[['intercept' ,'treatment']]) $ y = x.fit()
autos = autos[autos['registration_year'].between(1886, 2016)]
subred_num_tot.rename(index=str, columns={'subreddit': 'Subreddit', 'num_comments': 'Total_Num_Comments'}, inplace=True)
df
print('\nAverage value of each column:\n', google_stock.mean())
contributions = pd.read_csv("http://www.firstpythonnotebook.org/_static/contributions.csv")
model = AuthorTopicModel.load('/tmp/model.atmodel')
df2.user_id.nunique()
duration_df = hours[['Specialty', 'AppointmentDate', 'AppointmentDuration', 'Hours_Spent']]
df3=df2 $ df3['intercept'] = 1 $ df3[['control','ab_page']] = pd.get_dummies(df2['group']) $ df3.head()
cityID = '0eb9676d24b211f1' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Cleveland.append(tweet) 
data.isnull().sum()
df_uro_no_metac = df_uro.drop(columns = ['MET_DATE1']) $ df_uro_no_metac.head()
y_proba_argmax
loans_act_xirr=cashflows_act_investor.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
local_cor
labels = ['acc_id','acc','tweet_id','text','created_time','ex_lat','ex_long','place','lot_sw','lot_ne','lang'] $ tweet_df = pd.DataFrame(tweet_list,columns=labels)
lgbm_train=train[['air_store_id','visit_date']] $ lgbm_train['visitors'] = lgbmrstcv.predict(train[col].values)
snow.select("select * from nk_mnd_index order by index_date desc limit 5")
df_ml_6204_01.tail(5)
free_data.groupby('age_cat')['educ'].mean()
url_PIT = "https://steelers.strmarketplace.com/Images/Teams/PittsburghSteelers/SalesData/Pittsburgh-Steelers-Sales-Data.xls"
file_writer = tf.summary.FileWriter('./graph/bilstm_crf_elmo_bio_multi', tf.get_default_graph())
summary_bystatus = df.groupby('status')['MeanFlow_cms'].describe(percentiles=[0.1,0.25,0.75,0.9]).T $ summary_bystatus
X = df[features] $ y = np.ravel(df[target]) $ rfe = RFE(model, 13) $ rfe.fit(X, y)
gs = GridSearchCV(<model>,params,cv=5,scoring='neg_mean_squared_error')
datAll['zip'] = datAll['zip'].apply(pd.to_numeric, errors='coerce')
df_new[['CA','UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new.head()
new_doc = UnicodeDammit.detwingle(doc) $ print(new_doc) $ print(new_doc.decode('utf8')) $
df1.join([df2, df3, df4]).fillna('')
pd.merge(df1, df3, left_on="employee", right_on="name" )
full_data['order_date'] = pd.to_datetime(full_data.order_date,format= '%Y-%m-%d')
mean_vecs = np.mean(hidden_states, axis=1) $ max_vecs = np.max(hidden_states, axis=1) $ sum_vecs = np.sum(hidden_states, axis=1)
df_vow.plot()
print(duplicate)
top_songs.head()
X_counts_df.columns = count_vect_sample.get_feature_names() $ X_counts_df
pt.to_csv('C:\\Users\\ChandraMouli\\Desktop\\summary.csv')
train['Survived'].describe()
%%time $ lr1 = LogisticRegression(random_state=20, max_iter=10000, C=0.5, multi_class= 'ovr', solver= 'saga') $ lr1.fit(X, y) $ lr1.score(X_test, y_test)
type('A',(),{"a":1})
np.zeros(5)
data['cat'].value_counts()
df_all.age.fillna(value=0, inplace=True) $ df_all.age[df_all.age > 116] = 0 $ df_all.age[df_all.age < 15] = 0 $
reddit.corr()
run txt2pdf.py -o '2011 Snapshot.pdf' '2011 Snapshot.txt' $
Genres=movie_rating['genres'].values.tolist()
US_cases[US_cases.Merkmal_typ=='hauptmerkmal'].Merkmalcode.unique()
import os $ import glob $ import pandas as pd
set_renv_token("data_sci_8001_token.rds")
autos['ad_created'] = autos['ad_created'].str[:10] $ (autos['ad_created'] $ .value_counts(normalize=True,dropna=False) $ .sort_index())
frame.sort_values(by='b')
d9 = d8.unstack() $ d9
lims_query = "SELECT ephys_roi_results.id, specimens.id AS cell_id, specimens.name, specimens.ephys_roi_result_id \ $ FROM ephys_roi_results JOIN specimens ON specimens.ephys_roi_result_id = ephys_roi_results.id" $ lims_df = get_lims_dataframe(lims_query) $ lims_df.tail()
autos.price.unique()
if created: $     libraries.to_excel('/dev/shm/libraries.xlsx', index=False)
df['oc_postcodes'] = df.oc_results.apply(oc_postcodes)
input_data.normalized_commits.describe()
result = run_pipeline(make_pipeline(), '2015-05-05', '2015-05-05') $ print('Number of securities that passed the filter: %d' % len(result)) $ result.head()
d = {'Flavor': ['Strawberry', 'Vanilla', 'Chocolate'], 'Price': [3.50, 3.00, 4.25]} $ icecream = pd.DataFrame(data=d) $ icecream
malware[0]
rGraphData.head(20)
joined_patient.limit(10).toPandas()
places = api.geo_search(query="London", granularity="city") $ place_id_L = places[0].id $ print('London id is: ',place_id_L)
df[df.index.month.isin([1,2,3])].head()
df.iloc[0:5,:12]
%matplotlib inline
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_opb_chord_saddle[1][0])) $ perr = np.sqrt(np.diag(pcov_opb_chord_saddle[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
df.select_dtypes(include=categories).head()
df2['timestamp']=pd.to_datetime(df['timestamp'],format='%Y-%m-%d %H:%M:%S') $ df2.dtypes
feature_list = ["cmcnt_seg_tvl_p1y", "cmcnt_lh_seg_tvl_p1y", "cmsum_rev_tvl_p1y", "cmint_age_now"] $ vecAssembler = VectorAssembler(inputCols=feature_list, outputCol="features") $ feature_sel = vecAssembler.transform(all_feature)
import statsmodels.api as sm $ convert_old = 17739 $ convert_new = 17498 $ n_old = 147239 $ n_new = 147239
offseason18["InorOff"] = "Offseason"
data['Incident Zip'] = data['Incident Zip'].apply(fix_zip)
sum(df["statuses_count"][:660]>45000)
weather.loc[weather.NAME == 'RALSTON RESERVOIR, CO US'].boxplot(column="TMIN");
prepared[['goal','backers_count','days_open']].describe()
df_ad_airings_4['ad_duration'] = df_ad_airings_4['end_time'] - df_ad_airings_4['start_time']
df2 = pd.merge(donations, df_by_donor, how='inner', on='Donor ID') $ df3 = pd.merge(df2, donors, how='inner', on='Donor ID') $ print(df3.head())
del datatest['aux']
df_users_6_after=df_users_6[df_users_6['DSA_account_created_before_or_after']=='After']
rf1000 = RandomForestClassifier(n_estimators=1000) $ rf1000.fit(X, y)
r2 = yfs2.session.get(url, params={'format': 'json'}) $ r2.status_code
df_dates_new.to_csv('medium_urls_dates_unique_NEW.csv')
scaled_data = X
s.str.split('_',expand=True, n=5)  # limit expansion into n columns
submission2 = pd.DataFrame(submission, index=submission['id']) $ submission2[['proba']] = preds[:,1] $ submission2.drop(['id', 'proba2'], inplace=True, axis=1) $ submission2.to_csv('sub.csv')
authors_per_file = git_blame.groupby(['path']).author.nunique() $ authors_per_file[authors_per_file == authors_per_file.max()]
df_enhanced = df_enhanced.drop('rating_denominator', axis=1)
for i in range(-5, 0, 1) : $     data[f'High {i}d'] = data['High'].shift(-i) $ data = data.dropna() $ data.head()
df_archive_clean.text[24]
df_members.head()
open_prs_by_authors = response['0']['buckets'] $ print(buckets_to_df(open_prs_by_authors))
plt.boxplot(train_data['price'].values)
df[['Footnote']].isnull().sum() $
pd.merge(transactions,transactions,how='outer',on='UserID')
sns.barplot(x='user',y='number_of_retweets',data=most_retweeted_tweeps_sorted.tail(5))
house_data.info()
train = pd.read_json('./data/train.json')
props.head()
New_df = Type_date.groupby(['New_or_Returning']).get_group('New').reset_index() $ New_df.head()
'acct_type' in query_df.columns
usage_400hz_filter_pd = usage_400hz_filter.toPandas()
df_1.info()
ser5.iloc[1]#Purely integer-location based indexing for selection by position.
freqs = numpy.logspace(-2.8, -2, 5)
df1=pd.DataFrame({'key':['K0','K1','K2','K3'],'A':['A0','A1','A2','A3'],'B':['B0','B1','B2','B3']}) $ df2=pd.DataFrame({'key':['K0','K1','K2','K3'],'C':['C0','C1','C2','C3'],'D':['D0','D1','D2','D3']}) $ print(df1) $ print(df2)
final_titles_list
rng_dateutil = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz='dateutil/Europe/London')
tweets_df = pd.DataFrame(tweets_df, columns = ['id', 'retweeted','retweet_count','favorite_count'])
url = 'https://mars.nasa.gov/news/'
suspects_with_25_2['in_cp'] = suspects_with_25_2.apply(in_checkpoint, axis=1)
y_predicted = fit4.predict(X4) $ plt.plot(y4, y_predicted, 'b.') $ plt.title('Actual Gross Outcome vs. Budget') $ plt.xlabel('Budget') $ plt.ylabel('Domestic Gross Total')
model.most_similar("awful")
stations = session.query(Measurement).group_by(Measurement.station).count() $ print("There are {} stations.".format(stations))
UK_control_conversion = df_c_merge[(df_c_merge['group']== 'control') & (df_c_merge['country'] == 'UK')]['converted'].mean() $ UK_control_conversion
sizes = tdf['size'].as_matrix() $ print(f'Number of instances = {sizes.shape[0]}') $ print(f'Mean (population) = {np.mean(sizes):5.3f}') $ print(f'Standard deviation (population) = {np.std(sizes):5.3f}')
views_data = pd.read_json('data/pdpviews.ndjson', lines=True, chunksize=1000000) $ views_data_clean = pd.DataFrame([],columns=['browser_id', 'product_id', 'timestamp']) $ for views_chunk in views_data: $     views_chunk = views_chunk.drop(['price','source','user_id'],axis=1) $     views_data_clean = views_data_clean.append(views_chunk.loc[views_chunk.browser_id.isin(unique_buying_browsers)])
niners_offense = nfl[((nfl["HomeTeam"] == 'SF') | (nfl["AwayTeam"] == 'SF')) $                      & (nfl["DefensiveTeam"] != 'SF') ] 
save_n_load_df(joined, 'joined_elapsed_events.pkl')
prob_rf200 = rf200.predict_proba(Test) $ prob1_rf200 = pd.Series(x[1] for x in prob_rf200) $ Results_rf200 = pd.DataFrame({'ID': Test.index, 'Approved': prob1_rf200}) $ Results_rf200 = Results_rf200[['ID', 'Approved']] $ Results_rf200.head()
for column in df.columns: $     print column, df[column].isnull().sum()
data.head(20)
df_new.loc[ df_new['followers'] >= 200 , 'score_followers_1'] = 15 $ df_new.loc[ (df_new['followers'] < 200) & (df_new['followers'] >= 50) , 'score_followers_1'] = 10 $ df_new.loc[ (df_new['followers'] < 50) & (df_new['followers'] >= 30) , 'score_followers_1'] = 5 $ df_res['score_followers'] = df_new['score_followers_1'] $ df_res $
xml_in['publicationDate'] = pd.to_datetime(xml_in['publicationDate'], format='%Y-%m-%d', errors='coerce')
geocoded_evictions_df = eviction_df.join(geocoded_addresses_df.drop(['Defendant.Addr.Line.1', 'index'], axis=1).set_index('Case.Number')) $ geocoded_evictions_df.to_csv('geocoded_evictions.csv', encoding='utf-8') $ geocoded_evictions_df.head()
hdaysDF = trainDF[['MONTH', 'DAY_OF_MONTH', 'HDAYS']].drop_duplicates()
columns = orgs.columns
plt.hist(p_diffs) $ plt.axvline(x=diffs, color= 'red');
pp_dict(collection.find_one())
artistByID.filter(lambda line: '[unknown]' in line).take(5)
def sentiment_finder_vader(comment): $     analysis = analyser.polarity_scores(comment) $     return analysis['compound']
criteria = so['ans_name'].isin(['Scott Boston', 'Ted Petrou', 'MaxU', 'unutbu']) $ so.loc[criteria].head()
loans_fut_bucket_30360_xirr_orig*100 
df_stemmed_train = df['text_stemming'].tolist()
pd.set_option("max.rows", 10) $ result
noise_graf = pd.merge(noise, graf_counts, on='AFFGEOID')
df[['beer_name', 'brewery_name', 'rating_score']][(df.brewery_name.str.contains('Arcadia')) & (df.beer_name.str.startswith('IPA'))]
df.shape
for c in ccc: $     for i in vhd[vhd.columns[vhd.columns.str.contains(c)==True]].columns: $         vhd[i] /= vhd[i].max()
collect
z1.interval.isnull().sum()
joined_hist.joined = pd.to_datetime(joined_hist.joined) $ joined_hist = joined_hist.sort_values(ascending=True, by='joined') $ joined_hist.head() $
z_score, p_value = sm.stats.proportions_ztest([old_page_converted, new_page_converted], [n_old, n_new], alternative='smaller') $ z_score, p_value
stationCounts = engine.execute('SELECT station, count(*) AS station_cnt FROM measurement group by station order by count(*) desc').fetchall() $ print (stationCounts) $
data = allocate_equities(allocs=[0.25,0.25,0.25,0.25]) $ data.plot() $ plt.show()
params = {'n_estimators':[400], $           'min_samples_split':[1,5,10,20], $           'max_depth':[10,15]} $ rf = RandomForestClassifier(n_estimators=400, min_samples_split=100, n_jobs=4) $ best_params = optimizer(rf, train_x[variable_indices[:10]], train_y, params, n_jobs=2)
b.iloc[0:1]
y_test_under[bet_under].mean()
plt.scatter(aqi['AQI_eug'], aqi['AQI_cg'], alpha=0.2) $ plt.xlabel('Eugene/Springfield'); plt.ylabel('Cottage Grove');
df_arch_clean.timestamp = pd.to_datetime(df_arch_clean.timestamp)
df1_normal = df1.copy() $ df1_normal['y'] = np.log(df1_normal['y'])
sessions =pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/sessions.csv') $ sessions.head()
ridgemodel = linear_model.Ridge(alpha =1) $ fit2= ridgemodel.fit(X,y)
df['text2'] = vader_df.text
train_Bow.head()
prediction = predict_test(most_recent,train,all_data,84,38) $ display(prediction)
pred = pipeline2.predict(cvecogXfinaltemp) $ print classification_report(pred, ogy) $ print confusion_matrix(ogy, pred) $ print "cross val score accuracy :"+str(sum(cross_val_score(pipeline2, cvecogXfinaltemp, ogy))/3) $ print 'roc_auc score :'+str(sum(cross_val_score(pipeline2, cvecogXfinaltemp, ogy, scoring='roc_auc'))/3) $
counts_df_clean.head()
url = "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv" $ r = requests.get(url) $
segmentData['opportunity_size'] = segmentData.apply(oppAmount, axis=1)
print(DataSet_sorted['tweetText'].iloc[-1])
dfdaycounts.head()
All_tweet_data_v2=pd.melt(All_tweet_data_v2, id_vars=Remaining_columns, value_vars=stages, var_name="Remove", value_name='Stage')
date_df = date_df[['count', 'date', 'weekday', 'duration(min)']] $ date_df.head()
liberia_data.Description.value_counts()
autos.columns = ['date_crawled', 'name', 'seller', 'offer_type', 'price', 'abtest', $        'vehicle_type', 'registration_year', 'gearbox', 'power_ps', 'model', $        'odometer', 'registration_month', 'fuel_type', 'brand', $        'unrepaired_damage', 'ad_created', 'nr_of_pictures', 'postal_code', $        'last_seen']
form_btwn_teams.tail(3)
df_full.rename(columns=DATA_L2_HDR_DICT,inplace=True)
df.sum()
df2 = df[(df['group'] == 'treatment') == (df['landing_page'] == 'new_page')]
from sklearn.model_selection import train_test_split
which_files=[10,12,14] $ for k in range(len(which_files)): $     print('run txt2pdf.py -o ' + '"' + list_of_files[which_files[k]][:-4] + '.pdf' + '"  "' +\ $          list_of_files[which_files[k]] + '"') $
data.iloc[:3, :2]
n_new = df2[df2['group']=='treatment']['user_id'].nunique() $ n_new
image_predictions_clean.info()
result = result.sort_values('taskid') $ display(result.head(5))
from sklearn import preprocessing $ X = preprocessing.StandardScaler().fit(X).transform(X) $ X[0:5]
avg_km_series = pd.Series(avg_km_by_brand, dtype=int) $ price_vs_km["mean_odometer_km"] = avg_km_series $ price_vs_km
(autos["date_crawled"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_values() $         )
del room_temp.temperature
df_fireworks = df[df['Complaint Type'] == 'Illegal Fireworks'] $ df_fireworks.groupby(df_fireworks.index.month)['Created Date'].count().plot(kind="bar") $
known_shorteners += ['youtube.com']
url = "https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/09_Time_Series/Apple_Stock/appl_1980_2014.csv" $
sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller')
airline_training_set = nltk.classify.util.apply_features(extract_features, airline_tweets) $ NBClassifier = nltk.NaiveBayesClassifier.train(airline_training_set)
%%time $ entity_id_list =  list(engine.execute("SELECT entity_id, COUNT(*) FROM states \ $ GROUP BY entity_id ORDER by 2 DESC"))
ab_dataframe.query(' (group=="treatment" & landing_page != "new_page") | (group !="treatment" & landing_page == "new_page" ) ').shape[0]
del(df['Unnamed: 0'])
df2['ab_page'] = pd.get_dummies(df2.group)['treatment'] $ df2.head()
pd.get_dummies(df.A)
df['timestamp'] = df['timestamp'].str[0:10] $ df_TimeStamp = pd.to_datetime(df['timestamp'])
import pandas as pd $ import numpy as np $ import math $ import matplotlib.pyplot as plt $ %matplotlib inline
p_new = df2.converted.sum()/len(df2) $ p_new
closed_pr = PullRequests(github_index).is_closed()\ $                                       .since(start=start_date).until(end=end_date)\ $                                       .get_percentiles("time_to_close_days").by_period() $ print(get_timeseries(closed_pr, dataframe=True).tail())
data_donald.head()
df['artist'].value_counts()
combined = pd.concat([reddit, cvec_df], axis=1, join_axes=[reddit.index])
base_folder = os.path.abspath(os.getcwd()) $ os.chdir(".") $ db_folder = os.getcwd() + "/new_data" $ os.chdir(db_folder)
carrierDF.head()
SANDAG_age_df[SANDAG_age_df['AGE_RANGE'] == '10 to 19'].head()
training_df, test_df = scaled_df[:training_size], scaled_df[training_size:]
df_capsule["Capsule id"] = df_capsule["Capsule id"].apply(mvoid_to_bson_id) $ df_capsule["author user id"]=df_capsule["author user id"].apply(mvoid_to_bson_id) $ df_capsule["is draft"]=df_capsule["is draft"].apply(lambda x: 1 if x==True else 0) $ df_capsule["is public"]=df_capsule["is public"].apply(lambda x: 1 if x==True else 0) $ df_capsule["approximate capsule duration"]=df_capsule["approximate capsule duration"].astype('float') $
df.to_excel("msft.xls", sheet_name="msft")
import csv
GenVolMvt = (delta(df_new, df_old, ['GenerationMWH']) * (df_new['RealPrice'] - df_old['GenerationCost'].abs()/df_old['GenerationMWH'])).rename('GenVolMvt') $ GenCostMvt = ((df_new['GenerationCost'].abs()/df_new['GenerationMWH'] - df_old['GenerationCost'].abs()/df_old['GenerationMWH']) * df_new['GenerationMWH'] * (-1)).rename('GenCostMvt') 
df=df.drop('SEC filings',axis=1)
life = TextBlob("I love life") $ life.sentiment
n_new = df2[df2['landing_page']=='new_page'].shape[0] $ p_new = df2.converted.mean() $ new_page_converted = np.random.choice([0,1], n_new, p= [1-p_new, p_new]) $ unique, counts = np.unique(new_page_converted, return_counts=True) $ dict(zip(unique, counts))
new_read = pd.merge(read, article[['id','read_time']], how='left', left_on=['article_id'], right_on = ['id']) $ print(new_read.shape) $ new_read[0:10] 
c = pd.read_csv('countries.csv') $ df3 = df2.merge(c, on ='user_id', how='left') $ df3.head()
tweet_data['text_tokenized'] = tweet_data['text'].apply(lambda x: word_tokenize(x.lower())) $ tweet_data['hash_tags'] = tweet_data['text'].apply(lambda x: hash_tag(x)) $ tweet_data['@_tags'] = tweet_data['text'].apply(lambda x: at_tag(x))
sum(autos["registration_year"]==2018)
to_be_predicted_Day4 = 36.4833672 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
snow.select ("select count (distinct patient_id) from st_ra1")
df_pconverted = df.query('converted == 1')['user_id'].nunique() / df['user_id'].nunique() $ df_pconverted
np.random.seed(500) $ x=np.linspace(1,10,100)+ np.random.uniform(low=0,high=.5,size=100)   #purpose of this command???? $ y=np.linspace(1,20,100)+ np.random.uniform(low=0,high=1,size=100) $ print ('x = ',x) $ print ('y= ',y)
dfTemp.to_csv('../data/train_step1.csv')
broadcast_list_sc = sc.broadcast(broadcast_list)
trainDataVecs[100][0:10]
lm=sm.Logit(df2['converted'], df2[['intercept', 'CA', 'UK']]) $ results=lm.fit() $ results.summary()
q1 + 1 # Gives you the Q2...also ending in December!?
exiftool -csv -createdate -modifydate cisnwh8/cisnwh8_cycle1.MP4 cisnwh8/cisnwh8_cycle2.MP4 cisnwh8/cisnwh8_cycle3.MP4 cisnwh8/cisnwh8_cycle4.MP4 cisnwh8/cisnwh8_cycle5.MP4 cisnwh8/cisnwh8_cycle6.MP4 > cisnwh8.csv
archive_clean.to_csv('twitter_archive_master.csv')
from sklearn.preprocessing import StandardScaler $ scaler = StandardScaler() $ X = scaler.fit_transform(X) $ X = pd.DataFrame(X,columns=col_names) $ X.head(5)
df["TOTAL_PAYMENT"].sort_values(ascending=False)
swinbounds.head()
temp = df2.landing_page.value_counts() $ prob = temp/temp.sum() $ print "Probability of an individual received the new page:", prob[0]
m = regr.coef_[0][0] $ m
num_data=pd.DataFrame(df[['sale','volume_sold_l','bottles_sold','state_cost','state_retail','bottle_ml']],index=None) $ num_data.head()
small_frame.rbind(small_frame)
calls_df=calls_df.drop(["list_id","processed"],axis=1)
from sklearn.linear_model import LogisticRegression $ logreg = LogisticRegression() $ print(cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean())
plate_appearances = plate_appearances.merge(events, on='atbat_pk', suffixes=['_ignore', ''])
df2.drop(df2.index[1899], inplace=True)
print('Number of rows before duplicated value removed, {}.'.format(df2.shape[0])) $ df2.drop_duplicates(['user_id'], inplace=True) $ df2 = df2.drop('duplicated', axis=1) $ print('Number of rows after duplicated value removed, {}.'.format(df2.shape[0]))
dot818_dup = [496782300, 497731973, 530863807,572831958,606014331,486782526] $ fields = ['APP_APPLICATION_ID','APPLICATION_DATE','APP_SOURCE','APP_SSN','APP_LAST_NAME','APP_FIRST_NAME','DEC_FINAL_DECISION'] $ extract_all.loc[(extract_all.APP_SSN.isin(dot818_dup)), fields].to_excel(cwd+'\\DOT818 Duplicates Example.xlsx',index=False)
flight.write.parquet(pq_file_name)
cfd_nums = [vs for word in wsj $             for vs in re.findall(r'[a-z][a-z]', word.lower())] $ cfd = nltk.ConditionalFreqDist(cfd_nums) $ df_cfd = pd.DataFrame(cfd).fillna(value=0).astype(dtype=int) $ df_cfd
len(text[mask])
data = pd.DataFrame(tweet_ds)
avg_retweet = DataSet['tweetRetweetCt'].mean() $ median_retweet = DataSet['tweetRetweetCt'].median() $ print("Average number of retweets =", avg_retweet) $ print("Median number of retweets =", median_retweet)
with tb.open_file(filename='data/my_pytables_file.h5', mode='a') as f: $     f.create_array(where='/gr1/gr2/gr3', $                    name='some_not_yet_existing_array', $                    obj=[4, 5, 6, 7], $                    createparents=True)
df.loc[:, topics[0]:topics[-1]] = df.apply(lambda x: \ $                                            pd.Series([t in x['tags'] for t in topics], index=topics), axis=1)
df_columns.head() $
new_w = np.zeros((vs, em_sz), dtype=np.float32) $ for i, w in enumerate(itos): $     r = stoi2[w] $     new_w[i] = enc_weights[r] if r >= 0 else row_m
from bisect import bisect $ def commission_rate(sales_value, tier_ceiling=[1000, 2000, 3000], $                     rates=[0, 0.01, 0.03, 0.05]): $     index = bisect(tier_ceiling, sales_value) $     return rates[index]
weather_mean.iloc[2, 6]
duration_df.info()
df_weather.drop_duplicates(['STATION_NAME','YEAR','MONTH','DAY_OF_MONTH','HOUR'],inplace=True) $ df_weather.drop('DATE',axis = 1,inplace=True)
order_drop = order.drop(Imputation_columns,axis=1)
merged_portfolio['ticker return'] = merged_portfolio['Adj Close'] / merged_portfolio['Unit Cost'] - 1 $ merged_portfolio
mock_data = mock_data[(mock_data['ABPm_x'].isnull() == False) & (mock_data['ABPm_y'].isnull() == False) & (mock_data['ABPm'].isnull() == False) & (mock_data['ABPm'] > 0)].head(20) $ mock_data.to_json()
url =  "http://www.basketball-reference.com/teams/ATL/executives.html"
class_merged.to_csv('class_merged.csv',sep=',')
df_sumofmonth
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 7)) $ for i, (title, group) in enumerate(calculated_average_df.groupby(DEFAULT_NAME_COLUMN_COUNTRY)): $     group.plot.bar(x='Month', title=title, ax=axes.flat[i])
japanese_tweets = df[df['lang'] == 'ja']['text'] $ japanese_tweets
nbart_allsensors =nbart_allsensors.sortby('time')
so.shape
print confidence
!MASTER=spark://localhost:7077 pyspark
df_c_merge[['CA', 'UK', 'US']] = pd.get_dummies(df_c_merge['country']) $ df_c_merge.head()
import os.path as op $ my_gempro.save_json(op.join(my_gempro.model_dir, '{}.json'.format(my_gempro.id)), compression=False)
port_val=pos_vals.sum(axis=1) $ port_val.head()
excutable = utils.download_executable_lubuntu_hs(save_filepath)
new_page_converted = np.random.binomial(1, p_new, n_new) $ new_page_converted.mean()
%%bash $ grep -E '^ {2,6}"' md_traffic.json
mv_lens.title.value_counts().head()
exportID['avg_duration'] = exportID['event.longSum_sum_duration']/exportID['event.longSum_total_records']
df.head()
All_tweet_data_v2[All_tweet_data_v2.expanded_urls.isnull()]
spotify_url = 'https://open.spotify.com/user/spotify/playlist/37i9dQZF1DX0XUsuxWHRQd' $ client_id = '2f2a2d3c5b2f4621ae1758439dae20c4' $ client_secret = 'client_secret' $
df_concensus_uaa = df_concensus_uaa.set_index(['latest_consensus_created_date']) $ df_concensus_uaa
breed_predict_df_clean.info()
wrd_clean['rating_numerator'] = wrd_clean['text'].str.extract(r'(\d+\.?\d+?/\d+\.?\d+?)',expand=False) $ wrd_clean['rating_denominator'] = wrd_clean['text'].str.extract(r'(\d+\.?\d+?/\d+\.?\d+?)',expand=False)
hs.info()
naive_model = Naive() $ naive_model.train(df_train,df_train['Sales']) $ y_train_pred = naive_model.predict(df_train) $ print('Error on training set ', rmspe(y_train_pred, df_train['Sales'].values))
u = numpy.fft.rfft(Q1 - Qbar) $ y = numpy.fft.rfft(T1 - numpy.mean(T1))
df1.sample(5)
data_ar = np.array(data_ls) $ data = pd.DataFrame(data_ar, columns = ["fr_id","login_via_fb","wepay_status","has_benefactor", $                                         "use_video_as_main_img","title_length","desc_length","has_city", $                                         "images","videos","org_total_shares","updates","active"])
for tweet in results: $     data = {} $     data['tweet'] = tweet.text.encode('utf-8') $     data['datetime'] = tweet.created_at $     tweets.insert_one(data)
(df.isnull().sum() / df.shape[0]).sort_values(ascending=False)   # credit Ben shaver
print(len(data['data']['children']))
autos_pr['registration_year'].value_counts(normalize = True)
vect = CountVectorizer(stop_words="english", min_df=10) $ X = vect.fit_transform(text) $ X = pd.DataFrame(X.toarray(), columns = vect.get_feature_names()) $ model = Ridge(alpha = 1) $ model.fit(X, y)
rf
date = '2017-06-01' $ browser.find_element_by_id('cal-date-select-dev').clear() $ browser.find_element_by_id('cal-date-select-dev').send_keys(date) $ browser.find_element_by_xpath('//td//input[contains(@class, "site-button-small-dev submit")]').click()
prob_converted = df2.converted.mean() $ print (prob_converted)
futures_data
countries.country.unique()
z_score, p_value = sm.stats.proportions_ztest(count_converts, nobs_trials, $         alternative='larger', prop_var = cr_under_null) $ z_score, p_value $
%matplotlib inline
target_pf = test_portfolio[final_idx].copy()
mnnb.score(X_test_dtm, y_test)
gdf.plot();
weather_df.loc[weather_df["weather_main"].isin(["Dust", "Sand", "Smoke", "Squall"]), ["weather_main", "weather_description"]] = np.NaN $ weather_df = weather_df.fillna(method="ffill")
data_root_path = os.environ['DATA_PATH'] $ data_path = os.path.join(data_root_path , 'cs') $ model_path = os.path.join('..', 'models')
pd.Series(np.random.rand(5))
print(df.drop(['HL_PCT'], 1))
print("Today's Date is:", dt.date.today()) $ query_date = dt.date.today() - dt.timedelta(days=365) $ print("One year ago today is:", query_date) $
plt.hist(p_diffs) $ plt.axvline(x=0.000913, color='r')
df_twitter_extract_copy = df_twitter_extract_copy.drop('Unnamed: 0', axis = 1)
obj3
dfs = [breed_predict_df_clean, counts_df_clean, archive_df_clean] $ df_final = reduce(lambda left,right: pd.merge(left,right,on='tweet_id'), dfs ) $
melted_total = melted_total.dropna(subset=['Categories'])
data.loc[data.PRECIP.isnull()]
g_geo.head()
word_algebra(add=[u'wine', u'barley'], subtract=[u'grapes'])
submit = perf_test[['ID_CPTE']] $ submit['Default'] = predictions $ submit.to_csv('random_forest_baseline.csv', index = False)
reddit_comments_data.groupby('author_flair_text').count().orderBy('count', ascending = False).show(100, truncate = False)
url_template.format(start,)
df.index.weekday_name
pipe = make_pipeline(KNeighborsClassifier())
lr.score(preproc_training_test, training_test_labels)
solar_wind_df.to_excel(weather_folder + '/imputed_solar_wind.xlsx', index = False)
obj['clues']['b']
duplicated_list = twitter_archive_full[twitter_archive_full.tweet_id.duplicated()].tweet_id $ twitter_archive_full[twitter_archive_full.tweet_id.isin(duplicated_list)][['tweet_id','stage']].sort_values('tweet_id')
text.upper
check_presence = multiindex_output.isin(source_table_to_compare[['Email', 'Lineitem sku']]) $ check_presence[check_presence['step_no'] == True]
test_pred_svm = lin_svc_clf.predict(test_cont_doc)
df.sort_index(axis = 1, ascending = False)
convert=df2.query('converted=="1"').user_id.nunique()/df["user_id"].nunique() $ convert
outliers_age = airbnb_od.outlier_detection_serie_1d('age', cutoff_params=airbnb_od.strong_cutoff) $ outliers_age.head(10)
pprint(old_trump_tweets[0])
data_df.groupby('type')['ticket_id'].nunique()
aaron_query = "SELECT * FROM ephys_roi_results LIMIT 10" $ cell_df = get_lims_dataframe(aaron_query) $ cell_df.head() $
data = grouped_publications_by_author.copy()
pickle.dump(lsa_cv_data, open('iteration1_files/epoch3/lsa_cv_data.pkl', 'wb'))
top_conf = summary.groupby('week_id')sort_values(by=['week_id', 'confidence'], ascending=[True, False])
sandwich_corpus, sandwich_train_model, sandwich_dictionary = create_LDA_model(sandwich_train, $                                                                               'reviews_without_rare_words', 10, 40)
rng.to_period()
'AAbcEE'.replace('AA', 'BB')
df2['converted'].mean()*100
(actual_diff>p_diffs).mean()
indicator= 'din_winter' $ w.get_step_object(step = 3, subset = subset_uuid).get_indicator_data_filter_settings(indicator)
to_datetime = lambda x: pd.to_datetime(x)
new_page_converted = np.random.choice([0, 1], size=df_n.shape[0], p=[(1-p_new), p_new])
comment
sia = SIA() $ sentiment = stock['news_text'].apply(sia.polarity_scores) $ sent = pd.DataFrame(list(sentiment)) $ sent.index = stock.index
df.head() $ list1=list(df) $ df.head()
full_clean_df.sample(1)
df.dtypes
~((bs>3) & (bs<8))
t.index = [101, 101, 101, 102, 103] $ t
tip_sample.shape
membership['new_date'] = membership['new_date'].apply(lambda x:pd.to_datetime(x)) $ membership['new_date'] = membership['new_date'].apply(lambda x:x.strftime('%B-%Y'))
parameters = {'dates': [u'eventdate', $  u'created'], $  'categorical': [u'event', $  u'username', $  u'createdby']}
cluster_pd.head(22)
unsorted_df.sort_index(ascending=False)
var_list = ["Mkt_Cap2GDP", "Credit_2NonFinSec","Deposits_2GDP", "Household_Credit", "GDP", "Non_Financial_Credit_Ratio"] $ for var in var_list: $      new_name = var + "_growth" $      chinadata[new_name] = 100*chinadata[var].pct_change() $ chinadata.dropna(inplace = True)
model_uid='training-WWmHAB5ig' $ saved_model_details = client.repository.store_model(model_uid, {'name': 'Keras LSTM model'})
retweet_and_count = sns.factorplot(data=tweets_df, x="name", y="retweet_count", kind="box") $ plt.xticks(rotation=60)
df_exp.head(1)
keywords = [kw.lower().strip() for kw in keywords] $ keywords = [kw for kw in keywords if kw]
df2.drop([1899], axis = 0, inplace = True)
speak(getwiki(listenTo()))
Data.head()
volume_m = volumes.resample('M').sum()
index_strong_outliers = (strong_outliers_fare.is_outlier == 1)
for i in ['Updated Shipped diff']: $     t = df[i].hist(bins=500) $     t.set_xlim((300,8000)) $     t.set_ylim((0,500))
logit_mod = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ results = logit_mod.fit()
visit_num.head(n=15)
pd.set_option('max_colwidth', 100) #Its nice to see all columns
porn_df
df_example3 = rdd_example3.toDF() $ df_example3.registerTempTable("df_example3") $ print type(df_example3)
print(len(sent_tokenize(test_post))) $ sent_tokenize(test_post) # doesn't really split all sentences
dfData.get_dtype_counts()
im = Imputer(strategy='median') $ im.fit(X_train)
dog_ratings
df_unique_providers = df_unique_providers.reset_index(drop=True) $ df_unique_providers.head()
df_ml_51_01.tail(5)
data.shape
n_new = df2[df2['landing_page']=='new_page'].landing_page.count() $ n_new
pd.to_datetime(segments.st_time[:])
model_rf_14 = RandomForestClassifier(max_depth = 14, random_state=42) $ model_rf_14.fit(x_train,y_train) $ print("Train: ", model_rf_14.score(x_train,y_train)*100) $ print("Test: ", model_rf_14.score(x_test,y_test)*100) $ print("Differnce between train and test: ", model_rf_14.score(x_train,y_train)*100-model_rf_14.score(x_test,y_test)*100)
intervention_history.reset_index(inplace=True) $ intervention_history.set_index(['INSTANCE_ID', 'CRE_DATE_GZL', 'INCIDENT_NUMBER'], inplace=True)
control_df = df2[df2['group']=='control'] $ n_control = control_df.shape[0] $ 1.0*sum(control_df['converted']==1)/ n_control
from scipy.stats import norm $ print(norm.cdf(z_score)) # 0.905058312759 $ print(norm.ppf(1-(0.05))) # 1.64485362695 $
brand_counts = autos["brand"].value_counts(normalize=True) $ common_brands = brand_counts[brand_counts > .05].index $ print(common_brands)
new_df =  pd.value_counts(IKEA_data.user_location).reset_index() $ new_df.columns = ['user_location', 'frequency'] $ top50_city = new_df.head(50) $ new_df.head(10)
Distribution_df['Month'] = Distribution_df['Date_of_Order'].dt.month $ Distribution_df.head()
df_archive_clean = pd.merge(df_archive_clean,df_count_clean,on = "tweet_id")
grid_search.best_estimator_
df_joined['country'].unique()
idx = pd.IndexSlice $ transit_df_rsmpld = transit_df_rsmpld.sort_index().loc[idx['2016-01-01':'2017-12-31',:],:].sort_index() $ transit_df_rsmpld.info() $ transit_df_rsmpld.head()
water_body = 'SE654470-222700' $ temp_df = w.get_filtered_data(step = 2, subset = subset_uuid, indicator = 'din_winter', water_body = water_body)[['SDATE','MONTH', 'WATER_BODY_NAME', 'VISS_EU_CD', 'DIN','SALT_CTD', 'SALT_BTL']].dropna(thresh=7) $ print('Waterbodys left: {}'.format(temp_df.loc[temp_df['VISS_EU_CD'].isin([water_body.strip('SE')])]['WATER_BODY_NAME'].unique())) $ temp_df.loc[temp_df['VISS_EU_CD'].isin([water_body])] $
pd.Period('2011-01')
print('Coefficients: \n', regr.coef_) $ print('Intercept: \n', regr.intercept_)
data_scrapped['para'] = [[]] * len(data_scrapped) $ data_scrapped['tags'] = [[]] * len(data_scrapped) $ data_scrapped['bullets'] = [[]] * len(data_scrapped) $ data_scrapped['images'] = [[]] * len(data_scrapped) $ data_scrapped['links'] = [[]] * len(data_scrapped)
df2.tail()
df_clean = df_clean[df_clean['retweeted_status_id'].isnull()] $ df_clean = df_clean[df_clean['in_reply_to_status_id'].isnull()]
prob_newpage = df2[df2.landing_page=='new_page'].shape[0]/df2.shape[0] $ print('Probability that an individual recieved new page: ' + str(prob_newpage))
inputId = movies_df[movies_df['title'].isin(inputMovies['title'].tolist())] $ inputMovies = pd.merge(inputId, inputMovies) $ inputMovies = inputMovies.drop('genres', 1).drop('year', 1) $ inputMovies
eval_data = pd.concat([predictions, actuals]) $ eval_data['day'] = eval_data.index
trn_y.mean(), val_y.mean()
df_dummies = pd.get_dummies(df_categorical)
contain_geo_search = collection_reference.find({'geo' : { '$ne' : None}}) $ contain_geo_search
users.columns.tolist()
clean_users['creation_source'].value_counts()
merged.groupby(["contributor_firstname", "contributor_lastname", "committee_position"]).amount.sum().reset_index().sort_values("amount", ascending=False)
sess.get_data('eur curncy','fwd curve')
Base = automap_base() $ Base.prepare(engine, reflect=True) $ Base.classes.keys()
v = variables_df[variables_df['VariableCode'] == 'TP'] $ variableID = v.index[0] $ results = read.getResults(siteid=siteID, variableid=variableID, type="Measurement") $ resultIDList = [x.ResultID for x in results] $ len(resultIDList)
greaterpropdiff = p_diffs > act_diff $ greaterpropdiff.mean()
print('Answer 2.A: ', '\n') $ print(csv_df['names'].value_counts(), '\n') $ for i in set(csv_df['names']): $     print(i, ": ", len(set(csv_df['date'][csv_df['names']==i])))
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ WHERE twitter_day = 9 \ $ GROUP BY tag_text ORDER BY count DESC LIMIT 1;
titanic.pclass.value_counts(sort=False).plot(kind='bar')
print("This dataset contain {} rows, and {} columns.".format(df.shape[0],df.shape[1]))
df_predictions.Q0.value_counts()
subs_and_comments.head()
ticks.shift(1).head()
essential_genes = pd.read_excel("http://mbio.asm.org/content/9/1/e02096-17/DC7/embed/inline-supplementary-material-7.xlsx", header=1)
idx = data['dataset_data']['column_names'].index('Open') $ open_price = [day[idx] for day in data['dataset_data']['data'] if day[idx]] $ print('Highest and lowest opening prices in 2017 were {} and {}'.format(max(open_price), min(open_price)))
def calc_temps(start_date, end_date): $     return session.query(func.min(Measurements.tobs), func.avg(Measurements.tobs), func.max(Measurements.tobs)).\ $         filter(Measurements.date >= start_date).filter(Measurements.date <= end_date).all() $ print(calc_temps('2010-01-01', '2010-01-18'))
gap = test.date.min() - train.date.max() $ gap
df3 = df3.join(pd.get_dummies(df3['country'])) $ df3.head()
?df.plot
nycshp.ZIPCODE = nycshp.ZIPCODE.astype(int)
bigram_transformer = Phrases(sentences=flatSentenceList) $ model_bigram = models.Word2Vec(bigram_transformer[flatSentenceList], iter=5)
tweets[tweets['retweeted'] == 0]['chars'].describe()
df2.query( $     'group=="treatment" & converted==1').user_id.count() / df2.query( $     'group=="treatment"').user_id.count()
s4g.groupby('Symbol')
holidays_df.loc[holidays_df['date']==chk.head()['date'].iloc[0]].head()
data['MinW'] = data[odds_W].min(axis=1) $ data['MinL'] = data[odds_L].min(axis=1) $ display(data.MinW.head()) $ display(data.MinL.head())
b.find_by_xpath('//*[@id="day-section"]/div/div[3]/div[8]/div[2]/ul/li[2]/button').click()
s519397_df["prcp"].sum()
aapl.loc['2001':'2004',['open','close','high', 'low']].plot() $
s.index
df_amznnews_clsfd_2tick = df_amznnews_clsfd_2tick[['textblob_sent', 'vs_compound']].resample('1D').mean() $ print(df_amznnews_clsfd_2tick) $
df_tweets2['sentiment'] = np.random.uniform(-1,1, size=len(df_tweets2)) $ df_tweets2['sentiment_ori'] = np.random.uniform(-1,1, size=len(df_tweets2))
print('There are {:.0f} visa applications for jobs located in NYC in this dataset.'.format(df_h1b_nyc.shape[0]))
tweet_image_predictions_clean['tweet_id'] = tweet_image_predictions_clean['tweet_id'].astype('str')
ndays = 360 $ ntraj=10 $ dates=pd.date_range('20170101',periods=ndays) $ simret = pd.DataFrame(sigma*np.random.randn(ndays,ntraj)+mu,index=dates) $ simret
print(csgo_profiles.info())
new_columns = ['year.1','salary','year.2','empID','name'] $ print( len(new_columns) == len(df.columns) ) $ df.columns = new_columns $ df.head(2)
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=10000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
iso_json = json.loads(iso_response.text) $ iso_gdf = gpd.GeoDataFrame.from_features(iso_json['features']) $ iso_gdf[:]
df2 = df2.rename(columns={'date': 'ESPN_Play_Count'})
train_data.head()
basic_plot_generator("mention_count", "Saving an Image Graph" ,DummyDataframe.index, DummyDataframe,saveImage=True, fileName="dummyGraph")
tweets.dtypes
print(plan['requestParameters'])
airbnb_df['weekly_price'].fillna(airbnb_df['price']*7, inplace=True) $ airbnb_df['monthly_price'].fillna(airbnb_df['price']*30, inplace=True)
soup.ul.children
len(collection_reference.distinct('user'))
test['visitors'] = preds3 $ test['visitors'] = np.expm1(test['visitors']).clip(lower=0.) $ sub1 = test[['id','visitors']].copy() $ sub1.to_csv('sub_single_xgb_nocv.csv',index=False)
parties['Unique Key'].groupby(by= parties.index.dayofweek).count().plot()
n_old = 145274
df_2010['bank_name'] = df_2010.bank_name.str.split(",").str[0] $
sub_df = pd.DataFrame({"click_id":test["click_id"], "is_attributed":is_attributed})
locationsDF.crs = londonDFSubset.crs $ mergedLocationDF = gpd.sjoin(locationsDF, londonDFSubset, how="left", op='within') $ mergedLocationDF.head()
ids[pd.notnull(ids)].iloc[0]
prediction.describe()
temps_df.loc['2016-04-05']
cols = [ $     'processed_project_title', $     'processed_project_resource_summary', $     'processed_project_description', $     'processed_student_description']
df = df.merge(bwd, 'left', ['Date', 'Store'], suffixes=['', '_bw']) $ df = df.merge(fwd, 'left', ['Date', 'Store'], suffixes=['', '_fw'])
SMOOTH.plot_init_latency(smoothresult,option="")
nt.columns
linear_model_p3 = sm.OLS(df_c_merge['converted'], df_c_merge[['intercept', 'ab_page', 'US', 'UK']]) $ lin_results = linear_model_p3.fit() $ lin_results.summary()
df = df[df['ViewCount']!=0]
def Info_Dataframe(dataframe): $     return dataframe.info()
pd.isnull(df)
df2.user_id.duplicated().sum()
data = {'name': ['Alice', 'Bob', 'Charles', 'David', 'Eric'], $         'year': [2017, 2017, 2017, 2018, 2018], $         'salary': [40000, 24000, 31000, 20000, 30000]} $ df = pd.DataFrame(data, index = ['Acme', 'Acme', 'Bilbao', 'Bilbao', 'Bilbao']) $ df
df  # Transposing is not done in place
counts_df_clean['tweet_id'] = counts_df_clean.tweet_id.apply(str) $
filtered = pd.read_pickle('../data/filtered.pkl')
odometer.describe()
oil[oil.month_year==201411].sort_values(by='date')
df.corr()
plt.figure(figsize=(16,8)) $ dendrogram(features['bbc-news']['linkage'], orientation='top', $           p=300, truncate_mode='lastp', no_labels=True, color_threshold=0) $ plt.axes().get_yaxis().set_visible(False) $ plt.show()
meals = pd.read_csv('./raw_data/meals.csv') $ meals['Meal Categories'].fillna('', inplace=True) $ meals['Cuisine Type'].fillna('', inplace=True)
from gensim import corpora, models, similarities $ dictionary = corpora.Dictionary.load('/var/folders/2m/d1x8p4k179j6433svzq39zqh0000gn/T/deerwester.dict') $ corpus = corpora.MmCorpus('/var/folders/2m/d1x8p4k179j6433svzq39zqh0000gn/T/deerwester.mm') # comes from the first tutorial, "From strings to vectors" $ print(corpus)
list(set(weather["NAME"]))
autos=pandas.read_csv("autos.csv", encoding="Latin-1")
zip_1_df.TimeCreate = zip_1_df.TimeCreate.apply(lambda x:x.date()) $ zip_2_df.TimeCreate = zip_2_df.TimeCreate.apply(lambda x:x.date())
feature_imp=pd.DataFrame(list(zip(features,xgb_model.feature_importances_))) $ column_names= ['features','XGB_imp'] $ feature_imp.columns= column_names
appmag_lim = 21.0 $
autos.head()
dsDir = Path('data') $ bus = pd.read_csv(dsDir/'businesses.csv',encoding='ISO-8859-1') $ ins = pd.read_csv(dsDir/'inspections.csv') $ vio = pd.read_csv(dsDir/'violations.csv')
df_ab_raw['line_up'].sum()
pd.DataFrame(random_integers, index=[3, 2, 1, 0])
prs.to_csv("prs2.csv") $ pr_comments.to_csv("pr_comments2.csv") $ issues.to_csv("issues2.csv") $ issue_comments.to_csv("issue_comments2.csv")
df.shape
pd.isnull(doglist)
dr_new_8_to_16wk_arimax
autos = autos[autos['registration_year'] <= 2016] $ autos['registration_year'].value_counts(normalize=True).sort_index(ascending=True).hist(bins=20) $
ins2016 = ins[ins['year']==2016] $
dates = pd.date_range('2010-01-01', '2010-12-31') $ df1=pd.DataFrame(index=dates) $ print(df1.head())
query_date_2yearsago = dt.date.today() - dt.timedelta(days = 730) $ print(query_date_2yearsago)
len(slps.CompanyNumber.unique())
n_new = df_treatment.user_id.nunique() $ n_new
rng.tz_localize('Europe/Berlin')
df.duplicated().sum()
grammar = '' $ cp = nltk.RegexpParser(grammar) $ result = cp.evaluate(train_trees) $ print(result)
saved_model = ml_repository_client.models.save(model_artifact)
p.head(2)
df.index[2]
image_predictions[image_predictions.jpg_url.duplicated()].jpg_url #duplicates $ image_predictions[image_predictions.jpg_url=='https://pbs.twimg.com/media/CdHwZd0VIAA4792.jpg']
learner.save('lm_last_ft')
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response = requests.get(url) $ with open('image-predictions.tsv','wb') as file: $         file.write(response.content)
n_new = df2[df2['group']=='treatment'].user_id.count() $ n_new
plt.plot(X,data, "r") $ plt.legend(loc='upper left') $ plt.show()
df2['intercept'] = 1 $ df2[['control','ab_page']] = pd.get_dummies(df2['group'])
plt.hist(b56[~np.isnan(b56)],50); #50 signifies the # of bins
my_query = "SELECT * FROM specimens LIMIT 10" $ my_result = limsquery(my_query) $ first_element = my_result[0] $ print first_element
password = 'Mart_3.1415'
X[0].todense()
train_holiday = train_test_set.join(holidays_events, 'date', 'left_outer') $ train_holiday.show(2)
print(datetime.datetime.strftime(d2, "%Y-%m-%d")) $ type(datetime.datetime.strftime(d2, "%d-%b-%Y"))
df_int = (df / df.shift(1)) - 1 $ df_int.iloc[0] = 0 $ df_int.head()
print(dictionary.token2id['like'])
def compare_mean(myList, m): $     return myList>m $ scores
trd = [ i[idx_trade] for i in r_lol] $
from sklearn.cross_validation import KFold $ kf = KFold(25, n_folds=5, shuffle=False) $ print('{} {:^61} {}'.format('Iteration', 'Training set observations', 'Testing set observations')) $ for iteration, data in enumerate(kf, start=1): $     print('{:^9} {} {:^25}'.format(iteration, data[0], data[1]))
df.head(5)
df_new = df2.query('landing_page == "new_page"') $ p_new = df2['converted'].mean() $ print(p_new)
from scipy import stats $ stats.chisqprob = lambda chisq, df2: stats.chi2.sf(chisq, df2) $ results.summary()
err.subject.unique()
idx_high = data['dataset_data']['column_names'].index('High') $ idx_low = data['dataset_data']['column_names'].index('Low') $ change = [day[idx_high]-day[idx_low] for day in data['dataset_data']['data']] $ print('The largest change in any one day in 2017 was {:.2f}'.format(max(change)))
my_query = "SELECT * FROM coindesk" $ print(my_query)
url = ('http://dccouncil.us/calendar') $ browser = webdriver.Firefox() $ browser.get(url) $ wait = WebDriverWait(browser, 30)
allqueryDF.info()
mean_vecs = np.mean(hidden_states, axis=1) $ max_vecs = np.max(hidden_states, axis=1) $ sum_vecs = np.sum(hidden_states, axis=1) $ train_target_emb = mean_vecs
del result['new_date']
(taxiData2.Fare_amount < 0).any() # This Returns True, meaning there are values that are negative
gcv.best_estimator_.fit(fb_train.message, fb_train.popular)
print(DataSet_sorted['tweetText'].iloc[3])
discConvpct = discConvpct.filter(like='Closed Won').rename('discConversionPercent').to_frame().reset_index()
pd.read_sql(q, connection)
xgb.plot_importance(model_outer)
links = [] $ for link in JPL_soup.find_all('a'): $     links.append(link.get('href')) $ print(links)
import config
tweet_json.head()
pd.merge(transactions,transactions,how='outer',on='UserID')
import pandas as pd $ df = pd.read_csv("cpr-data.csv") $ df.head() $ import matplotlib $ %matplotlib inline
test_df = proc_test_df(test_data, nas)
def pragraph_to_setnences(str): $     return sent_tokenize(str)
import numpy as np $ data['float_time'] = data['processing_time'].apply(lambda x: $                                                   x/np.timedelta64(1,'D')) $ data
for i, x in enumerate(unprocessed_list): $     unprocessed_list[i] = x.split('\n')
ma = ffinal.groupby(['GAME_ID', 'season', 'TEAM_ID'])['pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l'].sum().reset_index() $ mb = ffinal.groupby(['GAME_ID', 'season', 'TEAM_ID'])['pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l'].sum().reset_index() $ ma.columns = ['game_id', 'team_id_ta', 'season', 'pts_la', 'ast_la', 'blk_la', 'reb_la', 'stl_la'] $ mb.columns = ['game_id', 'team_id_tb', 'season', 'pts_lb', 'ast_lb', 'blk_lb', 'reb_lb', 'stl_lb'] $ ma.head()
df_326_table = pd.read_sql(sql_326_table,conn_laurel) $ df_326_table.sort_values(by='Values',ascending=False)
import nltk; nltk.download('stopwords')
googletrend.head()
df_pd.sort_values(by='timestamp') $ df_pd.set_index("timestamp") $ train_frame = df_pd[0 : int(0.7*len(df_pd))] $ test_frame = df_pd[int(0.7*len(df_pd)) : ]
my_gempro.kegg_mapping_and_metadata(kegg_organism_code='mtu') $ print('Missing KEGG mapping: ', my_gempro.missing_kegg_mapping) $ my_gempro.df_kegg_metadata.head()
s.str.len()
resultItemUsersLen=topUserItemDocs.groupby(['item_id']).size() $ print min(resultItemUsersLen) $ print max(resultItemUsersLen) $ print np.median(resultItemUsersLen) $ print np.mean(resultItemUsersLen)
states.set_index(['day', 'location'], append=True)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print("Z_score:",z_score) $ print("P_value:",p_value)
actual_value_second_measure=pd.DataFrame(actual_value_second_measure) $ actual_value_second_measure.replace(2,1, inplace=True)
%matplotlib inline $ import pandas as pd $ brazil = pd.read_csv('https://quarry.wmflabs.org/run/215651/output/0/csv?download=true',parse_dates=[0],index_col=0) $ biology = pd.read_csv('https://quarry.wmflabs.org/run/215652/output/0/csv?download=true',parse_dates=[0],index_col=0) $
import datetime as dt
n_old = df2.query('group == "control"')['user_id'].nunique() $ n_old
x = [[1, 2], $     [3, 4]] $ np.concatenate([x, x], axis=1)
pd.options.display.max_columns = 35 $ %matplotlib inline
shopping_carts = pd.DataFrame(items) $ shopping_carts
df_arch_clean['rating_value'] = df_arch_clean['rating_numerator'] / df_arch_clean['rating_denominator']
twitter_archive_master.head()
es_rdd.take(1)
print "Before Jimmy, the amount of touchdowns per game was %.2f" % (18/pre_number) $ print "After Jimmy, the amount of touchdowns per game was %.2f" % (13/post_number)
df.isnull().sum()
print(lgb_cv.best_params_) $ print(lgb_cv.best_score_)
bus.describe()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new],[n_old, n_new], alternative='smaller') $ z_score, p_value $
tripduration_minutes = pd.Series(minutes_list)
cursor.execute(sq74) $ cursor.execute(sq75) $ results3 = cursor.fetchall() $ results3
print("Min " + str(tm['created_at'].min()) + " Max " + str(tm['created_at'].max()))
df.to_csv("../../data/msft_piped.txt",sep='|') $ !head -n 5 ../../data/msft_piped.txt
all_cards = all_cards[~all_cards.index.duplicated(keep = "first")]
print(model_arima121.aic,model_arima121.bic,model_arima121.hqic)
ebola_dirs = !ls ../data/ebola/ $ ebola_dirs
import json $ with open('temp_dates_dict.json', 'w') as fp: $     json.dump(temp_dates_dict, fp)
autos.rename(columns={"odometer":"odometer_km"}, inplace = True) $ autos.drop(["seller", "offer_type", "nr_of_pictures"], axis = 1, inplace = True) $ autos["price"] = autos["price"].str.replace("$","").str.replace(",","").astype(int) $ autos["odometer_km"] = autos["odometer_km"].str.replace("km","").str.replace(",","").astype(int) $ autos.head()
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ df['intercept'] = 1 $ df[['control','treatment']] = pd.get_dummies(df['group'])
van15_fin["stiki_mean"] = van_scores.groupby('userid')['stiki_score'].mean()
data_scrapped.drop_duplicates().shape
parameters = dict(n_estimators=[1000, 1500, 2000], criterion=['gini', 'entropy']) $ RFC_grid = GridSearchCV(RFC_model_grid, param_grid=parameters, cv=5, verbose=3)#, scoring='f1') $ RFC_grid.fit(X_train, y_train)
ave_ratings_over_time = tweet_archive_master[['timestamp', 'rating_numerator']].groupby(pd.Grouper(key='timestamp', freq='M')).mean() $ ave_ratings_over_time.index = tweet_archive_by_month.index.map(strip_date)
p_new = df2['converted'].mean() $ print(p_new)
df['bg_ratio'] = df['backers'] / df['goal'] $ df['bg_avg'] = df[['backers', 'goal']].mean(axis=1) $ df['bg_ratio_date'] = (df['backers'] / df['goal']) * df['date_diff']
pd.DataFrame([m.params for m in models], index=model_dates).T
from sklearn.svm import SVC $
from sklearn.decomposition import LatentDirichletAllocation $ lda = LatentDirichletAllocation(n_topics=10, $                                 random_state=123, $                                 learning_method='batch') $ X_topics = lda.fit_transform(X)
n_topics = 10 $ nmf = NMF(n_components=n_topics, random_state=0, init="nndsvd") $ nmf = nmf.fit(tfidf)
print("Leavetimes shape:") $ dfleavetimes.shape
t0 = time() $ auc = calculateAUC(cvData, bAllItemIDs, model.predictAll) $ t1 = time() $ print("auc=",auc) $ print("finish in %f seconds" % (t1 - t0)) $
date_crawled_count_norm.describe()
ltc = pd.read_json('ltc.json', lines=True) $ xrp = pd.read_json('xrp.json', lines=True) $ eth = pd.read_json('eth.json', lines=True)
from scipy.special import beta $ bayes_prob = beta(6 + 1, 5 + 1) / beta(3 + 1, 5 + 1) $ print("P(B|D) = %.2f" %bayes_prob)
a = dat['police district'].unique() $ a.sort() $ a
CO_profit.head()
new_page_converted = np.random.binomial(nnew, pnew)
plt.hist(p_diffs); $ plt.xlabel('(p_new - p_old)'); $ plt.ylabel('Frequency'); $ plt.title('Sampling Distribution of (p_new - p_old)');
df[['rating_numerator', 'favorite_count', 'retweet_count']].describe()
weights.ix['2017-09-12':'2017-09-21']
bp[bp["icustay_id"]==14882].plot(x="new charttime", $                                  y=["systolic", "diastolic"]) $ bp[bp["icustay_id"]!=14882].plot(x="new charttime", $                                  y=["systolic", "diastolic"]) $ bp.head()
df = pd.read_sql('SELECT COUNT(*) FROM address;', con=conn) $ df
jobs.loc[(jobs.FAIRSHARE == 2) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
f_c_doy = f_c_date.groupby('doy').sum() $ f_c_doy.reset_index(inplace=True) $ f_c_dow = f_c_date.groupby('dow').sum() $ f_c_dow.reset_index(inplace=True) $ f_c_dow.shape
hs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f') $ NAmer = hs.content['NAmer_dem_list.shp']
if not (os.path.isfile('txt2pdf.py')): $     destination = os.getcwd() $     os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $     shutil.copy('txt2pdf.py',destination) $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END)
y_pred_complex = model_eval(complex_model, X_valid, y_valid)[0]
data_df.desc[15]
print train.shape, test.shape
maint['datetime'] = pd.to_datetime(maint['datetime'], format="%Y-%m-%d %H:%M:%S") $ maint['comp'] = maint['comp'].astype('category') $ print("Total number of maintenance records: %d" % len(maint.index)) $ maint.head()
pred_probas_under_fm = gs_from_model_under.predict_proba(X_test) $ fm_bet_under = [x[1] > .6 for x in pred_probas_under_fm]
precipitation_df.set_index("date", inplace=True) $ precipitation_df.head(15)
import statsmodels.api as sm $ convert_old = ((df2['converted'] == 1) & (df2['landing_page'] == 'old_page')).sum() $ convert_new = ((df2['converted'] == 1) & (df2['landing_page'] == 'new_page')).sum() $ n_old = (df2['landing_page'] == 'old_page').sum() $ n_new = (df2['landing_page'] == 'new_page').sum()
print('reduce memory') $ utils.reduce_memory(user_logs) $ utils.reduce_memory(train)
df2.loc[df2["user_id"] == 773192,]
contractor_clean['address1'] = contractor_clean['address1'] .map(lambda x: x.lstrip('Attn:')) $ contractor_clean['address1'] =contractor_clean['address1'].str.replace('-', ' ') $ contractor_clean['address2'] =contractor_clean['address2'].str.replace('-', ' ')
_ = ok.grade('q01') $ _ = ok.backup()
my_cryptory = Cryptory(from_date="2016-01-01")
stats.norm.cdf((active_mailing - inactive_mailing) / SD_mailing)
model1 = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results1 = model1.fit()
if not os.path.exists('new_data_files'): $     os.mkdir('new_data_files') $ records3.to_csv('new_data_files/Q3B.csv')
tweet_json = pd.read_json('tweet_json.txt',lines= True) $ tweet_archive_enhanced = pd.read_csv('twitter-archive-enhanced.csv') $ tweet_image_predictions =  pd.read_csv('image_predictions.tsv',sep = "\t")
data()
pd_data
from sklearn.model_selection import train_test_split  # model selection rather than cross validation?
obs_diff = (new_page/df2.query('landing_page == "new_page"').shape[0]) - (old_page/df2.query('landing_page == "old_page"').shape[0]) $ obs_diff.mean()
df_tweets2
import logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)
avg_price_by_brand = dict.fromkeys(brands) $ for name in brands: $     brand_prices = autos.loc[autos["brand"] == name, "price"] $     brand_avg_price = round(brand_prices.mean(), 2) $     avg_price_by_brand[name] = brand_avg_price
plt.hist(p_diffs) $ plt.axvline(np.array(p_diffs).mean(),c='blue') $ plt.axvline(-0.0015, c='red')
testObjDocs = GMapsLoc_DFBuilder() $ print(testObjDocs)
pickle_it(SGD, 'sgd_classifier') $ SGD = open_jar('sgd_classifier')
n_new=df[df['group']=='treatment'].shape[0] $ n_new
ds.head()
print("Probability of treatment group converting is", $       df2[df2['group']=='treatment']['converted'].mean())
ss = StandardScaler()
n_users = df.user_id.nunique() $ n_users
del merged_portfolio_sp_latest_YTD['Date'] $ merged_portfolio_sp_latest_YTD.rename(columns={'Adj Close': 'Ticker Start Year Close'}, inplace=True) $ merged_portfolio_sp_latest_YTD.head()
pumashplc.head(1)
null_vals = np.random.normal(0, diffs.std(), diffs.size)
to_be_predicted_Day3 = 48.70640398 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
test_df.head()
nypd_complaints_total = data[data['Agency']=='NYPD']['Borough'].count() $ nypd_complaints_total
df_countries.user_id.nunique()
df_local_website = df[(~df.domain.isnull()) & $                       (df.domain != 'facebook.com') & $                       (df.domain != 'comettv.com')].drop_duplicates(subset=['domain'])
df_weather.loc[:, "precipitation_inches"] = df_weather.precipitation_inches.apply(lambda x: "0.005" if x == "T" else x) $ df_weather.loc[:, "precipitation_inches"] = df_weather.loc[:, "precipitation_inches"].astype(np.float) $ np.sort(df_weather.precipitation_inches.unique())
df.info()
predict_y = randomforest.predict(testx)
avg1_table = data_df[['ID','Segment','Country','Product','Units Sold']].copy() $ avg1_table.head()
title = soup.find_all('div', class_="content_title")[1].text.strip() $ description=soup.find_all('div', class_="rollover_description_inner")[1].text.strip()
url = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars" $ base_url = "https://astrogeology.usgs.gov" $ hemisphere_image_urls = []
df_pol.head(3)
subway2_df['Hourly_Exits'] = get_hourly(subway2_df['EXITS'])
conn.caslibinfo()
element = driver.find_element_by_xpath('//*[@id="middleContainer"]/ul[1]/li[3]/a') $ element.click()
import numpy as np
del train['project_subject_categories'], p, yar
from bs4 import BeautifulSoup $ import requests $ import pandas as pd $ import time $ from splinter import Browser
gmm.fit(X) $ labels = gmm.predict(X) $ labels
session['action'].value_counts().sort_values(ascending=False).head(20)
got_data.head()
df = condensed_xs.get_pandas_dataframe(xs_type='micro') $ df
engine.execute("SELECT count(*) FROM contractor").fetchall()
df = sean['Event Type Name'].value_counts() $ df_sean = pd.Series.to_frame(df) $ df_sean.columns = ['Count_Sean'] $ df_sean
p_new = df2['converted'].mean() $ (p_new)
Z = np.random.uniform(0,1,(10,10)) $ U, S, V = np.linalg.svd(Z) # Singular Value Decomposition $ rank = np.sum(S > 1e-10) $ print(rank)
one_station=df_daily5.loc[df_daily5['STATION']=='1 AV'] $ one_station.head()
joined.dtypes.filter(items=['Frequency_score'])
deep_learning_tweet_id = 839463666059599872
os.getcwd()
print("The number of unique user_ids in the new dataset is:  " + str(len(df2['user_id'].unique()))) $ print('Total number of user_ids:  ' + str(df2.shape[0]))
taxi_weather_df.head()
stocks_happiness=stocks_happiness.dropna(axis=0) $ stocks_happiness $
rdf = rdf.set_index('segment_id').join(accident_counts_per_segment)
weather = weather[weather.zip_code == 94107]
len(train_data[train_data.gearbox == 'automatik'])
guineaFullDf = pd.concat(frameList,axis=0) $ guineaFullDf.head()
sorted(Counter(reducedwordlist).items(),key=lambda wordtuple: wordtuple[1],reverse=True)[:30]
%bash $ gsutil ls -l gsL//$BUCKET/taxifare/ch4/taxi_preproc/
coarse_groups = openmc.mgxs.EnergyGroups(group_edges=[0., 20.0e6]) $ coarse_mgxs_lib = mgxs_lib.get_condensed_library(coarse_groups)
dataDir = "data/s&p_daily/quaterly_split/training" $
geocoded_addresses_df = addresses_df.reset_index()[['Case.Number', 'Defendant.Addr.Line.1']].reset_index().join(stacked_geo_df.set_index('case_idx')) $ geocoded_addresses_df.to_csv('geocoded_addresses.csv', encoding='utf-8') $ geocoded_addresses_df.head()
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
df.info() $
for res_key, df in data_sets.items(): $     df.to_pickle('final_'+res_key+'.pickle')
filtered_df[filtered_df['M_pay_3d'] == 0].shape[0]
newcolumns = df1.columns.str.strip() $ df1.columns = newcolumns
df_train.columns[np.array(list(map(lambda x: sum(df_train[x] == value), df_train.columns))) != 0]
cur.execute('SELECT material_id, long_name FROM materials WHERE alpha < 1 LIMIT 2') $ for c in cur: print('{} is {}'.format(*c))  # user the cursor as an iterator
train.columns, test.columns,store.columns, 
crime = pd.read_csv('clean_data/KCPD_Crime_Data_2017_clean.csv') $ moon = pd.read_csv('clean_data/Moon_Data_2017_cleaned.csv')
df_archive_clean = df_archive_clean.drop(["doggo","floofer","pupper","puppo"], axis = "columns")
PATH = 'data/aclImdb/' $ TRN_PATH = 'train/all/' $ VAL_PATH = 'test/all/' $ TRN = f'{PATH}{TRN_PATH}' $ VAL = f'{PATH}{VAL_PATH}'
top20_brands_mean_price = pd.Series(top20_brands_mean_price) $ top20_brands_mean_mileage = pd.Series(top20_brands_mean_mileage)
pd.read_sql(sql_sub, engine).head()
archive_df_clean["stage"].sample(10)
import datetime as dt $ df_cat_stat['launched']  = pd.to_datetime(data_imported_nonan['launched']).dt.date $ df_cat_stat['deadline']  = pd.to_datetime(data_imported_nonan['deadline'], infer_datetime_format=True).dt.date $ df_cat_stat['duration'] = (data_imported_nonan['deadline'] - data_imported_nonan['launched']).dt.days $ df_cat_stat.head(3)
prog_lang[prog_lang.Name == 'Python']
talks_train = pd.read_json('train2.json')
titanic_df[['cabin', 'cabin_floor']].head()
df[df['converted'] == 1].shape[0]/df.shape[0]
cols_to_export = ["epoch","src","trg","src_str","src_screen_str","trg_str","trg_screen_str"] $ mentions_df.to_csv("/mnt/idms/fberes/network/usopen/data/uso17_mentions_with_names.csv",columns=cols_to_export,sep="|",index=False)
import scipy.optimize as spo
pings.count()
X_new = new[feature_cols]
def get_list_hashtags_caption(the_posts): $     list_of_hastags = [] $     for i in list_Media_ID: $         list_of_hastags.append(the_posts[i]['caption_tags']) $     return list_of_hastags
import pandas $ data = pandas.read_excel("public_health.xlsx", sheetname="Sheet1", parse_dates=["Visit Date"],dayfirst=True) $ data.head(2)
df_times['Time of daily highs'].value_counts().max()
(autos['ad_created'] $  .str[:10] $  .value_counts(normalize=True,dropna=False) $  .sort_index() $ )
useful_indeed.isnull().sum() $
sum(tw.expanded_urls.isnull())
actual_difference = probability_treatment-probability_control $ greater_p_diffs = [i for i in p_diffs if i > actual_difference] $ propor_greater_p_diffs = (len(greater_p_diffs) / len(p_diffs))*100 $ print('The proportion of the p_diffs greater than the actual difference observed in ab_data.csv is: {}%'.format(propor_greater_p_diffs))
df['site_admin'].unique()
%%time $ ds_complete_temp_CTD_1988 = ds_complete_temp_CTD_1988.where((ds_complete_temp_CTD_1988['time.year'] > 1979),drop=True)
import statsmodels.api as sm $ convert_old = df2[(df2['landing_page']=='old_page')&(df2['converted']==1)].user_id.count() $ convert_new = df2[(df2['landing_page']=='new_page')&(df2['converted']==1)].user_id.count() $ n_old = length_of_old $ n_new = length_of_new $
ax = cell_df[cell_df['Class'] == 4][0:50].plot(kind='scatter', x='Clump', y='UnifSize', color='DarkBlue', label='malignant'); $ cell_df[cell_df['Class'] == 2][0:50].plot(kind='scatter', x='Clump', y='UnifSize', color='Yellow', label='benign', ax=ax); $ plt.show()
df.status.value_counts()
y_hat = linreg.predict(quadratic) $ plt.plot(y_hat,'-b') $ plt.show()
blah = test_collection.find({"URL": feral_url}) $ test_collection.find({"URL": data[1]['display_url']}).count() $ if test_collection.find({"URL": feral_url}).count() == 0: $     print('URL not in database, add it')
df2[df2['group']=='treatment']['converted'].mean()
responsesJsonList[1:5] # Citigroup in no.3
type(AAPL.columns)  # returns a pandas index  ( b/c each col is a series, therfore it has its own index?)
sns.boxplot(calls_df["length_in_sec"],orient='v')
dc['created_at'].hist(color="blue") # blue, David Cameron $ tm['created_at'].hist(color="orange") # orange, Theresa May
n_old = df2[df2['landing_page']=='old_page'].user_id.count() $ n_old
pt = pd.pivot_table(airbnb_df, index='date_listed', values='id', aggfunc=[len]) $ pt.rename(columns={'len':'num_listings'}, inplace=True) $ pt.head(10)
from google.colab import files $ uploaded = files.upload()
reddit['Date Only'].unique()
pl.hist(yc_new2['tipPC'], bins=100) $ pl.ylabel('N') $ pl.xlabel('tipPC') $ pl.title('Distribution of Tips as a percentage of Fare') $ print('The first moment is 4.89 and the second moment is 15.22')
stocks.loc['Apple']
lm = sm.OLS(df_new["converted"],df_new[["intercept","control","US","UK"]]) $ results = lm.fit() $ results.summary()
t.minute
sel_df = df[df['Beat'].isin(selected_beats)]
trump_bow.shape, trump_cleaned_bow.shape, trump_stemmed_bow.shape
origin=m2[['visitor_id_x','country']] $
not_in_misk = not_in_misk.copy()[not_in_misk.copy()['full_name_y'].isnull()][['name', 'forks_url_x', 'forks_count_x', 'created_at_x']] \ $     .rename(columns={'forks_url_x': 'forks_url', 'forks_count_x': 'forks_count', 'created_at_x': 'created_at'})
pd.read_csv("Data/microbiome_missing.csv", na_values=['?', -99999]).head(20)
df = data
embeddings_matrix.shape
n_old = len(df2.query('landing_page=="old_page"')) $ n_old #displaying the number of individuals receiving old page
np.mean(comps_count['competitor_count'])
future_day_of_week('1/1/00', 21)
age_hist.reset_index(level=0, inplace=True)
recipes['ingredients'].str.len().idxmax()
forecast = pd.DataFrame(forecast,index = future_dates_df.index,columns=['Forecast'])
new_converted_simulation = np.random.binomial(n_new, p_new, 10000) / n_new $ old_converted_simulation = np.random.binomial(n_old, p_old, 10000) / n_old $ p_diffs = new_converted_simulation - old_converted_simulation $ p_diffs
movies.shape
(act_diff < p_diffs).mean()
ab_file.nunique()
train.head()
print(metrics.classification_report(testy, ada_predict)) $
writer = pd.ExcelWriter("../visualizations/uber_day_of_week.xlsx")
run txt2pdf.py -o "2018-06-18 1525 2014 872 discharges.pdf"  "2018-06-18 1525 2014 872 discharges.txt"
cats_df['breed'].value_counts()
n_old = df2[df2['landing_page']=="old_page"].count()[0] $ n_old
from sklearn.cross_validation import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) $ model2 = LogisticRegression() $ model2.fit(X_train, y_train) $ model.score(X_train, y_train) $
x.insert(2, 'A') $ print(x) $ x.insert(3, [1, 2])  # Note: insert() is for elements, so [1, 2] is a single element, not expanded $ print(x)
nconvert = len(df2[(df2.converted==1) & (df2.group=='treatment')]) $ ntot = len(df2[(df2.group=='treatment')]) $ prob_2 = nconvert/ntot $ print(prob_2)
notus.loc[condition, 'country'] = notus.loc[condition, 'cityOrState']
vod.groupby('crncy').sum()['amt outstanding'].plot(kind='barh')
autos['registration_year'].describe()
n_user_days.value_counts().sort_index()
van15_fin['reverted_mode'] = van_final.groupby(['userid']).agg({'isReverted': lambda x:stats.mode(x)[0]})
cvec = CountVectorizer(ngram_range = (1,4), lowercase = True, $                        stop_words = 'english', max_features = 10000)
releases = pd.read_csv('../input/jail_releases.csv') $ bookings = pd.read_csv('../input/jail_bookings.csv')
df = final_annotations_df.dropna() $ df[df.young_present]['choice'].value_counts()
DATE = '2017-10-11' $ df_save = pd.read_hdf('walltimejobs_'+DATE+'.h5', 'df_save') $ result = get_result(df_save) $ display(result.head(5)) $ display(result.shape)
plt.figure(figsize=(12,8)) $ sns.violinplot(x=trump.source, y= 'num_unique_words', data= trump) $ plt.xlabel(' Source of Tweet', fontsize=12) $ plt.ylabel('NUmber of Unique Words', fontsize=12) $ plt.title('Number of Unique Words per Tweet by Source', fontsize=15)
ab_df.isnull().sum()
preproc_training_test = pd.DataFrame.as_matrix(training_test_val)
tweets2 = pd.concat([josh_tweets, matt_tweets], axis=0) $ tweets2.shape
rand_word = random.randint(0, len(d)) $ rand_word, d[rand_word]
temp = master_file['ROCK NAME'].value_counts() $ filter_ = r'\[(\d)*\]' $ temp.index = temp.index.astype(str).str.replace(filter_,'').str.strip().str.replace('  ',' ') $ temp = temp.groupby(temp.index).sum().sort_values(ascending=False) $ print(temp)
TotalNameEvents = TotalNameEvents.rename(index=str, columns={"dmax": "created_time", "events_Name": "events_name","politician_Name":"politician_name"}) $ TotalNameEvents["diff"] =  TotalNameEvents["diff"].astype('int', copy=False)+1
StockData = StockData.set_index('Date') $ StockData.head()
!wc -l *.csv
%%time $ convert_dictionary_to_sorted_list = lambda x: [x[a] for a in sorted(x)]
import requests $ import json $ import itertools
(actual_diff < p_diffs).mean()
cvec.build_analyzer()
X = pd.merge(X, meal_inferred_types[['meal_id', 'is_inferred_by_text_columns_breakfast', 'is_inferred_by_text_columns_brunch', 'is_inferred_by_text_columns_lunch', 'is_inferred_by_text_columns_dinner']], on='meal_id', how='inner')
df_max.head()
run txt2pdf.py -o '2014 Snapshot.pdf' '2014 Snapshot.txt' $
department_df_sub.apply(np.sum, axis = 1)# apply function to each row 
number_of_google_developed_apps_installed(df_apps, sample_id)
USvideos = pd.read_csv('data/USvideos.csv', parse_dates=['trending_date', 'publish_time'])
output = pd.DataFrame(data={"id":test["id"], "rating":predictions}) $ output.to_csv("svm.csv", index=False, quoting=3)
df = pd.read_csv('census.csv') $ df = df[df['SUMLEV']==50] $ df = df.set_index('STNAME').groupby(level=0)['CENSUS2010POP'].agg({'avg': np.average}) $ pd.cut(df['avg'],10)
model_data_df = final_data_prep(random_non_crashes_df,random_crashes_upsample_df)
run txt2pdf.py -o "OROVILLE HOSPITAL  Sepsis.pdf"   "OROVILLE HOSPITAL  Sepsis.txt"
def count_non_null(df, fld): $     has_field = np.logical_not(pd.isnull(df[fld])) $     return has_field.sum(), df.shape[0]
import os $ import requests $ import pandas as pd $ import tweepy $ import json
%%time $ df['created_at'] = pd.to_datetime(df['Created Date'])
dfcsv = df.loc[df['fileType'] == 'csv'] $ dfcsv['fileCount'].describe()
kmeans_model = KMeans(n_clusters=2, init='k-means++', random_state=42).fit(crosstab_transformed) $ c_pred = kmeans_model.predict(crosstab_transformed)
medals_data.head()
network_simulation["t_current"]=users["TwM_tCurrent"] $
from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS $ stop_words = ENGLISH_STOP_WORDS $ cv = CountVectorizer(stop_words=stop_words) $ dtm = cv.fit_transform(no_urls_all_engl) $ dtm
df = pd.concat([df, sentiments_df], axis=1) $ df.to_csv('file_output\\news_mood.csv') $ df.head()
import pandas as pd $ import matplotlib.pyplot as plt $ import datetime
df_new['intercept'] = 1 $ df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']]
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)
bt.plot_trades() $ ohlc.C.rolling(short_ma).mean().plot(c='green') $ ohlc.C.rolling(long_ma).mean().plot(c='blue') $ plt.legend(loc='upper left') $ pass
print(autos['ad_created'].str[:10].unique().shape) $ autos["ad_created"].str[:10].value_counts(normalize=True, dropna=False).sort_index()
new_seen_click_read_action_project_article = seen_click_read_action_project_article[seen_click_read_action_project_article['published_at'] < seen_click_read_action_project_article['seen_at']] $ print(new_seen_click_read_action_project_article.shape) $ new_seen_click_read_action_project_article.head()
cols_to_keep = ['Churn','gender', 'SeniorCitizen', 'Partner','Dependents','tenure','PhoneService','PaperlessBilling','MonthlyCharges'] $ data = df[cols_to_keep].join(dummy_IntServ.loc[:, 'IntServ_Fiber Optic':]) $ data = data.join(dummy_Contract.loc[:, 'Contract_One year':]) $ data = data.join(dummy_PayMethod.loc[:, 'PayMethod_Credit card (automatic)':]) $ print(data.head())
df.describe()
print('original list: ', target_docs[0]) $ print('tokenized list: ', vecs[0])
autos.info()
output_col = ['msno','how_long_it_will_expire','is_how_long_equal_to_plan_days', $               'is_how_long_longer_than_plan_days','days_longer_than_plan_days', $              'is_early_expiration','early_expiration_days' $              ] $ transactions[output_col]
df_weather = pd.read_csv("data/860638.csv") $ df_weather = df_weather.append(pd.read_csv('data/860640.csv'))
print_groups(mcg)
similarity_weight = cosine_similarity(sample_pivot_table) $ prefiltering_sw = prefiltering_of_neighbors(similarity_weight, 0.1) $ predicted_table = cs_classification_predicted_score_user_based(sample_pivot_table,prefiltering_sw,[0,1])
df.loc[:, 'id'].is_unique
gs.fit(x_train,y_train)
avg_reorder_days = prior_orders.groupby(["user_id"])['days_since_prior_order'].aggregate('count').reset_index(name='avg_days_prior_order') $ avg_reorder_days.head()
df_countries = pd.read_csv("countries.csv") $ df_countries.head()
customer = 1604011635399
tok_trn, train_labels = get_all(df_train, 1) $ tok_val, val_labels = get_all(df_val, 1)
model_lm3 = LogisticRegression() $ model_lm3.fit(X3_train, y3_train) $ y3_preds = model_lm3.predict(X3_test) $ confusion_matrix(y3_test, y3_preds)
xmlData['country'].value_counts()
top_brand_info['mean_price'] = mean_price $ top_brand_info
high_pdiff = [x > difference for x in p_diffs]
pd.Series([42, 13, 2, 69])
from pandas import DataFrame $ df = DataFrame.from_csv("DonaldTrump_statusesfacebook.csv") $ df.head()
term_freq_df.sort_values(by='total', ascending=False).iloc[:10]
def oppConversion(x): $     p = '' $     if type(x.lead_converted_date) == pd.tslib.Timestamp: p = 'convertedLead' $     return p
df['date_of_admission'] = pd.to_datetime(df['date_of_admission']) $ df_bill_data['date_of_admission'] = pd.to_datetime(df_bill_data['date_of_admission']) $ df = pd.merge(df, df_bill_data, on=['patient_id','date_of_admission']) $ print(df.shape)
access_logs_df.select(min('contentSize'),avg('contentSize'),max('contentSize')).show()
appleNeg = companyNeg2[companyNeg2['author_id'].str.contains('AppleSupport') | $                        companyNeg2['text'].str.contains('AppleSupport')]
dfmean = df.groupby('Single Name').mean()[df.groupby('Single Name').count()['Name']>6] $ dfmean.reset_index() #turn index into a column
ins['year'] = ins['new_date'].dt.year $ ins.head(5)
store_items = store_items.rename(index={'store 3': 'last store'}) $ store_items
target1.size(0)
df.tail()
max_ch_ol1 = max(abs(v.close-next(islice(o_data.values(), i+1, i+2)).close) for i, v in enumerate(o_data.values()) if i < len(o_data)-1) $ print('A one liner using islice: {:.2f}'.format(max_ch_ol1))
!ptdump 'data/NYC-yellow-taxis-100k.h5'  # try also h5dump
df_state_votes.sort_values("hill_trump_diff").head(10)
df_titanic_temp.age = categories $ df_titanic_temp.head()
tweets.head()
model.load_weights(weights_path) $ model.save('wikigrader/data/nn_model.hdf5')
y.value_counts()
Imagenes_data.describe()
import statsmodels.api as sm $ convert_old = df2.query('group == "control" and converted == 1').user_id.nunique() $ convert_new = df2.query('group == "treatment" and converted == 1').user_id.nunique() $ n_old = df2.query('group == "control"')['user_id'].nunique() $ n_new = df2.query('group == "treatment"')['user_id'].nunique()
contributions_by_day = sorted_by_date.groupby(['contb_receipt_dt']).sum().reset_index() $ contributions_by_day.contb_receipt_dt = pd.to_datetime(contributions_by_day.contb_receipt_dt) $ contributions_by_day.sort_values('contb_receipt_dt', inplace=True, ascending=True) $ contributions_by_day
df.CATS_Counter.head()
plt.plot(total['field4']) $ plt.show()
df = pd.read_csv("../../data/msft_with_footer.csv",skipfooter=2,engine='python') $ df
a=map(lambda x: x+'y',['a','b'])
autos.price.value_counts().sort_index(ascending=False).head(20)
graffiti2 = gpd.sjoin(p, g_geo, how='left')
display(data.head())
data = pd.read_csv("datosSinNan.csv")
n_new = df_new.shape[0] $ print(n_new)
batting_df.tail(30)
result_df = pd.DataFrame({'profile_id':profile_ids}) $ result_df['num_events'] = result_list
from scipy.sparse import coo_matrix
df.columns = ['Timestamp', 'Price']
stories = pd.concat([stories.drop('tags', axis=1), tag_df], axis=1) $ stories.head()
df_train['dow_date'] = df_train['date'].dt.day_name() $ df_test['dow_date'] = df_train['date'].dt.day_name() $
mpl.rc('figure', figsize=(6, 3.5)) $ prophet_model.plot(forecast,uncertainty=True) $ plt.show;
my_df["day_from"] = my_df["date"] - min(my_df["date"]) $ my_df["day_from"] = my_df["day_from"].dt.days $ my_df["time"] = my_df["timestamp"].dt.hour * 60 + my_df["timestamp"].dt.minute $ print(my_df.head())
import time $ ctime = obj['openTime']/1000 $ time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(ctime))
'Ohio' in frame3.columns
sample.asfreq('H')
engine.execute("SELECT * FROM measurements limit 5").fetchall()
df['score'] = df['rating_numerator'] / df['rating_denominator'] $ df['score'].describe()
vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,4)) $ lsa = TruncatedSVD(100, algorithm = 'randomized') $ clf = LinearSVC() $ pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer),('clf', clf)])
tvec = TfidfVectorizer(stop_words='english') $ X_train_counts = tvec.fit_transform(X_train) $ X_test_counts = tvec.transform(X_test)
print(etsamples.subject.unique()) $ print("There are %i subjects"%(etsamples.subject.unique().shape))
cashflows_act_investor_20150430[(cashflows_act_investor_20150430.id_loan==27) & (cashflows_act_investor_20150430.fk_user_investor==2190)].to_clipboard()
df['date'] = pd.to_datetime(df['date']) $ df
X = tickets[['meal_id', 'master_menu_id', 'percentage_of_seats_sold', 'sold', 'meal_date', 'days_to_sell', 'number_of_seats', 'ticket_price']]
avg_monthly = np.mean(df.month.value_counts()) $ std_monthly = np.std(df.month.value_counts()) $ print('The average beers drank per month is {:.2f} beers and the standard deviation is {:.2f} beers.'.format(avg_monthly, std_monthly))
twelve_months_prcp.plot(figsize = (10,7), rot = 45, use_index = True, legend=False) $ plt.ylabel('Precipation') $ plt.xlabel('Date') $ plt.title("Precipition in Hawaii from %s to %s" % (twelve_months_prcp.index.min(),twelve_months_prcp.index.max())) $ plt.show()
extractor = twitter_setup() $ tweets_dalai = extractor.user_timeline(screen_name="Pontifex", count=200) $ print("Number of tweets extracted: {}.\n".format(len(tweets_neta))) $ tweets_pope = extractor.user_timeline(screen_name="DalaiLama", count=200) $ print("Number of tweets extracted: {}.\n".format(len(tweets_erdog)))
grouped.sum()
dfcopy = df.copy() $ dfcopy.sort_values('date').head()
pd.to_datetime("2018/04/23")
ben_fin['stiki_percent']=ben_fin['stiki_mean']<0.1
fig, ax = plt.subplots(figsize=(14,8)) $ g = sns.boxplot(y='PRICE', x=condo_6['SALEDATE'].dt.year, data=condo_6, ax=ax) $ t = g.set_title("Distribution of Sale Price by Year")
pop_treat = len(df2[df2.group == "treatment"]) $ pop_treat
temp_df2.sort_values(by='titles').head()
a = df['a'] $ a
jobs = pd.read_csv("descriptions.csv.gz", compression='gzip')
type(live_capture) $
pd.DataFrame(A)
import requests
new_pg=df2[df2['landing_page']=='new_page'] $ len(new_pg)/len(df2)
p_received_new_page = df2.query('landing_page=="new_page"').user_id.nunique()/df2.user_id.nunique() $ p_received_new_page
p_new = df2[df2['landing_page']=='new_page']['converted'].mean() $ print("Probability of conversion for new page (p_new):", p_new)
df2[df2.duplicated(['user_id'],keep=False)].index[0]
print len(history_list)
enc_wgts = to_np(wgts['0.encoder.weight']) $ row_m = enc_wgts.mean(0)
from search_utils import tweet_time_2_epoch $ tweet_time_2_epoch('Mon Jan 08 12:22:33 +0000 2018')
cleaned_asf_people_human_df_saved.count()
master_list['Count'].describe()
def yearFixer(s): $     main_part = s[:-2] $     year_before = s.split('/')[2] $     year_after = str(20) + year_before $     return main_part+year_after
google = web.DataReader('MSFT','google',start,end) $ google.head()
len(df_questions["Capsule id"].value_counts())
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head() $ df_new['country'].astype(str).value_counts()
len(calls.location.unique())
df_control = df.query("group == 'control'") $ df_control.converted.mean()
pd.Series(Counter(missing_hours)).plot(kind="bar")
!gdalwarp -overwrite -t_srs epsg:4326 -srcnodata none -co compress=lzw %local_orig% %local_edit%
hp.sync_tmpos()
pd.Series(5, index=[100, 200, 300])
from microsoftml_scikit.utils.exports import dot_export_pipeline $ dot_vis = dot_export_pipeline(pipeline, ds_train) $ print(dot_vis)
pres_df['metro_area'].unique()
merged = df2.set_index('user_id').join(countries.set_index('user_id')) $ merged.head()
n_old = df_control.nunique()['user_id']
tl_2040 = pd.read_csv('input/data/trans_2040_ls.csv', encoding='utf8', index_col=0)
cm = confusion_matrix(y_main, ypred)
ts[pd.datetime(1951, 6, 1):pd.datetime(1952, 1, 1)]
back_to_h2o_frame = h2o.H2OFrame(pandas_small_frame) $ print(type(back_to_h2o_frame)) $ back_to_h2o_frame
df.query('landing_page == "new_page"')['user_id'].count()/df.shape[0]
data1.shape
package.descriptor['resources'][1]
data.to_csv("csvs/datosSinDuplicados.csv", index = False)
embarked = df_titanic['embarked'] $ print(embarked.describe()) $ df_titanic['embarked'] = df_titanic.embarked.astype('category')
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ z_score, p_value
merged_NNN.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount", ascending=False)
pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])
df2.set_index('user_id').index.get_duplicates()
store_items['suits'] = store_items['shirts'] + store_items['pants'] $ store_items
cleansed_search_df['Date'] = pd.to_datetime(cleansed_search_df['Date'], format="%Y%m%d", errors="coerce") $ cleansed_search_df
tfidf.fit(text)
datetime.now().strftime('It is %H:%M, on %h %d, %Y.')
print('RF:', rf.score(X_test, y_test)) $ print('KNN:', knn.score(X_test, y_test))
plt.plot(np.random.randn(100), np.random.randn(100), 'ro') $ plt.savefig('first_plot.png')
stories[['karma', 'score']].corr()
df_lda = pd.DataFrame(data_lda) $ print(df_lda.shape) $ df_lda = df_lda.fillna(0).T $ print(df_lda.shape)
X_train, X_test, y_train, y_test = sample_split(df_[selected_feature_updated])
dfgts_sorted = dfgts.sort_values(by=['date', 'team'], ascending=True).reset_index()
df3.isnull().sum()
n_old = df2.query("landing_page == 'old_page'")['landing_page'].count() $ n_old
len(finnal_data["u_id"].unique())
plt.show()
df2[df2['landing_page'] == "old_page"]['converted'].sum()
c = df.groupby(['education', 'purpose']).agg({'applicant_id': lambda x: len(set(x))}).reset_index() $ c['education+purpose'] = c['education']+c['purpose'] $ sns.barplot(data=c, x='education+purpose', y='applicant_id')
np.linspace(0,2,9)
df_img_predictions.describe()
print('Shifts without a pause: {}/{}'.format(len(df['pause'][df['pause'].astype('timedelta64[m]') == 0]), len(df)))
submissions.head()
image = image_soup .find('div', class_='carousel_items') $ image_url = image.article['style'] $ url = image_url.split('/s')[-1].split('.')[0] $ featured_image_url= 'https://www.jpl.nasa.gov' +'/s'+ url + '.jpg' $ print(featured_image_url )
data.columns
import pickle $ pickle.dump(knn_reg, open('model.sav', 'wb'))
if not database_exists(engine.url): $     create_database(engine.url) $ print(database_exists(engine.url))
s_n_s_epb_one.Date = pd.to_datetime(s_n_s_epb_one.Date,format="%Y%m%d")
df_tsv.info()
import requests $ import pandas as pd $ import numpy as np $
slDf = pd.concat([slCases, slDeaths],axis=1) $ slDf.index.name = 'Date' $ slDf.head()
pd.value_counts(appointments[appointments['Specialty'] == 'therapist']['Provider'])
test_ind["Pred_state_RF"] = trained_model_RF.predict(test_ind[features]) $ train_ind["Pred_state_RF"] = trained_model_RF.predict(train_ind[features]) $ kick_projects_ip["Pred_state_RF"] = trained_model_RF.predict(kick_projects_ip_scaled_ftrs)
testObj.buildOutDF(tst_lat_lon_df[600:])  ## end of the df added in $
ax = dfg.plot(rot=45) $ plt.tight_layout() $
url_CLEAN1A = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1A.csv" $ url_CLEAN1B = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1B.csv" $ url_CLEAN1C = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1C.csv" $
df2.query("user_id == '773192'")
store_items.size
next(iter(md.trn_dl))
data = {'Job_A': pd.Series(data = [2, 300.000]), $                  'Job_B': pd.Series(data = [6, 2])} $ job_requirements = pd.DataFrame(data) $ job_requirements
churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']] $ churn_df['churn'] = churn_df['churn'].astype('int') $ churn_df.head()
not_found_df = logs_df.filter(col('status')==404).cache() $ print('Found {0} 404 URLs'.format(not_found_df.count()))
(ab_data.converted == 1).mean()
def evaluate_classifier(classifier, x_test, y_test): $     pred_y = predict(classifier, x_test) $     return metrics.accuracy_score(y_test, pred_y) $ accuracy = evaluate_classifier(classifier, x_test , y_test) $ print (accuracy)
out = conn.loadtable('data/iris.csv', caslib='casuser') $ out
df.count()
twitter_archive_df_clean['expanded_urls'].isnull().any()
difference = total.xs_tally - absorption.xs_tally - scattering.xs_tally $ difference.get_pandas_dataframe()
lm = sm.Logit(df_new['converted'],df_new[['intercept','treatment','UK','US']]) $ results = lm.fit() $ results.summary()
df3['intercept'] = 1 $ logit3 = sm.Logit(df3['converted'], df3[['intercept','new_page','UK','US']]) $ result = logit3.fit() $ result.summary()
geo_file_name
total_df['Rating'].plot.hist(bins = 8) $ plt.title('Overall Rating Distribution') $ plt.ylabel("Frequency") $ plt.xlabel("Rating")
def create_df_grouped_by_date( tweets_df ): $     return tweets_df.groupby( Grouper( 'date' )).mean()
average = labmt.happiness_average.mean() $ happiness = (labmt.happiness_average - average).to_dict()
len(df2.query('converted==1').index)/len(df2.index)
df = data1.append(data2).reset_index() $ df['date'] = pd.to_datetime(df['starttime']) $ df.head(3)
saved
pd.options.display.max_rows = 100 $ pd.options.display.max_columns = 200
station_distance.columns.tolist()
df2['intercept']=1 $ df2['ab_page'] = df2['group'] == 'treatment' $ df2['ab_page'] = df2['ab_page'].astype(int)
cohorts_unstacked = cohorts['TotalUsers'].unstack(0).head(10) $ cohorts_unstacked_nonblank = cohorts_unstacked.drop(cohorts_unstacked.columns[0], axis=1) $ cohorts_unstacked_nonblank
new_page_converted
types_of_complaints = all_complaints['Descriptor'].value_counts() $ types_of_complaints.head()
df2['intercept'] = 1 $ df2[['new_page', 'old_page']] = pd.get_dummies(df['landing_page']) $ df2.head()
df['target']=df.num_comments.map(lambda x: 1 if x>=med else 0)
user.info()
writer = pd.ExcelWriter("../visualizations/uber_day_of_month.xlsx")
actions = pd.DataFrame(session['action'].value_counts().reset_index()) $ actions_top10 = actions.head(10) $ actions_top10
another_set = {2, 2.0, 'two', 'Two', 100, 'one hundred'}
args = mfclient.XmlStringWriter("args") $ args.add("where", "namespace=/projects/proj-hoffmann_data-1128.4.49/data") $ args.add("action", "get-path") $ args.add("size", "infinity") $ data_query = con.execute("asset.query", args.doc_text())
matches['home_team_win_or_draw'] = 0 $ matches.loc[matches['home_team_goal'] >= matches['away_team_goal'], 'home_team_win_or_draw'] = 1
weights = get_roll_weights('6/7/2017', expiry, prices.columns)
cur.close() $ con.close()
to_be_predicted_Day1 = 33.10 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
ind = pd.date_range(start= '01-07-2017',periods= 9,freq= '2W-MON') $ ind
df2.shape
gs_rfc_under.score(X_test, y_test_under)
X_train_, X_test_, y_train_, y_test_ = sample_split(df_std[selected_feature])
df = df.iloc[2:] $ df.head()
html_page = browser.html $ JPL_soup = bs(html_page, "lxml")
user_status_model.possibly_sensitive
hashtag_counts = hashtags.value_counts().head(10) $ hashtag_counts
data = check_y(df_1, delta_change=-3.0, start = 480, end = 600) 
sample_submission.head(5)
import pandas as pd $ import os
import scipy.stats as stats
(autos["last_seen"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index().head() $ )
cleaned_sampled_contirbutors_human_df_saved.schema
loans_df.isnull().sum()
df5 = pd.read_csv('2005.csv')
jun18=pd.read_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_2/01_target_summer2017_night0_with_hourangle.csv') $ jun18_ob=jun18[~jun18["Taken? (one x: two exposures. Two x's: 4. Three x's: 6)"].isnull()] $ matched_id2=[np.where((final0['name']==i)==True)[0][0] for i in jun18_ob['name']] $ observed18=jun18[["name","Taken? (one x: two exposures. Two x's: 4. Three x's: 6)","comments","FW","air mass"]] $ final1=pd.merge(final1, observed18, on='name', how='outer', indicator=True) $
print("Arrange multiple column values in ascending order") $ df.sort_values(by=['Indicator_id','Country','Year','WHO Region','Publication Status'], axis=0, inplace=True) $ df.sort_index(inplace=True) $ df[['Indicator_id','Country','Year','WHO Region','Publication Status']].head(3)
sns.violinplot(x="embark_town", y="age", hue="sex", data=titanic, split=True)
data_sets['15min'].to_pickle('final_15.pickle') $ data_sets['30min'].to_pickle('final_30.pickle') $ data_sets['60min'].to_pickle('final_60.pickle')
((~autos["registration_year"].between(1900,2016)).sum() / autos.shape[0])*100
autos.describe(include="all")
merged.amount.sum()
pprint(ldamodel.print_topics())
for t in trump_tweets[0:3]: $     print(t['created_at'])
import pandas as pd $ import numpy as np
print(abs(-12.0), '\n') $ print(len([1, 2, 3, 4, 5])) $ print(set([1, 2, 3, 4, 5, 5, 4]))
assert top50.dataset_id.isnull().sum() == 0
df.A > 0
top_songs['Artist'].isnull().sum()
class_merged.columns=['date','store_nbr','class','family','sum_unit_sales','no_items','no_perishable_items','items_onpromotion'] $ pd.DataFrame.head(class_merged)
ratings['rating_numerator'] = ratings['rating_numerator'] / ratings['rating_denominator'] * 10 $ ratings['rating_denominator'] = 10 $ ratings = ratings.round({'rating_numerator': 2})
mit = bq.Query(mitquery).to_dataframe()
pickle_it(LSVC, 'lsvc_classifier') $ LSVC = open_jar('lsvc_classifier')
data[data['Borough']=='Unspecified'][['Agency','Incident Zip']]
%matplotlib inline
image_pred['dog_species'] = image_pred.apply(lambda x: dog_species(x),axis=1)
dataBPL.tail(20)
num_rdd = access_logs_raw.count() $ num_rdd
y = new["Price"] $ x = new.drop("Price",axis=1) $ Y = new_scaled["Price"] $ X = new_scaled.drop("Price",axis=1)
Zn1Zn = (model.transmat_.T * Zn_post).T
with tf.name_scope("eval"): $     correct = tf.nn.in_top_k(logits, y, 1) $     accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) $     accuracy_summary = tf.summary.scalar('accuracy', accuracy)
from datetime import datetime $ date = datetime.strptime(logins.login_time.iloc[0], '%Y-%m-%d %H:%M:%S') $ print(type(date), date.weekday)
check_int('2017-01-17').number_repeat.max()
old_page_converted = np.random.choice(2, size=n_old ,p=[p_old,1 - p_old]) $ old_page_converted.mean()
airport_count = faa_data_pandas['AIRPORT'].value_counts() $ print(airport_count)
fig, ax = plt.subplots() $ load2017.plot(ax=ax, title="Actual load 2017 (DE-AT-LUX)", x='timestamp', y='actual', lw=0.7) $ ax.set_ylabel("Actual Load") $ ax.set_xlabel("Date")
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
merged_data = pd.merge(left=surveys_df,right=species_df, how='inner', left_on='species_id', right_on='species_id') $ merged_data.head()
closingprice = [] $ for ele in r.json()['dataset']['data']: $     closingprice.append(ele[4]) $ change = max(closingprice)-min(closingprice) $ print('The largest change between any two days based on Closing Price: ' +str(change))
from sklearn.metrics import classification_report $ print(classification_report(y_test,y_pred_mdl))
w.get_step_object(step = 3, subset = subset_uuid).indicator_objects['din_winter'].get_water_body_indicator_df(water_body = wb)
df.groupby(['education'])['loan_status'].value_counts(normalize=True)
y = df.iloc[:,7].values $ X = df.iloc[:,[0,1,2,3,4,5,6,8,9,10,11,12]].values
RDDTestScorees[0][1] = 100
top=df.groupby('breed').filter(lambda x: len(x) >= 20) $ top['breed'].value_counts().plot(kind = 'bar') $ plt.title('The Most Rated Breeds');
rddRow = rdd.map(lambda f: Row(col1 = f)) $ df = spark.createDataFrame(rddRow) $ df.show()
fillna_with_zeros(joined, 'nat_hol') $ fillna_with_zeros(joined, 'nat_event')
k = 4 $ neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train) $ neigh
df_yt_resolved.head(2)
df['province'] = map(lambda x: int(x),df.province) $ df['city'] = map(lambda x: int(x),df.city)
!head msft_temp.csv
coming_next_reason = questions['coming_next_reason'].str.get_dummies(sep="'")
mod = MyModel()
url = "https://www.cgv.id/en/schedule/cinema/" $ request = urllib.request.Request(url) $ page = urllib.request.urlopen(request) $ soup = BeautifulSoup(page,"lxml") $ soup.find('div', class_="sect-city").find_all('a')
r.head()
tfav.plot(figsize = (16, 4), color ='b') $
authors['mean'].head()
itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb')) $ stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)}) $ len(itos)
plotdf['forecast'] = plotdf['forecast'].interpolate() $ plotdf['forecastPlus'] = plotdf['forecastPlus'].interpolate() $ plotdf['forecastMinus'] = plotdf['forecastMinus'].interpolate()
print cust_data.drop_duplicates(keep='last').head(3) $ print cust_data.drop_duplicates(keep=False).head(3)
terms_bigram = bigrams(terms_stop) $ count_bigram = Counter() $ count_bigram.update(terms_bigram) $ print(count_bigram.most_common(5))
autos["price"].unique().shape
autos["price"]= autos["price"].astype(int)  $
time_series.info()
feature_importances.columns
store_items['new watches'] = store_items['watches'][1:] $ store_items
all_columns = pd.Series(list(df_twitter_archive) + list(df_img_predictions) + list(df_tweet_data)) $ all_columns[all_columns.duplicated()]
X_test = combine_X_data(test_projection, test_df.length, test_df.paragraphs, $                        test_df.num_words, test_df.spelling_errors, $                        test_df.average_sentence_length, test_df.sentence_length_variance) $ y_test = test_df.reward.values.astype(float) $ X_test,y_test
stack_with_kfold_cv.to_csv('stacking_input/stack_kfold_cv_pub_mine.csv',index=False)
trips = ["08-01", "08-02","08-03","08-04","08-05","08-06","08-07","08-08","08-09","08-10"]
data
dimension = vec_code.shape[-1] $ index = AnnoyIndex(dimension) $ for i, v in enumerate(vec_code): $     index.add_item(i, v) $ index.build(10)
aus = ['Sydney', 'New South Wales', 'Australia', 'Melbourne', 'Brisbane', 'Queensland', 'Canberra', 'Perth'] $ notus.loc[notus['country'].isin(aus), 'country'] = 'Australia' $ notus.loc[notus['cityOrState'].isin(aus), 'country'] = 'Australia' $ notus.loc[notus['country'] == 'Australia', 'cityOrState'].value_counts(dropna=False)
pvt.reset_index(inplace=True) $ pvt
mismatch_all = pd.concat([no_mismatch_newpage, no_mismatch_oldpage])
pred.img_num.value_counts() $
LARGE_GRID.plot_accuracy(raw_large_grid_df, option='dodge')
df2.groupby('group')['converted'].value_counts()/df.groupby('group')['converted'].count()
coefs = zip(predictor_cols,model.coef_) $ for coef in coefs: $     print(f"Our model predicts that for a unit increase in {coef[0]}, a team will receive **{round(coef[1],2)}** more mentions in the NYT during the regular season.\n")
dfx = df.reindex( index=dates[0:4], columns=list(df.columns) + ['E']) $ print(dfx, '\n') $ dfx.loc[dates[0]:dates[1], 'E'] = 1 $ print(dfx, '\n')
df_2009.dropna(inplace=True) $ df_2009
(r_clean * hist_alloc).sum(axis=1).hist(bins=50)
data.head()
print("Row names for data-text.csv:") $ df.index.values
RE_GENE_NAME = re.compile(r'Name=(?P<gene_name>.+?);') $ def extract_gene_name(attributes_str): $     res = RE_GENE_NAME.search(attributes_str) $     return res.group('gene_name') $ gene_df['gene_name'] = gene_df['attributes'].apply(extract_gene_name)
plt.hist(null_values) $ plt.axvline(obs_diff, c='red')
type(array)
cust_demo.age.hist(bins=50, color='R')
dfCleaningData = dfData.dropna() $ dfCleaningData.count()
emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]
data2.index = list(range(len(data2.index)))
classification_data.head(2)
aussie_results = [] $ for tweet in tweepy.Cursor(api.search, q='%23Aussie').items(2500): $     aussie_results.append(tweet) $ print(len(aussie_results))
nb_samples = 1460
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page'")['converted'].sum() $ convert_old $
doc_topic = model.doc_topic_ 
sp.str.get(-1) 
average_polarity_2012, count_polarity_2012=polarity_weekly(2012) $ average_polarity_2013, count_polarity_2013=polarity_weekly(2013) $ average_polarity_2014, count_polarity_2014=polarity_weekly(2014) $ average_polarity_2015, count_polarity_2015=polarity_weekly(2015) $ average_polarity_2016, count_polarity_2016=polarity_weekly(2016) $
accuracies=cross_val_score(estimator=classifier,X=X_train,y=y_train,cv=10,n_jobs=1)
import pandas as pd $ import numpy as np $ import tweepy $ import json $ import requests
sns.lmplot(x="age", y="satisfied", data=training, x_estimator=np.mean, order=1)
dfFull['1stFlrSFNorm'] = dfFull['1stFlrSF']/dfFull['1stFlrSF'].max()
df2['intercept']=1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group'])
subwaydf.iloc[10874:10880] #this low number seems to be because entries and exits resets
df.groupby(by = ['Item Description']).sum().sort_values(by = ['Sale (Dollars)'], ascending = False).head()
frame3.columns
with open(PredClass.save_model, 'wb') as w: $     pickle.dump(reg_final, w)
seed = 2210 $ (train1, dev1, modeling2) = (modeling1 $                              .randomSplit([0.4, 0.1, 0.5], seed=seed))
test_cleaned_bow = vect_cleaned_bow.transform(test_cleaned) $ test_stemmed_bow = vect_stemmed_bow.transform(test_stemmed) $ test_cleaned_tfidf = vect_cleaned_tfidf.transform(test_cleaned) $ test_stemmed_tfidf = vect_stemmed_tfidf.transform(test_stemmed)
df_new.groupby('country').count()
!pytest $
df_tweets2 = pd.read_csv('export_2016062306.csv', index_col = 0)
elec.mains().good_sections()
pd.value_counts(appointments[appointments['Specialty'] == 'doctor']['Provider'])
from datetime import datetime as dt, timedelta
genreTable.shape
iplot(data.groupby('user.login').size().sort_values(ascending=False)[:20].iplot(asFigure=True, kind='bar', dimensions=(750, 500)))
print len(live_df_filtered) # there are no multiple submissions for live datasets $ print len(live_df) $ print len(kd915_filtered) $ print len(kd915)
df_twitter_copy[['jpg_url', 'p1']] = df_twitter_copy[['jpg_url', 'p1']].fillna('None') $ df_twitter_copy['p1_conf'] = df_twitter_copy['p1_conf'].fillna(0)
df.info()
to_be_predicted_Day4 = 22.39046488 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
history_with_target.loc[~history_with_target.target.isnull(), ['target', 'target_test','time_delta', 'next_incident_type', 'MOTIF_ANNULATION_CODE', 'ACTUAL_START_DATE', 'ACTUAL_END_DATE', 'INCIDENT_TYPE_NAME', 'SCHEDULED_START_DATE', 'SCHEDULED_END_DATE',  'INCIDENT_STATUS_NAME', 'TYPE_BI', 'MOTIF_ANNULATION_CODE', 'MOTIF_ANNULATION_DESC']][50:100]
borough = {"BROOKLYN", "QUEENS", "MANHATTAN", "BRONX", "Unspecified", "STATEN ISLAND"} $ df_input_clean = df_input.filter(df_input["Borough"].isin(borough) == True)
temp_long_df['date'] = pd.to_datetime(temp_long_df['date']) $ temp_long_df['year'] = temp_long_df['date'].dt.year $ mean_temp = temp_long_df.groupby(['year'])['temp_c'].mean().reset_index() $ mean_temp = mean_temp[mean_temp['temp_c'] > 8]
g = sns.FacetGrid(data=dataset, col='rating') $ g.map(plt.hist, 'text length', bins=50) $
pd.PeriodIndex(start='2014-01', freq='3M', periods=4)
exportOI['join_col'] = exportOI['event.destination_subzone_name'] + exportOI['timestamp']
week31 = week30.rename(columns={217:'217'}) $ stocks = stocks.rename(columns={'Week 30':'Week 31','210':'217'}) $ week31 = pd.merge(stocks,week31,on=['217','Tickers']) $ week31.drop_duplicates(subset='Link',inplace=True)
for i in range(len(billstargs.billtext)): $     value = billstargs.get_value(i, 'billtext') $     billstargs.set_value(i, 'billtext', replace_resolution(value)) $
population
retweet_pairs["FromType"] = "Person" $ retweet_pairs["ToType"] = "Person" $ retweet_pairs["Edge"] = "Retweet" $ retweet_pairs.rename(columns={"screen_name":"FromName","retweeted_screen_name":"ToName"},inplace=True) $ retweet_pairs.head()
plt.hist(p_diffs) $ plt.axvline(x=obs_diff, color='red');
request_data_2017 = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31')
import matplotlib.pyplot as plt
print(len(tweets_data))
all_preds[0:5, 0, :]
precip_df.rename(columns={'prcp': 'precipitation'}, inplace=True) $ precip_df.set_index('date', inplace=True) $ precip_df.head()
def station_mean(df): $
pd.crosstab(result['timestampDate'], result['type']).head()
print('Most LaziestCanine: {}'.format(tweets_pp[tweets_pp.handle == 'Lazy dog'].sort_values('laziestcanine_pp', ascending=False).text.values[0])) $ print('Least LaziestCanine: {}'.format(tweets_pp[tweets_pp.handle == 'Lazy dog'].sort_values('laziestcanine_pp', ascending=True).text.values[0]))
capa2017.head()
for topic in topic_words: $     print(topic[:20])
Station.__table__
new_final_job = new_job_results["data"]["job"] $ new_final_result = new_job_results["data"]["result"] $ new_accuracy = new_final_result["acc_data"]["accuracy"] $ new_predictions = new_final_result["predictions_json"].get("predictions", [])
df_TempIrregular.to_csv('data/All_Irregularities_20180601_to20180607.csv', sep=',')
team_slugs_df.head()
tweet_json_df.info()
autos["gear_box"].unique() $
espacio_verde_caba.head(10)
daily_returns.plot(kind='scatter',x='SPY',y='XOM') $ plt.plot(daily_returns['SPY'],daily_returns['SPY']*beta_XOM + alpha_XOM,'-',color='r') $ plt.show()
df_user_prod_quant = pd.merge(df_out,transactions,how='left',on=['UserID','ProductID']) $ df_user_quantity = df_user_prod_quant.groupby(['UserID','ProductID'])['Quantity'].sum() $ df_user_quantity.reset_index().fillna(0)
logit2_countries = sm.Logit(newset['converted'], $                            newset[['ab_page', 'country_UK', 'country_US', 'intercept']]) $ result_final = logit2_countries.fit()
import statsmodels.api as sm $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = len(df2.query("group == 'control'")) $ n_new = len(df2.query("group == 'treatment'"))
lrs=loan_requests_indebtedness.id_loan_request[loan_requests_indebtedness.postcheck_data!='null'].unique()
c = PTOClient(baseurl, token)
p_convert = df2.converted.mean() $ p_convert
print contractor_clean['zip_part1'].head(); contractor_clean['zip_part2'].head() $
aru_df.info()
users_nan = (users.isnull().sum() / users.shape[0]) *100 $ users_nan
subred_num_avg.rename(index=str, columns={'subreddit': 'Subreddit', 'num_comments': 'Average_Num_Comments'}, inplace=True)
req_fields='fan_count' $
unique_desc_reg=class_merged_hol['description_regional_hol'].unique() $ print (unique_desc_reg)
trainwdummies = pd.concat([train, dummy_categories], axis=1) $ trainwdummies = trainwdummies.drop('Unnamed: 0', axis=1).copy() $ trainwdummies
from skafossdk import * $ print('Initializing the SDK connection') $ skafos = Skafos()
cp311.head(3)
null_rows = kickstarter.isnull().any(axis=1) $ kickstarter[null_rows]
mlp_df.plot(figsize=(10, 10)) $ plt.show()
bixi_hourly=bixi $ bixi_hourly['number_of_trips']=1
weather_warm = weather_all[warm] $ weather_warm.head()
df2 = df.drop(df.index[(df['group'] == 'control') & (df['landing_page'] == 'new_page')]) $ df2 = df2.drop(df2.index[(df2['group'] == 'treatment') & (df2['landing_page'] == 'old_page')])
total1=total.loc[:,['name','RA0','DEC0','priority']]
goo.info()
retweets_status_keys = list(retweets[0]._json.keys()) $ len(retweets_status_keys)
df3['Donation Received Date'] = pd.to_datetime(df3['Donation Received Date']) $ df3['Most Recent Donation'] = pd.to_datetime(df3['Most Recent Donation'])
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])
gdax_orderbook.head(10)
tweet = tweets[1] $ pp.pprint([att for att in dir(tweet) if '__' not in att])
ticks.shift(-1).head()
cur.execute('SELECT LangCd ,count(*) FROM surveytabl WHERE LangCd="QB";') $ cur.fetchall()
print airbnb_df[airbnb_df['room_type']>='Private room'].shape $ airbnb_df[airbnb_df['room_type']>='Private room']['room_type'].value_counts()
df_log.user_id.nunique()
from google.colab import files $ train_char = files.upload('train_char.npy') $ train_word = files.upload('train_word.npy') $ train_labels = files.upload('train_labels.npy')
from scipy.stats import norm $ critical_val = norm.ppf(1-(0.05/2)) $ critical_val
df['user_id'][df['converted'] == 1].count() / df['user_id'].count()
countries_df = pd.read_csv('countries.csv') $ df_ab_cntry = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
ctd_df.head(10)
gs_k150.score(X_test, y_test_over)
df2.query('landing_page == "old_page"').count().user_id
twitter_merged_data.hist(column='favorites'); $ plt.title('Favorites Histogram') $ plt.xlabel('Favorites') $ plt.ylabel('Count');
df2.query('converted==1')['user_id'].count()/df2.shape[0]
mojog_df
centers = kmeans.cluster_centers_ $ centers
games_df_v2.head()
from scipy.stats import norm $ z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ print('z_score', z_score) $ print('z_score significant value :', norm.cdf(z_score)) $ print('norm.ppf', norm.ppf(1-(0.05/2)))
drop_list = ['FEDFUNDS','DGS1MO','DGS3MO','DGS6MO','DGS1','DGS2','DGS3'] $ for drop_x in drop_list: $     df.drop(drop_x, axis=1, inplace=True)
near = np.concatenate([px.values, px.values[-1] + walk])
prec_wide_df = pd.concat([grid_df, prec_df], axis = 1) $ prec_wide_df.head()
s2.iloc[1]
sumTable = tips.groupby(["sex","day"]).mean() $ sumTable
mars_facts_df.columns = ['Characteristic','Data'] $ mars_df_table = mars_facts_df.set_index("Characteristic") $ mars_df_table
n_new = df2.query('landing_page == "new_page"').count()['user_id'] $ n_new
coeff_df = pd.DataFrame(train_df.columns.delete(0)) $ coeff_df.columns = ['Feature'] $ coeff_df["Correlation"] = pd.Series(logreg.coef_[0]) $ coeff_df.sort_values(by='Correlation', ascending=False)
df_new = df2.set_index('user_id').join(countries_df.set_index('user_id')) $ df_new.head()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ print('Z-score: ', z_score) $ print('P-value: ', p_value)
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/mydata.json" $ df = pd.read_json(path) $ df.head(5)
my_model_q4 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_dt, training='label') $ my_model_q4.fit(X_train, y_train) $ y_pred = my_model_q4.predict(X_test) $ accuracy = metrics.accuracy_score(y_test, y_pred) $ print("Accuracy: " +  str(accuracy))
Q2
df2.columns
df2_clean['p1'] = df2_clean['p1'].str.title() $ df2_clean['p2'] = df2_clean['p2'].str.title() $ df2_clean['p3'] = df2_clean['p3'].str.title()
calls_temp=pd.DataFrame(calls_df["length_in_sec"]) $ calls_temp=pd.DataFrame(KNN(k=3).complete(calls_temp),columns=calls_temp.columns) $ calls_df=calls_df.drop(["length_in_sec"],axis=1) $ calls_df["length_in_sec"]=calls_temp["length_in_sec"] $ calls_df.head()
df_new = pd.merge(df2,countries,on = 'user_id') $ df_new.head()
type(date_df.created_time.loc[0])
dict_tokens = corpora.Dictionary(tokens)
products.head(2) 
df.std()
ben_dummy
tweet_info['id'] = list(map(lambda tweet: tweet['id'], tweet_appenddata)) $ tweet_info['retweet_count'] = list(map(lambda tweet: tweet['retweet_count'], tweet_appenddata)) $ tweet_info['favorite_count'] = list(map(lambda tweet: tweet['favorite_count'], tweet_appenddata)) $
pres_df.drop('ad_length_tmp', inplace=True, axis=1) $ pres_df.head()
get_records('tbl1', 3)
plt = sns.boxplot(data=df, x="race_desc", y="time_detained", hue="race_desc", dodge=False)
local_path = 'data/clean_boulder_weather.csv' $ web_path   = 'https://raw.githubusercontent.com/chrisketelsen/csci3022/master/inclass-notebooks/data/clean_boulder_weather.csv' $ file_path = web_path $ df = pd.read_csv(file_path)
grid_svc.cv_results_['mean_test_score']
n_old = len(df2[df2['group'] == 'control']) $ print(n_old)
NB_results['sentiment'].value_counts().plot(kind='pie', title="Distribution of Colombian Election Twitter Sentiment", figsize=(6,6))
print(DataSet_sorted['tweetText'].iloc[4])
basic_smapes = [vector_smape(basic_pred[col], real[col]) for col in basic_pred.columns] $ complex_smapes = [vector_smape(complex_pred[col], real[col]) for col in complex_pred.columns]
tweetsIn22Mar.head() $ tweetsIn1Apr.head() $ tweetsIn2Apr.head()
import pandas $ df2 = pandas.DataFrame(list(dict(c).values())[1:20]) $ df2.head()
type(df_ser_dict)
df_2.shape
len(data)
chefdf = pd.merge(chefdf, chef03df,  how='left', left_on=['name','user'], $                   right_on = ['name','user'], suffixes=('','_03'))
autos = autos[autos['price_dollars'].between(0,350000)]
non_na_df['hour'] = (non_na_df.index.hour + 10) % 24 $ non_na_df['dayofweek'] = non_na_df.index.dayofweek
clintondf = pd.DataFrame(HC,columns = ['datetime', 'id', 'text', 'source'])
type(twitter_Archive['timestamp'].iloc[0]) $
df3.head(3)
%%time $ test1 = model.predict_generator(custom_generator(), steps = 100, workers=2, $                                use_multiprocessing=False)
new_page_converted = np.random.choice([0,1], size=n_new, replace=True, p=[1-p_new,p_new]) $ np.bincount(new_page_converted)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new,n_old],alternative='larger') $ z_score,p_value
results["units"].head()
print example1_df.printSchema()
news_period_df.loc[3, 'news_title']
goog.index = goog.index.to_datetime()
pn_and_qty = pd.read_csv(r'data/PN_and_QTY.csv')
converted_users2 = float(df2.query('converted == 1')['user_id'].nunique()) $ p2 = converted_users2/float(df2.shape[0]) $ print("The probability of an individual converting regardless of the page they receive is {0:.2%}".format(p2))
tfv = TfidfVectorizer(ngram_range=(1,4), max_features=2000) $ X1 = tfv.fit_transform(clean_text1).todense() $ print X1.shape
rain = session.query(Measurements.date, Measurements.prcp).\ $     filter(Measurements.date > last_year).\ $     order_by(Measurements.date).all()
mask = (df['message'].str.len() > 3) #Remove cases with message length < 3. $ df = df.loc[mask]
df = pd.DataFrame() $ df
s = pd.Series(['Tom', ' William Rick ', ' John', 'Alber@t ', np.nan, '1234','SteveSmith']) $ s
test_df['pred'] = np.argmax(pred,axis=1)
baseball_newind.iloc[:5, 5:8]
window = target_google['date'] >= '2017-04-12'
keep = tag_pairs["FromName"].value_counts() $ keep = keep[keep>5] $ keep.shape
specs_eval_api.append(specsJson["nebl.io"])
from pathlib import Path $ data_dir = Path('data') $ my_zip.extractall(data_dir)
df.loc['r2','B']
start = datetime(year=2018,month=1,day=29) $ td = dt.timedelta(days=45) $ end = start + td $ p=((f.loc[start][0]-f.loc[end][1])/f.loc[start][0])*100 $ print('The closing price of '+ ticker_var +' has changed by '+ str(p) + ' % between ' + str(start) + ' and '+ str(end) +'.')
df2[['Start', 'End', 'Week']]
mean_mileage = pd.Series(mean_mileage_brand).sort_values(ascending=False) $ mean_prices = pd.Series(mean_prices_brand).sort_values(ascending=False)
sqlContext.sql("select * from pcs where count > 1").show()
P_new = df2[df2['landing_page'] == 'new_page']['converted'].mean() $ print("P_new under null: ", P_new)
df2 = df2.round({'new_sales_perc':4}) $ df2.head()
tweets_clean.drop(columns = ['in_reply_to_status_id','in_reply_to_user_id','retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'], inplace = True)
input_folder = '/Users/annalisasheehan/Dropbox/Climate_India/Data/climate/CPC/cpc_global temperature/minimum temperature' $ glob_string = '*.nc'
import numpy.linalg as la
wrd_api_clean.head(3)
X = preprocessing.scale(X)
submit.head()
G = K_p/(tau_p*s + 1)*numpy.exp(-theta*s)
import random
mars_html_table = mars_df_table.to_html(classes='marsdata') $ mars_table = mars_html_table.replace('\n', ' ') $ mars_table
fulldata_copy['RSI'] = rsi['RSI'] $ fulldata_copy.columns = ['AUDUSD','RSI']
df_dem.iloc[46657]
def convert_to_datetime(dt_str): $     fmt = "%Y-%m-%d %H:%M:%S +0000" $     return dt.strptime(dt_str,fmt)
g.head()
y.sum()
xgb_model=xgb_model.fit(train_ind[features], train_dep[response])
print(advancedtrain.shape)
data_folder = '/Users/geoffperrin/Desktop/python_sandbox/NYPD/data'
df1['volatility']=(df1['Adj. High']-df1['Adj. Close'])/df1['Adj. Close'] $ df1['volatility'].head()
temp_df = pd.DataFrame(tobs_info, columns=['date','tobs']) $ temp_df.set_index('date', inplace=True) $ temp_df.head()
DD= df['domain'].value_counts().head(30).index.tolist() $ df['domain_d'] = [type_ if type_ in DD $                       else "OTHER" for type_ in X['domain']] $ print(df['domain_d'].nunique())
kickstarter["state"].unique()
model.doesnt_match("paris berlin london austria".split())
pd.Series([1,2,3]).mean()
data = pd.get_dummies(data,columns=['purpose', 'addr_state']) $ data['issue_d'] = data.issue_d.astype(int) $ data = data.drop('zip_code', axis=1) $ data.head()
cust_data1.columns
vio['new_date'] = vio["date"].apply(convert) $ vio['year']     = vio["new_date"].apply(lambda x: x.year) $ vio2016 = vio[(vio["year"] == 2016)] $ vio2016
tl_2030 /= 1000 $ tl_2030_norm = tl_2030 ** (10/11) $ tl_2030_norm = tl_2030_norm.round(1) $ tl_2030_alpha = tl_2030 ** (1/3) $ tl_2030_alpha = tl_2030_alpha / tl_2030_alpha.max().max()
item = Item(gis, item_id) $ item
enc_weights = to_np(weights['0.encoder.weight']) $ row_m = enc_weights.mean(0)
data.head()
type(df), len(df), df.shape
perf_train = pd.get_dummies(perf_train) $ perf_test = pd.get_dummies(perf_test) $ print('Training Features shape: ', perf_train.shape) $ print('Testing Features shape: ', perf_test.shape)
pd.Series(pd.DatetimeIndex(pivoted.T[labels==1].index).strftime('%a')).value_counts().plot(kind='bar');
twitter_archive[twitter_archive['name']=='None']['name']
df_by_donor.columns
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first() $ print(last_date)
sumAll = df['MeanFlow_cfs'].describe(percentiles=[0.1,0.25,0.75,0.9]) $ sumAll
archive[np.isnan(archive['in_reply_to_status_id']) != True].sample(5)
%timeit np.fromiter((xi + yi for xi, yi in zip(x, y)), dtype=x.dtype, count=len(x))
model.doesnt_match("france england germany berlin".split()) $
from sklearn.decomposition import PCA $ pca = PCA() $ pca.fit(data2Scaled[list]) $ pcaData = pca.transform(data2Scaled[list])
df_reg=injuries_hour[['date_time','Rain','injuries','wet','low_vis']] $ df_reg['hour']=pd.to_datetime(df_reg.date_time).dt.hour $ df_reg.head() $
store_items.fillna(method='backfill', axis=0) $
model = get_model() $ batch_size = 256 $ epochs = 3 $ X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233) $ RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)
df[df['user_id'] == 773192]
gs = df['Global_Sales'] * 1000000 $ zz = zscore(gs) $ zz[zz>3]
autos["price"].value_counts().head(10)
list(filter(lambda x:"commit" in x, jdf.columns))
cvec_synos = CountVectorizer(stop_words='english', max_features=50) $ vectorizers_synos = cvec_synos.fit_transform(df_more['Synopsis']).toarray() $ df_vec_synos  = pd.DataFrame(vectorizers_synos, columns=cvec_synos.get_feature_names()) $ print (df_vec_synos.shape) $ df_vec_synos.head()
sns.pairplot(intervention_train, hue='target')
import tweepy
CryptoComm = pd.read_csv('../data/cryptocurrency_Top_020118.csv', encoding='utf-8') $ nlp = spacy.load('en_core_web_sm') $ CryptoComm['Parsed'] = CryptoComm['CommentText'].apply(nlp)
grouped.get_group('MSFT')
female = crime.loc[crime['Sex']=='F'] $ female.head(3)
pd.DataFrame(tweetsDf.columns)
fin_df['reportDate'] = pd.to_datetime(fin_df['reportDate'])
top_songs['Track Name'].dtype
blob.sentences[0].polarity
df2.query("landing_page == 'new_page'").shape[0]/df2.shape[0]
table = df[['retweet_count', 'favorite_count']].groupby(by=df.source).agg(np.median) $ table.sort_values(by = 'retweet_count')
diff_between_group = mean_per_group.treatment - mean_per_group.control $ diff_between_group
df.user_id.nunique()
filtered_content_words = [] $ for word in tqdm(content_words): $     if content_counts[word] > 2: $         if not word in filtered_content_words: $             filtered_content_words.append(word)
client.experiments.get_status(experiment_run_uid)
f_user = os.path.join(data_dir, 'following_users.csv') $ f_term = os.path.join(data_dir, 'tracking_terms.csv') $ f_meta = os.path.join(data_dir, 'collection_meta.csv')
pd.unique(tweetsDF.time_zone)
def power_cohort(row): $     if row['atleast_one_course_completed_or_not']=='Completed atleast one course (Course started after 1st July 2018)': $         row['cohort']='Power User' $     return row['cohort'] $ df_users_6['cohort']=df_users_6.apply(power_cohort,axis=1)
jobs.loc[(jobs.FAIRSHARE == 24) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
print(result.f_test('SP500=0, Intercept=0'))
uniq_sorted_churned_plans_counts = sorted(uniq_churned_plans_counts,key=lambda x:x[0].tolist())
data = data.join(s, how='outer'); data  #Assignment to store the modified frame.
Pold=df2['converted'].mean() $ Pold
tweets_kyoto = pd.concat([df_user['screen_name'], df_user['id_user_str'],tweet_df, df_coor, df_place],axis=1)
annual_precip = (session $                 .query(Measurement.date, Measurement.prcp) $                 .filter(and_(Measurement.date <= '2017-12-31', Measurement.date >= '2017-01-01')) $                 .all()) $ annual_precip
Brooklyn_gdf = newYork_gdf[newYork_gdf.boro_name == 'Brooklyn'] $ Brooklyn_gdf.crs = {'init': 'epsg:4326'}
law_adds_file = "LAW-adds.txt" $ law_deletes_file = "LAW-deletes.txt"
df.drop(df.query("group =='treatment' and landing_page == 'old_page'").index, inplace=True) $ df.drop(df.query("group =='control' and landing_page == 'new_page'").index, inplace=True) $
lm=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA','UK','new_CA','new_UK']]) $ results=lm.fit() $ results.summary()
conditions_lower = conditions_m.str.lower() $ conditions_lower
contractor.tail()
try: $     binded_api() $ except Exception as e: $     print(e)
y_predicted =
df.sort_values('prob_off').head()
twitter_archive_master = twitter_archive_master.drop(twitter_archive_master[twitter_archive_master['rating_numerator'] >= 15].index)
pt_unit=pd.DataFrame.pivot_table(df_unit,index=['course_name','unit_name'],values=['uid'],aggfunc='count',fill_value=0) $ pt_unit_top=pt_unit.sort_values(by='uid',ascending=0)
cfs=cfs.sort_samples('Subject')
subred_num_tot.head(10)
excelDF.info()
top_20_breed_stats['rating_numerator'].sort_values(ascending=False).plot(kind='bar', ylim=[10,12])
autos.columns
cc['logmarket'] = np.log1p(cc['market']) $ plt.hist(cc['logmarket']) $ plt.show()
np.mean(df.num_comments)
with tb.open_file(filename='data/my_pytables_file.h5', mode='a') as f: $     earray = f.root.my_earray $     earray.append(sequence=matrix[1001:5000, :])
joined=join_df(all_df, oil_df, 'date')
rng = pd.date_range('3/6/2012 00:00', periods=15, freq='D')
(autos["date_crawled"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         ) $
np.exp(results_2.params)
df.groupby("pickup_dow")["cancelled"].mean()
to_be_predicted_Day1 = 42.83 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
logit.fit(X_mat, Y_mat) $ logit.score(X_mat, Y_mat)
f_device_hour_clicks = spark.read.csv(os.path.join(mungepath, "f_device_hour_clicks"), header=True) $ print('Found %d observations.' %f_device_hour_clicks.count())
train_df = replace(combats) $ print(train_df.head(5))
df = pd.read_csv('train.csv') $ breedMap = pd.read_csv('breed_size.csv') $ breedMap['Size'] = breedMap.Size.apply(lambda x: x.strip())
folder_names = [f"btime{i}" for i in range(1,len(h0)+1)] 
acs_df_2 = acs_df[['pop', 'age', 'pct_male', 'pct_white', 'homeval']] $ X_2 = acs_df_2.drop('homeval', axis=1).values $ y_2 = acs_df_2['homeval'].values
len(slFileList) == len(slDf)
odometer_counts = autos['odometer_km'].value_counts() $ print(odometer_counts.sort_index(ascending=True))
emb_szs
oppose_NNN.amount.sum()
from sqlalchemy import desc
test_df.head(5)
with tf.name_scope("eval"): $     correct = tf.nn.in_top_k(logits, y, 1) $     accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))
vow['Day'] = vow.index.day $ vow['month'] = vow.index.month $ vow['year'] = vow.index.year
brics = pd.DataFrame(dict_data) $ brics
flight_pd.to_csv('/home/ubuntu/parquet/flight_pd.csv', sep='\t')
import numpy as np $ import pandas as pd $ import seaborn as sns $ titanic = sns.load_dataset('titanic') $ titanic.head()
df_sched = pd.read_csv(schedFile, usecols = schedCols, $                  dtype = sched_dtypes)
data_for_model.groupby('country').describe().T
counting_dict
%matplotlib inline $ import seaborn; seaborn.set() $ import matplotlib.pyplot as plt $ data.plot() $ plt.ylabel("Hourly Bicycle Count");
import os $ os.getcwd()
clicking_conditions = pd.merge(left = df_click, right = users_conditions, left_on = 'user_session', right_on = 'user_id') $ clicking_conditions.drop(['user_session', 'experiment_id'], axis = 1, inplace = True) # to-do: can also remove action_label, action_type
dfMonth = dfDay.copy(deep=True)
k = 3 $ neigh = KNeighborsClassifier(n_neighbors=k).fit(X_train,y_train)
import sys, os $ insertIntoPath(os.path.join(os.getcwd(), '..', '..', 'playground')) $ insertIntoPath(os.path.join(os.getcwd(), '..', 'price'))
class First_scrapyItem(scrapy.Item): $     name = scrapy.Field() #attribute "name" to be extracted $     address = scrapy.Field() #attribute "address" to be extracted $
infered_relevant_info.groupBy(infered_relevant_info.genderize_gender).agg(F.count("*")).show()
farebox.head()
airports = pd.read_csv('flights/airports.csv') $ airports.head(3)
df['20180101':'20180103']
p_diff_obs=p_treatment-p_control $ p_diff_obs
tags = sentiments_df['User'].unique() $ print(tags) $ tweet_count = sentiments_df.groupby('User').Text.agg(['count']) $ print(tweet_count) $ print(type(tweet_count))
table_store = data_from_store.ix[pns == pn].reset_index(drop=True)
from bs4 import BeautifulSoup $ example1 = BeautifulSoup(train["comment_text"][0], "html.parser")
sql ="SELECT * FROM paudm.production" $ prod_paudm = pd.read_sql(sql,engine)
new_page_converted = np.random.binomial(1,P_new,n_new) $ new_page_converted
segmented_rfm['r_quartile'] = segmented_rfm['Recency'].apply(RScore, args=('Recency',quantiles,)) $ segmented_rfm['f_quartile'] = segmented_rfm['Frequency'].apply(FMScore, args=('Frequency',quantiles,)) $ segmented_rfm['m_quartile'] = segmented_rfm['Monetary_value'].apply(FMScore, args=('Monetary_value',quantiles,)) $ segmented_rfm.head()
autos.price.value_counts(dropna = False).head()
data.iloc[:,[22, 32]].sample(5)
repos.info() $ repos.head()
qrtSurge = ((qrt.shift(-3)- qrt) / qrt ) $ surge = qrtSurge[qrtSurge>1] $ surge.sort_values(ascending=False) $
com_grp.size()  # return panda Series object
df=df[df.Trip_duration >0]
df.groupby("pickup_dow")["booked_price"].mean()
y_label_test_OneHot.shape
def var_summary(x): $     return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),  x.std(), x.var(), x.min(), x.quantile(0.01), x.quantile(0.05),x.quantile(0.10),x.quantile(0.25),x.quantile(0.50),x.quantile(0.75), x.quantile(0.90),x.quantile(0.95), x.quantile(0.99),x.max()], $                   index=['N', 'NMISS', 'SUM', 'MEAN','MEDIAN', 'STD', 'VAR', 'MIN', 'P1' , 'P5' ,'P10' ,'P25' ,'P50' ,'P75' ,'P90' ,'P95' ,'P99' ,'MAX']) $ df.apply(lambda x: var_summary(x)).T
index = pd.date_range('2018-01-01 00:00:00', periods=4, freq='D') $ index
obj.drop(['d', 'c'])
print(autos['nrOfPictures'].sum())
code_n_name = session.query(Stock.stock_code, Stock.company).all()
test_pl.describe()
df_data.VITIMAFATAL.value_counts()
limited_contingency =  np.array(pd.crosstab(index=intervention_train_extract['target'], columns=intervention_train_extract['ORIGINE_INCIDENT']))
test= test.reset_index(drop = True) $ test['covered/total'] = pd.Series(predictions) $ datatest = pd.concat([train, test]) $ datatest = datatest.reset_index(drop = True)
import statsmodels.api as sm $ convert_old = sum(df2[df2['group'] == 'control']['converted']) $ convert_new = sum(df2[df2['group'] == 'treatment']['converted']) $ n_old = df2[(df2['group'] == 'control')].shape[0] $ n_new = df2[(df2['group'] == 'treatment')].shape[0]
df=pd.read_csv('ab_data.csv')
ga = pd.read_sql_query('select * from "events_gas"',con=engine)
df2['intercept'] = 1 $ df2[['ab_page_to_drop', 'ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop(['ab_page_to_drop'], axis=1) $ df2.head()
grouped = events.groupBy("user_id").agg( count("user_id") ) $ grouped = grouped.withColumnRenamed("count(user_id)", "count") $ grouped.schema
image_predictions_df.head()
df[(df.salary >= 30000) & (df.year == 2017)]
display_code('models.py',[234,238])
full_data.to_feather("../../../data/talking/nn_small_sample_full_data.feather")
wrd = pd.read_csv('twitter-archive-enhanced.csv') $ wrd.head()
destinationZip = destinationZip[['Unnamed: 0', 'ZIPCODE']] $ destinationZip.rename(columns={'ZIPCODE':'zip_dest'}, inplace=True) $ destinationZip.head()
from test_package.print_hello_function_container import print_hello_function as print_hello $ print_hello() $ from test_package.print_hello_function_container import print_hello_function $ print_hello_function() $
games_2017 = nba_df.loc[(nba_df.index.year == 2017), ] $ games_2017_sorted = games_2017.sort_values(by = ["Home.Attendance"], ascending = False)
n=13 $ df['Close'].pct_change(n).rolling(21).mean()
len(results)
fileList = list(filter(lambda x: re.match(".*json$", x), os.listdir()))
df1 = pd.read_csv("C:/Users/cvalentino/Desktop/UB/Project/data/tweets_publics_ext_all.csv", encoding='ANSI', $                  index_col='tweet_id', $                  sep=',') $
bigdf.to_csv('Combined_Comments-Fixed.csv')
m3 = np.dot(np.transpose(m2), m)  #taking the dot product of the A and B $ print("m3: ", m3)
s3 = pd.Series(np.arange(12, 14), index=[1, 2]) $ pd.DataFrame({'c1': s1, 'c2': s2, 'c3': s3})
df_mes['tpep_pickup_hour'] = pd.DatetimeIndex(df_mes['tpep_pickup_datetime']).hour $ df_mes['tpep_dropoff_hour'] = pd.DatetimeIndex(df_mes['tpep_dropoff_datetime']).hour
model = RandomForestClassifier(max_depth = 40, n_estimators = 50, min_samples_leaf = 2, random_state = 10)
from sklearn.tree import DecisionTreeClassifier $ DT_model = DecisionTreeClassifier(criterion="entropy", max_depth = 4) $ DT_model.fit(X_train,y_train) $ DT_model
df1_clean['test'].value_counts()
df_test_index = pd.DataFrame() $
plt.hist(p_diffs) $ plt.axvline(obs_diff, color='r')
final_df = final_df.join(n_features)
pd.options.display.max_colwidth = 300 $ data_df[['ticket_id','type','desc']].head(10)
skills = get_skills_dict(df)
noise_graph= noise_df.groupby(noise_df.index.month).count().plot(y='Unique Key', legend=False) $ noise_graph.set_xticks([1,2,3,4,5,6,7,8,9,10,11, 12]) $ noise_graph.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']) $ noise_graph.set_ylabel("Noise Complaints")
type(df_pop_ceb)
search['trip_end_weekday_m'] = search.apply(trip_end_weekday_m, axis=1)
V0 = 17.6639 $ r = 0.01
df_new['US_x_ab_page'] = df_new['US'] * df_new['ab_page'] $ df_new['UK_x_ab_page'] = df_new['UK'] * df_new['ab_page'] $ df_new.head() $
pumashplc = pumashp.merge(linkpp,on='puma',how='outer').fillna(0) $ pumashplc['linkNYCp100p'] = pumashplc['date_link_']/pumashplc['Pop']*100
cFrame['Cumulative'] = cFrame.groupby('Client')['Change'].apply(lambda x: x.cumsum()) $ cFrame.head(20)
print('The proportion of users converted is {}%'.format(round(df.converted.mean()*100,2)))
rate.dtypes # Making sure I'm not crazy
clf2 = Pipeline([ $     ('vectorizer', TfidfVectorizer(token_pattern=r'\w{2,}', ngram_range=(1,2), stop_words='english', sublinear_tf=True)), $     ('classifier', RandomForestClassifier()), $ ]) $ rf_model = train(clf2, model_data['clean_description'], model_data['clean_titles_cat']) $
df = tables[0] $ df.columns = ['Description', 'Values'] $ df
df['edition_type'].value_counts()
url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest' $
df1 = pd.DataFrame(np.random.randn(6,3),columns=["Col1","Col2","Col3"]) $ df1
predictions = [] $ for array in np.array_split(test_set[0], 100): $     result = linear_predictor.predict(array) $     predictions += [r['predicted_label'] for r in result['predictions']] $ predictions = np.array(predictions)
df2['date'] = pd.to_datetime(df2['timestamp']) $ df2['date'] = pd.DatetimeIndex(df2.date).normalize()
dfs.sort_values(["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"], inplace=True, ascending=False) $ dfs.drop_duplicates(subset=["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"], inplace=True)
def clear_all(): $
makeImage(getFrequencyDictForText(text))
ethPrice = pd.read_csv("btc.csv", sep=',',parse_dates=['date']) $ ethPrice.dtypes
print forcast_out
print('Convertion is ' + str(round(np.exp(us_converted.params['US']), 3)) + ' times as likely when comparing US users to non-US users, holding all else constant') $ print('Convertion is ' + str(round(np.exp(uk_converted.params['UK']), 3)) + ' times as likely when comparing UK users to non-UK users, holding all else constant') $ print('Convertion is ' + str(round(np.exp(ca_converted.params['CA']), 3)) + ' times as likely when comparing CA users to non-CA users, holding all else constant') $ print('Given the p-values of 0.898, 0.340, and 0.103 it does not appear any country is converting siginificantly. \ $ There does not appear to be any impact on conversion, when reviewing countries.') $
df.to_csv('ab_cleaned.csv', index=False)
precip_data_df1.tail(5)
index_replies = pd.Series(clean_archive[np.isnan(clean_archive['in_reply_to_status_id']) != True].index) $ index_retweets = pd.Series(clean_archive[np.isnan(clean_archive['retweeted_status_id']) != True].index) $ merged_indexes = pd.concat([index_replies,index_retweets]) $ clean_archive.drop(merged_indexes, inplace=True) $ clean_archive.reset_index(drop=True, inplace=True)
X_extra = Train_extra.drop('Approved', axis=1) $ y_extra = Train_extra['Approved']
soup.find('p', attrs={'class' : 'article-location-publishdate'})
rounds_df.loc[0]
gridCV.fit(X_train, y_train)
list(df_proj)
md2 = TextData.from_splits(PATH, splits, bs)
set(test[test['date_time'].isnull()].index)
hi_conf_no_gain = x.loc[(x['pred'] + x['pred_std']) < 0.5] $ hi_conf_no_gain
cityID = '8173485c72e78ca5' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Atlanta.append(tweet) 
observed_diff = df2.loc[treatment]['converted'].mean() - df2.loc[~treatment]['converted'].mean() $ np.mean(p_diffs > observed_diff)
altitude_only.altitude = altitude_only.altitude.apply(pd.to_numeric, args=('coerce',)) $ altitude_only.collection_started = pd.to_datetime(altitude_only.collection_started)
multi = pd.DataFrame(tweets['dril']) $ multi = multi.append(pd.DataFrame(tweets['LaziestCanine'])) $ multi = multi.append(pd.DataFrame(tweets['ch000ch'])) $ print(multi.shape)
print(ndvi_nc) $ for v in ndvi_nc.variables: $     print(ndvi_nc.variables[v])
df.sample(5)
del train_data['nrOfPictures'] $ del test_data['nrOfPictures']
import pandas as pd $ import numpy as np
from sklearn.model_selection import train_test_split $ from sklearn import svm $ from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_squared_log_error
positive_list=list(support_or_not.loc[support_or_not["compound"]>=pos_threshold].text)
sc.stop()
pats_chiefs_nov8_tweets = pats_chiefs_nov8_tweets.loc[pats_chiefs_nov8_tweets['UTC_Datetime'] <= '09-10-2017']
pd.crosstab(data.rate_marriage, data.affair.astype(bool)).plot(kind='bar') $ plt.title('Marriage Rating Distribution by Affair Status') $ plt.xlabel('Marriage Rating') $ plt.ylabel('Frequency') $ plt.show()
TEXT.vocab.itos[:12]
archive_clean['rating'] = archive_clean.apply(convert_to_float, axis=1)
results = [] $ for tweet in tweepy.Cursor(api.search, q='%23trump').items(100): $     results.append(tweet)
from bmtk.analyzer import plot_potential, plot_calcium $ plot_potential(cell_vars_h5='output/cell_vars.h5') $ plot_calcium(cell_vars_h5='output/cell_vars.h5')
z=loan_principals #repaid_loans_cash $ z[z.fk_loan==loan_test].T
for el in twitter_archive_master[twitter_archive_master['rating_denominator'] > 10]['full_text']: $     print(el)
plot_df = pd.DataFrame.from_dict({'train_loss':history.history['loss'], 'val_loss':history.history['val_loss']}) $ plot_df.plot(logy=True, figsize=(10,10), fontsize=12) $ plt.xlabel('epoch', fontsize=12) $ plt.ylabel('loss', fontsize=12) $ plt.show()
act_diffs = df2.query('group == "treatment"').converted.mean() - df2.query('group == "control"').converted.mean() $ act_diffs
active_2014_06 = data_seoul.loc[data_seoul["Month"]=='2014-06-01'] $ active_2015_06 = data_seoul.loc[data_seoul["Month"]=='2015-06-01'] $ active_2016_06 = data_seoul.loc[data_seoul["Month"]=='2016-06-01'] $ active_2017_06 = data_seoul.loc[data_seoul["Month"]=='2017-06-01']
from collections import Counter $ colleges = pd.read_csv('colleges.csv', dtype={'COFIPS':str}) $ colleges = colleges[colleges['NAICS_DESC']== 'Colleges, Universities, and Professional Schools'] $ college_dict = Counter(list(colleges.COFIPS)) $ broadband['colleges'] = broadband['county_fips'].map(college_dict)
intersections["avg_traffic_flow"] = intersections[normalized_columns].apply( $     lambda s: loaded_traffic_flow_prediction_model.predict([s])[0], axis=1)
df_active_user_metrics['group_code_activations'].value_counts()
elms_all_0604.ORIG_DATE.max()
import os $ import pandas as pd
instance.assumptionbreaker()
grid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6) $ grid.map(plt.hist, 'Age', alpha=.5, bins=20) $ grid.add_legend()
df1 = pd.DataFrame(np.random.randn(10,3),columns=['col1','col2','col3']) $ df1
df_complete_a['name'].value_counts()
tweets_df.head(10)
df_merge.columns
df_t_not_n = df[(df['group'] == 'treatment') & (df['landing_page'] == 'old_page')] $ df_not_t_n = df[(df['group'] == 'control') & (df['landing_page'] == 'new_page')] $ mismatch= len(df_t_not_n) + len(df_not_t_n) $ mismatch_df = pd.concat([df_t_not_n, df_not_t_n]) $ mismatch
str(type(x[0].get('attr-mov')))
incorrect_naics_indices = df_h1b_ft_US_Y.lca_case_naics_code[map(lambda x: x!=8, $                                                                  map(lambda x: len(str(x)), $                                                                      df_h1b_ft_US_Y.lca_case_naics_code))].index
df.shape
print len(graffiti) $ print len(cbg)
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer $ analyzer = SentimentIntensityAnalyzer() $ import pandas as pd $ pd.set_option('display.max_colwidth', -1)
ip['p1_conf'].describe()
appointments = pd.read_csv('./data/AppointmentsSince2015.csv')
drace_df = df_train.merge(temp.reset_index(),how='left', left_on='manager_id', right_on='manager_id') $ new_manager_ixes = drace_df['high_frac'].isnull() $ drace_df.loc[new_manager_ixes,['high_frac','low_frac', 'medium_frac','manager_skill']] = mean_values.values
store_items.fillna(0)
from keras.models import load_model
tweet_clock_time = [entry[11:19] for entry in tweet_time if entry[17:19] == '19'] $ print(tweet_clock_time) $
logit_mod_joined_result = logit_mod_joined.fit() $ logit_mod_joined_result.summary()
!ls $PUIDATA/archives
autos["price"] = autos["price"].str.replace("$", "") $ autos["price"] = autos["price"].str.replace(",", "") $ autos["price"] = autos["price"].astype("int")
autos = autos[autos["price"].between(1,350000)] $ autos["price"].describe()
S_lumpedTopmodel.decision_obj.hc_profile.options, S_lumpedTopmodel.decision_obj.hc_profile.value
train_df['num_photos'].ix[train_df['num_photos']>=16] = 16 $ num_photos = train_df['num_photos'].value_counts() $ x = num_photos.index $ y = num_photos.values $ sns.barplot(x, y )
from pprint import pprint $ def show_author(name): $     print('\n%s' % name) $     print('Topics:') $     pprint([(topic[0], model.show_topic(topic[0])) for topic in sorted(model[name], key=lambda x:x[1])])
pd.Series([2, 4, 6])
dfTweets = searchAndReturnTweetTable("@McDonalds", 500) $ display(dfTweets)
df_clean[['rating_numerator', 'rating_denominator']].info()
engine.execute('SELECT * FROM measurements LIMIT 10').fetchall()
len(Y_TFIDF_PRED)
df * 2
df['PRCP'].plot(kind='hist', figsize=(15,8), alpha=0.5); $ plt.xlabel('Precipitation (in)') $ plt.ylabel('Number of Days')
import pandas as pd $ import numpy as np $
df.columns
df.tail()
pickle_full = "Data Files/urinal-data-28-nov_clean.p"
autos['registration_year'].value_counts()
diffs = np.array(p_diffs) $ null_vals = np.random.normal(0, diffs.std(), diffs.size) $ plt.hist(null_vals); $ plt.axvline(x = obs_diff, color = 'red');
%matplotlib inline $ commits_per_year.plot(kind='line', title='Commits per year', legend=False)
df_img_predictions_copy.head()
rows = data[["t", "close"]].itertuples() $ list(rows)[:2]
def add_to_list(word_list, dictionary): $     for each in word_list: $         dictionary.append(each)
print("Min: {}, max: {}, mu: {}, std: {}".format( $ df["result"].min(), df["result"].max(), df["result"].mean(), df["result"].std())) $ df["result"].plot.hist(bins=100) $ plt.show()
corn.size()
df1 = pd.read_csv('ab_data.csv') $ df1.head()
names = Counter(df_main.name) $ names
msftA01 = msft['2012-01'][['Adj Close']] $ msftA02 = msft['2012-02'][['Adj Close']]
X.head()
sqlClient.delete_result(jobId)
pickle_in = open('linearregression.pickle', 'rb') $ clf = pickle.load(pickle_in)
session = Session(engine)
oil_nulls=pd.DataFrame(daily_sales[pd.isnull(daily_sales.dcoilwtico)]) $ dates_with_nulls=len(oil_nulls['date'].unique()) $ all_dates=len(daily_sales['date'].unique()) $ dates_with_nulls/all_dates
consumer_key
joined=join_df(joined, df, ['date', 'store_nbr'])
from pandas import ExcelWriter $ with ExcelWriter("../../data/all_stocks.xls") as writer: $     aapl.to_excel(writer,sheet_name='AAPL') $     df.to_excel(writer,sheet_name='MSFT')
raw_data = {'first_name': ['Jason', 'Molly', np.nan, np.nan, np.nan], $         'nationality': ['USA', 'USA', 'France', 'UK', 'UK'], $         'age': [42, 52, 36, 24, 70]} $ df = pd.DataFrame(raw_data, columns = ['first_name', 'nationality', 'age']) $ df
print ab_counts.first_name.values $ print type(ab_counts.first_name.values) $
Xs = StandardScaler().fit_transform(X) $ logreg_sentiment = LogisticRegression(random_state=42) $ scores = cross_val_score(logreg_sentiment, Xs, y, cv=skf) $ print 'Cross-validated LogReg scores based on sentiment:', scores $ print 'Mean of scores:', np.mean(scores)
shows['stemmed_keywords'] = shows['keywords'].dropna().apply(split_and_stem2)
validation.analysis(observation_data, distributed_simulation)
df=df[['gender','SeniorCitizen','Partner','Dependents','tenure','PhoneService','InternetService','Contract','PaperlessBilling','PaymentMethod','MonthlyCharges','Churn']] $ df.head()
print(soup.prettify())
raw_df = pd.concat([raw_df,pd.DataFrame(columns=([3]))]) $ raw_df[3] = False
props.value_counts()
S.decision_obj.stomResist.options
dr_ID = [7.0, 10.0, 16.0] $ RNPA_ID = [3.0, 9.0, 12.0, 13.0, 14.0, 15.0, 19.0, 25.0, 27.0, 30.0] $ ther_ID = [11.0, 17.0, 18.0, 23.0, 24.0, 26.0, 28.0, 29.0]
lm_withsubID = f_add_subID(matched_data=lm_forth)
print(regexResults.group(0))
df3.head()
tl_2040 /= 1000 $ tl_2040_norm = tl_2040 ** (10/11) $ tl_2040_norm = tl_2040_norm.round(1) $ tl_2040_alpha = tl_2040 ** (1/3) $ tl_2040_alpha = tl_2040_alpha / tl_2040_alpha.max().max()
json.loads(rec)
with pdfplumber.open('../pdfs/collections.pdf') as pdf: $     pages_with_data = pdf.pages[1:] $     for page in pages_with_data: $         df_to_append = page_to_df(page) $         df = df.append(df_to_append, ignore_index=True)
apple_tweets2.shape
df[pd.isnull(df.insert_id)].head()
driller.to_sql(con=engine, name='driller', if_exists='replace', flavor='mysql',index=False)
print(pandas_list_2d.head(2))
dataset = dataset.join(frequency_order_each_costumer, ['customer_id'],rsuffix="_total") $ dataset.head(3)
data_set.tail(5)
retailDf.registerTempTable("retailPurchases") $ sqlContext.sql("SELECT * FROM retailPurchases limit 2").toPandas()
ocsvm_bow.fit(trump_bow, y = y_true_bow) $ prediction_bow = ocsvm_bow.predict(test_bow) $ prediction_bow
def calc_temps(start_date, end_date): $     return session.query(func.min(Measurement.tobs), $     func.avg(Measurement.tobs), func.max(Measurement.tobs)).\ $         filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all() $ print(calc_temps('2012-02-28', '2012-03-05'))
%time fullDf['region']=fullDf.latLong.apply(lambda x:hitDetector.getRegion(x)[0])
from sklearn.ensemble import GradientBoostingRegressor
df2 = df2.drop_duplicates()
crime['Sex'].isnull().sum()
viscov = viscov.drop(columns = ['BTC Price', 'BTC Price Change', 'BTC Volume', 'ETH Price', $                                   'ETH Price Change', 'ETH Volume']) $ viscov = viscov.drop(viscov.index[-4:]) $ viscov
merged.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount", ascending=False) $
df_hate.describe()
dataset.head(2)
_ = ok.grade('q05') $ _ = ok.backup()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\FourFiles.zip" $ zf = zipfile.ZipFile(path) $ df = pd.read_csv(zf.open('vehicles.csv')) $ df.head(5)
import pandas as pd $ df = pd.DataFrame.from_dict(js, orient='columns')
df3 = df2 # Clone dataframe in case of a mistake
model.score(X, y)
df2['user_id'].nunique()
pickle.dump(data, open( "final_data.p", "wb" ) )
norm.cdf(z_score) $ norm.ppf(1-(0.05)) $
actual_old = df2.query('converted == 1 and landing_page=="old_page"').count()[0]/n_old
aggregates = join_aggregates()
hour = vol.index.hour $ hourly_volume = vol.groupby(hour).mean()
df.loc[1, "last_name"]
twitter_archive_full[twitter_archive_full.retweet_count == 77458][ $     ['tweet_id', 'stage','retweet_count']]
sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}
lossprob = fe.bs.smallsample_loss(2560, poparr, yearly=256, repeat=500, level=0.90, inprice=1.0) $
data_issues['changelog'][0]['histories'][4]
df_combined = df_combined[df_combined['tournament'].str.contains('World')]
df_sorted = df_measurement.groupby(['station'],sort=True).count() $ df_sorted.sort_values('id',ascending=False) $
data_df.info()
merged[merged.committee_position=="OPPOSE"].sort_values("amount", ascending=False)
chinadata.index
test['answer_dt'].describe()
df.shape
df.shape[1]
df_protest.rename(columns={'issid': 'id'}, inplace=True)
plt.title('probability distribution for true positives') $ plt.hist(x2, **kwargs)
import statsmodels.api as sm $ import pandas as pd $ from patsy import dmatrices
bigrams_fd = nltk.FreqDist() $ bigram_words = [ ','.join(map(str,bg)) for bg in nltk.bigrams(wordsX) ] $ bigrams_fd.update(bigram_words) $
scaler_fit.inverse_transform(predict_actual_df).shape
import pandas as pd $ import numpy as np $ import pickle $ from tqdm import tqdm $ from collections import Counter
twitter_archive_master = twitter_archive_master.drop(['in_reply_to_status_id','in_reply_to_user_id', $                                                      'retweeted_status_id','retweeted_status_user_id', $                                                     'retweeted_status_timestamp'],axis = 1)
train_features = bag_of_words_vectorizer.transform(train_corpus)
inspector =inspect(engine) $ inspector.get_table_names()
obs_diff = df2[df2['converted']==1]['user_id'].count() / df2.shape[0] $ obs_diff
model = gensim.models.Word2Vec(sentences, workers=4)
obs_diff=exp-con $ np.array(p_diffs) $ null_valls=np.random.normal(0,np.std(p_diffs),10000) $ plt.hist(null_valls) $ plt.axvline(obs_diff,c="red")
sns.barplot(x=top_com['num_comments'], y=top_com.index) # challenge: annotate values in the plot $ plt.xlabel("mean number of comments") $ plt.axvline(df['num_comments'].mean(), color='b', linestyle='dashed', linewidth=2) $ plt.axvline(top_com['num_comments'].mean(), color='r', linestyle='dashed', linewidth=2) $ plt.title("Top 5 active subreddits by mean # of comments");
X_train['age'] = df_imputed.iloc[:,0].values
props.prop_name
if 0 == 1: $     news_title_docs_high_freq_words_df = pd.concat([news_titles_sr, high_freq_words_sr], axis=1) $     news_title_docs_high_freq_words_df.columns = ['news_title_doc', 'high_freq_words'] $     news_title_docs_high_freq_words_df.to_pickle(news_title_docs_high_freq_words_df_pkl)
historical_scores = pandas.DataFrame(fetch_historical_scores("Pompeu Fabra University"))                                                                           $ historical_scores['time'] =pandas.to_datetime(historical_scores.timestamp, format='%Y-%m-%dT%H:%M:%SZ',errors='ignore') $ historical_scores = historical_scores.set_index('time') $
restaurants[["DBA", "BORO", "HipsterBorough"]]
xml_in_sample = xml_in[xml_in['authorName'].isin(random_authors_final)]
p_diffs = [] $ for i in range(10000): $     new_page_converted = np.random.choice([0,1], size = n_new, p=[1-p_new, p_new]) $     old_page_converted = np.random.choice([0,1], size = n_old, p=[1-p_old, p_old]) $     p_diffs.append(new_page_converted.mean()-old_page_converted.mean())
idx = np.tril(tmp_cov).astype(bool) $ idx
ax = df['SPY'].plot(title="SPY Rolling Mean",label="SPY")
log_mod_w_country = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US']]) $ log_mod_country_results = log_mod_w_country.fit() $ log_mod_country_results.summary()
df2 = df2.join(df_countries.set_index('user_id'), on='user_id')
mentions_begin, user_names_begin, user_screen_names_begin, num_tweets_begin, num_retweets_begin = au.get_mentions(ao18_qual_coll)
terms = tf.get_feature_names()
tt1_ok = tt1.loc[tt1_ok_indices]
data_ms[0]
p_new = df2.converted.mean() $ print('The convert rate for new_page under the null is: {}.'.format(round(p_new, 4)))
salesfull.dropna(how='any',inplace=True)
poverty.drop([391, 392, 393, 394, 395, 396, 397], inplace=True)
opensorces_clean_url = 'https://raw.githubusercontent.com/yinleon/fake_news/master/data/sources_clean.tsv' $ df_os = pd.read_csv(opensorces_clean_url, sep='\t')
merged_daily = data.set_index('closed_at').resample('D').size() $ merged_monthly_mean = merged_daily.resample('M').mean() $ merged_monthly_mean.index = merged_monthly_mean.index.strftime('%Y/%m')
props.head(7)
requests.get(saem_women)
tweets.head()
lm = sm.Logit(df4['converted'],df4[['intercept','UK','US']]) $ results = lm.fit() $ results.summary()
ts_split_over = TimeSeriesSplit(n_splits=3).split(X_train)
yhat_lr = lr.predict(X_test)
converted = df.query('group == "control" and converted == 1')['user_id'].count() $ total = df.query('group == "control"')['user_id'].count() $ old_conv_rate = converted / total $ old_conv_rate
priors_product_purchase= priors_product.groupby(["product_id"]).size().reset_index(name ='purchase_count') $ priors_product_purchase.head()
questions.to_csv('../data/clean.csv')
df2['converted'].mean() # probability of conversion for all groups in the experiment is 0.11959708724499628 $
feral_url = 'https://scontent-atl3-1.cdninstagram.com/vp/28607fb4ca62fc1cfe265fecffd76904/5AE6B6C4/t51.2885-15/e35/26066485_169583613655603_4826747482048299008_n.jpg' $ data[0]['display_url']
df.values
tweet_full_df['rating_numerator']=tweet_full_df['rating_numerator'].astype(float)
import subprocess $ output  = subprocess.call("ls",shell=True) $ output  = subprocess.check_output("ls",shell=True) $ print(output.decode('UTF-8'))
cluster = df.ix[:,:21]
!ncdump -h fake_buoy.nc
df2.rx_requested.value_counts()
run txt2pdf.py -o"2018-06-19 2015 FLORIDA HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2015 FLORIDA HOSPITAL Sorted by discharges.txt"
df2.query("landing_page == 'old_page'")['landing_page'].count()
mysom = tfsom.SOM.load_trained_som("./minisom.save")
example_measures = np.array([4,2,1,1,1,2,3,2,1]) $ example_measures = example_measures.reshape(1, -1) $ prediction = clf.predict(example_measures) $ print(prediction)
tweet_data_copy
containers[0].find("li", {"class":"name"}).a['title']
np.zeros((3,4),dtype=int)
ts.shift(1,freq="B")
tweets = tweets_orig.copy() $ ip = ip_orig.copy() $ extended_tweets_orig = extended_tweets.copy()
Image("/Users/jamespearce/repos/dl/data/dogscats/train/dog.5959.jpg")
corr=sess.get_historical_data('index:indu','px last', start_date='2014-01-01', end_date='2015-01-01', $                               format=bp.data_frame('date', 'Security')).pct_change().corr() $ corr.ix[0:5,0:5]
avg_day_of_month15 = avg_day_of_month15.rename(columns={"day_of_year":"rides"}) $ avg_day_of_month15.head()
bitcoin_github_issues_url = blockchain_projects_github_issues_urls[0] $ bitcoin_github_issues_df = pd.read_json(get_http_json_response_contents(bitcoin_github_issues_url))
obj['a']
189820/226306, 113698/189937, 113698/226306
data.head()
zipincome.ZIPCODE[:10]
order_num = pd_data.groupby(['appCode','orderType','year','month'])['id'].count()
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)
df['review'] = df['review'].apply(preprocessor) #use the apply method and send in the preprocessor function (applys the function to each row) $
pres_df['location'][0].split(',')
sns.lmplot(x="income", y="happy", data=training, x_estimator=np.mean, order=1)
print('Predicted price of the 1st house: {:.1f}'.format(prediction_multiple[0])) $ print('Predicted RSS: {}'.format(np.sum((prediction_multiple - test_output_multiple)**2)))
out_df = pd.DataFrame(pred) $ out_df.columns = ["low", "medium", "high"] $ out_df["listing_id"] = test_df.listing_id.values
X_train['Text_Tokenized'] = X_train.Text.apply(process) $ X_train.head()
df2b = df[((df['group']=='control') & (df['landing_page']=='old_page'))]
print(lastYear)
clean_predictions.columns
unsegmented_users.groupby('course_id').user_id.count()
file_name = f'data/tmp/{ticker}-{STAMP}-gain-raw'
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ z_score, p_value
df['id'].value_counts()    # this will help us to see if there is repetition on the titles $
for row in example1_df.take(2): $     print row $     print "*" * 20
!ls crossref-by-doi/ | grep -v ^1
retweeted_columns = ['retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'] $ twitter_archive_clean = twitter_archive_clean[twitter_archive_clean['retweeted_status_id'].isnull()] $ twitter_archive_clean = twitter_archive_clean.drop(columns=retweeted_columns)
cursor.execute("INSERT INTO person VALUES (7, 'Moore', 'Andrew');") $ pd.read_sql_query("SELECT * from person;", conn, index_col="id")
db.limit(load_buf_array, 30)[:]
print(df_users['bio1'].value_counts()) $
def convert(x): $     return pd.datetime(divmod(x, 10000)[0], divmod(x, 100)[0] % 100, x % 100)
arparams = np.array([.75, -.25]) $ maparams = np.array([.65, .35])
import requests $ from bs4 import BeautifulSoup
res_o = df.groupby(['customer_id', 'order_id']).agg(f_o).reset_index() $ res_c = res_o.groupby('customer_id').agg(f_c)
sum/len(output)
Test_extra = Test.merge(test_dum_clean, right_on='ID', left_index=True)
print(parser.HHParser)
df.head()
from gensim.models import Phrases $ from gensim.models.phrases import Phraser $ from gensim.models.word2vec import LineSentence $ import unicodedata
df.iloc[4,1]
soup.find_all(attrs={'id': 'gbar'})
rural_avg_fare = rural_type_df.groupby(["city"]).mean()["fare"] $ rural_avg_fare.head() $
df.shape
predictions = loadedModelArtifact.model_instance().transform(predict_data)
import nltk
df.shape
AAPL.iloc[-5:,:]
lasso = Lasso() $ lasso.fit(X1_train, y1_train) $ predicted_lasso_y = lasso.predict(X1_test) $ print("Lasso Mean Squared error:",mean_squared_error(y1_test, predicted_lasso_y)) $ print("Lasso R2 Score:",lasso.score(X1_test, y1_test))
src_fldr = "/home/ubuntu/s3/flight_1_5/extracted" $ dst_fldr = "/home/ubuntu/s3/flight_1_5/extracted11" $ mkfldr(dst_fldr) $ for txt_file in glob.glob(os.path.join(parent_dir, 'flight_1_5_price_2017-05-1[1-4]*.txt')): $     shutil.move(txt_file, dst_fldr)
import os $ import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline
twitter_archive_df['timestamp'].value_counts(ascending=True)[:10]
new_data["more_home_away"]['filter'] = pd.to_datetime(new_data["more_home_away"]["GAME_DATE_EST"]) $ on_date_games = new_data["more_home_away"][new_data["more_home_away"]['filter'] == date_str].sort_values("GAME_ID").GAME_ID.unique().tolist()
df[['Japan', 'United States']]
destination_frame_name = 'loan_modeling_table.hex' $ loan_stats = h2o.import_file(path="../data/LoanStats3a.csv", destination_frame=destination_frame_name)
abc.get_levels(0)
reddit = reddit.drop_duplicates(subset=['Titles', 'Subreddits'], keep='last', inplace=False) #dropping those $ reddit.head() $ reddit.shape #now I see those specific rows have been dropped $
idx = pd.IntervalIndex.from_arrays(df2.Start, df2.End, closed='both') $ idx
s3['2014-08':'2014-09']
import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
g_kick_data = kick_data_state.groupby(['blurb']) $ g_filtered = g_kick_data.filter(lambda x: len(x) > 1).sort_values(['blurb','launched_at']) $ len(g_filtered)
preds, model = runXGB(X_train, y_train, X_test, num_rounds=238) $ out_df = pd.DataFrame(preds) $ out_df.columns = ["high", "low", "medium"] $ out_df["listing_id"] = test.listing_id.values $ out_df.to_csv("./submission/submission_2017-04-24_r2.csv", index=False)
age.loc[[True, False, True, False, True, False]]
exiftool -csv -createdate -modifydate ciscij10/CISCIJ10_cycle2.mp4 ciscij10/CISCIJ10_cycle3.mp4 ciscij10/CISCIJ10_cycle4.mp4 ciscij10/CISCIJ10_cycle5.mp4 ciscij10/CISCIJ10_cycle6.mp4 > ciscij10.csv
mcqueen_ng.plot(kind='barh', figsize=(20,16));
spmL[:5], spmL[-5:]
sns.boxplot(x="brand", y="price", data=above_5percent_autos, showfliers=False)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ df = pd.read_table(path, sep ='\s+') # Index in the first column "col=0" and header on the first "row" $ df.head(5) $
locations = DataSet['userLocation'].value_counts()[:10] $ print(locations)
df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']] $ model_country = sm.Logit(df_new['converted'],df_new[['intercept','CA','US']]) $ results_country = model_country.fit() $ results_country.summary()
import statsmodels.api as sm $ convert_old = df.query('group == "control" and converted == 1').shape[0] $ convert_new = df.query('group == "treatment" and converted == 1').shape[0] $ n_old = df.query('group == "control"').shape[0] $ n_new = df.query('group == "treatment"').shape[0]
master_output_variables
businessyear(Exchange_df.Date_of_Order, Exchange_df.Sales_in_CAD)
autos.loc[autos["seller"] == 'gewerblich']
candidates['election_year'] = candidates.election_date.apply(get_cycle)
qnt(100)   # it accepts just a scalar i.e a single value as an argument
np.allclose(df1 + df2 + df3 + df4, $            pd.eval('df1 + df2 + df3 + df4'))
len(mentees)
df.dtypes
cityID = '84229b03659050aa' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Virginia_Beach.append(tweet) 
cliint
and_list = df.query("group=='treatment'& landing_page=='new_page'").index
series = pd.Series(list) $ series
def select_range(df,  min_value, max_value, col = "price"): $     df2 = df.loc[(df != 0).all(axis=1), :] $     return df2[df[col].between(min_value, max_value, inclusive=False)]
pd.timedelta_range(0, periods=9, freq="2H30T")
print('before, number of nan:', df_tweets['text'].isnull().sum()) $ df_tweets['text'].fillna('', inplace=True) $ print('after, number of nan:',df_tweets['text'].isnull().sum()) $ text_data = df_tweets['text'] $ list_text = list(df_tweets['text']) $
dreamophone_url = 'https://dreamophone.com/dream/2186'
len(joined_train_df)+len(joined_test_df)==len(joined)
pd.Timestamp('2014-12-14 17:30')
autos[autos['price'] > 200000].sort_values(by='price')
autos.shape
df.shape
props = pd.read_csv("http://www.firstpythonnotebook.org/_static/committees.csv")
p_diff = p_new - p_old $ p_diff
print(tsX[0:2,:])
lsi_tf = models.LsiModel.load(os.path.join(outputs, 'model_tf.lsi')) $ corpus_lsi_tf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lsi_tf.mm')) $ lsi_tfidf = models.LsiModel.load(os.path.join(outputs, 'model_tfidf.lsi')) $ corpus_lsi_tfidf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lsi_tfidf.mm'))
tweets.columns
h1 = re.findall(r'<h1>[\w ]+</h1>', html) $ print(h1) $ h1 = re.findall(r'<h1>([\w ]+)</h1>', html) $ print(h1)
%load "solutions/sol_2_4.py"
archive_clean.name.value_counts()
from load_data import dta
df = orgs[(orgs.founded_year >= 1990) & (orgs.founded_year <= 2016)].copy()
yc_new = yc_depart.merge(destinationZip, left_on='Unnamed: 0', right_on='Unnamed: 0', how='inner') $ yc_new.shape
baseline = 1 - reddit['comm_range'].mean() $ print 'Baseline accuracy is:', round(baseline, 2)
bands.sum(axis=0) $
with open('tweet_json.txt','r') as file: $     json_data=json.load(file) $ tweet_scores=pd.DataFrame(json_data) #load the file
paths_df.show(truncate=False)
import datetime
 shots_df = scrape_group_stats(base_url, 'shots')
drop1 = ['story','alternative name','NO OF RECORDS STOLEN','interesting story','UNUSED' , 'UNUSED.1', 'Exclude', 'Unnamed: 13', '1st source link', '2nd source link', '3rd source', 'source name'] $ Lab7 = Lab7[[col for col in Lab7.columns if col not in drop1]]
ifs = pd.read_csv('journalimpactfactors2017.csv') $ ifs['Journal'] = ifs['Journal'].str.lower() $ ifs = ifs[ifs['Journal Impact Factor']!='Not Available']
bthlst = list(df_bthlst[df_bthlst["md_id"].isin(bth_dlst)]["booth_id"])
dat_hcad_1 = dat_hcad['blk_range'] $ dat_hcad_2 = dat_hcad['77002'] $ dat_hcad_zip = pd.concat([dat_hcad_1,dat_hcad_2],axis=1) $ dat_hcad_zip.shape
raw_optim_etf_weights = helper.solve_qp(xtx.values, xty.values) $ raw_optim_etf_weights_per_date = np.tile(raw_optim_etf_weights, (len(returns.columns), 1)) $ optim_etf_weights = pd.DataFrame(raw_optim_etf_weights_per_date.T, returns.index, returns.columns)
total = df2[df2['group']=='control']['user_id'].unique().shape[0] $ conv = df2[((df2['converted']==1) & (df2['group']=='control'))]['user_id'].unique().shape[0] $ prob = conv/total $ print(prob)
prob = df2['converted'].mean() $ print("The probability of an individual converting is - {}".format(prob))
print type(X)
with open(djb_save) as f: $     djb_soup = bs4.BeautifulSoup(f, 'html.parser') $ dj_content = djb_soup.body.findAll('article') $ for x in dj_content: $ 	print(x.text)
train_data.vehicleType.fillna('kleinwagen', inplace = True)
dftouse.head()
import conda
ass15 = pd.read_csv('../../datasets/san_francisco/san_francisco/assessor_office/assessor_data_2014_2015.csv') $ ass14 = pd.read_csv('../../datasets/san_francisco/san_francisco/assessor_office/assessor_data_2013_2014.csv') $ class_codes = pd.read_csv('../../datasets/san_francisco/san_francisco/assessor_office/assessor_class_use_key.csv') $ ass15.columns = map(str.lower, ass15.columns) $ class_codes.columns = map(str.lower, class_codes.columns) $
tzs = DataSet['userTimezone'].value_counts()[:10] $ print(tzs)
yc_merged_drop = yc_merged[['Unnamed: 0', 'Fare_Amt', 'tripDurationHours', 'Trip_Distance', 'Tip_Amt']]
lr_df = df2.copy() $ lr_df.head()
row = 900
from splinter import Browser $ executable_path = {"executable_path": "/usr/local/bin/chromedriver"} $ browser = Browser("chrome", **executable_path, headless=False)
eval_sklearn_model(stock.true_grow, stock.predict_grow)
y_train = np.where(y_train == 'Charged Off', 1, 0)
run txt2pdf.py -o '2018-06-22 2012 FLORIDA HOSPITAL Sorted by payments.pdf'  '2018-06-22 2012 FLORIDA HOSPITAL Sorted by payments.txt' $
plt.figure(figsize = (20,20))        # Size of the figure $ sns.heatmap(telecom3.corr())
t2.info()
week5 = week4.rename(columns={35:'35'}) $ stocks = stocks.rename(columns={'Week 4':'Week 5','28':'35'}) $ week5 = pd.merge(stocks,week5,on=['35','Tickers']) $ week5.drop_duplicates(subset='Link',inplace=True)
val_small_data.click_timeHour.unique()
airquality_pivot = airquality_pivot.reset_index() $ print(airquality_pivot.index)
df2.head(2)
data['results']
x = x.values.reshape(-1, 1) $ y = y.values.reshape(-1, 1) $ linereg = LinearRegression().fit(x, y) $ print('m: ', linereg.coef_[0][0]) $ print('c: ', linereg.intercept_[0])
df2.dtypes
joined['Monetary_score'].describe()
user.loc["realDonaldTrump": "LaraLeaTrump"]
ggplot(aes(x="errorID"), errors) + geom_bar(fill="blue", color="black")
db_names = ["index_contents", "industry"] $ for db_name in db_names: $     DataAPI.schema.show_db_info(db_name)
for i in ['vo_propdescrip','empty_prop', $                    'llpg_usage', 'Ward', 'paymeth_code']: $     print(i,(pd.get_dummies(combined_df4, columns=[i])).shape)
df2[(df2.user_id == 773192)]
import sys $ !{sys.executable} -m pip install daiquiri
df_dn.to_csv('data/DayorNight.csv', date_format="%d/%m/%Y %H:%M:%S",index=False)
df['source'].value_counts()
print("confusion matrix: \n", metrics.confusion_matrix(actual_value_second_measure, predicted_outcome_first_measure))
import seaborn as sns $ %matplotlib inline
df_ad_airings_4['ad_duration_secs'] = (df_ad_airings_4['end_time'] - df_ad_airings_4['start_time']).map(duration)
df_final = df_grouped.merge(df_schoo11, on='school_name', how='left') $ df_final.drop(['budget_x', 'size_x', 'School ID','size_y', 'budget_y'], axis = 1, inplace=True) $ df_final
pd.date_range('11-Sep-2017', '17-Sep-2017', freq='2D')
users = df['user_id'].nunique() $ users
avg_preds = all_preds.mean(axis=0)
BDAY_PAIR_qthis.pair_age.describe()
df.set_index("id", inplace=True) $ df
import statsmodels.api as sm $ convert_old = len(df2[(df2['landing_page']=='old_page')&(df2['converted']==1)]) $ convert_new = len(df2[(df2['landing_page']=='new_page')&(df2['converted']==1)]) 
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)
df2[df2.converted == 1].shape[0]/ df2.shape[0]
tickets_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/tickets.csv')['Body'].read().decode('utf-8') $ tickets = pd.read_csv(StringIO(tickets_csv_string), header=0, delimiter='|') $ tickets['meal_created_date'] = pd.to_datetime(tickets['meal_created_date']) $ tickets['meal_date'] = pd.to_datetime(tickets['meal_date'])
new_page_converted = np.random.choice([0, 1], size=n_new, p=[1-p_new, p_new])
blocksBID=blocks.merge(df, left_on='bid_id', right_on='org_id', how='inner') $ blocksBID.head()
user = user.reset_index() $ user.head(3)
nj
%%time $ stmt = text("SELECT * FROM states where last_changed>=:date_filter") $ stmt = stmt.bindparams(date_filter=datetime.now()-timedelta(days=100)) $ allquery = engine.execute(stmt) $ allqueryDF = pd.DataFrame(allquery.fetchall())
greater_proportion = len([i for i in p_diffs if i > p_diff]) / len(p_diffs) $ print(f"Actual {p_diff}\nProportion {greater_proportion}") $ print(f"proportion percentage {greater_proportion * 100}")
df_characters.head(10)
df['Source'].unique()
y_test.head()
df_mes[(df_mes['mta_tax']!=0.5)&(df_mes['mta_tax']!=0)].shape[0]
tweets_df = tweets_df[tweets_df['Text'].apply(lambda x: lr.is_tweet_english(x))] $ tweets_df
len(seq)
train.isnull().sum()
df = read_mta('171028','180113') $ df.head(2)
grouped_df.to_csv('results.csv', index=False)
autos = autos[autos['price'].between(0, 1e5)]    # Filter out the outliers
df = pd.read_csv(r'C:\Users\Hank2\Desktop\Code_Nano\Python\ABtest\AnalyzeABTestResults 2\ab_data.csv')
newdf["Hour_of_day"] = taxiData.lpep_pickup_datetime.apply(getHour)
arma_mod20 = sm.tsa.ARMA(dta_713, (2,0)).fit(disp=False) $ print(arma_mod20.params)
slFullDf.index.get_level_values(0).value_counts()
stations = session.query(Station.station).count() $ print(f"There are {stations} stations.")
twitter_archive.loc[(twitter_archive['name'].str.islower()) & (twitter_archive['text'].str.contains('named'))]
databreach_2017.to_csv('LAB3_ticker.csv')
CONSUMER_KEY = 'Tnv8Z7EKKLzPUE8Hhux2Djb6S' $ CONSUMER_SECRET = 'fJJ2EqawJaUvC8UGcghinexcAHdML9wQKMwbQlSmm03a7C0q0m' $ ACCESS_TOKEN = '995091920803151874-cQoPco9WqeE6u2u7rbEeoF5LprUEUte' $ ACCESS_SECRET = 'y9nAyWXaFMe1MBYU8x3ac35mBAHl6NaQqNyxrjLX14sh9'
obs_diff = new_page_converted.mean()-old_page_converted.mean() $ obs_diff
joined = join_df(joined, df, ['Store', 'Date'])
tweet_df = pd.read_csv('tweet_json.txt') $ tweet_df.info()
tweets_clean = tweets.copy()
new_temp_df
df = pd.read_sql('SELECT p.customer_id, p.amount FROM payment p WHERE p.amount > (SELECT AVG(p.amount) FROM payment p);', con=conn) $ df $
process = CrawlerProcess({ $     'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)' $ }) $ process.crawl(XRateSpider) $ process.start()
iplot(data.groupby(data.created_at.dt.year).size().iplot(asFigure=True, kind="bar", dimensions=(750, 500)))
tia['duration'] = duration
log_columns=['FLOW','REVENUE'] $ log_df3 = df3.copy() $ log_df3[log_columns] = log_df3[log_columns].apply(np.log10)
import statsmodels.api as sm $ convert_old = df2.loc[~treatment]['converted'].sum() $ convert_new = df2.loc[treatment]['converted'].sum() $ n_old = len(df2.loc[~new_page]) $ n_new = len(df2.loc[new_page])
df_daily = df_daily.reset_index() $ df_daily.head()
prcp_query = session.query(func.strftime(measurements.date), (measurements.prcp)).\ $ filter(measurements.date <= last_date, measurements.date>= last_date-relativedelta(months=12)).all() $ prcp_query
combined_salaries.to_csv(directory+'03_cleaned_salaries_for_app.csv',index=False)
expirations = options_frame['Expiration'].unique()[-5:] $ iv_multi = options_frame[options_frame['Expiration'].isin(expirations)] $ iv_multi_call = iv_multi[iv_multi['OptionType'] == 'call'] $ iv_pivoted = iv_multi_call[['DaysUntilExpiration', 'Strike', 'ImpliedVolatilityMid']].pivot(index='Strike', columns='DaysUntilExpiration', values='ImpliedVolatilityMid').dropna() $ iv_pivoted.plot() $
df = pd.DataFrame.from_dict(tweets)
pct = pd.read_csv('170418_3_pp_w_foursquare_prep.csv') $ pct = pct.drop('Unnamed: 0',axis=1) $ pct.head(2)
twitter_df_merged[twitter_df_merged['rating_numerator'] == 0]
ser[-1]
df2.groupby(['group'])['converted'].mean()
sns.regplot(filtered_df['number_of_reviews'],filtered_df['availability_365'],ci=95)
autos.sort_values("price_dollars",ascending=False).head(10)
iplot(data.groupby('assignee.login').size().sort_values(ascending=False)[:20].iplot(asFigure=True, dimensions=(750, 500), kind='bar'))
pd.Period('2012-1-1', freq='D')
df2[(df2['group'] == 'treatment')]['converted'].mean()
p_newpage = df2.query('landing_page == "new_page"').count()['user_id']/df2.count()['user_id'] $ print('The probability of someone landing on the New page is {:2f}'.format(p_newpage)) $
mars_df = pd.read_html(mars_facts_url) $ mars_facts_df = pd.DataFrame(mars_df[0])
avg_per_seat_price # Average price of each team's PSLs across the entire sample
dummy_colnames = [clean_string(colname) for colname in df_dummies.columns] $ df_dummies.columns = dummy_colnames $ df_dummies.head()
from nltk.stem.snowball import SnowballStemmer $ stemmer = SnowballStemmer("german")
duplicates = df2[df2.duplicated(['user_id'])] $ duplicates.head()
Ser1.iloc[]
stadium_arr = pd.read_csv('./Data/arrests.csv') $ stadium_arr.pivot_table(index='season', values='arrests', aggfunc='sum').plot(kind='bar', title='Total Arrests Per Season') $ plt.savefig('arrestsPerYear.png', bbox_inches='tight')
gbctest = pd.DataFrame(labeled_features.loc[labeled_features['datetime'] >= pd.to_datetime('2015-10-01 00:00:00')]) $ gbctest['predicted_failure'] = model.predict(test_x)
tweets['chars'].plot(kind='hist')
merged = pd.merge(prop, contribs, on="calaccess_committee_id") 
pd.get_dummies(df.A, prefix='col')
systemuseData = systemuse[systemuse['Year']<2017] $ systemuseData = systemuse[systemuse.Total < 100000]
revs = data_df.text $ stars = data_df.stars
tweets_master_df.info()
df['site_admin'] = df['site_admin'].astype('int64')
df = pd.read_sql('SELECT * FROM actor WHERE first_name ilike \'Groucho\' and last_name ilike \'Williams\'', con=conn) $ df.head()
query = 'INSERT INTO samples VALUES(?, ?, ?, ?)' $ con.executemany(query, few_recs.values[1:])
df_group_by = df_group_by.reset_index() $
df.info()
priceChange = closePrice.pct_change(periods=5) $ priceChange.nlargest(5)
print(dfd.capacity_5F_max.describe()) $ dfd.capacity_5F_max.hist()
import pickle $ bild = pickle.load(file=open("facebookposts_bild.pickle", "rb")) $ spon = pickle.load(file=open("facebookposts_spon.pickle", "rb"))
print(autos['price'].unique().shape)
id_of_tweet = 932626561966247936 $ tweet = api.get_status(id_of_tweet) $ print(tweet.text)
import fasttext $ import gensim $ from gensim.models.fasttext import FastText $ from gensim.models import KeyedVectors
weather_date = weather_data['Date'].apply(format_time)
datAll['zip'] = datAll.blk_rng.map(dat_hcad_zip.set_index('blk_range').zip)
responses = pd.read_json('/Users/thomasmulhern/Downloads/pitchesDataJson/response.json')
df_new_coins = pd.DataFrame(list_coin_idx, columns=['symbol','name','url_ref','market_cap','index_price','index_volume','index_supply']) $ df_new_coins.to_csv(coin_index_csv,index=False, header=True)
images[images.duplicated]
TrainIndex = SampleIndex[:NumTrain]
df.resample('M').count()
out = conn.addtable(table='iris_sql2', caslib='casuser', $                     **sqldmh.args.addtable) $ out
sl.loc[sl.mindate!= sl.maxdate].head()
pop['NL']
domain = 'facebook.com'
constructor.sort_values(by='price',ascending=False)
conditions_m = messy['Conditions'] $ conditions_m.value_counts()
df['hashtags'] = df.text.str.extract('(#\w*)')
events.groupBy("event_type").count().toPandas()
hits_df = pd.read_csv('ipynb_counts.csv', index_col=0, header=0, parse_dates=True) $ hits_df.reset_index(inplace=True) $ hits_df.drop_duplicates(subset='date', inplace=True) $ hits_df.set_index('date', inplace=True) $ hits_df.sort_index(ascending=True, inplace=True)
df = df[df['AgeVideo']<700]
rm_SPY.plot(label="Rolling Mean",ax=ax)
excelDF[::100]
data.isnull().sum()
import re $ import pandas as pd
print_body(response)
df = pd.read_csv('./ab_data.csv') $ df.head()
drg_selected = inputNumber_Integer_in_List('DRG number?',DRG_levels) $
df_customers['MA_3'] = pd.rolling_mean(df_customers['number of customers'],window=3) $ df_customers['MA_6'] = pd.rolling_mean(df_customers['number of customers'],window=6)
topVideo_df = pivotTab_toDF(dataset = youtube_df, value="views", \ $                             indices = "title", aggregFunct = np.sum)[:10] $ print("Top 10 Youtube Videos:\n") $ topVideo_df
df2[['country_ca', 'country_uk', 'country_us']] = pd.get_dummies(df2['country']) $ df2 = df2.drop(['country_ca'], axis=1) $ df2.head()
train_session.isnull().sum()/train_session.shape[0]
pipe.fit(train, labelsTrain)
data.head(2)
crimes.head()
converted = df.query('converted==1') $ converted.shape[0]/df.shape[0]
len(df.genre.value_counts())
df.num_comments = df.num_comments.apply(lambda x: x.replace(' comment', ''))
q = pd.read_json(jsonx2, typ='frame')
(null_value > obs_diffs).mean()
print(json_sample)
predictions = model.predict(features_validation, verbose=0) $ print('R2 score = ',r2_score(y_validation, predictions), '/ 1.0') $ print('MSE score = ',mean_squared_error(y_validation_RF, predictions), '/ 0.0')
infinity.head(10)
flood_reports = pd.read_csv(INPUT_PATH + '311_2015_flood_reports.csv', parse_dates=[1])
datAll['zip'] = datAll['zip'].str.replace(' ','')
def load_json(value): $     tasks = json.loads(value) $     assert len(tasks) == 1 $     return tasks[0]
embedding_save_path = os.path.join(pathModel, "data.npy")
import nltk $ nltk.download()  # Download text data sets, including stop words
s = pd.Series([2,-1,3,5]) $ s
len([earlyScn for earlyScn in SCN_BDAY_qthis.scn_age if earlyScn < 3])
! wc -l failed_samples.txt
with open('/Users/bellepeng/Desktop/Metis/Projects/Project_AirBNB/data/sentiments.pkl', 'rb') as file: $     sentiments = pickle.load(file)
df_train = sales_by_storeitem(df_train) $ df_test['sales'] = np.zeros(df_test.shape[0]) $ df_test = sales_by_storeitem(df_test)
cars.isnull().sum()
stmt = stmt.columns(User.name, User.id, User.fullname, User.password)
null_columns= df.columns[df.isnull().any()] $ null_columns
ibm_hr_cat_dum = spark.createDataFrame(pd_cat) $ ibm_hr_cat_dum.show(3)
df_uro[ls_other_columns] = df_uro[ls_other_columns].applymap(clean_string)
df1 = pd.read_csv('https://raw.githubusercontent.com/kjam/data-wrangling-pycon/master/data/berlin_weather_oldest.csv') $ df1.head(2)
def get_dead_apps(serverlog): $     notify_log_strings = serverlog[serverlog['logString'].str.contains('Notified dead')]['logString'] $     deadapp_notif_list = list(notify_log_strings.apply(lambda x: x[x.find('[') + 1:-1].split(', '))) $     return deadapp_notif_list
guinea_data1 = guinea_data[guinea_data.Description.isin(['Total new cases registered so far', $                                                          'New deaths registered'])] 
mynewstyle(new_style_url, profilename="newprofile_34")
churned_plans
pandas_df = young.toPandas()
tt_final.describe()
print('Retweet tweets: ' + str(len(tweets_clean.query('retweeted_status_id == retweeted_status_id')))) $ print('Total tweets: ' + str(len(tweets_clean)))
vol = pd.DataFrame(raw, columns=['Volume']) $ vol.head()
def clean_tweet(text): $
to_be_predicted_Day1 = 38.13 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 1) & (finals["reb_l"] == 1), 'type'] = 'inside_gamers'
x = [1, 2, 3, 4, 5, 6] $ print(x[2:]) $ print(x[1:3]) $ print(x[-3:]) $ print(x[::2])
trump.head()
model_x = sm.formula.ols('y ~ C(x)', data = df).fit() $ anova_x_table = sm.stats.anova_lm(model_x, typ = 1) $ anova_x_table.round(3)
import pandas as pd $ import os.path
speeches_metadata.reset_index(inplace = True)
qualConvpct.head()
nba_df.loc[28:34]
p_diff = df2[df2['group'] == 'treatment']['converted'].mean() -  df2[df2['group'] == 'control']['converted'].mean() $ p_diff
df_rt = df[df.text.str.contains('^RT')] $ df_rt.head()
driver.quit() # Always remember to close your browser!
reddit_comments_data.select('body','sentiment','subjectivity').show(10)
Base = automap_base() $ Base.prepare(engine, reflect=True) $ Base.classes.keys()
prop_users_converted = df.converted.mean() $ prop_users_converted
df.neu.shape
airbnb_od.numeric_summary() # you can access to numeric
output= "SELECT * from user where followers>150000" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['user_id','user_name','followers','following','tweet_count'])
Xtr_scale = np.hstack((Xtr_scale,Xtr[:,7:])) $ Xts_scale = np.hstack((Xts_scale,Xts[:,7:]))
df.index  # Display the index only
df_archive_clean.drop(["retweeted_status_id","retweeted_status_user_id","retweeted_status_timestamp"], axis =1, inplace = True)
lasso=Lasso(alpha=0.1)
df.info()
df['acct_type'].unique().tolist()
merge = pd.merge(left = INC, right = weather, how = 'left', left_on = 'dateShort', right_on = 'dateShort') $ merge.head() $
giss_temp.plot(figsize=LARGE_FIGSIZE)
auto_new.CarYear.unique()
df_sample = df.sample(frac=0.1, replace=True).reset_index() $ df_sample $ X_train_, X_test_, y_train_, y_test_ = sample_split(df_sample[selected_feature])
X = pd.merge(X, latestTimeByUser, on="userid") $ X = pd.merge(X, uniqueCreatedTimeByUser , on ="userid") $ X.head(5)
pipe = pc.PipelineControl(data_path='examples/simple/data/fixed_process_data.csv', $                           prediction_path='examples/simple/data/predictions.csv', $                           retraining_flag=False) $ pipe.runPipeline()
mars_weather = tweet['text'] $ print(mars_weather) $
import matplotlib.pyplot as plt $ %matplotlib inline $ plt.bar(range(len(langdata1)),langdata1) $
fb.sort_index(inplace=True)
merged = pd.merge(props, contribs, on="calaccess_committee_id") $ merged.info()
upper_band = rm_SPY + 2*rstd_SPY $ lower_band = rm_SPY - 2*rstd_SPY
dates = [pd.Timestamp('2014-03-04').date(), pd.Timestamp('2013-05-23').date(), pd.Timestamp('2014-01-15').date()] $ df_modified = df[~df["updated_at"].dt.date.isin(dates)]
print('{0:.2f}%'.format((scores[0:2.5].sum() / total) * 100))
data.head()
device = train_data.groupby(['device.browser',"device.operatingSystem","device.isMobile"]).agg({'totals.transactionRevenue': 'sum'}) $ device = device.groupby(level=[0]).apply(lambda x:100 * x / float(x.sum())) $ device #.reset_index()
fat.plot_daily_ticker(ohlcv)
lr_grid.best_params_
twitter_archive_clean.head(1)
nfl['Date'] = pd.to_datetime(nfl['Date'])
autos["registration_year"].describe()
articles
group_brands = df2.groupby('brand').agg({"price": [min, max, mean]}) $ group_brands.columns = ["_".join(x) for x in group_brands.columns.ravel()] $ group_brands
(autos["ad_created"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index().tail(10) $ )
pd.Timestamp('1st of January 2018')
na_df["four"] = "Allen" $ na_df
df['Approximate Age at Designation'] = df['membership_date'].apply(lambda x: x.year)-df['yob']
pd.to_datetime('11/12/2010', format='%d/%m/%Y')
df.info()
laurel = open('conn_laurel.txt','r') $ hardy = open('conn_hardy.txt','r') $ conn_laurel = psycopg2.connect(laurel.read()) $ conn_hardy = psycopg2.connect(hardy.read())
day_counts = (daily_hashtag.groupBy('day', 'hashtag', 'week') $                            .count() $                            .sort('count', ascending=False) $              ).cache()
sorted(df['lambda'].unique())
df['timestamp']=pd.to_datetime(df['timestamp'], format='%Y-%m-%d')
client.repository.delete(experiment_uid)
race_vars = pd.get_dummies(df['race_desc']) $ gender_vars = pd.get_dummies(df['gender_desc'])
df
train_data.vehicleType.isnull().sum()
com311['Created Date']=com311['Created Date'].dt.date $ com311['Closed Date']=com311['Closed Date'].dt.date
dtypes={'store_nbr':np.int64,'city':np.str,'state':np.str,'type':np.str,'cluster':np.int64} $ stores = pd.read_csv('stores.csv',dtype=dtypes) # opens the csv file $ print("Rows and columns:",stores.shape) $ pd.DataFrame.head(stores)
data_df = pd.concat([R_trip, R_weather], axis=1)
plt.hist(null_values) #Simulation from the null $ plt.axvline(ControlConverted-TreatmentConverted,color="red")
len(team_search_count)
pandas_candlestick_ohlc(apple.loc['2017-12-04':'2018-01-15',:], otherseries = ["20d", "upperband", "lowerband"])
pickle.dump((fraud_data_updated),open('preprocess2.p', 'wb')) $
df.iloc[99:110,3:]
order_pay_price_sum = pd_data.loc[lambda pd_data: pd_data.pay > 0,:].groupby(['appCode','orderType','year','month'])['orderPrice'].sum()
df.loc[]
LSI_model.evaluate(corpus[-100])
print 'Skew =', weeklyTotals.skew() $ print 'Kurtosis =', weeklyTotals.kurt()
dfData.count()
html = load_file('moby_22_2.html')
import statsmodels.api as sm $ log_reg = sm.Logit(df['converted'], df[['intercept', 'treatment']])
tazs = gpd.read_file('vta_method/vta_method.shp') $ tazs = tazs.to_crs({'init': 'epsg:4326'}) $ tazs = tazs.loc[tazs['geometry'].geom_type == 'Polygon',] $ tazs = tazs[['VTA_TAZ','geometry']] $ tazs = tazs.set_geometry('geometry')
for team in sorted(elo_dict, key=elo_dict.get)[::-1]: $     print(team, elo_dict[team])
answer = 42 $ print(answer)
results = model.fit() $ results.summary()
california = california.query("FIRE_SIZE > 321")
import pandas as pd $ reviews=pd.read_csv("ign.csv") $ reviews.head()
import itertools $ nums = [1, 2, 3, 4] $ for perm in itertools.permutations(nums): $     print(perm)
sns.distplot(dfz.favorite_count.apply(np.log), color = 'red', label = 'Favorites') $ sns.distplot(dfz.retweet_count.apply(np.log), color = 'blue', label = 'Retweets')
im_clean.to_csv("image_predictions_master.tsv", sep='\t')
%sql SELECT twitter.tag_text FROM twitter \ $ WHERE twitter.key_word LIKE 'Roger Federer%'
clean_users.head()
individuals_metadata_df.to_csv("output/individuals_{}.tsv".format(date), index=False, sep="\t", na_rep="NA")
sum(df['converted'] == 1)/len(df['user_id'])
add_dummies_to_list(test_clean["SexuponOutcome"], list_of_test_features) $ add_dummies_to_list(test_clean["AnimalType"], list_of_test_features)
df_new['CA_new_page'] = df_new.CA * df_new.new_page $ df_new['UK_new_page'] = df_new.UK * df_new.new_page $ df_new.head()
tips.ndim $ len(tips) $ tips.shape
df1.Protocol.unique()
BLINK.plot_duration(blink)
from sklearn.preprocessing import Imputer
print(pandas_list_2d.iloc[[0]])
train.teacher_prefix[train.teacher_prefix.isnull()] = 'Teacher' $ try: $     test.teacher_prefix[test.teacher_prefix.isnull()] = 'Teacher' $ except: $     pass
top_k_features_not_churned
columns = inspector.get_columns('station') $ for c in columns: $     print(c['name'],c['type'])
hs
url = "https://twitter.com/marswxreport?lang=en" $ response = requests.get(url) $ soup = BeautifulSoup(response.text, 'html.parser') $ result = soup.find('div', class_="js-tweet-text-container") $ print(result)
s.ix[2:5].mean()
pd.merge(df6, df7, how='inner')
US_coeff = 0.0408 $ UK_coeff = 0.0506 $ np.exp(US_coeff), np.exp(UK_coeff)
new_page_converted.mean()
avg_data = tickerdata.groupby(pd.TimeGrouper(freq='12M')).mean()
summer[['Mean TemperatureC', 'Precipitationmm']].plot(grid=True, figsize=(10,5))
a
transactions[transactions.msno == '+++hVY1rZox/33YtvDgmKA2Frg/2qhkz12B9ylCvh8o=']
inter_day0 = pd.read_pickle(folderint + 'interactions-2016-11-06.pkl')
col = list(X_trainfinal.columns) $ col[2]= '1hr' $ X_trainfinal.columns = col $
returned_orders_data = combined_df.loc[combined_df['Returned'] == 'Yes'] $ print(returned_orders_data)
row_concat = pd.concat([uber1, uber2, uber3]) $ print(row_concat.shape) $ print(row_concat.head()) $ print(row_concat.tail()) $
df_new = df_new.join(pd.get_dummies(df_new.country)) $ df_new.head()
newcols = np.array(list(set(df.columns).union(zipcodesdetail.columns)))
ax = pre_analyzeable['prior_number_virtual_labs'].value_counts().plot(kind='bar') $ for p in ax.patches: $     ax.annotate(str(int(p.get_height())), (p.get_x() * 1.005, p.get_height() * 1.005), fontsize=25)
c.loc["c":"e"]
(elms_all.shape, elms_all_0611.shape)
url='https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response = requests.get(url) $ response
PDSQ = pd.read_table("pdsq01.txt", skiprows = [1], na_values= "999")
import datetime $ import pandas as pd $ date_now = pd.to_datetime('2018-07-27') $ multiple_party_votes_all['days_ago'] = [(date_now - date).days for date in multiple_party_votes_all['date']]
[(type(nd), nd.shape) for nd in read_in["ndarrays"]]
min_IMDB = scores.IMDB.min()
import pandas as pd $ import numpy as np
m3.clip = 25.
print('ensemble model performace is: {}'.format(np.around(f1_score(actual1, pred1, average='weighted'), decimals=5))) $
os.chdir('code')
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
offseason17["InorOff"] = "Offseason"
games_df_v2 = games_df.drop(['attendance','park_id','attend_mean'], axis=1)
autos['odometer_km'].value_counts()
df_byzone.shape
by_area = df.groupby('Reporting Area') $ by_area.describe()
with open('tweet_json.txt', 'r') as json_file: $     lines = [line.rstrip('\n') for line in json_file]
type(val)
train['diff_time_comment_registration'] = time_difference_feature(train) $ test['diff_time_comment_registration'] = time_difference_feature(test)
len(ibm_train.columns), len(feature_col)
df.Close.pct_change().median()
plt.show()
ob_diffs = df2.query('group == "treatment"').converted.mean() - df2.query('group == "control"').converted.mean()
stoi["the"], stoi["awesome"]
extract_deduped.APPLICATION_DATE_short.max()
store_items.fillna(method = 'ffill', axis = 0)
results_measurement = session.query(Measurements.station,Measurements.date,Measurements.prcp, Measurements.tobs).all() $ results_measurement
from sklearn.model_selection import cross_val_score, cross_val_predict $ from sklearn import metrics
adopted_cats.loc[adopted_cats['Color']=='Gray/Tortie','Color'] = 'Gray Tortie'
my_df.info()
print(df_aggregate.index.is_unique)
df['basic_search_visibility'] = df['Regional Monthly Search Volume']/df['Rank']
for tweet in public_tweets: $     print(tweet.text) $     analysis = tb(tweet.text) $     print(analysis.sentiment.polarity) $
n_new = (df2.landing_page == "new_page").sum() $ n_new
df = pd.read_pickle('C:/Users/Stacey/Downloads/NY_complaint_data_for_model.pkl')
df3[['other','ab_page']]= pd.get_dummies(df3['group'])
ser7
df_characters.head(10)
print('       Sponsored Jobs: ' + str(len(raw[raw.job_type == 'Sponsored']))) $ print('Unique Sponsored Jobs: ' + str(raw[raw.job_type == 'Sponsored'].hash.nunique()) + '\n') $ print('         Organic Jobs: ' + str(len(raw[raw.job_type == 'Organic']))) $ print('  Unique Organic Jobs: ' + str(raw[raw.job_type == 'Organic'].hash.nunique()))
bufferdf.Fare_amount.mean().ix[[2,3,4]]
crimes.columns = crimes.columns.str.replace(' ', '_') $ crimes.columns
df.head()
keydf = pd.DataFrame(pd.Series([i for i in keyli if not bool(re.search("[^0-9]", str(i)))]).unique())
pd.DataFrame(data=goles_contra, columns=['goles_en_contra'], dtype=str)
a=pd.DataFrame() $ a['1']=ins['score'].value_counts(normalize=True).index $ a['2']=ins['score'].value_counts(normalize=True).values $ a
df.dtypes
complete_df[complete_df[DEFAULT_NAME_COLUMN_TOTAL].isnull()].head()
db_review
USvideos[-10:] #last 10 rows
n_booths = len(bthlst) $ n_faulty = df_bug[u'Service Location'].unique().size $ n_fit = n_booths - n_faulty $ print "Out of", n_booths, "booths", n_faulty, "had fault"
test_df = get_full_snapshot_urls_df(cam_urls[0]); test_df
stock['target_class'] = stock.target.apply(lambda x: 1 if x >= 0 else 0)
result1[['last_v_T']].sub(result1['First_v_T'], axis=0) $ result1['Guinea_deaths Mean'] = result1['last_v_T']/result1['count_v_T'] $ result1
doctype_by_day.columns
pd.Series(np.random.randn(5))
f_df = new_df[features_col]   # features $ t_df = new_df['meantempm']    # target $ mu = f_df.mean() $ sigma = f_df.std() $ n_f_df = (f_df - mu) / sigma $
hashtags = pd.Series(hashtags_series.sum()) $ mentions = pd.Series(mentions_series.sum())
sm = SMOTE(random_state=12, ratio = 'auto') $ x_res, y_res = sm.fit_sample(X_tfidf, y_tfidf)
1/np.exp(.0506), 1/np.exp(.0408)
m = pd.Period('2017-12', freq = 'M') # This creates a monthly time period $ m # You might be able to leave off the freq argument
g1800s.columns
drop_table(cur_a, 'hotel')
twitter_mean.head()
df.isnull().sum()
autos['unrepaired_damage'].unique()
pd.DataFrame(authors_test)[0].value_counts()
df.loc[:'s2']
notus.loc[notus['country'].isin(mexico), 'country'] = 'Mexico' $ notus.loc[notus['cityOrState'].isin(mexico), 'country'] = 'Mexico' $ notus.loc[notus['country'] == 'Mexico', 'cityOrState'].value_counts(dropna=False)
c.execute('SELECT sum(average_high) FROM weather') $ print(c.fetchall())
au.save_df(dfd, f'data/heat-pump/proc/hp_specs')
first_result.contents[2]
words = [w for w in words if w not in stopwords.words('english')] $ print(words)
n = 3652 $ phi1 = -2 $ phi2 = 2 $ y = np.zeros(n) $ y[1:n] = phi1*x[0:(n-1)] + phi2*z[0:(n-1)] + r[1:n]
pmol = PandasMol2().read_mol2('./data/40_mol2_files.mol2.gz')
df[(abs(df['Open']-df['High'])<0.5 ) & (abs(df['Close']-df['Low'])<0.5)]
encoder = LabelEncoder() $ y = encoder.fit_transform(news['CATEGORY']) $ vectorizer = CountVectorizer(max_features=300) $ x = vectorizer.fit_transform(news['TEXT']) $ x = np.concatenate((trainDataVecs,x.toarray()), axis=1) # x can't be too large...
df2[df2['converted'] == 1].shape[0] / df2.shape[0]
stocks_df
observations.to_csv('datasample.csv')
df = pd.read_csv('census.csv') $ df = df[df['SUMLEV']==50]
tweet_archive_clean.stage = tweet_archive_clean.stage.apply(lambda x:x.replace('None','')) $ tweet_archive_clean.stage = tweet_archive_clean.stage.apply(lambda x:x.replace('doggopuppo','doggo')) $ tweet_archive_clean.stage = tweet_archive_clean.stage.apply(lambda x:x.replace('doggofloofer','doggo')) $ tweet_archive_clean.stage = tweet_archive_clean.stage.apply(lambda x:x.replace('doggopupper','doggo')) $ tweet_archive_clean.drop(['doggo', 'floofer','pupper', 'puppo'],  axis=1, inplace=True)
precipitation_count = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >= "2016-08-23").count() $ print(" There are {} results being analyzed over the past 12 months".format(precipitation_count))
df_vow.sort_values('Date', inplace=True) $ df_vow.set_index('Date', inplace=True) $ df_vow.index = df_vow.index.to_datetime()
1 + 1
bnb[bnb['age']>1000].plot(kind='hist', y='age', bins=20)
image_pred = pd.read_csv('image-predictions/image-predictions.tsv',sep='\t') $ image_pred.head()
most_yards = niners_offense.sort_values(by='Yards.Gained', ascending=False)[:50]
np.random.seed(1) $ s = pd.Series(np.random.randn(5)) $ s
model.add(Dense(3)) $ model.add(Activation('softmax'))
df_train["totals.transactionRevenue"].fillna(0, inplace=True)
new_price = np.array([price[1:] for price in b_cal_q1['price']]) $ print(new_price) $ print(len(new_price))
import os $ print(os.getcwd())                   # print current working directory $ print(os.listdir(os.getcwd()))       # print contents of the current working directory $ print(os.path.split(os.getcwd())[0]) # print the parent directory. Repeat as needed to get to higher directories.
std_for_each_weekday = std_for_each_weekday[['weekday', 'std(duration)']] $ std_for_each_weekday
df.reset_index(inplace=True) $ df=df.merge(bwd, 'left', ['date', 'store_nbr'], suffixes=['', '_bw']) $ df=df.merge(fwd, 'left', ['date', 'store_nbr'], suffixes=['', '_fw'])
user_summary_df[['followers_count', 'following_count', 'tweet_count', 'original', 'quote', 'reply', 'retweet', 'tweets_in_dataset']].describe()
import datetime $ start = datetime.date(2014, 1, 1) $ weekly_data = yahoo_finance.download_quotes("GILD", start_date=start, interval=yahoo_finance.WEEKLY) $ print weekly_data
df.head(2)
df_a['click_rec_menues']
df2['low_tempf'].mean()
print(" the probability of an individual converting regardless of the page they receive = ", df2.converted.mean())
data_NL_Q1 = importdata('NL', 'Tennet_Q1') $ data_NL_Q2 = importdata('NL', 'Tennet_Q2') $ data_NL_Q3 = importdata('NL', 'Tennet_Q3') $ data_NL_Q4 = importdata('NL', 'Tennet_Q4')
popularity.info()
pn_qty[pn]['1cstoreinfo']
df_map_name = pd.DataFrame.from_dict(df_map_name)
t_indices = [] $ for _, indices in test_indices.items(): $     t_indices = t_indices + indices
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
options_frame['ModelError'].hist() $
people.index
girls_by_name.loc[['PRIYANKA', 'HARJIT', 'KEIKO', 'NATALYA'], :]
from sklearn import metrics
pd.Timestamp('17:55')
len(df_yt)
tweets.info()
sorted(entity_relations.items(), key=lambda x: x[1], reverse=True)
regular_traffic.head()
s.rank(ascending=False)
word_to_vec_map["it's"]
final_topbikes['Distance'].count()
print(result.summary2())
bed_df.to_csv('bed_df.csv',index=False)
service_path = 'https://internal-nginx-svc.ibm-private-cloud.svc.cluster.local:12443' $ ml_repository_client = MLRepositoryClient()
cbg = cbg[['AFFGEOID', 'GEOID', 'COUNTYFP', 'geometry']]
restaurants = pd.read_csv("/home/ubuntu/data/restaurant.csv", dtype=unicode, encoding="utf-8")
!rm train.zip
fmt = '%Y-%m-%d %H:%M:%S' $ dateparse = lambda dates: pd.datetime.strptime(dates, fmt) $ rawautodf = pd.read_csv('used-cars-database/autos.csv', sep=',',encoding='Latin1',parse_dates = ['dateCrawled','dateCreated','lastSeen'], date_parser = dateparse)
accuracies.mean()
reviewsDF = pd.read_csv('ABT.csv', index_col=False, encoding='UTF-8')
from sklearn.linear_model import LinearRegression $ lin_reg = LinearRegression() $ lin_reg.fit(ac_tr_prepared, ac_tr_label)
crime_data[0][3] $
run txt2pdf.py -o"2018-06-19 2014 FLORIDA HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2014 FLORIDA HOSPITAL Sorted by discharges.txt"
sns.boxplot(x=df.author, y=df.created_at.dt.hour); $ plt.xticks(np.arange(10), ('collins', 'hclinton', 'hirono', 'hoeven', 'mccain', 'obama', 'ryan', 'sanders', 'schwarzenegger', 'trump'), rotation = 50);
df4.country.unique()
lda_model
contract_history.OPTION.fillna("NO", inplace=True)
c = ptoclient.PTOClient("https://v3.pto.mami-project.eu/", token)
df_usertran = pd.merge(users,transactions,how='left',on='UserID') $ df_ = df_usertran.drop_duplicates(subset='UserID') $ df_ = df_.reset_index(drop=True) $ df_ $
v_invoice_sat.drop(v_invoice_sat_dropper, axis=1, inplace=True) $ invoice_sat.drop(invoice_sat_dropper, axis = 1, inplace=True)
building_pa_prc=pd.read_csv("buildding_00.csv")
df_predictions_clean.img_num.value_counts()
autos['unrepaired_damage'].unique()
new_page_converted=np.random.choice([0,1],size=145310, p=[mean2, 1-mean2])
model = linear_model.LinearRegression() $ print ('Linear Regression') $ linear_model =  reg_analysis(model,X_train, X_test, y_train, y_test)
rankings_USA.query("rank <= 10")['rank_date'].count() $
avg_day_of_month15.to_excel(writer, index=True, sheet_name="2015")
payments_all_yrs = payments_all_yrs.loc[:, :].sort_values(['disc_times_pay','25'], ascending=[False,True]) $ payments_all_yrs = payments_all_yrs.reset_index(drop=True) $ payments_all_yrs.tail()
Z = np.zeros((5,5), [('x',float),('y',float)]) $ print(Z) $ Z['x'], Z['y'] = np.meshgrid(np.linspace(0,1,5), $                              np.linspace(0,1,5)) $ print(Z)
results_dict = load_existing_results(subject='R1389J', experiment='catFR5', $                                      sessions=[1], stim_report=True, $                                      db_loc=report_db_location, rootdir=rhino_root).compute() $ list(results_dict.keys())
menus.to_csv('./cleaned/menus.csv', header=False, index=False)
df_2018.shape
df2[['control','ab_page']]=pd.get_dummies(df2['group']) $ df2.head()
weather.Date = pd.to_datetime(weather.Date)+MonthEnd()
index_weighted_returns = generate_weighted_returns(returns, index_weights) $ etf_weighted_returns = generate_weighted_returns(returns, etf_weights) $ helper.plot_returns(index_weighted_returns, 'Index Returns') $ helper.plot_returns(etf_weighted_returns, 'ETF Returns')
session_user = pd.crosstab(session_top_subset['user_id'], session_top_subset['action']) $ session_user.head()
tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn.npy') $ tok_val = np.load(CLAS_PATH/'tmp'/'tok_val.npy')
QUIDS2 = QUIDS.copy() $ QUIDS2["week"] = QUIDS["week"].apply(lambda x: x if np.isnan(x) else int(x)) $ import seaborn as sns $ ax = sns.boxplot(x="week", y="qstot", data=QUIDS2).set_title("Descrease in depressive symptoms over time")
bacteria2
df2['intercept'] = 1 $ lm = sm.OLS(df2['converted'], df2[['intercept', 'old_page', 'ab_page']]) $ results = lm.fit() $ results.summary()
def getNegativeTweets(tweetTable): $     negativeTweetTable = tweetTable[tweetTable['SA'] == -1] $     return negativeTweetTable $ negativeTweets = getNegativeTweets(data) $
import matplotlib.pyplot as plt $ import numpy as np $ import pandas as pd $ import seaborn as sns $ %matplotlib inline
autos.describe(include = 'all')
from bigml.api import BigML $ api = BigML()
ADP=df['NASDAQ.ADP'] $
df = df.drop(['nrOfPictures','postalCode','abtest'], axis=1)
merged_df = pd.merge(left=surveys_df, right=species_df, how='left', on='species_id')
dataAnio.head()
df.loc['20180101':'20180103',['A','C']]
df.isnull().values.any()
for letter in list('ABCDEFGH'): $     ab_groups[letter].revenue.plot();
df2.drop_duplicates()
numpy.random.seed(7) $ dataframe = read_csv('YESBANK.NS.csv', sep=",") $ dataframe.dropna(inplace=True) $ dataframe.head()
xs = [1,2,3,4,5] $ ys = [5,4,6,5,6]
jobs_data['clean_description'].head(5)
params = {'q': 'data science', $           'result_type': 'recent'} # popular also possible
reddit.head(2)
df_categories.tail()
df1617 = getSeasonStats() $ iterator = DataIterator(df1617) $ iterator.getPlayerStats('Lebron James', df1617)
df = pd.read_csv('data/test1.csv', parse_dates=['date']) $ df
tips_sentiment = tips_sentiment.rename(columns={'Positve Percentage': 'Tip Positve Percentage', $                                                 'Negative Percentage': 'Tip Negative Percentage', 'Business_Focus': 'Tip Business Focus'})
data = [{'a': 1, 'b': 2},{'a': 5, 'b': 10, 'c': 20}] $ df1 = pd.DataFrame(data, index=['first', 'second'], columns=['a', 'b']) $ df2 = pd.DataFrame(data, index=['first', 'second'], columns=['a', 'b1']) $ print(df1) $ print(df2)
df
tweet_df = pd.read_csv('../data/twitter_personality/tweets_full_parsed.csv', lineterminator='\n')
nypd_df_raw = pd.read_csv(data_folder+'/nypd_7_major_felony_incidents.csv') $ nypd_df = nypd_df_raw[['Created Date', 'Closed Date', 'Agency', 'Complaint Type', 'Descriptor', 'Location Type', \ $                        'Community Board', 'Borough', 'Latitude', 'Longitude']] $ nypd_df = nypd_df[nypd_df.Descriptor == "Blocked Sidewalk"] $ nypd_df.head()
adf_check(dfs['Stock First Difference'].dropna())
import githubid.githubid as gid
IBMspark_df = sqlContext.createDataFrame(IBMpandas_df) $ for row in IBMspark_df.take(2): $     print row
model_json = model.to_json() $ with open("model/model.json", "w") as json_file: $     json_file.write(model_json)
dsd = df.pop('anotherupper')
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
df.ix[306]['yob']
parsed_events["team_size"] = parsed_events.team_size.fillna(1).astype(int) $ parsed_events.distance = parsed_events.distance.astype(int) * parsed_events.team_size $ parsed_events.assign(original = raw.event).sample(5)
df_nullcount = df_closed.isnull().apply(np.count_nonzero) $ print df_nullcount
row_to_drop = df2[df2.user_id == duplicate_userid].index[0] $ df2 = df2.drop(row_to_drop)
df_breed = pd.read_csv('image-predictions.tsv', sep = '\t') $ df_breed.head(3)
print(df.shape) $ df.dtypes
urban_avg_fare = urban_type_df.groupby(["city"]).mean()["fare"] $ urban_avg_fare.head()
extract_all.loc[(extract_all.APP_DOB.isin(['1961/08/09','19610809','08/09/1961'])), $                 ['APP_APPLICATION_ID','APPLICATION_DATE_short','APP_PRODUCT_TYPE','APP_LOGIN_ID', $                 'APP_FIRST_NAME','APP_MIDDLE_NAME','APP_LAST_NAME','APP_SSN', $                 'APP_DOB','APP_CELL_PHONE_NUMBER','DEC_LOAN_AMOUNT1']].sort_values('DEC_LOAN_AMOUNT1', ascending=False)
df = pd.DataFrame(emails) $ df
no_null_events = events.filter("store_id is not null")
sim_prop_new_converted = new_page_converted.mean() $ sim_prop_old_converted = old_page_converted.mean() $ print(sim_prop_new_converted - sim_prop_old_converted)
order_data.head()
fullDf[fullDf.levelIndices=='CR'].level.value_counts()
c.find_one({'albums.released': {'$gt': 1980}})
conditions_m.unique()
users_converted = df['converted'].mean()*100 $ print("The proportion of users converted - {}%".format(round(users_converted,2)))
merged2 = merged2[merged2['AppointmentDuration'] <= 90]
baseball_newind.iloc[:5, 5:8]
suspects_with_25_1.apply(adding_to_map, axis=1)
df = pd.read_excel("msft.xls") $ df.head()
calls_df.describe(include=["object"])
df.plot.hist()
print xmlData.dtypes $ print '' $ print csvData.dtypes
training_data,holdout = train_test_split(liquor2015_combined,shuffle=True,test_size=0.10,random_state=123)
example_model = graphlab.linear_regression.create(train_data, target = 'price', $                                                   features = example_features, $                                                   validation_set = None)
s = pd.Series([1,2.3,np.nan,"a"]) $ s
scores_median = np.median(sorted(raw_scores)) $ print('The median is {}.'.format(scores_median))
a = df1.io_state[2][-3:] $ int(a,16)
set(sakhalin_data_in_bbox.id.values) - set(sakhalin_filtered.id.values)
authors[authors['count'] >= 10].plot(kind='scatter', x='count', y='mean', alpha=0.3)
now = dt.datetime.now() $ print("type of now:", type(now)) $ now
autos = autos.drop("number_of_photos",axis=1) $ autos.head(3)
pd.read_html('https://httpbin.org/basic-auth/myuser/mypasswd')
ab_df2.query('group == "treatment"')['converted'].mean()
df['label'] = df[forecast_col].shift(-forecast_out)
loans.shape
df_vimeo_selected.to_csv('/Users/nikhil.mogare/Desktop/DSA_Reporting/Week3_Sep19/CSVs/vimeo_selective.csv')
sum_vec = vec_lst_add_subtract(pandp_top_vec, bible_top_vec, mode='add') $ find_most_similar(sum_vec, all_top_vecs, title_lst, vec_in_corp='N')
for i in levelmean.columns[1:]: $     tokendata[i+"_byLevel"] = round(100*tokendata[i] / levelmean[i],2)
control_group = df2.query('group == "control"') $ converted_control = control_group[control_group.converted == 1] $ p_control = converted_control.count()/control_group.count() $ p_control
from sklearn.model_selection import train_test_split $ labels = df[df.year == 2017]['label'] $ test_size= 0.3 $ X_train, X_test, y_train, y_test = train_test_split(vectorized_text_labeled, labels, test_size=test_size, random_state=1)
merged_data['cs_creation_day'] = merged_data['customer_creation_date'].dt.day $ merged_data['cs_creation_month'] = merged_data['customer_creation_date'].dt.month $ merged_data['cs_creation_year'] = merged_data['customer_creation_date'].dt.year
autos['date_crawled'].str[:10].value_counts(normalize = True).sort_index()
print("Probability of control group converting:", $       df2[df2['group']=='control']['converted'].mean())
df2['intercept'] = 1 $ df2[['control', 'ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('control', axis=1) $ df2.head()
liquor=pd.read_csv('../Assets/Iowa_Liquor_sample.csv',parse_dates=['Date'],infer_datetime_format=True)
age_hist.age = pd.to_numeric(age_hist.age) $ age_hist.head() $ age_hist.info()
groupby_breed.head()
 newdf.isnull().any()
text = "Dr. Smith graduated from the University of Washington. He later started an analytics firm called Lux, which catered to enterprise customers." $ print(text)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)
print('RMSE LGBMRegressor: ', RMSLE(np.log1p(train['visitors'].values), lgbmrscv.predict(train[col])))
raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1], $                        dtype=np.float32, name="raw_weights")
calls_nocontact_simp = calls_nocontact.drop(columns=['ticket_id', 'issue_description', 'city', 'state', 'location', 'geom']) $ calls_nocontact_simp.head()
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_autocorrelation(doc_duration, params=params, lags=30, alpha=0.05, \ $     title='Weekly Doctor Hours Autocorrelation')
users.query("created_at < '1984-01-01 00:00:00'")
df_ind_site = df_providers[['id_num','drg3','discharges','disc_times_pay']].sort_values(['id_num'],\ $               ascending=[True]) $ df_ind_site.head() $ df_sites_to_eliminate = (df_providers.groupby(['id_num','name','drg3','discharges','year'])[['disc_times_pay']].sum()) $ df_sites_to_eliminate.head()
house_data = pd.read_csv("/Users/Jenny/Documents/Thinkful/random downloaded data/csvFiles/kc_house_data.csv")
import requests $ import time
pd.set_option("display.max_rows", 12)
autos["price"] = autos["price"].str.replace("$", "") $ autos["price"] = autos["price"].str.replace(",", "") $ autos["odometer"] = autos["odometer"].str.replace("km", "") $ autos["odometer"] = autos["odometer"].str.replace(",", "")
tweets_df.info()
blob = TextBlob(tweets[0]) $ blob.sentences[:10]
b[b%2 == 0] = -1 $ b
print('Change the column Indicator to Indicator_id and dispaly (first 2 records)') $ df.rename(columns = {'Indicator':'Indicator_id'}).head(2)
ps.to_timestamp('D', how='end')
knn5.fit(X_train, y_train)
tweet_json = 'tweet_json.txt' $ tt_json_df = pd.read_json(tweet_json, lines= True)
timecat_df = tcat_df[tcat_df.userTimezone.notnull()] $ timecat_df = timecat_df[['Hashtag', 'tweetCreated', 'tweetFavoriteCt', 'tweetID', 'tweetRetweetCt', 'tweetSource', $                          'userID', 'userLocation', 'userName', 'userScreen', 'userTimezone']] $ timecat_df.head() $ timecat_df.size
df2.query('group == "treatment"').shape[0]/df2.shape[0]
model = models.HdpModel(corpus, id2word=dictionary)
X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 1)
nba_df.set_index("Date", inplace = True)
mi_diccionario
p_mean = np.mean([p_new, p_old]) $ print("Probability of conversion according to null hypothesis (p_mean) is", $       p_mean)
import pysb   #allows for interaction with Sciencebase $ import getpass $ import time $ import urllib.request as ur $ import os
df_predictions.info()
taxiData.columns
df_birth[df_birth.population > 1000000000] $
iris.loc[:,"Species"].cat.categories
final['state'][final['area (sq. mi)'].isnull()].unique()
total_sales['ratio'] = (sales_diff['Sale (Dollars)_y'].values / sales_diff['Sale (Dollars)_x'].values)
df_master_select = df_master.copy() $ df_master_select = df_master_select[merge_features]
df_clean = pd.merge(left=df_enhanced,right=df_json, left_on='tweet_id', right_on='id', how = 'left') $ df_clean = df_clean.drop(['id', 'retweeted'], axis = 1) $ df_master = pd.merge(left=df_clean,right=df_breed, left_on='tweet_id', right_on='tweet_id', how = 'left')
image_predictions_df.head()
P_old = df2.converted.mean() $ print("The convert rate for p-old under the null is {}.".format(P_old))
plt.hist(km.labels_)
df[:2]
sq72= "CREATE TEMPORARY TABLE  time_22 ( SELECT * FROM NBA_tweets where TweetCreated >='2018-04-16 19:00:00' and   TweetCreated <'2018-04-16 21:00:00' )" $ sq73="SELECT word, COUNT(*) total FROM ( SELECT DISTINCT Num, SUBSTRING_INDEX(SUBSTRING_INDEX(TweetText,' ',i+1),' ',-1) word FROM time_22, ints) x where word like'%#%'GROUP BY word HAVING COUNT(*) > 1 ORDER BY total DESC, word;"
sorted(twitter_df.rating_denominator.unique())
google_stock['Adj Close'].describe()
df.query('(group=="treatment" and landing_page=="old_page") or (group=="control" and landing_page=="new_page")').shape[0]
df_nomatch = df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page'))== False]
covtype_df.head()
n_new = df2.query('landing_page == "new_page"').shape[0] $ n_new
drawPoliGraph(G, 0.1)
baby_scn_postgen['BABY_CREATED'].min()
sale_prod_sort = sale_average[sale_average.Product == 'Amarilla'] $ sale_prod_sort
actual_difference = df2[df2 [ 'group' ] == 'treatment' ][ 'converted' ].mean() - (df2[df2['group'] == "control"]['converted'].mean()) $ actual_difference
autos.head()
cpq['On Zayo Network Status'].value_counts()
df2.shape
sub_dataset[ sub_dataset['WordCount'] == 0 ]['WordCount'].count()
folder_source_of_files = '/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS'
BID_PLANS_df = query_df(BID_PLANS_URL)
y[20:30]
getArtistListenedByUser(2030067)
my=df_mas.copy()
ciscid4, ciscih8, ciccij10
df = pd.DataFrame({'foo': [1,2,3], 'bar':[0.4, -1.0, 4.5]}) $ df.values
taxi_hourly_df.loc[taxi_hourly_df["missing_dt"] == True, :].shape
import matplotlib.pyplot as plt $ %matplotlib inline $ plt.style.use("ggplot") $ deaths_XX_century.set_index('decade').plot()
n_old,n_new
extract_topic_discribution_features(ldamodel,dictionary,posts_words_vectors)
ratings=pd.read_csv('..\\Data\\ml-20m\\ml-20m\\ratings.csv')
df_signup.pivot_table('country_destination', ['signup_method', 'signup_app'], aggfunc="count")
df['days'] = pd.to_datetime(df['timestamp']) 
for para in soup.find_all('p'): $     print(para.get_text())
page_old = df2.converted.mean() $ page_old
for j in top_tracks: $     top_tracks[j] = [(i.split(",")[0] + "," + i.split(",")[1] + "," + i.split(",")[2] + "," + i.split(",")[3]) for i in top_tracks[j]] $
result.inserted_id
age.loc[['Alice', 'Bob']]
pd.DataFrame ([[101,'Alice',40000,2017], $                [102,'Bob',  24000, 2017], $                [103,'Charles',31000,2017]], index   = ['r1','r2','r3'] )
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, func, inspect, desc
f = open(path_play + "/all_demo_train","w",encoding="utf-8") $ for line in all_doc: $     f.write(line) $ f.close()
std_df = choose_local_df('STD') $ std_df.loc[std_df['Sold_to_Party']=='0000101348'][['Sold_to_Party','Sales_document','Material_Description','Unit_Price','Document_Date']]
mentions_df = mentions_df[(mentions_df["epoch"] >= 1515416400) & (mentions_df["epoch"] < 1517230800)]
print(np.min(lat), np.max(lat), np.min(lon)-360, np.max(lon)-360)
full_data.to_pickle('D:/CAPSTONE_NEW/jobs_data_full.pkl')
opt_fn = partial(optim.Adam, betas=(0.7, 0.99))
%%sql $ select * from hive.entities.repos limit 10
reddit.info()
df_questionable[df_questionable['clickbait'] == 1]['link.domain_resolved'].value_counts(25).head(25)
df1.columns = df2.columns $ df=pd.concat([df1,df2]) $ df.head()
parsed_train_sample.show(10, truncate=False)
ioDF.columns.tolist()
np.shape(temp_fine)
data[data['Borough']=='Unspecified'].groupby('Agency').count()
import statsmodels.api as sm $ convert_old = df2[df2['group'] == 'control'].converted.sum() $ convert_new = df2[df2['group'] == 'treatment'].converted.sum() $ n_old = df2[df2['group'] == 'control'].converted.size $ n_new = df2[df2['group'] == 'treatment'].converted.size
nnew=df2.query("group == 'treatment'") $ n_new=nnew.shape[0] $ n_new
text = [sample_text] + list(trunc_df.description)
products = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/products.csv') $ products.head()
df_c = pd.read_csv('countries.csv') $ df_c.head()
project_id= ... ## ADD YOURS $ general_volume_types = pd.read_gbq(query=query, project_id=project_id, dialect='standard')
log_UK = sm.Logit(new['converted'],new[['intercept','country_UK']]) $ r = log_UK.fit() $ r.summary()
leaderboard = pd.read_json('../metadata/leaderboards.json') $ leaderboard['best'] = pd.to_numeric(leaderboard['best'], errors='coerce') $ leaderboard.head()
twitter_archive_clean['dog_stages']=stage $ twitter_archive_clean = twitter_archive_clean.drop(columns=['doggo','floofer','pupper','puppo'])
print(predictions)
model.doesnt_match("man tiger woman child ".split()) $
cityID = '2409d5aabed47f79' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Rochester.append(tweet) 
common_dictionary = pd.read_csv('google-10000-english.txt', header=None) $ dictionary.columns=['Word']
merge_event.loc[(merge_event.location == 'United States')].groupby('nweek')['user_id'].nunique().plot(label="in US") $ merge_event.loc[(merge_event.location != 'United States')].groupby('nweek')['user_id'].nunique().plot(label="out US") $ plt.legend() $ plt.show()
df.describe(include=['datetime'])
list(data.dropna(thresh=int(data.shape[0] * .9), axis=1).columns) #set threshold to drop NAs
df.groupby('raw_character_text')['episode_id'].nunique().head()
word_vectors = model_ft.wv
fig, ax = plt.subplots(figsize=(15.0, 10.0)) $ actual_train_results_df = pd.DataFrame(actual_train_results) $ ax = sns.distplot(actual_train_results_df["prediction_status"]) $ ax.get_figure().text(0.90, 0.01, "AntiNex - v1", va="bottom", fontsize=8, color="#888888") $ plt.show()
ggplot(etsamples_100hz.loc[1:35000].query("eyetracker=='el'"),aes(x="pa",y="pa_diff"))+geom_point()
num_row = df.shape[0] $ print("{} rows in the dataset.".format(num_row))
add_datepart
grinter1.number_int.max()
df_A.loc['s1':'s2']
outcome['OutcomeDate'] = pd.to_datetime(outcome['DateTime'],format ='%m/%d/%Y %I:%M:%S %p' ) $ outcome['Date of Birth'] = pd.to_datetime(outcome['Date of Birth'],format ='%m/%d/%Y' ) $ outcome = outcome.drop(['DateTime','MonthYear','Name','Breed','Color'],axis=1)
cityID = '319ee7b36c9149da' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Arlington.append(tweet) 
df_columns.index.month.value_counts().sort_index().plot(kind='barh') $
conn.fileinfo('data')
df2 = df2.join(country_dummies);
import requests $ response = requests.get("http://api.open-notify.org/iss-now.json") $ print(response.status_code)
df2[['US','UK','CA']]=pd.get_dummies(df2['country']) $ df2.head()
archive_df.info()
image_predictions.p1.value_counts()
!find / -name index.html > links.txt $ !cat links.txt
dl_match[1]
props.head()
import datetime $ data['yyyymm'] = data['Created Date'].apply(lambda x:datetime.datetime. $                                            strftime(x,'%Y%m')) $ data['yyyymm']
df[::2].head()
url_template = "http://www.reddit.com/?count={}&after={}" $ max_results = 5001 # Set this to a high-value (5000) to generate more results. $ results = [] $ for start in range(25, max_results, 25): $     pass
X_words = X
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier
to_be_predicted_Day3 = 25.11485584 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
ax = users.created_at.hist(bins=144) $ ax.set_xlabel('Date') $ ax.set_ylabel('# Users') $ ax.set_title("Users' account creation per month")
sentiments_pd.to_csv("NewsMood.csv", encoding="UTF-8")
results.summary()
train_data = data1[data1['dataset'] == 'train'] $ train_data = train_data.drop('dataset', axis = 1) $ train_data.sample(3)
doctype_by_day.min().sort_values(ascending=False)[:10].index
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop(['CA'], axis=1) $ df_new.head()
festivals.head(3)
results = logm.fit() $ results.summary()
avg_longest_interval = interval_c.mean() $ res_c_i.iloc[:, -1] = res_c_i.iloc[:,-1].fillna(avg_longest_interval + res_c_i.iloc[:, -2])
extract_all.loc[(extract_all.duplicated('APP_SSN')==True) $                &(extract_all.APP_SOURCE=='DOT818') $                &(extract_all.application_month=='2018-04'), $                fields].sample(3)
df = pd.read_csv(dp('tmdb_5000_movies.csv'), encoding='utf-8') $ df
%matplotlib inline $ plt.plot(data.sepal_length, data.sepal_width, ls ='', marker='o')
rng.asobject.values[0]
query_potholes = pgh_311_data_merged['REQUEST_TYPE'] == "Potholes" $ query_bloomfield = pgh_311_data_merged['NEIGHBORHOOD'] == "Bloomfield" $ bloomfield_pothole_data = pgh_311_data_merged[query_potholes & query_bloomfield] $ print(bloomfield_pothole_data.shape) $ bloomfield_pothole_data.head()
train.grade.value_counts()
credit_new = new['sum(kredit)'][0] $ credit_return = return_['sum(kredit)'][1] $ revenue_new = new['sum(price)'][0] $ revenue_return = return_['sum(price)'][1] $
throwaway_df = sqlContext.createDataFrame([('Anthony', 10), ('Julia', 20), ('Fred', 5)], ('name', 'count')) $
wildfires_df['FIRE_YEAR'].unique()
movies['Position'].head()
sample_dic.keys()
pf_data.tail()
r.json()
df_date[michaelkorsseries].groupby("date").sum().sort_values(by="postcount",ascending=False)['postcount'][:10]
user = pd.read_csv("users.csv") $ user.head(3)
import numpy as np $ Z = np.random.random(30) $ print(Z) $ print(Z.mean())
pickle.dump(final_df,open('../proxy/dataset1','wb'))
df.head()
logit_mod = sm.Logit(df3['converted'], df3[['ab_page', 'UK', 'US', 'intercept']]) $ df3_results = logit_mod.fit()
os.chdir(root_dir + "data/") $ df_fda_drugs_reported = pd.read_csv("filtered_fda_drug_reports.csv", header=0)
most_freq = data[(data['longitude'] == 103.93700000000001) & (data['latitude'] == 1.34721)]
import networkx as nx $ from operator import itemgetter $ import community
assert len([col for col in cols_to_drop if col in twitter_archive_clean.columns])==0
pred = pd.read_table('image-predictions.tsv', sep = '\t') $ pred.info()
my_df["user"] = my_df["user"].astype("object") $ print(my_df["user"].describe())
data_file = pandas.ExcelFile(file_path)
data.columns
reddit_data = reddit_data[(reddit_data['date'] > '2017-05-01')] $ reddit_data
tmp_3 = linear_model.BayesianRidge() $ tmp_3.fit(X_training_3,y_training) $ y_pred_no_f = tmp_3.predict(X_final_test_3) $ r2_score(y_final_test, y_pred_no_f)
ridg = Ridge() $ ridg.fit(X_train, y_train) $ predicted_ridg_y = ridg.predict(X_test) $ print("Ridge Mean Squared error:",mean_squared_error(y_test, predicted_ridg_y)) $ print("Ridge R2 Score:",ridg.score(X_test, y_test))
normal_model = Prophet(yearly_seasonality =True,weekly_seasonality= True,daily_seasonality = True) $ normal_model.fit(df1_normal); $ normal_future = normal_model.make_future_dataframe(periods= 6 , freq = 'M')    $ normal_forecast = normal_model.predict(normal_future) $ plotzero = normal_model.plot(normal_forecast) $
url = form_url(f'actions/{action_id}/metricSeries') $ response = requests.get(url, headers=headers) $ print_body(response, max_array_components=3)
path="../output/positve_model.pickle"
tweet_archive_clean = tweet_archive_df.copy()
pickle.dump(posts, open('data/posts_with_ratings_summary.dat', 'wb'))
df['HL_PCT'] = (df['Adj. High'] - df['Adj. Low']) / df['Adj. Close'] * 100.0 $
cur.execute('SELECT * FROM materials') $ cur.fetchall()  # fetch all the results of the query
'__call__' in dir(type(Magic))
celtics = celtics[['game_id','season','date','day_of_week','time','hour','minute','late','opponent','playoff']]
df_arch.describe() $
def calc_temps(start_date, end_date): $     return session.query(func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)).\ $         filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all() $ print(calc_temps('2012-02-28', '2012-03-05'))
londonDFSubsetWithCounts.head()
len(df.query('converted == 1'))/len(df)
merkmale1=calc_creditworthy(merkmale)
intervention_train[intervention_train.index.duplicated(keep=False)]
between_all_posts.describe()
df=pd.read_csv('~/Downloads/Iowa_Liquor_sales_sample_10pct.csv') $ df.columns=['date','store_number','city','zip','county_number','county','category','category_name','vendor_number','item_number','item_descript','bottle_ml','state_cost','state_retail','bottles_sold','sale','volume_sold_l','volume_sold_g']
autos[["ad_created", $        "date_crawled", $        "registration_year", $        "registration_month", "last_seen"]].info()
example_string_list = ['an','example','list','of','strings'] $ example_int_list = [123,34,3,12,32,14] $ example_nested_list = [example_string_list,example_int_list] $ flattened = [item for sublist in example_nested_list for item in sublist] $ flattened
top_10_authors = git_log['author'].value_counts().head(10) $ top_10_authors
z_score, p_val = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller') $ ('p-value:',p_val,'z-score:', z_score)
null_vals = np.random.normal(0,np.std(p_diffs),10000)
twitter_final['tweet_id'] = twitter_final.tweet_id.apply(lambda row: str(row))
df['group'].value_counts()
def lemmatizing(tokenized_text): $     text = [wn.lemmatize(word) for word in tokenized_text] $     return text $ infinity['text_lemmatized'] = infinity['text_nostop'].apply(lambda x: lemmatizing(x))
prediction_simple = predict_output(test_simple_feature_matrix, weights_simple) $ prediction_multiple = predict_output(test_multiple_feature_matrix, weights_multiple)
print('Mean:',np.mean(timedifference)) $ print('Median:',np.median(timedifference)) $ print('Max:',np.max(timedifference))
import numpy as np $ import pandas as pd $ pd.set_option('display.max_columns', 10) $ pd.set_option('display.max_rows', 10)
df_joined = df2.join(df_countries.set_index('user_id'), on='user_id') # joining the 2 data frames on the 'user_id' column $ df_joined.head()
schema = StructType([ $     StructField("c1", FloatType()), $     StructField("c2", StringType()), $     StructField("c3", DateType()), $ ])
data4.to_file('Twitters_FSGutierres.shp', driver='ESRI Shapefile')
df2=df.copy()
X_train, X_test, y_train, y_test = train_test_split(X,y)
df_ad_airings_5['location'].isnull().sum()
week_count_list = station.groupby(['DAYOFWEEK']) ['TOTALTRAFFIC'].sum() $ week_count_list.head() $ plt.plot(week_count_list)
test_kyo1 = tweets_kyoto_filter[tweets_kyoto_filter['ex_lat']>35.01] $ test_kyo1 = test_kyo1[test_kyo1['ex_long']>135.725300]
consumer_key = <your code> $ consumer_secret = <your code> $ access_token = <your code> $ access_token_secret = <your code>
high_rev_acc_opps_net.columns
(df $  .groupby(["C/A", "UNIT", "SCP", "STATION", "DATETIME"]) $  .ENTRIES.count() $  .reset_index() $  .sort_values("ENTRIES", ascending=False)).head(1)
p_diffs = [] $ for _ in range(int(1e4)): $     new_pc = np.random.binomial(nnew, p = pnew) $     old_pc = np.random.binomial(nold, p = pold) $     p_diffs.append(new_pc / nnew - old_pc / nold)
F.mse_loss(input=pred1, target=target1)
rfc = RandomForestClassifier(max_depth = 10, n_estimators=5, random_state=42) $ rfc.fit(X_train, y_train) $ rfc.score(X_test, y_test)       # scoring the  on test Data
df.date
autos.groupby('brand')['price'].mean().sort_values(ascending = False).head(20)
run txt2pdf.py -o"2018-06-19 2015 MAYO CLINIC HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2015 MAYO CLINIC HOSPITAL Sorted by discharges.txt"
terror.head()
unigram_chunker = UnigramChunker(train_trees) $ print(unigram_chunker.evaluate(valid_trees))
import pandas as pd $ from datetime import timedelta $ %matplotlib inline $ vow = pd.read_csv('./datasets/vow.csv') $ vow.head()
np.arange(0,2,9)
commits_per_weekday.plot.bar();
date_mask = (liquor['Date'] >= "2015-01-01") & (liquor['Date'] <= "2015-12-31") $ liquor_2015 = liquor[date_mask].sort_values(by=['Date'])
%matplotlib inline
len(df2[(df2["group"] == "treatment") == (df2["landing_page"] == "old_page")]) $
p_old = df2[df2['landing_page'] == "old_page"]['converted'].mean() $ p_old
cursor = fs.find({"filename" : "scansmpl.pdf"}).limit(3) $ print "Found", cursor.count(), "documents" $ for doc in cursor: $     print doc.uploadDate
n_old = size_control $ n_old
mean_sea_level.index.name = "date" $ mean_sea_level
df4.head(3)
def get_rebalance_cost(all_rebalance_weights, shift_size, rebalance_count): $     assert shift_size > 0 $     assert rebalance_count > 0 $     return None $ project_tests.test_get_rebalance_cost(get_rebalance_cost)
AAPL_array=df["NASDAQ.AAPL"].dropna().as_matrix() $ model_arima = ARIMA(AAPL_array, (2,2,2)).fit() $ print(model_arima.params)
var_per_portfolio = res.groupBy('date', 'portfolio').sum() $ var_per_portfolio = var_per_portfolio.map(lambda r: (r[0], r[1], r[2], float(var(np.array(r[3:]) - r[2])))) $ var_per_portfolio = sql.createDataFrame(var_per_portfolio, schema=['date', 'portfolio', 'neutral', 'var'])
precipitation_df.plot() $
from sklearn import metrics $ print metrics.accuracy_score(y_test, predicted) $
precip_df = precip_df.sort_index(ascending = True) $ precip_df.head()
df.query('converted == "1"').user_id.nunique() / df['user_id'].nunique() $
df_2015 = df.sort_values('store_number') $ df_2016 = df.sort_values('store_number')
df_link_yt = df_yt.merge(df_yt_resolved, left_on = 'yt_id', right_on='video_id', how='left')
autos.columns
df_2.head()
em_sz = 400 $ num_hidden = 1150 $ num_layers = 3
ax1 = sns.boxplot(x = tweets_clean['favorite_count']) $ ax1.set(ylabel = 'favorite_tweets')
data[data['Body_Count']>200].sort_values(by='Body_Count', ascending=False)
df2.head()
search_results = gis.content.search('title: USA Major Cities', $                                     'Feature Layer') $ major_cities_item = search_results[0] $ major_cities_item
data.sort_values(by = ['lat-lon','description'], inplace=True)
product_searches_df['ga:eventAction'].value_counts()
Image(filename='healthy_breakfast.jpg')
data.groupby(['Year'])['Salary'].sum()
realdiff=(treatmentdf[treatmentdf.converted==1].count()[0]/treatmentdf.count()[0])-(controldf[controldf.converted==1].count()[0]/controldf.count()[0]) $ (p_diffs>realdiff).mean()
source = api.create_source('data/Airline+Weather_data.csv')
converted_first_week = first_week_day_num.asfreq("B") # class only happens on business days $ converted_first_week
G.add_path([0,1,2,3,4,5,6,7,8,9]) $ H = G.subgraph([0,1,2,3,4,5,6,7,8,9]) $ H.edges()           
temp = temp.rename(columns={"month_of_year":"month"})
segmentData.head()
del datatest['lat-lon']
melted_total = pd.melt(total_df, id_vars = ['Res_id','Name', 'Neighbourhood','Url','Review_count', 'Rating'], $                         value_vars = ['cat1','cat2','cat3'], value_name = 'Categories').drop('variable',axis = 1)
import numpy as np $ from sklearn.model_selection import train_test_split
cpq_status = cpq_business.loc[cpq_business['On Zayo Network Status'] == 'Not on Zayo Network']
avg_att_2015_BOS = nba_df.loc[(nba_df["Season"] == 2015) & (nba_df["Team"] == "BOS"), "Home.Attendance"].mean() $ round(avg_att_2015_BOS, 0)
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs) $ plt.title('The normal distribution of mean differents of new_page and old_page converted');
pd.options.display.max_columns = 200
cursor =  db.posts.find({'$and': [{'timestamp': {'$gte': datetime.datetime(2016, 9, 10, 0,0,0)}}, $                          {'timestamp':{'$lt': datetime.datetime(2016, 9, 11, 0, 0, 0)}}, $                          {'type': "image"}]}, $                projection=['location','timestamp','likes.count','images.low_resolution','tags'])
print('The proportion of users converted:',df['converted'].mean())
for i in wmd_list: $     if np.isinf(i) == False: $         print('{} is a valid value'.format(i)) $     else: $         print('{} is NOT a valid value'.format(i))
autos.loc[(autos.registration_year > 2016)|(autos.registration_year < 1900),'registration_year'] = np.nan
df_user_product_ids.head(5)
import numpy as np $ import matplotlib.pylab as plt $ %matplotlib inline $ from sklearn import linear_model
print(len(countdf['name'])) $ print(len(countdf['name'])-len(count1df['name'])) $ print(len(countdf['name'])-len(count6df['name']))
air_rsrv, air_store, air_visit, date_info, hpg_rsrv, hpg_store, air_hpg_rel = tables
s_cal = pd.read_csv('seattle/calendar.csv') $ s_list = pd.read_csv('seattle/listings.csv') $ s_rev = pd.read_csv('seattle/reviews.csv')
print("Converted proportion : " + str(df.converted.mean()))
df2_duplicated_row
final.head(1)  # Inspecting progress
df_new['week1'] = np.where(df_new['timestamp'] <= '2017-01-09', 1, 0) $ df_new['week2'] = np.where((df_new['timestamp'] > '2017-01-09') & (df_new['timestamp'] <= '2017-01-16'), 1, 0) $ df_new['week3'] = np.where(df_new['timestamp'] > '2017-01-16', 1, 0)
discover = tmdb.Discover() $ response = discover.movie(page = 1, primary_release_year=2016, certification_country='US', $                           sort_by='vote_average.desc')
WT_PATH = Path('./models/wt103') $ WT_PATH.mkdir(exist_ok=True)
movies
autos['registration_year'].describe()
archive_copy
import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline $ %config InlineBackend.figure_format = 'svg'
df['text_extended'] = df.index1[898:1061].apply(tweet_extend) $
TrainData_ForLogistic.head()
obs_diff_conversion
df.tail(5)
ts.dt.weekday
test_data_features_tfidf = vectorizer_tfidf.fit_transform(clean_test_reviews_sw) $ test_data_features_tfidf = test_data_features_tfidf.toarray() # Numpy arrays are easy to work with $ print(test_data_features_tfidf.shape)
gender
logit_countries_model = sm.Logit(df4['converted'], df4[['country_UK', 'country_US', 'intercept']]) $ results_countries = logit_countries_model.fit()
sns.set_style("darkgrid") $ plt.figure(figsize=(8, 4)) $ errors['errorID'].value_counts().plot(kind='bar') $ plt.ylabel('Count')
poverty_columns_new[0:2]=['Community Name', 'Community Area']
!head ../../data/msft_modified.csv
b[b.T.sum()==c].index.min()
order_drop.columns
X = pd.get_dummies(cats_df, columns=['breed'], drop_first=True)
df_archive_clean.info()
logit_mod = sm.Logit(df3['converted'], df3[['intercept','c2','c3']]) $ results = logit_mod.fit() $ results.summary() $
reason_for_visit.info()
def trip_end_weekday_m(x): $     if x['search_type'] == 'CAR': $         return trip_end_weekday_car(x['trip_end_date_weekday']) $     else: $         return trip_end_weekday_fh(x['trip_end_date_weekday'])
df_customers['number of customers'] = np.exp(df_customers['log number of customers'])
guineaDf.head()
lesson_date + timedelta(days=368, seconds=2)
print_txt2pdf_many_sites()
dtc = DecisionTreeClassifier(max_leaf_nodes=1002) $ scores = cross_val_score(dtc, X, np.ravel(y,order='C'), cv=10) $ print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
import urllib $ iris_url="http://aima.cs.berkeley.edu/data/iris.csv" $ urlRequest=urllib.request.Request(iris_url) $ iris_file=urllib.request.urlopen(urlRequest) $ irisIterator=pd.read_csv(iris_file,sep=',',header=None,decimal='.',names=['sepal_length','sepal_width','petal_length','petal_width','target'],iterator=True)
duration_test_data
learner.sched.plot_loss()
import pandas as pd $ git_log = pd.read_csv('datasets//git_log.gz', sep='#', encoding='latin1', header=None,  names=["timestamp", "author"], compression='infer') $ git_log.head(5)
from selenium import webdriver $
logreg = LogisticRegression() $ logreg.fit(X_train, y_train)
fit2.resid.hist();
simResis_hour = calc_total_et(results_simpleResistance) $ BallBerry_hour = calc_total_et(results_BallBerry) $ Jarvis_hour = calc_total_et(results_Jarvis)
from curation_common import * $ from htsworkflow.submission.encoded import DCCValidator
df = pd.read_csv('small_df_for_developing_and_debugging.csv', parse_dates = ['date']) $ df.head()
df_os[df_os['domain'] == 'infowars.com']
print(df['Site Fill'].value_counts(dropna=False))
df_ab_page['UK_ind_ab_page'] = df_ab_page['UK']*df_ab_page['ab_page'] $ df_ab_page['US_ind_ab_page'] = df_ab_page['US']*df_ab_page['ab_page'] $ df_ab_page['CA_ind_ab_page'] = df_ab_page['CA']*df_ab_page['ab_page']
model_unigram.most_similar(positive=['house','pool'],topn=5)
page.text
tw_clean = tw_clean.drop(tw_clean[tw_clean.rating_denominator != 10].index)
import numpy as np $ a=np.random.randn(3, 3) $ a
df.isnull().sum()
new_page_converted = np.random.choice(2,size = 145310,p = [p_new,(1 - p_new)]) $ p_new_bootstrapped = new_page_converted.mean() $ print(p_new_bootstrapped)
score_b = score[(score["score"] < 90) & (score["score"] >= 80)] $ score_b.shape[0]
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','US','UK']]) $ results=logit_mod.fit() $ results.summary()
print 'DataFrame df_visitsbyCountry: ', df_visitsbyCountry.shape $ df_visitsbyCountry.head()
rfctest = pd.DataFrame(labeled_features.loc[labeled_features['datetime'] >= pd.to_datetime('2015-10-01 00:00:00')]) $ rfctest['predicted_failure'] = model.predict(test_x)
from twitter_tokens import *
plt.style.use('ggplot') $ bb['close'].apply(rank_performance).value_counts().plot(kind='barh')
df.set_index('Project Name').to_csv('PipelineInventorySales_Clean.csv')
rest = datatest[datatest.surface_total_in_m2.isnull()] $ train = datatest[datatest.surface_total_in_m2.notnull()] $ test = train[train['covered/total'].isnull()] $ train = train[train['covered/total'].notnull()]
trump_time = df[["time","count_trump"]][df["count_trump"]>0][1:] $ trump_time.head()
datetime.time(datetime(2015,12,14,15,17,30))
df_Sessions.head()
df1_clean.rating_denominator = 10
list_to_change = [southwest, east_end, addicks, $                   johnson_space, texas_city, baytown_shallow, baytown_deep, $                   seabrook, clear_lake_shallow, clear_lake_deep, pasadena]
df.groupby("same_location")["cancelled"].mean()
parse_dict['category']['category_'] = parse_dict['category']['slug'].str.split('/') $ parse_dict['category']['category'] =parse_dict['category']['category_'].str[0] $ parse_dict['category']['subcategory'] = parse_dict['category']['category_'].str[1] $ df_cat = parse_dict['category'][['category','subcategory']]
yy = np.sqrt(gwt.ggSimp(2,0.0))/2 $ print( yy )
ncTest = nc.Dataset('Dropbox/aula_nc/ncTest.nc', 'w', format='NETCDF4')
nmf_tfidf_topic6_sample = mf.random_sample(selfharmmm_final_df, criterion1='non_lda_max_topic', value1='nmf_tfidf_topic6', use_one_criterion=True)
with open('turnstile_160305.txt') as f: $     reader = csv.reader(f) $     rows = [[cell.strip() for cell in row] for row in reader]
date + pd.to_timedelta(np.arange(12), 'D')
words = pd.read_sql("SELECT subreddit, body FROM comments", con=engine) $ words.head()
commits_per_year.sort_values(by='author', ascending=False)
autos["odometer"] = autos["odometer"].str.replace("km", "").str.replace(",", "").astype(int) $ autos.rename({"odometer": "odometer_km"}, axis = 1, inplace = True)
list(df.columns.values)
pp.head()
df_msg = pd.DataFrame.from_records(ods.messages)
cust_data1=cust_data.assign(No_of_30_Plus_DPD=cust_data.No_of_30_59_DPD+cust_data.No_of_60_89_DPD+cust_data.No_of_90_DPD, $                            MonthlySavings=cust_data.MonthlyIncome*0.15)
tsvd = TruncatedSVD(n_components=25) #  try algorithm='arpack' to keep from crashing
df3 = df2.copy() $ df3 = df3.join(df_country.set_index('user_id'), on='user_id') $ df3.head()
learn=ConvLearner.pretrained(f_model,data,metrics=metrics)
st_streams.close()
import datetime $ (events[ (events.doc_type == "CNI") $        & (events.index.date == datetime.date(2017, 1, 10)) ] $        .head(5)) $
fashion.shape
df2.drop_duplicates('user_id',keep='first',inplace=True) $ df2[df2.duplicated('user_id',keep=False)] $
print(rmse)
PredClass.df_feat.shape
df.head()
df.shape[0]
rng
type(retweets), type(retweets[0])
import collections $ import csv $ import itertools $ import json $ import pandas as pd $
import os $ PROJECT = 'coursera-192902'    # CHANGE THIS $ BUCKET = 'single-bucket'  # CHANGE THIS $ REGION = 'us-central1' # CHANGE THIS
from oauth2client.service_account import ServiceAccountCredentials $ scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive'] $ credentials = ServiceAccountCredentials.from_json_keyfile_name('API Project-f22fe0b03992.json', scope)
ohe_feats = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser'] $ for f in ohe_feats: $     df_all_dummy = pd.get_dummies(df_all[f], prefix=f) $     df_all = df_all.drop([f], axis=1) $     df_all = pd.concat((df_all, df_all_dummy), axis=1)
df['B'] = 1 $ df
train.isnull().sum()
archive_clean.info()
dat.info()
pd.Timestamp('2010/11/12')
len(tweets_kyoto_filter)
type(ts.index)
msno.matrix(titanic)
import pandas as pd $ import numpy as np $ from sqlalchemy import create_engine $ engine = create_engine('mysql+pymysql://root:kobi5555@0.0.0.0/proddb')
S_1dRichards.decision_obj.thCondSoil.options, S_1dRichards.decision_obj.thCondSoil.value
archive_clean.to_csv('archive_clean.csv') $ images_clean.to_csv('images_clean.csv') $ popularity_clean.to_csv('popularity_clean.csv')
p_education=portland_census2.drop(portland_census2.index[39:]) $ p_education=p_education.drop(p_education.index[:37]) $ p_education.head()
dfData['rqual_score5'].hist()
ldamodel = train_lda(dictionary,posts_words_vectors,number_of_topics)
mustang.model
np.sum(my_df.isnull().any(axis=1))
print("Number of edited comments: ",len(df['edited'].value_counts()))
yr15 = 271/243 $ total = 1759/2034 $ print('Ratios for: yr15 = ',yr15, '...and total = ', total)
df_vow.columns = ['Date','Open','High','Low','Close','Volume'] $ df_vow.columns.values $
counts_compare['Count_Diff'] = counts_compare['Tweet_Count'] - counts_compare['ESPN_Play_Count']
bEgypt = CustomBusinessDay(weekmask = 'Sun Mon Tue Wed Thu')
twitter_ar.head(2)
df = sm.datasets.get_rdataset("Guerry", "HistData").data   # this is a familiar Pandas dataframe $ df.head()
a + b
source_df = pd.read_json('chart_data.json')[2:]
trip_data.head()
lin_mod = sm.OLS(df_new['converted'], df_new[['intercept', 'ab_page', 'US', 'new_US', 'UK', 'new_UK']]) $ result = lin_mod.fit() $ result.summary()
autos.columns = ['date_crawled', 'name', 'seller', 'offer_type', 'price', 'abtest', 'vehicle_type', 'registration_year', 'gearbox', 'power_ps', 'model', 'odometer', 'registration_month', 'fuel_type', 'brand', 'unrepaired_damage', 'ad_created', 'num_photos', 'postal_code', 'last_seen'] $ autos.head()
twitter_df.tweet_id.sample(1)
logistic_countries2 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'US']])
train_file = r"D:\Git\Sellthrough2\STR_Pipeline\temp_data\data20180716\feature20180716\mdata_noopen\Data_Train_FeatureTactic_train_feature20180716mask_0.1.txt" $ valid_file = r"D:\Git\Sellthrough2\STR_Pipeline\temp_data\data20180716\feature20180716\mdata_noopen\Data_Train_FeatureTactic_test_feature20180716mask_0.1.txt" $ eval_file = r"D:\Git\Sellthrough2\STR_Pipeline\temp_data\data20180716\feature20180716\mdata_noopen\Data_Train_FeatureTactic_emk_feature20180716mask_0.txt"
print(f'standard deviation of tracking error: {np.std(pipe.tracking_error)}')
owns[['user_id', 'repo_id', 'created_at']].to_csv('data/new_subset_data/new_subset_owns.csv', sep='\t', index=False)
x = make_df('AB', [0, 1]) $ y = make_df('AB', [2, 3]) $ y.index = x.index # make duplicate indices! $ display('x', 'y', 'pd.concat([x, y])')
df3.join(highly_delayed.head(5), on='Origin', how='inner')['Carrier'].unique()
calc_freq(clinton_pivoted)
data = df[base_col].values $ training_split_cut = int(split_pct*len(df))
df_wu = pd.read_csv('weather_kdca0917.csv', index_col='date', parse_dates=True) $ df_wu.dtypes
local.display_dataset_graph(1)
documents = data['clean_text'] $ vectorizer = CountVectorizer(stop_words='english',min_df=0.01,max_df=1.0,ngram_range=(1,6), max_features = 15000) $ X = vectorizer.fit_transform(documents)
spark = SparkSession(sc)
df['price_doc'].hist(bins=100)
p_diff = p_new- p_old $ print("Difference in probability of conversion for new and old page (not under H_0):", p_diff)
v_invoice_hub.drop(v_invoice_hub_dropper, axis = 1, inplace = True)
ac['Monitoring Start'].describe()
f0 = lv_workspace.get_data_filter_object(step=0) 
n_new = df2[df2['landing_page'] == 'new_page'].user_id.count() $ n_new
p_diffs = np.array(p_diffs) #converting to numpy array as taught in nano degree $ plt.hist(p_diffs);
external_users=df_users['uid']
from elasticsearch import Elasticsearch $ es = Elasticsearch([{'host': 'elasticsearch', 'port': 9200}]) $ es
from sqlalchemy import create_engine, func, inspect $ inspector = inspect(engine) $ inspector.get_table_names() $
plt.rcParams['axes.unicode_minus'] = False $ dta_57.plot(figsize=(15,5)) $ plt.show()
ttarc_clean = ttarc.copy() $ imgp_clean = imgp.copy() $ tt_json_clean = tt_json.copy()
shirt_1 = Shirt("blue", "M", "M", 40)
pd_data.status[0]==4
movies = pd.read_csv('Data/movie_df.csv', encoding='utf8', converters={'genre_ids':literal_eval}) $ movies.release_date = pd.to_datetime(movies.release_date) $ movies.head(20)
df = pd.read_csv("CSV/Iowa_Liquor_sales_sample_10pct.csv") $ print df.columns $ df.head()
df[12].plot()
print(pd.value_counts(train_df['device'])[:20])
import pystore $ import quandl
from IPython.core.interactiveshell import InteractiveShell $ InteractiveShell.ast_node_interactivity = "all"
itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq] $ itos.insert(0, '_pad_') $ itos.insert(0, '_unk_')
pd.read_csv("Data/microbiome.csv", chunksize=14)
df_h1b_nyc[df_h1b_nyc.pw_1>300000][['pw_1','lca_case_wage_rate_from','lca_case_wage_rate_to']]
df_sites = df_sites.sort_values(['disc_times_pay'], ascending=[True]) $ df_sites = df_sites.reset_index(drop=True) $ df_sites.head()
table = pd.read_html(url_mars_facts) $ table[0]
autos['make'].cat.codes.head()
new_page_converted= np.random.binomial(1, p_new, n_new)
df2.loc[df2['user_id'].duplicated(),:]
lm2=sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US']]) $ results=lm2.fit() $ results.summary()
df_train.head()
display('x', 'y', "pd.concat([x, y], keys=['x', 'y'])")
feature_names = vectorizer.get_feature_names() $ if feature_names: $     feature_names = np.asarray(feature_names)
soup = BeautifulSoup(html, 'lxml') $ titles = soup.findAll("a", {"data-event-action": "title"})
import matplotlib.pyplot as plt $ %matplotlib inline $ plt.scatter(training_pending_ratio[:, 0], training_pending_ratio[:, 1], color='r') $ plt.xlabel('Training Active Listing') $ plt.ylabel('Polynomial Pending Ratio')
df['US'] = 0 $ df['UK'] = 0 $ df.loc[df.country == 'US', 'US'] = 1 $ df.loc[df.country == 'UK', 'UK'] = 1 $ df.head()
df3.loc[:,'C'] = np.array([2] * len(df3)) $ df3
shows.shape
df_questionable_3[df_questionable_3['state_AZ'] == 1]['link.domain_resolved'].value_counts()
' '.join(str(o) for o in trn_lm[0])
co.steady_states
p_old = conversion_df2 $ p_old
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2.head()
support.amount.sum()
writing_commit_df = commit_df.query("(characters_added > 0 or characters_deleted > 0) and merge == 0") $ stats['manuscript_commits'] = len(writing_commit_df)
msft = pd.read_csv('msft.csv') $ msft.head()
tcat_df = tcat_df.append(tcat) $ tdog_df = tdog_df.append(tdog)
df2=df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == True].copy() $ df2.shape
column_dataloaders = {x: torch.utils.data.DataLoader(column_datasets[x], batch_size=128, $                                              shuffle=True, num_workers=0) $               for x in ['train', 'val']}
main()
students.weight.plot.hist(rwidth=.9, bins=np.arange(95,205,10))
os.listdir(os.getcwd() + "/2018-05-26/")[0]
autos['registration_year'].describe()
all_sets["releaseDate"] = pd.to_datetime(all_sets["releaseDate"])
datAll['Offense_count'] = np.where(~np.isnan(datAll['# Of']),datAll['# Of'],np.nan) $ datAll['Offense_count'] = np.where(~np.isnan(datAll['# Of Offenses']),datAll['# Of Offenses'],datAll['Offense_count']) $ datAll['Offense_count'] = np.where(~np.isnan(datAll['# Offenses']),datAll['# Offenses'],datAll['Offense_count']) $ datAll['Offense_count'] = np.where(~np.isnan(datAll['# offenses']),datAll['# offenses'],datAll['Offense_count']) $ datAll['Offense_count'] = np.where(~np.isnan(datAll['Offenses']),datAll['Offenses'],datAll['Offense_count'])
plt.hist(_p_diffs) $ plt.show()
areas_dataframe = areas_dataframe.sort_values(by='rate', ascending=False) $ areas_dataframe = areas_dataframe.reset_index() $ areas_dataframe = areas_dataframe.drop('index', 1) $ areas_dataframe[areas_dataframe.rate >= .5]
propiedades.loc[:,'year'] = pd.to_datetime(propiedades.loc[:,'created_on']).dt.year $ propiedades.loc[:,'month'] = pd.to_datetime(propiedades.loc[:,'created_on']).dt.month $ propiedades.sample()
df2[df2.converted == 1].shape[0]/df2.shape[0]
wikiMarvelSoup = bs4.BeautifulSoup(wikiMarvelRequest.text, 'html.parser') $ print(wikiMarvelSoup.text[:200])
station_count = station_df['Station ID'].nunique() $ print(station_count)
spark.sql('select * from my_analysis_work.example_timeseries limit 10').toPandas()
top_songs['Date'].isnull().sum()
tweet_archive_clean = tweet_archive_clean[(tweet_archive_clean['name'] != 'a')]
unique_urls.sort_values('num_authors', ascending=False)[0:50][['url', 'num_authors']]
autos['registration_year'].value_counts().sort_index(ascending=False).head(20)
import seaborn as sns $ import matplotlib.pyplot as plt $ %matplotlib inline
df.mean(axis='columns')
v_item_hub.shape
out = query.get_dataset(db, id=ds_info["DatasetId"][0])
del(promo_df)
joined.dropna(inplace=True) $ len(joined[joined.Close.isnull()])
pop_rec = graphlab.popularity_recommender.create(sf_stars, $                                                 user_id = 'user_id', $                                                 item_id = 'business_id', $                                                 target = 'stars')
freqword = defaultdict(list) $ for word, freq in Counter(wordlist).items(): $     freqword[freq].append(word) $ for freq in sorted(freqword): $     print('count {}: {}'.format(freq, sorted(freqword[freq])))
temp_obs = session.query(Measurement.date, Measurement.tobs).\ $ filter(Measurement.station == mostactive_station).\ $ filter(Measurement.date >= '2016-08-18').\ $ order_by(Measurement.date).all()
sentiment_df["Text"]=text $ sentiment_df.head()
log_mod = sm.Logit(df2.converted,df2[['intercept','ab_page']]) $ result_1 = log_mod.fit()
df2[df2.duplicated('user_id')]
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs) $ plt.title("Simulated differences in conversion rates under null hypothesis", fontsize=14) $ plt.xlabel('Difference in Probability', fontsize=12) $ plt.axvline(treat_convert - control_convert, color='r')
import pandas as pd $ df_train = pd.read_json('data/train.json')
ranked.set_index('date').resample('M').count()
(9+2)//3
plt.rcParams['figure.figsize'] = [16,4] $ plt.plot(pd.to_datetime(mydf3.datetime),mydf3.fuelVoltage, 'g.', markersize = 2); $ plt.xlim(datetime.datetime(2017,11,15),datetime.datetime(2018,3,28))
zip_2_df.head()
s_n_s_epb_two.rename(columns = {0:"Count"},inplace=True)
d + pd.tseries.offsets.Week(weekday=4)
logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA','UK']]) $ results = logit_mod.fit() $ results.summary()
dfg["cluster"] = first_cluster.predict(docs)
status_dummies = pd.get_dummies(fb.status_type, dummy_na=True, prefix="st") $ fb_features = pd.concat((fb,status_dummies), axis=1) $ fb_features['message_character_count'] = fb_features['message'].apply(lambda x: len(x)) $ fb_features['message_word_count'] = fb_features['message'].apply(lambda x: len(x.split()))
AAPL.head(5)
df[((df.group == 'control') & (df.landing_page == 'new_page')) | (df.group == 'treatment') & (df.landing_page == 'old_page') ]['user_id'].count() $
from pyspark.sql.types import DoubleType, IntegerType $ for col_name in data.columns: $     data = data.withColumn(col_name, data[col_name].cast(DoubleType()))
df = df.swaplevel('Description', 'UPC EAN') $ df.head(3)
iou_metric(test_r.Pixels.values[62], predicted_image)
arma_mod50 = sm.tsa.ARMA(dta_713, (5,0)).fit(disp=False) $ print(arma_mod50.params)
merged_df_prec = pd.concat([merged_df, prec_group], axis=1)
df['gender']= df['gender'].astype(str) #converting the numerical field of gender values (0,1,2) to strings $ grouped_genders = df.groupby('gender').count() #grouping by gender $ sorted_gender = (grouped_genders.tripduration.sort_values(ascending = False)).head()#and arranging in descending order $ sorted_gender
print(train_data.isnull().sum()) $ print(test_data.isnull().sum())
data = pd.read_csv("train.csv")
kNN100 = KNeighborsClassifier(n_neighbors=100)
for i in range(artificial_returns.shape[1]): $     plt.plot(returns.iloc[:,i], color=colors[i]) $ plt.legend(loc=3, bbox_to_anchor=(1.0,0.5)) $ plt.show()
Grouping_Year_DRG_discharges_payments.head()
import numpy as np $ result=results.home_score $ result.apply(np.median)
r.groupby(level=0)['net'].sum().plot()
askreddit['num_comments'].value_counts().sort_index(ascending=False)[:10]
def daily_normals(date): $     sel = [func.min(measurements.tobs), func.avg(measurements.tobs), func.max(measurements.tobs)] $     return session.query(*sel).filter(func.strftime("%m-%d", measurements.date) == date).all() $ daily_normals("01-01")
data.dtypes
top20_branks_mean_price_mileage = top20_brands_mean_price
tcp.columns
move_34p23s34p = (breakfastlunchdinner.iloc[1, 1] $                + breakfastlunchdinner.iloc[3, 2] $                + breakfastlunchdinner.iloc[1, 3]) * 0.002 $ move_34p23s34p
pd.Series(unseen_predictions).value_counts()
plt.subplots(figsize=(15, 6)) $ sn.barplot(actions_top10['index'],actions_top10['action'], palette="Set3")
highly_retweeted=tizibika[tizibika['retweets']==tizibika['retweets'].max()]
aSL.shape, eSL.shape
merged_predictions_archive = pd.merge(left=twitter_archive_clean, right=most_confident_predictions, how='inner', on=['tweet_id']).copy()
extract_all.loc[(extract_all.application_month=='2018-04') $                &(extract_all.APP_SOURCE=='LEADSMKT') $                &(extract_all.app_branch_state=='MO') $                &(extract_all.LEAD_MIN_SELL_PRICE==15)].APPLICATION_DATE_short.min()
agency_borough.size().unstack().plot(kind='bar')
check_df = pd.read_csv(file_name) $ check_df.head()
plt.figure(figsize=(12,6)) $ sns.countplot(x='sourceID', data=articles)
from pandas.tseries.offsets import *
k_var_concat.head()
len(dftop) $
np.exp(-2.0375)
color = dict(boxes='DarkGreen', whiskers='DarkOrange', medians='DarkBlue', caps='Gray') $ data[['MAG_MAX', 'TMAX', 'TMED', 'TMIN', 'MAG_MED']].plot.box(color=color)
df.iloc[3,] # is this a row or column?
X=col[0]
dfLikes["date"] = dfLikes["created_time"].apply(lambda d: datetime.strptime(d, '%Y-%m-%dT%H:%M:%S+%f').strftime('%Y%m%d'))
JAR_FILE = "/usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.7.0.jar" # eg. /usr/lib/hadoop-mapreduce/hadoop-streaming.jar $ HDFS_DIR = "/user/root/hw3" # eg. /user/root/hw3 $ HOME_DIR = os.getcwd() 
autos['odometer_km'].value_counts()
week12 = week11.rename(columns={84:'84'}) $ stocks = stocks.rename(columns={'Week 11':'Week 12','77':'84'}) $ week12 = pd.merge(stocks,week12,on=['84','Tickers']) $ week12.drop_duplicates(subset='Link',inplace=True)
df_new['stack_answer_count'] = u.answers.count
cityID = '44d207663001f00b' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Mesa.append(tweet) 
varieties = dataset['variety'].tolist() $ wine_count = Counter(varieties) $ nr_reviews = len(varieties) $ threshold = 200
df = pd.read_csv('output.csv', delimiter=',') $ df $ df.dtypes
df2=df2.drop('cntry_CA',axis=1)
full[full['Readmitted'] == 1].groupby(['Patient']).diff()['AdmitDate'].tail()
df3.describe()
iowa_fc = iowa_fc_item.layers[0]
k = 7 $ neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train_knn,y_train_knn) $ neigh
to_be_predicted_Day3 = 82.26272357 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
tweets.head()
df['tt_lat'] = df['tweet_location_coord'].apply(lambda x: None if type(x) is float else x['lat']) $ df['tt_lng'] = df['tweet_location_coord'].apply(lambda x: None if type(x) is float else x['lng'])
np_array = np.random.randint(1, 10, size = 16).reshape(4, 4) $ print(np_array) $ df = pd.DataFrame(np_array) $ df
print("n(old) = ",old)
nyc_census_tracts.crs
pd.value_counts(train_data["geoNetwork.subContinent"],normalize=True)
model = gensim.models.Word2Vec(sentences, min_count=1)
mnb.fit(tfidf_X_train, tfidf_y_train) $ print('Training set score:', mnb.score(tfidf_X_train, tfidf_y_train)) $ print('\nTest set score:', mnb.score(tfidf_X_test, tfidf_y_test)) $ print('\nCross Val score:',cross_val_score(mnb, tfidf_X_test, tfidf_y_test, cv=5))
mp2013 = pd.period_range('1/1/2017','12/31/2017',freq='M') $ mp2013
commit_df = pandas.read_table('commits.tsv') $ commit_df.tail(2)
item_onpromotion = train_holiday_oil_store_transaction_item_test.select("item_nbr", *exprs)
top_supporters.head()
print('Percentage of CA on the dataset :', n_CA/N*100) $ print('Percentage of UK on the dataset :', n_UK/N*100) $ print('Percentage of US on the dataset :', n_US/N*100)
len(df['user_id'].unique())
frame = pd.DataFrame(data) $ frame
user1 = user1[(user1.CallDate.dt.dayofweek == 5) | (user1.CallDate.dt.dayofweek == 6) ] $
query = "Trump" $ language = "en" $ results = [] $ for tweet in tweepy.Cursor(api.search, q=query, lang=language).items(500): $     results.append(tweet)
df2[df2.user_id.duplicated()==True].index
old_page_converted= np.random.choice([1, 0], size=n_old, p=[p_mean, (1-p_mean)]) $ old_page_converted.mean()
autos['odometer_km'].sort_index(ascending=False).head()
from sklearn.tree import DecisionTreeClassifier
noNulls.orderBy(sort_a_desc).show(5)
list(r_ord.values())[2] $
df.tail() # default --> prints last 5 rows
data.isnull().any()
df_lampposts_loc['LOCAL'].value_counts()
payments.sample(10)
lm=sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results=lm.fit() $ results.summary()
results3.summary()
print(test_df[test_df.author.isnull()].shape[0]) $ print(test_df.shape[0])
my_list = [0.25, 0.5, 0.75, 1.0] $ data = pd.Series(my_list) # "the constructor" $ data
dir(friends_n_followers) $ help(friends_n_followers.sort_values)
tw_clean.sort_values('favorite_count').tail(1)
twitter_data_v2[~twitter_data_v2.tweet_id.isin(tweet_data_v2.tweet_id)]
try: $     surprise_slice[0] $ except KeyError as e: $     print("Key error:", e)
df.tail()
new_items = [{'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4}] $ new_store = pd.DataFrame(new_items, index = ['store 3']) $ new_store
fashion.groupby(fashion.index).size()
for model_name in nbsvm_models.keys(): $     test_probs[model_name] = nbsvm_models[model_name].predict_proba(test_cont_doc)
import pandas as pd $ import numpy as np $ from os import path
import pandas as pd $ df = pd.read_csv('./assets/USArrests.csv', index_col=0) $ df.head()
df['SA'] = np.array([ sentiment_finder(comment) for comment in df['body'] ])
sum(df.Category.isnull())
len(df.DATE.unique()), len(df)
control_convert = (control[control['converted']==1].user_id.nunique())/(control['user_id'].nunique()) $ control_convert
normals
vectorizer = CountVectorizer(analyzer='word', stop_words='english', tokenizer=lambda text: text, $                              lowercase=False, binary=True, min_df=5) $ spmat = vectorizer.fit_transform(x_tokens)
assert(tw[tw.text.str.startswith('RT @')].shape[0] == sum(tw.retweeted_status_id.notnull()))
Gun = G.to_undirected() $ politiciansPartyDict = nx.get_node_attributes(Gun, "party") $ partitions = community.best_partition(Gun, weight='weight')
train = read_data('data/train.tsv') $ validation = read_data('data/validation.tsv') $ test = pd.read_csv('data/test.tsv', sep='\t')
import statsmodels.api as sm $ convert_old = df2.query('landing_page=="old_page"').query('converted==1')['user_id'].count() $ convert_new = df2.query('landing_page=="new_page"').query('converted==1')['user_id'].count() $ n_old = df2.query('landing_page=="old_page"')['user_id'].count() $ n_new = df2.query('landing_page=="new_page"')['user_id'].count()
old_page_converted = np.random.binomial(1, p_old, n_old) $ old_page_converted[:5]
sqlContext = SQLContext(sc) $ sqlContext
datetime(2018, 1, 1) + Hour(3)
for url in soup.find_all('a'): $     print(url.get('href'))
import numpy as np $ import h5py $ import gdal, osr, os $ import matplotlib.pyplot as plt
readertest = pd.read_csv(dataurl+'test.csv.gz', sep=',', compression='gzip', chunksize=100000)
df.dropna(how='any')  # Drop the rows with any missing data (not in-place)
df3.country.unique()
station = pd.read_csv('station.csv', sep=',', parse_dates=['installation_date'], $                       infer_datetime_format=True,low_memory=False)
test = datatest[datatest['covered/total'].isnull()] $ train = datatest[datatest['covered/total'].notnull()]
p_diffs = [] $ for _ in range(10000): $     boot_new_page_converted=np.random.binomial(n=1,p=pnew,size = n_new) $     boot_old_page_converted = np.random.binomial(n = 1,p = pold, size =n_old ) $     p_diffs.append(boot_new_page_converted.mean() - boot_old_page_converted.mean())
import csv
imdb_df.drop(axis=1, labels='Unnamed: 0', inplace=True) $ imdb_df.drop(axis=1, labels='Unnamed: 4', inplace=True) $ imdb_df $
tweet_info.tail()
twitter_archive_master = pd.concat([df, image,tweet_text], axis=1, join='inner')
df.groupby('Cat')['Size'].count().plot(kind='bar')
print('The number of unique genera in 200 km circle around the Petropavlovsk-Kamchatsky city:', len(petropavlovsk_filtered.genus.unique()))
df2.sort_values('total_likes',inplace=True,ascending=False) $ top_likes = df2.head(10)['id']
dataA = pd.read_csv("data/dataPorUbicacion_Anios_tmax.csv", header=None)
import pandas as pd $ import glob $ import fnmatch $ import os $ import re
df_merge.groupby(['school_name', 'grade']).math_score.mean().unstack()
print('Bitcoin movement predictors accuracy score') $ logreg.score(X_test,y_test)
df2.query('group == "treatment"')['converted'].mean()
daily_revenue=file4.groupby('date').agg({'trans_amt':'sum'}).sort(desc("date")) $ daily_revenue.show() $ daily_revenue.count() #43 days of record 
data_vi.index.weekday
n_new_page = len(df2.query("group == 'treatment'")) $ print(n_new_page)
wrd_clean['name'] = wrd_clean['name'].apply(lambda x: 'NaN' if x == "None" else x) $ wrd_clean['doggo'] = wrd_clean['doggo'].apply(lambda x: 'NaN' if x == "None" else x) $ wrd_clean['floofer'] = wrd_clean['floofer'].apply(lambda x: 'NaN' if x == "None" else x) $ wrd_clean['pupper'] = wrd_clean['pupper'].apply(lambda x: 'NaN' if x == "None" else x) $ wrd_clean['puppo'] = wrd_clean['puppo'].apply(lambda x: 'NaN' if x == "None" else x)
dfFull.LotFrontage = dfFull.LotFrontage.fillna(dfFull.LotFrontage.mean())
learner.save_encoder('lm1_enc') # We've got a good one saved. 8.24
import pickle $ output = open('speeches_all_bill_data.pkl', 'wb') $ pickle.dump(speeches_all_bill_data, output, protocol=4) $ output.close()
lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)
age_plot = df2017.groupby(['Week Ending Date','age'])['Sales'].sum()
df2.user_id.duplicated().sum()
results = getResults(data) $ results
df.index
html_table = df.to_html() $ df.to_html('table.html') $ html_table
df_group_by.head()
temp = train.groupby("hour")["any_spot"].mean() $ train['hour_more_parking'] = train['hour'].isin(temp[temp>0.4].keys()) $ val['hour_more_parking'] = val['hour'].isin(temp[temp>0.4].keys())
fashion.info()
def check_wait_corr(): $     mini_df = data[['Order_Qty','wait']] $     mini_df['wait'] = pd.to_numeric(mini_df['wait']) $     return mini_df $ check_wait_corr().corr()
data.info()
dup
np.array(df.index)
tfidf_vocab = np.array(tfidf_vectorizer.get_feature_names())
df['date'] =  pd.to_datetime(df['date'], format='%m/%d/%y') $ df
company_vacancies['weekday'] = company_vacancies['created'].apply(lambda x: x.weekday() + 1) $ company_vacancies['hour'] = company_vacancies['created'].apply(lambda x: x.hour)
from sklearn.metrics import mean_squared_error $ y_pred = lin_svr.predict(X_train_scaled) $ mse = mean_squared_error(y_train, y_pred) $ mse
box.pop(0)
learner.sched.plot()
df2[['CA', 'UK']] = pd.get_dummies(df2.country)[['CA', 'UK']]
import matplotlib.pyplot as plt $ % matplotlib inline $ plot=twitter_data.rating_numerator.hist(bins=50) $ plot.set_ylim([0,10])
support.groupby(['contributor_firstname','contributor_lastname'])['amount'].sum().reset_index().sort_values('amount', ascending = False)
r = requests.get('https://httpbin.org/basic-auth/myuser/mypasswd') $ r.status_code
print(df['user_id'].nunique())
holdout_preds = lr.predict(x_holdout)
len(blame_raw)
tweet_hour['tweet_text'].map(tweet_tokenizer.tokenize)
url_test = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?" + \ $       "&start_date=2017-01-01&end_date=2017-01-02&api_key=" + API_KEY $ req_test = requests.get(url_test)
c.execute(query1) $ results=c.fetchall() $ df = pd.DataFrame(results) $ df = df.rename(columns={0:"ID",1: "Name",2:"Type"}) $ df
import pandas as pd $ %matplotlib inline $
labels[np.argmax(sample_y1[0])]
twitter_archive_enhanced[twitter_archive_enhanced['name'].str.islower()].name.unique()
tools[0]
Z = np.random.randint(0,10,(3,3)) $ print(Z) $ print(Z[Z[:,1].argsort()])
n_new = df2.query('landing_page == "new_page" and group == "treatment"').user_id.nunique()
raw_sample_sizes = num_authors_by_project.withColumn( $     "sample_size_1", $     compute_num_required_sample_1("author_count")).persist()
np.exp(-0.0150)
%%time $ excelref_df = update_file_links()
def calc_temps(start_date, end_date): $     results = session.query(func.min(Measurements.tobs), func.avg(Measurements.tobs), func.max(Measurements.tobs)).\ $         filter(Measurements.date >= start_date).filter(Measurements.date <= end_date).all() $     return pd.DataFrame(results, columns= ["Avg_temp","Min_temp","Max_temp"]) $ print(calc_temps('2016-08-23', '2016-09-05'))
df_new[['country_CA','country_UK','country_US']] = pd.get_dummies(df_new['country']) $ df_new.head()
import pandas as pd $ import numpy as np
gd_predictions = np.dot(f_arr, theta) $ print(gd_predictions)
recalls = df[df['ACTION REQUESTED'].str.contains('Recall') == True] $ recalls.head(2)
df.truncate(before='2014-MAY-2', after='2014-MAY-3')
import plotly as pltly $ import plotly.tools as tls
yc_depart = yc_merged_drop.merge(departureZip, left_on='Unnamed: 0', right_on='Unnamed: 0', how='inner') $ yc_depart.shape
train.pivot_table(values = 'Fare', index = 'Hour', aggfunc = np.mean)
builder.select(builder.sales_acct_id_data).count()
df['Complaint Type'].value_counts().head(10)
perceptron = Perceptron() $ perceptron.fit(X_train, Y_train) $ Y_pred = perceptron.predict(X_test) $ acc_perceptron = round(perceptron.score(X_test, Y_test) * 100, 2) $ acc_perceptron
ti_tmall.rename(columns={'review_image':'image_url','harvest_product_description':'product_description','retailer_product_code':'rpc','user_id':'username'}, inplace=True) $ ti_tmall['store'] = 'Tmall' $ ti_tmall = ti_tmall[ti_clm] $ ti_tmall.shape
input_data.snow_depth.value_counts()
forecast_steps = 36
speakers.sort(columns = 'id',inplace = True)
df2[df2['user_id'].duplicated()==True]
with open('D:/CAPSTONE_NEW/indeed_com-jobs_us_deduped_n_merged_20170817_113502269355122.json', encoding="utf-8-sig") as data_file: $     json_data2 = j.load(data_file)
offices.head()
EPEXvolumes = dfEPEX[1::2].transpose().stack() # take every second row starting one row in $ EPEXvolumes.index = index # assign generated index to volume data $ EPEXvolumes.head() # verify extracted volume data
len(np.unique(df['user_id']))
d.reshape(3,-1)  #3 rows, number of columns determined automatically.
value_counts = combined_df['State'].value_counts(dropna = False) $ count_results = pd.DataFrame({'State':value_counts.index, 'Count':value_counts.values}) $ count_results = count_results[['State', 'Count']] $ print(count_results)
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_opb_brace_saddle[1][0])) $ perr = np.sqrt(np.diag(pcov_opb_brace_saddle[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']]
retweet_pairs[["FromType","FromName","Edge","ToType","ToName","Weight"]].to_csv("For_Graph_Commons.csv",index=False)
df['x'].unique()
YS1315 = pd.read_csv('yahoostocks1315.csv',parse_dates = True,engine ='c',index_col = 0, sep = ',') $ YS1315
to_be_predicted_Day2 = 14.78510842 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
new_page_converted = np.random.choice(2, size=145310, replace=True, p=[0.8804, 0.1196])
!hdfs dfs -cat 321_sorted-results-output/part-* | head -n50
row_idx = (slice(None), slice(None), slice('Bob', 'Guido'))  # all years, all visits, Bob + Guido $ health_data_row.loc[row_idx, 'HR']
df_data_1['DATE'] = pd.to_datetime(df_data_1['DATE']) $ df_data_1['ID'] = df_data_1['ID'].astype('category') $ df_data_1.dtypes
def GroupColFunc(df, ind, col): $     if df[col].loc[ind] > 0: $         return 'Group1' $     else: $         return 'Group2'
df_mes[df_mes['travel_time'] < 0]
matplotlib.style.use('seaborn')
import pandas as pda $ import matplotlib.pyplot as plt $ from sklearn import datasets, linear_model $ from sklearn.metrics import mean_squared_error, r2_score
text=soup.get_text().encode('ascii','ignore') $ print text
plt.hist(p_diffs); $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('simulated 10,000 values in p_diffs'); $ plt.axvline(x=act_diff,color ='red')
retweets = megmfurr_tweets[megmfurr_tweets['text'].str.contains("RT")] $ megmfurr_tweets[megmfurr_tweets['text'].str.contains("RT")]['text'].count() # 1,633
df_t_best_means.shape
model2 = sm.Logit(df_new.converted, df_new[['intercept','US', 'CA']]) $ result2 = model2.fit() $ result2.summary()
token_sendcnt["ID"] = token_sendcnt.sender $ token_receivecnt["ID"] = token_receivecnt.receiver
sub1['final_solved'] = sub1.groupby(['hacker_id', 'challenge_id'])['solved'].transform('max')
y = x.loc[:,"A"] $ y.mean() $ np.mean(y)
data.map(add_one).collect()
soup = BeautifulSoup(html, 'lxml')
type(people)
plt.bar(years,deaths) $ plt.title('Bar Graph: Celebrity Deaths per year') $ plt.show()
stations = session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).\ $ order_by(desc(func.count(Measurement.station))).all() $ stations
Line_Treatement=df.loc[(df['landing_page']=='new_page') & (df['group']=="treatment"),].shape[0] #Find no of entries where they do line up $ Line_Control=df.loc[(df['landing_page']=='old_page') & (df['group']=="control"),].shape[0] $ print("The total no of times new page and treatment dont line up are",Total_entried-Line_Upentries-Line_Control)
image_predictions_clean['p1_dog'].value_counts()
subway3_df = subway2_df[(subway2_df['Hourly_Entries'] > 0) & (subway2_df['Hourly_Entries'] < 100000)& (subway2_df['Hourly_Exits'] > 0) & (subway2_df['Hourly_Exits'] < 100000)]
np.average(new_page_converted)- np.average(old_page_converted)
names_count = authors_with_name.groupBy("parsed_info.first").agg(F.count("*").alias("names_count")) $ names_count.sort(names_count.names_count.desc()).show()
s=USFederalHolidayCalendar()
data.head()
health_data_row.loc[2013, 1, 'Guido']  # index triplet
d6tstack.utils.read_excel_advanced(cfg_fnames[0], $                                    sheet_name='Sheet1', header_xls_range="B2:E2").head()
mar.info()
for df in (joined,joined_test): $     df.loc[df.CompetitionDaysOpen<0, "CompetitionDaysOpen"] = 0 $     df.loc[df.CompetitionOpenSinceYear<1990, "CompetitionDaysOpen"] = 0
relevance_scores_df = pd.DataFrame(relevance_scores[0]).mean(axis=1) $ relevance_scores_df.describe()
series + pandas.Series({'a': 2})
pl.hist(yc_new4['tipPC'], bins=30) $ pl.ylabel('N') $ pl.xlabel('tipPC') $ pl.title('Distribution of Tips as a percentage of Fare, under 100% and over 1%') $ print('The first moment is 20.44 and the second moment is 9.04')
df.nunique()
speakers.set_index('id',drop = True,inplace = True)
Merge.head()
tweet = result[0] $ for param in dir(tweet): $     if not param.startswith("_"): $         print("%s : %s\n" % (param, eval('tweet.'+param)))
import os $ from IPython.display import display, Latex, Markdown $ from client.api.notebook import Notebook $ ok = Notebook('hw2.ok')
df_final['join_days'] = (now - df_final['first_trx']).dt.days
dates = pd.to_datetime([datetime(2015, 7, 3), '4th of July, 2015', '2015-Jul-6', '07-07-2015', '20150708']) $ dates
reviewsDFslice = reviewsDF[:25924] $ reviewsDFslice.tail(50)
s.resample('Q', label='left').head()
print(n_old) $ print(n_new) $ print(convert_old) $ print(convert_new)
df_new['intercept']=1 $ log_mod= sm.Logit(df_new['converted'], df_new[['intercept','CA', 'UK']]) $ results= log_mod.fit()
table.info()
n_new = df2.query('landing_page == "new_page"')['user_id'].count() $ print(n_new)
reddit_comments_data = reddit_comments_data.withColumn('sentiment',senti_udf(reddit_comments_data.body))
poverty.columns=poverty.iloc[0, :]
%load -s regression_contour teaching_plots.py
data = df_norm(data[39000:]) $ data.plot()
new_page_p = new_page_converted.mean() $ old_page_p = old_page_converted.mean() $ new_page_p - old_page_p
gc.collect()
list1
features['f12'] = features['f07'].dropna().groupby(level='date').rank(pct=True) $
all_tables_df.loc[:, 'OBJECT_NAME']
state_lookup
timestamps_df = pd.DataFrame(times_events, columns=time_columns) $ timestamp_left_df = timestamps_df[timestamps_df['timestamp'] != ""] $ timestamp_left_df.head()
active_companies_psc_applies_unique_count
Magic.__repr__
X = df['subreddit'] $ y= df['HIGH_LOW']
p_old_obs = df2.query('group == "control"').converted.sum() / df2.query('group == "control"').shape[0] $ p_new_obs = df2.query('group == "treatment"').converted.sum() / df2.query('group == "treatment"').shape[0] $ diff_obs = p_new_obs - p_old_obs $ diff_obs
len(df2.query('converted == 1')) / len(df2)
projects.to_csv('data_table/project.csv', index = False)
rain_df = pd.DataFrame(rain) $ rain_df.head()
pd.merge(staff_df, student_df, how = 'outer',left_index= True, right_index= True)
cursor.execute(sq72) $ cursor.execute(sq73) $ results2 = cursor.fetchall() $ results2
c.execute(create_public_table) $ con.commit()
dd2=cfs.diff_abundance('Subject','Control','Patient', random_seed=2018)
twitter_archive_master.loc[(twitter_archive_master.name == 'a'),'name'] = 'Alpha' $ twitter_archive_master.loc[(twitter_archive_master.name == 'his'),'name'] = 'His'
autos = autos[autos["price"].between(200,250000)]
spp_plot = spp[(spp.season.isin(s)) & (spp.term.isin(t))] $ ved_plot = ved[(ved.season.isin(s)) & (ved.term.isin(t))] $ vhd_plot = vhd[(vhd.season.isin(s)) & (vhd.term.isin(t))] $ vwg_plot = vwg[(vwg.season.isin(s)) & (vwg.term.isin(t))]
stream_listener = StreamListener() $ stream = tweepy.Stream(auth=api.auth, listener=stream_listener, tweet_mode="extended")
df.printSchema()
twitter_archive_enhanced_clean = twitter_archive_enhanced.copy()
import time $ t0 = time.time() $ rnd_clf.fit(X_train, y_train) $ t1 = time.time()
df2.query('group == "treatment"')['user_id'].nunique() / total_users
df_Inner_trans_users = pd.merge(transactions,users,how="inner",on="UserID") $ df_Inner_trans_users
def str_merge(str1, str2): $     return "{} {}".format(str1, str2)
x_normalized = intersections_irr[for_normalized_columns].values.astype(float)
plt.figure() $ plt.hist(df['log_price'],bins=100) $ plt.show()
proportion = len([i for i in p_diffs if i > (prob_treatment_converted-prob_control_converted)])/len(p_diffs) $ proportion
df2 = df[['patient_id', 'ltv', 'num_consults', 'tag', 'age', 'doc_id', 'rx_requested','account_created', $             'first_consult', 'latest_consult', 'rx_list', 'oral_rx', 'female', 'state_num', 'treatment', $             'days_active', 'days_repayment', 'avg_c_gap', 'category']]
fav = twitter_final.filter(['favorite_count','dog_species']).sort_values(by='favorite_count',ascending=False).head(10) $ fav.plot.bar('dog_species')
df2.groupby(['group'])['converted'].mean()[1] - df2.groupby(['group'])['converted'].mean()[0]
data['Location']= data['Location'].astype(str) $ locations = pd.DataFrame(data['Location'].str.split(',').tolist(), columns = ['City','State','blah','',',']) $ locations = locations[['City','State']] $ locations.head()
from summa import summarizer
honeypot_df['ts_unix'] = honeypot_df['time'].apply(lambda x: pd.to_datetime(x).timestamp())
data = pd.read_csv('datafinal.csv') $ xprep = data.copy() $ xprep.drop('irlco', axis=1, inplace=True) $ xprep.columns.values[2] = '1hr'
image_predictions=pd.read_csv('image_predictions.tsv', sep='\t') #load the data of predictions $ image_predictions.head()
import sqlite3 $ sqlite_file = 'mydb' $ cnx = sqlite3.connect(sqlite_file) $ c = cnx.cursor()
to_be_predicted_Day4 = 25.12100163 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
X_train, X_test, y_train, y_test = train_test_split(X_features,tweets_1['sentiment'], test_size = .2) $
store_items = store_items.drop(['store 2', 'store 1'], axis = 0) $ store_items
for x in range(5): $     print Matt[x]['text'] $     print('--')
breakfastlunch = pd.merge(breakfast, lunch, on='STATION') $ breakfastlunchdinner = pd.merge(breakfastlunch, dinner, on='STATION') $ breakfastlunchdinner
s = pd.Series([95, 61, 17, np.nan, 87, 13]) $ s
df.drop(index=100, inplace=False)       # single row
def print_rmse(model, name, input_fn): $   metrics = model.evaluate(input_fn=input_fn, steps=1) $   print 'RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['loss'])) $ print_rmse(model, 'validation', make_input_fn(df_valid))
print('There is a {:.2f}% chance of being readmitted within 30 days'.format(100*full['WillBe<=30Days'].mean())) $
contractor_clean['zip_part1'] = contractor_clean.zipcode.str.split('-').str[0] $ contractor_clean['zip_part2'] = contractor_clean.zipcode.str.split('-').str[1]
plt.figure(figsize=(16, 5)) $ plt.plot(news.date, news.title.apply(len));
df_clean.info()
df['fileCount'].describe()
texts[5]
topics_top_doc = docs_by_topic.idxmax()[['TopicWt']] $ topics_top_doc['DocText'] = [submissions.iloc[x]['text'] for x in topics_top_doc['TopicWt']] $ topics_top_doc
df['what_is_car_model'] = df.apply(lambda x: basic_tagging(x['Keyword'], tags), axis=1)
mod = sm.tsa.statespace.SARIMAX(model_df.close, trend='n', order=(1,1,3), seasonal_order=(0,1,1,80)) $ results = mod.fit() $ results.summary()
from numpy import * $ import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ from collections import Counter $
shifts_w_pauses = df[df['pause'].astype('timedelta64[m]') > 0] $ user_counts = shifts_w_pauses.groupby('user').count()['duration'] $ print('Users who use pauses: {}\n\n'.format(len(user_counts))) $ print(user_counts)
model.fit(predictors, target)
stages_with_decision_tree = stages + [dt]
link_pattern = '\[[\w ]*\]\([\w+]*[://]*[w]*[/.][\w]*[/.][/\w-]*\)'
weekly_hashtag_window = Window.partitionBy('week', 'hashtag')\ $                             .orderBy(functions.desc('count'))\ $                             .rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)
df2 = df2.drop_duplicates('user_id')
bwd_train = roll_train_df.rolling(7,min_periods=1).sum() $ bwd_test = roll_test_df.rolling(7,min_periods=1).sum()
dates = ['2017-July-02', 'Aug 03 1985', '3/17/19'] $ df = pd.DataFrame(list('abc'), index = pd.to_datetime(dates) ) $ df.index
sub_df.shape
url_data = pd.DataFrame.from_csv(MONGO_URL_DATA_FILE_PATH)
prob_group2 = df2.query("group == 'treatment'")["converted"].mean() $ print("In the 'treatment' group the probability they converted is {}.".format(prob_group2))
user_age_udf = udf(get_user_age, IntegerType())
top_supports = support.groupby(["contributor_firstname", "contributor_lastname"]).amount.sum().reset_index().sort_values("amount", ascending=False).head(10)
1/np.exp(-0.0408),np.exp(0.0099)
merged1 = pd.merge(left=appointments, right=reason_for_visit, how='left', left_on='MeetingReasonForVisitId',\ $                   right_on='Id')
sox.ix[(pd.to_datetime(sox.date) >= dt.datetime(2013,10,4)) & (pd.to_datetime(sox.date) < dt.datetime(2014,1,1)),'playoff'] = 1
url=("/Users/maggiewest/Projects/cpi3.xlsx") $ cpi = pd.read_excel(url)
X_test
df_goog['Year'] = df_goog.index.year $ df_goog['Month'] = df_goog.index.month $ df_goog['Day'] = df_goog.index.day $ df_goog.head(2)
users = users[email_bool] $ users.info()
merged_NNN.amount.sum()
pd.date_range('2005', periods=4, freq='Q')
coin_mean > .5
p = sns.factorplot('vehicleType',data=autodf,kind='count') $ p.set_xticklabels(rotation=30) #letitbe
help('modules')
so[(so['score'] >= 100) | (so['answercount'] >= 10)].head()
fig,axes = plt.subplots(1, 2, figsize = (16,4), sharey= True) $ axes[0].plot_date(x=obama.created_at, y = obama.n_uwords,linestyle = '-',marker='None') $ axes[1].plot_date(x=trump.created_at, y = trump.n_uwords,linestyle='solid',marker='None') $ plt.savefig("fig/n_uword_comparison.png")
newfile.to_excel('most_excellent.xlsx', sheet_name='test_sheet')
df_dd.shape $ df_dd.head()
compiled_data[pd.isnull(compiled_data['botometer'])==True][0:2]
df[df['SA'] < 0].subreddit.value_counts()[:3]
x-m  #the trailing dimension matches, and m is stretched to match the 2 rows of x.
group.shift(1)[['inspection_date', 'results']]
data.plot()
vectorizer = TfidfVectorizer(analyzer = "word", tokenizer = None, preprocessor = None, stop_words = None, max_features = 20000) 
soup.find_all(class_='chorus')
predictions = model.predict(x_test)
plantlist[plantlist['commissioned'] <= 1900]
df_long
df = pd.DataFrame({'one' : pd.Series(np.random.randn(3), index=['a', 'b', 'c']), $     'two' : pd.Series(np.random.randn(4), index=['a', 'b', 'c', 'd']), $     'three' : pd.Series(np.random.randn(3), index=['b', 'c', 'd'])}) $ df
sl.loc[sl.wpdx_id=='wpdx-00063117']
m = Prophet()
package.descriptor['resources'][1]['name'] = 'winemag-reviews' $ package.descriptor['resources'][1]
count1df = pd.DataFrame(chef02) $ count1df = count1df.drop_duplicates(subset=['name', 'user']) $ count1df.info()
df2['intercept'] = 1 $ df2[['ab_page', 'old_page']] = pd.get_dummies(df2['landing_page']) $ df2.drop(['old_page', 'landing_page', 'group'], axis = 1, inplace = True) $ df2.head()
df_new[['old_page','new_page']]=pd.get_dummies(df_new['landing_page']) $ df_new=df_new.drop('old_page',axis=1)
base_col = 't' $ df.rename(columns={target_column: base_col}, inplace=True)
%matplotlib inline $ plt.plot(games[games['TEAM_ABBREVIATION_HOME'] == 'ATL'].sort_values(by='GAME_DATE_HOME')['CumulFT_HOME']) $ plt.ylim(0,1.0)
print('The proportion of users converted is {}.'.format(round(df['converted'].mean(),4)))
pgh_311_data.index = pd.to_datetime(pgh_311_data['CREATED_ON']) $ pgh_311_data.head()
df_tweet_data.tweet_id.nunique()
data_scaled.head()
s2 = s.reindex(['a', 'c', 'e', 'g']) $ s2['a'] = 0 $ s2
df.groupby('author').count().sort_values('text',ascending=False).nlargest(10,'text')
df['word_count'] = df['word_count'].round(2)
authors_grouped_by_id = distinct_authors_with_gh.groupBy("project_name", "new_unique_id").agg( $     collect_set(F.col("email")).alias("emails"), $     F.last(F.col("Author")).alias("Author"), $     F.first("github_username").alias("github_username"), $     F.max("latest_commit").alias("latest_commit")) $
dictionary $ shape = [len(a) for a in dictionary.values()]
import json $ import pandas as pd $ pd.options.display.max_colwidth = 1024
df_with_ctr = pd.merge(df, ctr, how="left", on=["Rank", "Device"])
tokendata.head()
mentioned_bills_all['votes_api_url_2'] = mentioned_bills_all['votes_api_url'].fillna('')
import pandas as pd $ step_data = [3620, 7891, 9761, 3907, 4338, 5373]
def get_body_media(the_posts): $     list_of_body_media = [] $     for i in list_Media_ID: $         list_of_body_media.append("https://www.instagram.com/p/" + i) $     return list_of_body_media
df = df.sort_values(by=['date']) $ df = df.rename(columns={'prcp':'precipitation'}) $ df.head(10)
model3.summary()
Measurement = Base.classes.measurement $ Station = Base.classes.station
convert_old = df2[(df2["landing_page"] == "old_page") & (df2["converted"] == 1)]["user_id"].count() $ convert_new = df2[(df2["landing_page"] == "new_page") & (df2["converted"] == 1)]["user_id"].count() $ n_old = n_old $ n_new = n_new
import datetime as dt $ Todays_date = dt.date.today() - dt.timedelta(days=365) $ print("Today's Date:", Todays_date)
mydata.to_csv("Data-5year-2012-20180617.csv")
import pandas as pd $ import numpy as np
autos['registration_year'].describe()
unemp.columns = ['quarter', 'rate']
max(change_closingrice)
posts_df.to_csv("dataset_and_features.csv",encoding="utf8",index=False)
X_unseen = df.iloc[len(df)-len(df_test) : len(df)+1] $ y_unseen = df['bound_at'].iloc[len(df)-len(df_test) : len(df)+1]
priors_product_purchase_spec= priors_product.groupby(["user_id","product_id"]).size().reset_index(name ='purchase_count_spec') $ priors_product_purchase_spec['userprod_id']=priors_product_purchase_spec['product_id'] + priors_product_purchase_spec['user_id'] *100000 $ priors_product_purchase_spec.head(10)
pax_raw.info()
def list_dataset(name,node): $     if isinstance(node, h5py.Dataset): $         print(name) $ f.visititems(list_dataset)
df_twitter_archive.sample(25)
mean_new - mean_old
import requests $ import collections $
tweet_data.to_csv('twitter_archive_master.csv', index= False)
print(example_model.get('coefficients'))
print(df_by_donor.columns.get_level_values(0)) $ print(df_by_donor.columns.get_level_values(1))
analyze_set.plot(kind='scatter',x='favorites',y='retweets', alpha = 0.4) $ plt.xlabel('Favorites') $ plt.ylabel('Retweets') $ plt.title('Retweets and Favorites') $ plt.savefig('a.png')
df_sorted_by_date.describe()
nasa_url = "https://mars.nasa.gov/news/" $ response = requests.get(nasa_url)
weights, sentiments = sentiment_analyzer(df19.Tweet) $ df19["Senitments"] = sentiments $ df19["Weights"] = weights
%%bash $ python3 generate_tfrecord.py $ mv test.record data $ mv train.record data
from datetime import datetime $ ELEC_DATE = datetime(2016, 11, 8) $ INAUG_DATE = datetime(2017, 1, 20)
raw = raw[~(raw['posted'] == '30+ days ago')]
df3.iat[2, 3] = 0 $ df3
rating_error_rows =[] $ for i in range(len(twitter_archive)): $     if str(twitter_archive['rating_numerator'][i]) not in twitter_archive['text'][i]: $         rating_error_rows.append(i) $ print(rating_error_rows)
archive_clean.info()
cats_df = cats_df.set_index(['Unnamed: 0']) $ cats_df.index.name = 'seq' $ cats_df['remove'] = False #Used to identify samples to be removed
es.get(index='sw', doc_type='people', id=4)
X = preprocessing.StandardScaler().fit(X).transform(X) $ X[0:5]
len(users) $ users.drop(users[users['friends_count'].str.contains(r'[A-Za-z]') == True].index, inplace=True) $ len(users)
df['director'] = df['director'].fillna('Unknown') $ df['production_companies'] = df['production_companies'].fillna('Unknown') $ df['genres'] = df['genres'].fillna('Unknown')
proportion = df[df['converted']==1].user_id.nunique()/number_rows $ print('The proportion od users converted is {}'.format(proportion))
xml_in['authorName'].shape
len(users) $ users.drop(users[users['followers_count'] == 'followers_count'].index, inplace=True) $ len(users)
rfr = RandomForestRegressor(random_state=0) $ cv_score = cross_val_score(rfr, features_regress_vect, overdue_duration, $                            scoring='neg_median_absolute_error', cv=5) $ 'MedianAE score: {:.3f}, std: {:.3f}'.format(np.mean(cv_score), np.std(cv_score))
z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new], [n_old, n_new],alternative='smaller') $ print(z_score,p_value)
df_h1b_mv_ft = df_h1b_mv[df_h1b_mv.full_time_pos=='Y'] $ print('There are {:.0f} visa applications for full-time jobs located in Mountain View in this dataset.'.format(df_h1b_mv_ft.shape[0])) $
100*np.exp(fundret.loc[date:].cumsum())
table_layouts = dict() $ for dataset_name in layouts: $     table_layouts[dataset_name] = (Row(layouts[dataset_name], create_data_table(panel_summary[dataset_name])))
to_be_predicted_Day2 = 43.30506441 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
df_img_predictions.info()
find_id_num('METHODIST HEALTHCARE MEMPHIS HOSPITALS')
pd.set_option('display.max_colwidth', -1)
products
test_collection = db.test_collection
ds = remote_data.sel(time='2014-08-19').sum('time') $ print ds
import statsmodels.api as sm
twitter_df_clean = twitter_df_clean.drop(['doggo', 'floofer','pupper','puppo'], axis=1)
df_imputed = pd.DataFrame(df_imput)
data.tail(-2)
joined = joined.astype(intDict)
!wget http://files.fast.ai/data/aclImdb.tgz --directory-prefix=data
all_df.head(2)
hidden_states = embedding_model.predict(test_vecs[:, 1:])
AgeVideo = [] $ for i in df['PublishedDate']: $     AgeVideo.append(convert_to_days(str(i))) $ df['AgeVideo'] = AgeVideo
r = requests.get('https://{}/api/v3/orgs/{}/repos'.format(baseurl, source_org), \ $          headers=header, params={ 'type': 'all' , 'page': 4 })
df_thermal = df_downsampled['thermal'] $ df_downsampled = df_downsampled.drop(columns=['thermal'])
iris.head().iloc[:,:1]
df_train.loc[:, 'is_attributed'] = df_train.loc[:, 'is_attributed'].astype(np.int8) $ df_valid.loc[:, 'is_attributed'] = df_valid.loc[:, 'is_attributed'].astype(np.int8) $ df_test.loc[:, 'is_attributed'] = df_test.loc[:, 'is_attributed'].fillna(0).astype(np.int8)
sns.heatmap(data_for_model.corr(),cmap='coolwarm',annot=True)
df2.shape[0]
df['text'] = df['text'].astype(str).str.lower() $ Counter(" ".join(df["text"]).split()).most_common(100)
regr = linear_model.LinearRegression()
rows = df.shape[0] $ df = df.dropna()
import pyodbc $ import pandas as pd $ import datetime
train_holiday_oil_store_transaction_item_test_004 = train_holiday_oil_store_transaction_item_test_004.join(holiday_events_df, 'date', 'left_outer') $
intervention_train.loc[:, columns_with_nan].head(100)
ix = df_test.index[0] $ ix
df_countries.country.nunique()
data.loc[(100, slice(None), 'put'),'Vol'].head()
hawaii_station_df = pd.read_csv(r"\Users\Josue\Desktop\\hawaii_stations.csv") $
Google_stock.isnull().any()
test_float['balcon'] = 0 $ test_float.loc[test_float.description.str.contains('balcon|terraza', na=False), 'balcon'] = 1 $ test_float.balcon.value_counts()
prop.plot(kind='bar') $ plt.title('Is the Donor a Teacher?') $ plt.show()
eclf3 = VotingClassifier(estimators=[('lr', alg), ('calC', alg7), ("ranFor", alg2), ("DT", alg6)], voting='soft') $ eclf3.fit(X_train, y_train) $ probs = eclf3.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
predictions = model.predict(x_test, batch_size=1024, verbose=1)[0:len(X_test)] $ prediction_df = pd.DataFrame(predictions, columns=["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]) $ combined_df = new_comments_df.join(prediction_df) # join the comment dataframe with the results dataframe $ combined_df.head(15)
cur = conn.cursor() $ cur.execute('UPDATE actor SET first_name = \'HARPO\' WHERE first_name ilike \'Groucho\' and last_name ilike \'Williams\';') $
yhat = neigh.predict(X_test) $ yhat[0:5]
%%timeit -n1 -r2 $ T4 = get_rental_concession_vec(rentals['description'])
cursor=db.bookings.aggregate([{"$group" : {"_id" : "$status", "num_status" : {"$sum" : 1}}}]) $ df =  pd.DataFrame(list(cursor)) $ df $
df2.converted.mean() $
df_DRGs.head(2)
xmlData.describe()
data2=data.set_index("date") $ data2.head()
top20
titanic.count()
jobPostDF = pd.read_csv("data/data job posts.csv")
df_errors = pd.DataFrame(new_errors, columns = ['tweet_id']) 
df[['Principal','terms','age','Gender','education']].head()
p__old = df2['converted'].mean() $ p__old
df2 = df2.drop(df2[(df2.user_id == 773192) & (df2['timestamp'] == '2017-01-09 05:37:58.781806')].index) $ df2[df2['user_id'] == 773192]
AAPL.mean()
df2['converted'].sum() / df2.shape[0]
engine.execute( $     "SELECT tc.constraint_name,  kcu.column_name FROM  information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name where tc.TABLE_NAME =  'contractor'").fetchall()
data['Incident Zip'].unique()
df.groupby("pickup_month")["cancelled"].mean()
archive_clean = archive_clean[archive_clean['in_reply_to_status_id'].isnull()] $ archive_clean = archive_clean[archive_clean['retweeted_status_id'].isnull()]
fig,ax=plt.subplots(1,2,figsize=(15,3)) $ ax[0].boxplot(joined['CompetitionOpenSinceYear'],vert=False) $ ax[1].boxplot(joined['CompetitionDaysOpen'],vert=False)
result = logit.fit() $ result.summary2()
for i in image_predictions_rows: $     if i in list(image_predictions_clean.index): $         print("Failed, {} still in dataframe".format(i)) $         break
train_df["description_score"] = train_df['description'].apply(process)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
df['Clean Tweets'] = np.array([clean_tweet(tweet) for tweet in df['Tweets']])
new_page_converted = np.random.choice([1, 0], size=len(df2_treatment.index), p=[df2.converted.mean(), (1-(df2.converted.mean()))])
sum(df[df['group'] == 'treatment']['landing_page'] != 'new_page') + sum(df[df['landing_page'] == 'new_page']['group'] != 'treatment')
rng_berlin[5]
distinct_authors_latest_commit.schema
obs_diff = p_treatment_convert - p_control_convert $ obs_diff
df_bq[df_bq.day_created.isin(days_unmatched)].sort_values('PaymentDate')
df2[df2['user_id']==773192]
df1 = pd.DataFrame([pd.Series(np.arange(10, 15)), pd.Series(np.arange(15, 20))]) $ df1
mentions = api.GetMentions() $ print([m.text for m in mentions])
missing_values = missing_values_table(perf_test) $ missing_values.head(20)
countries_df = pd.read_csv('countries.csv')
df_new.query('group == "treatment" and country =="UK"')['converted'].mean() $
year_labeled=2017 $ year_predict=2018 $ description_labeled = df[df.year==year_labeled]['description'] $ description_predict = df[df.year==year_predict]['description']
values = tweetsIRMA[['tweet_lat','tweet_long','irma_lat','irma_long']]
np.shape(rhum_fine)
blob.polarity
df = read_csv("thegatewaypundit.csv") $ df.shape
zone_train.shape $
data.head(1)
unique_user = df['user_id'].nunique() $ unique_user
pd.unique(df_users.creation_source.values)
Mars_df = Mars_tables[0] $ Mars_df.columns = ['0', '1'] $ Mars_df.rename(columns = {'0':'Mars Information', '1': 'Facts'}, inplace = True) $ Mars_df
pca_df.head()
f1_score(Y_valid, final_valid_pred_nbsvm1, average='weighted', labels=np.unique(final_valid_pred_nbsvm1.values))
a[a.find(':')]
mig_l1 = mi.groupby(level=0) $ print_groups(mig_l1)
chefdf = chefdf.drop_duplicates(subset=['name', 'user']) $ len(chefdf.name)
frames_Sl = [first_values_Sl, last_values_Sl, count_Sl] $ result_Sl = pd.concat(frames_Sl, axis=1)
stoplist = stopwords.words('english') $ texts = [[word for word in pape.split() if word not in stoplist] for pape in all_text]
def next_nearest_id(row): $     this_date = pair.DATETIME[row.pair_id] $     next_id = nearest_id(this_date + np.timedelta64(1, 'D')) $     next_date = pair.DATETIME[next_id] $     return next_id if ((next_date - this_date) > np.timedelta64(20, 'h')) else None $
plt.plot(ds_ossm['time'],ds_ossm['met_salsurf'],'r.') $ plt.title('CP04OSSM, Sea Surface Salinity') $ plt.ylabel('Salinity') $ plt.xlabel('Time') $ plt.show()
q_specific = c.submit_query(PTOQuerySpec().time("2018-06-10", "2018-06-12") $                                              .target("170.218.212.80"))
fires = pd.read_csv('../data/model_data/1979-2016_fire_likelihood.csv', index_col=0) $ fires['date'] = pd.to_datetime(fires['date']) $ fires['state'] = fires['state'].astype('category') $ fires['month'] = fires['month'].astype('category') $
y = X.iloc[:,-1] $ X = X.iloc[:,0:-1] $ col_names = X.columns $ print(X.head(5)) $ print("\n",y.head(5))
pd.set_option('display.float_format', lambda x: '%.3f' % x)
closeSeriesR.head()
p_diffs = np.array(p_diffs) $ ab_data_diff = df[df['group'] =='treatment']['converted'].mean() - df[df['group'] =='control']['converted'].mean() $ (ab_data_diff <p_diffs).mean()
df.user_id.nunique() # to get the unique user id counts
ma.columns = ['game_id', 'team_id_ta', 'season', 'pts_la', 'ast_la', 'blk_la', 'reb_la', 'stl_la', $              'defenders_a', 'facilitator_a', 'game_winners_a', 'inside_gamers_a', 'normal_a', 'pure_scorers_a', 'useless_a'] $ mb.columns = ['game_id', 'team_id_tb', 'season', 'pts_lb', 'ast_lb', 'blk_lb', 'reb_lb', 'stl_lb', $              'defenders_b', 'facilitator_b', 'game_winners_b', 'inside_gamers_b', 'normal_b', 'pure_scorers_b', 'useless_b']
dulp.columns
z_score,p_value = sm.stats.proportions_ztest(count=[convert_old,convert_new],nobs=[n_old,n_new],alternative='smaller') $ z_score,p_value
weekly = data.resample('W').sum() $ weekly.plot(style=[':', '--', '-']) $ plt.ylabel('Weekly bicycle count');
p_new - p_old
df2['user_id'].nunique()
df.info()
content_performance_bytime['date'] = pd.to_datetime(content_performance_bytime['dimensions_date_id'])
session.query(func.count(Station.station)).all() $
pre = get_pre_survey()
teams = pd.read_sql_query('select * from Team', conn)  # don't forget to specify the connection $ print(teams.shape) $ teams.head()
df1_clean = df1 $ df2_clean = df2 $ df3_clean = df3
feature_importances[:10].plot.bar()
corpus = [[(1, 0.5)], []]  # make one document empty, for the heck of it $ corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.mm'), corpus)
df_usertran = pd.merge(users,transactions,how='left',on='UserID') $ df_ = df_usertran.drop_duplicates(subset='UserID') $ df_ = df_.reset_index(drop=True) $ df_
df.sort_values("stamp",inplace=True) $ df.head()
year10 = driver.find_elements_by_class_name('yr-button')[9] $ year10.click()
pres_df['state'] = pres_df['split_location_tmp'].map(lambda x: x[1]) $ pres_df['state'].head()
dat[dat.x.isnull()]
pax_path = 'paxraw_d.csv' $ %time pax_raw = pd.read_csv(os.path.join(data_dir, pax_path), dtype=type_map)
final['Crime Count in 5k02'] = final['Crime Count in 5k02'].fillna(0)
model = sm.OLS.from_formula('arrests ~ game_result', stadium_arr) $ results = model.fit() $ print(results.summary())
load2017inh= load2017[load2017.index % 4 == 0] $ load2017inh.head()
findNumbers = r'\d+' $ regexResults = re.search(findNumbers, 'not a number, not a number, numbers 2134567890, not a number') $ print(regexResults.group(0))
highest_station = active_stations_data[0][0] #Storing the highest station as a variable $ highest_station
target_var = 'win_differential' $ y_data = use_data.loc[:, target_var]
df_temp = df.drop_duplicates("user_id") $ df_temp.converted.mean()
autos["odometer_km"].value_counts().sort_index(ascending=True)
import pickle
actual_new_page_converted=df2.query('landing_page=="new_page"')['converted'].value_counts()[1]/\ $ df2.query('landing_page=="new_page"').shape[0]
pd.value_counts(dr_new['ReasonForVisitName'])
from sqlalchemy import and_,or_,not_
merged2 = merged2.fillna(0)
df['6/3/2015':'6/4/2015']
conn.upload('/u/username/data/iris.csv', casout=dict(name='iris2', caslib='casuser'))
df.head()
[x[:6] for x in results_single[:3]]
df = actuals.merge(backcast, on='Gas_Date')
tl_list = [1, 3, 10, 30, 60, 90, 120, 150, 180, 210, 240, 300] $ tl_list2 = [180] $ cross_validation(tl_list)
df_columns[ ~df_columns['Descriptor'].isnull() & df_columns['Descriptor'].str.contains('Loud Music/Party') ]['Day of the week'].value_counts() $
clean_rates.cuteness.value_counts()
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], $                                               nobs=[n_new, n_old],alternative='larger') $ print("z-score:", z_score,"\np-value:", p_value)
factor_ts = index_ts.pivot(index='price_date', columns='ticker', values=['adj_close_price'])['adj_close_price'] $ factor_ts.head()
page.text
tweets["created_at"] = pd.to_datetime(tweets["created_at"]) $ tweets.head()
api_token = os.environ['GITHUB_API_TOKEN'] $ headers = {'Authorization': f'token {api_token}'}
for t in topics: $     df[t] = False
import netCDF4 as nc $ import pandas as pd $ import matplotlib.pyplot as plt $ plt.rcParams['figure.figsize'] = [15, 7]
np.exp(-0.015)
wikiMeritRequest = requests.get(wikipedia_meritocracy) $ print(wikiMeritRequest.text[:1000])
twitter_merged_data.plot('stage',  kind='bar'); $ plt.title('Stage Bar') $ plt.xlabel('Stage') $ plt.ylabel('Rating');
y_pred_mdl = mdl.predict(X_test) $ y_train_pred_mdl=mdl.predict(X_train) $ print("Accuracy of logistic regression classifier on on test set: {:0.5f}".format(mdl.score(X_test, y_test)))
df.head()
browser = webdriver.Chrome('chromedriver.exe') $ browser.get(url) $ time.sleep(5) $ html = browser.page_source $ soup = BeautifulSoup(html, "html.parser") $
df_con.head()
freq_df = pd.DataFrame(freq, columns=['date', 'tobs']) $ freq_df.set_index('date', inplace=True, ) $ freq_df.head()
df2[df2.duplicated(subset='user_id')==True]['user_id']
lr.fit(X_train,y_train)
df_goog.Date $ type(df_goog.Date.loc[0])
print df.shape[0] + noloc_df.shape[0]
reconstruction_targets = tf.cond(mask_with_labels, # condition $                                  lambda: y,        # if True $                                  lambda: y_pred,   # if False $                                  name="reconstruction_targets")
liberia_data.columns = ['Bomi County', 'Bong County', 'Date', 'Gbarpolu County', 'Grand Bassa', $        'Grand Cape Mount', 'Grand Gedeh', 'Grand Kru', 'Lofa County', $        'Margibi County', 'Maryland County', 'Montserrado County', 'Totals', $        'Nimba County', 'River Gee County', 'RiverCess County', 'Sinoe County', $        'Unnamed: 18', 'Description']
measure_df.head() $
def senior(x): $     if 'Senior' in x: $         return 1 $     return 0 $ df_more['Senior'] = df_more['Title'].apply(senior)
countries_df = pd.read_csv('./data/countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
df.drop(['p1', 'p1_conf', 'p1_dog', 'p2', 'p2_conf', 'p2_dog', 'p3', 'p3_conf', 'p3_dog',], axis=1, inplace=True)
tbl2 = pd.concat([amzn,gspc], axis=1, join='outer')[1:] $ tbl2.columns = ['AMZNr','GSPCr'] $ tbl2['250beta'] = tbl2['GSPCr'].rolling(window=250).cov(tbl2['AMZNr'])/tbl2['GSPCr'].rolling(window=250).var() $ tbl2['250alpha'] = tbl2['AMZNr'].rolling(window=250).mean() - tbl2['250beta'] * tbl2['GSPCr'].rolling(window=250).mean() $ tbl2.tail()
events['categories'] = events.category.apply(lambda l: json.loads(l)) $ events.categories.iloc[0]
df_tweets[df_tweets.oembed.isnull()]
master = master.drop_duplicates(subset='hash') $ master = master.reset_index(drop=True) $ final_len = len(master)
week21 = week20.rename(columns={147:'147'}) $ stocks = stocks.rename(columns={'Week 20':'Week 21','140':'147'}) $ week21 = pd.merge(stocks,week21,on=['147','Tickers']) $ week21.drop_duplicates(subset='Link',inplace=True)
(pd.Series(list_tables_with_insert(dumpfile)).value_counts()).sort_index()
data["author"].describe()
breed_predict_df_clean['tweet_id'] = breed_predict_df_clean.tweet_id.apply(str) $
weekdays = [] $ for date in logins['datetime']: $     weekdays.append(date.weekday()) $ logins['weekday'] = weekdays
all_gen1_verse=[] $ for verse in gen1: $     gen1verse = sent_tokenize(verse) $     all_gen1_verse.extend(gen1verse) $ print(all_gen1_verse)
countries = wb.get_countries() $ countries.iloc[0:10].ix[:,['name','capitalcity','iso2c']]
vs = len(itos) $ vs,len(trn_lm)
props.prop_name.value_counts().reset_index()
train_fs2=kd_utils.load_data('../feature_data/train_fs2.pkl') $ test_fs2=kd_utils.load_data('../feature_data/test_fs2.pkl')
ls
topic_names_filepath = os.path.join(intermediate_directory, 'topic_names.pkl') $ with open(topic_names_filepath, 'w') as f: $     pickle.dump(topic_names, f)
gender_counts
df_nuevo = df.sample(3005)
cust_data.MonthlyIncome = cust_data.MonthlyIncome.astype(str) $ cust_data.dtypes $ table.column.astype(float) $ stores.TotalSales.astype(str)
df = PredClass.df_model $ print df.dtypes.loc[df.dtypes == 'object']
target.shape
rfmodel.score(X_test,y_test)
import pyLDAvis.gensim $ pyLDAvis.enable_notebook() $ data = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False) $ pyLDAvis.save_html(data, 'data/lda.html')
(p_diffs > obs_diff).mean()
full_globe_temp.index
dc_counter_frame.to_csv('dc_count.csv') $ tm_counter_frame.to_csv('tm_count.csv') $
vsim = 0.00544
train = import_all(data_repo + 'intervention_train.csv')
stepwise_model.fit(train2)
b = 'hmmm...what does this do to a?' $ print a
pd.DataFrame(sorted([[n, len(g)] for n,g in pd.groupby(topics, by=topics.name)], key=lambda x: x[1])[-9:], columns=["name", "count"])
goals_df[['PKG', 'PKA']] = goals_df['PKG/A'].str.split('/', expand=True) $ goals_df.drop('PKG/A', axis=1, inplace=True)
start = datetime.now() $ model_svm = SVC(C=1.0, kernel='rbf', degree=3, gamma=.05) $ model_svm.fit(Xtr.toarray(), ytr) $ print( model_svm.score(Xte.toarray(), yte)) $ print((datetime.now()-start).seconds)
files = os.listdir("data")
print('No. of rows in Dataset:',df.shape[0])
repos.forked_from = pd.to_numeric(repos.forked_from)
temp_df 
from sklearn.feature_selection import SelectFromModel
cig_data_SeriesCO[1:] + cig_data_SeriesCO[:-1]
autos.head()
y = df['effortProj'].values $ df.drop('effortProj',axis=1,inplace=True) $ df.drop('projID',axis=1,inplace=True) $ X = df.values
raw.name.values[0], raw.name.values[10]
xs = xs_df.as_matrix() $ ys = ys_df.as_matrix()
print("Probability of treatment group converting:", df2[df2['group']=='treatment']['converted'].mean())
df_bthlst = pd.read_csv('../../data/essentials/md_paths_v3.csv', index_col=[0]) $ df_bthlst = df_bthlst[~df_bthlst["client_display_name"].isnull()]
strategy.df_data().head()
trainData, testData = dataset.randomSplit([0.6, 0.4])
drop_table(cur_a, 'customer_male') $ drop_table(cur_b, 'customer_female')
data.drop_duplicates()
df_final.head(1)
hawaii_measurement_df.head()
df_geo = pd.DataFrame(sub_data["id_str"]).reset_index(drop=True) $ df_geo["geo_code"] = geo_code $ df_geo.head()
import statsmodels.api as sm $ convert_old = sum(df2.query('group == "control"')['converted']) $ convert_new = sum(df2.query('group == "treatment"')['converted']) $ n_old = len(df2.query('group == "control"')) $ n_new = len(df2.query('group == "treatment"'))
%matplotlib inline $ twitter_archive_full[twitter_archive_full.stage != 'None'].stage.hist().set_title('Distribution by stage') $ twitter_archive_full[twitter_archive_full.stage != 'None'].groupby('stage').tweet_id.count()
dat_dow.vgplot.line(value_name='Hospital mortality rate')
unique_usrs =(df.user_id.unique()) #unique users in df dataset $ len(unique_usrs) #number of unique users in the df dataset
ab_data_nr_rows = ab_data.shape[0] $ ab_data_nr_rows
popularity.info()
token = pd.concat(tokenli,0) $ token.index = range(len(token))
print(df2['converted'].mean())
autos = autos.rename(columns={"odometer": "odometer_km"})
temp = df2[(df2['user_id']==773192)] $ print(temp)
p_new = new_page_converted.mean() $ p_old = old_page_converted.mean() $ p_new - p_old
conn_b.commit()
grouped1.sort_values(ascending=False)
journalist_retweet_gender_summary(journalists_retweet_df) $
y=dataframe1['CCI_30'] $ plt.plot(y) $ plt.show()
tmax_day_2018
df['is_member'] = ["Member" if date is not None else "Not Member" for date in df['purchase_date']] $ df.head(3)
average_cost = 210285 / count_google_after_april $ average_cost
df.groupby('Complaint Type').count().sort(desc("count")).show(10)
merge[merge.columns[45]].value_counts() $
nold = len(df2.loc[(df2['landing_page'] == 'old_page')]) $ nold
tz_pytz = pytz.timezone('Europe/London')
person = df[df["screen_name"]==person] $ person.head(100)
X_test.iloc[0:5,:12]
df.head()
weather_parquet = join(weather_dir, zip_file[:-3]+'parquet') $ print(weather_parquet) $ df = sqlContext.read.load(weather_parquet) $ df.show(1)
db_connection = ittconnection('prodcopy') $ query = (" SELECT * FROM signal_signal WHERE `signal` = 'ANN_simple' ") $ signals_df = pd.read_sql(query, con=db_connection) $ signals_df.tail(6)
X_new = test_tfidf.loc[:207] $ new_x = X_new.iloc[:,index_smote] $ new_y = test_tfidf['y']
outlier_detection.outlier_detection_1d(cutoff_params=outlier_detection.basic_cutoff).head(20)
cityID = '946ccd22e1c9cda1' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Pittsburgh.append(tweet) 
pd.concat([d1, d3])
session.head()
neg = [x['neg'] for x in blah] $ pos = [x['pos'] for x in blah] $ neu = [x['neu'] for x in blah] $ snt = [x['compound'] for x in blah] $ op_sent = pd.DataFrame({'usr_neg': neg, 'usr_pos': pos, 'usr_neu': neu, 'usr_snt': snt})
arrPriceList = closingPrices[closingPrices.sort_values(ascending = False)]
cvPredictions = cvModel.transform(testData)
df_info(dfs[1])
sample_df['entity_type'].unique()
bigdf_read.reset_index().loc[1399301]
countries_count = countries_df['country'].value_counts() $ countries_count
y2 = df3['converted'] $ X2 = df3[['intercept', 'new_page', 'UK', 'US']] $ X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.20, random_state=0)
load2017 = load2017.dropna()
dc2015 = dc[(dc['created_at'] >= '2015-03-30') & (dc['created_at'] <= '2015-05-07')] $ dcelse = dc[(dc['created_at'] < '2015-03-30') | (dc['created_at'] > '2015-05-07')] $ if ((dc2015.shape[0] + dcelse.shape[0]) == dc.shape[0] ): $     print("Ok")
import pandas as pd $ data2 = pd.read_csv('/content/us-consumer-finance-complaints.zip', compression='zip', header=0, sep=',', quotechar='"') $
loanTree = DecisionTreeClassifier(criterion="entropy", max_depth = 4) $ loanTree # it shows the default parameters
df["TOTAL_PAYMENT"].plot()
posts_in_response_to_user_collected_saved.printSchema()
graf_counts2.reset_index(inplace=True)
twitter_dataset.to_csv('Twitter_archive_master.csv') $ image_copy.to_csv('Image_master.csv') $ tweet_data_copy.to_csv('Twitter_api.csv') $ df_copy.to_csv('Twitter_archive.csv')
len(train),len(test)
p_old = prob_conv $ p_old
autos_p['ad_created'].str[:10].value_counts(normalize = True, dropna = False).sort_index(ascending = True).head(100)
print("General information of each data field:\n") $ for column in my_df.columns: $     print("Field:", column) $     print(my_df[column].describe()) $     print()
b_cal_q1.loc[:,'price'] = pd.to_numeric(b_cal_q1['price'], errors='coerce')
%%time $ average_neighbor_degree = convert_dictionary_to_sorted_list(nx.average_neighbor_degree(network_friends))
df['day'] = df.index.day $ df['month'] = df.index.month $ df['year'] = df.index.year $ df.head()
Xtr_m = np.mean(Xtr[:,:7],axis=0) $ Xtr_std = np.std(Xtr[:,:7],axis=0) $ Xtr_scale = (Xtr[:,:7]-Xtr_m)/Xtr_std $ Xts_scale = (Xts[:,:7]-Xtr_m)/Xtr_std
small_train = train[train['is_booking'] == 1] $ small_train.shape
df2.head()
Base.classes.keys()
pd.read_csv("Data/microbiome.csv", chunksize=15)
p_convert_control = df2.query('group == "control"').converted.mean() $ p_convert_control
from training import Environment, Agent $ %load_ext autoreload $ %autoreload 2
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data2.csv" $ mydata = pd.read_csv(path, sep =',', header=None) $ mydata.head(5)
df1=df['author'].groupby(df['author']).count() $ df1.head()
seconds_len
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"').query('converted == 1').shape[0] $ convert_new = df2.query('landing_page == "new_page"').query('converted == 1').shape[0] $ n_old = df2.query('landing_page == "old_page"').shape[0] $ n_new = df2.query('landing_page == "new_page"').shape[0]
expected_index = pd.DatetimeIndex(start=raw.index[0], end=raw.index[-1], freq='0.25H') $ ideal_index_df = pd.DataFrame(index=expected_index) $ print(ideal_index_df.head()) $ len(raw.index), len(expected_index)
df2.loc[df2['user_id'] == 773192]
new_misalign = df.query('landing_page == "new_page" & group != "treatment"').count()[0] $ old_misalign  = df.query('landing_page == "old_page" & group != "control"').count()[0] $ print(new_misalign + old_misalign)
with open('components/pop_models/excitatory_pop.json') as exc_data: $     exc_prs = json.load(exc_data) $ pprint.pprint(exc_prs)
df.sum().plot() $
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],alternative = 'smaller') $ z_score, p_value, norm.cdf(z_score)
df_joined.groupby(['country','ab_page_new_page']).converted.count()
hp.sites[:5]
stations_df  = pd.DataFrame(stations,columns=["Station Name", "Station ID"]) $ stations_df
pd.Index(unique_vals).get_indexer(to_match)
n_new = df2.query("landing_page == 'new_page'")['landing_page'].count() $ n_new
cm_svm = confusion_matrix(y_final, svm_predicted)
df = pd.read_sql('SELECT first_name, last_name FROM actor', con=conn) $ df.head()
loan_principals.head()
df_q = pd.read_sql(query, conn, index_col=None) $ df_q.head(15)
search_tweets('can')
exploration_titanic.print_infos('consistency', print_empty=False) $
autos['last_seen'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
pd.read_sql("SELECT * from blog order by blog_id desc limit 10", conn)
from bmtk.utils.spike_trains import SpikesGenerator $ sg = SpikesGenerator(nodes='network/mthalamus_nodes.h5', t_max=3.0) $ sg.set_rate(10.0) $ sg.save_csv('thalamus_spikes.csv', in_ms=True)
s1 = pd.Series([0, 1, 2, 3]) $ s2 = pd.Series([0, 1, 2, 3]) $ s3 = pd.Series([0, 1, 4, 5]) $ d = pd.concat([s1, s2, s3], axis=1) $ d
posts.insert_many(new_posts)
temp_series_freq_2H = temp_series.resample("2H").mean() $ temp_series_freq_2H
pprint(q_specific.metadata(reload=True))
retweet_pairs["FromType"] = "Person" $ retweet_pairs["ToType"] = "Person" $ retweet_pairs["Edge"] = "Retweeted" $ retweet_pairs.rename(columns={"screen_name":"FromName","retweeted_screen_name":"ToName"},inplace=True) $ retweet_pairs.head(10)
kNN500.fit(X_extra, y_extra)
sessions['SessionDate']=pd.to_datetime(sessions['SessionDate']) $ sessions.merge(users,how='inner', left_on=['UserID','SessionDate'],right_on=['UserID','Registered'])
df_trump_tweets = pd.read_csv('Trump_Tweets.csv') $ text_data = df_trump_tweets['text'] $ text_data.fillna('', inplace=True) $ list_text = list(text_data) $ trump_words = preprocessing(list_text)
import pandas as pd $ import numpy as np
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ from collections import OrderedDict, Counter $ %matplotlib inline
transactions[transactions.msno == '++1Wu2wKBA60W9F9sMh15RXmh1wN1fjoVGzNqvw/Gro=']
def trip_end_date(x): $     return re.findall(r'\d{4}-\d{2}-\d{2}', x)[-1]
adopted_cats.loc[adopted_cats['Point']==1,'mix_color'] = 1
n_new = df2.query("landing_page == 'new_page'")['landing_page'].count() $ n_new
month['DATE/TIME'] = month['DATE'] + " " + month['TIME'] $ month['DATE/TIME'] = pd.to_datetime(month['DATE/TIME'], infer_datetime_format=True) $ month = month[['LOCATION', 'DATE/TIME', 'ENTRIES']] $ print month.info() $ month.head(20)
contractor[contractor.contractor_number.duplicated() == True]
df_ad_airings_filter_3['start_time'].min()
authors_with_name.select("parsed_info.first", "parsed_info.title").show()
g = sns.distplot(dftop['temp'], label = "days most complaints") $ g = sns.distplot(weather['temp'], label = "all weather data") $ g.legend(loc='upper left') $ plt.savefig('Top_complaints_days_temp_dist_versus_all_data.png')
df_countries = pd.read_csv('./countries.csv') $ df_countries.head(2)
ozzy.buddy.doginfo()
from sklearn.model_selection import train_test_split $ x = df.drop(['active'],axis = 1) $ y = df['active'] $ x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = .2,random_state = 3)
cityID = '2a93711775303f90' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Milwaukee.append(tweet) 
corpus = matutils.Sparse2Corpus(counts)
to_be_predicted_Day1 = 21.18 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
import os $ REGION = 'us-central1' # Choose an available region for Cloud MLE from https://cloud.google.com/ml-engine/docs/regions. $ BUCKET = 'qwiklabs-gcp-2b885fc55a4b5f06' # REPLACE WITH YOUR BUCKET NAME. Use a regional bucket in the region you selected. $ PROJECT = 'qwiklabs-gcp-2b885fc55a4b5f06'    # CHANGE THIS
for airline in airline_names: $     print(airline) $     tweepy_pull_tweets_store_db(airline, coll_ref, date_since, num_tweets = 100)
%%time $ del extract_deduped_with_elms_v2['LOAN_AMOUNT_'] $ extract_deduped_with_elms_v2.to_csv(cwd + '\\ELMS-DE backup\\extract_deduped_with_elms_0611.csv', index=False)
clean_appt_df = clean_appt_df.rename(index=str, $                      columns = {"Hipertension": "Hypertension", $                       "Handcap": "Handicap"}) $ clean_appt_df = clean_appt_df.drop(clean_appt_df[clean_appt_df['Age'] < 0].index) $ clean_appt_df.describe()
df_archive_clean["tweet_id"] = df_archive_clean["tweet_id"].astype(str)
mydmh = MyDMH()
df_users_products.groupby(["UserID","ProductID"])["Quantity"].sum().reset_index()
times.dt.year
display(Markdown(q5b_answer))
autos = autos[autos["price"].between(1,351000)]
from neurom import viewer
full_data.dtypes#['age']=='float64'
query.get_dataset(db, id=ds_info["DatasetId"][0], columns="ndarrays", get_info_items=True)
cats_df = cats_df.set_index(['Unnamed: 0']) $ cats_df.index.name = 'seq'
summed = a + b $ summed.iloc[4, 0] = np.nan $ summed
print(X_train.shape, y_train.shape) $ print(X_test.shape, y_test.shape) $
df[['beer_name', 'simple_style', 'brewery_name', 'brewery_country']][df.rating_score == top_score]
X_conv_train.shape
par = pst.parameter_data #get a ref to the parameter data dataframe $ wpars = par.pargp=="welflux" $ par.loc[wpars] $
from pygeocoder import Geocoder
intervention_history['target_test'] = intervention_history['time_delta'].between(pd.Timedelta('1 days'), pd.Timedelta('182 days')) \ $                                         & (intervention_history['next_incident_type'] != 'Entretien') \ $                                         & intervention_history['MOTIF_ANNULATION_CODE'].isnull()
module_id = 'cl_Jx8qzYJh' $ res = ml.classifiers.classify(module_id, textList, sandbox=False)
season10["InorOff"] = "In-Season"
pd.merge(df1, df3, left_on="employee", right_on="name" ).drop('name', axis=1) #axis=1 is column (=0 is row) $
type(all_points)
edgesWeight = edges $ edgesWeight['Weight'] = weights $ edgesWeight['Tags'] = tags $ edgesWeight.to_csv('../CreatedCSVs/edgesWeightned'+country+'.csv',index=False) $ edgesWeight.head()
temp_long_df.head()
ax = pre_analyzeable['prior_sims_phet'].value_counts().plot(kind='bar') $ for p in ax.patches: $     ax.annotate(str(int(p.get_height())), (p.get_x() * 1.005, p.get_height() * 1.005), fontsize=25)
print(np.exp(results.params)) $ print(1/np.exp(results.params))
df3 = df2[df2['group'] == 'control'] $ num_group = df3.user_id.count() $ num_converted = df3[df3['converted'] == 1].user_id.count() $ p_old_actual = num_converted / num_group $ p_old_actual
shows2.shape
pd.options.display.float_format = '{:20,.4f}'.format
ts.shift(5, freq=pd.datetools.bday)
S_distributedTopmodel.basin_par.filename
df_western.groupby(['release_decade'], as_index = False)['id'].count()
old = (df2.query('group =="control"')['converted']==1).mean() $ old
df = pd.read_csv("/Users/Dave/Desktop/Programming/Personal Projects/Bikeshare_Seattle_SF_Kaggle/tripSF.csv") $ weather = pd.read_csv("/Users/Dave/Desktop/Programming/Personal Projects/Bikeshare_Seattle_SF_Kaggle/weatherSF.csv") $ stations = pd.read_csv("/Users/Dave/Desktop/Programming/Personal Projects/Bikeshare_Seattle_SF_Kaggle/stationSF.csv")
joined_df = df_usage.join(df_users.set_index('id'),on='id')
!wget http://files.fast.ai/part2/lesson14/rossmann.tgz
%%timeit -n1 -r2 $ tags = {} $ for i, el in tqdm(rentals['description'].iteritems()): $     tags[i] = get_rental_concession(el) $ T = pd.Series(tags)
df = w.data_handler.get_all_column_data_df() $ df.columns $ df[df.SEA_AREA_NAME.isnull()].loc[:,['DEPH', 'SALT_BTL', $      'SDATE', 'VISS_EU_CD', 'WATER_BODY_NAME', 'SEA_BASIN', $    'SECCHI', 'TEMP_BTL', 'VISS_EU_ID', 'WATER_DISTRICT', 'WATER_TYPE_AREA', 'MONTH']] $
import statsmodels.api as sm $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = len(df2.query("group == 'control'")) $ n_new = len(df2.query("group == 'treatment'"))
df
!pwd
import pandas as pd $ import os.path $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
weather.loc[weather.events == 'rain', 'events'] = "Rain" $ weather.loc[weather.events.isnull(), 'events'] = "Normal"
s = pd.Series() $ print (s) $ type(s)
from sqlalchemy.sql import select
df.iloc[[0, 3], 1] = np.NaN  # Not a number $ df
ac_tr_prepared.shape
prop = props[props.prop_name == "PROPOSITION 064- MARIJUANA LEGALIZATION. INITIATIVE STATUTE."]
plot_time_metric_byvar(df=content_performance_bytime, metric='pageviews', byvar='document_type')
lb = articles['tokens'].map(len).quantile(.025) $ ub = articles['tokens'].map(len).quantile(.975) $ articles = articles.loc[(articles['tokens'].map(len)>lb) & (articles['tokens'].map(len)<ub),:]
mod2 = sm.Logit(df_new['converted'], df_new[['intercept', 'UK','CA','ab_page']]) $ results2 = mod2.fit()
cohort_retention_df.to_csv('retention_cohorts_518.csv')
n_old = len(df2.query("group == 'control'")); $ n_old
for name in file_list: $     if "download" in name: $         print(name)
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $ new_page_converted = new_page_converted.shape[0] $ new_page_converted
null_vals = np.random.normal(0, np.std(p_diffs), p_diffs.size) $ p_null = df2.query('landing_page == "new_page"').converted.mean() -\ $ df2.query('landing_page == "old_page"').converted.mean() $ plt.hist(null_vals); $ (null_vals > p_null).mean() $
len(lxml.html.tostring(_html))
test.shape
X_train, y_train = X[training_set],y[training_set] $ X_test, y_test = X[test_set], y[test_set]
vip_df.head()
pipe_lr_3 = make_pipeline(tvec, lr) $ pipe_lr_3.fit(X_train, y_train) $ pipe_lr_3.score(X_test, y_test)
def averaged_word_vectorizer(corpus, model, num_features): $     vocabulary = set(model.wv.index2word) $     features = [average_word_vectors(sentence, model, vocabulary, num_features) for sentence in corpus] $     return np.array(features)
url = 'http://www.bbc.co.uk/sport/football/premier-league/results'
tokenizer_porter('running like running and thus they run') $
bp_series = pd.Series(brand_prices) $ bm_series = pd.Series(brand_mileage) $ price_and_mileage = pd.DataFrame(bp_series, columns=['mean_price']) $ price_and_mileage['mean_mileage'] = bm_series $ price_and_mileage
percentage = (42/9211)*100 $ percentage
df_en['polarity_blob'].plot.hist(figsize=(10,5), bins=70, title="TextBlob Histogram")
acs_df = acs_df.set_index(['year', 'tract']).sort_index().reindex(crime_and_permits_df.index) $ acs_df['crime_count'] = crime_and_permits_df['crime_count'] $ acs_df['permit_count'] = crime_and_permits_df['permit_count'] $ acs_df.head()
failed.describe(include='all') $
beirut['WindDirDegrees'] = beirut['WindDirDegrees'].astype('float64') $ beirut['EET'] = to_datetime(beirut['EET'])
dsDir = Path('data') $ bus = pd.read_csv("data/businesses.csv",encoding='ISO-8859-1') $ ins = pd.read_csv("data/inspections.csv" ) $ vio = pd.read_csv("data/violations.csv" ) $
image_pred_df.rename(columns={'id': 'tweet_id'}, inplace=True) $ tweet_json_df.rename(columns={'id': 'tweet_id'}, inplace=True)
df_by_donor.rename(columns = {'Donor ID': 'Donor ID', 'Donation Amount min': 'Minimum Donation', 'Donation Amount max': 'Maximum Donation', 'Donation Amount mean': 'Mean Donation', 'Donation Amount sum': 'Total Donations', 'Donation ID count': 'Count of Donations by Donor', 'Donation Received Date max': 'Most Recent Donation'}, inplace=True) $ print(df_by_donor.head())
slot_dic
top_supporters.to_csv("top_supporters.csv")
archive.rating_denominator.value_counts().sort_values()
order_with_cust = pd.merge(cust_age,order_item, how='inner', left_on=['CUSTOMER_ID'], right_on =['CUSTOMER_ID']) $ order_with_cust.head()
lang_detection(tweet) $ lang_occur = pd.DataFrame() $ lang_occur['language'] = languages
ratings.describe()
idx = df_providers[ (df_providers['id_num']==260096) &\ $                    (df_providers['year']==2011) &\ $                    (df_providers['drg3']==39) ].index.tolist() $ idx $
BallBerry_ET_Combine
ss = StandardScaler(with_mean=False) # to maintain sparsity
raw_data.dtypes
print("Odds against Bob winning: %i to 1" %((1. - freq_prob) / freq_prob))
!pip install --user pydotplus
test_d = {'Date':dates, 'Point_Change': increment, 'Total_Score': score}
reframed = series_to_supervised(scaled, 1, 1) $ ref=series_to_supervised(bitcoin_market_info.values,1,1) $ ref.drop(ref.columns[[7,8,9,10,11]], axis=1, inplace=True) $ print(ref.head())
sp500.iloc[[0, 2]]
pivoted.T.shape
test_active_listing_dummy = active_listing_count_binarizer.transform(X_test[['Active Listing Count ']].values) $ print('test active listing dummy: ', test_active_listing_dummy[0:5], test_active_listing_dummy.mean()) $ test_pending_ratio_dummy = pf_poly.transform(X_test[['Pending Ratio']].values) $ print('test pending ratio dummy: ', '\n', test_pending_ratio_dummy[0:5]) $ print(test_active_listing_dummy.shape)
with open('./data/model/age_prediction_sk.pkl', 'rb') as picklefile: $     grid = pickle.load(picklefile)
goals_df
archive_df.shape,image_file.shape,tweet_df.shape
results_page.summary()
df_en = df[df['lang']=='en']
users_fin=users_van.append(users_ben) $ users_fin.head()
people.sort_values(by="age", inplace=True) $ people
iris.plot(x="Petal.Length",y="Sepal.Width", kind="scatter", c=np.array(["black", "cyan", "orange"]))
df['fileSizeMB'].describe()
pd.set_option("display.max_columns", 100)
cat_vars=[ 'category', 'main_category', 'currency','country'] $ cont_vars=['goal', 'pledged', 'backers','usd_pledged','usd_pledged_real','usd_goal_real']
tweet_file = './data/tweets.json'
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?" + "&start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY $ r = requests.get(url)
my_model_q3_label = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_nb, training='label') $ my_model_q3_label.fit(X_train, y_train) $ my_model_q3_label.stackData.head()
dup_id = df2.user_id.value_counts().argmax() $ print('Duplicated user_id: {}'.format(dup_id))
tweet = api.get_status(id = '892420643555336193') $ print(type(tweet)) $ print(tweet.keys())
col_bools = [True, False, False] * 4 $ col_bools
lm=sm.OLS(df2['converted'],df2[['intercept','ab_page']]) $ results=lm.fit() $ results.summary()
from rl.callbacks import TrainEpisodeLogger $ class TrainEpisodeLoggerPortfolio(TrainEpisodeLogger): $
df = pd.merge(train, $ transactions[(transactions.transaction_date < datetime.strptime('2017-03-01', '%Y-%m-%d'))], $ on='msno', $
full_data = pd.DataFrame({'cust_id':data_cus_id,'order_date':data_dates})
total_avg_prc_mon_day = hawaii_measurement_df[["Percipitation"]] \ $ .groupby([ \ $    hawaii_measurement_df["Date"].dt.strftime('%m-%d') $ ]).mean() $ total_avg_prc_mon_day.head()
beforeUsers=fullDf[fullDf.index<pd.datetime(2015,4,25)].user
hawaii_measurement_df.head(10)
rng = pd.date_range(start='1/1/2017', periods=72, freq='B') $ rng $
hashed_test.show(5, truncate=False)
AAPL.merge(GOOGL)
data.info()
trump['hours'] = trump.index.hour $ trump['weekday'] = trump.index.weekday_name
names = [] $ counts = [] $ for i in range(len(fdist_top)): $     names.append((fdist_top[i])[0]) $     counts.append((fdist_top[i])[1])
treatment_df = df2.query('group == "treatment"') $ treatment_df.user_id.nunique()
p_diffs = [] $ for i in range(10000): $     new_page_converted = np.random.binomial(1, p_new,size = n_new) $     old_page_converted = np.random.binomial(1, p_old,size = n_old) $     p_diffs.append (new_page_converted.mean() - old_page_converted.mean())
temp_cat[0] < temp_cat[3]
dt.fit(data[['expenses', 'floor', 'lat', 'lon', 'property_type', \ $              'rooms', 'surface_covered_in_m2', 'surface_total_in_m2']], \ $         data[['price_aprox_usd']])
df_ad_airings_5['state'].unique()
dfUsers[:20]
session.query(Station.station).all()
[r for r in all_results('/committee/C00008664/', {})]
sales_pivot = df.pivot_table(index=['Company','Product'], $                              values=['Quantity','Price'], $                              aggfunc=[np.sum,np.mean], $                              fill_value=0) $ sales_pivot.head()
df.iloc[[0, 56, 1033, -1]]
feature_df['including'] = list_including $ feature_df['excluding'] = list_excluding $ feature_df['feature'] = ["Elevator", "Laundry", "Doorman", "Hardwood", "Dishwasher"] $ feature_df
drugTree.fit(X_trainset,y_trainset)
au.find_some_docs(ao18_coll,sort_params=[("id",1)],limit=3)
p_new = df2['converted'].mean() $ print ("convert rate for p_new under the null :{} ".format(round(p_new, 4)))
cust_data1.dtypes
s.values
tweet_archive_clean.tweet_id  = tweet_archive_clean.tweet_id.astype(str)
df1 = df.drop(['Area Id','Variable Id','Symbol'],axis=1)
excelDF.head()
df_total.tail()
def percentConvert(value): $     full = value * 100 $     return '{0:.1f}'.format(full)
metrics, predictions = pipeline.test(X_train, y_train, output_scores=True) $ print("Performance metrics on training set: ") $ display(metrics) $ print("Individual scores: ")
state = environment.reset() $ state, reward, done=environment.execute(env.action_space.sample()) $ state.shape
data = pd.DataFrame(r.json()['dataset']['data'], columns=['Date', $    'Open','High','Low','Close','Change','Traded Volume','Turnover', $    'Last Price of the Day','Daily Traded Units','Daily Turnover'])
import numpy as np $ print("mean:",np.mean(df_concat_2.message_likes_rel)) $ print("max:",max(df_concat_2.message_likes_rel)) $ print("min:",min(df_concat_2.message_likes_rel)) $ print("std:",np.std(df_concat_2.message_likes_rel))
mape(eval_df['prediction'], eval_df['actual'])
to_be_predicted_Day1 = 21.25 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
df_ad_airings_filter_3.shape
from sklearn.linear_model import LogisticRegression $ from sklearn.model_selection import train_test_split $ from sklearn.feature_selection import RFE
URL = "http://www.reddit.com/hot.json" $ headers={'User-agent': 'kiros Bot 0.1'} $ res = requests.get(URL, headers=headers) $ data = res.json()
df2.drop([1899], inplace=True); $ df2.count()
df_everything_about_DRGs.head()
from sklearn.datasets import load_iris $ clf = SuperLearnerClassifier() $ iris = load_iris() $ clf.fit(iris.data, iris.target) $ cross_val_score(clf, iris.data, iris.target, cv=10, verbose=0)
df[0:5]
final_ticker_data = avg_data.T
df_mes = pd.read_csv('I:\Javier Resano\Curriculum\Caso Carto\yellow_tripdata_2017-06.csv', sep=',')
labels_dedupe = labels.drop_duplicates(['funding_round_uuid','investor_uuid']) $ labels_dedupe = labels_dedupe[labels_dedupe.investor_uuid.isin(investors_df.uuid)].copy()
train_small_data.head(1)
from IPython.display import Image $ url = "https://s3.amazonaws.com/leiwen/dmfa/star_schema.png" $ Image(url=url)
result
control_converted = df[df['group'] == 'control']['converted'].mean() $ treatment_converted = df[df['group'] == 'treatment']['converted'].mean() $ diff = treatment_converted - control_converted $ (diff < np.array(p_diffs)).mean()
negative_examples.to_csv('training_data/utah_negative_examples.csv')
df[df['acct_type'] == 'fraudster_event']
print(avg_temp)
print(voters.PermCategory.unique()) $ voters.PermCategory.value_counts(dropna=False)
for i in ['pts', 'oreb', 'dreb', 'reb', 'ast', 'stl', 'blk', 'to']: $     main["{}_d".format(i)] = main['{}_ta'.format(i)] - main['{}_tb'.format(i)]
print (brighter_24 - brighter_21) $ print float((brighter_24 - brighter_21)*100)/float(total_stars), '%' $
stops_per_crime_per_month_ = stops_month_window_["sum_stops"].Count/stops_month_window_["sum_window_crimes"].Count
file = 'data/pickled/Emoticon_NB4/full_emoji_dict.obj' $ gu.pickle_obj(file, emoji_dict)
SCR_PLANS_df['start_date'] = pd.to_datetime(SCR_PLANS_df['scns_created'].apply(lambda x:x[np.argmin(x)])).dt.strftime('%Y-%m')
df_sb = pd.read_csv("sobeys_all.csv", encoding="latin-1")
dict_category['slug'].split('/')[0] # parent category
links = hems_soup.find_all("a", {"class": "itemLink"}) $ links
expiry = datetime.date(2015, 1, 5) $ msft_calls = Options('MSFT','yahoo').get_call_data(expiry=expiry) $ msft_calls.iloc[0:5,0:5]
!rm SIGHTINGS.csv -f $ !wget https://www.quandl.com/api/v3/datasets/NUFORC/SIGHTINGS.csv
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'useless'
ls
user.query("screen_name == 'Trump'")
s + pd.tseries.offsets.DateOffset(months=2)
monthly_gain_summary.tail(10)
total_users = df.nunique()['user_id'] $ print("Number of unique users are : {}".format(total_users))
print('Number of SLPs that have declared at least one statement: ' + str(len(active_psc_statements[active_psc_statements.company_number.isin(slps.CompanyNumber)].company_number.unique()))) $ print('Proportion of SLPs in active companies dataset that declared a statement: ' + str(round(len(active_psc_statements[active_psc_statements.company_number.isin(slps.CompanyNumber)].company_number.unique()) / len(slps.CompanyNumber.unique()),2)))
CumSums = Lags.groupby(level=1).cumsum() $ CumSums.columns = ['Cum'+c[:-4] for c in CumSums.columns] $ CumSums.head()
accounts.head()
db=client.parcelled $ db.collection_names()
session_summaries = [] $ for summary_loc in fr5_session_summary_locations: $     summary = FRStimSessionSummary.from_hdf(summary_loc) $     session_summaries.append(summary)
import matplotlib.pyplot as plt $ import datetime $ from matplotlib import style $ style.use('ggplot')
import os,sys,inspect $ currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe()))) $ parentdir = os.path.dirname(currentdir) $ sys.path.insert(0,parentdir) 
a_sevenPointSeven = df['a'] == 7.7
df1['Hour'] = pd.to_datetime(df1['Date'], format='%H:%M').dt.hour # to create a new column with the hour information $ df1.head()
df_new.head() $ df_new.country.unique()
p_diffs = np.array(p_diffs) $ p_val = (p_diffs > actual_diff).mean() $ p_val
Z = np.arange(9) $ print(Z.reshape((3, 3)))
months = {datetime(2000,i,1).strftime("%b"): i for i in range(1, 13)} $ df['Month ID'] = df['Month'].map(months)
dfJobs.ix[4009]
url_cerberus = "https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced" $ url_schiaparelli = 'https://astrogeology.usgs.gov/search/map/Mars/Viking/schiaparelli_enhanced' $ url_syrtis = 'https://astrogeology.usgs.gov/search/map/Mars/Viking/syrtis_major_enhanced' $ url_valles = 'https://astrogeology.usgs.gov/search/map/Mars/Viking/valles_marineris_enhanced'
cohorts = cohorts.reset_index() $ cohorts = cohorts.set_index(['CohortGroup', 'CohortPeriod']) $ cohorts.head()
session = Session(engine)
exceldmh = dmh.PandasDataFrame(exceldf)
tbl.head()
from sklearn.metrics import mean_absolute_error, mean_squared_error $ from math import sqrt $ from pysumma.Validation import validation
pm_data['time_stamp'] = pd.to_datetime(pm_data.time_stamp) $ pm_data.sort_values(['unit_number','time_stamp'], inplace=True) $ pm_data.reset_index(inplace=True)
ufos_df = spark_df.map(lambda x: Row(**dict(x.asDict(), year=int(x.Reports[0:4]))))
age_category1 = df_titanic_temp.loc[df_titanic_temp['age'] <= age_median, 'age'] $ age_category2 = df_titanic_temp.loc[df_titanic_temp['age'] > age_median, 'age'] $ print(len(age_category1)) $ print(len(age_category2))
y_predit = exported_pipeline.predict(X)
plt.scatter(mario_game.NA_Sales, mario_game.EU_Sales) $ print('The pearson correlation between the NA sales and EU sales is {}' \ $       .format(pearsonr(mario_game.NA_Sales, mario_game.EU_Sales)[0]))
frame
like_the_donald = ['conservative', 'asktrumpsupporters', 'hillaryforprison', 'uncensorednews', 'askthe_donald', 'libertarian', 'mr_trump', 'conspiracy']
startups_USA.head()
msft_cum_ret.resample("M").mean()
stocks_pca_m3
funded = train.loc[train.final_status == 1, :] $ failed = train.loc[train.final_status == 0, :]
fill_zipcode = lambda x: '0'*(5-len(str(x))) + str(x) $ x1 = pd.DataFrame([[1, '8820'], [2, 8820]], columns=['a','b']) $ x1.b = x1.b.apply(fill_zipcode) $ x1
total = ac.isnull().sum().sort_values(ascending=False) $ percent = (ac.isnull().sum()/ac.isnull().count()).sort_values(ascending=False) $ missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) $ missing_data
print(text2int("twenty")) $ print(text2int("first")) $ print(text2int("thirtieth")) $ print(text2int("twenty eighth")) $ print(text2int("twelfth"))
%matplotlib inline
ioDF.author_id_x.nunique()
new_page_converted = np.random.binomial(1,p_new,n_new)
n_new = df2.query('landing_page == "new_page"').nunique()['user_id'] $ n_new $
df_arch.pupper.value_counts()
f.savefig('../figs/changes_spikel23_exp.jpg')
init_ind = 82 $ trans = acc.transactions.loc[82] $ trans
df =  prepareDF(td, makeCopy=True) $ axes=scatter_matrix(df, alpha=0.2, diagonal='kde', figsize=(10, 5)); # diagonal can be 'hist'; $ corr = df.corr().as_matrix() $ for i, j in zip(*plt.np.triu_indices_from(axes, k=1)): $     axes[i, j].annotate("%.3f" %corr[i,j], (0.8, 0.8), xycoords='axes fraction', ha='center', va='center') $
eda(df)
cust_data.MonthlyIncome
sl.head()
print(soup.prettify()[0:500])
num_dr_new.index
df_dummies = pd.get_dummies(df_countries,columns=['country']) $ df4 = df3.merge(df_dummies,on='user_id') $ df4.head()
df_new['US_ab'] = np.multiply(df_new['ab_page'],df_new['US']) $ df_new['CA_ab'] = np.multiply(df_new['ab_page'],df_new['CA']) $ df_new['UK_ab'] = np.multiply(df_new['ab_page'],df_new['UK']) $ df_new.head(10) $
dict(list(result.items())[:20])
autos["last_seen"].str[:10].value_counts(dropna = False, normalize = True).sort_index()
if __name__ == '__main__': $     get_all_tweets("@megmfurr")
k150_bets_over = [x[1]>.6 for x in pred_probas_over_k150]
(p_diffs > 0.000913).mean()
X_train.shape, y_train.shape
browser.click_link_by_partial_text('Valles Marineris Hemisphere Enhanced') $ html = browser.html $ soup = BeautifulSoup(html, 'lxml')
labels['Key_id'].iloc[40]
run txt2pdf.py -o"2018-06-18  2015 291 disc_times_pay.pdf"  "2018-06-18  2015 291 disc_times_pay.txt"
len(products)
merged.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount", ascending=False)
mask = df.isnull() $ newdf=df.head(-8) $ del newdf['Md'] $ newdf.tail()
top_songs = top_songs.dropna(axis=0, how='any')
dr_existing_patient_data_plus_forecast
df_protest.loc[0, 'start_date'].weekday()
df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'], $                     'supervisor': ['Carly', 'Guido', 'Steve']}) $ df4
s_mountain + s_pacific
collection.list_snapshots()
data_vi_week
df2['intercept'] = 1 $ df2['ab_page'] = np.where(df2['group']=="treatment", 1, 0) $ df2.head()
result[0]
def add_dollar(value): $     value = str(value * 100.00) + '%' $     return value
left_cols = ['Reg_date', 'id_partner', 'name'] $ right_cols = ['date_created', 'id_partner', 'campaign'] $ users_cost = pd.merge(users_cost, Costs, how = 'left', left_on=left_cols, right_on=right_cols) $ users_cost.head()
df_subset2 = df_subset[df_subset['Total Est. Fee'] <= 1000000]
user_logs[user_logs.msno == '29V0Jm3Xli1dy9UFeEL/BH2EMOr62DgeGLeKAKfE07k=']
agent_id = 'PPO' $ path = 'logs/marketorderenvadjusted/ppo/PPO_MarketOrderEnvAdjustment-v0_0_2018-08-25_13-17-26p0_z43cl/' $ checkpoint = 160 $ env, agent = load_env_agent(agent_id, path, checkpoint) $ result, trades, states, actions, rewards, quotes = run_through_all_data(env, agent)
df["grade"].cat.categories = ["very good", "good", "very bad"] $ df["grade"]
def conv(x): $     return time.mktime(datetime.datetime.strptime(str(x), "%Y-%m-%d %H:%M:%S").timetuple())
T = tf.one_hot(y, depth=caps2_n_caps, name="T")
df.head()
import numpy as np $ import pandas as pd $ import warnings $ warnings.filterwarnings('ignore')
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print('Z-Score: ',z_score) $ print('P-Value: ', p_value)
sales_df.groupby('Country').sum()['Revenue'].sort_values(ascending=False)
print grid.best_score_ $ print grid.best_params_ $ print grid.best_estimator_
to_be_predicted_Day5 = 56.18025818 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
from sklearn.metrics import confusion_matrix $ confusion_matrix(y_test, pred)
print (set(senti_val))
Measurement_data = session.query(Measurement).first() $ Measurement_data.__dict__
picker = df_nona.groupby('district_id').app_id.nunique() $ single_app = picker[picker==1] $ df_nona[df_nona.district_id.isin(single_app.index)].install_rate.hist()
scaler = MinMaxScaler() $ data[intFeatures] = scaler.fit_transform(data[intFeatures])
dat.status[dat.completed.notnull()].unique()
gdax_trans['Timestamp'] = pd.to_datetime(gdax_trans['Timestamp'], format="%d/%m/%Y %H:%M:%S")
pd.Series(index = feats_used, data = rf.feature_importances_).sort_values().plot(kind = 'bar')
ggplot(aes(x='datetime',y='MeanFlow_cfs'), data = df) +\ $     geom_line() +\ $     xlab("Year")  +\ $     ylab("Mean Flow (cfs)") +\ $     ggtitle("Neuse River near Clayton, NC")
df_json_tweet['tweet_id'] = df_json_tweet['tweet_id'].astype('int64') $ df_json_tweet.info()
np.array(df[['Visitors','Bounce_Rate']])
[d[0] for d in reps][:20]
len(set(df.title))
y_hat = lr.predict(df_pol_d) $ df_pol['y_hats'] = y_hat
schema = [val[0] for val in result] $ schema
df1.tail(5)
NewConverted = df2.query('converted == 1')['user_id'].count() $ HAlte = (NewConverted/NewTotalUser) $ print("Alternate Convert Rate: ",HAlte)
print(weights_A.shape,end='\n') $ print(weights_A)
df.loc['2018-05-21']
finals[finals.pts_l==1].shape
graf=df.copy()
loans_df.shape
popCon = pd.DataFrame(likes.groupby(by=['friend','content']).size()) $ popCon.columns = ['counts'] $ popCon = popCon.reset_index('content') $ popCon.sort_values(by='counts', ascending=False).head(10)
test_doc = doc_id_list[['test' in doc_id for doc_id in doc_id_list]] $ print("test_doc is created with following document names: {} ...".format(test_doc[0:5]))
churn_df.size
df.drop(['MINUTE'], axis = 1, inplace=True)
twitter_json = r'data/twitter_01_20_17_to_3-2-18.json' $ tweet_data = pd.read_json(twitter_json)
yt.get_video_metadata(video_id[0], key, parser=P.default)
feat_type[220:240]
if not os.path.isdir('output/heat_demand'): $     os.makedirs('output/heat_demand')
def load_keys(path): $     ...
giss_temp.to_excel(writer, sheet_name="GISS temp data") $ full_globe_temp.to_excel(writer, sheet_name="NASA temp data")
train_df.head(1)
autos.drop(["num_photos", "seller", "offer_type"], axis = 1)
mentioned_bills_all = mentioned_bills_all[['active', 'bill_slug', 'committees', 'congress', $             'enacted', 'primary_subject', 'sponsor_id','sponsor_party', 'sponsor_state', $             'vetoed', 'votes_api_url','votes_chamber', 'votes_date','votes_time', $             'votes_total_no', 'votes_total_not_voting','votes_total_yes', 'cosponsored_by_mt1_party', $            'cosponsored_ratio_mr40pc', 'primary_suject_no_months_mentioned', 'subject_nicheness']]
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline $ plt.style.use("ggplot")
df2 = pd.read_csv('comma_delim_clean.csv', index_col='id') $
df_agg= df_merge.groupby('start_date').agg({'id_x':'count','mean_temperature_f':'mean','cloud_cover':'mean'}).reset_index() $ df_filter = df_agg[(df_agg['start_date']<='2013-09-30')&(df_agg['start_date']>='2013-09-01')] $ df_filter['start_date'] = pd.to_datetime(df_filter['start_date'])
print('Max train ID: %d. Max test ID: %d' % (np.max(NYT_train_raw['UniqueID']), np.max(NYT_test_raw['UniqueID']))) $ joined = NYT_train_raw.merge(NYT_test_raw, how = 'outer')
cityID = '3df0e3eb1e91170b' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Columbus.append(tweet) 
n_new=df2[df2['group']=='treatment'].shape[0] $ print(n_new)
southern_sea_level = pd.read_table("http://sealevel.colorado.edu/files/current/sl_sh.txt", $                                    sep="\s+") $ southern_sea_level
df['Month'] = df['Date'].dt.month
knn_reg.score(x_test,y_test),np.mean(y_test)
cust_demo.dtypes
df['domain'].value_counts().head(20).plot(kind='bar') $ plt.xticks(rotation=70);
print('Training job name: {}'.format(rcf.latest_training_job.job_name))
ad_group_performance[ $     pd.isnull(ad_group_performance['AvgConversionValue']) $ ]
labels_for_menu_about_latent_features = ['menu_about_related_to_chinese_food', 'menu_about_related_to_philippino_food', 'menu_about_related_to_street_food', 'menu_about_related_to_a_celebration', 'menu_about_related_to_jewish_food', 'menu_about_related_to_sushi', 'menu_about_related_to_italian_food']
conn = engine.connect() $ inspector = inspect(engine)
a = 5 $ b = 0.5 $ df['X'] = a + b * norm.rvs(size=N, loc=df['W']) $ df['Y'] = a + b * norm.rvs(size=N, loc=df['W'])
mainstream_facts_metrics = mainstream_facts_metrics.drop(['id_x', 'id_y', 'created_at_x', $        'updated_at_x', 'created_at_y', $        'updated_at_y'], axis=1).copy()
pd.Period("2018-04") # inferred as Month
archive_clean = archive_clean.loc[~archive_clean['tweet_id'].isin(remove_list),:]
tweet_df.info() $
my_df.head()
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=7000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
df8 = df7.set_index(pd.DatetimeIndex(df7['Date'])) $ df8 = df8[['BG']].copy() $ df8_lunch = df8.between_time('11:00:00', '13:00:00') $ df8_lunch
countries_df = pd.read_csv('countries.csv') $ countries_df.head()
station_count#check
WorldBankdf.registerTempTable("worldbank")
prices['Ether'] = eth['Value']
joined=join_df(joined,weather,["State","Date"]) $ joined_test=join_df(joined_test,weather,["State","Date"]) $ sum(joined['Max_TemperatureC'].isnull()),sum(joined_test['Max_TemperatureC'].isnull())
delimited_hourly.reset_index(inplace=True) $ delimited_hourly.head()
tfav.plot(figsize=(16,4), label="Likes", legend=True,title="Popularity of Donald Trump over time") $ tret.plot(label="Retweets", legend=True) $ trep.plot(label="Replies", legend=True) $ plt.xlabel('Time') $ plt.savefig("fig/trump_popularity_time.png");
df_merged.boxplot(column='favorite_count');
temp.plot(kind='bar', figsize=(10, 3));
ebola_melt['type'] = ebola_melt.str_split.str.get(0) $ ebola_melt.head()
df_nona.groupby('app_id').describe()
y_score_test_rfc = rfc.predict_proba(X_test)[:,1] $ metrics.roc_auc_score(y_test, y_score_test_rfc) 
autos['price'].value_counts().sort_index(ascending = False).head(15)
os.chdir('images')
import requests $ import re $ import pandas as pd
s519397_df["date"] = pd.to_datetime(s519397_df["date"], format='%Y-%m-%d') $ s519397_df.info()
joined['oil_30_day_change']=joined['oil_30_day_change'].astype(np.float32) $ joined['transactions']=joined['transactions'].astype(np.float32)
def mean_absolute_percentage_error(y_true, y_pred): $     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100
modern_balk_king.head(15)
print(type(texas_city.index)) $ print(type(addicks.index)) $
print(model) $ print(model.wv.vocab)
df_joined=df_countries.join(df2_dummy) $ df_joined.tail(1)
mw.content.replace(to_replace='http://www.youtube',value='https://www.youtube',inplace=True, regex=True) $
train_ratio = 0.75 $ train_size = int(samp_size * train_ratio); print(train_size) $ val_idx = np.flatnonzero( $     (df.index<=datetime.datetime(2014,9,17)) & (df.index>=datetime.datetime(2014,8,1)))
likes.groupby(by=['year','month']).count().plot(kind="line")
joined = load_df('joined_transactions.pkl')
inforatio= (intercept/100)/std_err $ inforatio
season14["InorOff"] = "In-Season"
adopted_cats['Neutered'] = adopted_cats['Sex upon Outcome'].str.split().str[0] $ adopted_cats['Female'] = adopted_cats['Sex upon Outcome'].str.split().str[1]
calls_df.loc[(calls_df["list_id"]==998) | (calls_df["list_id"]==999),"dial_type"]="MANUAL" $ calls_df.loc[(calls_df["dial_type"]!="MANUAL"),"dial_type"]="AUTO"
SAEMRequest = requests.get(saem_women) $ print(SAEMRequest.text[:100])
type(api)
train_data.to_feather('data/train_data') $ test_data.to_feather('data/test_data')
df2[df2['user_id'].duplicated()] $
mean_df['mean_price'] = abp_series $ mean_df
temp_series = pd.Series(temperatures, dates) $ temp_series
gdp/pop
model = RandomForestClassifier(n_estimators = 10) $ model.fit(X, y)
tweet = 'I have lived in China all my life, but I was born in Lisbon' $ print("{} :  {}".format(color_formatting(ner.is_tweet_about_country(tweet, 'PT')), tweet)) $ print("\t\t| \n\t\t|-> {}".format(ner.get_countries_from_content(tweet)) )
df.head()
X = reddit[['vader_neg','vader_pos','vader_neu','vader_compound','title_len']]
if not os.path.isdir('output/electricity_demand'): $     os.makedirs('output/electricity_demand')
records['State'].value_counts().plot.bar() $ plt.title("Students per State", fontdict={'fontsize': 14}); $ plt.savefig('images/barplot_state.png')
telecom3.to_csv("E:\IIIT Bangalore AIML\Group Assignment 2\\telecom_churn_data_clean2.csv", sep=',', index=False)
ridge3 = linear_model.Ridge(alpha=-0.6) $ ridge3.fit(X_17, y2) $ (ridge3.coef_, ridge3.intercept_)
d={'c_A':[1,2,3,4],'c_B':[2,4,6,8],'c_C':[3,6,9,12],'c_D':[4,8,12,16]}
b = np.load('myfile.npy')  #Load the data into variable b. $ os.remove('myfile.npy')  #Clean up.
df[df.isna()].count()
df_temp_casts_CTD_1988
conn.fetch(table=dict(name='iris_df', caslib='casuser'), to=5)
df_tot[df_tot.Population==0] = df_tot[df_tot.Population==0].fillna(0) $ df_tot.Poverty.fillna(0, inplace=True) $ df_tot.Income.fillna(df_tot.Income.median(), inplace=True)
df1.isnull().sum()
xmlData['country'].value_counts()
diffSD = np.sqrt(active_sd**2/active_count + inactive_sd**2/inactive_count) $ diffSD
n_old = df2.query('landing_page == "old_page"').nunique()['user_id'] $ n_old
scores = raw_scores.value_counts().sort_index() $ scores
replacements = { $ 'location': {'A':'Hyderabad', 'B':'Mumbai'} $ } $ df = df.replace(replacements, regex=True) $ print(df.head(2))
shots_df[['PKG', 'PKA']] = shots_df['PKG/A'].str.split('/', expand=True) $ shots_df.drop('PKG/A', axis=1, inplace=True)
df_usnpl['state_level_media'] = df_usnpl['state_level_media'].replace(remap_vals)
sum([i**2 for i in range(1, 100 + 1)])
glm_binom_feat_2 = H2OGeneralizedLinearEstimator(family='binomial', solver='L_BFGS', model_id='glm_v5', Lambda=0.001) $ glm_binom_feat_2.train(covtype_X, covtype_y, training_frame=train_bf, validation_frame=valid_bf)
outfile = MasterDf.to_json(path_or_buf=None, orient='records', date_format='iso', double_precision=10, $                   force_ascii=True, date_unit='s', default_handler=None, lines=False, compression=None, index=True) $ with open('try1.json', 'w') as f: $     f.write(outfile)
df.info()  # return text output
corn.filter(lambda x: x["price"].min() > 10, dropna = False)
weather = weather.merge(events, left_index = True, right_index = True)
htrain=htrain.drop(['air_store_id','visit_date','id']) $ htest=htest.drop(['air_store_id','visit_date','id'])
modifications_over_time = commits[['author', 'timestamp', 'modifications']].groupby( $     [commits['author'], $      pd.Grouper(freq=TIME_FREQUENCY)]).sum().reset_index() $ modifications_over_time.head()
np.exp(-0.0507), np.exp(-0.0099)
adds["YBP sub-account"].replace(195099, 590099, inplace= True) $ adds
print(dftemp[(dftemp['Variable Name']=='National Rainfall Index (NRI)') & ((dftemp['Value']>950) | (dftemp['Value']<900))])
print(testObjDocs.__doc__)  # note: formatting is messed up if you do not use print() on the doc string
df1=files4.pivot_table(index='jobcandidate', columns='assessmentname', values='assessmentscore') $ df1=df1.fillna(0) $ df1new = pd.DataFrame(df1.to_records()) $ df3=pd.merge(df2,df1new,on='jobcandidate',how='left') $ df3.shape
classify_df.shape
from sklearn.cross_validation import train_test_split $ x_train, x_test, y_train, y_test = train_test_split(x_train_data, y_train_data, test_size=0.1, random_state=1337)
x = np.arange(1,101) $ y = np.tile(x,5) $ news_df['Tweet No.'] = y $ news_df = news_df[['Tweet No.', 'Source Acc.', 'Tweet', 'Date', 'Compound Score', 'Pos Score', 'Neu Score', 'Neg Score']] $ news_df.head()
df.head()
my_data.to_csv('examples/simple/data/fixed_process_data.csv', index=False)
print classification_report(y_pred_rsskb, y_test) $ print 'Accuracy: ', accuracy_score(y_pred_rsskb, y_test) $ print 'ROC AUC: ', roc_auc_score(y_pred_rsskb, y_test)
guinea_data.Description.value_counts()
tickerdata = tickerdata.reindex(all_weekdays)
df2['intercept'] = 1 $ df2[['ab_page', 'ab_page2']] = pd.get_dummies(df['landing_page']) $ df2.head()
df.sentiment.value_counts()
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.choice([0,1], size=n_new, p=[p_new, 1-p_new]).mean() $     old_page_converted = np.random.choice([0,1], size=n_old, p=[p_old, 1-p_old]).mean() $     p_diffs.append(old_page_converted - new_page_converted) $
ibm_hr_int = ibm_hr_target.select(numerical) $ ibm_hr_int.show(3)
log_mod = sm.Logit(df_combined['converted'], df_combined[['intercept', 'country_CA', 'country_UK']]) $ results = log_mod.fit() $ results.summary()
rnd_search_cv.best_estimator_
import tweepy $ import pandas as pd $ import matplotlib.pyplot as plt
test_movie = "sen to chihiro no kamikakushi" $ pwd = pairwise_distances(tfidf_matrix, tfidf_matrix[title_list_lower.index(test_movie)].reshape(1,-1), metric='cosine')
X2.head() #original label encoded data for Sierra Leone
sqlContext.sql("select * from pcs where count > 1").show()
import shutil $ shutil.rmtree(fp_ex) $ db_store = pathlib.Path("/active/examples/local_database/") $ shutil.rmtree(db_store)
consumerKey = 'XXXXXXXXXXXXXX' $ consumerSecret = 'XXXXXXXXXXXXXX' $ auth = tweepy.OAuthHandler(consumer_key=consumerKey, consumer_secret=consumerSecret) $ api = tweepy.API(auth)
import numpy as np $ import pandas as pd
stores = pd.read_csv("DataSets/stores.csv") $ stores = stores.assign(GTSales = stores.TotalSales * stores.Total_Customers,TotalExpenses = stores.OperatingCost + stores.AcqCostPercust) $ stores
model_ft[word_list[1000]]
new = pd.read_csv('phillydata/Philly_Elevation_SegId.csv') $ new.columns
tmdb_movies['release_date'] = pd.to_datetime(tmdb_movies['release_date'], infer_datetime_format=True)
unique_users = df['user_id'].unique().shape[0] $ unique_users $
exiftool -csv -createdate -modifydate MVI_0011.mp4 > out.csv
f_os_hour_clicks.show(1)
def machine_learning(x): $     if 'Machine Learning' in x: $         return 1 $     return 0 $ df_more['Machine Learning'] = df_more['Title'].apply(machine_learning)
len(train[train['feature_list']==''])
from nltk.tokenize.casual import TweetTokenizer $ from nltk.corpus import stopwords $ from nltk.stem import PorterStemmer $ import string
lr = 6e-4 $ learn.fit(lr, 20, cycle_len=1, use_clr=(10,10))
csvDF.columns=['CustID','Fname','Lname','Age','Job']
s_tobs_cnt = session.query(weather.station, func.count(weather.tobs)).\ $     filter(weather.date.between('2015-01-01','2015-12-31')).\ $     group_by(weather.station).order_by(func.count(weather.tobs).desc()).all() $ s_tobs_cnt
df4[['CA','UK','US']] = pd.get_dummies(df4['country']) $ df4.head()
pd.DataFrame(random_integers, index=[3, 2, 0])
data_df.groupby('tone')['ticket_id'].nunique()
for follower in tweepy.Cursor(api.friends).items(): $     print(follower.name)
bar=sess.get_intraday_bar_data('ibm us equity', event_type='TRADE', bar_interval=60, $                                start_datetime=datetime.datetime.now() - datetime.timedelta(days=7))
X.shape
sum(df.Category.value_counts().unique())
Fireworks = df[df['Complaint Type'] == 'Illegal Fireworks']
df.loc[:, 'posted_datetime'] = df.loc[:, 'posted_datetime'].str[1:] 
autos["odometer_km"].value_counts().sort_index(ascending=True)
(len(df[df.location_id==0]), $  len(df[df.raw_location_text=='']), $  len(df[(df.location_id==0) & (df.raw_location_text=='')]))
average_crimes_current = average_crimes[(average_crimes['year']==2017) & $                                                (average_crimes['half_year']=="first")]
dates = pd.date_range('2016-04-01', '2016-04-06') $ dates
pd.Period(pd.datetime.today(), 'D') - pd.Period('1/1/1970', 'D')
print ("For the tweet = ", train_.tweetText.values[10] ) $ print (" ") $ print ("The following features has been created:") $ print (" ") $ print (v_train[0][0])
from urllib import request $ response = request.urlopen('https://www.zhihu.com/question/286512966/answer/450313380') $ print(response.read().decode('utf-8'))
df.groupby(['episode_id']).location_id.shift().head()
train[(train.age>90) & (train.age<1000)].age.value_counts()
lr_model_newton.fit(X_train, y_train)
df_goog.dtypes
df.describe()  # Show the basic statistics
df['Vader'] = np.array([ sentiment_finder_vader(comment) for comment in df['body'] ])
df2= df.drop(unwanted.index, axis=0)
contribs.median()
StockData.tail()
twitter_archive_df_clean[twitter_archive_df_clean['name'] == None]
autos[['date_crawled','date_created','last_seen']][0:5]
df2[df2['group']=='control']['converted'].mean()  #Probability of the users in control group who coonverted
q = res['contents']['quotes'][0] $ print(q['quote'], '\n--', q['author'])
results_list
mp2013 = pd.period_range('1/1/2013','12/31/2013',freq='M') $ mp2013
len(git_log)
print(pd.value_counts(train_df['ip'])[:20])
from scipy.stats import norm $ norm.cdf(-1.3109241984234394)# where 0.002 is Zscore calculated by ztest
output= "Create view ViewDemo as select user_id, tweet_content, retweets from tweet as t inner join tweet_details as td where t.tweet_id=td.tweet_id order by td.retweets desc;" $ cursor.execute(output) $
cityID = 'c0b8e8dc81930292' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Baltimore.append(tweet) 
raw_df = pd.DataFrame.from_csv(path=raw_data, header=None)
movie1=ratings[ratings['movieId'] == 1] $ movie1['ceiled_ratings'] = movie1['rating'].apply(np.ceil)
desc_stats.transpose()
tweets_df.language.value_counts() $
def sharpe(returnseries): $     mean = 12*np.mean(returnseries) $     vol = np.sqrt(12)*np.std(returnseries) $     return np.round(mean/vol, 3)
type(archive_df_clean['timestamp']) $ archive_df_clean.head(10)
autos["last_seen"].str[:10].value_counts(normalize=True, dropna=False).sort_index().plot(kind="bar", title="Last_seen", colormap="Blues_r")
df.to_excel('data/analysis2/Dow2.xlsx') $ df.tail(3)   
prepared_train = prepared_train[prepared_train.comments < 1100]
tweetsDF.dtypes
shows['stemmed_plot'] = shows['stemmed_plot'].dropna().apply(reassemble_plots)
ca_all.shape[0], shape0
plt.rcParams['figure.figsize'] = (15, 5) $ timezones.plot(kind='bar') $ plt.xlabel('Timezones') $ plt.ylabel('Tweet Count') $ plt.title('Top 10 Timezones tweeting about #TomPetty')
teams_df.head()
leavers = users[users['LastAccessDate'] < (pd.Timestamp.today() - pd.DateOffset(years=1))] $ leavers.groupby(['DaysActive'])['CreationDate'].count().sort_values(ascending = False).iloc[0:10]
test_collection.insert_one(temp_dict)
pd.set_option('display.max_columns',100) $ df.head(10000)
from sklearn import tree $ clf = tree.DecisionTreeClassifier(random_state = 100, $                                max_depth=3, min_samples_leaf=5) $ clf.fit(X_train,y_train)
test_array = np.concatenate([test_active_listing_dummy, test_pending_ratio_dummy], axis=1) $ print(test_array.shape)
engine = create_engine('postgres://%s@localhost/%s'%(username,dbname)) $ print(engine.url)
df_drug_counts['year'] = df_drug_counts.index # dash likes values from columns, not from indexes
a.A /= a.A.max()
df['price_doc'].values
p_diffs=[] $ for i in range(10000): $     new_page_converted = np.random.choice([1, 0], size=nnew, p=[np.mean([pnew, pold]), (1-np.mean([pnew, pold]))]).mean() $     old_page_converted = np.random.choice([1, 0], size=nold, p=[np.mean([pnew, pold]), (1-np.mean([pnew, pold]))]).mean() $     p_diffs.append(new_page_converted - old_page_converted) $
todaysTweets.head()
df_genres = df.groupby("genre") $ print(df_genres['score'].mean().sort_values(ascending=False))
intervention_train = intervention_train.merge(update_date, left_on=['INSTANCE_ID', 'CRE_DATE_GZL'], right_index=True)
df_clean.info()
re.match('^\w+@[a-zA-Z_]+?\.[a-zA-Z]{2,3}$', 'abc@gmail.com').group()
url= 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json' \ $      '?start_date=2017-01-01&end_date=2017-12-31&api_key={}' \ $         .format(API_KEY) $ r = requests.get(url)
print(df2['landing_page'].value_counts())
df_new['CA_new_page'] = df_new['new_page']* df_new['CA'] $ df_new['US_new_page'] = df_new['new_page']* df_new['US'] $
p_old=df2[df2['converted']==1].shape[0]/df2.shape[0] $ p_old
df2 = df2.drop(df2.index[2893])  #dropped one value of the duplicated value $ df2.shape
hpdvio[hpdvio['BuildingID']==1][['BuildingID', 'InspectionDate', 'ApprovedDate', 'OriginalCertifyByDate', 'OriginalCorrectByDate', $                                  'CertifiedDate', 'NOVIssuedDate', 'NOVDescription']]
sns.boxplot(data.antiguedad.values) $ plt.show()
result['timestamp'].describe()
df_new.groupby(['country','group']).converted.mean()
a_diff=df2.groupby('group')['converted'].mean()['treatment']-df2.groupby('group')['converted'].mean()['control'] $ a_diff
user_extract['user_id'] = user_extract['user_id'].astype(str) $ user_extract['created_at'] = pd.to_datetime(user_extract['created_at'])
df4.info()
sou_df['parse'] = sou_df.text.apply(nlp)
stepwise_model.fit(train)
P_diffs = np.array(p_diffs) $ P_diffs $
df2['landing_page'].value_counts()[0]/df2.shape[0]
to_be_predicted_Day4 = 48.29359653 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
data.to_csv("Data.csv",index=False)
df_comms = pd.DataFrame(columns=['comm_id','comm_msg','comm_date'])
y_series = pd.Series(y) $ y_series.head()
station_count = session.query(func.count(Station.id)).all() $ station_count
df.groupby('episode_id')['character_id'].nunique().agg(['min', 'mean', 'max'])
cityID = '3877d6c867447819' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fort_Wayne.append(tweet) 
appointments.head()
df2_new['ab_page_UK']=df2_new['ab_page'] * df2_new['UK'] $ df2_new['ab_page_CA']=df2_new['ab_page'] * df2_new['CA']
s.str.split(' ')
grouped_modern = modern_train.groupby(['country_destination'],as_index=False)
% matplotlib inline
df2[df2.user_id.duplicated()]
for v in contin_vars: $     joined[v] = joined[v].fillna(0).astype('float32') $     joined_test[v] = joined_test[v].fillna(0).astype('float32')
UKres.summary()
f_counts_minute_ip.show(1)
df1 = df1.drop(0) $ df2 = df2.drop(0) $ df2 = df2.drop(columns=["PERIOD", "Name"])
top_supports.head(5).amount.plot.barh()
S_lumpedTopmodel.decision_obj.thCondSoil.options, S_lumpedTopmodel.decision_obj.thCondSoil.value
df3.groupby(['STATION', 'DATE']).sum()
for leg in plan['plan']['itineraries'][0]['legs']: $     print('distance = {:,.2f} | duration = {:.0f} | mode = {} | route = {} | steps = {}'.\ $ format(leg['distance'], leg['duration'], leg['mode'], leg['route'], len(leg['steps'])))
to_be_predicted_Day4 = 22.34270784 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
driver.get('https://submissions.scholasticahq.com/journals/mobilization/dashboard') $
df = df[pd.notnull(df['Consumer complaint narrative'])] $ df.head()[['Product', 'Consumer complaint narrative']]
trend_de.head()
import yaml $ with open('../../conf/config.yaml') as f: $     dataMap = yaml.safe_load(f) $ from sqlalchemy import create_engine $ engine = create_engine('mysql+pymysql://%s:%s@localhost/securities_master' % (dataMap['securities_master']['id'], dataMap['securities_master']['pw']), echo=False)
ebola_melt['str_split'] = ebola_melt.type_country.str.split('_') $ ebola_melt.head()
engine.execute('SELECT * FROM measurement LIMIT 5').fetchall()
countries_df.country.unique()
df2[((df2['group'] == 'control') & (df2['converted'] == 1))].user_id.count() / df2.user_id.count()
tw = pd.read_csv("twitter-archive-enhanced.csv") $ tw.info()
df.to_csv(data_path+"/master.csv")
obs_diff = (df2.query('landing_page == "new_page"').converted.mean() - $             df2.query('landing_page == "old_page"').converted.mean()) $ obs_diff
botre = re.compile('bot') $ for i in users: $     if botre.match(i['_id'].lower()): $         print i['_id'], ' with : ',i['count'], 'items input' $
df2 = df2.drop([2893])
folder_name = 'wrangling_project_files' $ if not os.path.exists(folder_name): $     os.makedirs(folder_name)
prep = Preprocessor('yelp') $ paths = prep.paths
ab_df.head()
import os $ import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt
type(labels)
print '{} cases where district did not provision any accounts, assume deployed but no usage'.format(len(df[df.install_rate==0]))
import seaborn as sns $ sns.lmplot(x='hour', y='start', data=hours, aspect=1.5, scatter_kws={'alpha':0.2})
bnbAx.head(5)
df['Complaint Type'].value_counts()  #Categories
df_prep13 = df_prep(df13) $ df_prep13_ = pd.DataFrame({'date':df_prep13.index, 'values':df_prep13.values}, index=pd.to_datetime(df_prep13.index))
pivoted_data.resample("Y").sum().plot(figsize=(10,10))
Bronx_gdf = newYork_gdf[newYork_gdf.boro_name == 'Bronx'] $ Bronx_gdf.crs = {'init': 'epsg:4326'}
df3[df3['STATION'] == '103 ST'].groupby(['DATE']).sum().plot(figsize=(10,3))
df_clean.info()
tz_cat = timecat_df.groupby('userTimezone')[['tweetRetweetCt', 'tweetFavoriteCt']].mean() $ tz_cat.head()
df['MeanFlow_mgd'] = 
df2[df2['landing_page']=='new_page'].shape[0]
test_data.columns
%time preds = np.stack([t.predict(df) for t in m.estimators_]) $ np.mean(preds[:,0]), np.std(preds[:,0])
_ = ok.grade('q05e') $ _ = ok.backup()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ mydata  = pd.read_csv(path, sep =' ') $ mydata.head(5)
print('Metadata of the First Dataframe (created from data-text.csv)\n') $ df.info() $ print('\n\nMetadata of the Second Dataframe (created from berlin_weather_oldest.csv)\n') $ df1.info()
closeSeriesQ.pct_change().tail()
url = 'http://space-facts.com/mars/' $ tables = pd.read_html(url) $ tables $ df = tables[0] $ df.columns = ["Description", "Value"]
appointments['AppointmentCreated'] = pd.to_datetime(appointments['AppointmentCreated'], errors='coerce') $ appointments['AppointmentDate'] = pd.to_datetime(appointments['AppointmentDate'], errors='coerce')
df_master = pd.merge(archive_copy, images_copy, how = 'left', on = ['tweet_id'] ) $ df_master = pd.merge(df_master, api_copy, how = 'left', on = ['tweet_id']) $ df_master.info()
len(incorrect_breeds)
from sklearn.neighbors import KNeighborsClassifier $ from sklearn.metrics import confusion_matrix
2018033 - 2018028
import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline
data['date'] = pd.to_datetime(data['Timestamp'],unit='s').dt.date $ group = data.groupby('date') $ Daily_Price = group['Weighted_Price'].mean() $ Daily_Price.head()
!wget -nc https://s3.amazonaws.com/leiwen/dmfa/Data.csv
len(nullCity2018)
strategy.df_orders().head()
b.append(5) $ print a
word = 'convolution' $ word = 'machine' $ w2v.create_3d_tsne_gif('test3.gif', word, number_closest_words=25, num_positions = 10, delete_images=True, figsize = [15, 15]) $ print('Done.') $
mig_l12 = mi.groupby(level=['Symbol', 'Year', 'Month']) $ print_groups(mig_l12)
df2.sort_values(['user_id'], inplace = True) $ df2.reset_index(drop = True, inplace = True) $ df_merge = df2.join(df_countries, rsuffix = '_country') $ df_merge.drop(['user_id_country'],  axis = 1, inplace = True) $ df_merge.head()
idx = df.index[((df['group'] == 'treatment') & (df['landing_page'] == 'old_page')) | ((df['group'] == 'control') & (df['landing_page'] == 'new_page'))] $ idx ## Store the index of the mismatched rows
msftAC.shift(1, freq='S')
string_2018 = 'extracted_data/tmin.2018.csv'
df_merge.info()
%%bash $ while IFS='' read -r line || [[ -n "$line" ]]; do $     grep "It works!" $line $ done < links.txt
from os.path import basename $ import os $ import glob
df_new[['CA', 'UK']] = pd.get_dummies(df_new['country'])[['CA','UK']] $ df_new.head()
games_2017_sorted.loc[:, ["Team", "Opp", "Home", "Home.Attendance"]].tail(10) # games are duplicated 
from itertools import islice
df_loan3=pd.DataFrame(df_loan).merge(actual_payments.loc[actual_payments.iso_date==EOM_date.date(),['fk_loan','in_arrears_since', $                                     'in_arrears_since_days',u'in_arrears_since_days_30360','bucket','bucket_pd']], $                                           left_index=True, right_on='fk_loan')
convert_new = df2.query("landing_page == 'new_page'")['converted'].sum() $ convert_new
print(future_forecast)
df = dfs[0] $ df
print(mi_diccionario.keys(), mi_diccionario.values())
df_final['recency'] = (now - df_final['last_trx']).dt.days
data[data['y_flag'] == 1][['dataset_datetime', 'dataset_location', 'SpO2', 'SpO2_change', 'Inspired Tidal Volume', 'Inspired Tidal Volume_change', 'Mean Airway Pressure_change', 'Respiratory Rate_change']].head(20)
from sqlalchemy_arxiv import Session, articles_raw, articles_vectors $ from sqlalchemy import func
temps_df.loc['2014-07-03']
for doc in corpus: $     print(doc)
sample = df2.sample(df2.shape[0], replace=True) $ old_page_converted = sample.query('landing_page == "old_page"')['converted'] $ old_page_converted.mean()
from selenium import webdriver $ from bs4 import BeautifulSoup $ driver = webdriver.PhantomJS() $ ieeeUrl='https://conferences.ieee.org/conferences_events/conferences/search?q=*&date=all&from=2018-01-01&to=2018-12-31https://conferences.ieee.org/conferences_events/conferences/search?q=*&date=all&from=2018-01-01&to=2018-12-31&pos=3' $ driver.get(ieeeUrl)
spark.sql("select count(*) from ufo_data").toPandas()
image_predictions_copy[image_predictions_copy.p1_dog == True]
trips.trip_duration.hist() $ plt.xlabel('Trip Duration in Seconds') $ plt.ylabel('Number of Trips') $ plt.suptitle('Trip Duration Distribution') $ plt.show()
with open('./data/model/age_prediction_sk.pkl', 'wb') as picklefile: $     pickle.dump(grid, picklefile)
df['lead_mgr_counts'] = df.groupby(['lead_mgr'])['lead_mgr'].transform('count') $
autos = autos[autos['price'].between(1,351000)] $ autos['price'].describe()
len(df2[(df2['group']=='treatment') & (df2['converted'] == 1)]) / len(df2[df2['group'] == 'treatment']) 
forest = RandomForestClassifier(max_depth=10, n_estimators=5) $ forest.fit(X_train_matrix, y_train)
print(roc_auc_score(y_test, y_pred))
pcaData = pd.DataFrame(data=pcaData) 
df_uk[df_uk['ab_page'] == 0]['converted'].mean()
checkCustomer(2082)
from scipy.stats import norm $ norm.cdf(z), norm.ppf(.95)
df.groupby('Year')['Points'].agg([np.sum, np.mean, np.std])
import pymms $ mysdc = pymms.MrMMS_SDC_API(start_date='2017-07-11', end_date='2017-07-11', $                             data_root='/Users/argall/MrWebData/mms/')
features = create_feature_df(df=df_including_future_games)
df_users_3=df_users_2
def normalize_event_properties_column(df): $     df_event_properties = json_normalize(data=df.event_properties) $     df_event_properties = df_event_properties.set_index(df.index) $     standardize_column_names(df_event_properties) $     return df.merge(df_event_properties, left_index=True, right_index=True) $
t = 4 $ m_test['predicted_purchases'] = bgf_test.conditional_expected_number_of_purchases_up_to_time(t, m_test['frequency'], m_test['recency'], m_test['T']) $ m_test.sort_values(by='predicted_purchases').tail(15)
punct_re = r'' $ trump['no_punc'] = ...
df_new['abp_CA'] = df_new['ab_page']*df_new['CA'] $ df_new['abp_UK'] = df_new['ab_page']*df_new['UK'] $ df_new['abp_US'] = df_new['ab_page']*df_new['US']
plt.plot(df['transmission'].sort_values(), df['id'], 'ro') $ plt.xlabel('Transmission') $ plt.ylabel('Mechanism ID') $ plt.show()
from splinter import Browser $ from bs4 import BeautifulSoup
df_wide = pd.DataFrame(np.random.randint(0, 100, 25).reshape(5, 5), $                     index=list('abcde'), $                     columns=list('vwxyz')) $ df_wide
(df2.query('group == "control"').converted).mean()
poverty_data=poverty.iloc[poverty_data_rows,:]
len(toNodes), len(fromNodes)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df3.set_index('user_id'), how='inner')
sns.violinplot(x="borough", y="complaint_type",data = samp311)
prob_new_page = df2.query('landing_page == "new_page"').user_id.count() / df2.user_id.count() $
validation_features = bind_features(validation, train_test="train").cache() $ validation_features.count()
acc.find(description='DeScription 12', case=True)
my_df.columns = ['text', 'polarity'] $ my_df.head() $
nb = naive_bayes.GaussianNB() $ cv_score = cross_val_score(nb,X_train,y_train,cv=10)
test_orders_prodfill.head()
tweet_en['text'].apply(lambda x: "I'm at" in x).value_counts()
tw_clean['timestamp'] =  pd.to_datetime(tw_clean['timestamp'], format='%Y-%m-%d %H:%M:%S +%f')
lin_svc_clf.fit(X_train_cont_doc, Y_train_lab)
number_rows = len(df) $ print('The number of rows in the dataset is {}'.format(number_rows))
df2[df2.duplicated(['user_id'],keep=False)]
df[['country','currency']].head()
few_recs = pd.read_csv("../data/microbiome/microbiome.csv", nrows=4) $ few_recs
with graph.as_default(): $     train_epoch = 58 $     train_batch = 0 $     saver = tf.train.Saver(var_list=tf.global_variables()) $     saver.restore(sess,"models/{}/model_{}".format(model_name,1))
df = pd.read_csv('titanic.csv')
API_KEY = 'vq_k_-sidSPNHLeBVV8a'
mit.date = pd.to_datetime(mit.date)
df[['retweet_count', 'favorite_count']].describe()
classifier=LogisticRegression() #classifier using Logistic regression
kl = pd.read_csv(path+"datas/kowloon.csv", index_col=0)
df_clean.reset_index(inplace=True)
c.count('\n')
f_ip_app_minute_clicks.show(1)
to_week = lambda x: x.dt.week
model = pipeline.fit(train)
image_predictions_clean = image_predictions_clean[(image_predictions_clean.p1_dog == True) | $                                                   (image_predictions_clean.p2_dog == True) | $                                                   (image_predictions_clean.p3_dog == True)]
sim_pnew_minus_pold = (new_page_converted.mean() - old_page_converted.mean()) $ sim_pnew_minus_pold
months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug']
list(db.osm.aggregate([{"$group":{"_id": "$type", "count":{"$sum": 1}}}]))
to_be_predicted_Day1 = 82.55 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new
float(pca.explained_variance_ratio_[0])*100
df_byzone.head(2)
tree = DecisionTreeClassifier(criterion='entropy',max_depth=5) $ tree.fit(X_train,y_train)
acc.find(description='DeScription 12')
import pandas as pd $ git_log = pd.read_csv('datasets/git_log.gz', $                       sep='#',encoding='latin-1',header=None, $                       names=['timestamp', 'author']) $ git_log.head(5)
complete_df['group_country'] = complete_df['group'] + '_' + complete_df['country'] $ complete_df.head()
df[df.index.month.isin([1,2,3])].head()
weekly_cases.cumsum().plot()
new_page_converted=np.random.binomial(n_new,p_new) $ new_page_converted
to_be_predicted_Day1 = 36.31 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
students['weight']
log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = log_mod.fit() $ results.summary()
prob_treat =df2.query("group=='treatment'").converted.mean() $ print('The probality of an individual converting in the treatment group converting is {}'.format(prob_treat))
df_modeling_categorized = pd.get_dummies(df_modeling, columns=['country', 'currency'])
systems.to_sql(con=engine, name='systems', if_exists='replace', flavor='mysql',index=False)
df.drop('zipcode_initial', axis=1, inplace=True)
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US']])
sample_weight = np.array([1 if not p else 1.25 for p in joined_train.perishable])
df = pd.read_csv('data.csv', parse_dates=['created_date'])
df_unique = df.user_id.nunique() $ print("There are {} unique users in the dataset.".format(df_unique))
df_test.head()
d6tstack.convert_xls.XLStoCSVMultiFile(cfg_fnames,output_dir = 'test-data/output', $                                        cfg_xls_sheets_sel_mode='name_global',cfg_xls_sheets_sel='Sheet1',logger=PrintLogger()).convert_all() $
import pandas as pd $ row_df = pd.DataFrame(rows) $ row_df.head() $ row_df.to_excel("comparison.xls")
features_df.head(10)
data3.groupby(['year']).sum().reset_index()
df.num_comments= df.num_comments.apply(lambda x:0 if x <= 74 else 1)
df_unique_providers.tail()
mydata.loc['1995-01-03'] # Specifies the date that you want
new_pred_prob = grid.predict_proba(X_new)[:, 1] $ new_pred_prob
month_zero = autos.loc[autos["registration_month"] == 0, :].index $ autos = autos.drop(index = month_zero)
NYG = pd.read_excel(url_NYG, $                     skiprows = 8)
sandwich_train_model.print_topics(num_topics = 10 ,num_words = 10)
zipShp.head()
sales_vs_targets = pd.merge(sales, targets) $ print(sales_vs_targets)
df_characters.sort_values('num_lines', ascending=False).head(10)
out.to_csv('./data/train_for_map.csv', encoding='utf-8', sep=',')
with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f: $     f.create_array(where='/', $                    name='array_1d', $                    title='A one-dimensional Array', $                    obj=[0, 1, 2, 3])
df_comment.sort_values(by='likes_count',ascending=False) $ df_comment = df_comment[df_comment.likes_count>10]
sourcedict = {'source':source, 'system id': systemid, 'system':system, 'pls':pls, 'source type': sourcetype, $           'source use':sourceuse, 'win':win, 'wrnum':wrnum, 'DEHN source id':sourcecode, 'source id':sourceid} $ sources = pd.DataFrame(sourcedict)
statetotals = pd.DataFrame(locations['State'].value_counts()) $ statetotals.head(n=10)
data_year_df = pd.DataFrame.from_records(data_year, columns=('Date','Station','Prcp')) $ data_year_df.head()
df = file.to_dataframe(mode='pivot') $ df.head()
con = sqlite3.connect('db.sqlite') $ con.commit() $ con.close()
!cat Data/microbiome.csv
df_significant_feature = pd.DataFrame(significant_features, index=range(len(significant_features)), columns=['xfeature'])
SVPOL(data/'realdata'/'DD.dat').to_dataframe().head()
review_2
df_yt['yt_id'] = df_yt['url'].apply(yt.strip_video_id_from_url)
flight_cancels["SCHED_DEP_DATETIME"] = pd.DatetimeIndex(flight_cancels.SCHED_DEP_DATETIME) $ flight_cancels.set_index("SCHED_DEP_DATETIME", inplace=True) $ flight_cancels = flight_cancels.drop([pd.Timestamp('2014-03-09 02:00:00'), pd.Timestamp('2014-11-02 01:00:00'), pd.Timestamp('2015-03-08 02:00:00'), pd.Timestamp('2015-11-01 01:00:00'), pd.Timestamp('2016-03-13 02:00:00'), pd.Timestamp('2016-11-06 01:00:00'), pd.Timestamp('2017-03-12 02:00:00')]) $ flight_cancels.index = flight_cancels.index.tz_localize('America/New_York')
f = "/home/sala/Work/Data/SwissFEL/Storage/list.all-files.bz2" $ headers = ["Inode number", "gen number", "Snapshot ID", ] $ headers += ["kb_allocated", "sep1", "filesize", "sep2", "user_id", "sep3", "fileset_name", "sep4", "creation_date", "creation_time"] $ headers += ["Seperator", "Filename"] $ df = pd.read_csv(f, sep=r"\s+", names=headers, ) $
df_clean['dog_nick'] = df_clean[['doggo','floofer','pupper','puppo']].mask(df_clean[['doggo','floofer','pupper','puppo']].eq('None')).fillna('').sum(axis=1)
raw_full_df.shape
df.head()
! pip install future
AAPL.iloc[0:200,0:4].diff().hist(bins=25)
closing_prices_shift = closing_prices[1:] $ two_day_changes = [abs(future - now) for (now, future) in zip(closing_prices, closing_prices_shift)] $ max_two_day_change = max(two_day_changes) $ print("Largest change between any two days: {}".format(max_two_day_change))
meals_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/meals.csv')['Body'].read().decode('utf-8') $ meals = pd.read_csv(StringIO(meals_csv_string), header=0)
df_nona.groupby('district_id').district_size.mean().hist(bins=12) $ df_nona['segment'] = pd.cut(df_nona.district_size, bins=[0,1000,4000,120000]) $ df_nona.groupby('segment').district_id.nunique()
access_token = "" $ access_token_secret = "" $ consumer_key = "" $ consumer_secret = ""
M_check1 = session.query(Measurement).statement $ M_df = pd.read_sql(M_check1,session.bind) $ M_df.head()
df['destination'] = [1 if x == 'US' else 0 for x in df.country_destination]
emails_dataframe['institution'].value_counts()
fb_desc = fb.description
df2.describe() $
print(sensor.site) $ print(sensor.unit)
df["grade"] = df["raw_grade"].astype("category") $ df["grade"]
max_tweets=1 $ for tweet in tweepy.Cursor(api.search,q="ivanka").items(max_tweets): $     print(tweet)
active_stations = session.query(measurements.station, func.count(measurements.station)).\ $             group_by(measurements.station).\ $             order_by(func.count(measurements.station).desc()).all() $ active_stations
tsprior2
classifier.fit(x_train, y_train, steps = 100)
s.resample('30D', fill_method='ffill').head(10)
data[['clean_text','issue']].head(5)
tt_final = ttarc_clean.merge(tt_json_clean, left_on = 'tweet_id', right_on='id', how='outer') $ tt_final = tt_final.merge(imgp_clean, left_on = 'tweet_id', right_on='tweet_id', how='outer') $ tt_final.timestamp = pd.to_datetime(tt_final.timestamp, infer_datetime_format=True) $ tt_final.info()
df_ml_6208_01.tail(5)
print(len(data['data']['children']))
n_new = df2[df['group']=='treatment'].shape[0] $ n_new
fdist.hapaxes()
tc_ecolls_to_exclude = pd.read_csv('IDs to exclude\elec colls to exclude\TC electronic colls to exclude.txt', $                                    sep='\t', dtype=str) $ tc_ecolls_to_exclude
vect = TfidfVectorizer(ngram_range=(2,4), stop_words='english') $ summaries = "".join(josh_tweets['text']) $ ngrams_summaries = vect.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(20)
df_amznnews_clsfd = df_amznnews.iloc[yes_ind] $ print(df_amznnews_clsfd.info()) $ print() $ print(df_tick.info())
cp311 = pd.concat([cp311,dfstatus],axis=1,join='inner').copy()
ml=clean_prices.groupby(['name']).tail(1).loc[:,'EMA_ratio':'5dayvol']
b = regr.intercept_[0] $ b
tobs_results = session.query(Measurement.station, Measurement.tobs).filter(Measurement.date.between('2017-08-01', '2018-07-31')).all() $ tobs_results
combined_df4['bin_label']=combined_df4['bill_bal'].apply(lambda x: 1 if float(x)<0 else 0) $ combined_df4['bin_label'].value_counts()
df_tweet_clean.tweet_id = df_tweet_clean.tweet_id.astype(str)
state_DataFrames['OH_D_con'].head(30)
pd.Series(5, index=[100,200,300])
!pwd #location of the current working directory. $
g_all = pd.read_csv('https://assets.datacamp.com/production/course_2023/datasets/gapminder.csv') $ print(g_all.shape) $ print(g_all.head())
test_prediction = rdf_clf.predict(X_test[columns])
jobs.loc[(jobs.FAIRSHARE == 132) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
df['UP']=np.where(df['Close']>df['Open'],1,0) $ df
test_data = pd.read_csv('data/kaggle_data/test-val.csv', header=None, sep=" ", names=list_feature_names) $ test_data.head(5)
from sklearn.metrics import jaccard_similarity_score $ from sklearn.metrics import f1_score $ from sklearn.metrics import log_loss
s = np.array([3, 2, 4, 1, 5]) $ s[s > np.mean(s)]  # Get the values above the mean
store_items.fillna(method = 'backfill', axis = 0)
f.shape
weather_dt[0][1]
data['affair'] = (data.affairs > 0).astype(int)
pol_users.drop('array_agg', axis=1, inplace=True)
from datetime import datetime $ datetime(year=2015, month=7, day=4)
filled.ix['2011-11-03':'2011-11-04'].plot() $ plt.ylim(103.5, 104.5)
size_control
hours = bikes.groupby('hour_of_day').agg('count') $ hours['hour'] = hours.index $ hours.start.plot(color = 'lightgreen') $
pd.Period('2012', freq='A')
statement = "SELECT unique_key, complaint_type, created_date, closed_date FROM noise_311" $ cursor.execute(statement) $ complaints = [] $ for row in cursor: $     complaints.append(row) $
extract[['rating_numerator', 'rating_denominator']] = pd.DataFrame(extract.rating.values.tolist(), index = extract.index)
df['Product'].value_counts().head()
import pandas_datareader.data as web $ import datetime $ start = datetime.datetime(2013,1,1) $ end = datetime.datetime(2017,11,26) $ df = web.DataReader('KOTAKBANK.NS', 'yahoo', start, end)
plt.scatter(X,y2) $ plt.plot(X, np.dot(X_17, linear.coef_) + linear.intercept_, c='red') $ plt.plot(X, np.dot(X_17,ridge2.coef_) + ridge2.intercept_, c='b') $ plt.xlabel('Hour of Day') $ plt.ylabel('Count')
h2o_age = h2o.H2OFrame(df_age)
for el in twitter_archive_master['name'].sample(20): $     if el != 'nan': $         print(el.split(' '))
df2[df2['user_id']==773192]
bucket.upload_dir('data/wx/tmy3/raw/', 'wx/tmy3/raw', clear_dest_dir=True)
tfav_a.plot(figsize=(16,4), label="Likes", legend=True) $ tret_a.plot(figsize=(16,4), label="Retweets", legend=True);
rfc = RandomForestClassifier() $ model=rfc.fit(X_train, y_train)     # fitting the training data
tweets_df.favorite_count.describe()
dates = ['2016-02-15', '2016-03-15', '2016-04-15']
common_keywords = vectorized.columns
likes['content'] = likes['title'].map( lambda x : contentMask(x))
balance = r.groupby(level=0)[['net']].sum()
output = pipeline.fit(flight7).transform(flight7) $ output = output.withColumnRenamed('price_will_drop_num', 'label') $ output.cache()
transactions_to_products_response = get_transactions_to_products_report(analytics) $ transactions_to_products_df = create_dataframe_from_response(transactions_to_products_response) $ transactions_to_products_df = transactions_to_products_df[['ga:transactionId', 'ga:dimension2', 'ga:date', 'ga:productName', 'ga:itemRevenue', 'ga:itemQuantity']] $ transactions_to_products_df
from sklearn.feature_extraction.text import TfidfVectorizer $ tfidfvec = TfidfVectorizer() $ dtm_tfidf_df = pandas.DataFrame(tfidfvec.fit_transform(df.body).toarray(), columns=tfidfvec.get_feature_names(), index = df.index) $ dtm_tfidf_df
scr_end_date = [SCR_PLANS_df.loc[cid,'canceled_at'][np.argmax(SCR_PLANS_df.loc[cid,'scns_created'])] if SCR_PLANS_df.loc[cid,'cancel_at_period_end'][np.argmax(SCR_PLANS_df.loc[cid,'scns_created'])] == False else SCR_PLANS_df.loc[cid,'current_period_end'][np.argmax(SCR_PLANS_df.loc[cid,'scns_created'])] if SCR_PLANS_df.loc[cid,'cancel_at_period_end'][np.argmax(SCR_PLANS_df.loc[cid,'scns_created'])]==True else None for cid in scr_churned_ix]
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_opb_chord_saddle[0][0])) $ perr = np.sqrt(np.diag(pcov_opb_chord_saddle[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
grid_search.best_params_
test_features = bag_of_words_vectorizer.transform(test_corpus)
df_new['ab_page_US'] = df_new['ab_page'] * df_new['US'] $ df_new['ab_page_UK'] = df_new['ab_page'] * df_new['UK'] $ df_new['ab_page_CA'] = df_new['ab_page'] * df_new['CA']
intersections_final = intersections[final_columns]
averages = buckets_to_df(response['aggregations']['0']['buckets']) $ averages = averages.fillna(0)
df = df[df["name_region"].str.contains("Uppsala")] $ display(df.head())
cust_data["NewColumn"] = "" $
df2_new['intercept']=1 $ logit_c=sm.Logit(df2_new['converted'],df2_new[['intercept','CA','UK']]) $ result_c=logit_c.fit() $ result_c.summary()
rnd_reg_2.fit(X_train_2, y_train_2)
def data_scientist(x): $     if 'Data Scientist' in x: $         return 1 $     return 0 $ df_more['Data Scientist'] = df_more['Title'].apply(data_scientist)
page_soup.title
merged.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount", ascending=False).head()
print(df.loc[sd:ed].head()) #more pythonic? $ print(df[sd:ed].head())
plt.hist(p_diffs) $ plt.xlabel('p_diffs') $ plt.ylabel('Conversion Count') $ plt.title('Simulated p_diffs (10k)');
final_df.corr()["ground_truth_adjusted"][names]
high_rev_acc = accounts[accounts[' Total BRR '] >= 500000] $ high_rev_acc_opps = pd.merge(high_rev_acc, count_bldg_opps, on=['Account ID'], how='inner') $ high_rev_acc_opps_net = pd.DataFrame(high_rev_acc_opps[high_rev_acc_opps['On Zayo Network Status'] == 'On Zayo Network'])
data.dropna(thresh=2)
for col in b_cal.columns: $     print(f'{col}....{len(b_cal[col].unique())}')
type(df.Date[0])
stock_data = pd.concat([data_1, data_2])
authors_grouped_by_id_saved.schema
df.dtypes
to_trade = np.abs(dfprediction['y_hat']) > 0.01 $ to_trade.sum() # will result in 29 trades $ dfprediction[to_trade]
country_dummies = pd.get_dummies(df_new['country']) $ df_new = df_new.join(country_dummies) $ df_new.head(5)
properati.drop(inplace=True,labels=['extra','image_thumbnail','state_name','description','title','properati_url'],axis=1)
df_tweet_clean.describe()
type(tables[0]) #just the first tqble
parties['Unique Key'].groupby(by= parties.index.dayofweek).count().sort_values(ascending = False).head(1)
print('Number of unique user_ids in df2 is {}'.format(len(df2.user_id.unique()))) $
sl_data.Description.value_counts()
df2 = df2.drop(2893) $ df2.info()
df_master_select.shape
reddit_comments_data.groupby('parent_id').count().orderBy('count', ascending = False).show(100, truncate = False)
join_c.orderBy(F.desc("party_id_orig"),F.desc("aggregated_prediction")).select('party_id_orig','aggregated_prediction','predicted').show(500)
pd.Timestamp('now', tz='Asia/Riyadh')
sample_sizes.show()
df2['intercept'] = 1 $ df2[['drop_this', 'ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('drop_this', axis=1) $ df2.head()
qualification = segmentData[segmentData['opportunity_conversion'] == 'convertedLead']
predicted_image/255
SBChamps.set_index("Team")
scr_retention_df.to_csv('retention_paid_518.csv')
ccl.set_index("Date", inplace=True)
archive_df_clean.loc[archive_df_clean['name'] == 'Bella'] $
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets']))) $ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets']))) $ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
df.loc['a', 'ii', 'z']
rounds['announced_on'] = pd.to_datetime(rounds['announced_on'], errors = 'coerce')
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn as sns
cityID = 'dc62519fda13b4ec' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Tampa.append(tweet) 
num_assets
autos.unrepaired_damage.value_counts(normalize=True, $                                      dropna=False)
train = pd.read_csv('./data/train.csv')
building_pa.sample(5)
Gaussian
lrs=[lr/9,lr/3,lr]
e_date = dataset['User_Created_At'].min() $ l_date = dataset['User_Created_At'].max() $ print('Earliest account creation date: ', e_date) $ print('Latest account creation date: ', l_date)
cleaned_texts = df.cleaned_text.apply(lambda x: ' '.join(x))
x1 = merged_df_cut["word"] $ x2 = merged_df_cut["category"] $ y1 = merged_df_cut["weight"] $ y2 = merged_df_cut["weight1"]
sheets = {last_word(sheet_name): data_file.parse(sheet_name) $           for sheet_name in cam_sheet_names}
strip_func = lambda x: x.strip() if isinstance(x, str) else x $ df = df.applymap(strip_func)
autos['date_crawled'].str[:10].value_counts().sort_index()
train_df["display_address"] = encoding(train_df,"display_address") $ train_df["street_address"] = encoding(train_df,"street_address") $ train_df["manager_id"] = encoding(train_df,"manager_id") $ train_df["building_id"] = encoding(train_df,"building_id")
texts = dfg['text']
print(archive_copy.shape[0], predictions_copy.shape[0], tweet_data_copy.shape[0])
todaysFollowers.to_csv('Data/todaysFollowers_'+date+'.csv',sep=';',index=False)
data = raw_data[['goal', 'country', 'currency', 'backers_count', $                  'deadline', 'launched_at']] $ data.head()
twitter_archive_master = clean_archive.merge(data_tweets) #automatically merge on the intersection of the columns $ twitter_archive_master= twitter_archive_master.merge(clean_predictions)
terms_stop = [term for term in list(itertools.chain.from_iterable(obama_cleaned_words)) if term not in stop] $ count_stop = Counter() $ count_stop.update(terms_stop) $ print(count_stop.most_common(5))
total2=total[total['priority']=='A'] $ total2.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2016_1/objsA_fall2016.csv',index=False)
theft.dtypes
itemTable.shape[0]
all_labels = [] $ for i in train_labels: $     all_labels.extend(i) $ for i in test_labels: $     all_labels.extend(i)
nr_switches = sum(list_pth_coin.shift(1) != list_pth_coin) $ nr_switches = nr_switches - 1 # but the first one was NA $ print("The number of months a coin at place 10 switched was", $       nr_switches.sum(), $       "out of", len(mcap_mat), "months")
df1['const'] = 1
rate = survey[survey.question.str.contains('Please rate this assessment from one to five, five being the highest')].copy()
f = open("insertObs.sql", "w") $ f.write(s) $ f.close()
df_wm['Polarity'] = df_wm['cleaned_text'].apply(polarity_calc) $ df_wm['Subjectivity'] = df_wm['cleaned_text'].apply(subjectivity_calc)
df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', nrows=500000) $ df.head()
dates_cols = ["date_crawled", "ad_created", "last_seen"] $ for temp in dates_cols: $     autos[temp] = autos[temp].str[:10].str.replace('-','').astype(int)
stock_daily.head()
cov_matrix = daily_ret.cov() $ cov_matrix
weekdays_avg.fillna('No Data',inplace=True) $ weekdays_count.fillna('No Data',inplace=True) $ weekends_avg.fillna('No Data',inplace=True) $ weekends_count.fillna('No Data',inplace=True)
for col in ['MultipleLines']: $      df_raw[col].replace({'Yes': 1, 'No': 0, 'No phone service': 2}, inplace=True)
df.head()
tweets[tweets['retweeted'] == 1]['chars'].describe()
df_uk[df_uk['ab_page'] == 1]['converted'].mean()
discrp['ride_total_time'] = discrp['completed_on'] - discrp['started_on'] $ discrp[['miles', 'total_fare', 'ride_total_time']] $
features2 = ['monetary', 'frequency', 'recency']
df2 = df.query('group == "control" & landing_page == "old_page" | group == "treatment" & landing_page == "new_page"')
print(f"Loaded {len(tw)} of {bininfo['number-selected-tweets']} tweets.")
grid_search = H2OGridSearch(model = rf_v1, $                             hyper_params = hyper_parameters, $                             search_criteria = criteria)
df.head()
import pandas as pd $ import pickle $ df = pickle.load(open("C:/Users/Anke/Masterarbeit/df_concat_2.pickle", "rb")) $ type(df)
pandas_df = pd.read_csv("./SIGHTINGS.csv") $ pandas_df.head()
url = 'https://www.cgv.id/en/schedule/cinema/041/2018-02-22' $ request = urllib.request.Request(url) $ page = urllib.request.urlopen(request) $ soup2 = BeautifulSoup(page,"lxml") $ soup2.find_all('div', class_='schedule-container')[0].select('.disabled')
default = donations['Donation Amount'].value_counts().head(10) $ default.plot(kind='bar') $ plt.title('Donation Amount') $ plt.ylabel('frequency') $ plt.xlabel('Dollars Donated')
month+1 # from above
lims_query = "SELECT specimens.name as cell_name, specimens.barcode, specimens.donor_id, donors.id \ $ FROM specimens JOIN donors ON specimens.donor_id = donors.id \ $ WHERE specimens.ephys_roi_result_id IS NOT NULL" $ df = get_lims_dataframe(lims_query) $ df.tail()
df['converted'].mean()  #returns average of a value
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[Nnew, Nold], alternative='larger') $ print("z-score:", z_score,",p-value:", p_value)
print 'A Datetime index range selection:' $ msft.loc['2012-01-03':'2012-01-5']
temperatura_global = pd.read_csv('GlobalTemperatures.csv')
for cardname in all_cards.index.unique(): $     reprints = all_cards.loc[cardname] $     if len(reprints.shape) > 1: $         merged_dicts = merge_dicts(list(reprints.printings)) $         reprints.iat[0, list(reprints.columns).index("printings")].update(merged_dicts) # updates the first entry of the card with the completed dictionary
sampleSize=ratings.groupby("rating").count().reset_index()['userId'].min() $ sampleSize
print(type(some_rdd),type(some_df)) $ print('some_df =',some_df.collect()) $ print('some_rdd=',some_rdd.collect())
print train.shape $ print weather.shape
combined_turnstile.groupby(["SCP", "DATE"]) [["TOTALENTRIES"]].sum()
df.isnull().sum()
tweet_df_clean.columns
df_providers = df_providers[['id_num', 'drg', 'name', 'street', 'city', 'state', 'zipcode', 'region', $        'discharges', 'avg_charges', 'total_payment', 'medicare_payment', $        'year', 'drg3', 'disc_times_pay', 'Zip', 'zip_length', 'discharge_rank', $        'payment_rank']]  
autos.info()
import numpy as np $ import pandas as pd $ import xarray as xr $ for m in [np, pd, xr]: $     print(m.__name__, m.__version__)
assert sorted(troll_tweets.columns) == sorted(expected_tweet_cols) $ assert sorted(pol_tweets.columns) == sorted(troll_tweets.columns)
df3[df3["user_id"] == 851104]
rank_meters['daytime_7_12'].most_common(11)[1:]
aoi = {u'geometry': {u'type': u'Polygon', u'coordinates': [[[-121.3113248348236, 38.28911976564886], [-121.3113248348236, 38.34622533958], [-121.2344205379486, 38.34622533958], [-121.2344205379486, 38.28911976564886], [-121.3113248348236, 38.28911976564886]]]}, u'type': u'Feature', u'properties': {u'style': {u'opacity': 0.5, u'fillOpacity': 0.2, u'noClip': False, u'weight': 4, u'color': u'blue', u'lineCap': None, u'dashArray': None, u'smoothFactor': 1, u'stroke': True, u'fillColor': None, u'clickable': True, u'lineJoin': None, u'fill': True}}}
archive.sample(10)
df.shape[0]
tweets_unique = tweets092815.append(tweets092315.append(tweets.append(oldtweets)))
missings = messy.isnull() $ missings
df.at[dates[0],'A']
split_pct=0.75 $ X_train, y_train, X_test, y_test, scaler = train_test_split(df, split_pct=split_pct, scale_data=True)
df_tweet_data
breakfastlunchdinner['totals'] = (breakfastlunchdinner['breakfast'] + $                                   breakfastlunchdinner['lunch + brexits'] + $                                   breakfastlunchdinner['dinner']) $ breakfastlunchdinner.sort_values('totals',ascending=False, inplace=True) $ breakfastlunchdinner.reset_index()
final['cv_avg_tenure'] = final['totalExperience']/final['cv_total_jobs']
p(sys.getdefaultencoding)
from xgboost import XGBRegressor
df_new=df2.query("landing_page=='new_page'") $ n_new=len(df_new) $ n_new
dfNYC = pd.read_csv("311_Service_Requests_from_2010_to_Present.csv")
autos=autos.drop('name',1)
pts_df = points.unstack() $ pts_df
df_new2['intercept'] = 1 $ df_new2.head()
def day_of_week(date): $     days_of_week = dict(zip(range(7), ['monday','tuesday','wednesday','thursday','friday','saturday','sunday'])) $     return days_of_week[date.weekday()] $ day_of_week(lesson_date)
pd.concat([s1, s2], verify_integrity=True)
pd.datetime.today()
pd.Timestamp('2017-01-10 8:05AM')
nan_tables = {} $ for res_key, df in data_sets.items(): $     data_sets[res_key], nan_tables[res_key] = find_nan( $         df, res_key, headers, patch=True)
records3 = records.copy()
obs_start_date = (dt.datetime.strptime(vac_start_date,"%Y-%m-%d") - dt.timedelta(days=365)).strftime("%Y-%m-%d") $ temp_ranges = calc_temps(obs_start_date,vac_start_date)
autos["odometer"] = autos["odometer"].str.replace("km", "") $ autos["odometer"] = autos["odometer"].str.replace(",", "")
stocks.duplicated().sum()  # Checking for duplicated data 
print('Get subset data of rows excluding rows no. 5, 12, 23, and 56') $ lst_exclude = [5, 12, 23, 56] $ df[~df.index.isin(lst_exclude)].head(15)
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = "smaller") $ print("The computed z-score is: {} and the p-value is: {}".format(z_score, p_value))
df.iloc[:,1:3] = df.iloc[:,1:3].apply(pd.to_datetime, errors='coerce') $ df['Trip_duration']=df['Lpep_dropoff_datetime']-df['lpep_pickup_datetime']
new_n = df2[df2['group'] == 'treatment'].shape[0] $ new_n
seventh
ufos_df3 = sqlContext.sql("select Count, Reports, substr(Reports, 1, 4) as year from ufo_sightings limit 10") $ ufos_df3.show(3)
checkDF = pd.read_csv(path_join(DATA_DIR, 'best_submition/submission_RIt.csv'))
trt_con = df2.groupby('group', as_index=False).describe()['converted']['mean'][1] $ print("P(trt_con) = %.4f" %trt_con)
df.to_pickle("clean_df.pkl")
len(data_2017_subset[data_2017_subset.borough.str.match('(Unspecified)', na=False)])
plt.show()
def RMSLE(y, pred): $     return metrics.mean_squared_error(y, pred)**0.5
df1.info(), df2.info(), df3.info()
all_brands = autos["brand"].unique() $ print("Unique brands in autos dataset:\n{}".format(all_brands))
collect.get_iterator()
Lab7_Redesign = pd.merge(Lab7, Stock_price, left_on='Ticker_Symbol',right_on='Symbol', how='right')
df_image_tweet2 = df_image_tweet[df_image_tweet.tweet_id.isin(id_list2)] $ df_clean4       = df_clean3[df_clean3.tweet_id.isin(id_list2)]
df.loc[df['user_id'] == 773192]
department_df_sub.apply(lambda x: x.max() - x.min()) # apply function to each column 
grid_df.tail()
np.arange(10,25,5)
for r in accuracy_result: $     print r['section']
old_page_converted = np.random.choice([0, 1], $                                       p=[1. - p_convert_control, p_convert_control], $                                       size=n_control, $                                       replace=True) $ old_page_converted.mean()
X_test['Text_Tokenized'] = X_test.Text.apply(process) $ X_test.head()
df17 = pd.read_csv('2017.csv')
df_json = pd.read_json('tweet_json.txt', orient='records', lines = True) $ df_json.head(3)
df_date_vs_count.head()
df_new.head()
it_df['bidDeadline'] = pd.to_datetime(it_df['bidDeadline'], errors='coerce') $ it_df["buyer_name"] = it_df.buyers.apply(getMainBuyerName)
p_diffs = [] $ for i in tqdm(range(1,10000)): $     new_converted = np.random.choice([1, 0], size=len(df_new), p=[P_mean, (1-P_mean)]) $     old_converted = np.random.choice([1, 0], size=len(df_old), p=[P_mean, (1-P_mean)]) $     p_diffs.append(new_converted.mean() - old_converted.mean())
df_survival_by_donor = df_survival[['Donor ID', 'Donation Received Date']].groupby('Donor ID') $ mean_time_diff = df_survival_by_donor.apply(lambda df: df['Donation Received Date'].diff().mean()) $
sns_path = "../Datasets/Beat_Data/stop_search_beat.csv" $ s_n_s_df =  pd.read_csv(sns_path)
log_regression = sm.Logit(combined_df['converted'], combined_df[['UK', 'CA']]) $ output = log_regression.fit() $ output.summary()
ms_df = mt_df[mt_df['mortality_salience'] == 1] $ print("HI tweets between 18:07-18:21 AM 2019-01-13 labelled as mortality_salience: {}".format(len(ms_df))) $ print("percent mortality salience: {}".format(len(ms_df)/len(mt_df)))
grouped_by_date_df = full_df.groupby('created')['listing_id'].count().reset_index().copy() $ grouped_by_date_df.columns = ['created_date','count_of_listings'] $ grouped_by_date_df.head(5)
weblogs.count()
import pandas as pd $ typ = 'resampled' $ file_name = "data/survey_Brussels_NonResidentialElectricity_wbias_projected_dynamic_resampled_10000_{}.csv" $ sample_survey = pd.read_csv(file_name.format(2016), index_col=0) $ sample_survey.head()
piv_train = df_train.shape[0] $ df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True) $ df_all = df_all.drop(['id', 'date_first_booking'], axis=1) $ df_all = df_all.fillna(-1)
dftop.head()
n_old = sum(df2.group == 'control') $ n_old
df["retweeted_screen_name"].value_counts().head(20)
df_characters[df_characters.character_id>0].sort_values('num_lines', ascending=False).head(10)
df_final_edited.head(1)
len(results)
df_u=df.user_id.nunique() $ print("Number of unique users in the dataset is :{}".format(df_u))
data.info()
types_subtypes.to_csv('../output/subtypes_complaints.csv')
kDistArray_2 = kDist(crosstab_transformed, 2) $ kDistArray_3 = kDist(crosstab_transformed, 3)
dfm = df.median(axis=0)
squares['square of 3']
df2.query("group == 'treatment'").count()['group']
ozzy.bark()
groceries
m = pd.read_sql_query(QUERY, conn) $ m
convert_new, n_new
actual_diffs = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ actual_diffs
%%bash $ cd /data/LNG/Hirotaka/ASYN $ mkdir -p t $ awk '$2>0.05 && $2<0.95 && $3>0.8 {print $0}' maf001rsq3/PPMIsorted.info |\ $     LANG=C sort > maf001rsq3/PPMImaf05rsq8sorted.info
for Quarter in range(1,5): $     StockData['Date-Quarter{}'.format(Quarter)] = (StockData['Date'].dt.quarter == Quarter).astype(int)
hc.sql('create table asm_wspace.usage_400hz_2017_Q4 as select * from tmp_400hz_usage')
df.head(2)
df_ad_airings_5['state'].value_counts()
most_common_registered_postcodes_slps = active_companies[active_companies['company_type'] == 'Limited Partnership for Scotland'].groupby(['RegAddress.PostCode'])['CompanyNumber']\ $     .agg(lambda x: len(x.unique())).sort_values(ascending=False).head(20) $ most_common_registered_postcodes_slps.to_excel('data/for_further_investigation/top_20_slp_postcodes.xlsx') $ most_common_registered_postcodes_slps
contribs.iloc[imin] 
raw[raw.job_type == 'Organic'].hash.value_counts()[0:5]
acc.get_monthly_balance(range(4))
reviews = np.array(tf.review) $ reviews_vector = vectorizer.transform(reviews) $ predictions = clf.predict(reviews_vector) $
X_test = Feature_test $ X_test[0:5]
df_all_repaid_latest.index.names=['date','rating_base']
df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ['E']) $ df1.iloc[0:2,6] = 1. $ df1
print(well_data.head())
EMBEDDING_FILE = 'input/glove.840B.300d.txt' $ train= pd.read_csv('input/train.csv') $ test = pd.read_csv('input/test.csv')
hrefs = tree.xpath('//a') $ for href in hrefs: $     print(href.text)  $     print(href.attrib)  
weather_mean.loc['HALIFAX':'Ottawa', ['Pressure (kPa)', 'Temp (deg C)']]
inspector = inspect(engine) $ inspector.get_table_names()
reviews['listing_id'].unique()[:5]
df_customers.head() $
val ssc = new StreamingContext(new SparkConf(), Seconds(1)) $ val my_stream = ssc.socketTextStream(hostname, port)
import statsmodels.api as sm $ convert_old = len(df2[(df2['landing_page']=='old_page')&(df2['converted']==1)]) $ convert_new = len(df2[(df2['landing_page']=='new_page')&(df2['converted']==1)]) $ n_old = len(df2.query('group == "control"')) $ n_new = len(df2.query('group == "treatment"'))
len(youthUser2017)
autos["price"] = autos["price"].str.replace("$", "").str.replace(",", "").astype(int) $ autos["price"].head()
df['2017-02']
for item in result_set: $     print(item.index,item.relationship)
df1 = pd.DataFrame({"A":["A1", "A2"], $                     "B":["B1","B2"]},index=[1,2]) $ df2 = pd.DataFrame({"A":["A3", "A4"], $                     "B":["B3","B4"]},index=[3,4]) $ pd.concat([df1,df2])
to_be_predicted_Day2 = 48.63080568 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
df2 = pd.read_csv('2002.csv')
top_supporters['contributor_fullname'] = top_supporters.contributor_firstname + " " + top_supporters.contributor_lastname $
import pandas as pd $ income = pd.read_excel('Household Income Year 2013 to 2017 Aff.xlsx') $ income
autos.describe(include = 'all')
bo.loc[bo['BORO']==1,'BORONAME'][0]
frame2.values
house[:1]
print(df['Confidence'].unique())
shows['fixed_runtime'] = shows['fixed_runtime'].dropna().apply(int)
def answer_eight(): $     Top15=answer_one() $     Top15['Estimated population'] = Top15['Energy Supply'] / Top15['Energy Supply per Capita'] $     return Top15.sort_values('Estimated population',ascending=False).iloc[2].name $ answer_eight()
df3 = pd.DataFrame((df2['Ret_stdev']).resample('M').last())
df_new['CA_ab_page'] = df_new['ab_page'] * df_new['CA'] $ df_new['UK_ab_page'] = df_new['ab_page'] * df_new['UK'] $ df_new.head()
subwaydf['DESC'].value_counts()
df.head(5)
import pandas as pd
AAPL[['close', 'volume']].merge(GOOGL[['close', 'volume']], $                       left_index=True, $                       right_index=True, $                       suffixes=['_AAPL', '_GOOGL'])
ser = pd.Series(["practical","data","science","tutorial"], index = [5,6,7,8]) $ ser
url = "https://space-facts.com/mars/"
df_user['user.name'].value_counts()[:10]
column_check = inspector.get_columns('station') $ for check in column_check: $     print(check['name'],check['type'])
screwtopcupsvolcc=((4.2)/2)**2*5.64;screwtopcupsvolcc#makes one label serving of 90 cal
atloc_opp_dist_tabledata = atloc_opp_dist_count_prop_byloc.reset_index() $ create_study_table(atloc_opp_dist_tabledata, 'locationType', 'remappedResponses', $                    location_remapping, atloc_response_list)
pres_df.drop('time_from_creation_tmp', inplace=True, axis=1) $ pres_df.head()
hours['Hours_Spent'] = hours['AppointmentDuration'] /60
for col in totals_columns: $     games[col+'_HOME'].hist(label=col, alpha=0.25) $ plt.legend()
nba_df.size
sentiments_df = sentiments_df[['Count','User','Date', 'Unix', 'Text','Compound','Positive','Negative','Neutral']] $ print(sentiments_df.head())
(df2.query('group =="control"')['converted']==1).mean()
df.describe()
api = twitter.Api(consumer_key=twitter_key, $                   consumer_secret=twitter_secret, $                   access_token_key=twitter_token, $                   access_token_secret=twitter_token_secret)
y_pred[:5]
nobel = pd.read_excel('data/nobel_prizes.xlsx', sheetname='nobel_prizes') $ population = pd.read_excel('data/nobel_prizes.xlsx', sheetname='population')
df2 = df1.loc[lambda x: pd.notnull(x['Embarked'])] $ df2.shape
freq_port = train_df.Embarked.dropna().mode()[0] $ freq_port
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s') $ logging.debug('Start of program')
mr.at_time('16:00')
groups_topics_unique_df = groups_topics_df.drop_duplicates(subset=['group_id'])
myIP= table1['ip_address'].values $ low= table2['lower_bound_ip_address'].values $ high= table2['upper_bound_ip_address'].values $ lowDiff= [min(myIP[i]-low) for i in range(0,len(myIP))]
csvData[csvData['street'].str.match('.*Drive.*')]['street']
SCN_BDAY.birthdate.describe()
data = data.rename(columns={0:'time',1:'sentiment'})
ax=nypd.groupby(by=nypd.index.hour).count().plot(y='Unique Key', label='NYPD') $ dot.groupby(by=dot.index.hour).count().plot(y='Unique Key', ax=ax, label='DOT') $ dpr.groupby(by=dpr.index.hour).count().plot(y='Unique Key',ax=ax, label='DPR') $ hpd.groupby(by=hpd.index.hour).count().plot(y='Unique Key', ax=ax, label='HPD') $ dohmh.groupby(by=dohmh.index.hour).count().plot(y='Unique Key', ax=ax, label='DOHMH')
df_movies = pd.merge(df, xml_in_sample, left_on=['movieId'], right_on=['authorId'],  how='left')[['movieId', 'authorName']] $ df_movies.drop_duplicates(subset=None, keep='first', inplace=True)
df2 = pd.read_csv('ab_updated.csv')
import numpy as np $ import statsmodels.formula.api as smf $ mod = smf.ols("cellphone ~ np.log(gdp)", dat).fit() $ print(mod.summary())
import hashlib $ print hashlib.md5("puneetbharti@gmail.com").hexdigest()
df2[df2['converted'] == 1].count()[1]/df2.shape[0]
sales_df[sales_df['Product_Id'] == 5].groupby(['Country']).sum()['Quantity'].sort_values(ascending=False) $
callbacks = [EarlyStopping('val_loss', patience=2), ModelCheckpoint('movie_weights.h5', save_best_only=True)]
svr = SVR() $ lnr = LinearRegression() $ knr = KNeighborsRegressor() $ dtr = DecisionTreeRegressor() $ gbr = GradientBoostingRegressor()
df_cen = pd.merge(df_tot, df_geo, on='Tract', how='outer') $ df_cen.sample(5)
regional_hol=regional_holidays[['date','description','locale','locale_name']] $ regional_hol.columns=['date','description_regional_hol','locale_regional','state'] $ pd.DataFrame.head(regional_hol)
from bmtk.analyzer import spikes_table $ spikes_table(config_file='config.json')
fitted = lr.fit(X_train, y_train)
facts_metrics.shape
dot.attr('edge', fontcolor='darkred')
match.groups()
pred = clf.predict(x_test) $ print(metrics.accuracy_score(y_test, pred))
scipy.stats.kruskal(df3["tripduration"], df4["tripduration"])
open_idx = afx['dataset']['column_names'].index('Open') $ print('The index of the "Open" column in the dataset is {}\n'.format(open_idx)) $ afx_data_len = len(afx['dataset']['data']) $ open_values = [entry[open_idx] for entry in afx['dataset']['data'] if entry[open_idx]] $
print("Probability an individual recieved new page:", $       len(df2[df2['landing_page']=="new_page"])/len(df2))
df_train = df_total[df_total['is_test'] == False].drop('is_test', axis=1) $ df_test = df_total[df_total['is_test'] == True].drop('is_test', axis=1)
df2.c2_bin.value_counts()
all_tweets = pd.read_csv('all_tweets.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')
command = "cat %s > %s" %(' '.join([s_lyc_proteins_fasta, a_thaliana_proteins_fasta, $                                o_sativa_proteins_fasta, v_vinifera_proteins_fasta, g_max_proteins_fasta, $                                swiss_prot_embryophte_proteins_filtered_fasta]), all_protein_evidence_fasta) $ send_commands_to_queue('cat_protein_evidence',command, queue_conf)
loans_df.verification_status.value_counts()
store_items = store_items.set_index('pants') $ store_items
expiry = {'ESU2': datetime(2017, 9, 21), $          'ESZ2': datetime(2017, 12, 21)}
df['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['CA', 'US']]) $ results = log_mod.fit() $ results.summary()
df.iloc[]
co_sum = CO_profit['Profit Including Build Cost'].sum() $ buildings_co = len(CO_profit)
s = lv_workspace.get_step_1_object('A')
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer $ from sklearn.decomposition import NMF, LatentDirichletAllocation
from statsmodels.stats.diagnostic import acorr_ljungbox $
df2.head() $ df2 = df2.set_index('user_id').join(df3.set_index('user_id'))
uber1.to_csv('uber1.csv', index=False) $ uber2.to_csv('uber2.csv', index=False) $ uber3.to_csv('uber3.csv', index=False)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=100)
col_names = ['labels', 'text']
access_logs_parsed = access_logs_raw.map(parse_apache_log_line).filter(lambda x: x is not None)
converted = df.groupby(['converted']).nunique().user_id[1] # Number of converted users $ converted_proportion = (converted / number_of_users) * 100  # Get converted users proportion $ converted_proportion
print(tfidf[new_vec]) # vector score
import statsmodels.api as sm $ convert_old = df2.query('group == "control"and converted == 1').count()[0] $ convert_new = df2.query('group == "treatment"and converted == 1').count()[0] $ n_old = df2[df2['landing_page']=="old_page"].count()[0] $ n_new = df2[df2['landing_page']=="new_page"].count()[0]
df2.query('group == "treatment"').user_id.size #145310 147276
vow.set_index('Date', inplace=True)
twitter_df_clean.loc[twitter_df_clean.rating_numerator > 10, 'rating_numerator'] = 10
dict = {1:'JAN',2:'FEB',3:'MAR',4:'APR',5:'MAY',6:'JUN',7:'JUL',8:'AUG',9:'SEP',10:'OCT',11:'NOV',12:'DEC'}
df_final.sort_values(by='Pct_Passing_Overall', ascending=False).head(5)
words_only_sk = [term for term in words_sk if not term.startswith('#') and not term.startswith('@')] $ corpus_tweets_streamed_keyword.append(('words', len(words_only_sk))) # update corpus comparison $ print('The number of words only (no hashtags, no mentions): ', len(words_only_sk))
df2.drop_duplicates(['user_id'] , inplace = True)
df = pd.DataFrame.from_dict(gender_counts, orient='index') $ df.plot(kind='bar',color = 'y') $ pyplot.xlabel('Gender')  $ pyplot.ylabel('count') 
for t in trump_tweets[0:3]: $     print(t['created_at'])
len(rng2)
test.isnull().sum()
elms_all_0611.iloc[1048575:].to_excel(cwd+'\\ELMS-DE backup\\elms_all_0611_part2.xlsx', index=False)
df2.query("user_id == @duplicated_user_id")
sub_df.to_csv("dnn100_sub.csv", index=False)
total_tokens_sk = len(all_tokens_sk) $ corpus_tweets_streamed_keyword.append(('total tokens', total_tokens_sk)) # update corpus comparison $ print('Total number of words (including mentions, hashtags and links) in the collection: ', total_tokens_sk)
construction.to_sql(con=engine, name='construction', if_exists='replace', flavor='mysql',index=False)
calls_nocontact.issue_type.value_counts()
chunker = ConsecutiveNPChunker(train_trees) $ print(chunker.evaluate(valid_trees))
df_columns['Hour of day'] = df_columns['Created Date'].str.extract(r"(\d\d):\d\d:\d\d", expand=False) $ df_columns['AM|PM'] = df_columns['Created Date'].str.extract(r"(AM|PM)", expand=False) $
metadata_dict = dict(zip(metadata['Variable'], metadata['Explanation']))
cpi_sdmx = DSDparser(tree_cpi,ns)
plt.pie(typeby_city, labels=typeof_city,explode=explode, colors=colors, $         autopct="%1.1f%%", shadow=True, startangle=45) $ plt.axis("equal")
Z = np.arange(1,15,dtype=np.uint32) $ R = stride_tricks.as_strided(Z,(11,4),(4,4)) $ print(R)
df.info()     # looking on the details of the information and memory size
user_data = sc.textFile("file:/path/*") $ user_profile = user_data \ $     .map(lambda line: line.split(',')) \ $     .map(lambda words: (words[0], words[1:]))
model.compile(optimizer = 'adam', loss = 'mean_squared_error') $ print("Loss function: " + model.loss)
 result = db.profiles.create_index([('reinsurer_id', pymongo.ASCENDING)], $                                   unique=True) $  sorted(list(db.profiles.index_information()))
df_clean.text.sample().tolist()
tweets_clean.head()
df.select('latitude').distinct().sort('latitude', ascending=True).show(10)
activeDF = get_active_on_date(filteredPingsDF_str, D0.isoformat()) $ activeDF_str = "activeDF" $ sqlContext.registerDataFrameAsTable(activeDF, activeDF_str)
to_be_predicted_Day4 = 55.24901111 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
irisRDD.take(5)
df['gap_open_pct'] = (df['dollar_change_open']/df['open_price']) $ df['open_to_close_pct'] = (df['dollar_change_close']- df['dollar_change_open'])/df['open_price']
r_start_date = '2017-01-01' $ r_end_date = '2017-12-31' $ r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key='\ $                  +API_KEY+'&start_date='+r_start_date+'&end_date='+r_end_date)
with open(os.path.join(folder_name, url.split('/')[-1]), mode='wb') as file: $     file.write(response.content)
files3.head()
from pandas_datareader import data $ ebay = data.DataReader('EBAY', start='2017', end='2018', data_source='iex')['close'] $ ebay.plot()
tweets.tail()
loans_df = loans_df.query('home_ownership != "ANY"')
pair.tail(10)
ac['Eligibility Date'].describe()
merge['TotalPoverty'] = merge['C17002e2'] + merge['C17002e3'] $
from hs_restclient import HydroShare, HydroShareAuthBasic
df2.query('group == "control"') $ df2.query('group == "control" and converted == 1') $
counter = collections.Counter(data_from_1c[u'Part no.'])
maeG = np.sum(np.absolute(predGdf.values-xGR.values)) $ rmseG = np.sqrt(((predGdf.values-xGR.values) ** 2).mean()) $ print "FIT G  :  RMSE  \  MAE  = ", rmseG, "  \  ", maeG, "\n"
df_parcel = pd.DataFrame(json.loads(response.content)) $ df_parcel.head()
s.tail(1)
mismatch_grp1 = df.query("group == 'treatment' and landing_page == 'old_page'") $ print("times the treatment group user lands incorrectly on old_page is :: {}".format(len(mismatch_grp1))) $ mismatch_grp2 = df.query("group == 'control' and landing_page == 'new_page'") $ print("times the control group user incorrectly lands on new_page is :: {}".format(len(mismatch_grp2))) $ print("times the new_page and treatment don't line up is :: {}".format(len(mismatch_grp1) + len(mismatch_grp2)))
lr = LinearRegression() $ type(lr)
import pandas as pd $ tfidfnmf_topics = pd.DataFrame(nmf_doc_top) $ tfidfnmf_tmp = tfidfnmf_topics
df['domain'].value_counts().plot(kind='hist') $ plt.xticks(rotation=70);
DATADIC = pd.read_csv('DATADIC.csv') $ list(DATADIC['FLDNAME'].unique()) $ DATADIC[['FLDNAME', 'TEXT']].head()
Distribution_df = Exchange_df[['New_or_Returning', 'Sales_in_CAD']].resample('M', how=('sum')) $ Distribution_df = Distribution_df[Distribution_df.index > '2012-07-31'].reset_index() $ Distribution_df.head()
repos.id.sort_values().head()
key = "VQ9pfbdfDxTMbF6jdHm6" $ aapl = quandl.get("WIKI/AAPL", start_date="2006-12-25", api_key = key) $ aapl.head()
hist(df3.tripduration, bins = 20, color="red", label = "Female", normed = 1) $ plt.xlabel("Trip Duration in seconds", fontsize=12) $ plt.ylabel("Amount of trips", fontsize=12) $ plt.title("Trip Duration Histogram female users", weight='bold', fontsize=14) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
caps2_output_round_1_tiled = tf.tile( $     caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1], $     name="caps2_output_round_1_tiled")
lc_review = pd_review["text"][0].lower() $
people['teacher'] = 'dave' $ people
exiftool -csv -createdate -modifydate cisnwe5/Cisnwe5_cycle4.MP4 > cisnwe5.csv
friends_n_followers['userFriendsCt'].plot(kind = 'bar') $ plt.title('Friends by Time Zone') $ plt.xlabel('Time Zone') $ plt.ylabel('Number of Friends') $ plt.show()
mgxs_lib.build_hdf5_store(filename='mgxs.h5', directory='mgxs')
cursor1=db.bookings.find({"consignee.city":"Indore"},{'booking_id':1,'_id':0,'booking_items.is_cod':1})
dp = docProcessor(rtnType='word',stopwordList=stops) $ testParsedPD = [" ".join(dp.transform(s)) for s in dfCleaningData[dfCleaningData['geo_code'] == 'TheHill']['property_description']] $ print 'len(testParsedPD): ' + str(len(testParsedPD))
writtens = df5['HT'].where(df5['new_id'] == 'written').dropna() $ plt.hist(writtens, bins=50)
scaler=StandardScaler() $ scaler.fit(x_train)
tu = tweets.loc[300000:300100].copy() $ %timeit tu["related_attack"] = tu.apply(lambda t: match_attacks(t, terror) , axis = 1)
for f in read_in["files"]: $     fp = getattr(processing_test.files, f) $     print(json.load(open(fp.load())))
timeindex = pd.Series(['1/1/2018', '1/4/2018', '1/5/2018', '1/7/2018', '1/8/2018']) $ timeindex = pd.to_datetime(timeindex) $ data_to_interp = [1, np.nan, 5, np.nan, 8] $ df_to_interp = pd.DataFrame(data_to_interp, index=timeindex) $ df_to_interp
print(len(all_posts))
df_new[['UK','US','CA']] = pd.get_dummies(df_new['country']) $ df_new.drop('UK',axis=1, inplace=True) $ df_new.head()
import tweepy $ import json $ import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt
autos["brand"].value_counts(normalize=True)
offer_group_desc_dict = {} $ first_genre_dict = {} $ entity_type_dict = {}
d8 = d7.unstack() $ d8
df.loc[:, ['B', 'D']] # notice lack of parentheses here!
df_predictions_clean.head()
logit_new = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'UK']]) $ results_new = logit_new.fit() $ results_new.summary()
df.groupby('episode_id')['id'].nunique().head()
len(SCN_BDAY)
state_party_df = pd.concat(state_DataFrames_list, axis=1, join='outer') $ state_party_df.columns = state_keys_list $ state_party_df.head(20)
yhat = DT_model.predict(X_test) $ yhat
df = pd.read_csv("ab_data.csv") $ df.head(3)
df_archive_clean.info()
fixture.head()
urls = data["url"]
repeated_user=df2.groupby(['user_id']).size().idxmax() $ print (repeated_user)
sentiment_df.to_csv("sentiment.csv", encoding = "utf-8-sig", index = False)
scoring_input_data.dtypes
scenarios_rdd = sc.textFile(csv_filename).map(lambda l: l.split(",")).map(parse2)
turnaround_planes_df.to_excel('turnaround_planes_df1.xlsx', index=False)
joined=join_df(joined, items_df, 'item_nbr')
from credentials import *
y = y.apply(lambda x: 0 if x <= 67 else 1)
import gc $ del full_data $ gc.collect()
pd.to_datetime(a[0]).date() == start_idx
p_val = (p_diffs > act_diff).mean() $ print("Proportion greater than actual difference: %.4f" %p_val)
weather_data['Date'] = weather_date; weather_data.head()
df.shape[0]
intersections_irr['estimated_number_vehicles'] = [get_number_of_vehicles_per_segment(row['avg_traffic_flow'],row['speed'],row['SHAPE_Leng']) for index,row in intersections_irr.iterrows()]
df_trips.describe()
treatment_group = len(df2.query('group=="treatment" and converted==1'))/len(df2.query('group=="treatment"')) $ treatment_group
df.describe(include=[np.object])
archive_copy['full_text'] = archive_copy['full_text'].str.replace('&amp;', '&')
dir(result[0])  # to show the attributes of result[0]
test = only_cards["XLN"] $ test.head()
from datetime import timedelta $ start=datetime(2020, 10, 1) $ start+timedelta(5)
a[a.find(':') + 1:].strip()
df3 = pd.DataFrame(df3_list, columns = ['tweet_id', 'retweet_count', 'favorite_count']) $ df3 = df3.sort_values('retweet_count').reset_index(drop=True)
pd.Series(np.array(['a','b','c','d','e']))  # from np.array
deployment_details
combined = exportOI.set_index('join_col').join(exportID.set_index('join_col'), lsuffix="exportOI", rsuffix="exportID", how = "inner")
complete_ratings_file = os.path.join(dataset, 'ml-20m', 'ratings.csv') $ complete_ratings_raw_data, complete_ratings_raw_data_header = read_file(complete_ratings_file) $ complete_ratings_data = remove_header(complete_ratings_raw_data, complete_ratings_raw_data_header, 3) $ print ("There are %s recommendations in the complete dataset" + str(complete_ratings_data.count()))
n_old = (df2.query('group == "control"')['converted'] >= 0).sum() $ n_old
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[n_new, n_old]) $ print("z-score:", z_score, $       "\np-value:", p_value)
import pandas as pd $ %matplotlib inline
import statsmodels.api as sm $ from scipy import stats $ stats.chisqprob = lambda chisq, df2: stats.chi2.sf(chisq, df2) $ log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = log_mod.fit()
weather_data2.columns = weather_data1.columns.values; weather_data2.head()
petropavlovsk_filtered.shape
tokens = pd.DataFrame({'token':X_train_tokens, 'one_star':one_star_token_count, 'five_star':five_star_token_count}).set_index('token')
data.tail(3)
x = df2[df2['group']=='control']['converted'].mean() $ print("{:.2%}".format(x))
if not os.path.isdir('output'): $     os.makedirs('output')
df.columns
encoder_model_inference.save('encoder_model_inference.h5')
pd.merge(msftAR0_5, msftVR2_4)
df.index.day
v_invoice_hub.merge(v_item_hub, left_on='fk_s_change_context_id_cr', right_on='fk_s_change_context_id_cr')
frame.applymap(format)
from sklearn.preprocessing import PolynomialFeatures $ x = np.array([2, 3, 4]) $ poly = PolynomialFeatures(3, include_bias=False) $ poly.fit_transform(x[:, None])
df2['day'] = df2["DN"][(df2["DN"] == "D")] $ df2['night'] = df2["DN"][(df2["DN"] == "N")] $ df2.head(3)
prcp_date.describe()
YS1517.Volume.plot()
validation.analysis(observation_data, Jarvis_simulation)
tweet_data.describe()
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = sm.graphics.tsa.plot_acf(resid_6201.values.squeeze(), lags=40, ax=ax1) $ ax2 = fig.add_subplot(212) $ fig = sm.graphics.tsa.plot_pacf(resid_6201, lags=40, ax=ax2)
dog_stage_list
df_hour['hour'] = map(lambda x: x.hour, df_hour.created_at)
defaults = np.seterr(all="ignore") $ Z = np.ones(1) / 0 $ _ = np.seterr(**defaults) $ with np.errstate(divide='ignore'): $     Z = np.ones(1) / 0
season_team_groups.size().sort_values(ascending=False).head(5)
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import seaborn
df.to_csv("clean_df.csv")
tweet_archive_clean.text.head(2)
t.converted.sum()/t_rows
dict_location['type']
daily_ret = calc_daily_ret(closes) $ daily_ret.plot(figsize=(8,6));
cross_val_score(lr,X_train,y_train,cv=10).mean()
ids = df2['user_id'] $ df2[ids.isin(ids[ids.duplicated()])]
df_train['dow_visitStartTime'] = df_train['visitStartTime'].dt.day_name() $ df_test['dow_visitStartTime'] = df_train['visitStartTime'].dt.day_name()
logit_result = logit_mod.fit() $ logit_result.summary()
list_of_genre_1990s = list() $ respond = requests.get("https://en.wikipedia.org/wiki/1990s_in_music") $ soup = BeautifulSoup(respond.text) $ l = soup.find_all('ul') $ list_of_genre_1990s.append(l[3].text.split(','))
fitted.score(X_test, y_test)
autos.describe(include='all')
np.mean(b)  # Get the mean of all the elements
for t in df[df.sentiment == df.sentiment.max()].text[:10]: $     print(t + '\n')
df_test_index.iloc[6260, :]
horror_readings=horror_readings.groupby('visitor_id').size()
gbm_regressor = H2OGradientBoostingEstimator(distribution="gaussian",ntrees=10, max_depth=3, min_rows=2, learn_rate=0.2)
unordered_df.head()
twitter_archive[~twitter_archive.retweeted_status_id.isnull()].head()
test_clean = test_data.drop("Name", axis=1) $ test_clean = test_clean.drop("Color", axis=1) $ test_clean = test_clean.drop("Breed", axis=1) $ test_clean = test_clean.drop("AgeuponOutcome", axis=1) $ print test_clean.head()
train_frame
TC_adds = "TC-adds.txt" $ TC_deletes = "TC-deletes.txt"
col.head(2)
train_test_set.rdd.getNumPartitions()
def load_tweets(path): $     ...
building_pa_prc_zip_loc=building_pa_prc_shrink[filt_zip_loc]
act_stations = session.query(Measurement.station,func.count(Measurement.station)).\ $                group_by(Measurement.station).\ $                order_by(func.count(Measurement.station).desc()).all() $ act_stations
list_of_non_rle_and_secret_base_pscs = non_rle_pscs[non_rle_pscs.secret_base == True].company_number.unique().tolist() $ non_rle_company_and_secret_base_at_top_of_chain = pd.DataFrame(graph.run("MATCH p=(c1:Company)<-[:CONTROLS*0..]-(c2:Company)\ $ WHERE c2.uid IN {list_of_non_rle_companies_with_secret_base}\ $ RETURN DISTINCT (c1.company_number)",list_of_non_rle_companies_with_secret_base=list_of_non_rle_and_secret_base_pscs).data()) $ len(non_rle_company_and_secret_base_at_top_of_chain)
cols = ['funding_round_uuid','company_uuid','company_name','country_code','state_code','region','city','zipcode', $         'company_category_list','category_group_list','short_description','description','founded_on','founded_year', $         'funding_round_type','funding_round_code','raised_amount_usd','announced_on','primary_role']
test_id = pax_raw.seqn.values[1] $ pax_sample = pax_raw[pax_raw.seqn==test_id].copy()
df['country'].replace('', None,inplace = True) $ df['delivery_method'].fillna(value=4,inplace=True) $ df['currency_match']=(df['country'].map(Country_dictionary) == df['currency']).replace([True,False], $ df['venue_name_exits'] = df['venue_name'].isnull().replace([False, True], [0,1])                                                                                      [1,0])
sub = pd.read_csv('../input/sample_submission.csv') $ sub['Tag'] = final_test_pred_nbsvm1 $ sub.to_csv('../submissions/nvsvm.csv', index=False)
business_df['prop']=business_df['prop'].apply(str) $ business_df['prop'].head()
cgc2 = CensusGeoCoder(template='tmp/geo-try2-{}', concurrency=20) $ cgc2.geocode_addresses(fail_df)
data['Sales'].resample('D').mean().autocorr(lag=1) $
df1.info() $
posts.find_one({"author": "Mike"})
LUM.plot_mean(all_lum)
wrd_clean['rating_numerator'] = wrd_clean['text'].str.extract(r'([0-9]{1,2}/10)',expand=False) $ wrd_clean['rating_denominator'] = wrd_clean['text'].str.extract(r'([0-9]{1,2}/10)',expand=False)
converted_first_week.shift(2) # shift values by 2 periods 
df_train.index = pd.to_datetime(df_train['date']) $ df_train.drop('date', axis=1, inplace=True) $ df_test.index = pd.to_datetime(df_test['date']) $ df_test.drop('date', axis=1, inplace=True)
df['Zip Code'].replace('712-2', '51529', inplace=True) $ df['Zip Code']=df['Zip Code'].astype(int) $ df['Zip Code']=df['Zip Code'].replace(56201, 52601)
submission_full[['proba']].max()
date_ny['date'] = pd.to_datetime(date_ny.year*10000+date_ny.month*100+date_ny.day,format='%Y%m%d') $ date_ny['shannon'] = date_ny['count']
slps = active_companies[active_companies['company_type'] == 'Limited Partnership for Scotland']
movies
df_daily.dropna(subset=["PREV_DATE"], axis=0, inplace=True) $ df_daily.head(5)
c1.sum()
rec_items('alpa.poddar@gmail.com', product_train, item_vecs, user_vecs, customers_arr, products_arr, item_lookup)
df_cal['start_date'][0]
calls_df.loc[(calls_df["call_type"]=="Not Interested") | (calls_df["call_type"]=="Not Eligible"),"phone number"].nunique()
year_info_df.describe()
MostHourlyEntries = subway3_df.nlargest(100,'Hourly_Entries')
underlying_symbol = 'IBM' $ options_obj = pd.Options('IBM', 'yahoo') $ options_frame_live = options_obj.get_all_data() $ options_frame_live.to_pickle('options_frame.pickle')
pd.options.mode.chained_assignment = None # This just stops Python from popping up an annoying warning $ dframe_team.drop(dframe_team.columns[[6,7]], inplace=True, axis=1) # dropping 'cut_year' and 'end_cut' columns, the 'tenure' column could be interesting
f_yt_resolved = '/scratch/olympus/projects/ideology_scaling/congress/youtube_links_resolved.tsv' $ df_yt_resolved = pd.read_csv(f_yt_resolved, sep='\t') $ f_yt_raw = '/scratch/olympus/projects/ideology_scaling/congress/youtube_links_raw.csv' $ df_yt = pd.read_csv(f_yt_raw)
start = dt.strptime("2018-01-01", "%Y-%m-%d") $ day_list = [start + relativedelta(days=x) for x in range(0,365)] $ future_day = pd.DataFrame(index=day_list, columns= df_day.columns) $ df_day = pd.concat([df_day, future_day])
print("P converting:", df2.converted.mean())
control_converted=df2.query('group=="control"').converted.mean() $ control_converted
tips_analysisDF = pd.read_csv('text_preparation/yelp_tips_prepared.csv', index_col=False, encoding='latin-1')
df.head()
logistic_file = '../data/model_data/log_pred_mod.sav' $ pickle.dump(logreg, open(logistic_file, 'wb'))
series.values # values is the raw data
events.loc[events.swimstyle == "breaststroke", ["distance", "swimstyle", "speed", "time", "category", "course"]].sort_values('speed')
htest['visitors']=0
wrd_clean['tweet_id'] = wrd_clean['tweet_id'].astype('str') $ wrd_clean['in_reply_to_status_id'] = wrd_clean['in_reply_to_status_id'].astype('str') $ wrd_clean['in_reply_to_user_id'] = wrd_clean['in_reply_to_user_id'].astype('str') $ wrd_clean['retweeted_status_id'] = wrd_clean['retweeted_status_id'].astype('str') $ wrd_clean['retweeted_status_user_id'] = wrd_clean['retweeted_status_user_id'].astype('str')
logit_ab_country = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA', 'ab_page','US_ab_page', 'CA_ab_page']]) $ results_ab_country = logit_ab_country.fit()
logits = tf.layers.dense(embed, len(vocab), activation=None, $     kernel_initializer=tf.random_normal_initializer())
df.notnull().count()
strategy.marketdata('BTC').df().head()
np.allclose(df1 + df2 + df3 + df4, pd.eval('df1 + df2 + df3 + df4'))
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head() $
alerts[0]
master_df.name.nunique()
new_converted_simulation = np.random.binomial(n_new, p_new, 10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old, 10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation
f2(preds,y)
cars['car_age']= 2016- cars['yearOfRegistration'] $
df_A.iloc[:, lambda x : [0,2]]
feeds.head()
pop_treat_con = len(df2[(df2["group"] == "treatment") & (df2["converted"] == 1)]) $ pop_treat_con
df_city_reviews.cache()
repos = pd.read_pickle('data/pickled/new_subset_repos.pkl')
res.summary()
z_score , p_value
df_no_cat = df_onc_no_metac.drop(columns = ls_other_columns)
df_countries = pd.read_csv('countries.csv') $ df_countries.country.unique()
df.last_name.isnull().sum()
site_values = ulmo.cuahsi.wof.get_values(wsdlurl, location, variable, $                                          start=dataset['begin_date'], end=dataset['end_date'])
first_result.contents[1][1:-2]
dt_features_test['state_changed_at'] = pd.to_datetime(dt_features_test['state_changed_at'],unit='s')
(df_final[df_final['Scorepoints'] == 160]).head()
nnew = df2[df2['group'] == 'treatment'].shape[0] $ print(nnew)
mc_results_w = ssm.MultiComparison(df['y'], df['w']) $ mc_results_w_tukey_hsd = mc_results_w.tukeyhsd() $ print(mc_results_w_tukey_hsd)
txns[txns['token']=='DAI'].describe()
MAX = len(test_clas) $ test_ds = TextDataset(test_clas[:MAX], test_labels[:MAX]) $ test_dl = DataLoader(test_ds, bs, transpose=True, num_workers=1, pad_idx=1) # for ease of submission, don't sort out of order
consumer_key = os.environ["consumer_key"] $ consumer_secret = os.environ["consumer_secret"] $ access_token = os.environ["access_token"] $ access_token_secret = os.environ["access_token_secret"]
X_test.head()
m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
df_ml_701_01.tail(5)
sentences = []  # Initialize an empty list of sentences $ print("Parsing sentences from training set") $ for review in train["review"]: $     sentences += review_to_sentences(review, tokenizer) $
yt.get_featured_channels(channel_id, key)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ z_score, p_value
for df in list_to_change: $     df.drop("Compaction", axis = 1, inplace = True) $     df.drop("diff_", axis = 1, inplace = True) $ addicks.head() # to see the dropped columns
analyze_set['date']=pd.to_datetime(analyze_set['date']) $ analyze_set['month']=analyze_set['date'].dt.month $ analyze_set['month'].value_counts()
lr = LogisticRegression() $ lr.fit(X_train_dtm, y_train) $ lr.predict(X_test_dtm)
df_tte_all.columns
plot_acf(BPL_electric['Total_Demand_KW'], lags=62)
paired_df_grouped.n_best_co_occurence.describe()
input_col = ['msno','payment_plan_days','transaction_date', 'membership_expire_date',] $ transactions = utils.read_multiple_csv('../../input/preprocessed_data/transactions',input_col)
n_new = df2[df2['group']=='treatment'].count()[0] $ n_new
croppedFrame = cFrame[cFrame.Date < lastDay] $ print(croppedFrame.loc[cFrame['Client'] == 'AT&T']) $ croppedFrame.tail() $
import csv $ import sqlite3
between_all_posts = time_between(tweets['created_at'])
del dfRegMet["idUser"]
tfidf_doctopic = clf.fit_transform(tf_idf)
cd
users[users.id == 9236]
os.getcwd() $ os.chdir('../')
tw = [status for tweet in tweets for status in tweet]
z_values, _ = create_latent(nn_aae.nn_enc, train_loader) $ print(z_values[:,0].mean(), z_values[:,0].std()) $ print(z_values[:,1].mean(), z_values[:,1].std()) $ recon_x = create_sample(nn_aae.nn_dec, z_values) $ plot_mnist_sample(recon_x)
frame2['state']
from pyspark.sql import Window, functions as F $ w1 = Window.partitionBy('user_id').orderBy('date') $ df_city_reviews = df_city_reviews.withColumn('seq', F.row_number().over(w1))
conversion_df2 = df2['converted'].mean() $ conversion_df2
evals = np.array([m.evaluate(X_test, y_test, batch_size=256) for m in models])
autos.price.max()  # we have a high max probably an outlier. 
import pandas as pd $ import csv $ pd.set_option('display.max_colwidth', -1) # do not truncate comments row, need to see this for cleaning
date.strftime("%A")
len(tweets_original) + len(tweets_rt)
frames = [df_piotroski, df_kosdaq]
y_tr = target_data['Prediction'].values $ plt.hist(y_tr,bins=2000) $ plt.xlim((0,10000))
pd.DataFrame(result, columns=schema)
df_c
df2[df2.duplicated(['user_id'],keep=False)== True]
from IPython.core.display import display, HTML $ display(HTML("<style>.container { width:100% !important; }</style>")) # maakt de jupyter notebook cellen 100% breed $ %matplotlib inline $ from IPython.core.interactiveshell import InteractiveShell $ InteractiveShell.ast_node_interactivity = "all" # om meerdere grafieken/tabellen in 1 cel te kunnen laten zien.
RNPA_existing_forecast, RNPA_existing_data_plus_forecast = get_ARIMA_predictions(data=RNPA_existing_hours, order=(4,1,2), start=start_date,\ $                     end=end_pred, typ='levels') $ RNPA_existing_data_plus_forecast = get_ARIMA_output_df(RNPA_existing_data_plus_forecast, num_RNPA_existing)
s_mean_df = pd.DataFrame(station_mean, columns=["date", "avg_prcp"]) $ print(len(s_mean_df.index)) $ s_mean_df.info() $ s_mean_df.head(5)
act_diff = df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean() $ act_diff $
%matplotlib inline $ x_data, y_data = (df["Chirps"].values,df["Temp"].values) $ plt.plot(x_data, y_data, 'ro') $ plt.xlabel("# Chirps per 15 sec") $ plt.ylabel("Temp in Farenhiet") $
appointments['Specialty'].loc[appointments['Specialty'].isin(doctors)]= 'doctor' $ appointments['Specialty'].loc[appointments['Specialty'].isin(RN_PAs)] = 'RN/PA' $ appointments['Specialty'].loc[appointments['Specialty'].isin(therapists)] = 'therapist'
enrollments['cohort_availability'] = enrollments.content_availability.apply(lambda ts: ts.date() - datetime.timedelta((ts.date().isoweekday() + 4) % 7))
result3 = df.eval('(A + B) / (C - 1)') $ np.allclose(result1, result3)
autos["registration_year"].describe()
psy.isnull().sum().sum()
sakhalin_filtered
len(quotes[0]['close'])
reddit_comments_data.orderBy('score', ascending = False).select('score').show(10)
goles_favor["Brasil": "Colombia"]
def get_active_on_date(DF_str, date_str): $                date=date_str) $     active = spark.sql(query).cache() $     return active
txt = txt.set_index('Date') $ txt = txt.sort_index()
colnames = ['username','userid','blocked_time','blocked_reason','type'] $ type(colnames) $ users=pd.read_table('users.tsv',sep="\t",header=None,names=colnames) $ users=pd.DataFrame(users) $ users.head()
filtered_tweets_df = tweetdf.loc[~tweetdf.latlng.isin(badgeo_df.latlng)]
clf = MultinomialNB() $ clf.fit(train_features.toarray(), authors_train)
df4 = pd.read_csv('2004.csv')
norm.ppf(1-(0.05/2)) # Tells us what our critical value at 95% confidence is
for index, row in groupedvalues.iterrows(): $     print(row.Memory)
if created: $     biosample.to_excel('/dev/shm/biosamples.xlsx', index=False)
df.loc[:,['name','year']]
from scipy import sparse
df[df.state_cost==425] $ df[df.item_number==995381].item_descript
import quandl, math $ import numpy as np $ import pandas as pd $ from sklearn import preprocessing, cross_validation, svm $ from sklearn.linear_model import LinearRegression
from scipy import sparse # Need this to create a sparse array $ scalingDF_sparse = sparse.csr_matrix(scalingDF)
confidence = clf.score(X_test, y_test) $ print(confidence)
sub_gene_df['type'].value_counts()
data = pd.read_csv('data.csv') $
cityID = '249bc600a1b6bb6a' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Aurora.append(tweet) 
prop = donors['Donor Is Teacher'].value_counts(normalize=True) $ print(prop)
tobs_df = pd.DataFrame.from_records(station_tobs) $ tobs_df.head()
tu
sns.set_style("darkgrid") $ plt.figure(figsize = (10,5)) $ sns.barplot(months.index, months.values, palette= "GnBu_d").set_title("Rides per Month")
gnbtest['predicted_failure'] = model.predict(test_x)
height=df_A['Student_height'] $ type(height)
results=lm.fit() $ results.summary()
retweets = df[~df["retweeted_screen_name"].isnull()] $ retweet_pairs = retweets[["id","screen_name","retweeted_screen_name"]].groupby(["screen_name","retweeted_screen_name"]).agg({"id":"count"}).rename(columns={"id":"Weight"}) $ retweet_pairs.reset_index(inplace=True) $ retweet_pairs.sort_values("count",ascending=False).head()
dataset.head()
for i in loading.print_topics(num_words=8): $     for j in i: print(j)
%matplotlib inline $ import matplotlib.pyplot as plt
state_party_df['National_D']['2016-08-01':'2016-08-07'].sum() / 7
stock_data.index=pd.to_datetime(stock_data['latestUpdate']) $ stock_data['latestUpdate'] = pd.to_datetime(stock_data['latestUpdate'])
df2gdb(connections,fileloc,'connections')
plt.style.use('seaborn') $ breed_ax = breed_ratings[1:51].plot(kind='bar', figsize=(20,8)) $ breed_ax.set_title("Highest Rated Dog Breeds on WeRateDogs", fontsize = 24) $ breed_ax.set_xlabel("") $ plt.savefig('plots/breed_ratings.png', bbox_inches='tight')
shows['release_date'].head()
btc_price_df.index = pd.to_datetime(btc_price_df.index)
goog['closedHigher'] = goog.Close > goog.Open
print(os.path.abspath(holding_file_name)) $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.chdir('../') $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $
estimate_model(estimator)
df = pd.DataFrame(data = pd.Series(range(12)).reshape(3, 4), columns = list('abcd')) $ df
mask=f>=0.5 $ print(f[mask])
autos['odometer'] = autos['odometer'].str.replace("km","") $ autos['odometer'] = autos['odometer'].str.replace(",","") $ autos['odometer'] = autos['odometer'].astype(float) $ autos.rename({'odometer':'odometer_km'}, axis = 1,inplace = True)
unique_Dx = [] $ for col in Dx_cols: $     for Dx in full_orig[col].unique(): $         unique_Dx.append(Dx)
df2 = df2.drop_duplicates(subset=['user_id'])
from fbprophet import Prophet
pickle.dump(selfharmmm_final_df, open('iteration1_files/epoch3/selfharmmm_final_df.pkl', 'wb'))
data.plot()
merged.sort_values("amount", ascending=False)
archive_clean['doggo'].replace('None', np.nan, inplace=True) $ archive_clean['floofer'].replace('None', np.nan, inplace=True) $ archive_clean['pupper'].replace('None', np.nan, inplace=True) $ archive_clean['puppo'].replace('None', np.nan, inplace=True)
history = add_train_target_value_in_history(history, train)
crimes['2011-06-15']['NEIGHBOURHOOD'].value_counts().head(5)
pivoted.columns
tr_indices = [] $ for _, indices in train_indices.items(): $     tr_indices = tr_indices + indices
delay_count_dfs = perspective_time_dfs.copy()
c.set_index('cutoff_time')['days_to_next_churn'].plot(); $ plt.vlines(x = (c.loc[c['churn'] == 1, 'cutoff_time']).values, ymin = 0, ymax = 200, color = 'r'); $ plt.ylabel('Days to Next Churn');
sns.distplot(filter_iphone(orig_tweets[orig_tweets['date'].dt.year==2016])['hour'], color='b') $ sns.distplot(filter_android(orig_tweets[orig_tweets['date'].dt.year==2016])['hour'], color='g')
def remove_punctuation(text): $     exclude = set(string.punctuation) $     return "".join(ch for ch in text if ch not in exclude)
! cd /home/ubuntu/s3/flight_1_5 $ ! aws s3 sync s3://flight.price/flight_1_5 . 
(labels.loc[labels['churn'] == 1, 'cutoff_time']).values[0]
pred.describe()
scaler = MinMaxScaler() $ X_train = w2v_to_features(w2v, list_tokens) $ X_train = scaler.fit_transform(X_train)
title_tokens.reset_index(inplace=True, drop=True)
CSV_URL = ('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')
np.corrcoef(total_sales.ratio.values, total_sales.sales_change_growth.values)
row = df3.query('group == "treatment"').index $ df3.set_value(index=df3.index, col='intercept', value=1) $ df3.set_value(index=row, col='ab_page', value=1) $ df3[['intercept','ab_page']] = df3[['intercept','ab_page']].astype(int)
w.apply_data_filter(step = 0) # This sets the first level of data filter in the IndexHandler 
bnbAx.timestamp_first_active.plot.hist()
for df in list_to_change: $     df["diff_"] = df.Compaction.diff() $     df.fillna(0, inplace = True) $ addicks.head()
items = range(1000) $ items
segmentData.opportunity_amount.describe()
print cust_data.assign(Dups = cust_data.duplicated()).head(5) $
data = pd.read_json('challenge.json')
twitter_archive_clean.loc[twitter_archive_clean['tweet_id']==835246439529840640,'rating_numerator']=13 $ twitter_archive_clean.loc[twitter_archive_clean['tweet_id']==835246439529840640,'rating_denominator']=10
festivals['Index'] = range(1, len(festivals) + 1) $ list(festivals.columns.values) $ festivals.head(3) $
tweets_df = pd.read_csv("tweets_df.csv", encoding="utf-8")
del df_train['moy_visitStartTime'] $ del df_test['moy_visitStartTime']
stock_df.head()
ct = pd.crosstab(df.daystogetresult_grp, df.xseller) $ ax = ct.plot.bar(stacked=True, width = 0.35) $ plt.legend(title='xseller') $ plt.ylabel('count') $ plt.show()
np.random.seed(31) $ random.seed(31) $ tf.set_random_seed(31)
df2['intercept'] = 1 $ df2[['ab_page', 'ab_page_2']] = pd.get_dummies(df2['landing_page']) $ df2 = df2.drop(columns = ['ab_page_2'], axis = 1) $ df2.head() $
p_old = (control_converted / total_control) $ p_new = (treatment_converted / total_treatment) $ p_new_minus_p_old = p_new - p_old $ print(p_new_minus_p_old)
plt.hist(p_diffs) $ plt.xlabel('p_diff') $ plt.ylabel('frequency') $ plt.title('P_diff simulated 10000 times.');
data = settlement_lin_df $ data = data[data["happy"]<=10] $ holdout = data.sample(frac=0.05) $ training = data.loc[~data.index.isin(holdout.index)]
image_clean.info()
ci = set(test[test['srch_ci'].isnull()].index) $ co = set(test[test['srch_co'].isnull()].index) $ print(ci,co)
top_songs = pd.read_csv('./data.csv')
news_df_grouped = news_df.groupby('Source Acc.')
sum([1 for row in U_B_df.cameras if len(row) > 2])
a_diffs = df2[df2['group'] == 'treatment']['converted'].mean() -  df2[df2['group'] == 'control']['converted'].mean()
train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) $ test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])
speakers.whylisten = speakers.whylisten.apply(lambda x: re.sub("<.*?>", "", x))
temp_df.plot(kind='bar', figsize=(10, 3));
df3.query('country == "US"').converted.mean(),df3.query('country == "UK"').converted.mean(),df3.query('country == "CA"').converted.mean()
merged_visits = visited.merge(dta) $ merged_visits.head()
print(train_data.fuelType.isnull().sum()) $ print(test_data.fuelType.isnull().sum())
from sklearn.ensemble import RandomForestRegressor $ from sklearn.metrics import mean_squared_error, r2_score $ clf = RandomForestRegressor(n_estimators=20, max_depth = 50) $ clf.fit(X_train, y_train) $ print "RMSE of prediction for test set is: ", np.sqrt(mean_squared_error(y_test, clf.predict(X_test)))
df_image_clean.head()
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == 1').shape[0] $ convert_new = df2.query('landing_page == "new_page" and converted == 1').shape[0] $ n_old = df2.query('landing_page == "old_page"').shape[0] $ n_new = df2.query('landing_page == "old_page"').shape[0] 
df = pd.merge(dfleavetimes, dftrips, how='left', on=['TripID', 'DayOfService'], suffixes=('_leave', '_trips'))
rows = df.shape[0] $ rows
general_info.head()
df_nullcount.plot(kind='bar')
from IPython.display import Image $ from IPython.core.display import HTML $ Image(url= "https://image.slidesharecdn.com/tdscspeech-151114010319-lva1-app6891/95/science-in-text-mining-8-638.jpg?cb=1447463123 ")
bitcoin_github_issues_df[['title', 'html_url', 'created_at','updated_at']].head()
from sklearn.preprocessing import MinMaxScaler $ scaler = MinMaxScaler() $ train['load'] = scaler.fit_transform(train) $ train.head(10)
twitter_archive_clean.info()
testheadlines = test["text"].values $ basictest = basicvectorizer.transform(testheadlines) $ predictions = basicmodel.predict(basictest)
daily = counts.resample('d').sum() $ daily['Total'] = daily.sum(axis=1) $ daily = daily[['Total']] # remove other columns
Difference=ControlConverted-TreatmentConverted $ print("The propotion of p_diffs greater than actual differnce observed equals",(null_values > Difference).mean())
type(table)
! gcloud ml-engine predict --model $MODEL_NAME --version $VERSION_NAME --json-instances less_than_50K.json
search_df
bin(42 | 59)
DataAPI.write.update_index_contents(index_code="A", trading_days=trading_days, override=False, log=False)
fires.head() # For some reason, we get no output from this command
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0]
df = web.DataReader('2317.tw','yahoo',datetime(2017,1,1))
vec = vectorizer.fit_transform(convo_frame['q'])
print 'Number of nulls in the data set: \n', df.isnull().sum(),'\n\n' $
store_items = store_items.drop(['bikes'], axis=1)  # bikes column $ store_items
suspects_with_27_1['loc'].value_counts()/suspects_with_27_1['loc'].value_counts().sum()*(suspects_with_27_1['loc'].value_counts()/suspects_with_27_1['loc'].value_counts().sum()).apply(log10)
mit.groupby('week_day').num_commits.sum()
giss_temp = giss_temp.drop("Year") $ giss_temp
mod = linear_model.BayesianRidge() $ mod.fit(X_training,y_training) $ coefs_sorted = sort_coefs(mod, X_training) $ coefs_sorted.head(20)
index # with freq='M' it uses last day of month by default
npd.node_names()
scores  = movies[['grossRank', 'scoreRank']]#creating an array of our gross rank and gross score $ movies['HarMean'] = scores.apply(stats.hmean, axis=1)#loading in the array and apply the hmean method to the data creating a harmonic mean.
labels.head()
temp = ['low','high','medium','high','high','low','medium','medium','high'] $ temp_cat = pd.Categorical(temp) $ temp_cat_removed = temp_cat.remove_categories('low') $ temp_cat_removed
c[['lat', 'long']] = c[['lat', 'long']].fillna(0) $ c['lat'] = c['lat'].astype(str) $ c = c.loc[c.lat != '`'] $ c['lat'] = c['lat'].astype(float)
merged_df['Hour'] = merged_df.index.hour
data
PTruePositive = (PPositiveTrue * PTrue) / PPositive $ "%.2f" % (PTruePositive * 100) + '%'
df_vow.head()
f(132.52012385, True)
def calc_temps(start_date, end_date): $     return session.query(func.min(Measurements.tobs), func.avg(Measurements.tobs), func.max(Measurements.tobs)).\ $         filter(Measurements.date >= start_date).filter(Measurements.date <= end_date).all() $ temp_range = (calc_temps('2012-02-28', '2012-03-05')) $ print(temp_range)
train_data_features_tf = vectorizer_tf.fit_transform(clean_train_reviews) $ train_data_features_tf = train_data_features_tf.toarray() # Numpy arrays are easy to work with $ print(train_data_features_tf.shape)
cpi_sdmx.names['Dimension'][1]
print('Shape of Train data:', train.shape) $ print('Shape of Test data:', test.shape)
autos = pd.read_csv("autos_2.csv", encoding = "Latin-1") $ autos.head()
df2['intercept'] = 1; $ df2['ab_page'] = df2.group.eq('treatment').mul(1)
x = K.placeholder(dtype="float", shape=X_train.shape) $ target = K.placeholder(dtype="float", shape=Y_train.shape) $ W = K.variable(np.random.rand(dims, nb_classes)) $ b = K.variable(np.random.rand(nb_classes))
sp500[sp500.Price < 100]
from sqlalchemy.orm import sessionmaker $ Session = sessionmaker(bind=engine) $ session = Session()
avg_km_by_brand = dict.fromkeys(brands) $ for name in brands: $     brand_km = autos.loc[autos["brand"] == name, "odometer_km"] $     brand_avg_km = round(brand_km.mean()) $     avg_km_by_brand[name] = brand_avg_km
df_null = df[(df['group'] == 'control') & (df['landing_page'] == 'old_page')] $ df_alt = df[(df['group'] == 'treatment') & (df['landing_page'] == 'new_page')] $ df_dummy = pd.concat([df_null, df_alt]) $ df_count = df['converted'].count() - (df_null['converted'].count() + df_alt['converted'].count()) $ print (df_count) $
with open('clean_tweets_full.pkl', 'wb') as picklefile: # wb: write, binary $     pickle.dump(fullDF, picklefile) #dump data into pickle file
bod = boedoEnMeses.loc[boedoEnMeses['yearmonth'] == '2017-04', :].sort_values('price_usd_per_m2', ascending=False) $ bod["price_usd_per_m2"].describe()
csv_filename = "/Users/miguel/Jottacloud/devel/osqf2015/data/scenarios2.csv"
grouped = joined_data.groupBy('pk_id').agg(F.max(F.col('val')).alias('max_kva')) \ $ .withColumn('used_400hz', F.col('max_kva') > 10)
isinstance(b, (int, float))
retweet_per_doggy.plot(kind='bar') $ plt.ylabel('Number of Retweets') $ plt.xlabel('Dog Type') $ plt.title('Average Retweets for Dog Type')
print('min wavelength:', np.amin(wavelengths),'nm') $ print('max wavelength:', np.amax(wavelengths),'nm')
%bash $ mkdir sample $ gsutil cp "gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*" sample/train.csv $ gsutil cp "gs://$BUCKET/taxifare/ch4/taxi_preproc/valid.csv-00000-of-*" sample/valid.csv
qualification['opportunity_qualified_date'] = pd.to_datetime(qualification.opportunity_qualified_date)
twitter_archive_enhanced_clean.to_csv('twitter_archive_master.csv') $ tweets_clean.to_csv('tweets_stats.csv')
to_be_predicted_Day2 = 21.20397968 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
log_1.columns
users_converted = df.query('converted == 1').user_id.count() $ users_conv_uniq = df.query('converted == 1').user_id.nunique() $ users_total = df.shape[0] $ users_unique = df.user_id.nunique()
data.sort_index(inplace=True)
station_by_week=station_by_week.reset_index()
corpus = df_creator(article_strings)
pivot_table_pos_count = pd.pivot_table(bb_df, values = ['Wt'], index = ['Pos'], aggfunc = 'count') $ pivot_table_pos_count['pct'] = pivot_table_pos_count.Wt / pivot_table_pos_count.Wt.sum() $ pivot_table_pos_count.sort_values(by = ['pct'], ascending = False)
titanic.sex.value_counts(sort=False).plot(kind='bar')
transform = TfidfVectorizer(lowercase=False, min_df=.01) $ tf_idf_matrix = transform.fit_transform(back2sent.values) $ tf_idf_matrix.shape
lr2 = LinearRegression() $ lr2.fit(train_data, train_labels)
investors_df = investors_df[(investors_df.most_recent_investment >= '2015-10-15') & $                             (investors_df.investment_count > 5)].copy()
apple = apple[['open','high','low','close','adj_close']].set_index(apple['date']) $ apple.head()
data1=data1.interpolate()
nt = nt[(~nt["catfathername"].isin(np.append(hk["catfathername"].unique(),kl["catfathername"].unique())))]
df_mes2 = df_mes2.head(shape1)
deaths_XX_century = deaths_by_decade.reset_index()[mask_XX_century] $ deaths_XX_century.set_index('decade')
df = pd.read_csv('data/btc-market-price.csv', header=None) $ df.columns = ['Timestamp', 'Price'] $ df['Timestamp'] = pd.to_datetime(df['Timestamp']) $ df.set_index('Timestamp', inplace=True)
c_df=c_df.drop_duplicates(keep='first') $ c_df.size
autos['last_seen'] = autos['last_seen'].str[:10] $ last_seen_count_norm = autos['last_seen'].value_counts(normalize=True, dropna=False) $ last_seen_count_norm.sort_index()
prob_contr =df2.query("group=='control'").converted.mean() $ print('The probality of an individual converting in the control group converting is {}'.format(prob_contr))
import statsmodels.api as sm $ df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df['landing_page'])['new_page'] $ df2.tail()
print('{} / {} '.format(data['Voluntary Soft-Story Retrofit'].notna().sum(), len(data['Voluntary Soft-Story Retrofit']))) $ print('{:.3f} % '.format(data['Voluntary Soft-Story Retrofit'].notna().sum() * 100 / len(data['Voluntary Soft-Story Retrofit']))) $ data[data['Voluntary Soft-Story Retrofit'].notna()]['Voluntary Soft-Story Retrofit']
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],alternative='smaller') $ z_score, p_value $
autos["brand"].unique()
df_cond = df.dropna(subset=['lead_result_max_bucket'])
leg = pd.DataFrame(leg, columns = ["TLO_id","last_name","first_name", "filerHoldOfficeCd"])
print(0 * np.nan) $ print(np.nan == np.nan) $ print(np.inf > np.nan) $ print(np.nan - np.nan) $ print(0.3 == 3 * 0.1)
test_delta = test['date'].max() - test['date'].min() # 61 days $ train_delta = train['date'].max() - train['date'].min() # 243 days
autos['price'].describe()
frame.sort_index(axis='columns')
print ('most used hashtag in Top Retweet is : Brexit')
find_rank_given_ID_DRG_YEAR_discharges(100007,2014,154)
n_old = df2.query('landing_page=="old_page"').count()[1] $ n_old
df = spark.createDataFrame(rdd, schema) $ df.printSchema()
tweet_archive_clean.shape $
act_diff = df2[df2['group']=='treatment']['converted'].mean() - df2[df2['group']=='control']['converted'].mean() $ (act_diff < p_diffs).mean()
y_hat=model.predict(X_test)
old_page_converted = np.random.binomial(1, p_old,size = n_old)
import pandas as pd $ from matplotlib import pyplot as plt
!wget http://files.fast.ai/part2/lesson14/rossmann.tgz --directory-prefix={PATH}
item_item_rec = graphlab.recommender.item_similarity_recommender.create(sf_stars, $                                                                         user_id = 'user_id', $                                                                         item_id = 'business_id', $                                                                         target = 'stars')
l = [[12,12],[11,11],[1,10],[10,1],[99,99]] $ l.sort(key=lambda x : x[1], reverse=True) $ l
rules.sort_values(['lift'], ascending=False)
eve_new= len(df_aft.query('ab_page==1')) $ eve_old= len(df_aft.query('ab_page==0')) $ eve_prob =df_aft.converted.mean()
df_archive_clean["tweet_id"].describe()
site_valsdf['valuedatetime'] = site_valsdf.apply(lambda x: pd.to_datetime(x['valuedatetime']).tz_localize(dateutil.tz.tzoffset(None, $                                                                                                                                int(x['valuedatetimeutcoffset'])*60*60)).astimezone(pytz.UTC), axis=1)
print(DataSet_sorted['tweetText'].iloc[0])
journalists_mentioned_by_male_summary_df = journalist_mention_summary(journalists_mention_df[journalists_mention_df.gender == 'M']) $ journalists_mentioned_by_male_summary_df.to_csv('output/journalists_mentioned_by_male_journalists.csv') $ journalists_mentioned_by_male_summary_df[journalist_mention_summary_fields].head(25)
set(hourly_dat.columns) - set(out_temp_columns+incremental_precip_columns+general_data_columns+ wind_dir_columns)
n_new = sum(df2.landing_page == 'new_page') $ n_new
grouped_dpt_city.aggregate(np.mean) # means based on two groups
score_fs.shape[0] / score.shape[0]
cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=cvEvaluator) $ cvModel = cv.fit(trainingData)
session.query(Measurement.station, func.count(Measurement.id)).group_by(Measurement.station).order_by(func.count(Measurement.id).desc()).all()
autos['odometer'] = autos['odometer'].str.replace('km', '').str.replace(',', '').astype(int)
df_merge = archive_clean.merge(image_clean,how='left',on = 'tweet_id',) $
re.findall('title="(.*?)"', slices[4])
ttarc.shape
data.to_csv("output2.csv",index=False)
tf_idf.shape
df3[df3['group']=='treatment'].head()
Raw_Forecast = Sandbox.copy()
df.groupby('Username').sum() [k for k, count in df['Username'].value_counts().iteritems() if count > 1]
theft.DATE_OF_OCCURRENCE = pd.to_datetime(theft.DATE_OF_OCCURRENCE)
%whos
t_retweets = pd.Series(data=data['RTs'].values, index=data['Date'])
dtypes={'item_nbr': np.int64,'family':np.str,'class':np.int64,'perishable':np.int64} $ items = pd.read_csv('items.csv',dtype=dtypes) # opens the csv file $ print("Rows and columns:",items.shape) $ pd.DataFrame.head(items)
autos[['date_crawled','ad_created','last_seen']][0:5]
df2_new[df2_new['group']=='treatment'].head()
print(city_names.index) $ print(cities.index)
def updateDictBMO(description): $     descriptionAmtDict[description] = descriptionAmt(description)
sorted(scores.loc[168],reverse = True)
difference = p_new - p_old $ greater= [i for i in p_diffs if i > difference] $ p_greater = len(greater)/len(p_diffs) $ p_greater
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.choice([0,1], size=n_new, p=[p_new, (1-p_new)]) $     old_page_converted = np.random.choice([0,1], size=n_old, p=[p_old, (1-p_old)]) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean())    
acc.get_monthly_balance('2016-11')
df.groupby(by = ['County']).sum().sort_values(by = ['Sale (Dollars)'], ascending = False).head()
!spark-submit --master spark//localhost:7077 --name 'App Name' script.py data/*
b = a
dfjoined.columns = [['created_date', 'count_complaints_day', 'dayofweek','complaint_type', 'count_type_day']]
dataset = pd.read_csv("NYTimesBlogTrain.csv")
firstWeekUserMerged.isnull().sum()
dff.head()
repos_ids = pd.read_sql('SELECT DISTINCT(repo_id) AS repos_ids FROM repos;', con)
twitter_archive_df.tail(3)
print (train_data_features.shape)
def analyse_correlation(df, dep_variable, indep_variable): $     for v in indep_variable: $          sb.regplot(x=v, y=dep_variable, data=df) $          plt.show() $ analyse_correlation(cats_df.dropna(), 'age at death', ['hair length', 'height', 'number of vet visits', 'weight'])
%matplotlib inline
count_vectorizer = CountVectorizer()
import statsmodels.api as sm $ convert_old = df2[(df2["landing_page"] == "old_page") & (df2["converted"] == 1)]["user_id"].count() $ convert_new = df2[(df2["landing_page"] == "new_page") & (df2["converted"] == 1)]["user_id"].count() $ n_old = df2[df2['group'] == 'control'].shape[0] $ n_new = df2[df2['group'] == 'treatment'].shape[0]
pd.set_option('display.max_columns',53) $ df
combined_df = countries_df.set_index('user_id').join(df.set_index('user_id'), how='inner') $ combined_df['country'].value_counts()
print_txt2pdf_single_site()
autos.drop(["seller", "offer_type","nr_of_pictures"], axis = 1, inplace = True)
[row["longitude"], row["latitude"]] for row in morning_rush.iloc[:5].iterrows()
recip_treatment = round(1/np.exp(-0.0150), 2) $ print('For every 1 unit decrease in conversion, treatment is {} times as likely holding all else constant.'.format(round(recip_treatment, 2)))
model.init_sims(replace=True)
df = pd.read_csv( $     os.path.join('../resource/postproc/cache', 'subj1/trial1', 'acq1.ifc'), $     sep='\t' $ ) $ df.head()
type(model.wv.syn0) $ len(model.wv.vocab) $ model.wv.syn0.shape
data['SMA1'] = data.rolling(50).mean() $ data.tail()
archive_clean.drop('timestamp',axis=1,inplace=True)
p_old_abtest=df2.query('group == "control"')['converted'].mean() $ p_old_abtest
weather_mean.index
lfiles = list_files(ftp)
log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])
nold=df2.query('landing_page=="old_page"')['converted'].shape $ print(nold[0])
print(autos['price'].unique()) $ print(autos['odometer'].unique())
from collections import Counter
pumaBB = pd.merge(pumaBB.iloc[:,:-1],pumaPP[['B28002_001E','NAME']],on='NAME') $ pumaBB['pcBB'] = pumaBB.B28002_004E/pumaBB.B28002_001E * 100
data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, num_workers=4) $ learn = ConvLearner.pretrained(arch, data, precompute=True)
any_of_these = [10029,280060,280040,280077] $ idx = all_sites_with_unique_id_nums_and_names[all_sites_with_unique_id_nums_and_names['id_num'].isin(any_of_these)].index.tolist() $ all_sites_with_unique_id_nums_and_names.loc[idx]
new_dems.newDate[new_dems.Sanders.isnull()] $
df2[['control','ab_page']]=pd.get_dummies(df2['group'])
df2['intercept']=1 $ df2[['control', 'ab_page']]=pd.get_dummies(df2['group']) $ df2.drop(labels=['control'], axis=1, inplace=True) $ df2.head(10)
future_df['forecast'] = forecast
df2.dtypes $ df2['intercept']=1 $ df2[['dummy','ab_page']]=pd.get_dummies(df2['group']) $ df2 = df2.drop('dummy',axis=1) $ df2.head()
session.query(measurement.date).order_by(measurement.date).first()
rain_stats = rain_2017_df.describe() $ rain_stats
def to_loc(x): $     return str(x[5]) + "," + str(x[6])
cust = cust.sort_values(['transaction_date', 'membership_expire_date']).reset_index(drop = True)
df_protest.loc[df_protest.TownCity_Name=='Cape Town'].head().index
df['intercept'] = 1 $ log_mod = sm. Logit (df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'US']]) $ results = log_mod.fit() $ results.summary()
for col in full.columns: $     print(full[col].value_counts())
df_page1 = len(df2.query("landing_page == 'new_page'")) / df2.shape[0] $ print("{} is the probability that an individual received the new page.".format(df_page1))
extract_all.loc[(extract_all.APPLICATION_DATE_short>=datetime.date(2018,6,1)) $                &(extract_all.APP_PRODUCT_TYPE.isin(['TL','RL'])) $                &(extract_all.CLA_RiskScore==1) $                &(extract_all.app_branch_state=='GA')].groupby('DEC_LOAN_AMOUNT1').size()
holidays
df.groupby('userid').sum()
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head() $ df_new['country'].unique() $
p_old=df2.converted.mean() $ p_old
usersDf.account_created_at.hist()
print "The probability of an individual converting in the treatment is {0:.4f}%".format(float(treat_con[0])/sum(df2['group']=='treatment')*100)
display(Markdown(q3d_answer))
df_visits_master = df_totalVisits_day                                                $ years_unique = df_visits_master['year'].unique() $ years_unique
autos["ad_created"].str[:10].value_counts(normalize=True, dropna=False).sort_index()[:40].plot(kind="bar", title="Ad_created (1-40th)", colormap="Blues_r")
df_new[['CA','UK','US']]= pd.get_dummies(df_new['country']) $ df_new.head()
import numpy as np $ ids = np.random.choice(small_train['user_id'].unique(), 100000) $ small_train = small_train[small_train['user_id'].isin(ids)] $ small_train.shape
df_cod2["Cause of death"] = df_cod2["Cause of death"].apply(standardize_cod) $ df_cod2["Cause of death"].value_counts()
df.head(10)
finalDf = finalDf.drop_duplicates(['ZIPCODE'], keep='first');
pd.value_counts(dr_existing['ReasonForVisitName'])
likes = data[data['Likes'] == likes_max].index[0] $ print(likes) $ retweets  = data[data.RTs == retweet_max].index[0] $ print(retweets)
df_cryptdex = df_history[['symbol','date','close','movement_delta']].copy()
joined.head()
df.groupby(df.index.hour).count().sort_values(by='Created Date', ascending = False)
text0 = textract.process(datapath2 / onlyfiles[0]).decode('utf-8') $ print(text0[0:1000])
my_data_test_path = "/Users/leima/OneDrive - University of New Mexico/data/mybilividdata/failed_vid_data.csv" $ my_data_test = np.genfromtxt(my_data_test_path, delimiter=',')
pred.shape
weather_data_used = weather_data['20130101':'20171231'] $ weather_data_used = pd.DataFrame(weather_data_used.temp, index=weather_data_used.index) $ weather_data_null = weather_data_used[weather_data_used['temp'].isnull()] $ weather_data_null = weather_data_null.fillna(0) $ weather_data_null.groupby([weather_data_null.index.year,weather_data_null.index.month]).agg('count').head(60) $
tweet = result[0] $ for item in dir(tweet): $     if not item.startswith("_"): $         print("%s : %s\n" % (item, eval('tweet.'+item)))
users_usage_summaries
df = df.drop_duplicates(subset='id', keep='last') $ df.drop(columns='Unnamed: 0', axis=1, inplace=True) $ print(df.shape)
data['len']  = [len(x.text) for x in tweets]  # number of characters in a tweet (think length) $ data['ID']   = [x.id for x in tweets] $ data['Source'] = [x.source for x in tweets] $ data['Likes']  = [x.favorite_count for x in tweets] # favorite_count $ data['RTs']    =  [x.retweet_count for x in tweets]  # retweet count
z_score , p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ z_score , p_value
states = kickstarter.groupby('state').size() $ states
p_converted = df.query('converted == 1').user_id.nunique()/num_users $ p_converted
df2[df2['landing_page']=='old_page'].count()-1
mock_data = mock_data[(mock_data['ABPm_x'].isnull() == False) & (mock_data['ABPm_y'].isnull() == False) & (mock_data['ABPm'].isnull() == False) & (mock_data['ABPm'] > 0)].head(10) $ mock_data.head(2)
status.dtypes
import pickle $ output = open('speeches_metadata.pkl', 'wb') $ pickle.dump(speeches_metadata, output) $ output.close()
active_fire_zone_df=census_pd_complete[census_pd_complete.county.isin(['Ventura County','Santa Barbara County'])] $ active_fire_zone_df $
output_variables = S.modeloutput_obj.read_variables_from_file() $ output_variables
from matplotlib import pyplot as plt $ %matplotlib inline
flight7.show(2) $ flight7.count()
countries_df = pd.read_csv('countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
X_flat = tf.reshape(X, [-1, n_output], name="X_flat") $ squared_difference = tf.square(X_flat - decoder_output, $                                name="squared_difference") $ reconstruction_loss = tf.reduce_sum(squared_difference, $                                     name="reconstruction_loss")
van_final['diffs'] = van_final['diffs'].fillna(0)
%matplotlib inline $ df_concat_2.boxplot(column="message_likes_rel",by="page") $
df_ml_56 = df.copy() $ df_ml_56.index.rename('date', inplace=True) $ df_ml_56_01=df_ml_56.copy()
dates = num2date(times[:], times.units) $ print([date.strftime('%Y-%m-%d %H:%M:%S') for date in dates[:10]]) # print only first ten...
df_archive.info()
df_questionable_2.head(3)
GRID_accuracy_table = pd.concat( $     [LARGE_GRID.make_table_accuracy(raw_large_grid_df).assign(        algorithm='hmm_nosmooth'), $      LARGE_GRID.make_table_accuracy(raw_large_grid_df_engbert).assign(algorithm='engbert'     )]) $ GRID_accuracy_table
df_archive_clean["source"] = df_archive_clean["source"].replace('<a href="http://twitter.com/download/iphone" rel="nofollow">Twitter for iPhone</a>', $                                                                "Twitter for iPhone")
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print(z_score, p_value) $
to_be_predicted_Day5 = 21.30655088 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
f_close_clicks_device_train = spark.read.csv(os.path.join(mungepath, "f_close_clicks_device_train"), header=True) $ f_close_clicks_device_test = spark.read.csv(os.path.join(mungepath, "f_close_clicks_device_test"), header=True) $ print('Found %d observations in train.' %f_close_clicks_device_train.count()) $ print('Found %d observations in test.' %f_close_clicks_device_test.count())
import tweepy $ authentication = tweepy.OAuthHandler(config.consumer_key, config.consumer_secret) $ authentication.set_access_token(config.access_token, config.access_secret) $ api = tweepy.API(authentication)
display(sm.stats.proportions_ztest([convert_old,convert_new], [n_old,n_new],alternative='larger'),sm.stats.proportions_ztest([convert_old,convert_new], [n_old,n_new],alternative='smaller')) $
plt.hist(p_diffs) $ plt.title('Simulated difference : New & Old Page') $ plt.xlabel('Page Difference') $ plt.ylabel('Frequency');
csvData.head(5)
mismatch.head(15)
print d.variables['time']
result['createdBy'].value_counts()
new_colorder = [ 'empID', 'name', 'salary', 'year1', 'year2'] $ df[new_colorder]
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(lr.set_index('user_id'), how='inner')
systems = ['Pws','Ind','Mgt'] $ for i in systems: $     systemscraper(0,3000,1000,i,syspath) $     systemscraper(10000,13000,1000,i,syspath)
chunk_review=pd.read_csv("ign.csv",chunksize=14) $ r=pd.Series({chunk.iloc[0].title:chunk.score for chunk in chunk_review}) $ r
image_predictions_clean.info()
fig, ax = plt.subplots(figsize=(8,4)) $ Ralston.hist(column="TMAX", ax=ax); $
eug_counts = aqi.groupby(['AQI Category_eug']).size() $ (eug_cg_counts / eug_counts).unstack(fill_value=0)
shows2 = pd.read_csv('shows_good_data.csv') $
plt.hist(p_diffs) $ plt.xlabel('p diffs') $ plt.ylabel('Numbers') $ plt.title('10000 simulated p_diffs'); $
!wget -O teleCust1000t.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/teleCust1000t.csv
expenses_df.melt(id_vars = ["Day", "Buyer"], value_vars = ["Type"])
model.create_timeseries(scenario) $ model.close_db()
list(zip(df.columns, [type(x) for x in df.ix[0,:]]))
techmeme['sources'] = techmeme.extra_sources.copy() $ for i, list_ in enumerate(techmeme.sources): $     list_.append(techmeme.original_source[i])
from sagemaker.predictor import csv_serializer, json_deserializer $ rcf_inference.content_type = 'text/csv' $ rcf_inference.serializer = csv_serializer $ rcf_inference.accept = 'application/json' $ rcf_inference.deserializer = json_deserializer
len(conditions.unique())
all_indicators.ix[:, 0:1]
l = MyStreamListener() $ l
merged2.shape
df_new['US'] = pd.get_dummies(df_new['country'])['US']
twitter_status = unique_urls[(unique_urls.domain == domain) & (unique_urls.url.str.contains('/status/'))] $ twitter_status.sort_values('num_authors', ascending=False)[0: 50][['url', 'num_authors']]
USvideos['trending_date'] = pd.to_datetime(USvideos['trending_date'],format='%y.%d.%m') $ USvideos['time_to_trend'] = (USvideos.trending_date - USvideos.publish_time).astype('timedelta64[ms]')/3600000 $ USvideos.info()
compdf = jobPostDF[jobPostDF['date']>='2013-12-30 00:00:00']
from sklearn.linear_model import LinearRegression as LR $ model = LR(fit_intercept=True, normalize=True, n_jobs=-1) $ model.fit(X,y) $ print model.coef_ $ print model.score(X,y)
altitude_only.info()
(v_item_hub.loc[:, item_hub.columns] == item_hub).sum()
us = geo[index]
au.plot_user_popularity(au.filter_for_support(popular_trg_df, min_times=5),day_list)
get_raw()
dcw = dc.groupby(['YearWeek'], as_index=False).mean() $ tmw = tm.groupby(['YearWeek'], as_index=False).mean() $ dcw.head(5) $ tmw.head(5) $
%sql \ $ SELECT DISTINCT twitter.user_id, twitter.tweet_text FROM twitter \ $ WHERE twitter.tweet_text REGEXP 'Roger Federer|Tennis';
uber_15["day_of_month"].value_counts().head()
np.nan == np.nan
prcp_analysis_df = df.rename(columns={0: "Date", 1: "precipitation"}) $ prcp_analysis_df.head()
right(np.array([3, 0]), np.array([0, 4]))
pd.value_counts(ac['Registration Date']).head()
youthUser4['creationDate'] = pd.to_datetime(youthUser4['creationDate']) $ youthUser4['year'] = youthUser4['creationDate'].dt.year $ youthUser4['month'] = youthUser4['creationDate'].dt.month $ youthUser4.to_csv('lrng/youthUser4.csv')
!find {TRN} -name '*.txt' | xargs cat | wc -w
suspects_with_27_1['loc'] = suspects_with_27_1.apply(to_loc, axis=1)
import statsmodels.api as sm $ convert_old = sum(df2[df2.landing_page == 'old_page'].converted) $ convert_new = sum(df2[df2.landing_page == 'new_page'].converted) $ n_old = df2[df2.landing_page == 'old_page'].shape[0] $ n_new = df2[df2.landing_page == 'new_page'].shape[0]
print 'Total new data = {}'.format(len(data_archie))
print('Here we can see that there are missing values for several features in store:', store.isnull().sum().sum())
print(c_df.isnull().sum()) $
test_preds_df = pd.DataFrame(test_preds,index=test_target.index,columns=['kwh_pred']) $
venues_df[['venue_id','venue_type']].groupby('venue_type').count()
%env INPUT_DATA_FILE data.json
averages = means.reset_index(drop=False) $ averages
d_dates['post'].sum()
df2.index[df2['user_id'].duplicated()==True].tolist()
len(images_df)
text = text.lower() $ print(text)
svm_classifier.score(X_test, Y_test)
import calendar $ month2int = {v.lower():k for k,v in enumerate(calendar.month_name)} $ month2int    
n_new = df2.query('landing_page=="new_page"').count()[0] $ n_new
f_counts_week_ip = spark.read.csv(os.path.join(mungepath, "f_counts_week_ip"), header=True) $ print('Found %d observations.' %f_counts_week_ip.count())
results = client.get(path311, select=select, where=where, limit=limit)
data = pd.read_csv('./fake_company.csv') $ data
calls_df=calls_df.drop(["postal_code"],axis=1)
imputed = np.exp(df_imputed.iloc[na_index,0])
model.summary()
test = pandas.read_csv('test.csv')
re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', cfd_index['rt'][10])
graphs_filename = graphs_title + ".html" $ output_file(graphs_filename, title=graphs_title) $ show(tabs)
df = pd.read_csv('result_summary_combined.csv', na_values=['NaN'])
results.query('home_score > @min_home_score')
au.find_some_docs(uso17_coll,limit=3)
DataSet = toDataFrame(results) $ DataSet.head()
energy = load_data()[['load']] $ energy.head()
fp7_proj.shape, fp7_part.shape, fp7.shape
motion_df.tail()
go_no_go_times = go_no_go.groupby('subject_id').Time.unique() $ simp_rxn_time_times = simp_rxn_time.groupby('subject_id').Time.unique() $ proc_rxn_time_times = proc_rxn_time.groupby('subject_id').Time.unique()
holidays = [datetime(2018, 1, 5), datetime(2018, 3, 14)]
i_unique_user = df.user_id.nunique() $ i_unique_user
X.shape
len(data_2012['date'].unique())
print(seen_and_click.shape) $ seen_and_click[0:20]
!python OpenSeq2Seq/run.py --config_file=OpenSeq2Seq/example_configs/nmt.json --logdir=./nmt --mode=infer --inference_out=pred.txt
avgPurchP = train.groupby(by='Product_ID')['Purchase'].mean().reset_index().rename(columns={'Purchase': 'AvgPurchaseP'}) $ train = train.merge(avgPurchP, on='Product_ID', how='left') $ test = test.merge(avgPurchP, on= 'Product_ID', how='left')
image_df_clean.sample(10)
rt.sample(5)
dates_by_tweet_count['Volume'].corr(dates_by_tweet_count['Percent_change'])
print("P-CG-converting:", $       df2[df2['group']=='control']['converted'].mean())
print(' the probability of an individual converting regardless of the page they receive is {}'.format(round(df2['converted'].mean(),4)))
percent_change_features = df.pct_change().shift() $ percent_change_features.columns = [col + ' PCT Change' for col in percent_change_features.columns]
precip_df = query2pandas(precip) $ precip_df = precip_df.set_index('Date')
data.treatment = 1 $ data
import statsmodels.api as sm; $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = len(df2.query("group == 'control'")) $ n_new = len(df2.query("group == 'treatment'"))
to_be_predicted_Day4 = 25.10824769 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
sensors = hp.search_sensors(type='electricity', system='solar') $ print(sensors) $ df = hp.get_data(sensors=sensors, head=head, tail=tail, diff=True, unit='W') $ charts.plot(df, stock=True, show='inline')
df[['Visitors','Bounce_Rate']]
noise_graf = noise_graf.drop('geometry',axis=1)
roc_auc_score(y_test, y_pred_mdl)
df_vow.plot() $ df_vow[['Open','Close','High','Low']].plot()
y_pred = xgb_model.predict(X_test)
df = df.query('breed != "Not identified"')
with open('clean_tweets_sample.pkl', 'wb') as picklefile: # wb: write, binary $     pickle.dump(sampleDF, picklefile) #dump data into pickle file
print len(targetUsersRank['norm_rank']) $ print len(targetUsersRank[targetUsersRank['norm_rank']<NEGATIVE_THRESHOLD]) $ print len(targetUsersRank[targetUsersRank['norm_rank']<VERY_NEGATIVE_THRESHOLD])
df_test = text_classifier.predict(df_test) $ df_test.head()
df_users_6['unit_flag'].unique()
rmse_scores = np.sqrt(mse_scores) $ print(rmse_scores)
stool_df.head()
df_merged.drop(['created_at'], axis=1,inplace=True);
twitter_archive_clean.shape[0]
pickle_full = "Data Files/sorted_stays.p"
df_twitter_copy.at[2335, 'rating_numerator'] = '9' $ df_twitter_copy.at[2335, 'rating_denominator'] = '10'
df_precipitation = pd.melt(df_precipitation, id_vars=['Year'], var_name='Month', value_name='Precipitation')
airbnb_od = OutliersDetection(df_airbnb)
Pold = df2.converted.mean() $ Pold
print(players.shape) $ players.head()
np.array(df1.index)
df_us = df_new[df_new['country'] == 'US'] $ df_us['converted'].mean()
df.groupby('season')['episode_id'].nunique().hist()
len(df2[df2['landing_page'] == 'new_page']) / len(df2)
search = []    $ for values in archive_df_clean['text']: $     search.append(re.search(r'([0-9]+(\.)?([0-9])*)', values).group()) $ archive_df_clean['clean_numerator'] = search $
high_ranked_question_threshold = np.percentile(questions_scores, 90) $ high_ranked_question_threshold
nmf_cv_df, nmf_tfidf_df, lsa_cv_df, lsa_tfidf_df, lda_cv_df, lda_tfidf_df = mf.gen_df_per_combo(nmf_cv_data, nmf_tfidf_data, lsa_cv_data, lsa_tfidf_data, lda_cv_data, lda_tfidf_data, 7) $
steemit_accounts = unique_urls[(unique_urls.domain == domain) & (unique_urls.url.str.contains('.com/@'))] $ steemit_accounts.sort_values('num_authors', ascending=False)[0:50][['url', 'num_authors']]
etr = ExtraTreesRegressor(random_state=1)
soup.img
x = tags['Count'][0:20] $ y = tags['TagName'][0:20] $ sns.barplot(x, y, color = 'g')
newesr[['first', 'second']] ** 2
print(clf.class_count_)
%matplotlib inline
tesla.nlp_text = tesla.nlp_text + ' ' $ tesla = tesla.groupby('date')['nlp_text'].sum() $ tesla = pd.DataFrame(tesla) $ tesla.columns = ['tesla_tweet']
bixi=pd.read_csv('OD_2018-07.csv') $ stations=pd.read_csv('Stations_2018.csv')
data = data.loc[(data.lat<-34) & (data.lat>-35) & (data.lon>-60) & (data.lon<-57.5)]
vect = CountVectorizer(ngram_range=(1,2)) $ vect.fit_transform(['no i have cows', 'i have no cows']).toarray(), vect.vocabulary_
df2.join([df1, df3, df4], how='outer').fillna('')
results.summary()
print(q3a_answer)
baseball.reindex(id_range, fill_value='charliebrown', columns=['player']).head()
clf = LogisticRegression(fit_intercept=True).fit(X, y)
google_stock.tail() $ google_stock.tail(10)
(p_diffs > obs_diff).mean()
dfs_morning = (dfs_morning.sort_values(by=['STATION', 'DATE_TIME']) $                .groupby(['STATION', 'DATE_TIME'])['ENTRIES'] $                .sum() $                .reset_index()) $ dfs_morning['DATE'] = pd.to_datetime(dfs_morning['DATE_TIME']).apply(lambda x:x.date())
students.weight.plot.hist()
nrows = df.shape[0] $ print(nrows)
n_new = len(df2_treatment.index) $ n_new
lfiles = [x for x in lfiles if '060' in x]
total = len(df2.index) $ total_converted = (df2['converted'] == 1).sum() $ print(total_converted / total)
building_pa_prc_shrink=pd.read_csv('buildding_01.csv',parse_dates=['permit_creation_date'])
import statsmodels.api as sm $ convert_old = sum((df2.group == 'control')&(df2.converted == 1)) $ convert_new = sum((df2.group == 'treatment')&(df2.converted == 1)) $ n_old = sum(df2.landing_page == 'old_page') $ n_new = sum(df2.landing_page == 'new_page')
yhat = LR.predict(X_test) $ yhat
len([1 for h in heap if len(h.tweets) == 2])/len(heap)
%%time $ data = get_url_data()
df.head(5)
columns = inspector.get_columns('measurement') $ for c in columns: $     print(c['name'],c['type'])
station_count = session.query(Station).count() $ station_count
cityID = 'e4a0d228eb6be76b' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Philadelphia.append(tweet) 
df2['tweet_id'][(df2['predict_3_breed'] == True)].count()/2075 $
df2[df2.duplicated('user_id', keep=False)]
fb_day_time_gameless = fb_day_time[fb_day_time.service_day.isin(all_game_dates) == False]
unique_users = df.nunique()['user_id'] $ print("The number of unique users in the dataset is: {}".format(unique_users)) $
df_with_metac1 = pd.concat([df_onc_no_metac, df_dummies], axis=1)
print(len(comm_merge),len(dat))
cust_data1['Deciles']=cust_data1['Deciles'].astype('str')
df.text[0]
autos['price'].head(10)
import pandas as pd
merged.groupby(["contributor_firstname", "contributor_lastname", "committee_position"]).amount.sum().reset_index().sort_values("amount", ascending=False)
shiny = pd.read_csv('\\\\allen\\programs\\celltypes\\workgroups\\rnaseqanalysis\\shiny\\patch_seq\\mouse_patchseq_VISp_20171204_collapsed90\\mapping.df.with.bp.90.csv')
census_pd_complete=pd.read_csv("California_census_data_with_location.csv",delimiter=',',encoding="ISO-8859-1") $ census_pd_complete.head()
subject_id_to_group = {} $ for key, value in ref_dict.items(): $     if key in patient_group_dict: $         subject_id_to_group[value]= patient_group_dict[key]
result2.summary()
to_be_predicted_Day3 = 22.39050512 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
tweet = tweets[3] $ tweet.retweets()
assert not tcga_target_gtex_expression_hugo_tpm.isnull().values.any() $ assert not treehouse_expression.isnull().values.any() $ assert np.array_equal(tcga_target_gtex_expression_hugo_tpm.index, treehouse_expression.index)
import tweepy $ import pandas as pd $ import matplotlib.pyplot as plt $ pd.set_option('display.mpl_style', 'default')
dfJobs['DESCRIPTION'].ix[4009].split('\\n')
country = pd.read_sql_query('select * from Country', conn)  # don't forget to specify the connection $ print(country.shape) $ country.head()
lims_query = "SELECT name, barcode FROM specimens \ $ WHERE specimens.ephys_roi_result_id IS NOT NULL" $ lims_df = get_lims_dataframe(lims_query) $ lims_df.tail()
_ = ok.grade('q05f') $ _ = ok.backup()
import json $ tweets = api.search(q = "#modi", count = 100) $ for tweet in tweets: $     posts.insert_one(tweet._json)
results.summary()
subtweets_live_list = [] $ non_subtweets_live_list = []
question_2_dataframe_in_top_zips = question_2_dataframe[question_2_dataframe['incident_zip'].isin(top_10_zipcodes_by_population)]
tweets_p1_month = twitter_archive_master.loc[:, ['Month', 'Year', 'Frist_pred_conf']]
zero_rev_acc_opps.drop(labels=[' Total BRR ', ' AnnualRevenue ', $        'NumberOfEmployees', ' DandB Revenue ', 'DandB Total Employees'], axis=1, inplace=True)
word_vecs.sample(10)
df['net_profit'] = (df['state_bottle_retail'] - df['state_bottle_cost']) * df['bottles_sold']
sales = pd.DataFrame(sale2_table) $ sales
df['converted'].mean() * 100
n_new = df2[(df2['group'] == 'treatment')].shape[0] $ n_new
s4.shape
summed.fillna('missing')  # changed dtype to "object"
for i, v in enumerate(twitter_df.columns): $     print(i, v)
unique_title_artist = pd.unique(list(zip(df.loc[:,'title'], df.loc[:,'artist']))) $ len_unique_title_artist = len(unique_title_artist) $ print(len_unique_title_artist) $ unique_title_artist[:10] $ batch_size = 1000
valid_msno.shape
vulnerability_type_histogram = cved_df.groupby(by=['vulnerability_type'])['cwe_id','cve_id'].count() $ vulnerability_type_histogram $
total_rech_amt_6_7 = (telecom["total_rech_amt_6"] + telecom["total_rech_amt_7"] + telecom["total_rech_data_amt_6"] + telecom["total_rech_data_amt_7"]) /2.0 $ amont_70_pc = np.percentile(total_rech_amt_6_7, 70.0) $ print('70 percentile of first two months avg recharge amount: ', amont_70_pc); print_ln();
import os, sys $ import pandas as pd $ import subprocess $ from phonlab.utils import dir2df
import lifelines
session.query(Measurements.date).order_by(Measurements.date.desc()).first()
daily_rank = functions.rank() \ $                       .over(daily_window) \ $                       .alias('rank')
old_page_converted=np.random.choice([1,0],size=nold,p=[pold,1-pold])
plt.plot(df['Gross Sales'],df['Units Sold'],'.')
print('Total null values in name column: ', data.name.isnull().sum()) $ print('\nInconsistent data for the missing name rows\n') $
building_pa_specs = pd.read_csv('DataDictionaryBuildingPermit.csv') $ building_pa_specs
display(Markdown(q6b_answer))
filing_ts_df = pd.DataFrame(all_patents_df.filing_date.value_counts()).sort_index() $ filing_ts_df = filing_ts_df.rename(columns={'filing_date':'filings'}) $ filing_ts_df['running_total'] = filing_ts_df.filings.cumsum() $ filing_ts_df.info()
d6 = d5.T $ d6
user = user.set_index("screen_name") $ user.head(3)
use_columns=['total fuel (mmbtu)', 'generation (MWh)', $                                                'elec fuel (mmbtu)'] $ eia_extra = (eia_total_monthly.loc[idx[:,:,:], use_columns] - $              eia_facility_fuel.loc[idx[:,:,:], use_columns]) $ eia_extra.loc[idx[['HPS', 'DPV'],:,:], use_columns] = eia_total_monthly.loc[idx[['HPS', 'DPV'],:,:], use_columns] $
df2_new['intercept'] = pd.Series(np.zeros(len(df2_new)), index=df2_new.index) $ df2_new['ab_page'] = pd.Series(np.zeros(len(df2_new)), index=df2_new.index)
my_df["company_create"].plot() $ my_df["company_active"].plot() $ plt.ylabel("Ncompany") $ plt.legend() $ plt.show()
cityID = '389e765d4de59bd2' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Glendale.append(tweet) 
sns.factorplot(y='Average_Housing_permits',x='Date',data=df_newhouse,kind='bar',aspect=7)
n_new = len(df2[df2.group == 'treatment']) $ n_new
a = tips.loc[:,"tip"] $ a.head()
!hdfs dfs -cat {HDFS_DIR}/p32cfr-output/part-00001 {HDFS_DIR}/p32cfr-output/part-00000 > p32cfr_results.txt
beirut[['Mean TemperatureC', 'Mean Humidity']].plot(grid=True, figsize=(15,10), subplots=True)
ser.sum()
df2[df2.duplicated('user_id') == True]['user_id']
Test.AutoRun(1, True)
tweets['retweeted'].value_counts()
tweets=collect_tweet_text(tizibika)
fields = [num for num in xrange(15)] $ weather_df = pd.read_csv(weather_file, parse_dates=[['Date', ' TimeCST']], usecols=fields) $ print "Weather data loaded." $ weather_df.head()
n_new = df2.query('group == "treatment"')['converted'].count() $ n_new
nitrodata['ActivityStartDate'] = pd.to_datetime(nitrodata['ActivityStartDate'],format="%Y-%m-%d")
us[us['country'].isna()]
df = pd.DataFrame(list('abcd'), index = [pd.Timestamp('2017-01-01'), pd.Timestamp('2017-01-08'), $                                          pd.Timestamp('2017-01-15'), pd.Timestamp('2017-01-22')]) $ df
consumer_key = 'tRby6pJGgzaN1Y9OOFU8nOzCV' $ consumer_secret = 'aL7BAVZ4UwBQ1HyffIxsCG2da8BdJTcFD3WziDe3mePFFLoA2u' $ access_token = '18666236-DmDE1wwbpvPbDcw9kwt9yThGeyYhjfpVVywrHuhOQ' $ access_token_secret = 'cttbpxpTtqJn7wrCP36I59omNI5GQHXXgV41sKwUgc'
df_mas.dog_stage.value_counts()
sentiments_pd.head()
from sklearn.model_selection import train_test_split
overallFireplaces = pd.get_dummies(dfFull.Fireplaces)
data = pd.read_csv('csvs/datosFiltrados.csv', low_memory=False)
PHI = pd.read_excel(url_PHI, $                     skiprows = 8)
engine = enginegetter.getEngine()
df_mod.head()
data = [1,2,3,4,5] $ df = pd.DataFrame(data) $ print(df)
len(nba_df)
daily_normals(['05-25','05-26','05-27','05-28','05-29','05-30','05-31','06-01','06-02','06-03','06-04','06-05','06-06','06-07','06-08'])
reviewsDF.head()
import requests $ import pandas as pd $ from bs4 import BeautifulSoup
ml.run_ml_flow(df1)
department_df["Standardized_Revenue"] = grouped_dpt["Revenue"].transform(zscore) # transform revenue to be total departmental revenue $ department_df
df['Complaint Type'].value_counts()
df2[df2['user_id'].duplicated(keep = False)]
df_mas=df_mas[df_mas.rating_numerator>9] $ df_mas=df_mas[df_mas.rating_numerator<15]
df['y'].plot.box(notch=True)
youthUser2 = pd.merge(left=youthUser1, right=city1, how='left', left_on='cityId', right_on='_id' ) $ youthUser2.head()
high_rev_acc_opps_net.shape
df_repairs
sum(df2['converted'])
classifier = train_classifier(articles_train, authors_train, articles_test, authors_test, $                               SVC(kernel='linear')) $ print(classifier.score(test_features_tokenized, authors_test))
df.shape[0] #finding number of rows in a dataset
df_sites.loc[125:135] $ id_numbers_to_drop = df_sites.loc[:len(idx_places_less_million),'id_num'] $ id_numbers_to_drop.sort_values()[:10]
n_new=df2[df2['landing_page']=='new_page']['converted'].count() $ n_new
import statsmodels.api as sm $ log_regression =sm.Logit(df2['converted'], df2[['intercept','treatment']]) $ output = log_regression.fit()
counts = count_vectorizer.transform(final_tweets).transpose()
ts = pd.Series(np.random.randn(len(rng)), index=rng).resample('D').mean() $ ts
new_reps.Trump.astype("float64").describe()
pd.set_option('display.max_rows', 500)
df.converted.mean()
viscorr = viscorr.drop(columns = ['BTC Price', 'BTC Price Change', 'BTC Volume', 'ETH Price', $                                   'ETH Price Change', 'ETH Volume']) $ viscorr = viscorr.drop(viscorr.index[-4:]) $ viscorr $
np.sqrt(information_ratio)
print(p.shape) $ print(p)
match[match.iloc[:,55 :66].notnull().any(axis=1)].iloc[:5,55 :66] # get rows with non-nulls in columns 55 :66
recommendationTable_df = recommendationTable_df.sort_values(ascending=False) $ recommendationTable_df.head()
roc_score=100*roc_auc_score(y_test, y_hat) $ print(roc_score)
import pyspark.sql.functions as func $ parsed_test.groupBy().agg(func.max(col('id'))).show()
committees_NNN.info()
date = pd.to_datetime(date_str) $ date
users = users.join(newgeo)
ts_utc.tz_convert('US/Eastern')
df_selection.describe()
asdf['Trump']=asdf['text'].apply(lambda x: 'TRUMP' in x.upper()) $ colsToUse=['wk','WkEnd','Trump','n'] $ aDict={'WkEnd':'max','n':'count','Trump':'sum'} $ wkTrump=asdf[['wk','WkEnd','Trump','n']].groupby('wk').agg(aDict)
df2_treatment = df[df.group == 'treatment'] $ (df2_treatment.converted == 1).sum()/len(df2_treatment)
df_merged.nlargest(10, 'p1_conf').loc[df_merged['p1_dog'] == True]
database['DataFrame 0']
autos = autos[(autos['year_of_registration']>=1910)&(autos['year_of_registration']<=2016)] 
X_train = X_train.loc[:,[col for col in X_train.columns if col not in ['Footnote','CountyName','Month']]]
WHOregion = df['WHO Region'].values $ WHOregion
trainx, testx, trainy, testy = train_test_split( cleaned_df, cleaned_df_y)
revenue.tail()
d + pd.tseries.offsets.YearEnd()
so = stackexchange.Site(stackexchange.StackOverflow) $
df_germany = df_germany.drop(df_germany.columns[0], axis=1)
X_test.shape
type(prcp_analysis_df.precipitation[0])
portvals = ml4t.compute_portvals(dforders=dforders, dfprices=trade_data_btc.df_h, trend=X_btc_test.index $                                 , start_val=10000, commission=0.0029, impact=0.001) $ portvals
author_data.iloc[39]
test_orders_prodfill['reordered']=y_pred_mdl7
team_search_count_df = pd.DataFrame.from_dict(team_search_count, orient='index').reset_index() $ team_search_count_df.columns = ['name', 'search_count'] $ teams_df = pd.read_csv('~/dotaMediaTermPaper/data/teams_df.csv') $ teams_df = pd.merge(team_search_count_df, teams_df, on='name', how='right')
engine.execute('SELECT * FROM measurements LIMIT 10').fetchall() $
b_cal.head(2)
def get_list_tot_vidviews(the_posts): $     list_tot_vidviews = [] $     for i in list_Media_ID: $         list_tot_vidviews.append(the_posts[i]['activity'][-1]['video_views']) $     return list_tot_vidviews
faulty_names = archive_clean[archive_clean.name.str.islower()].name $ print(faulty_names)
not_in_misk = data_science_immersive_df.merge(dsi_me_1_df, how='left', on='name')
df_h1b_ny = df_h1b[map(lambda x: x[1].lca_case_workloc1_state=='NY' $                        or x[1].lca_case_workloc2_state=='NY', $                        df_h1b.iterrows())] $ print('There are {:.0f} visa applications for jobs located in NY in this dataset.'.format(df_h1b_ny.shape[0]))
for element in x: $     print element['id'] $     print(element['full_text']) $     print('--')
s2.iloc[1:3]
import quandl $ import numpy as np $ import pandas as pd $ import datetime as dt $ import matplotlib.pyplot as plt
timeseries.tail()
twitter_archive_master[(twitter_archive_master.iloc[:,8:12].sum(axis=1) == 0) & (twitter_archive_master['has_stage'] == 1)]
rent_db3.describe()
to_be_predicted_Day2 = 31.2931565 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
dfHashtags.head()
1 in a_list
import pandas as pd $ import numpy as np $ import time $ finalGoodTargetUserItemInt=pd.read_pickle('datasets/goodNonColdTargetUserInt.pkl')
data['y'] = (data['Global_Sales'] > 1)
df.loc[df.Sentiment==0, ['description','Sentiment']].head(10)
a_result_1 = df1.append([df3]) # same as option 1 above $ a_result_1
a = show_yearly_by_DRG(my_list[0]) $
pos_tweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] > 0] $ neu_tweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] == 0] $ neg_tweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] < 0]
occurrences = np.asarray(vectorized_text_labeled.sum(axis=0)).ravel() $ terms = vectorizer.get_feature_names() $ counts_df = pd.DataFrame({'terms': terms, 'occurrences': occurrences}).sort_values('occurrences', ascending=False) $ counts_df
train_X, test_X, train_y, test_y = train_test_split( $     article_titles, article_isRelevant, test_size=0.25, random_state=0) $
visualize_tree(columns, maximum_depth=1)
QUIDS_wide $ QUIDS_wide.dtypes $ QUIDS_wide["qstot_0"] = pd.to_numeric(QUIDS_wide["qstot_0"], errors='raise', downcast='integer') $ QUIDS_wide["y"] = pd.to_numeric(QUIDS_wide["y"], errors='raise', downcast='integer')
import numpy as np $ import pandas as pd $ from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS $ from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
glm_binom_feat_1 = H2OGeneralizedLinearEstimator(family='binomial', solver='L_BFGS', model_id='glm_v4') $ glm_binom_feat_1.train(covtype_X, covtype_y, training_frame=train_bf, validation_frame=valid_bf)
twitter_count = pd.DataFrame(twitter_count, columns=['base_twitter_count'])
temp_cat.describe()
df[df['processed'].isna() == True]
autos_real['registration_year'].value_counts(normalize=True)
ser5.index
print(festivals.dtypes) $ print(festivals.info()) $
import pandas as pd $ pd.__version__
df.T
unique_users = df2.user_id.nunique() $ unique_users
df.plot(y = 'Close')
(reddit["engagement"] > 0).sum()
%matplotlib inline $
data = data[(data['Latitude'].notnull()) & $             (data['Longitude'].notnull())  & $             (data['Closed Date'].notnull())]
keras_entity_recognizer.fit(df_train)
logit_mod = sm.Logit(df_new2['converted'], df_new2[['intercept', 'ab_page', 'UK_pages']]) $ results = logit_mod.fit() $ results.summary()
sub1 = sub1[['hacker_id', 'challenge_id', 'time']]
autos.head()
loans_fut_bucket_30360_xirr_groups*100
details.shape
df = pd.read_csv("crypto_sent.csv")
y = new["Purchased"] $ x = new.drop("Purchased",axis=1) $ Y = new_scaled["Purchased"] $ X = new_scaled.drop("Purchased",axis=1)
stations
session = Session(engine) $
TX_profit.head()
import pandas as pd $ import matplotlib.pyplot as plt $ import seaborn as sns # package for nice plotting defaults $ sns.set()
try: $     ValidHHFile.parser.parseLine('41B0044210100320100101-9999-9999   25   24') $ except parser.BadLine as err: $     print(err)
merge[merge.columns[7:22]].head(3)
image_clean.sample(5)
norm.ppf(1-0.05/2)
lista = api.search('assholes',count=100) $ len(lista)
print(dfx.dropna(how='any'))
LR_grid.best_params_
weather.head()
a= find_DRGs_in_given_year(2015) $ a.sort() $ a[45]
index
purchases['Hear About'].value_counts()
twitter_ar.to_csv('twitter_archive_edited.csv',index=False)
slack = Slacker(slack_api_token) $ channels = slack.channels.list() $ for channel in channels.body['channels']: $     print(f'Channel {channel["name"]} Purpose: {channel["purpose"]["value"]}')
new_comments_df = pd.read_csv('input/test.csv') # Replace 'test.csv' with your dataset $ X_test = test["comment_text"].str.lower() # Replace "comment_text" with the label of the column containing your comments
train_norm.head(3)
age_category1 = df_titanic_temp.loc[df_titanic_temp['age'] < age_median, 'age'] $ age_category2 = df_titanic_temp.loc[df_titanic_temp['age'] >= age_median, 'age'] $ print(len(age_category1)) $ print(len(age_category2))
np_places = np.array(countries)
prop_users_converted = round(df.converted.mean() * 100) $ print('The proportion of users converted is {}%.'.format(prop_users_converted))
lm = sm.formula.ols(formula='Sales_in_CAD ~ Date_of_Order', data = New_df).fit() $ lm.params
sky = TextBlob("The sky is blue") $ sky.sentiment
obs_diff = treatment_convert - control_convert $ obs_diff
simband_time_df.set_index('subject_id', inplace=True)
daily.rolling(50, center=True, $               win_type='gaussian').sum(std=10).plot(style=[':', '--', '-']);
season_team_groups.aggregate(np.mean).sort_values(by = "Tm.Pts", ascending = False).head(5)
autos['brand_model'].value_counts().head()
len(b_cal_q1['price'])
daily_sales=pd.merge(daily_sales,oil,on=['date'],how='left') $ print("Rows and columns:",daily_sales.shape) $ pd.DataFrame.head(daily_sales)
received_old_page = len(df2.query('landing_page == "old_page"')) $ received_old_page
temps_df.Missouri - temps_df.Philadelphia
average_for_each_calendar_month = s.resample('M').mean() #Calculating the avarage of each calender month in series 's' $ print(average_for_each_calendar_month) #Printing the average of values in each calender month in series 's'
mb = pd.read_csv("Data/microbiome.csv", index_col=['Taxon','Patient'])
cabin_reduce = cabin.dropna() $ cabin_floor = cabin_reduce.astype(str).str[0] $ cabin_floor.value_counts().plot(kind='pie', autopct='%.2f', fontsize=20, figsize=(12, 12)) $ cabin_floor.value_counts()
q1 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q1) $ q1.get_shape
pd.DataFrame( $     rng.randn(6, 3), $     index = pd.date_range('20180101', periods = 6), $     columns = ['C', 'B', 'A'] $ )
age_hist = pd.crosstab(index=goodreads_users_df[goodreads_users_df['age'].notnull()]['age'], columns="count") $ age_hist['age_freq'] = age_hist['count'] * 100 / age_hist.sum()['count'] $ age_hist = age_hist.sort_index(ascending=True) $ age_hist.head(10)
rfr.fit(features_regress_vect, overdue_duration)
import pandas as pd $ import numpy as np
tweets = megmfurr_tweets[megmfurr_tweets["text"].str.contains("RT")==False] $ megmfurr_tweets[megmfurr_tweets["text"].str.contains("RT")==False]['text'].count() # 895
df_test = pd.read_csv(DATAPATH+'test.csv', names=colNames, dtype=colDtypes, header=0)
df.head(3)
null_vals=np.random.normal(0,p_diffs.std(),len(p_diffs)) $ plt.hist(null_vals) $ plt.axvline(x=obs_diff,color='red')
bottom_10_above_20[['tweet_id','rating_numerator', 'rating_denominator', 'text']]
predY = bikescore.run(tsX[0:5,:]) $ trueY=str(pd.DataFrame(data={"Actual Values":np.squeeze(np.reshape(tsY[0:5], newshape= [-1,1]), axis=1)}))
def normalize(x): $     return np.array(x / np.max(x)) $ tests.test_normalize(normalize)
shifted_backwards.tail(5)
user_summary_df.head()
df2.loc['2016-06-19':'2016-10-20']
to_be_predicted_Day1 = 21.01 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
log_mod_m = sm.Logit(df_countries['converted'], df_countries[['intercept','UK_ind_ab_page','US_ind_ab_page']]) $ results_m = log_mod_m.fit() $ print("DONE") $
tweets_df.created_at.describe()
ET_Combine
round(date_df['duration(min)'].describe(), 2)
df_archive_clean.head()
from sklearn.feature_extraction.text import CountVectorizer $ from sklearn.model_selection import train_test_split
archive_df = pd.read_csv('twitter-archive-enhanced.csv', sep = ',')
MATTHEW_92_USERS_AC.sample(10)
df.loc[df.category ==1022200, 'category_name'] = 'TEQUILA'
countries = pd.read_csv('./airbnb firstdestinations/countries.csv')
scaler_M7 = StandardScaler().fit(training_X)
unsorted_df.sort_values(by=['col2','col1'],ascending=False)
print('Given that an individual was in the \'treatment\' group, the probability they converted is 0.118808')
print('Coefficients: \n', regr2.coef_) $ print('Intercept: \n', regr2.intercept_)
df1.shape
social_disorder = scale_values(social_disorder, list(range(1,10))) $ social_disorder.head()
for idx, row in df_trips.iterrows(): $     df_trips.loc[idx, "trip_started"] = row["trip_requested"] + np.random.randint(60, 60 * 15)
plt.hist(df_elect['Polarity'], bins=15) $ plt.title('Tweet #Election2018') $ plt.show()
stations = session.query(Measurement).group_by(Measurement.station).count() $ print (f"There are {stations} stations")
QLESQ = QLESQ[(QLESQ["level"]=="Level 1") & (QLESQ["days_baseline"] <= 7)] $ QLESQ.shape
for x in results: $     print(x[3])
df.first_day_pctchg = (df.first_day_close-df.offer_price)/df.offer_price $ df['pct_chg_opencls'] = ((df['dollar_chg_opencls']/df['open_price'])) $ df['dollar_chg_opencls'] = (df['dollar_change_close'] - df['dollar_change_open'])
pd.DataFrame(records3.describe().loc['mean', ['Age', 'GPA', 'Days_missed']])
(autos["last_seen"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         )
df_new[['CA','UK','US']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop('CA',axis=1) #Making the CA column the baseline for country $ df_new.head()
p_stats.head(1)
MATTHEWKW.head()
snow.drop_table("st_rvo_me_ref")
birth_dates.head(3)
stock.news_sources = stock.news_sources.fillna('') $ mlb = MultiLabelBinarizer() $ stock = stock.join(pd.DataFrame(mlb.fit_transform(stock.news_sources), columns=mlb.classes_, index=stock.index)) $ stock = stock.drop('news_sources', 1)
df.columns
user_answered_counts = df.set_index('user_id').index.value_counts().sort_index()
!curl -I 'http://www.scielo.br/scielo.php?script=sci_arttext&pid=S1414-32832017000200349&lng=en&nrm=iso&tlng=pt&debug=xml'
df = pd.concat(map(pd.read_csv, glob.glob("*.csv")))
spark_df = sqlContext.createDataFrame(pandas_df)
import sys
tf_mat1 = tf.constant(matrix1) $ tf_mat2 = tf.constant(matrix2)
telemetry_feat = pd.concat([telemetry_mean_12h, $                             telemetry_sd_12h.ix[:, 2:6], $                             telemetry_mean_24h.ix[:, 2:6], $                             telemetry_sd_24h.ix[:, 2:6]], axis=1).dropna() $ telemetry_feat.describe()
people.groupby(lambda x:GroupColFunc(people,x,'b')).std()
merged_complaints_per_tract_capita.sort_values( $     by = 'unique_complaints_per_capita', $     ascending = False $ ).head(5)
subwaydf['4HR_Entries'].nsmallest(20)
df = pd.DataFrame(results[:10], columns=['']) $ df.set_index('', inplace=True) $ df.head(10)
df2['intercept']=1 $ model=sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ result=model.fit() $
word_count = Counter() $ for sent in df_links[df_links['link.domain'] == 'hannity.com']['tweet.text'].values: $     word_count.update([w for w in sent.split() if w not in stopwords.words('English')]) $ word_count.most_common(10)
properati['currency'].value_counts(dropna=False)
All_tweet_data_v2.retweeted_status_timestamp=pd.to_datetime(All_tweet_data_v2.retweeted_status_timestamp) $ All_tweet_data_v2.retweeted_status_timestamp.dropna().head()
reddit.info()
transactions.info()
tweet_json_df = pd.DataFrame(columns= ['id','retweet_count', 'favorite_count'])
state = env.reset() $ state, reward, done, info=env.step(env.action_space.sample()) $ state.shape
import dotce $ stats = dotce.LanguageStats()
inputNetwork = NetworkBuilder("inputNetwork")
e2.class_var = 'e2 class var is actually an instance var' $ print(e.class_var) $ print(e2.class_var) $ print(e.__dict__) $ print(e2.__dict__)
grouped_by_date_df = full_df.groupby('ymd')['listing_id'].count().reset_index().copy() $ grouped_by_date_df.columns = ['date','count_of_listings'] $ grouped_by_date_df.head(3)
(autos["date_crawled"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_values() $         )
train_downsampled = training.sampleBy('label', fractions={0.0: 0.135, 1.0: 1.0}, seed=123).cache() $ train_downsampled.groupby('label').count().show() $ testing.groupby('label').count().show() $
clean_measure = measure.fillna(0)
import matplotlib.pyplot as plt $ import numpy as np $ import pandas as pd $ import seaborn as sns
data2017_list
articles['pair_id'] = articles.apply(lambda row: nearest_id(row.created_at.tz_convert(None)), axis=1)
countries_df.info()
df.Opened.dt.year.value_counts().sort_index()/len(df)
one_day_users = users[(users['LastAccessDate'] < (pd.Timestamp.today() - pd.DateOffset(years=1))) & $       (users['DaysActive'] == 0)] $ p_one_day_users = one_day_users.shape[0] / users.shape[0] * 100 $ print('Number of users that left the same day of signing in (one-day-users):', one_day_users.shape[0]) $ print('Percent of one-day-users out of total users: %.2f' % p_one_day_users + '%')
for row in coll.find(): $     print(row)
twitter_archive_df_clean[~pd.isnull(twitter_archive_df_clean['retweeted_status_id'])]
new_page_converted.mean()-old_page_converted.mean()
print(tweet_df.groupby(["Tweet Source"])["Tweet Source"].count()) $ tweet_df.head()
len(SCN_BDAY_qthis)
len(list_to_merge)
loadRetailData = sc.textFile("OnlineRetail.csv.gz") $ for row in loadRetailData.take(5): $     print row
train_label = df_2017['label']
n_new = treatment_df.shape[0] $ print('The number of individuals in the treatment group is n_new = {}.'.format(n_new))
theft.loc[12]
df_length = df.shape[0] $ print('No. of rows in the Dataset: ' + str(df_length) ) $
calls_nocontact.neighborhood_district.value_counts()
print(len(df['badge_#'].unique()), 'individual traffic officers.')
ab_data_p_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ p_diffs = np.array(p_diffs) $ (p_diffs>ab_data_p_diff).mean()
edge_types_file = directory_name + 'edge_types.csv'   # Contains info. about every edge type $ edges_file = directory_name + 'edges.h5'             # Contains info. about every edge created
df = df.append(pd.DataFrame([{'age':20,'gender':'F','name':'qoo'}]),ignore_index=True)
treatment_ctr=treatment_df.query('converted==1').user_id.nunique()/treatment_df.query('converted==0').user_id.nunique() $
df_link_yt['video_category'].replace(video_categories).value_counts()
cfModel.summary()
df['intercept']=1
df.groupby('season')['episode_id'].nunique().hist()
calls_df.pivot_table(["length_in_sec"],["status"],aggfunc="mean").sort_values("length_in_sec",ascending=False)
import pandas as pd $ rng = pd.date_range('1/1/2018',periods=100, freq='D')  # 'D' is days $ rng
!head data/test_users.csv
model3.compile(loss=loss, $           optimizer= "adam", $           metrics=[f1])
small_ratings_data = remove_header(small_ratings_raw_data, small_ratings_raw_data_header, 3)
points[[p.endswith("a") for p in points.index]]
type(df['timestamp'][0])
new_page_converted = np.random.binomial(new_n, new_p) $ new_page_converted
df_course_after=pd.merge(df_course,df_users_6_after,left_on='uid',right_on='uid',how='inner')
details.isnull().sum()
df.head(3)
df['nd_key_formatted'] = df['nd_key'].str[:5] $ df['concept_finish'] = df['concept_completed_before_convert']/df['num_concept'] $ df.head()
liquor2016_q1 = liquor2016[liquor2016.Date.dt.quarter == 1]
learner= md.get_model(opt_fn, em_sz, nh, nl, $     dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4]) $ learner.metrics = [accuracy] $ learner.freeze_to(-1)
df_tte_all[df_tte_all['ItemDescription'] == '$0.13 per alarm-month']
df_new.groupby('country').nunique() $ df_new[['CA','UK','US']] = pd.get_dummies(df_new['country']) $ df_new['intercept'] = 1
grouped_publications_by_author['authorId'].nunique()
sub_df['time_from_deadline'] = [(x-imp_date).total_seconds() for x in sub_df['date_created']]
pandaDataFrame = pd.DataFrame({ 'Date' : pd.Timestamp('20180101'), $                                 'Retweet_Count' : np.random.randint(5, size=4), $                                 'Text' :["Hey all!","#Fake Data","Not real","text"] $                               }) $ pandaDataFrame
temp_fine = np.zeros((843, 26, 59)) $ for i in range(843): $     temp_mon = temp_us_full[i] $     interp_spline = interpolate.RectBivariateSpline(sorted(lat_us), lon_us, temp_mon) $     temp_fine[i] = interp_spline(grid_lat, grid_lon)
averagerank = rankings_USA.groupby('year')['rank'].mean() $ averagerank.plot(y='rank', x='year') $
df2_rows_removed = df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0] $ print('Now, the number of rows where the landing_page and group columns don\'t align is {}.'.format(df2_rows_removed))
append_wait_indicator()
print(df_new['adNetworkType'].value_counts()) $ df_new.loc[:,'adNetworkType_google']=np.nan $ df_new.loc[df_new['adNetworkType']=='Google Search','adNetworkType_google']=0 $ df_new.loc[df_new['adNetworkType']=='Search partners','adNetworkType_google']=1 $ print(df_new['adNetworkType_google'].value_counts())
gdf = gdf[gdf['seqid'].isin(chromosomes_list)] $ gdf.drop(['start', 'end', 'score', 'strand', 'phase', 'attributes'], axis=1, inplace=True) $ gdf.sort_values('length').iloc[::-1]
commits_per_repo = pd.read_pickle('data/pickled/commits.pkl') $ commits_per_repo.info()
tweet_lang_hist = pd.crosstab(index=tweets_df["tweet_lang"], columns="count") $ tweet_lang_hist['lang_freq'] = tweet_lang_hist['count'] * 100 / tweet_lang_hist.sum()['count'] $ tweet_lang_hist = tweet_lang_hist.sort_values('lang_freq', ascending=False) $ tweet_lang_hist.head(10)
tag_df.head()
for k in range(initial_year,final_year + 1): $     idx = df_providers[ (df_providers['year']==k)].index.tolist() $     drgs_that_year = df_providers.loc[idx,'drg3'].unique() $     drgs_that_year.sort() $     print(k,len(drgs_that_year ))
!python extract_dl1.py -h
import seaborn as sns
swinbounds = swPos[swPos.inbound == True]
n_new= df2['landing_page'].value_counts()["new_page"] $ n_new
all_p.value_counts().head(10)
dd=cfs.diff_abundance('Subject','Control','Patient')
df_upsampled = pd.concat([df_majority, df_minority_upsampled]) $ df_upsampled.irlco.value_counts()
match_results.tail(3)
journalists_mention_summary_df = journalist_mention_summary(journalists_mention_df) $ journalists_mention_summary_df.to_csv('output/journalists_mentioned_by_journalists.csv') $ journalists_mention_summary_df[journalist_mention_summary_fields].head(25)
twitter_data.info()
p_n0 = p_mean $ p_o0 = p_mean
mySql = spark.sql('SELECT dateTime, endpoint FROM AccessLog WHERE responseCode = 403') $ mySql.show() $
df_ad_airings_5.info() $
rain_df = pd.DataFrame(rain) $ rain_df.head()
df.sort_values('Shipped Created diff',ascending=False).head()
from sqlalchemy import update $ stmt = update(employee).where(employee.c.id==1).values(age=49) $ stmt1 = update(employee).where(employee.c.id==1).values(marital_status='Married') $ conn.execute(stmt) $ conn.execute(stmt1)
for index, row in df_rand.iterrows(): $     df_rand.loc[index,'is_ch_company'] = row.postcodes in row.ch_postcodes
import datetime
retweet_relation=tweets.loc[notnaindex,["user_name","user_followers","retweet_status_user_name","retweet_status_follower_count"]]
y_t = knn_model.predict(X_train)
X = pd.get_dummies(X, prefix='sub')
np.mean(new_page_converted) - np.mean(old_page_converted)
query_result1.features[0].geometry
conn_b.commit()
pd.read_table?
zf_train = zipfile.ZipFile('train.csv.zip', mode='r') $ zf_test = zipfile.ZipFile('test.csv.zip', mode='r')
ts[datetime(2018, 1, 3)]
pickledf = pd.read_pickle('cleandf')
type(announcement_dates)
t = final_df.loc[max_vals_idx] $ t.website_url[:20]
autos.rename({"odometer": "odometer_km"},axis=1,inplace = True)
DataSet = ds[ds.userTimezone.notnull()] $ len(DataSet)
result = clf_RF_WV.predict(testDataVecs) $ result_prob = clf_RF_WV.predict_proba(testDataVecs) $ print(result[0:10]) $ print(result_prob[0:10])
archive_clean.info()
so_head[criteria]
pd.Series(bnbB.age).isnull().any()
df2.drop('group',inplace=True, axis=1) $ df2.head(2)
print lr.score(X,y) $ print scores $ print np.mean(scores)
df.query("group == 'control' and landing_page != 'old_page'").index
data['online'].value_counts()
total_y = list(youTubeTitles.values[:2500,1]) + list(pornTitles.values[:2500,1])
vect = CountVectorizer(stop_words=stop_words, ngram_range=(2,4), min_df=0.0001) $ X = vect.fit_transform(fashion.text)
df_userid = pd.DataFrame({"UserID":users["UserID"]}) $ df_Tran = pd.DataFrame({"ProductID":products["ProductID"]}) $ df_userid['Key'] = 1 $ df_Tran['Key'] = 1
print(p.shape) $ p.head()
test = pd.read_csv(dataurl+'test.csv.gz', sep=',', compression='gzip')
engine = create_engine('postgresql://ellieking@localhost:5432/givedata')
import lda $ model = lda.LDA(n_topics=30, n_iter=1500, random_state=1) $ model.fit(tf)
%%time $ df.body.apply(lambda x: sleep(.0001)).head()
autos['num_photos'].value_counts()
class Player: $     id: 
from sklearn.linear_model import LogisticRegression
df.rename(columns={"PUBLISH STATES":"Publication Status","WHO region":"WHO Region"},inplace=True)
pd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])
contractor_merge = pd.merge(contractor_clean, state_lookup, $                             on=['state_id'], how='left')
df
data[['returns', 'strategy']].cumsum().apply(np.exp).plot(figsize=(10, 6))
tweet_archive_enhanced_clean['p1_dog'].value_counts()[True]/tweet_archive_enhanced_clean['p1_dog'].count()
logodds.drop_duplicates().sort_values(by=['count']).head(10) $ logodds.drop_duplicates().sort_values(by=['count']).tail(10) $
merged1['Specialty'].isnull().sum(), merged1['Specialty'].notnull().sum()
pd.DataFrame(records.loc[:, records.columns != 'ID']).hist(); $ plt.savefig('images/histograms.png')
max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(history.history['val_acc'])) $ print('Maximum accuracy at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(max_val_acc))
classifier=XGBClassifier()
twitter_archive.info()
fileURL = "https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2015-09.csv" $ taxiData = pd.read_csv(fileURL)
!tar -xvzf rossmann.tgz
page_logit = sm.Logit(df2['converted'],df2[['intercept','ab_page','UK','US']]) $ result = page_logit.fit() $ result.summary()
merged = pd.DataFrame.merge(mojog_df, youtube_df,on='movie_name', how = 'inner') $ merged.head()
os.listdir()
archive_df.rating_denominator.value_counts(sort=False)
trigrams_fd = nltk.FreqDist() $ trigram_words = [ ','.join(map(str,tg)) for tg in nltk.trigrams(wordsX) ] $ trigrams_fd.update(trigram_words) $
iris.iloc[iris.iloc[:,1].between(3.5, 3.6).values,1]
df_ind_site.groupby(['id_num', 'year','drg3'])['disc_times_pay'].agg([np.sum, np.mean, np.std, np.median, $                      np.var, np.min, np.max, percentile(50), percentile(95)])
clean_words=[x.lower() for x in clean_words] $ clean_words.sort() $ clean_words.sort(key=len) $ clean_words
df_new['CA_ind_ab_page'] = df_new['CA']*df_new['ab_page'] $ df_new['US_ind_ab_page'] = df_new['US']*df_new['ab_page'] $ df_new.head()
questions.head()
ax = tm_week_df['Incident_number'].loc[day_order].plot(kind="bar", $                                                        legend=False, $                                                    figsize=(8,5)); $ ax.set_ylabel("Number of vegetation fires") $ ax.set_xlabel("Day of the week")
clean_madrid.dropna(inplace=True) $ clean_madrid.reset_index(drop=True,inplace=True)
overallFullBath = pd.get_dummies(dfFull.FullBath)
s.str.contains(' ')
df2_conversion = df2[df2['converted'] == 1] $ probability = (len(df2_conversion)/n_unique_users) $ print ("The probability of an individual converting regardless of the page is: {:.4f}".format(probability))
con = sqlite3.connect('db.sqlite') $ con.close() $ df.head()
smooth_stanmodel = SMOOTH.compileModel() # to facilitate plotting 
print('Proportion of users converted: {}'.format(df.query('converted == 1').shape[0] / df.shape[0]))
%%time $ %%memit # not detecting os execution? $ import os $
page = pb.Page(commons_site, 'File:Parlament Europeu.jpg') $ page.exists()
r_client_previous = ft.Relationship(es['clients']['client_id'], $                                     es['loans']['client_id']) $ es = es.add_relationship(r_client_previous)
type(tweets_master_df['timestamp'][0])
actual_payments_combined.columns
print "{:,} total pings".format(filteredPingsDF.count()) # ~1.3B pings
total_treatment = df2[(df2['landing_page'] == "new_page")].count() $ print(total_treatment)
dill_file = open('model.dill', 'rb') $ model = dill.load(dill_file)
potential_accounts_buildings_info = (pd.merge(sites_on_net, sites_no_net,\ $          on=['Account ID'],\ $          how='outer').sort_values(by='# Buildings on Net', ascending=False)).fillna(0) $ potential_accounts_buildings_info['# Buildings not on Net'] = potential_accounts_buildings_info['# Buildings not on Net'].astype(int)
ti_clm = ['store','url','rpc', 'product_description', $           'review_id','review_date', 'username', 'review_text', 'review_rating', $           'variants', 'image_url'] $
(df.converted > 0).sum()
props.head()
title_sum = preproc_titles.sum(axis=0) $ title_counts_per_word = list(zip(pipe_cv.get_feature_names(), title_sum.A1)) $ sorted(title_counts_per_word, key=lambda t: t[1], reverse=True)[:] $
wrd_clean['puppo'].value_counts()[:10]
X= preprocessing.StandardScaler().fit(X).transform(X) $ X[0:5]
import tweepy $ import pandas as pd $ import matplotlib.pyplot as plt
pd.options.display.max_columns
proj_df['Project Need Statement']
results.summary2()
cityID = '30344aecffe6a491' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fremont.append(tweet) 
clinton_deck = clinton_df[clinton_df['source'] == 'TweetDeck'] $ clinton_deck.head()
np.ones([2, 3])  #There's also np.zeros, and np.empty (which results in an uninitialized array).
subject_df = latest_df[['classification_id', 'subject_ids', 'subject_data']].copy() $ subject_df['subject_data'] = subject_df.subject_data.apply(lambda x: list(json.loads(x).values())[0]['Filename'])
pitches = pd.read_json('/Users/thomasmulhern/Downloads/pitchesDataJson/pitch.json')
arrows = pd.read_csv('input/data/arrow_positions.csv', encoding='utf8', index_col=[0,1])
bnbAx.first_device_type.value_counts()
lr_y_score = model.predict_proba(X_test)[:, 1] #[:,1] is formatting the output $ lr_y_score
len([earlyScr for earlyScr in SCN_BDAY_qthis.scn_age if earlyScr < 3])/SCN_BDAY_qthis.scn_age.count()
from bs4 import BeautifulSoup $ example1 = BeautifulSoup(df.text[279], 'lxml') $ print(example1.get_text())
df2[['test', 'ab_page']] = pd.get_dummies(df2['group']) $ df2['intercept'] = 1 $ df2 = df2.drop(['test'], axis = 1) $ df2.head()
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).sort_index() 
articles.shape
shopping_carts = pd.DataFrame(items) $ shopping_carts
bitcoin_price.head()
df_unique = pd.DataFrame(unique_feature_list)
number_unpaids.plot.scatter(x='score', y='paid_status', figsize=(20,10))
paired_df[['dataset_1', 'dataset_2']] = paired_df.paired_datasets.apply(pd.Series) $ paired_df = paired_df.loc[:, ['dataset_1', 'dataset_2','co_occurence']] $ paired_df = paired_df.loc[paired_df.co_occurence > 1]
advancedmodel = LogisticRegression() $ advancedmodel = advancedmodel.fit(advancedtrain, train["rise_in_next_day"])
import numpy as np $ import pandas as pd $ import datetime $ autos = pd.read_csv("autos.csv", encoding="Latin-1")
nba_df.head(2)
old_page_converted = np.random.choice([0,1], n_old, replace=True, p=[1-p_old,p_old]) $ np.bincount(old_page_converted)
sdsw['WAGE_RATE_EST'] = sdsw[['WAGE_RATE_OF_PAY_FROM', 'WAGE_RATE_OF_PAY_TO']].max(axis=1).fillna(method='bfill') $ len(sdsw)
dfstatus = pd.get_dummies(cp311['status'])
transfer_duplicates.apply(lambda row: smoother_function_part2(row["Year"], row["Month"], row["Day"], row["Smoother"]), axis=1);
df.start_date = pd.to_datetime(df.start_date, format='%m/%d/%Y %H:%M')
max((stock - mini).abs())
hdf.loc[slice('adult', 'child'), :].head()
sakhalin_data_in_bbox.shape
X = df.drop(['body', 'closed_date', 'completed_by', 'created_date', 'title', 'comments'], axis=1) $ print(X.shape) $ preds = classifier.predict(X) $ print(preds)
df.dtypes.head(20)
data['date'] = [dateutil.parser.parse(x) for x in data['tweet_created']]
plt.hist(p_diffs); $ plt.title('Histogram of 10,000 simulations') $ plt.xlabel('p_diffs values'); $ plt.ylabel('Frequencies');
total.load_from_statepoint(sp) $ absorption.load_from_statepoint(sp) $ scattering.load_from_statepoint(sp)
transactions.head(2)
summary_hc = df_measures_users[df_measures_users['ms']==0][score_variable].describe() $ print('Average Score of HC Group: {} (SD {})'.format( $     round(summary_hc.loc['mean'],2), round(summary_hc.loc['std'],2)))  
master_file['TECTONIC SETTING'].value_counts()
for c in tqdm_notebook(cols): $     siim[c] = encod.inverse_transform(siim[c])
proj_df['Project Need Statement'].str.startswith('I need').value_counts()
len(df['user_id'].unique())
trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy') $ val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')
frame = pd.DataFrame(np.arange(12).reshape(( 4, 3)), $                   index =[['a', 'a', 'b', 'b'], [1, 2, 1, 2]], $                   columns =[['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']]) $ frame
df_train = pd.concat([df_train, df_train_val]) $ del df_train_val $ gc.collect() $ print('train shape: ' + str(df_train.shape))
store_items.fillna(method='backfill', axis=0)
new_page_converted = new_page_converted[:145274] $
max(house_data['date']), min(house_data['date'])
filename = patient+'_mvarmodel_results.hdf' $ adjmat_load = loadarray(filename) $ print adjmat_load.data.shape $ print adjmat_load.metadata.keys()
x_normalized.index = intersections_irr.index
df.head()
df.to_csv('ab_edited.csv', index=False)
S_distributedTopmodel.decision_obj.simulStart.value, S_distributedTopmodel.decision_obj.simulFinsh.value
a
grad_GPA_mean = records3[records3['Graduated'] == 'Yes']['GPA'].mean() $ grad_GPA_mean
to_be_predicted_Day4 = 36.48514248 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
offseason15["InorOff"] = "Offseason"
count = grouped_months.count() $ count=count.rename(columns = {'Totals':'count_v_T'}) $
! ls -l {path_to_zips} 
total = df['user_id'].nunique() $ total
plans_set = set() $ [plans_set.add(plan) for plans_combination in np.unique(USER_PLANS_df['scns_array']) for plan in plans_combination ] $ plans_set
pd.crosstab(aqi['AQI Category_eug'], aqi['AQI Category_cg'], margins=True)
basicmodel = LogisticRegression() $ basicmodel = basicmodel.fit(basictrain, train["rise_in_next_day"])
len(df_license_appl)
corpus = [dictionary.doc2bow(text) for text in texts] $ corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'deerwester.mm'), corpus)  # store to disk, for later use $ for c in corpus: $     print(c)
investor_fut_bucket_30360_xirr_orig.hist(bins=np.linspace(-.5,.5,110))
cols = ['id_partner', 'Order_14', 'Order_30', 'Order_60', 'Cost_14', 'Cost_30', 'Cost_60'] $ users_orders = users_orders[cols] $ users_orders.head()
import numpy as np $ slicer.apply(np.mean)
s3.reindex(np.arange(0,7), method='ffill')
obs_diff = control_pro - treatment_pro $ obs_diff
from rain import *   # tool in this repo $ df = ds.to_dataframe() $ e = Event(df)
df.describe()
df.info()
price_by_brand = {} $ for b in brands: $     price_by_brand[b] = autos.loc[autos['brand']==b, 'price'].mean() $ price_by_brand
treatment['converted'].sum() / treatment.shape[0]
df = pd.DataFrame(results, columns=['date', 'precipitation']) $ df.set_index(df['date'], inplace=True) $ df.tail()
tl_2020 = pd.read_csv('input/data/trans_2020_ls.csv', encoding='utf8', index_col=0)
graf_counts['AFFGEOID'] = graf_counts['AFFGEOID'].astype(str)
import pickle
from IPython.display import IFrame $ IFrame('readme.html', width=800, height=450)
dau = activeDF.select("cid").distinct().count()*100 $ print "DAU for {}: {:,}".format(D0.isoformat(), dau) # ~110M users
total_users_treat = df2.query('group == "treatment"')['user_id'].nunique() $ user_treat_conv = df2.query('group == "treatment" & converted == 1')['user_id'].nunique() / total_users_treat $ user_treat_conv
data2.drop('dates', axis = 1, inplace = True)
a = [] $ a.append(1) $ a.append(2) $ a
parsed_email_data  = pd.read_csv("parsed-emails.csv") $ parsed_email_data.head(10) #only display the first 10 emails
df2.drop(2893, inplace=True)
df["Clus_km"] = labels $ df.head(5)
decades = [1960, 1970, 1980, 1990, 2000, 2010, 2020] $ decade_names = ['1960', '1970', '1980', '1990', '2000', '2010'] $ df_western['release_decade'] = pd.cut(df['release_year'], decades, labels=decade_names) $ df_western.head() $
diff=df.index[7]-df.index[2] $ diff.seconds
for ind, sex in gender.iteritems(): $     if type(sex)==list: $         print ind,sex
wrangled_issues_df
df2.query('converted==1')['user_id'].count()/df2['user_id'].count()
num_features = 300    # Word vector dimensionality                      $ min_word_count = 40   # Minimum word count                        $ num_workers = 4       # Number of threads to run in parallel $ context = 10          # Context window size                                                                                    $ downsampling = 1e-3   # Downsample setting for frequent words $
rhum_fine = np.zeros((844, 26, 59)) $ for i in range(844): $     rhum_mon = rhum_us_full[i] $     interp_spline = interpolate.RectBivariateSpline(sorted(lat_us), lon_us, rhum_mon) $     rhum_fine[i] = interp_spline(grid_lat, grid_lon)
ride_demand.head()
df[['product_type','price_doc']].groupby('product_type').median()
%matplotlib inline $ import pandas as pd $ train_data = pd.read_csv('../data/train.csv', parse_dates=['DateTime']) $ train_data.head()
df['org_facebook'].nunique()
df_byzone.tail(2)
dummy_var_df = pd.DataFrame(dummy_var_df)
tod=today_str()
dfNiwot.head()
plt.hist(p_diffs, edgecolor='gray'); $ plt.axvline(x=obs_diff, color='k',linestyle='dashed');
import matplotlib.pyplot as plt $ %matplotlib inline
nlp = spacy.load('en') $ op_ed_articles['full_text_tokenized'] = op_ed_articles['full_text'].apply(lambda x: nlp(x)) $
signals['short_mavg'] = aapl['Close'].rolling(window=short_window, min_periods=1, center=False).mean() $ signals['long_mavg'] = aapl['Close'].rolling(window=long_window, min_periods=1, center=False).mean() $ signals['signal'][short_window:] = np.where(signals['short_mavg'][short_window:] > signals['long_mavg'][short_window:], 1.0, 0.0)   $ signals['positions'] = signals['signal'].diff()
season07["Category"] = "2007 Season" # Again, this assigns aseason identifier to the transactions.
spelling_dictionary[:10]
alg4 =AdaBoostClassifier() $ alg4.fit(X_train, y_train) $ probs = alg4.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
random_numbers['2015-05-01':'2015-08-31'].idxmax()# for second 4 months May to August
dfRegMet2016.shape[0] + dfRegMet2016.shape[0] +dfRegMet2016.shape[0] +
for league in root_level_leagues: $     print_league(league['id'], league['name'], depth=0)
len(df_license_opening)
active_with_type = gbq.read_gbq(query3+query4, project_id=project_id,dialect = "standard")
cols_to_show = ['Indicator_id', 'Country', 'Year', 'WHO Region', 'Publication Status']  # picked the columns based on the output format shown in assignment sheet $ df[cols_to_show].sort_values(['Year','Indicator_id','Country','WHO Region' ], ascending=[True, True, True, False]).head() $
import psycopg2
cityID = 'e41805d7248dbf1e' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Modesto.append(tweet) 
with open('data/chefkoch_11.json') as data_file:    $     chef11 = json.load(data_file) $ clean_new(chef11) $ chef11df = convert(chef11) $ chef11df.info()
print(corpus_Tesla[2])
data.loc['NL']
df = pd.DataFrame({'A': rng.rand(5), $                   'B': rng.rand(5)}) $ df
pp = pc_cz_fl.index.values[0] $ pc_cz_fl.ix[pp]['CUISINE_PROB']
df_train = df_train.drop(high_null_columns, axis=1) $ df_test = df_test.drop([ c for c in high_null_columns if c in df_test.columns], axis=1)
details.head(5)
%load_ext watermark $ %watermark -a 'Gopala KR' -u -d -v -p watermark,numpy,pandas,matplotlib,nltk,sklearn,tensorflow,theano,mxnet,chainer,seaborn,keras,tflearn,bokeh,gensim
data = quandl.get_table('WIKI/PRICES', ticker = tickers, $                         qopts = { 'columns': ['ticker', 'date', 'adj_close'] }, $                         date = { 'gte': '2011-11-31', 'lte': '2017-12-31' }, $                         paginate=True)
df5[['CA', 'US']] = pd.get_dummies(df5['country'])[['CA', 'US']] $ df5.head()
os.getcwd() $ model_path = 'task1_pm_model.h5'
print("Unique users:", len(df2.user_id.unique())) $ print("Non-unique users:", len(df2)-len(df2.user_id.unique()))
loans_df.home_ownership.value_counts()
clean_liverp.to_csv('clean_liv.csv')
onlyfiles = [f for f in os.listdir(datapath3) if os.path.isfile(datapath3 / f) and f.endswith('.png')] $ onlyfiles.sort() $ print('Files in the folder:') $ for i, w in enumerate(onlyfiles): $     print(i+1, '--' ,w)
base_date = dt.datetime.strptime("2017-08-23", "%Y-%m-%d") $ YrFrombd = base_date - dt.timedelta(days=365) $ print(YrFrombd)
grid_search.best_score_
theft.iloc[4]
to_plot = customer_emails[['Email', 'Days Between Int']] $ plt.title("Actual number of days between purchases") $ sb.distplot(to_plot['Days Between Int'].dropna()) $
submission_full['proba'].mean()
df_2008['bank_name'] = df_2008.bank_name.str.split(",").str[0] $
print(nba_df.columns.values.tolist())
pair.tail()
cc.close_ratio.describe()
df_archive['source'].value_counts()
df2 = df2.join(df_countries.set_index('user_id'), on='user_id') $ df2.head()
def load_csv(csv): $     p=os.path.join("data/", csv) $     print (p) $     data=pd.read_csv(p, encoding = "ISO-8859-1", engine='python') $     return data 
featuresToPreprocessByMean = ['Store', 'CompetitionDistance', $        'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', $        'Promo2SinceWeek', 'Promo2SinceYear']
cpq_business = pd.merge(cpq, buildings, how='inner')
df.loc[:,"message"] = df.message.str.replace('\d+', '') #Remove numbers.
r.summary2()
df.hist(bins=100, figsize=(11,8),color='black') $ plt.show()
pres_df.shape
top_5_percent.sum() / autos_pr['brand'].value_counts().sum()
ndarray_rt = bsonnumpy.sequence_to_ndarray( $     (doc.raw for doc in collection.find({"retweeted_status": {"$exists": True}})), $     dtype_rt, $     collection.count(), $ )
to_be_predicted_Day2 = 38.20895047 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
results_simpleResistance, out_file1 = S.execute(run_suffix="simpleResistance_hs", run_option = 'local')
from IPython.display import display_html
n_new = df2.query('landing_page == "new_page"')['user_id'].nunique() $ n_new
sfs1.subsets_
uber_15.head()
clean_tweet(df['Tweets'][0])
(visit_num.reset_index(level=[0, 1], drop=True) $  .to_frame() $  .rename( $     columns={'inspection_date': 'num_fails'} $ ))
import pandas_datareader.tsp as tsp $ tspreader = tsp.TSPReader(start='2015-10-1', end='2015-12-31') $ tspreader.read().head()
tdf = sns.load_dataset('tips') $ tdf.sample(5)
import datetime as dt
pgh_311_data['REQUEST_TYPE'].value_counts(ascending=True).plot.barh()
outlier_detection.strong_cutoff
contractor[contractor.duplicated() == True]
sns.barplot(words, importance)
for feed in feeds: $     print("feed {}".format(feed)) $     agency = json.loads(requests.get('{}index/agencies/{}'.format(base_url, feed)).text) $     print(agency) $     print('') $
n_new = df2[ df2['landing_page'] == 'new_page' ]['user_id'].count() $ n_new
%timeit ' '.join(L)
print(train['first_device_type'].unique())
customer_purchases_1to3 = df_proc1["Counter"].isin(["1","2","3"]) $ df_r = df_proc1.loc[customer_purchases_1to3,:].copy()
len(sample)
recommend = survey[survey.question.str.contains('On a scale of one to ten, how likely are you to recommend this to your friends?')].copy()
df[df.sentiment == 4].head(10)
df_archive["puppo"].value_counts()
df_merge = campaign_data.merge(train, on = 'campaign_id') $ campaigns = df_merge.groupby('campaign_id')['is_open','is_click'].agg(np.mean) $ campaigns['click_given_open'] = campaigns['is_click'] / campaigns['is_open'] $ campaigns['is_open'].sort_values(ascending = False)[0:5]
run txt2pdf.py -o "VIA CHRISTI HOSPITALS WICHITA, INC  Sepsis.pdf"   "VIA CHRISTI HOSPITALS WICHITA, INC  Sepsis.txt"
dfs[index_max]['Time'][dfs[index_max]['Outside Temperature'] == dfs[index_max]['Outside Temperature'].max()].values[0]
ac['Monitoring Start'].groupby([ac['Monitoring Start'].dt.year]).agg('count')
df = pd.read_csv('../Data/bidsmerged_update__2_.csv') $ df.shape $
ax = ign.release_month.value_counts().sort_index().plot(kind = 'bar') $ tick = plt.xticks(range(0,13),['January','February','March','April','May','June', \ $                                'July','August','September','October','November','December']) $ ax.set_ylabel('Games released')
df2.user_id.nunique()
femalenew = femalebyphase.rename(columns={"Offense":"Female"}) $ femalenew.head(3)
clean_df.to_csv('clean_tweet.csv',encoding='utf-8')
vals = bacteria_data.value.copy() $ vals[5] = 1000 $ bacteria_data
def calc_daily_returns(closes): $     return np.log(closes/closes.shift(1))[1:]
grouped_dpt.aggregate(np.mean) # means based on one group
def unique(df): $     return df.id.nunique() $ print(unique(ltc)) $ print(unique(eth)) $ print(unique(xrp))
df2.groupby('OriginCityName')['Origin'].unique().tail(15) # number of airports code per City
target_docs[:10]
sents = [] $ for i in range(30): $     sents.append(' '.join([k for val in train_x[i] for k,v in words.items() if v==val])) $ sents
brand_model_database = pd.DataFrame(models_series) $ brand_model_database_real = pd.DataFrame(brand_model_database[0].str.split(" ", expand = True)) $ brand_model_database_real.rename(columns={0: "model" , 1: "brand"}, inplace = True) $ brand_model_database_real
archive_clean.text.str.extractall( r"(\d+\.?\d*\/\d+\.?\d*\D+\d+\.?\d*\/\d+\.?\d*)") $
y = np.asarray(churn_df['churn']) $ y [0:5]
df2.user_id.nunique(), countries_df.user_id.nunique()
evaluator = keras_entity_recognizer.evaluate(predictions)
dates = pd.date_range('2010-01-01', '2010-12-31') $ df = get_data(symbols=[],dates=dates)
annotations_df = pd.merge(raw_annotations_df.drop('answers', axis=1), answers_df, left_index=True, right_index=True) $ annotations_df.tail(5)
data.set_index(['Record ID'], inplace=True)
test.show(5)
goo.head()
import pandas as pd $ from IPython.display import display $ df = pd.read_sql_query("select * from track limit 10;", conn) $ df $
import os $ import sagemaker $ from sagemaker import get_execution_role $ sagemaker_session = sagemaker.Session() $ role = get_execution_role()
res_c_i = res_c.merge(interval_c.to_frame(), left_index=True, right_index=True)
for treaty in treaties.find({"reinsurer": "AIG"}): $     pprint.pprint(treaty)
extract_all.loc[(extract_all.APPLICATION_DATE_short>=datetime.date(2018,5,1)) $                &(extract_all.APP_PRODUCT_TYPE.isin(['TL','RL'])) $                &(extract_all.DEC_FINAL_DECISION.isin(['A','J','D']))].sort_values('app_branch_state').app_branch_state.unique()
df_train.corr()
df_kyt.head()
1/np.exp(-0.0099), np.exp(-0.0506)
y[20:30] #I changed the range of the index so I could confirm that 
data.info()
tweets_df.head()
df3[['CA','UK','US']] = pd.get_dummies(df3['country']) $ logit_model = sm.Logit(df3['converted'], df3[['intercept','ab_page','CA','UK']]) $ result = logit_model.fit() $ result.summary()
payment = dblight.apply(grab_payment,axis=1) $ dblight = dblight[["ViolationStreetNumber","ViolationStreetName"]].merge(payment,left_index=True,right_index=True) $ dblight["blight_inc"] = 1
job_requirements.size
X = reddit_master['title'] $ y = reddit_master['Class_comments'].apply(lambda x: 1 if x == 'High' else 0)
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA', 'US']] $ df_new.head() $
knn.score(x_test,y_test > 0)
sox['hour'] = sox.START_TIME.str.slice(0,2).astype(np.int) $ sox['minute'] = sox.START_TIME.str.slice(3,5).astype(np.int) $ sox.ix[sox.START_TIME.str.contains(' PM'),'hour'] += 12 $ sox.minute = (sox.minute / 15).astype(np.int) * 15 $ sox['time'] = sox['hour'].astype(np.str).str.pad(2, fillchar='0') + ':' + sox['minute'].astype(np.str).str.pad(2, fillchar='0') + ':00'
guido_text = soup.text $ print(guido_text[:500])
rankings_USA.query("rank >= 30")['rank_date'].count() $
reason_for_visit = pd.read_csv('./data/MeetingReasonForVisits.csv')
alice_sel_shopping_cart = pd.DataFrame(items, index = ['glasses', 'bike'], columns = ['Alice']) $ alice_sel_shopping_cart
tweets['sentiment_score'] = tweets['sentiment'] $ (tweets['sentiment_score'] == 0).value_counts()
pivoted = np.round(FTSE_key_events_df*100, decimals=2) # Rounds down to two decimals, and multiplies with 100 for presentation purposes only $ pivoted $
traintrain_pd= traintrain.pivot_table(values = 'Visits', index = 'Page', columns = 'date') $ trainvalidation_pd= trainvalidation.pivot_table(values = 'Visits', index = 'Page', columns = 'date')
df.month[:5]
prediction['p1'][:10]
from itertools import islice
y_test_under[fm_bet_under].mean()
from pyspark.sql.functions import udf $ from pyspark.sql.types import DoubleType, StructType $ def get_pred(probability): $     return(round(probability.toArray().tolist()[1],6)) $ get_pred_udf = udf(get_pred, DoubleType())
shows[['release_date', 'first_year']]
plt.hist(p_diffs, bins=20) $ plt.title('Simulated p_diffs of Converted Success Rate') $ plt.xlabel('p_diff') $ plt.ylabel('Counts');
impressions.info()
conn.rollback()
pred = clf.predict(x_new) $ print(metrics.accuracy_score(y_new, pred)) $ feat_imp = np.stack([new_feature_cols, clf.feature_importances_], axis=1) $ print(np.sort(feat_imp, axis=0)[::-1])
new_page_converted = np.random.choice(2,size=n_new,p=[1-p_new,p_new])
last_close_price = USEquityPricing.close.latest $ top_close_price_filter = last_close_price.top(200)
df_archive = pd.read_csv("twitter-archive-enhanced.csv",sep=",")
S.decision_obj.stomResist.value = 'BallBerry' $ S.decision_obj.stomResist.value
toy.shape $ toy.head()
import matplotlib.pyplot as plt $ from sklearn.metrics import roc_curve, auc $ actual =perf_test[['Default']] $ predi = predictions
submit = test[['id', 'price_usd']]
retweet_relation.to_csv("../output/retweet_relation.csv",encoding="utf-8")
move_34p14u14u = (breakfastlunchdinner.iloc[3, 1] $                + breakfastlunchdinner.iloc[5, 2] $                + breakfastlunchdinner.iloc[5, 3]) * 0.002 $ move_34p14u14u
datetime_df.dtypes
auth = tweepy.OAuthHandler(consumer_token, consumer_secret) $ auth.set_access_token(access_key, access_secret)
session_v2.shape
from scipy import stats
df.mean()
w.get_filtered_data(step = 2, subset = subset_uuid, indicator = 'din_winter', water_body = 'SE654470-222700').MONTH.unique()#[['MONTH', 'SEA_AREA_NAME', 'SEA_AREA_CODE']]
df2['intercept'] = 1
import requests $ import json $ url = "https://www.dcard.tw/_api/posts?popular=false" $ resp = requests.get(url, headers=headers)
type(categDF_encoded)
df_raw = df_raw.loc[df_raw.artist.notnull(),:]
df2=df.select("name", "favorite_color") $ df2.show()
gas_df['MONTH'] = pd.to_datetime(gas_df['MONTH']+'-'+gas_df['YEAR'],format='%b-%Y') $ del gas_df['YEAR'] $ gas_df.head()
Ytr_m = np.mean(Ytr[:,:7],axis=0) $ Ytr_std = np.std(Ytr[:,:7],axis=0) $ Ytr_scale = (Ytr[:,:7]-Ytr_m)/Ytr_std $ Yts_scale = (Yts[:,:7]-Ytr_m)/Ytr_std
data
dates = pd.date_range('2010-01-01', '2010-12-31') $ df = get_data(symbols=[],dates=dates) $ df.head()
invoice = invoice.str.replace("$","").astype(float) $ invoice.sum()
sqlc = sqlite3.connect('iris.db')
test_data.isnull().sum()
fullDF = pd.concat([tweets_streamedDF, tweetsDF], ignore_index = True)
data['cat'].shape
stock.target_class.value_counts()
gene_df['gene_name'].unique().shape
tfv = TfidfVectorizer(ngram_range=(1,3), max_features=2500) $ X = tfv.fit_transform(clean_text)
(ab_diff < p_diffs).mean()
n_top_words = 10 $ for i, topic_dist in enumerate(topic_word): $     topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words):-1] $     print('Topic {}: {}'.format(i, ' '.join(topic_words)))
output = lmp.run(lammps_exe, 'nvt.in', return_style='object')
T = 6 $ HORIZON = 1
csvData['street'] = csvData['street'].str.replace(' Southeast', ' SE') $ csvData['street'] = csvData['street'].str.replace(' South', ' S')
yc_new1 = yc_new.merge(zipincome, left_on='zip_depart', right_on='ZIPCODE', how='inner') $ yc_new1.head()
bigdf_read = pd.read_csv('Combined_Comments-Fixed2.csv', index_col=0)
ben_final['diffs'] = ben_final['diffs'].fillna(0)
tweets_df[tweets_df.possibly_sensitive == True]
y = df['comments'] $ X = df['title'] $ X = X.apply(lambda x: PorterStemmer().stem(x))
print(ols_predictions)
from fuzzywuzzy import fuzz
%store -r extract_deduped_0424 $ extract_deduped = extract_deduped_0424.copy()
df_grp = df2.groupby('group') $ df_grp.describe()
top_supporters
s = 'The food is good and the atmosphere is nice' $ s
len(sel_df[sel_df.BlockRange.isnull()])
df_users_6.head()
S = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure08/summa_fileManager_riparianAspenPerturbRoots.txt')
last_tweet = browser.find_by_id('stream-items-id')[0]
t0, t1 = lin_reg.intercept_[0], lin_reg.coef_[0][0] $ t0, t1
fps.pyldavis_fp
print(data['dataset_data']['frequency'])
Q1 = numpy.array(h.logdict['Q1'][startcut:]) $ T1 = numpy.array(h.logdict['T1'][startcut:])
testObjDocs.outDF[975:1000]  ## spot check run on batches of 25 records to find the bad ones (b/2 900 and 1000) $
exiftool -csv -createdate -modifydate cisnwf6/Cisnwf6_cycle3.MP4 > cisnwf6.csv
print("Number of Mitigations in Enterprise ATT&CK") $ mitigations = lift.get_all_enterprise_mitigations() $ print(len(mitigations)) $ df = json_normalize(mitigations) $ df.reindex(['matrix', 'mitigation', 'mitigation_description', 'url'], axis=1)[0:5]
liberia = concatenated.loc[concatenated['country'] == 'liberia'] $ liberia =liberia.fillna(0) $ dropped_liberia =liberia[['Date','National','country']] $ dropped_liberia['Date'] = pd.to_datetime(dropped_liberia['Date']) $ dropped_liberia.head() $
df2 = df2.drop_duplicates(subset='user_id')
df['datetime'] = df['datetime'].map(convert_a)
df_pivot = pd.pivot_table(df_convert, values='concept_finish', index=['user_id','nd_key_formatted'],columns=['course_key'], aggfunc=np.sum).fillna(0).reset_index()
tweets = extractor.user_timeline(screen_name="DARRENHARDY", count=200) $ print("number of tweets extracted: {}.\n".format(len(tweets)))
to_drop = ['in_reply_to_status_id', 'in_reply_to_user_id', $            'retweeted_status_id', 'retweeted_status_user_id', $            'retweeted_status_timestamp'] $ df_clean = df_clean.drop(to_drop, axis=1)
(p_diffs > obs_diff).mean()
image_predictions_clean = image_predictions_df.copy()
df.groupby('domain').count().sort_values('id',ascending=False).head(25)
result.isnull().sum(axis=0)
session.query(Measurement.date).order_by(Measurement.date.desc()).first() $
LSVC = SklearnClassifier(LinearSVC()) $ LSVC.train(train_set)
e_p_b_one.rename(columns = {0:"Count"},inplace=True)
df8_lunch.mean() # mean is a bit higher than the smaller df6 data set
df_test = df[:10]
import glob $ import pandas as pd $ pattern = 'uber?.csv' $ csv_files = glob.glob(pattern) $ print(csv_files)
df
df_new['new_page_UK']=df_new['new_page']*df_new['UK'] $ df_new['new_page_US']=df_new['new_page']*df_new['US']
obs_diff = p_treatment_obs - p_control_obs $ obs_diff
for c in df_test_supplement.columns: $     print(c, df_test_supplement[c].dtype)
pd.read_pickle('data/city-util/proc/misc_info.pkl', compression='bz2')
with open('tweet_json.txt', 'w') as outfile:  $     json.dump(df3_list, outfile)
coordinates $
p_old = df2.query("converted==1").user_id.nunique()/df2.user_id.nunique() $ p_old $
data_scrapped['tags'] = tags $ data_scrapped['para'] = para $ data_scrapped['bullets'] = bullets $ data_scrapped['images'] = images $ data_scrapped['links'] = links
print(re.match('AA', 'AAbc')) $ print(re.match('AA', 'bcAA'))
for n in fromNodes: $     dot.node(n)
train_data, test_data, train_labels, test_labels = train_test_split(spmat, y_data, test_size=0.10, random_state=42)  
all_df.to_pickle('data/all_political_ads.pickle') $
print item_sat.shape $ print v_item_sat.shape
ben_fin['stiki_percent'].value_counts()
dirname = "/home/fdutka/Dropbox/ProjektyNaukowe/BTime/simulations/porous/btime3"
tablename='news' $ pd.read_csv(read_inserted_table(dumpfile, tablename),delimiter=",",error_bad_lines=False).head(10)
df.dtypes[0]
df = data['Open'] $ print(df['AAPL'])
cpi_sdmx.names['Dimension']
xgboost = pickle.load(open('xgboostmodel.sav', 'rb')) $ pipeline2 = pickle.load(open('logregmodel.sav', 'rb'))
dataframe['2013-01-01':'2013-01-31']
API.rate_limit.get_limit("/statuses/show/:id")
new_w = np.zeros((vs, em_sz), dtype=np.float32) $ for i,w in enumerate(itos): $     r = stoi2[w] $     new_w[i] = enc_wgts[r] if r>=0 else row_m
convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = len(df2.query("group == 'control'")) $ n_new = len(df2.query("group == 'treatment'"))
X_train = train_df.drop(['target', 'true_grow'], 1) $ y_train = train_df.true_grow
f_channel_hour_clicks = spark.read.csv(os.path.join(mungepath, "f_channel_hour_clicks"), header=True) $ print('Found %d observations.' %f_channel_hour_clicks.count())
eda_helper(finaldf)
backup = clean_rates.copy() $ names = clean_rates.text.str.extract(r'(?P<intro>is|Meet|to|named) (?P<name>[A-Z]\w+)(\.| )', expand=True) $ names.loc[names.name.isnull(),'name'] = 'None' $ clean_rates.loc[:, 'name'] = names.loc[:, 'name'].copy()
client = WatsonMachineLearningAPIClient(wml_credentials)
sum(df2.converted)/len(df2.converted)
reddit['Morning Hours'] = reddit['Hours'].apply(lambda x: 1 if x<9 and x>4 else 0)
list(tweet_archive_clean.columns.values)
archive_df.rating_numerator.value_counts(sort=False)
value = 365243
grid = GridSearchCV(pipeline, cv=3, param_grid=param_grid)
best_values
weekdays_avg=pd.concat([weekdays_avg_2012,weekdays_avg_2013,weekdays_avg_2014,weekdays_avg_2015,weekdays_avg_2016], axis=1) $ weekdays_count=pd.concat([weekdays_count_2012,weekdays_count_2013,weekdays_count_2014,weekdays_count_2015,weekdays_count_2016], axis=1) $ weekends_avg=pd.concat([weekends_avg_2012,weekends_avg_2013,weekends_avg_2014,weekends_avg_2015,weekends_avg_2016], axis=1) $ weekends_count=pd.concat([weekends_count_2012,weekends_count_2013,weekends_count_2014,weekends_count_2015,weekends_count_2016], axis=1) $
df_nd101_d = df_pivot_days[df_pivot_days['nd_key_formatted'] == 'nd101'] $ df_nd101_d.head()
data2 = pd.read_csv("Final_Merged_Output2.csv")
!curl -XGET 'http://elasticsearch:9200/_cat/indices?v'
loans_act_20150430_repaid_xirr=cashflows_act_investor_20150430_repaid.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
nold = df2.query('landing_page == "old_page"').count()[0] $ print ("The population of Oldpage is : {}".format(nold))
diffs=np.array(p_diffs) $ null_values=np.random.normal(0,diffs.std(),diffs.size) $ plt.hist(null_values) #Simulation from the null $ plt.axvline(ControlConverted-TreatmentConverted,color="red") $
to_be_predicted_Day2 = 34.59184648 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
ccc = ['DK', 'DE', 'NO', 'SE']
returns = price.pct_change()
y_final = trainDF['ARR_DELAY'].values
from sklearn.svm import SVC
type(test5result)
climate_df.plot(kind ="line", figsize=(8,6)) $ plt.yticks(np.arange(0, 4.5, step=0.5)) $ plt.legend(loc="upper left") $ plt.tight_layout() $ plt.savefig("hawaii_Date_Precipitation.png",bbox_inches="tight")
dup_df = df2[df2['user_id'].duplicated(keep = False)] $ a = dup_df.groupby('user_id').apply(lambda x: list(x.index)) $ a
sentiment_df.plot(x="Tweets Ago", y='Compound',title='News Mood Analysis', kind='scatter') $ plt.show()
df['converted'].mean()
houses_test.SalePrice.head()
temp_table = pd.DataFrame(np.column_stack([dates]), columns=["Min-Temp","Avg-Temp","Max-Temp"]) $ temp_table
data.columns = ['West', 'East'] $ data['Total'] = data.eval('West + East')
df2[df2.user_id.duplicated(keep = "first")]
output = output.sample(False, 0.1, seed=0)
groupby_imputation.head()
df_first = transactions.groupby(["UserID"]).first() $ df_first['UserID'] = df_first.index # Assign a column that is equal to the index created from the Groupby $ df_first
names = ['Max TemperatureF', 'Min TemperatureF', 'Max Humidity', 'Max Wind SpeedMPH', 'PrecipitationIn', 'weekday', 'holiday', 'day_hrs', $          'snowtotal', 'Fog', 'year', 'sunsethour', 'CloudCover', 'bike_events'] $
data_spd = pd.DataFrame() $ data_spd['tweets'] = np.array(tweet_spd) $ data_spd.head(n=3)
%%HTML $ <blockquote class="twitter-tweet" data-lang="en"><p lang="und" dir="ltr">Hey <a href="https://twitter.com/HillaryClinton?ref_src=twsrc%5Etfw">@HillaryClinton</a> <a href="https://twitter.com/hashtag/ReleaseTheMemo?src=hash&amp;ref_src=twsrc%5Etfw">#ReleaseTheMemo</a> <a href="https://twitter.com/hashtag/pleasedontsuicideanymore?src=hash&amp;ref_src=twsrc%5Etfw">#pleasedontsuicideanymore</a><a href="https://twitter.com/hashtag/Obamagate?src=hash&amp;ref_src=twsrc%5Etfw">#Obamagate</a> <a href="https://twitter.com/hashtag/maga?src=hash&amp;ref_src=twsrc%5Etfw">#maga</a><a href="https://twitter.com/hashtag/SchumerShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#SchumerShutdown</a> <a href="https://twitter.com/hashtag/LockHerUp?src=hash&amp;ref_src=twsrc%5Etfw">#LockHerUp</a> <a href="https://twitter.com/hashtag/LockThemAllUp?src=hash&amp;ref_src=twsrc%5Etfw">#LockThemAllUp</a> <a href="https://twitter.com/hashtag/Gitmo?src=hash&amp;ref_src=twsrc%5Etfw">#Gitmo</a></p>&mdash; Q.UIETMUMBLES (@quietmumbles) <a href="https://twitter.com/quietmumbles/status/954503731852267521?ref_src=twsrc%5Etfw">January 19, 2018</a></blockquote> $ <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
rational_da['Sentiment'] = np.array(labels)
import pandas as pd $ import numpy as np $ autos = pd.read_csv("autos.csv", encoding='Latin-1')
import sys $ import os $ log_file_path = "/Users/lisurprise/zhichao/bin/god/fudan/spark-tutorial-f/tutorial/access_log_Aug95"
df['statement_type'].unique()
mean_mileage_by_brand = {} $ for b in sel_brand: $     mean_mileage = autos.loc[autos["brand"] == b, "odometer_km"].mean() $     mean_mileage_by_brand[b] = int(mean_mileage) $ mean_mileage_by_brand
pd.date_range('2015-07-03', '2015-07-10')
temp_df = temp_df.reset_index()[['titles', 'timestamp']]
fb_day_time.head()
df.group.value_counts()
data.date.values
query = "SELECT DATE(CAST(year AS INT64), CAST(mo AS INT64), CAST(da AS INT64)) as created_date, temp, wdsp, mxpsd, gust, max, min, prcp, sndp, snow_ice_pellets FROM `bigquery-public-data.noaa_gsod.gsod20*` WHERE _TABLE_SUFFIX BETWEEN '10' AND '17' AND stn = '725053' AND wban = '94728'" $ weather = read_gbq(query=query, project_id='opendataproject-180502', dialect='standard')
X_train.set_index('id', inplace=True)
sns.factorplot('name',kind='count',size = 10,aspect = 2.8, data=wcPerf1_df) $
df_compare=pd.read_csv('Unique Providers December 31, 2017.csv') $
Measurement = Base.classes.measurement $ Station = Base.classes.station $ session = Session(engine)
merged.info()
HAMD = pd.read_table("hrsd01.txt", skiprows = [1], na_values= "-9")
hr["chart delta"] = \ $ hr.apply(lambda x: (x["charttime"] - $                         x["realtime"]).total_seconds(), axis=1) $ hr.head()
pd.concat([a, b])
atdist_info_opp_dist_tabledata = atdist_opp_dist_info_count_prop_byloc.reset_index() $ create_study_table(atdist_info_opp_dist_tabledata, 'locationType', 'emaResponse', $                    location_remapping, atdist_info_response_list)
%%time $ degree_centrality = convert_dictionary_to_sorted_list(nx.degree_centrality(network_friends))
data.head()
code = ga.get_code()
df = df.groupby('msno').apply(make_order_number)
df.columns
conn.caslibinfo(caslib='research')
df.ID.dtype
barcelona[(barcelona['Apparent Mean Temp C'] >= 22) & (barcelona['Apparent Max Temp C'] <= 35)]['GMT']
cols = [topicfeat, topicfeat+docfeat, topicfeat+usrfeat, topicfeat+docfeat+usrfeat]
rng[2:4]
s1.values
md.trn_ds[0].text[:12]
ad_group_performance[ $     ['CampaignName', 'AdGroupName', 'Impressions'] $ ]
df_lineup = df.query("(group == 'control' and landing_page == 'new_page') or (group == 'treatment' and landing_page == 'old_page')") $ print("{} times the new_page and treatment don't line up.".format((df_lineup.shape[0]) )) $
twitter_archive_df_clean = twitter_archive_df_clean[pd.isnull(twitter_archive_df_clean['retweeted_status_id'])]
init_sigma = 0.01 $ W_init = tf.random_normal( $     shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims), $     stddev=init_sigma, dtype=tf.float32, name="W_init") $ W = tf.Variable(W_init, name="W")
temps_df['Difference'] = temp_diffs $ temps_df
get_real_types(tweets)
df3 = df2 $ df3['intercept'] = 1 $ df3['ab_page'] = pd.get_dummies(df['group'])['treatment'] $ df3.head()
station_distance.shape $ station_distance.columns $ station_distance.tail()
data.shape
dfRegMet2016.info()
open_users
dr_new.columns $
bg_df = pd.DataFrame(bg_s) # create dataFrame $ bg_df # a nice name to remind you this one is a DataFrame
temps1['2016-04-04']
lead_classes.fillna(0,inplace = True) $ lead_classes[lead_classes == 't'] = 1
numbers = {"two": 2, "three": 3, "four": 4, "five": 5, "six": 6, "eight": 8, "twelve": 12}
ks_goals = ks_projects.groupby(["goal", "state"]).size().reset_index(name='counts') $ ks_goal_success = ks_goals.drop(ks_goals.index[ks_goals.state != 'successful']) $ ks_goal_success = ks_goal_success.drop(ks_goal_success.index[ks_goal_success.counts < 300]) $ ks_goal_success = ks_goal_success.sort_values(by = ['goal'], ascending = True) $ ks_goal_success.set_index('goal', inplace=True)
pnls = df_res.map( lambda r: {'date': r.date, $                           'neutral': r.neutral, $                           'var': float(var(r.scenarios.array, neutral_scenario=r.neutral))}).toDF().toPandas()
validation.analysis(observation_data, simple_resistance_simulation_1)
aapl = pd.io.data.Options('AAPL', 'yahoo') $ data = aapl.get_all_data() $ data.iloc[0:6, 0:4]
temperature_sensors_df['state'].hist(bins=10);  # Plot histogram of all temperature sensors $ plt.xlabel("Temperature $^\circ$C");
csvData[csvData['street'].str.match('.*North.*')]['street']
train_data, test_data, train_labels, test_labels = train_test_split(spmat, labels, test_size=0.10, random_state=42)  
final_.head()
type(inception_date)
df2[df2.user_id==773192]
dates = [pd.datetime(2012, 5, 1), pd.datetime(2012, 5, 2), pd.datetime(2012, 5, 3)]
analyzer.polarity_scores("The book was good.")
model = tf.global_variables_initializer()  # model is used by convention
twitter_url = "https://twitter.com/marswxreport?lang=en" $ browser.visit(twitter_url)
! unzip -p {path_to_zips}On_Time_On_Time_Performance_2015_1.zip | head -n 2
ideas.isnull().sum() # Checking for missing data
df2[['CA','UK','US']] = pd.get_dummies(df2['country']) $ df2 = df2.drop(['US'],axis=1) $ df2.head()
join_d.count()
train.head()
graffiti = graffiti.replace(np.nan, 0)
p_diffs=[] $ for i in range(10000): $     new_page_converted=np.random.choice([1,0],size=nnew,p=[pnew,1-pnew]) $     old_page_converted=np.random.choice([1,0],size=nold,p=[pold,1-pold]) $     p_diffs.append(new_page_converted.mean()-old_page_converted.mean()) $
TrainData_ForLogistic = dum.drop(['City_Code', 'Employer_Code', 'Customer_Existing_Primary_Bank_Code', 'Source', $                                  'DOB', 'Lead_Creation_Date'], axis=1)
processed_tweets['processed_tweet'] = processed_tweets.tweetText.apply(processAll)
bus = cX_test.loc[p_flag & t_flag,['name', 'text', 'city', 'prob']].copy() $ bus = bus.sort_values(by='prob', axis=0, ascending=False) $ bus.head()
file4=file3.join(loc, file3.pole_id == loc.pole, "left_outer").drop(loc.pole) $ file4.show(3)
months= austin['yr_mo'].value_counts().sort_index()
gid.get_repo_version?
tips.groupby(["sex", "day"]).mean().reset_index()
cv = CountVectorizer(stop_words=stop_words) $ dtm = cv.fit_transform(no_urls_all_engl) $ tt = TfidfTransformer(norm='l1',use_idf=False) $ dtm_tf = tt.fit_transform(dtm)
preg.columns
country = pd.get_dummies(df2['country']) $ df2 = df2.join(country) $ df2.head()
major_cities_layers = major_cities_item.layers $ major_cities_layers
result = df2.groupby('group') $ result.describe()
df.loc['r1']
analyze_set[analyze_set['tweet_id']==666102155909144576].jpg_url
crimes.dtypes
page.oldest_revision['user']
time2close = pandas_ds["time2close"]
pnew = df2['converted'].mean() $ print(pnew) #creating the mean converted values as pnew
df['Date'] = pd.to_datetime(df['Date']) $ df.set_index('Date', inplace=True) $
X = X.dropna()
np.nansum(vals2), np.nanmin(vals2), np.nanmax(vals2)
X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.2, random_state=23)
oppose=merged[merged.committee_position=="OPPOSE"]
df.loc['Total']['AveragePrice']
(autos["ad_crawled"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         )
condition = (notus['cityOrState'].isna() == False) & (notus['country'].isna() == True) $ notus[condition]
print myIP[0] $ print low[low > myIP[0]].min() $ print high[high < myIP[0]].max() $ print low[low > myIP[0]].min() $
df_ml_692_01.tail(5)
import pandas as pd $ import numpy as np
c>0.7
new_page_converted = df2.query('converted == "1"').user_id.nunique()
from sklearn import preprocessing $ le = preprocessing.LabelEncoder() $ le.fit(list(set(df[:1000]['brand']))) $
import pandas as pd $ from math import sqrt $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
dfBill = df[df['Memo'].apply(returnCategory)=="Bill Payment"]
alg3 = KNeighborsClassifier(5) $ alg3.fit(X_train, y_train) $ probs = alg3.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
prec_group = precipitation.groupby(['valid']).agg({"precip_in" : np.max})
hmeq.summarize(cardinality=dict(name='hmeq_card', replace=True))
df_concat_2.head()
nypd_df.head()
sns.regplot(x = np.arange(-len(my_tweet_df[my_tweet_df["tweet_source"] == "fantastic ms."]), 0, 1),y=my_tweet_df[my_tweet_df["tweet_source"] == "fantastic ms."]["tweet_vader_score"],fit_reg=False,marker = "o",scatter_kws={"color":"teal","alpha":0.8,"s":100}) $ ax = plt.gca() $ ax.set_title("Sentiment Analysis (fantastic ms.)",fontsize = 12)
total_new_page = (df2['landing_page'] == 'new_page').sum() $ total_new_page/unique_users
stations.installation_date = pd.to_datetime(stations.installation_date, format = "%m/%d/%Y").dt.date
data = data_current["By Name"].value_counts() $ print(data)
r = q_pathdep_obs.results() $ r
gDate_vProject = gDateProject_content.unstack(level=1, fill_value=0) $
from sklearn.cross_validation import KFold
df.head()
for i in range(10,len(myIP)): $     print table2[table2['lower_bound_ip_address']== low[low < myIP[i]].max()]['country'] $
subs = df.groupby(['subreddit'])['subreddit_subscribers'].max()
len(taxi_hourly_df.index.unique())
df.sentiment.value_counts()
temperature_2017= hawaii_measurement_df[['Date','Temperature']] $ temperature_2017_df = pd.DataFrame(temperature_2017[(temperature_2017['Date'] >= '2016-08-01')\ $                                 & (temperature_2017['Date'] <= '2017-08-23')]).set_index('Date') $ temperature_2017_df.head()
X_train, X_test, y_train, y_test = train_test_split(X, y, $                                                    test_size=0.3, $                                                    stratify=y, $                                                    random_state=123)
state_party_df.fillna(0, inplace=True) $ state_party_df.head(20)
p_old = .1196
tweets_raw = tweets_raw.drop(axis= 1, labels=  ["id", "user_name"]) $ tweets_raw = tweets_raw.dropna()
db_users = pd.read_sql_query ("SELECT * FROM \"user\"", conn) $
imgp_clean.tweet_id = imgp_clean.tweet_id.astype(str)
ffr.index.month
from collections import Counter
pd.DataFrame(records2.describe().loc['mean', ['Age', 'GPA', 'Days_missed']])
x['Gain +1d'] = y
ac['Dispute Resolution Start'].groupby([ac['Dispute Resolution Start'].dt.year]).agg('count')
df_data = pd.read_csv('db246968-e567-442f-8108-b86a280107d3.csv')
weather_yvr_dt.head()
df2 = tier1_df.reset_index() $ df2 = df2.rename(columns={'Date':'ds', 'Incidents':'y'})
df.describe()
print 'Total cycling distance of the whole trip: \t%.2f km \nTotal time cycled: \t\t\t\t%s h|m|s' % (sum( $     cycling_df['ttl_cyc_km']),secToHours(sum(cycling_df['ttl_cyc_seconds'])))
(p_diffs < obs_diff).mean() + (p_diffs > (np.mean(p_diffs) + np.mean(p_diffs) - obs_diff)).mean()
df.columns
last_year_temp = [float(result[0]) for result in last_year_temp if result!=None]
df2= df.drop(delte, axis=0)
DAILY_TWEET_CUTOFF=30 $ aggdf['latlng'] = list(zip(aggdf.lat, aggdf.lng)) $ badgeo_df = aggdf.loc[aggdf['tweet_count'] > DAILY_TWEET_CUTOFF*tdelta] $ badgeo_df $
p_r_curve(y_test, y_fit[:,1])
 df.sum(axis=1) #means across the column
df = df.withColumn("post_creation_date", df["post_creation_date"].cast(TimestampType()))
list(car_data)
p_diffs = [] $ for _ in range(10000): $     p_new = np.random.choice([0, 1], size=n_new, p=[1-convert_new, convert_new]).mean() $     p_old = np.random.choice([0, 1], size=n_old, p=[1-convert_old, convert_old]).mean() $     p_diffs.append(p_new - p_old)
from scipy.stats import norm $ norm.cdf(z_score),norm.ppf(1-(0.05/2))
classification_orgs = df[['primary_role','zipcode','status','short_description','description','category_group_list', $                          'founded_on','founded_year','uuid']]
Date=pd.to_datetime(crimes['Date']) $ crimes.index=Date $ crimes.sort_index(inplace=True)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ z_score, p_value
ols_model = statsmodels.formula.api.ols('sentiment ~ group', data=name_sentiments).fit() $ ols_model.summary().tables[0]
import datetime
DB_URL = "mysql+pymysql://hass:12345@192.168.0.30/homeassistant?charset=utf8" $ engine = create_engine(DB_URL)
autodf.model = autodf.model.astype('category')
pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))
R_weather = weather_df.resample(sample_rate).mean() $ R_weather.Weather = weather_df.Weather.resample(sample_rate).max() $ R_weather = R_weather.interpolate('linear', axis=0) $ R_weather.head()
df[df < 0]
cities_df = pd.DataFrame() $ weather_df = pd.DataFrame()
print(autos.price.describe().apply(lambda x: format(x, 'f'))) #lambda to supress scientific notation convertion $ print(autos.price.value_counts().head(10).sort_index(ascending=True)) $ print(autos.odometer_km.describe()) $ print(autos.odometer_km.value_counts().head(10).sort_index(ascending=True))
df_archive_clean["name"].value_counts()[0:15]
import matplotlib.pyplot as plt $ plt.figure() $ plt.imshow(image) $ plt.show()
round(len(df_twitter[df_twitter.dog_label == 'floofer']) / len(df_twitter.dog_label), 3)
df19 = pd.read_csv("Sentiment_Analysis_of_67_Tweets_About_AAPL.csv") $ df19 = df19.drop("Sentiment",axis =1)
sl_pf_v2[['ACCOUNT_ID','CHARGE_OFF_COUNT','DELIN_14']].sample().ACCOUNT_ID
c_aux=[] $ for i in range(0,l2): $     if i not in seq: $         c_aux.append(c_counts[i]) $ col.append(np.array(c_aux))
s519397_df["prcp"].mean()
import pandas as pd $ % matplotlib inline
df.filter(like='year')
len(controlled_by_popular_person(active_psc_records,100).company_number.unique())
API_KEY = 'MHso4itbSsk44SDbsyWv' $ import requests $ import numpy as np $ url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-05-12&end_date=2018-05-12&API_KEY'
conf_matrix = confusion_matrix(y_test, lr1.predict(X_test), labels=[1,2,3,4,5]) $ conf_matrix
au.find_some_docs(uso17_qual_coll,sort_params=[("id",1)],limit=3)
mentions_df["date"] = mentions_df["epoch"].apply(lambda x: time.strftime('%Y-%m-%d', time.localtime(x))) $ mentions_df["time"] = mentions_df["epoch"].apply(lambda x: time.strftime('%H:%M:%S', time.localtime(x)))
n_new = df2[df2.group == 'treatment'].count()[0] $ n_new
data.describe(include=['O'])
autos['odometer_km'].value_counts() $
unique_users_num = df.user_id.nunique() $ print('Unique users: {}'.format(unique_users_num))
lm=sm.OLS(dff['user_id'], dff[['intercept','ab_page']]) $
medals_data = medals.assign(oecd=medals.index.isin((oecd_year[oecd_year<1997]).index).astype(int))
print(df.loc[sd:ed][['GOOG','GLD']].head()) #more pythonic? $ print(df[sd:ed][['GOOG','GLD']].head())
tmp_cov.index.names[0]
really_large_dataset = 100
from gdax_helpers import *
LabelsReviewedByDate = wrangled_issues_df.groupby(['Status','DetectionPhase']).created_at.count() $ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True, grid=False) $
%matplotlib inline $ import matplotlib.pyplot as plt $ from matplotlib import style $ style.use('ggplot')
net_loans_exclude_US_outstanding.loc[KR_select].head()
p_new_null = df.converted.mean() $ p_new_null
def createDataPoints(centroidLocation, numSamples, clusterDeviation): $     X, y = make_blobs(n_samples=numSamples, centers=centroidLocation, $                                 cluster_std=clusterDeviation) $     X = StandardScaler().fit_transform(X) $     return X, y
tweet_df.head() $
import numpy as np
mask = [(row in pn_qty[pn]['1cstoreinfo']) for row in table_1c.iterrows()]
from scipy.stats import norm $ norm.cdf(z_score), norm.ppf(1-0.05)
merge[merge.columns[47]].value_counts() $
df['timestamp'].dt
rddScaledScores.reduce(lambda s1,s2: s1 + s2) / rddScaledScores.count()
data_df[data_df['cycles'] == 21]['created_at'].min()
index = pd.to_datetime(non_na_df['created_at']) $ non_na_df.index = index
blight_inc = pd.read_csv(processed_path+"blight_incident_count.csv") $ blight_inc.head()
vip_df.info()
(df2['landing_page']=='new_page').mean()
df = pd.DataFrame(sites)
merged.groupby("committee_name_x")
X_mice.iloc[na_index, 1] = np.NAN
len(df[~(df.event_properties == {})])
df2 = df2.drop_duplicates(subset='user_id')
co_buildings_latlong = pd.merge(CO_profit, cpq_status, on='Building ID', how='inner') $ tx_buildings_latlong = pd.merge(TX_profit, cpq_status, on='Building ID', how='inner') $ ga_buildings_latlong = pd.merge(GA_profit, cpq_status, on='Building ID', how='inner')
data_df.describe()
import tweepy $ import pandas as pd $ import matplotlib.pyplot as plt
trunc_df.head()
ufos_df.plot(kind='bar', x='date_posted', y='count(1)', figsize=(6, 2))
t_likes = pd.Series(data=data['Likes'].values, index=data['Date']) $ type(t_likes)
age_hist.head(10)
stock['daily_gain'] = stock.close - stock.open $ stock['daily_change'] = stock.daily_gain / stock.open
pivoted_table.plot(legend = False, alpha = 0.01)
users_orders = users_orders.groupby('id_partner', as_index=False).sum().sort_values('Cost_14') $ users_orders.Cost_14 = users_orders.Cost_14/users_orders.Order_14 $ users_orders.Cost_30 = users_orders.Cost_30/users_orders.Order_30 $ users_orders.Cost_60 = users_orders.Cost_60/users_orders.Order_60 $ users_orders
c = conn.cursor() $ c.execute("INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)") $ conn.commit() $ conn.close()
print pd.pivot_table(data=df, $                      index='date', $                      columns='item', $                      values='status', $                      aggfunc='mean')
with ZipFile('{0}.zip'.format(datapath / zipfile), 'r') as myzip: $     myzip.extractall(datapath)
df_countries = pd.read_csv('./countries.csv') $ df_countries = df_countries.set_index('user_id') $ df_countries.head() $
treatment_df = df2.query('group == "treatment"') $ treatment_conv = treatment_df.query('converted == 1').user_id.nunique() / treatment_df.user_id.nunique() $ treatment_conv
array_masked = np.ma.masked_values(array, 99.9) $ print(array_masked[0,:])
tmin = temp_analysis[0][0] $ tavg = temp_analysis[0][1] $ tmax = temp_analysis[0][2] $ print (tmin, tavg, tmax)
grouped = df.groupby('hash_id')
df_aggregate = (df[['date','serial_number','failure']].groupby('serial_number',as_index = True) $                 .agg({'date':'count', 'failure':'sum'}) $                 .rename(columns={'date': 'date_count', 'failure': 'failure_sum'}) $                 .sort_values(by=['failure_sum'],axis=0,ascending=False)) $ df_aggregate.head(5)
scale = [x for x in range(1,11)] $ recscale = [rec1, rec2, rec3, rec4, rec5, rec6, rec7, rec8, rec9, perec]
os_br_colmns = list(dbdata) $ os_br_keep = ['id_os','Android', 'iOS', 'Windows', 'Chrome', 'Safari', 'IEMobile'] $ os_br_colmns = [e for e in os_br_colmns if e not in os_br_keep] $ dbdata.drop(dbdata[os_br_colmns],axis=1,inplace=True) $ dbdata
! rm -rf ~/s3/comb/flight_v1_0.pq
test_df.head()
np.shape(temp_fine)
df2['intercept'] = 1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group']) $ df2.head(n=10)
data = {'date': ['2014-05-01 18:47:05.069722', '2014-05-01 18:47:05.119994', '2014-05-02 18:47:05.178768', '2014-05-02 18:47:05.230071', '2014-05-02 18:47:05.230071', '2014-05-02 18:47:05.280592', '2014-05-03 18:47:05.332662', '2014-05-03 18:47:05.385109', '2014-05-04 18:47:05.436523', '2014-05-04 18:47:05.486877'], $         'battle_deaths': [34, 25, 26, 15, 15, 14, 26, 25, 62, 41]} $ df = pd.DataFrame(data, columns = ['date', 'battle_deaths']) $ df
line?  need to correct dec dates for year flip
likes = pd.read_csv('../data/cleanInput/likes.csv') $ likes.head()
a={'x':1,'y':2,'z':3} $ b={'w':10,'x':11,'y':2}
grammar = "NP_chunk: {<DT>?<NN><JJ>*}"
model = Ridge(alpha = 1) $
df_campaigns.describe()
newdf.reset_index(inplace=True)
all_sets.describe()
len(addicks.Compaction)
Numerator=df2.query("landing_page=='new_page'").shape[0] $ print("The probality that he recieves new page equals",Numerator/df2.shape[0])
data[data['processing_time']>datetime.timedelta(148,0,0)]
x =  store_items.isnull().sum().sum() $ print('Number of NaN values in our DataFrame:', x)
print df
tips['total_bill'] = pd.to_numeric(tips['total_bill'], errors='coerce') $ tips['tip'] = pd.to_numeric(tips['tip'], errors = 'coerce') $ print(tips.info()) $
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=8000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
a = master_file.iloc[:, 28:-1].count(axis=0) $ a = len(master_file.index) - a $ print(a)
df_columns.head() $
vocab = vectorizer.get_feature_names() $ print (vocab)
for place in meanLon.index: $     datatest.loc[((datatest.lon.isnull()) & (datatest.place_name == place)), 'lon'] = meanLon.loc[place]['lon'] $     datatest.loc[((datatest.lat.isnull()) & (datatest.place_name == place)), 'lat'] = meanLat.loc[place]['lat']
neo_github_issues_df[['title', 'html_url', 'created_at','updated_at']].head()
users
calls_df.info()
for i,df in enumerate(dfs): $     df['origdb'] = i $
df_archive_clean.timestamp = pd.to_datetime(df_archive_clean.timestamp)  
sampling_rules = { $     "e_BuildingSqm >= 2000": 10, $ }
dftest = parsedatetime(test)
hrefs = soup.find_all('a') $ hrefs
df4['country'].value_counts()
len(set(bookings['hash_id'])) == len(bookings)
train[train.age>117].age.shape
dframe_team['end_cut'] = pd.to_datetime(dframe_team['end_cut'], infer_datetime_format=True, errors='coerce') $ dframe_team['start_cut'] = pd.to_datetime(dframe_team['start_cut'], errors='coerce') $ dframe_team['Tenure'] = dframe_team['End'] - dframe_team['Start'] $ dframe_team
authors_grouped_by_id_saved.show()
import statsmodels.api as sm $ logit = sm.Logit(df['converted'],df[['intercept','treatment']]) $ results = logit.fit() $ results.summary()
pd.MultiIndex(levels=[['a','b'], [1,2]], $              labels=[[0,0,1,1], [0,1,0,1]])
! wget https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/PAXRAW_D.ZIP $ ! mv PAXRAW_D.ZIP {data_dir} $ ! unzip {data_dir}PAXRAW_D.ZIP -d {data_dir}
r2_score(y_dev, best_model['model'].predict(X_dev))
day_of_week(df)
columns.keys()
sum(trumpint.num_shares),sum(cliint.num_shares)
staff
df_total = stack_shifted_sales(df_total, days_deltas=[1]) $ df_total.dropna(inplace=True)
iris_dataset['data'].shape
!h5ls -r 'data/my_pytables_file.h5'
bucket_name = "i-agility-212104.appspot.com"
s.loc[s > 100].head()
merged2 = pd.merge(merged2, actor_metric, on="movie", how="inner") $ merged2 = pd.merge(merged2, director_metric, on="movie", how="inner") $ merged2 = merged2.drop(["actors","directors"],1) $ merged2.head()
for a, b in sss: $     print(a) $     print(b)
train_binary_dummy = pd.get_dummies(train_binary, columns = categorical) $ train_binary_dummy.head()
df['youtube_search_url'] = df.apply(lambda x: urllib.parse.quote_plus(base_search_url + x['title'] + x['artist'], safe='/:?='), axis=1)
from sklearn.metrics import classification_report $ print(classification_report(fashion.index, fashion['predicted']))
len(train_preprocessed[train_preprocessed.isnull().any(axis=1)])
van15_fin['EditTime>15'] = van15_fin['diffs']>15
df_out = df.iloc[:, 1:(df.shape[1]-6)] $ df_out.index = df['V1'] + "[" + df['FXNCLASS'] + "] " + df['SYMBOL'] $ fig=plt.figure(figsize=(15, 80), dpi= 80, facecolor='w', edgecolor='k') $ sns.heatmap(df_out,cmap='RdBu_r', vmin=0, center=1.301, vmax=7.301)
soups = [soup(requests.get(url).text, 'html.parser') for url in article_urls]
df_pr = df_pr[['year', 'item', 'date', 'location', 'title', 'body']] $ df_pr
crime_wea['Date']=pd.to_datetime(crime_wea['Date']) $ crime_wea.index=crime_wea['Date']
df2[(df2['landing_page']=='new_page')].count()[0]
bmp_series = pd.Series(bmp_dict) $ bmm_series = pd.Series(bmm_dict)
headings = json_normalize(data_test, [['dataset', 'column_names']])
index_change = df_new[df_new['group']=='treatment'].index $ df_new.set_value(index=index_change, col='ab_page', value=1) $ df_new.set_value(index=df_new.index, col='intercept', value=1) $ df_new[['intercept', 'ab_page']] = df_new[['intercept', 'ab_page']].astype(int) $ df_new = df_new[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
sources.to_sql(con=engine, name='sources', if_exists='replace', flavor='mysql', index=False)
a400hz.head()
tweet.author
df_archive['rating_denominator'].value_counts()
df.describe(include='all')
stock.tail()
db_tables=inspector.get_table_names() $ db_tables
results_country.summary()
prob_control_converted - prob_treatment_converted 
print(avi_data.columns) $ dims = avi_data.shape $ print('\n (Tuples, Columns)', avi_data.shape)
DF = sqlContext.createDataFrame(records,['name_pair','count']).withColumn("word1", F.col('name_pair'))#.withColumn("x4", lit(1))
(p_diffs > prob_treatment - prob_control).mean()
dfa_1=dfa["driver_count"].sum() $
from itertools import chain $ nl=list(chain(*ll)) $ nl
autos["registration_year"].value_counts(normalize=True)
autos.loc[autos["registration_year"]>2016, "registration_year" ].shape
norms_df = pd.DataFrame(norms, columns=['tmin', 'tavg', 'tmax']) $ norms_df['date'] = trip_dates $ norms_df.set_index(['date'], inplace=True) $ norms_df.head()
thisWeek = tweets.loc[tweets['created_at'] > endDay] $ tweets = tweets.loc[tweets['created_at'] < endDay]
prophet_df = pd.DataFrame() $ prophet_df['y'] = df[target_column] $ prophet_df['ds'] = df['date']
baseball_newind.index.is_unique
path = './' $ teamname = 'team_foo' $ out_name = path + teamname + '_submission.csv'
stemmed_tokens = [] $ for t in clean_tokens: $     t = nltk.PorterStemmer().stem(t) $     stemmed_tokens.append(t) $ stemmed_tokens[:10]
goals_df = pickle.load(open('goals_df.pkl', 'rb'))
G = nx.DiGraph() $ G.add_nodes_from(dfUsers['userFromId']) $
df_characters = create_simpsons_characters_dataframe(df)
df_ad_state_metro_2.plot(x='state', y='ad_duration_secs', kind='bar') $ df_state_victory_margins.plot(x='state', y='Percent margin', kind='bar') $
image_file=pd.read_csv('image-predictions.tsv',sep='\t')
Z1 = np.random.randint(0,10,10) $ print (Z1) $ Z2 = np.random.randint(0,10,10) $ print (Z2) $ print(np.intersect1d(Z1,Z2))
df.apply(np.mean,axis=1)
old_page_converted = np.random.binomial(1, p_old,n_old) $ old_page_converted
col=['ab_page', 'intercept', 'converted'] $ data=df3[col].join(country.ix[:, 'CA':]) $ data.head()
Celsius.temperature.__doc__
print('Most positive tweets:') $ for t in trump.sort_values('polarity', ascending=False).head()['text']: $     print('\n  ', t)
new_page_prob = total_treatment / (total_control + total_treatment) $ print(new_page_prob)
for (i, x) in enumerate(json_dict): $     json_dict[0][5] = x[2]-x[3] $ max_change = max(json_dict, key=lambda x: float(0) if x[5] is None else x[5]) $ print("\nLowest opening prices were for the stock in this period was: {:.2f} on {}".format(max_change[5],max_change[0]),"\nAll the elements of this data :", max_change) $
austin.drop(258, inplace=True)
more_recs = pd.read_csv("../data/microbiome/microbiome_missing.csv").head(20)
print("Number of NAN values before: ",df_usa.isnull().sum())    #printing number of NaN values $ df_usa = df_usa.fillna(0) $ print("Number of NAN values after: ", df_usa.isnull().sum())    #printing the values after the NaN deletion
vwap.ix['2011-11-01 09:27':'2011-11-01 09:32']
result.summary2()
history_df = pd.read_sql('measurement',engine) $ history_df['station'].value_counts()
df.groupby("cancelled")["winter_pickup"].mean()
df["booking_application"][~mask].value_counts() / df["booking_application"][~mask].count()
ndf = new_df.copy() $ ndf.index = range(new_total) $ ndf.head()
two_day_sample.reset_index(inplace=True)
datetime.datetime.now().year
predictions=model.predict(X) $ rounded =[round(x[0]) for x in predictions] $ print(rounded)
result_control_2.summary()
import pantab $
group_1 = df.groupby(['C/A','UNIT','STATION', pd.Grouper(freq='D',key='DATETIME')]).sum() $ len(group_1) $ group_1.head()
df2 = df.copy() $ df2['E'] = ['one', 'one','two', 'three','three','four', 'four'] $ df2
data = pd.read_csv('data_redditv2.csv')
df.loc['2017-01-12':'2017-02-05', ['Open', 'Close']].plot(figsize=(15,5));
tweets_gametitle
apache_people_df.schema
print("Total proportion of Converted users :: {}%".format((df['converted'].mean())*100))
holdout_residuals = holdout_preds - y_holdout $ print holdout_residuals
print perf.display()
c = 2 # NUMBER OF CLASSES $ m = get_rnn_classifier(bptt, 20*bptt, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1, $           layers=[em_sz*3, 50, c], drops=[dps[4], 0.1], $           dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3]) $
db_criterion =pd.read_sql_query( "SELECT * FROM criterion",conn) $ db_vote =pd.read_sql_query( "SELECT * FROM vote",conn) $ db_peer_review =pd.read_sql_query( "SELECT * FROM review",conn) $ db_expert_review = pd.read_sql_query("SELECT * FROM  expert_review",conn) $ db_voter_score = pd.read_sql_query("SELECT * FROM vote_rating", conn)
female_journalists_retweet_df = journalists_retweet_df[journalists_retweet_df.gender == 'F'] $ female_journalists_retweet_by_gender_df = pd.merge(user_summary_df[user_summary_df.gender == 'F'], female_journalists_retweet_df.groupby(['user_id', 'retweet_gender']).size().unstack(), how='left', left_index=True, right_index=True)[['F', 'M']] $ female_journalists_retweet_by_gender_df.fillna(0, inplace=True) $ female_journalists_retweet_by_gender_df['all'] = female_journalists_retweet_by_gender_df.F + female_journalists_retweet_by_gender_df.M $ female_journalists_retweet_by_gender_df.describe()
f_ip_app_hour_clicks.show(1)
result.head()
x = topics_data.comms_num $ y = topics_data.score $ print("Correlation between Number of Comments and Total Score is:", round(np.corrcoef(x, y)[0,1], 2)) $
n_old = df2.query('landing_page == "old_page"')['user_id'].count() $ print(n_old)
cr_treatment = df2[df2['group'] == 'treatment']['converted'].mean() $ cr_treatment $
yc_new2.dropna() $ yc_new2.isnull().sum()
from sklearn.feature_extraction.text import CountVectorizer $ from sklearn.feature_extraction.text import TfidfVectorizer $ cvec = CountVectorizer(stop_words='english', max_features=100) $ tvec = TfidfVectorizer(stop_words='english', max_features=100)
data.describe()
RNPA_existing_hours = RNPA_existing_hours[1:]
sensor = hp.find_sensor('53b1eb0479c83dee927fff10b0cb0fe6') $ sensor
loans_plan_origpd_noshift_xirr=cashflows_plan_origpd_noshift_all.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
dataframe.groupby('month').daily_worker_count.agg(['count','min','max','sum','mean'])
len(greater_than_difference) / len(p_diffs)
import pandas as pd $ import numpy as np
df2.drop_duplicates(subset='user_id',inplace=True)
honeypot_df.drop(['time_stamp1','time_stamp2','time_stamp3'], axis = 1, inplace = True)
n_old = len(df2_control) $ n_old
newdf.head()
base_df.show(truncate=False)
house_data.tail(20)
learn.fit(lr, 3, cycle_len=1, cycle_mult=2)
INDEX = np.random.choice(len(text[mask]), 5000)
log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = log_mod.fit() $
df.insert(2,"Com Rate %", 5) $ df.head(3)
titanic.pivot_table('survived', index='sex', columns='class')
Users_first_tran
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.TAB.txt" $ mydata = pd.read_csv(path, sep= '\t') $ mydata.head(5)
df_concat.drop(df_concat.columns[[0,1,2]], axis=1, inplace= True)
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost','ooIdleCost','corrCost','floor_date','floor_10min'], axis=1) $ dfY = data['corrCost']
!cat ~/.keras/keras.json
_ = ok.grade('q05a') $ _ = ok.backup()
df_h1b_nyc_ft.pw_1.describe()
geom = [Point(xy) for xy in zip(new_sample['Longitude'],new_sample['Latitude'])] $ crs = {'init':'epsg:4326'} $ threeoneone_geo = gpd.GeoDataFrame(new_sample,crs = crs,geometry=geom)
details = client.repository.get_experiment_details(experiment_uid)
print(len(minutes_list))
us.loc[condition, ['cityOrState']] = us[condition]['cityOrState'].str.extract(r'([A-Z]{2})') $ us['cityOrState'].value_counts(dropna=False)
elms_sl.shape[0],elms_pl.shape[0], elms_tl.shape[0]
type(new_df['Document_Date'][2])
merged1 = merged1[ordered_columns]
specsUrl = {} $ with open('obj/specsUrl21052018.pkl', 'rb') as handle: $     specsUrl = pickle.load(handle) $
pd.date_range('2015-07-03', periods=8)
stat_data
df_new.drop('CA', axis=1, inplace=True) $ df_new.head()
with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f: $     f.create_group(where='/', name='my group')
new_page_converted = np.random.choice([1,0],size=n_new,p=[p_mean,(1-p_mean)]) $ new_page_converted.mean()
new_dems.newDate.describe() $
from functools import reduce $ nl=reduce(lambda x,y:x+y,ll) $ nl
c.values
autos_p['date_crawled'].str[:10].value_counts(normalize = True, dropna = False).sort_index(ascending = True)
df2['intercept']=1 $ df2[['control','ab_page']] = pd.get_dummies(df2['group']) $ df2.head()
auth = {'AuthMethod' : 'password', $         'Username' : account, $         'AuthString' : password} $ import xmlrpc.client $ url = "https://r2labapi.inria.fr:443/PLCAPI/"
% matplotlib inline
posts_df['processed_text'] = [process_text(s.lower()) for s in tqdm(posts_df['Body'])]
import os $ ratings_file = os.path.join(ml_data_dir, 'ratings.csv') $ movies_file = os.path.join(ml_data_dir, 'movies.csv') $ ratings = pd.read_csv(ratings_file) $ movies = pd.read_csv(movies_file)
tweet_errors
tweet_archive_enhanced_clean['text'] = tweet_archive_enhanced_clean['text'].str.replace('\shttp\S+','')
for c in ccc: $     for i in vwg[vwg.columns[vwg.columns.str.contains(c)==True]].columns: $         vwg[i] /= vwg[i].max()
bus["postal_code"] = bus["postal_code"].fillna("MISSING") $ bus
df_new['UK_ind_ab_page'] = df_new["UK"]*df_new['ab_page'] $ df_new['US_ind_ab_page'] = df_new["US"]*df_new['ab_page'] $ df_new['CA_ind_ab_page'] = df_new["CA"]*df_new['ab_page'] $
name_error_rows =[] $ for i in range(len(twitter_archive)): $     if twitter_archive['name'][i] !="None" and twitter_archive['name'][i] not in twitter_archive['text'][i]: $         name_error_rows.append(i) $ print(name_error_rows)
base_date = dt.datetime.strptime("2017-08-23", "%Y-%m-%d") $ numdays = 365 $ date_list = [base_date - dt.timedelta(days=x) for x in range(0, numdays)] $ print(date_list)
!ls -l ../data/raw
oppstage.loc[1].plot(kind='bar', stacked=True, figsize=(12,6));
evaluator.get_metrics('phrase_level_results')
cooks['is_cooking_reason_meet'] = cooks['is_reason_meet'] $ cooks['is_cooking_reason_brand'] = cooks['is_reason_brand'] $ cooks['is_cooking_reason_money'] = cooks['is_reason_money']
x['Gain +1d'] = y_valid
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2.head(10)
a[a.find('t')]
P.columns
gb = df2.groupby('landing_page').count() $ gb
autoDf.createOrReplaceTempView("autos") $ SpSession.sql("select * from autos where hp > 200").show()
df_protest.loc[df_protest.towncity_name=='Johannesburg', 'start_date'].dtype
base_url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json" $ api_url = "&api_key="+API_KEY $ url = base_url+"?limit=1"+api_url $ r = requests.get(url)
twitter_ar.sample(2)
paradasColectivos["cantidad_paradas"] = np.where(paradasColectivos["lineas"] == "{}", 0, paradasColectivos.lineas.str.count(",") + 1) $ paradasColectivos
w_aux=[] $ for i in range(0,l2): $     if i not in seq: $         w_aux.append(w_counts[i]) $ col.append(np.array(w_aux))
print('Probability of an individual converting:',df2['converted'].mean())
clicks.info()
import statsmodels.api as sm
(p_diffs > actual_difference).mean()
calls_df=calls_df.drop(["call_date","time"],axis=1)
com311 = com311[['Unique Key', 'Created Date', 'Closed Date', 'Agency', 'Complaint Type', 'Descriptor' $                  , 'Location Type', 'Incident Zip', 'Incident Address', 'Street Name', 'City', 'Facility Type' $                  , 'Status', 'Due Date', 'Latitude','Longitude']]
TEXT.numericalize([md.trn_ds[0].text[:12]], device=-1)
com_grp.agg([np.mean,np.sum])
classifier = tf.contrib.learn.Estimator(model_fn = cnn_model)
output= "SELECT user_id, user_name from user limit 10" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['user_id','user_name'])
train_orders=orders[orders['eval_set']=='train'] $ trains=pd.merge(order_products_train, train_orders,how='left', on='order_id') $ trains.head()
Meter1.ModeSet('Res4W')
taxi_hourly_df["missing_dt"] = np.NaN
scr_retention_df.fillna(0,inplace=True)
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $ new_page_converted
df.columns
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ WHERE twitter_day = 4 \ $ GROUP BY tag_text ORDER BY count DESC LIMIT 1;
births_by_date.index = [pd.datetime(2012, month, day) $                        for (month, day) in births_by_date.index] $ births_by_date.head()
df_enhanced['rating_10_scaled'] = df_enhanced['rating_10_scaled'].astype('float')
df2 = df2.drop_duplicates('user_id', keep='first')
a = np.array([1, 2, 3]) $ a
whos DataFrame
analyzer = SentimentIntensityAnalyzer()
sdss_final=sdss_df $ sdss_final
props.prop_name == "PROPOSITION 064- MARIJUANA LEGALIZATION. INITIATIVE STATUTE."
with ValidHHFile.path.open() as file: $     aSingleLine = file.readline() $ print(aSingleLine)
preds['predict']
bb_pivot_table_2.plot() $ plt.ylabel('Inches, Weight') $ ax = plt.gca() $ ax.set_title("Min & Max NBA Player Height & Weight Over Time") $ plt.show()
tweets.retweet_status_follower_count.describe()
df_columns.index.month.value_counts().sort_index() $
df2['intercept'] = 1
X_cvec_synos = pd.concat([df_vec, df_city_dummy, df_state_dummy, df_vec_synos], axis=1) $ X_cvec_synos.head()
to_be_predicted_Day1 = 50.15 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
restaurantsExcelFile = pd.ExcelFile("Restaurants.xlsx");
autos['price'].value_counts().sort_index().head(20)
df2 = df.copy() $ df2[df2.isnull()] = 0 $ df2
df.shape[0] #number of rows of df dataset
stock_subset = stock_data[ [ 'open', 'close' ] ] $ print(display(stock_subset.head(5)))
sns.countplot(calls_df["status"],hue=calls_df["call_time"])
train_shifted = train_shifted.dropna(how='any') $ train_shifted.head(5)
noise_graf['AFFGEOID'] = noise_graf['AFFGEOID'].astype(str)
svdtrun = TruncatedSVD(n_components=8)
components_names= ('component_1', 'component_2', 'component_3','component_4','component_5', $                                                                             'component_6', $                                                                             'component_7', $                                                                             'component_8')
df_weekly.sort_values(by='retweets_average', ascending=False)
df.query("landing_page=='old_page' and group=='control'").shape[0]
df_mas['rating_numeric']=df_mas.rating_numerator/df_mas.rating_denominator
p_diffs=np.array(p_diffs)
print len(biased_train_15) $ print len(biased_train_33) $ print len(biased_train_60) $ print len(biased_train_05)
def kelvin_to_celsius(temp): $     return temp - 273.15
len(genre)
poverty.head(10)
scores[:1.625].sum()
!wget -O moviedataset.zip https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/moviedataset.zip $ print('unziping ...') $ !unzip -o -j moviedataset.zip 
kickstarter = pd.read_csv('kickstarter.csv') $ kickstarter.head()
Aussie_df = Aussie_df[Aussie_df.userTimezone.notnull()] $ len(Aussie_df) $ len(Aussie_df)/100*100 $
weather_data3.columns = weather_data1.columns.values; weather_data3.head()
data = pd.read_csv('fatal-police-shootings-data.csv') $ print(data.shape) $ data.info()
np.exp(0.0506)
data['screen_resolution'].value_counts()
p_old=p_new $ print(p_old)
top_supports['contributor_cleanname'] = top_supports.apply(combine_names, axis=1)
cust_data1.drop('MonthlySavings1', axis=1).head(2) # it creates new data $ cust_data.head(2)
labels=poverty.iloc[label_rows, 0].tolist()
data.head(10)
import volatility as vm
df_new_eng = df_user_engagement.groupby(["user_id", "visited"]).size().reset_index(name='count') $ df_eng_mult_visits = df_new_eng[df_new_eng["count"] > 2] $ df_eng_mult_visits.head()
json_tweets = pd.DataFrame(df_list, columns = ['tweet_id', 'favorites', 'retweets', $                                                'user_followers', 'user_favourites', 'date_time']) $ json_tweets.to_csv('tweet_json.txt', encoding = 'utf-8', index=False)
to_be_predicted_Day3 = 14.52670269 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
df.isnull().any()
events = events.withColumn('event_date', events['event_date'].cast('date'))
cur.execute("SELECT * FROM test.test_table;")# LIMIT 10;") $ for r in cur.fetchall(): $    print(r)
df = pd.read_csv("http://ichart.yahoo.com/table.csv?s=MSFT&" + $            "a=5&b=1&c=2014&" + $            "d=5&e=30&f=2014&" + $            "g=d&ignore=.csv") $ df[:5]
events.toPandas().head()
msk = np.random.rand(len(df)) < 0.8 $ train = cdf[msk] $ test = cdf[~msk]
wrd_clean['expanded_urls'] = wrd_clean['tweet_id'].apply(lambda x: 'https://twitter.com/dog_rates/status/'+str(x)+'/photo/1')
result.summary()
ab_df = pd.read_csv('./ab_data.csv')
import pymongo $ from pymongo import MongoClient $ client = MongoClient() $ client = MongoClient('localhost', 27017)
train['created'] = pd.to_datetime(train['created'], format='%Y-%m-%d %H:%M:%S') $ import datetime as dt $ train['day_created'] = train['created'].dt.weekday $ train['month_created'] = train['created'].dt.month $ train['hour_created'] = train['created'].dt.hour
us['country'] = us['country'].str.replace(r'.','') $ us['cityOrState'] = us['cityOrState'].str.replace(r'.','') $ us['country'] = us['country'].str.strip()
display(df['Market'].head(1)) $ display(df['Market'].head(2))
sales_df.groupby('Country').count()['Revenue'].sort_values(ascending=False) $
h = pd.read_sql_query(QUERY, conn) $ h $
archive.head()
data.shape
print(data.describe())
convert_old = df2.query('landing_page == "old_page" and converted == 1').shape[0] $ convert_new = df2.query('landing_page == "new_page" and converted == 1').shape[0] $ n_old = len(df2.query('group == "control"')) $ n_old = len(df2.query('group == "treatment"')) $
paragraphs = soup.find_all('p') $ paragraphs
plt.scatter("obs_count","status", data=pm_final,marker='o', alpha = 0.25) $ plt.xlabel("Cumulative number of readings") $ plt.ylabel("Status") $ plt.title('Failure over time') $ plt.show()
def calPercentage(dataframe, tokenCount, categoryList): $     for category in categoryList: $         dataframe[category] = float("{0:.2f}".format(dataframe[category] / dataframe[tokenCount]  * 100)) $     return dataframe
%%time $ ent_count = defaultdict(int) $ for doc in nlp.pipe(texts, batch_size=100, disable=['parser','ner']):  $     matcher(doc) # match on your text $ print(ent_count)
dataframe.loc[rows,columns] # .loc can take row ID $ dataframe.iloc[row_ix,columns_ix]
def calc_temps(start_date, end_date): $     return session.query(func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)).\ $         filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all() $ print(calc_temps('2012-02-28', '2012-03-05'))
breakfastlunchdinner.plot(x="STATION", y=["breakfast", "lunch + brexits", "dinner"], figsize = (20,9), kind="bar", rot=30, fontsize=15, title="Average Daily Weekday Traffic for Top 10 Stations at B/L/D");
df_2016['sales_jan_mar'] = [y if ((x.month >=1) & (x.month <=3)) else 0.0 for x, y in zip(df['Date'],df['sale_dollars'])]
to_be_predicted_Day3 = 48.60496872 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
dt.date()
print("Length of X is - ", len(X)) $ print("Length of y is -", len(y))
Y_lin_reg = lin_reg.predict(X2) $ from sklearn import metrics $ print('RMSE:', np.sqrt(metrics.mean_squared_error(Y2, Y_lin_reg))) $ print('Variance explained: ', metrics.explained_variance_score(Y2, Y_lin_reg))
df['count'].plot()
df['Complaint Type'].head()
s4g = combined[['Symbol', 'Adj Close']].reset_index() $ s4g.insert(1,'Year',pd.DatetimeIndex(s4g['Date']).year) $ s4g.insert(2, 'Month',pd.DatetimeIndex(s4g['Date']).month) $ s4g[:5]
data_dummies.to_csv('invoices_dummies.csv')
df_mes[(df_mes['average_speed'] == np.inf) | (df_mes['average_speed'] > 100)]
leadPerMonth = segmentData[['opportunity_month_year', 'lead_source']].pivot_table( $                                 index='opportunity_month_year', $                                 columns='lead_source', aggfunc=len)
(16753 + 655 + 2173 + 9516)/.30
df.shape
type(digits.data)
df['fileCount'].hist(bins=30)
D1[0:5]
!pip install pandas_datareader
train.sort_values('num_points', ascending=False).head()
total = total.sample(frac=1).reset_index(drop=True) $ total.head()
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'US_ind_ab_page', "US","ab_page"]]) $ results = log_mod.fit() $ results.summary()
tt_json = tt_json_df[['id', 'favorite_count', 'is_quote_status', 'retweet_count']] $ tt_json.head()
%timeit ' '.join([stemmer(word) for word in regex.sub('' ,'The vintage 1930 beaded earrings are still calling to me.').split()])
sdf = Item(gis, item_id).layers[0].query(out_sr={'wkid': 4326}).df $ sdf.head()
import matplotlib.collections as mc $ def visualize_samples(samples, discretized_samples, grid, low=None, high=None): $
for column in df_categorical: $     df_categorical[column].unique()
df_bud = pd.read_csv(budFile, usecols = budCols, $                  dtype = bud_dtypes)
emails_dataframe.to_csv("email-data-with-institution.csv", index=False)
df["postcount"] = 1
df2.head() # delete later
df = pd.read_sql('SELECT p.customer_id, p.amount FROM payment p WHERE p.amount > 4.2006673312979002;', con=conn) $ df $
new_stock_data = stock_data.drop('volume', axis = 1) $ print(display(new_stock_data.head()))
df.head()
SANDAG_ethnicity_df[SANDAG_ethnicity_df['ETHNICITY'] == 'White'].head()
df_pca = pd.DataFrame(pca.transform(df_norm), columns=labels) $ df_pca.head()
df.set_index('created_at', inplace=True)
archive_copy[['tweet_id', 'id_str']].info()
from pandas.tools.plotting import lag_plot $ dataSeries = pd.Series(Q3['Average Temperature'])
df = df[['geoid','index_col','t10walk']] $ df = df.merge(pred, on='index_col', how='left').merge(geog, on='geoid', how='left') $ df.to_csv(OUTPUT_FOLDER + 'beh_nyc_walkability_prediction.csv', index=False)
df.info()
act = pd.read_csv('data/PornBots/porn_list.csv')['141182021'].tolist()
def slice_metrics_2(group): $     return { $             'precision' : sk_metrics.precision_score(group.Label, group.PredictedLabel), $             'recall' : sk_metrics.recall_score(group.Label, group.PredictedLabel) $            }
(v_invoice_hub.loc[:, invoice_hub.columns] == invoice_hub).sum()
action_id = 'd68585ed-b922-4fd7-86bf-d78e22ccbc8d' $ url = form_url(f'actions/{action_id}') $ response = requests.get(url, headers=headers) $ print_body(response, max_array_components=3)
ts['1/31/2011']
autos['date_crawled'].str[:10].describe()
autos['brand'].value_counts(normalize=True)
zp = zip(list(trade_data_dict.values()),list(trade_data_dict.values())[1:]) $ (mx, st_d, ed_d) = max([(abs(t.close-p.close), t.date, p.date) for t, p in zp]) $ print('The largest intraday price change was: {:.2f}'.format(mx)) $ print('Between trading days {} and {}'.format(st_d, ed_d))
consumer_key = 'OnnL3a3Rl1L64G3mNvnn6w6YR' $ consumer_secret = 'M0p49XPYYjo48DWb8aTmTYl9M7gJ0Fe7D2KRQM5wZPqpa6wJaX' $ access_token = '946781751589957638-S6dJ0Z7bZdyYViHR1MPSW0Cw0RlIXJG' $ access_token_secret = 'Q1TmGZanEuHl9zPifruVObUkwqV9NCQFrwy9XIdNvo4ze'
clf = RandomForestClassifier(n_estimators=300).fit(X, y)
print("Number of Techniques in ATT&CK") $ print(len(all_attack['techniques'])) $ techniques = all_attack['techniques'] $ df = json_normalize(techniques) $ df.reindex(['matrix', 'created','tactic', 'technique', 'technique_id', 'data_sources'], axis=1)[0:5]
sample_dic['Lenddo'] = 1999 $ sample_dic
duplicated_user_df = df2[df2.duplicated(['user_id'], keep=False)] $ duplicated_user_df
joined_test = join_df(joined_test, df, ['Store', 'Date'])
cur.execute(sql_all_tables) $ all_tables_df = pd.DataFrame( $     cur.fetchall(), columns=[rec[0] for rec in cur.description])
team_groups.aggregate(np.sum).sort_values(by = "Tm.TRB", ascending = False)
% ls *.csv
unsorted_df.sort_values(by=['col1'],ascending=False,kind="heapsort")
len(after)
for x in extract_all.columns.values: $     if x.find('OTHER')>-1: $         print(x)
%pycat bikescore.py $
for key, value in close.items(): $     if value == min_dif_Q5: $         print key, value
df_R.tail()
props.prop_name
infinity.head()
df_delta = df[df['open']
assert trn_df.shape[0] == trn_y.shape[0] $ assert val_df.shape[0] == val_y.shape[0]
token_sendReceiveAvg_month = token_sendReceiveAvg_month[["ID","sendcount","receivecount"]]
autos = autos.drop(index = odometer_outliers)
p_diffs = np.array(p_diffs) $ prop_pdiffs_greater = ((p_diffs > actual_diff).sum()) / len(p_diffs) $ print('The proportion of the p_diffs that are greater than the actual difference observed in ab_data.csv is {}'.format(prop_pdiffs_greater))
plt.hist(SCP_ENTRY_weektotals)
tweets_csv=load_csv('Twitter.csv') $ tweets_csv.head()
print np.mean(low_polarity['polarity'])*len(low_polarity) $ print np.mean(high_polarity['polarity'])*len(high_polarity)
plt.plot(Q1) $ plt.plot(T1)
dictionary = corpora.Dictionary(tweets_list)
combined_df3.keys()
new_page_converted = np.random.choice([0,1],n_new,p=(p_new,1-p_new))
reddit_df.head()
max_features = 20000 $ maxlen = 80  # wrap the texts after this number of words (among top max_features most common words) $ batch_size = 32 $ n_epoch = 3
import statsmodels.api as sm $ convert_old = sum(df2_control['converted']) $ convert_new = sum(df2_treatment['converted']) $ n_old = n_old $ n_new = n_new
a=df2.nunique()['user_id'] $ print("There are "+str(a)+" unique users in the new dataset")
treatmentdf=df2[df2.group=="treatment"] $ treatmentdf[treatmentdf.converted==1].count()[0]/treatmentdf.count()[0]
bands.to_csv('../data/bands.csv')
y_predict=[round(ii[0]) for ii in model.predict(x)] $ deviate=[0.5 for aa,bb in zip(y,y_predict) if aa==bb] $ plt.figure() $ plt.plot(y,marker='s',linestyle='') $ plt.plot(deviate,marker='h',markersize=1,linestyle='',color='k') $
n_new = df2[df2["landing_page"] == "new_page"].count() $ n_new = n_new[0] $ n_new
df_columns[df_columns['Agency']=='DOT']['Complaint Type'].value_counts().head() $
df[['Name','Standard_Plat','Platform_x','Platform_y']]
shows['release_monthday'] = shows['release_date'].dropna().apply(lambda x: x.strftime('%d')) $ shows['release_monthday'] = shows['release_monthday'].dropna().apply(lambda x: str(x)) $ shows['release_monthday'] = shows['release_monthday'].dropna().apply(lambda x: int(x))
(details['Average Rating'] == 0).value_counts()
reviews_sample.to_csv('text_preparation/abt_text_analysis_prepared.csv') $
df2=df2[df2.index!=2893]
round(autos["price"].describe())
                        'release_date': 'Released','runtime': 'Runtime', 'title': 'Movie Title', $                         'vote_average': 'Average Rating','vote_count': 'Number of Ratings'}, inplace=True)
with open('tweet_json.txt') as json_file:  $     data = json.load(json_file) $     for tweet in data[:4]: $         print(tweet)
unigram_feats = sentim_analyzer.unigram_word_feats(allNeg, min_freq=4) $ len(unigram_feats) $ sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)
from google.datalab.ml import TensorBoard $ TensorBoard().start('gs://eim-muse/analysis/hallelujah-effect/models/')
len(df.query('group == "treatment" and landing_page != "new_page"'))+len(df.query('group != "treatment" and landing_page == "new_page"'))
df_clean3.loc[1635, 'text']
precip_12mo_df=pd.DataFrame(precip_12mo) $ precip_12mo_df.head() $ mod1precip_12mo_df=precip_12mo_df.set_index('date') $ mod1precip_12mo_df.head()
X_trainset.shape $ y_trainset.shape $
ids
most_retweeted_tweeps_sum=tizibika.groupby('user')['retweets'].sum()
blame = \ $   blame_raw.raw.str.extract( $       "(?P<sha>.*?) (?P<path>.*?) \((?P<author>.* ?) (?P<timestamp>[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2} .[0-9]{4}) *(?P<line>[0-9]*)\) .*", $       expand=True) $ blame.head()
s3_upload.upload_file(local_orig, s3_bucket, s3_key_orig, $                          Callback=ProgressPercentage(local_orig)) $ s3_upload.upload_file(local_edit, s3_bucket, s3_key_edit, $                          Callback=ProgressPercentage(local_edit))
author_data.loc["leyroft"]
bild = bild[:5000] $ spon = spon[:5000]
data = data[data['processing_time']>=datetime.timedelta(0,0,0)]
print(temp_nc) $ for v in temp_nc.variables: $     print(temp_nc.variables[v])
df.iloc[0]
lda_tf2 = models.LdaModel.load(os.path.join(outputs, 'model_tf2.lda')) $ corpus_lda_tf2 = corpora.MmCorpus(os.path.join(outputs, 'corpus_lda_tf2.mm'))
rain_df.describe()
sorted_m3 = np.sort(m3,axis=None)[::-1].reshape(m3.shape) $ print("sorted m3: ", sorted_m3)
df_final.head(5)
np.r_[np.random.random(5), np.random.random(5)]
df_en[['PosOrNeg', 'polarity_vader']]
mytext=list(map(lambda tweet: remove_stop_words(tweet), support_or_not.text)) $ support_or_not.text=mytext
api_copy = df_filtered.copy() $ api_copy.info()
for i,val in enumerate(diff): $     if val == max(diff): $         print(i) $
images = pd.read_csv('image-predictions.tsv', sep ='\t')
set(df.funding_rounds)
y = model_df.score_str #labels $ X = model_df.drop(['score_str'], axis=1) #variables
df2[df2['converted']==1].shape[0]/df2.shape[0]
contract_history.head(50)
sub1 = sub1[sub1['final_solved']==0]
prec_group = prec_group.reindex(taxi_weather_df.index)
returns = data[tickers] / data[tickers].shift(1) - 1 $ returns = returns.fillna(method='ffill').dropna() $ returns.plot()
full_orig['MonthAdmit'] = full_orig['AdmitDate'].apply(lambda x: x.month)
dates=pd.date_range('2010-01-01', '2011-12-31')[int(df2.shape[0]*0.6):]
df2.query('landing_page == "new_page"').shape[0]
zstat_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print(zstat_score, p_value) $ print("The Pvalue for null Hypothesis is {}".format(p_value))
df_person = pd.read_sql_query("SELECT * FROM person", conn) $ df_grades = pd.read_sql_query("SELECT * FROM grades", conn) $ df_person.merge(df_grades, how="inner", left_on = "id", right_on="person_id")
X = train[feature_cols + ['author_popularity']] $ X_new = new[feature_cols + ['author_popularity']]
AAPL['MA20'] = AAPL['close'].rolling(20).mean() $ AAPL.head()
%timeit df.pivot(index='date', columns='item', values='status')
na_df.loc["a", "four"] = np.nan # fills as None since replacing a str $ na_df.loc["b", "four"] = None # fills as None $ na_df
print(action.shape) $ action.head()
consumer_key = ' ' $ consumer_secret = ' ' $ access_token = ' ' $ access_token_secret = ' '
female_journalists_retweet_summary_df = journalists_retweet_summary_df[journalists_retweet_summary_df.gender == 'F'] $ female_journalists_retweet_summary_df.to_csv('output/female_journalists_retweeted_by_journalists.csv') $ female_journalists_retweet_summary_df[journalist_retweet_summary_fields].head(25)
data2.dtypes
vectorizer = CountVectorizer(stop_words=stopWords, strip_accents = 'ascii' )
cols = [desc[0] for desc in cur.description] $ cols
to_be_predicted_Day4 = 43.11800827 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
train_ratio = 0.75 $ train_size = int(samp_size * train_ratio); train_size $ val_idx = list(range(train_size, len(df)))
X_train.columns
tweets_streamedDF.dtypes
trigram_converter = CountVectorizer(ngram_range=(3,3), token_pattern='(?u)\\b\\w+\\b') $ x3 = trigram_converter.fit_transform(review_df['text']) $ x3
df_birth.tail()
tree2conlltags(result)
archive_clean.info()
Base.prepare(engine, reflect=True)
cond = ((datatest.surface_covered_in_m2.isnull()) & (datatest.surface_total_in_m2.isnull())) $ rest = datatest[cond == False] $ train = rest[rest.surface_covered_in_m2.notnull()] $ rest = rest[rest.surface_covered_in_m2.isnull()] $ test = datatest[cond]
data3 = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=["Tweets"])
parsed_sierra_df.head()
df['Difference'] = df['Closed'] - df['Open']
df2.groupby(by=df2['TT']).count()
sum(trumpint.num_comments),sum(cliint.num_comments)
top_5_percent.index
print ('Summary:') $ print (summarize(text2, word_count=150))
df['timestamp'] = pd.to_datetime(df['timestamp']) df['timestamp'].dt.strftime('%m/%d/%Y') $ df['timestamp'].dt.strftime('%m/%d/%Y') 
nltk_Tokenize = [u' nltk_Tokenize'] $ nltk_Tokenize = ('utf-8') $ dir (nltk_Tokenize)
plt.plot(W,RMSE_list) $ plt.xticks(np.arange(min(W),max(W)+1,6.0)) $ plt.ylabel('RMSE') $ plt.xlabel('Window (months)') $ plt.title('RMSE by window')
train_bf, valid_bf, test_bf = add_features(train_b, valid_b, test_b)
X_test = scaler.transform(total_test_features.values) $ print(X_test)
submit.head(1)
%store -r extract_deduped_0501 $ extract_deduped = extract_deduped_0501.copy()
count
len(df2[df2['landing_page']=='new_page'])/len(df2)
department_df["Revenue_Range"] = grouped_dpt["Revenue"].transform('max') - grouped_dpt["Revenue"].transform('min') $ department_df
out_columns=[primary_temp_column] +incremental_precip_columns+general_data_columns +wind_dir_columns #columns to include in output $ save_name=Glacier.lower()+ Station + "_15min_"+"LVL2.csv" #filename $ save_pth=os.path.join(save_dir, save_name)
pattern = '(.*)(ID\d)(.*)' $ matchresult = re.match(pattern, 'WIDTH_ID1')
tf
soup.name
datatest['covered/total'] = datatest['surface_covered_in_m2']/datatest['surface_total_in_m2']
from pandas import to_datetime, TimeGrouper, set_option, read_csv $ set_option("display.max_rows",100) $ set_option("display.max_colwidth",500)
support[["amount", "committee_name_x"]].sort_values("amount", ascending=False).head(20)
tweets_p1_month['M_Y'] = tweets_p1_month.loc[:, 'Month'].astype(str) + '-' + tweets_p1_month.loc[:, 'Year'].astype(str)
data = {} $ data['nodes'] = nodes_lst $ data['links'] = links_r
knn.score(x_test,y_test > 0)
textList = tweets['text'][0:1000] $ textList= textList.tolist()
db.bookings.update_many({"status":"Inititated"},{"$set":{"status":"Init"}})
loan_stats.set_name('loan_status', 'loan_result') 
M7_2016_RMSE
print(autos['price'].unique().shape) $ print(autos['price'].describe()) $ print(autos['price'].value_counts().sort_index(ascending=False).head(20)) $ print(autos['price'].value_counts().sort_index(ascending=False).tail(20))
df.head()
precip = session.query(Measurement.date, Measurement.prcp).group_by(Measurement.date).\ $                     having(Measurement.date.like('2016%')).all() $ precip $
gf = open(processed_path+"grid.json","w") $ json.dump({"box":gps_box,"grid":gdef},gf) $ gf.close()
targettraffic['weekday'] = targettraffic['DATE_TIME'].apply(lambda x:x.weekday()) $ targettraffic['weekdayname'] = targettraffic['DATE_TIME'].apply(lambda x:x.weekday_name)
df['amount_cleanup'] = df.amount_initial.str.replace(',', '') $ df['amount_cleanup'] = df.amount_cleanup.str.replace('$', '') $ df['amount'] = df.amount_cleanup.astype(float)
path='Data\\Yelp Data' $ yelppath = [os.path.join(root, name) $             for root, dirs, files in os.walk(path) $             for name in files $             if name.endswith((".json", ".jsons"))]
age_gender_bkts.info()
fraud_df = fraud_df.merge(country_tz[['country','gmt_hour_avg']],on='country',how='left') $ fraud_df.fillna(0.0,inplace=True)
dr_2018 = dr_2018.resample('W-MON').sum() $ RNPA_2018 = RNPA_2018.resample('W-MON').sum() $ ther_2018 = ther_2018.resample('W-MON').sum()
df['Sale (Dollars)'] = df['Sale (Dollars)'].str.replace(r'[$,]', '').astype('float')
df.min(axis=1) # axis=1, row-wise
s = pd.Series(np.random.np.random.randn(5), index=list('abcde')) $ s
pd.set_option('max_colwidth', 100)
n_new = df2[df2['landing_page']=='new_page']['user_id'].nunique() $ n_new 
ridge2 = linear_model.Ridge(alpha=0.5) $ ridge2.fit(x3, y) $ (ridge2.coef_, ridge2.intercept_)
df_combined[['CA','UK','US']] = pd.get_dummies(df_combined['country'])
horizAAPL = AAPL.sort_index(axis=1) $ horizAAPL.head()
ibm_hr_final = ibm_hr_int.join(ibm_hr_cat_dum) $ for c in ibm_hr_final.columns: $     ibm_hr_final = ibm_hr_final.withColumn(c, ibm_hr_final[c].cast(IntegerType())) $ ibm_hr_final.printSchema()
apnew = df2.query('group == "treatment"').converted.mean() # actual p new $ apold = df2.query('group == "control"').converted.mean() # actual p old $ actualdiff = apnew - apold $ actualdiff
for lig in ligatures: $     re_lig = re.compile(lig) $     for pape in all_papers: $         pape['article'] = re.sub(re_lig, ligatures[lig], pape['article'])
person = "KianMcIan" $ person_counts = df[df["screen_name"]==person].groupby(TimeGrouper(freq='30min')).agg({"id":"count"}).rename(columns={"id":"count"}) $ person_counts.reset_index(inplace=True) $ person_counts.head()
edftocsv.edftocsv(inputFile, outputFileHeaders, outputChanHeaders, outputData, True)
messy.to_csv('data/weather_yvr_cleaned.csv', index=False)
namesList[:10]
df_bill_data[df_bill_data['patient_id'] == sample_repeat]['amount'].sum()
df_regression=df2 $ df_regression.head() $ df_regression.group.unique() $ df_regression['intercept']=1 $ df_regression[['control', 'treatment']] = pd.get_dummies(df['group']) $
A = int(dframe_team.iloc[0]['Draft_year']) # grabs the first year of 'Draft_year' $ dframe_team.set_index('Draft_year').reindex(range(A,2016)) # Sets a new index, starting with the first year of the team $ dframe_team = dframe_team.set_index('Draft_year').reindex(range(A,2016)).reset_index() $ dframe_team.head(10)
active_users.shape # there are only 7479 active users out of 22884 signed users $
hist = model.fit(X_train, Y_train, batch_size=30, epochs=400, verbose=0, validation_split=0.2)
dem["date"] = dem.apply(lambda line: datetime.strptime(str(line['date']),"%m/%d/%y").date(),axis=1)
most_common_dog = df_merge.p1.value_counts().head() $ most_common_dog
df6 = df4.where( (hours > 10) & (hours < 13)) # show lunch data rows only $ df6 = df6[df6['BG'].notnull()] $ df6 # got same data as previous technique
matches._get_numeric_data().columns.tolist()
full['<=30Days'] = full['WillBe<=30Days'] $ full.drop('WillBe<=30Days',axis=1,inplace=True)
g = sns.jointplot(y="PRICE", x="LIVING_GBA", data=condo_6, kind="hex", size=8)
brands = autos["brand"].value_counts(normalize=True)[autos["brand"].value_counts(normalize=True)>0.02].index.tolist() $ brands
the_hospitals = df.loc[:20,'id_num'] $ the_hospitals  = list(set(the_hospitals)) $ len(the_hospitals) $ show_sepsis_combined(0)
auth = tweepy.OAuthHandler(config.consumer_key, config.consumer_secret) $ auth.set_access_token(config.access_token, config.access_token_secret) $ api = tweepy.API(auth)
df.set_index(['Timestamp'], inplace = True)
subwaydf.iloc[114053:114065] #this high number seems to come because the time increments from 4/2 to 4/4.
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=35000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
df3['WEEK'] = df3['DATE'].dt.week
nu_fiss_xs = fuel_xs.get_values(scores=['(nu-fission / flux)']) $ print(nu_fiss_xs)
total_users = df2['user_id'].nunique() $ converted_users = df2[df2['converted'] == 1].count() $ conversion_prob = converted_users/total_users $ print(conversion_prob)
submit.to_csv("properati_dataset_sample_submision.csv", index = False)
table1.describe()
lines = ['blue','green','orange','red']
All_tweet_data_v2.timestamp= All_tweet_data_v2.timestamp.str.replace(' \+0000','') $ All_tweet_data_v2.timestamp.head() $ All_tweet_data_v2.info()
print ('First Five Rows in the Training Data:\n',training_active_listing_dummy[0:5])
foo = pd.Series([np.nan, -3, None, 'foobar']) $ foo
p = p.to_crs({'init':'epsg:4269'})
from sklearn.model_selection import cross_val_score $ scores = cross_val_score(estimator, volume_weather[features], volume_weather['y']) $ scores
bruins.head()
files8['Tenure']=files8.EndDate-files8.StartDate $ files8.head()
sum(austin['distance_travelled']==0) $
df_sp = df_data[df_data.CIDADE=='S.PAULO']
url = 'https://mars.nasa.gov/news/'
pct = pct.drop('precinct',axis=1)
autos['price'] = (autos['price'] $                  .str.replace('$','') $                  .str.replace(',','') $                   .astype(float) $                  )
df_all.xs(299,level='fk_loan')
model_df['weekday'] = "x" $ model_df['day_period'] = "x"
stories.dropna(axis=1).shape
top_brand
from arcgis.features import FeatureLayerCollection
inputPath1 = "graphdata/people_images.csv" $ vertices = sqlContext.read.options(header='true', inferSchema='true').csv(inputPath1) $ vertices.show(5)
ideas.columns  # Inspecting progress
print('Donations: \n','Rows: ', donations.shape[0], 'Columns: ', donations.shape[1]) $ print(donations.dtypes) $ print('Donors: \n','Rows: ', donors.shape[0], 'Columns: ', donors.shape[1]) $ print(donors.dtypes) $
plt.plot(ds_cnsm['time'],ds_cnsm['met_salsurf_qc_results'],'.') $ plt.title('CP01CNSM, OOI QC Results SSS') $ plt.ylabel('Salinity') $ plt.xlabel('Time') $ plt.show() $
pandas_df = pandas_df.drop(['level_0','index'],axis=1,errors='ignore')
%%time $ svc = SVC(random_state=20, C=10, decision_function_shape='ovo', kernel= 'rbf') $ svc.fit(X_tfidf, y_tfidf) $ svc.score(X_tfidf_test, y_tfidf_test)
dirs = list(Path('../../../data/talking').iterdir()) $ dirs
us.loc[us['country'].str.len() == 2, 'country'].value_counts(dropna=False)
    ufo_pandas = pandas_df.toPandas() $     ufo_pandas.describe()
df_total = pd.concat([weekday_df, month_df, df_total], axis=1)
random_tokens = random_sample['tokens'].tolist() $ random_corpus = createCorpus(random_tokens)
wrd_api_clean
len(email_gender_unique[email_gender_unique['request.gender'] == 0])
category_dummies = df[list(categories)]
X = pd.get_dummies(reddit['Subreddits']) $ y = reddit['Above_Below_Median'] $ from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
pd.to_datetime([1])
print y_train
pd.crosstab(Vy.reshape(-1),prediction)
a = b = 0 $ print(a, b)
autos["num_photos"].value_counts()
df_ad_airings_5.dropna(subset= ['location'], inplace=True)
sp500.loc['MMM']
status.shape
image_clean = image_clean.drop_duplicates(subset=['jpg_url'], keep='last')
scowl3[:10]
df.head()
tx, ty = tsne[:,0], tsne[:,1] $ tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx)) $ ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty))
e = user_log_counts.groupby(["msno"], as_index=False)["logs_count"].sum() $ e[e.msno == '29V0Jm3Xli1dy9UFeEL/BH2EMOr62DgeGLeKAKfE07k=']
df.to_csv("BaseFinale")
np.random.seed(123456) $ dates = ['2014-08-01','2014-08-02'] $ ts = pd.Series(np.random.randn(2),dates) $ ts
sl.second_measurement.sum()
cnn_fox = [] $ cnn_fox.append(cnn_data) $ cnn_fox.append(fox_data) $ cnn_fox_data = pd.concat(cnn_fox)
nx.info(tweetnet)
p_diffs = new_page_converted - old_page_converted $ p_diffs
df = df_questions[df_questions.columns-["test attempt","user id"]]
print (r.json())
dtypes={'date':np.str,'type':np.str,'locale':np.str,'locale_name':np.str,'description':np.str,'transferred':np.bool} $ parse_dates=['date'] $ holidays_events = pd.read_csv('holidays_events.csv', dtype=dtypes, parse_dates=parse_dates) # opens the csv file $ print("Rows and columns:",holidays_events.shape) $ pd.DataFrame.head(holidays_events)
rmse_CBoE $
df.index.name = 'DateTime' $ df
n_old = df2.query('landing_page == "old_page"')['user_id'].nunique() $ n_old
red_test = pca.transform(test_float_pca) $ red_test
cashflows_act_investor[cashflows_act_investor.id_loan==1545]
df['names'].value_counts()
autos["price"].value_counts().sort_index(ascending=True).head(10)
archive_clean=pd.merge(twitter_archive_clean, image_predictions_clean, on='tweet_id', how='left')
df.index
ls = pd.to_datetime([obs['ResultDateTime'] for obs in results]) $ print('Start Date: {0} | End Date: {1}'.format(ls.min().isoformat(), ls.max().isoformat()))
find_emoji = emoji_pattern.findall(text) $ print(find_emoji)
conn.addtable(table='iris_db', caslib='casuser', $               **dbdmh.args.addtable)
drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7
pax_raw.paxcal.value_counts() / len(pax_raw)
print('Logistic regression F1_score: ', f1_score(y_final, lr_predicted)) $ print('Logistic regression Jaccard: ', jaccard_similarity_score(y_final, lr_predicted)) $ print('Logistic regression LogLoss: ', log_loss(y_final, lr_predicted))
df[df['Re_Tweets'] == max_Re_Tweets]
features = ['Name','AnimalType', 'AgeuponOutcome', 'Breed', 'SexuponOutcome_Neutered Male', 'SexuponOutcome_Spayed Female', \ $             'SexuponOutcome_Unknown']
import datetime as dt $ now = dt.date(2018,1,1)
mean_absolute_percentage_error(y_test, y_pred)
well_behaving_clients =\ $     set(subset.filter(lambda p: p.get("clientId") not in misbehaving_clients).map(lambda p: p.get("clientId")).collect()) $ all_clients = misbehaving_clients + list(well_behaving_clients)
autos.shape[0]
cercanasAfuerteApacheEntre50Y75mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 50) & (cercanasAfuerteApache['surface_total_in_m2'] < 75)] $ cercanasAfuerteApacheEntre50Y75mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre50Y75mts.apply(descripcionDistancia2, axis = 1) $ cercanasAfuerteApacheEntre50Y75mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
dr_test_data = duration_test_data[duration_test_data['Specialty'] == 'doctor'] $ RN_PA_test_data = duration_test_data[duration_test_data['Specialty'] == 'RN/PA'] $ therapist_test_data = duration_test_data[duration_test_data['Specialty'] == 'therapist']
prs.to_csv("prs.csv") $ pr_comments.to_csv("pr_comments.csv") $ issues.to_csv("issues.csv") $ issue_comments.to_csv("issue_comments.csv")
agency_group = data.groupby('Agency') $ agency_group.size().plot(kind='bar')
res.summary()
plt.hist(null_vals)
clean_stations.to_csv('../clean_stations.csv', index=False)
print("dfDay = ",dfDay['Contract Value (Daily)'].sum(), "dfDay Project Count = ", dfDay['Project Name'].nunique())
len(df2.query('group=="treatment"'))/len(df2)
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
d.join(df).head(10)
df2[((df2['group'] == 'treatment') ==(df2['landing_page'] == 'new_page')) == False].shape[0]
manager.image_df['p_hash'].isin(tree_features_df['p_hash']).describe()
fdist.plot(100, cumulative=True)
ti_suning.rename(columns={'review_image':'image_url','harvest_product_description':'product_description','retailer_product_code':'rpc','user_id':'username'}, inplace=True) $ ti_suning['store'] = 'Suning' $ ti_suning = ti_suning[ti_clm] $ ti_suning.shape
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\ $ filter(Measurement.station == 'USC00519281').all() $
inspector = inspect(engine) $ inspector.get_table_names() $
df.Standard_Plat.value_counts()
dateset.reset_index(inplace=True)
set(status_keys) - set(user_status_keys)
datatest.loc[datatest.place_name == "Malvinas Argentinas",'lat'] = -34.482412 $ datatest.loc[datatest.place_name == "Malvinas Argentinas",'lon'] = -58.717893
import pandas as pd $ import re $ import pprint as pp
dftouse_four.head()
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'larger') $ z_score, p_value
df
to_be_predicted_Day1 = 21.16 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
cs.hist(cumulative=True, normed=100, bins=100, histtype='step')
byMonth = df.groupby('Mo')
autos.price.value_counts().sort_index(ascending=False).head(20)
users = pd.read_csv('../data_20180120/customer.csv').fillna('') $ users.head()
datasets_ref = pd.read_csv('../list_of_all_datasets_dgfr/datasets-2017-12-13-18-23.csv', sep=';') $ datasets_slug_id = datasets_ref.set_index('slug')['id'].to_dict() $ datasets_id_slug = datasets_ref.set_index('id')['slug'].to_dict()
genes.head()
check_out('2016-12-18')
df_daily2 = df_daily.groupby(["C/A", "UNIT", "STATION", "DATE"]).DAILY_ENTRIES.sum().reset_index()
df.query('converted == 1').user_id.nunique() / df.user_id.nunique()
df3 = pd.concat([non_align_1, non_align_2]) $ df2 = df.drop(df3.index) 
df = pd.read_csv("data3.csv") $ df.head(n=10)
from datetime import datetime $ date = nc.num2date(time, 'hours since 1800-01-01 00:00:0.0') $ ts = pd.Series(date, index = date) $ ts.head()
dicttagger_food = DictionaryTagger(['food.yml'])
high_rev_acc_opps_net.head()
learn.save("dnn100")
tree_tunned.fit(X_train, y_train)
remote_data = xr.open_dataset('http://hydromet-thredds.princeton.edu:9000/thredds/dodsC/UrbanRainfall/Phoenix.nc') $ print remote_data
reduced_df.to_pickle(pretrain_data_dir+'/pretrain_data_01.pkl')
summary.columns
model_name = "300features_40count_10context" $ model.save(model_name)
start=time.time() $ def timestamp_to_strftime(timestamp): $     return pd.to_datetime(timestamp, unit='ms', utc=True).astimezone('Asia/Shanghai').strftime('%Y-%m-%d %H:%M:%S') if timestamp else '' $ pd_data.createdAt.map(get_datetime_from_microsecond) $ print(time.time()-start)
df['incident_zip'] = df.incident_zip.astype(int) $ df = df[df['incident_zip'] < 12000 ]
data['total_reactions']=data['react_angry']+data['react_haha']+data['react_like']+data['react_love']+data['react_sad']+data['react_wow'] $ data
df_cryptdex.head(5)
news_df = (news_df $            .loc[news_df['num_reactions'] - $                 news_df['num_likes'] >= 10,:])
S_lumpedTopmodel.initial_cond.filename
vocab = vectorizer.get_feature_names() $ dist = np.sum(train_data_features, axis=0)
ADNI_diagnosis_data_description[['FLDNAME','TEXT']][ADNI_diagnosis_data_description['FLDNAME']=='DX']
tweet_text.head(10)
tmp_df = geocoded_df.copy() $ tmp_df[[x for x in tmp_df.columns if x.endswith('Date')]] = tmp_df[[x for x in tmp_df.columns if x.endswith('Date')]].astype('str') $ tmp_df['Judgment.Date'].head()
precip_data_df.head(3)
csv = 'F:/web mining/webmining project/crawler/annotation_data/samsung/LR_clean_tweet.csv' $ my_df = pd.read_csv(csv,index_col=0) $ my_df.head()
from sklearn.linear_model import LogisticRegression $ logmodel = LogisticRegression() $ logmodel.fit(X,y)
df_run['zone'].isnull().sum()
betweenness_dict = nx.betweenness_centrality(G) # Run betweenness centrality $ nx.set_node_attributes(G, betweenness_dict, 'betweenness') $
trump.describe()
results.summary()
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 4) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
df_joined.groupby('country').converted.mean()
messages_clean = [x for x in messages_clean if x] $ print "Now, number of messages is %s" % len(messages_clean) $ messages_clean[590:595] $
import statsmodels.api as sm $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = df2.query("group == 'control'").shape[0] $ n_new = df2.query("group == 'treatment'").shape[0]
feats = {} # a dict to hold feature_name: feature_importance $ for feature, importance in zip(X_test.columns, rf.feature_importances_): $     feats[feature] = importance #add the name/value pair $ importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})
print(autos['price'].describe()) $ print(autos['price'].unique().shape) $
pres_df['hour_aired'].value_counts()
df_clean.info()
full['Readmitted'] = full.groupby(['Patient'])['Patient'].transform('count') $ full['Readmitted'] = full['Readmitted'].map({2:1,1:0})
m1 = VAR(queries=['de','het'], timefield = 'META.ADDED', granularity = 'day')
pwd
grid_df = pd.DataFrame(data = us_grid) $ grid_df.columns = ['glon', 'glat'] $ grid_df = pd.concat([grid_id, grid_df], axis=1) $ grid_df.head() $
len(porn_ids)
autos.describe(include = "all")
 X = pd.merge(X,title_tokens, left_index=True, right_index=True)
data[['Sales']].resample('D').mean().rolling(window=200, center=True).mean().plot()
tweets['retweeted'] = tweets['retweeted_status'].notna() $ tweets = tweets.drop(columns=['retweeted_status'])
imgp.describe()
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine,inspect, func
n_old = df2.query('landing_page == "old_page"').user_id.count() $ n_new = df2.query('landing_page == "new_page"').user_id.count()
tweets_df_clean[tweets_df_clean['id'].duplicated()]
S.decision_obj.simulStart.value = "2006-07-01 00:00" $ S.decision_obj.simulFinsh.value = "2007-08-20 00:00"
station_distance.tail() $ station_distance.shape
def unix_to_datetime(i): $     u = datetime.datetime.fromtimestamp( $         int(i) $     ).strftime('%Y/%m/%d %H:%M') $     return u
df = austin.pivot_table(index='yr_mo', columns='weekday', values='started_on', aggfunc='count') $ df = df.reindex_axis(['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat'], axis=1) $ print(df)
vhd['season'] = vhd.index.str.split('.').str[0] $ vhd['term'] = vhd.index.str.split('.').str[1]
churned_ordered['start_date'] = pd.to_datetime(churned_ordered['scns_created'].apply(lambda x:x[0])).dt.strftime('%Y-%m')
INQ2017.Create_Date.dt.month.value_counts().sort_index()
rf = RandomForestClassifier() $ rf.fit(X_train_all, y_train) $ rf.score(X_test_all, y_test)
len(train_texts), len(val_texts)
duration_df = duration_df[duration_df['AppointmentDuration'] <= 90]
df["weekend_pickup"] = (df["pickup_dow"].isin([4, 5, 6])).astype(int)
prepared_train = prepared_train[prepared_train.views < 600000]
df = df[df["current_state"].isin(["finished", "cancelled", "no_showed", "started", "payment_declined_cancelled"])]
actual_payments[actual_payments.fk_loan==27]
autos["brand"].value_counts(normalize=True)[:10]
grouped2 = df.groupby(['weekday','hour']) $ cntdf2 = pd.DataFrame({'count' : grouped2.size()}).reset_index() $ piv2 = cntdf2.pivot('weekday','hour','count') $ sns.heatmap(piv2, cmap="YlGnBu").set_title('Tweet Frequency')
df2[df2['user_id'] == 773192] #check
new_page_converted = np.random.binomial(n_new,p_new) $ new_page_converted
import pandas as pd $ import numpy as np $ df = pd.DataFrame(np.random.randn(4,3),columns = ['col1','col2','col3']) $ df
tipsDF = pd.read_csv("yelp_tips.csv", encoding = 'latin-1') $ tipsDF.head()
df.head(10)
S_lumpedTopmodel.forcing_list.filename
n_old = df2.query("group == 'control'").shape[0] $ n_old
walmart= pd.Period('2017Q1', freq='Q-JAN') # We have to specify Q ending in January
np.exp(-0.0333)
%load "solutions/sol_2_15.py"
airbnb_df.filter(like='host', axis=1).head()
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn; seaborn.set()  # set plot styles
conf_matrix = confusion_matrix(y_tfidf_test, svc.predict(X_tfidf_test), labels=[1,2,3,4,5]) $ conf_matrix # this is very bad, all results are classified as 5 point.
ValidNameEvents.head(1)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ print(z_score, p_value)
df_variables = pd.read_csv("../01_data preprocessing/data new/variables.csv",encoding="utf-8",sep=",")
read_table("HistoricalS_and_P500.csv", sep=",", index_col=0, parse_dates=True, converters={0:convert_date})
InfinityWars_Predictions = infinity.join(InfinityWars_PRED_df).encode('utf-8').strip() $ InfinityWars_Predictions.columns = ['tweet','language','prediction'] $ InfinityWars_Predictions = InfinityWars_Predictions.drop(['language'],axis = 1)
inconvertible = dep.show_overview() $ inconvertible
res_val = gridCV.predict_proba(X_val) $ res_val = res_val[:,-1] $ res_val[res_val>0.45] = 1 $ res_val[res_val!=1] = 0 $ print(res_val)
np.datetime64('2015-07-04')
print('Log Reg score: {:.4f}'.format(scores))
new_page_converted = np.random.binomial(1, p_new,n_new) $ p_new_sample = new_page_converted.mean() $ p_new_sample $
autos['last_seen'].str[:10].head()
import pandas as pd $ CSV_PATH = 'data/Combined_News_DJIA.csv' $ df = pd.read_csv(CSV_PATH, index_col=0) $ df.index = pd.DatetimeIndex(df.index)
cr = df2.converted.sum()/df2.converted.count() $ cr
dfdinner = df[(df['TIME'] == '19:00:00') | (df['TIME'] == '20:00:00')] $ dfstationmeandinner = station_mean(dfdinner)
type_city=pd.Series(totol_fare["type"]) $ fare_city=pd.Series(totol_fare["fare"]) $ colors = ["gold","lightskyblue","lightcoral"] $ explode = (0, 0, 0.1)
j.head()
print ("Number of rows in dataset:",len(df))
(ggplot(all_lum.query("subject=='VP3'"),aes(x="td",y="gx",color="subject"))+stat_summary(geom="line"))
vacancies.head()
md2 = TextData.from_splits(PATH, splits, bs)
html_table_marsfacts = html_table_marsfacts.replace('\n', ' ') $ html_table_marsfacts
words = text.split() $ print(words)
kimanalysis.listfiles(result)
endpoint_instance = wml_credentials['url'] + "/v3/wml_instances/" + wml_credentials['instance_id'] $ header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken} $ response_get_instance = requests.get(endpoint_instance, headers=header) $ print response_get_instance $ print response_get_instance.text
min(TestData.Lead_Creation_Date_clean), max(TestData.Lead_Creation_Date_clean)
train_ratio = 0.75 $ train_size = int(samp_size * train_ratio); train_size $ val_idx = list(range(train_size, len(df)))
map(get_all_tweets, influencers)
new_crs.mean()
from IPython.display import Image $ from IPython.core.display import HTML $ Image(url= "https://www.tensorflow.org/images/softmax-nplm.png") $
git_log['author'].value_counts().size
countries_df  = pd.read_csv('./countries.csv') $ countries_df.head()
may_acj_data.head()
np.random.seed(123456) $ dates = pd.date_range('8/1/2014', periods = 10) $ s1 = pd.Series(np.random.randn(10), dates) $ s1[:5]
vec2 = TfidfVectorizer(stop_words=stopword_list) $ vec2
df2['active_bin'] = df2['days_active'].map(lambda x: 1 if x > 16.38 else 0)
lv_workspace.get_filtered_data(step = 2, subset = 'B') # water_body and subset keywords does notseem to work, all waterbodies are returned
pd.merge(crimes, weather, on='date', how='left').head()
df_CLEAN1B.head(5)
turnstiles = df.groupby(['STATION','C/A','UNIT','SCP']) $ print('There are {} unique turnstiles.'.format(len(turnstiles))) $
!image=my_model:v1.0.0 $ !docker build ./container --tag $(image)
sf_dense_zip = ["94102", "94103", "94104", "94105", "94107", "94108", $             "94109", "94110", "94111", "94112", "94114", "94115", $             "94116", "94117", "94118", "94121", "94122", "94123", $             "94124", "94127", "94131", "94132", "94133", "94134"]
plt.rcParams['axes.unicode_minus'] = False $ dta_58.plot(figsize=(15,5)) $ plt.show()
records2 = records.copy()
zipped = zip(selected_feature_updated[1:], model.feature_importances_[1:] ) $ zipped_dict = dict(sorted(zipped, key=lambda x: -x[1])[:3]) $ list(zipped_dict.keys())
trump.info()
sandwich_ratings.head(3)
tweet_json.source.value_counts()
gdf = gdf.copy() $ gdf['length'] = gdf['end'] - gdf['start'] + 1 $ gdf.head()
users = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/users.csv') $ users.head()
session = Session(engine)
s3_files[0][0:-4]
!rm train_cont.zip $ !rm test_cont.zip
combined = db1 $ combined = combined.append([db2,db3,db4,db5,db6,db7,db8,db9,db10,db11,db12,db13,db14,db15,db16,db17,db18,db19]) $ combined=pd.DataFrame(combined) $ combined.head()
count_vectorizer = CountVectorizer() $ X_train = count_vectorizer.fit_transform(message)
test_preds = lr.predict(test_features)
import requests $ from bs4 import BeautifulSoup $ import re
data = pd.read_csv('barbsList99.csv')
df_predictions_clean.p1 = df_predictions_clean.p1.str.title()
w_change = w - w.shift(1) $ w_change[coins_infund].T.sum().plot() $ plt.title('Total weight change per month') $ plt.show()
print("mean_new:",np.mean(df_concat_2.message_likes_rel))
mb.loc['Proteobacteria']
fed_reg_dataframe[fed_reg_dataframe.index > '2017-01-20']
from nltk.corpus import stopwords $ stop = stopwords.words('english') $ [w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] $ if w not in stop]
Base.classes.keys() $
df.drop([1899], inplace = True)
df2.converted.mean()
import numpy as np $ import pandas as pd $ import math $ from ipywidgets import interact, interactive, fixed, interact_manual, VBox, FloatSlider $ import ipywidgets as widgets
a = np.arange(1, 5); np.sqrt(a)
df4[df4['country_UK']==1]['converted'].mean()
X = df2[['intercept', 'ab_page', 'CA', 'US', 'ab_page*CA', 'ab_page*US']]
data.loc[(80, slice(None), 'put'), :].iloc[0:5, 0:4]
df2['intercept'] =1 $ mapper = {'treatment':1, 'control':0} $ df2['ab_page'] = df2.group.map(mapper)
lr=1e-3 $ lrs = lr
train['default'] = np.where(train.loan_status == 'Charged Off', 1, 0)
start_date = '2014-10-20' $ end_date = (now + datetime.timedelta(days=180)).strftime('%Y-%m-%d') $ model_dates = [today, '2017-01-01', '2016-01-01']
text = ["Este","tweet", "no", "es", "positivo"] $ get_negation_features(text)
findname('This is a Sizzlin Menorah spaniel from Brooklyn named Wylie. Lovable eyes. Chiller as hell.')
plot = dfPriceCalculations.plot(title='Peak and Base Price per Day') $ plot.set(ylabel='Price') $ plt.xticks(rotation=45)
len(sentences)
df_h1b_ft_US_Y = df_h1b_ft_US_Y.drop(incorrect_naics_indices)
model.predict(numpy.array([[3, 92.6, 109.3, 2, 12, 26],[2, 10.4, 43.5, 3, 26, 5]]))
drop_columns_list = ['date','serial_number','failure','fails_soon','seq_id','max_work_day','final_failure'] $ X_train = df_train.drop(columns=drop_columns_list) $ X_test = df_test.drop(columns=drop_columns_list) $ Y_train = df_train.iloc[:,5] $ Y_test = df_test.iloc[:,5]
X.describe()
df.dropna()
path =r'Data/ebola/sl_data' $ sl =read_data(path, 'variable', 'sl') $ frames = [guinea, liberia, sl] $ concatenated = pd.concat(frames) $
df4=df_new.drop(['CA','aa_page'], axis=1) $ df4['intercept']=1 $ lmm=sm.OLS(df4['ab_page'],df4[['intercept','UK','US']]) $ result1=lmm.fit() $ result1.summary()
from scipy.stats import norm $ norm.cdf(z_score),norm.ppf(1-(0.05/2)) $
consumer_key = config.consumer_key $ consumer_secret = config.consumer_secret $ access_token = config.access_token $ access_token_secret = config.access_token_secret
so_head.index
is_business_id_unique = True # put your final answer True/False here
components3
autos.describe(include='all')
print(f'dataframe shape: {playerAttr.shape}') $ print(playerAttr.columns) $ playerAttr.head(3)
data_scrapped = pd.read_csv("data_scraped.csv")
X_mice['age'] = np.log(X_mice['age'])
log_mod_new = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US']]) $ results_new = log_mod_new.fit() $ results_new.summary()
%matplotlib inline $ import matplotlib.pyplot as plt # this imports the plotting library in python}
date_column = sets_node['date'][DATA] $ type(date_column)
wrd_clean['rating_numerator'].describe()
idx = pd.IndexSlice
print('Example of the original tweets for the word "{}" that was detected as an event:'.format(new_events.iloc[0].hashtag)) $ dictionary[new_events.iloc[0].hashtag].head(10)
bow_df.columns = ['col'+ str(x) for x in bow_df.columns] $ tfidf_df.columns = ['col' + str(x) for x in tfidf_df.columns]
df[df['Complaint Type'] == 'Illegal Fireworks']['Unique Key'].groupby(df[df['Complaint Type'] == 'Illegal Fireworks'].index.dayofyear).count().plot()
import sys
df2.drop(df2[df2['user_id'].duplicated(keep = False)].index[0], inplace = True)
foo = events.groupBy(["user_id", "event_date"]).count()#.agg( count("user_id"), count("event_date") )
lda_tf2.save(os.path.join(outputs, 'model_tf2.lda')) $ corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lda_tf2.mm'), corpus_lda_tf2)
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
clf = LogisticRegression() $ clf.fit(X_train, y_train)
df.describe()
1/np.exp(-0.0149)
sentiments_pd["Date"] = pd.to_datetime(sentiments_pd["Date"]) $ sentiments_pd.sort_values("Date", inplace=True) $ sentiments_pd.reset_index(drop=True, inplace=True) $ sentiments_pd.head()
S.decision_obj.stomResist.value = 'simpleResistance' $ S.decision_obj.stomResist.value
ab_data['user_id'].nunique()
test_df[(test_df.labels>.4) & (test_df.labels<.5)].text.iloc[10]
c_df.groupby('DEVICE_MODEL').size().shape[0]
with open('tfidf_vect_5_29.pkl', 'wb') as piccle3: $     pickle.dump(t_vect, piccle3)
df = pd.read_csv('twitter-archive-enhanced.csv')
data.loc[data['hired']==1].groupby('category').num_completed_tasks.mean()
support.amount.sum() / oppose.amount.sum()
model_lm = LogisticRegression() $ model_lm.fit(X_train, y_train) $ y_preds = model_lm.predict(X_test) $ confusion_matrix(y_test, y_preds)
X_train, X_test, y_train, y_test = train_test_split(X, $                                                     y, $                                                     test_size=0.3, $                                                     random_state=42)
plt.hist(p_diffs) $ plt.title('Sampling Distribution of 10,000 P-Differences') $ plt.ylabel('Frequency') $ plt.xlabel('P-Diffs')
n_old = df2.query("landing_page == 'old_page'")['landing_page'].count() $ n_old
recoveries[recoveries.fk_loan==921]
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt
def save_combined_df( df_to_save, file_name ): $     df_to_save.to_csv( f'../../data/{file_name}.csv' )
logit_countries2 = sm.Logit(df4['converted'], $                            df4[['ab_page', 'country_UK', 'country_US', 'intercept']]) $ result3 = logit_countries2.fit()
autos["num_pictures"].value_counts()
more_100_df['username'].value_counts()
df.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/ratings_v1.csv', sep=',', encoding='utf-8', header=True)
df.head()
tweet_hour['tweet_text'].apply(tweet_tokenizer.tokenize).head()
datatest.loc[datatest.place_name == 'Abril Club de Campo', 'lat'] = -34.802221 $ datatest.loc[datatest.place_name == 'Abril Club de Campo', 'lon'] = -58.164291
s519281 = session.query(weather.date, weather.tobs).\ $  filter(and_(weather.date.between('2015-01-01','2015-12-31'), weather.station == 'USC00519281')).\ $  order_by(weather.date.asc()).all() $ s519281
df.iloc[8]['Profile_Pic_URL']
bounds.max_latitude
len(years)
artistAliasDF[artistAliasDF.MisspelledArtistID == 1000010].show() $ artistAliasDF[artistAliasDF.MisspelledArtistID == 2082323].show()
cryptos.iloc[0]
walking_df.head()
mpiv.head()
predicted_outcome_first_measure = holdout_results[holdout_results.second_measurement==0].\ $ groupby('wpdx_id').cv_predictions.sum()
Plot_Boxplot(rankings['rank_change'])
def workflow(working_data, get_split, train_model, get_rmse,n_train = 250,n_test = 50,look_back = 1): $     X_train, Y_train, X_test, Y_test, scaler, start_point = get_split(working_data, n_train, n_test) $     model = train_model(X_train, Y_train, X_test, Y_test) $     RMSE, predictions = get_rmse(model, X_test, Y_test, scaler, start_point, working_data, n_train) $     return RMSE, predictions
new_page_converted = df2.query('landing_page == "new_page"').sample(n_new, replace=True)
data['age']
df_cal['is_all_day'].hist(bins=4)
weather_mean = weather_all.groupby('Station Name').mean() $ weather_mean
df_train.head
vect = TfidfVectorizer(ngram_range=(2,5), stop_words='english') $ summaries = "".join(una_tweets['text']) $ ngrams_summaries = vect.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(20)
pandas.Series([1,2,3], index=['foo', 'bar', 'baz'])
autos['price'].round(4) $ print(autos['price'].describe()) $ print(autos['price'].value_counts().sort_index().head()) $ print(autos['price'].value_counts().sort_index().head()) $
from sklearn.metrics import f1_score $ f1_score(y_test, yhat, average='weighted') 
autos.info() $ autos.head()
bwd.drop('store_nbr', 1, inplace=True) $ bwd.reset_index(inplace=True) $ fwd.drop('store_nbr', 1, inplace=True) $ fwd.reset_index(inplace=True)
ad_group_performance['CTR'] = ( $     ad_group_performance['Clicks'] / $     ad_group_performance['Impressions'] $ ) $ ad_group_performance
tesla['Close'].describe() # the describe function will give us a statistical summary
print(len(train_df['channel'].unique()))
rf = RandomForestClassifier() $ rf.fit(X_train, y_train)
dta.shape
for column in trip_data_sub.columns: $     print column, ":", trip_data_sub[column].dtype
%load "solutions/sol_2_28.py"
df["extended_tweet"] = df["extended_tweet"].apply(lambda x: x["full_text"]  if type(x) == dict else x )
def extract_ncomments_from_result(result): $     comments = [x.text for x in result.find_all('a', {'class':'bylink comments may-blank'})] $     return comments
open_prs = PullRequests(github_index) $ open_prs.is_open() $ num_open_prs = open_prs.get_cardinality("id_in_repo").get_aggs() $ open_prs.get_cardinality("id_in_repo").by_authors("author_name") $ response = open_prs.fetch_aggregation_results()['aggregations']
lmscore.summary2()
meeting_status.info()
liberiaDfOld = liberiaDf.copy()
logit_mod = sm.Logit(df_new.converted, df_new[['intercept', 'new_page', 'UK', 'US']]) $ results = logit_mod.fit() $ results.summary()
df = pd.read_csv("ab_data.csv") $ df.head()
lr = LinearRegression() $ scores = cross_val_score(lr, X, y, cv=5) $ print('Cross-validated scores:', scores.mean())
numbers = {'integers': [1,2,3], 'floats': [1.1, 2.1, 3.1]} $ numbers_df = pd.DataFrame(numbers) $ numbers_df
DataSet['prediction'] = Predictions[:, 1]
df = pd.read_csv('data/test1.csv') $ df
df['_merge'].value_counts()
print("LED on") $ GPIO.output(18,GPIO.HIGH) $ time.sleep(1)
df2['converted'].mean() $
pp = rf.predict_proba(X_train) $ pp = pd.DataFrame(pp, columns=['The_Onion_Prob', 'VICE_Prob', 'GoldenStateWarriors_Prob'])
all_sets.cards["XLN"].columns
import matplotlib.pyplot as plt $ import numpy as np $ plt.plot(np.random.rand(50).cumsum())
from sklearn.preprocessing import LabelEncoder $ le = LabelEncoder() $ cat_var = ["VendorID","Store_and_fwd_flag","RateCodeID","Trip_type ","Payment_type"] $ for var in cat_var: $     trip_data_sub[var] = le.fit_transform(trip_data_sub[var].astype('category')) 
html_table.replace('\n', '')
mod.model.n_features
from sklearn.linear_model import LinearRegression $ from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score $ import numpy as np
n_old = df2[df2['landing_page']=='old_page'].landing_page.count() $ n_old
autos["price_euro"].value_counts().head(20)
a[a.find(':') + 1:]
train['frequent_author'] = train.author.isin(frequent_authors.index).astype(int) $ train.groupby('frequent_author').popular.mean()
total_df.head()
from sklearn.pipeline import Pipeline
type(new_df['created_time'][0]) $
df.groupby('converted').count()
autos["odometer"] = autos["odometer"].str.replace("km","") $ autos["odometer"] = autos["odometer"].str.replace(",","") $ autos.odometer = autos.odometer.astype(float) $ autos.rename({"odometer":"odometer_km"}, axis =1, inplace = True)
new_reps.apply(lambda x : x.isnull().sum())
new_texas_city.tail(10)
twitter_goodreads_users_df.isnull().sum()
data2['date'] = pd.to_datetime(data2['date'],unit='s')
np.sqrt(np.mean((actual.values - imputed.values)**2))
trunc_df.loc[list(airbnb_10)]
forest_clf = RandomForestClassifier(random_state=0) $ forest_clf.fit(X_train, Y_train)
df_merged[df_merged['link.domain'] == 'bit.ly']['link.domain_resolved'].value_counts().head(25)
autos
df_B.to_csv("classB.csv",index=None)
prop['parcelid'].nunique(dropna=False)
le.fit(df2['category']) $ df2["cat_num"] = le.fit_transform(df2['category'])
df['2014']
df
df2=df
with open('data/image-predictions.tsv', 'wb') as file: $     file.write(r.content)
T = 0 $ folder = 'trainW-'+str(T) $ train = pd.read_csv('../../input/preprocessed_data/trainW-{0}.csv'.format(T))[['msno']] $
r=df.rolling(window =3,min_periods=1) $ r['A'].aggregate(np.sum)
yelp_dataframe.head(2)
Base = automap_base() $ Base.prepare(engine, reflect=True) $
n_old = len(df2[df2['landing_page'] == 'old_page']) $ print (n_old)
cbow_m5 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 30, size=100)
new_page_converted = np.random.choice([0,1], size = n_new, p = [1-pnew, pnew]) $ print(new_page_converted)
train.head(3)
users.to_csv('data/2017/users.csv', index=False) $ repos_users.to_csv('data/2017/repos-users-geocodes.csv', index=False) $ repos_users.to_csv('data/2017/repos-users.csv', index=False)
nitrodata = pd.merge(left=sites, $                      right=nitrogen, $                      how='right', $                      on='MonitoringLocationIdentifier') $ nitrodata.shape
new.to_csv('dates.csv') $ test.to_csv('count.csv')
df_new['us_intercept'] = df_new['country'].replace(('US','UK','CA'),(1,0,0)) $ lm = sm.OLS(df_new['converted'],df_new[['intercept','us_intercept']]) $ lm.fit().summary()
total.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_3/target_winter2017_night2.csv',index=False)
df_clean=df_clean.drop(['contributors',         $ 'coordinates',         $ 'created_at',           $ 'entities','retweeted'  , 'extended_entities',          $ 'extended_entities','expanded_urls','geo','id_str','retweeted_status',],axis=1) 
r.iloc[:6][cols_to_see]
p_new = df2[df2['converted'] == 1].user_id.nunique() / df2.user_id.nunique() $ p_new
cc.market.describe()
new_cases_liberia_concat['Total_new_cases_liberia'] = new_cases_liberia_concat["New Case/s (Suspected)"] + new_cases_liberia_concat['New Case/s (Probable)']+new_cases_liberia_concat['New case/s (confirmed)'] $ new_cases_liberia_concat.head()
import pandas as pd $ df = pd.read_csv("data1.csv") $ print(df)
df2 = df.iloc[10:11,0:7].T $ print(df2)
cum_sum_percentage_payments = list(np.cumsum(percent_of_total_list)) $ print('cum_sum_percentage_payments is of type:',type(cum_sum_percentage_payments)) $ cum_sum_percentage_payments[:4] $ [i for i,v in enumerate(cum_sum_percentage_payments) if v > 50][:10]
df = pd.DataFrame({'Char':chars,'Target':y_true})
for values in ftr_imp: $     print(values)
df2 = df2.drop_duplicates('user_id')
del wordcloud
n_old = df2.query("landing_page == 'old_page'")['user_id'].count() $ n_old
dict_user_means = ...
p = getpass.getpass() $ try: $     conn = psycopg2.connect("dbname='cablegate' user='rsouza' host='localhost' password='{}'".format(p)) $ except: $     print("I am unable to connect to the database")
uniqueusers=df.user_id.value_counts() $ uniqueusers.size $
df.shape
with open('/Users/asapehrsson/dev/learn/notebook_sentry/failing_rich_push_events.json') as f: $     raw = json.load(f)
df_twitter_copy = df_twitter_copy[df_twitter_copy['in_reply_to_status_id'].isnull()]
lg_params= { $     "penalty" :['l1',"l2"], $     'C':[1,10,20,40,100,200,500], $     "n_jobs":[-1] $ }
clean_en_test_df = pd.DataFrame(parse_data(en_test_df['text'].tolist()))
crimes_by_yr_month = pd.DataFrame(datAll.groupby([datAll['year'],datAll['month']]) $                                .agg({'Offense_count':'sum'}))
to_be_predicted_Day2 = 17.69737131 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
dtypes={'date':np.str,'store_nbr':np.int64,'class':np.str,'family':np.str,'sum_unit_sales': np.float64,'no_items':np.float64,'no_perishable_items':np.float64,'items_onpromotion':np.float64,'dcoilwtico':np.float64,'city':np.str,'state':np.str,'type':np.str,'cluster':np.str,'transactions':np.float64} $ parse_dates=['date'] $ class_merged = pd.read_csv('class_merged_excl_hol.csv', names=['date','store_nbr','class','family','sum_unit_sales','no_items','no_perishable_items','items_onpromotion','dcoilwtico','city','state','type','cluster','transactions'],skiprows=1) # opens the csv file $ print("Rows and columns:",class_merged.shape) $ pd.DataFrame.head(class_merged)
Used_credit_card = pd.DataFrame(taxiData.Tip_amount) $ Used_credit_card.loc[Used_credit_card.Tip_amount > 0, "Tip_amount"] = 1 #Catergorize 1 as credit cards used for payment $ taxiData["Used_credit_card"] = Used_credit_card $ taxiData["Hour_of_day"] = newdf.Hour_of_day $ gpCreditCard = taxiData.groupby([taxiData.Used_credit_card]) $
house_data['date'] = pd.to_datetime(house_data['date'])
concat_2 = pd.concat([df1, df4, df2], sort=False) $ concat_2
weekdays = [] $ for i in range(0, len(date_df)): $     weekdays.append(date_df.iloc[i]['date'].strftime('%A'))
import inca 
volume_yearly = vol.groupby(vol.index.year).sum()
pd.options.display.float_format = '{:,.2f}'.format $ print("Shape of training is", X.shape) $ X.head().T
(p_diffs > p_diff_actual).mean()
df_cal = pd.read_csv("calendar.csv",index_col=None)
last_id(soup)
indexed = df.ix['2017-06-01 00:00:00':'2018-06-01 00:00:00'] $ indexed.head()
p_c=df2.query("group=='control'")['converted'].mean() $ p_c
url = 'http://www.espn.com/nfl/game?gameId=400927752' $ src = requests.get(url) $ score = extract_game_data(src.text) $ df = convert_gamedata_to_df(score)
df1=pd.read_csv("https://s3.amazonaws.com/tripdata/201610-citibike-tripdata.zip")  #October 2016
X_train_all.head()
df_melt['result_type'] = df_melt.apply(lambda x: get_correct_result_type(x), axis=1)
df2 = pd.read_csv('ab.csv') #reading the new dataset file
df.to_json("json_data_format_records.json", orient="records") $ !cat json_data_format_records.json
df_dd = pd.DataFrame() $ for date_col in df_datecols: $     dd_col = date_col + '_index_dd' $     df_dd[dd_col] = (df_uro['index_date'] - df_datecols[date_col]).dt.days
learner.fit(lrs / 2, 1, wds=wd, use_clr=(32, 2), cycle_len=1)
for row in df_Sessions: $     df_Sessions['rt'] = df_Sessions['text'].str[0:2] $ df_Sessions.head(n=20) $ df_Sessions['retweet?'] = np.where(df_Sessions['rt'] == 'rt',1,0) $ df_Sessions.head(n=5)
(pf.cost.sum()/100)/(max(pf.day)-min(pf.day)).days
def normalize_data_column(df): $     df_first_event = json_normalize(df.data) $     df_first_event = df_first_event.set_index(df.index) $     df_first_event['first_event'] = [False if pd.isnull(e) else e for e in df_first_event.first_event] $     return df.merge(df_first_event, left_index=True, right_index=True)
plt.hist(taxiData.Trip_distance, bins = 50, range = [150,taxiData.Trip_distance.max()]) $ plt.xlabel('Traveled Trip Distance') $ plt.ylabel('Counts of occurrences') $ plt.title('Histogram of Trip_distance') $ plt.grid(True)
merged1['Specialty'].value_counts()
current_len = len(youtube_urls.items()) $ print(current_len)
dfOther = df[df['Memo'].apply(returnCategory)=="Other"]
df['margin_val'].min()
df.iloc[7:11][['Genre','genre','Name','Release_Date','_merge','Standard_Plat','Platform','Year_of_Release']]
df["booking_user_agent"][~mask].value_counts() / df["booking_user_agent"][~mask].count()
(p_diffs > diff_obs).mean()
unique_users = len(df['user_id'].unique().tolist()) $ print('There are ' + str(unique_users) + ' unique users in the dataset.')
for df in (joined,joined_test): $     df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32) $     df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32) $     df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32) $     df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)
df.head()
image_clean.head(5)
df_ab_raw.shape
cat_outcomes['outcome_subtype'] = np.where(pd.isnull(cat_outcomes['outcome_subtype']), $                                            cat_outcomes['outcome_type'], $                                            cat_outcomes['outcome_subtype'])
len(train),len(test)
type(full_globe_temp.values)
len(users) $ users.drop_duplicates(subset='screenName', keep='last', inplace=True) $ len(users)
df = pd.read_sql('SELECT * FROM country WHERE country IN (\'Afghanistan\',\'Bangladesh\', \'China\')', con=conn) $ df
flight_phase.to_excel('flight_phase.xlsx') $ print("Done Writing!")
control_convert = df2[df2['group']== 'control'].converted.mean() $ print(control_convert)
query_sakhalin_bbox = tuple(zip(['lonl', 'latl', 'lonu', 'latu'], map(str, sakhalin_shp.bbox))) $ print(query_sakhalin_bbox)
df_clean.describe()
from datetime import datetime $ xml_in['days_diff_to_publication'] = (datetime.now() - xml_in['publicationDate']).astype('timedelta64[D]')
image_clean.drop(['p1','p1_conf','p1_dog','p2','p2_conf','p2_dog','p3','p3_conf','p3_dog'],axis=1,inplace=True)
df.min()
birth_dates.set_index("BirthDate_dt").loc["2014-01-01":,:]
print('Method 1: ', '\n') $ for i in set(csv_df['names']): $     print(i, ": ", csv_df['platform'][(csv_df['names']==i) & (csv_df['platform']=='Facebook')].value_counts(), '\n') $ print('Method 2 ', '\n') $ csv_df.groupby(['names', 'platform'])['date'].count()
Celsius.temperature.__class__
try: $     open('test_data//write_test.txt', mode='w').read() $ except Exception as error_message: $     print(error_message)
y1 = tweets1['handle'].map(lambda x: 1 if x == 'itsamandaross' else 0).values $ print max(pd.Series(y1).value_counts(normalize=True))
twitter_archive_clean.info()
df_uro = df_uro.drop(columns = ls_metac_colnames)
df_btc = pd.read_csv('C:\\Users\\dashs\\Downloads\\cryptocurrency_social-master\\bitcoinprice.csv')
import logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
df = pd.DataFrame(data, index = ['label 1', 'label 2', 'label 3']) $ df
import re
lemma = WordNetLemmatizer() $ def lemmatize(text): $     return ' '.join(lemma.lemmatize(word) for word in text.split()) $ texts = [text for text in cleaned_texts.values if len(text) > 1] $ lemmatized_texts = [lemmatize(text).split() for text in texts]
import lightgbm as lgb $ mdl =lgb.LGBMClassifier(boosting_type ='gbdt',objective ='binary', num_leaves =80, learning_rate =0.1) $ mdl.fit(X_train,y_train)
crime
ac_tr['deleted'].value_counts()
results1.summary()
if df.isnull().values.any() == False: $     print('No rows have missing values') $ else : print(df.info())
from sklearn.feature_extraction.text import CountVectorizer $ def bow_extractor(corpus, ngram_range=(1,1)): $     vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range, max_features = 5000) $     features = vectorizer.fit_transform(corpus) $     return vectorizer, features
print(html)
df_archive_clean.rating_denominator = 10
model = MixedInputModel(emb_szs, len(df.columns)-len(cat_vars), $                    0.04, 1, [1000,500], [0.001,0.01], y_range=y_range)
creds = urllib.request.base64.encodebytes(bytes("%s:%s" % (config["tcat"]["user"], config["tcat"]["passwd"]), "utf-8")).decode().strip() $ opener.addheaders.append(("Authorization", "Basic " + creds))
park.loc[park.named_fda_drug.notnull()][:5]
datafile = "nyc_311_data_subset-2.csv" $ data = read_311_data(datafile)
print('No. of unique users in Dataset:',df.user_id.nunique())
corpus = data["Improvements_vec"]
station_distance = TripData_merged3 $ station_distance.iloc[:2] $ station_distance.shape
to_be_predicted_Day4 = 17.79222525 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
season_groups.first() # first row of each group 
n_new = len(df2.query("group == 'treatment'")) $ print('The n_new is: {}.'.format(n_new))
weather_yvr_dt['Datetime'] = pd.to_datetime(weather_yvr_dt['Datetime'])
extract = df_twitter_copy.text.str.extract('(\d.?\.?.?.?\/..\d?)', expand = True) $ extract = extract[0].str.split('/') $ extract = pd.DataFrame(extract) $ extract.columns = ['rating']
comments_df.head()
r = c.update_one({'name.last': 'Bowie'}, $                  {'$inc': {'albums.0.released': -1}})
portfolio_df.head()
pd.DataFrame ([{"name":"Yong", "id":1,"zkey":101},{"name":"Gan","id":2}])
orig_p_new=df2[df2['group']=='treatment'].converted.mean() $ orig_p_old=df2[df2['group']=='control'].converted.mean() $ orig_p_diff=orig_p_new-orig_p_old $ p_diffs=np.array(p_diffs) $ (p_diffs>orig_p_diff).mean()
pickle.dump(nmf_tfidf_df, open('iteration1_files/epoch3/nmf_tfidf_df.pkl', 'wb'))
cityID = 'adc95f2911133646' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Colorado_Springs.append(tweet) 
X_train_term.shape
dictionary = corpora.Dictionary(text_list) $ dictionary.save('dictionary.dict') $ print dictionary
dsg.vatts('u')
food["created_time"].head()
plt.scatter(med['longitude'],med['latitude'])
num_base_twitters = df1.shape[0] + df2.shape[0] + df3.shape[0]
from random import random
datetimes = ['timestamp', 'created_at'] $ for field in datetimes: $     archive_copy[field] = archive_copy[field].astype('datetime64[ns]')
props=pd.read_csv("http://www.firstpythonnotebook.org/_static/committees.csv")
groceries.loc[['eggs', 'apples']]
df_pol[df_pol['pol_id']=='Liberal']['domain'].value_counts().head(12).plot(kind='bar');
pp = PostProcess(run_config='config/run_config_notebook.yml', $                  model='MESSAGE_GHD', scen='hospitals baseline', version=None)
joined_df.usage_duration.to_frame().head()
df2.converted.sum()/df2.converted.count()
!apt install cuda-8-0 -y
act_diff = df2[df2['group'] == 'treatment']['converted'].mean() -  df2[df2['group'] == 'control']['converted'].mean() $ print("{} is the actual difference observed in ab_data.csv. ".format(act_diff)) $ p_diffs = np.array(p_diffs) $ print("{} is the proportion of the p_diffs are greater than the actual difference .".format((act_diff < p_diffs).mean())) $
z_score, p_value 
new_page_converted = np.random.choice([0,1], size=n_new, p=[1-p_new, p_new]) $ new_page_converted.mean()
autos['price'].describe()
autos = autos.rename(columns={'odometer':'odometer_km'})
transactions.merge(transactions,how='inner',on='UserID').head(5)
dfd = dfs.drop_duplicates(subset = 'text', inplace=False)
p_new = df2['converted'].mean() $ print('Convert rate for p_new under the null :: ',p_new)
from nltk.tokenize import TweetTokenizer $ from nltk import pos_tag $ from nltk.stem import WordNetLemmatizer $ import re
with open('data/kochbar_02.json') as data_file:    $     kochbar02 = json.load(data_file) $ koch02df = preprocess(kochbar02) $ koch02df.info()
time_local = (np.array(time_utc) - time_utc[0]) $ key_press = np.ones((l,1)) $ plt.plot(time_local, key_press, 'ro') $ plt.show()
isinstance(nlp,spacy.lang.en.English)
classification_df.corr()['best'].sort_values()
mario_game = sales.loc[sales.Name.str.lower().str.contains('mario') == True,:].copy()
pd.isnull(pd.read_csv("Data/microbiome_missing.csv")).head(20)
grouped_by_origin['DepDelay'].mean().sort_values(ascending=False).head()
f_counts_week_app = spark.read.csv(os.path.join(mungepath, "f_counts_week_app"), header=True) $ print('Found %d observations.' %f_counts_week_app.count())
df.isnull().sum()
classifier.get_variable_names()
df.columns
challenge_contest = challenge_contest.drop_duplicates()
tweets = pd.read_csv('twitter-archive-enhanced.csv')
from sklearn.model_selection import train_test_split
files4= files4.rename(columns={'jobid':'jobId'}) $ files4['jobcandidate']=files4['jobId'].astype(str)+'-'+files4['candidateid'].astype(str)
from collections import Counter $ words = set() $ word_counts = data['Tweets'].apply(lambda x: pd.value_counts(x.split(" "))).sum(axis = 0) $
import warnings $ warnings.simplefilter('ignore', FutureWarning) $ from pandas import * $ beirut = read_csv('BeirutWeatherHistory2015.csv', skipinitialspace=True)
prob_conv
train.Page.value_counts().shape
len(labels)==len(communities)
df.shape
import statsmodels.api as sm   #stats model is imported to implement OLS and z-test methods $ convert_old = len(df2.query('landing_page == "old_page" & converted == 1')) $ convert_new = len(df2.query('landing_page == "new_page" & converted == 1')) $ convert_old,convert_new
new_page_converted = np.random.binomial(1, p_new, n_new) $ new_page_converted
USvideos.tail(10)
p_old = df2['converted'].mean()
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ print('z-score: {}'.format(z_score)) $ print('p-value: {}'.format(p_value))
day_ahead_price_df = pd.read_excel(day_ahead_folder + '/new-DA-price.xlsx')
df = pd.DataFrame(recent_prcp_data).dropna() $ df['date'] = pd.to_datetime(df['date']) $ df.set_index('date', inplace=True) $ df.head()
model = gensim.models.doc2vec.Doc2Vec(size=200, min_count=3, iter=200) $ model.build_vocab(tag_tokenized) $ model.train(tag_tokenized, total_examples=model.corpus_count, epochs=model.iter) $ print(model.docvecs[10])
noise_graf.head(2)
df['converted'].sum()/df['user_id'].count()
trainwdummies
dfs = pd.read_html('http://www.contextures.com/xlSampleData01.html', header=0) $ dfs[0].head()
bars.close.at_time('16:00')
pgh_311_data_merged['CREATED_ON'] = pd.to_datetime(pgh_311_data_merged['CREATED_ON']) $ pgh_311_data_merged.info()
usnpl_clean_url = 'https://raw.githubusercontent.com/yinleon/usnpl/master/data/usnpl_newspapers_twitter_ids.csv' $ df_usnpl = pd.read_csv(usnpl_clean_url) $ len(df_usnpl)
glm_multi_v3
df2[df2['converted'] == 1].count()/df2.shape[0]
help(web.DataReader)
df_daily = df_daily.groupby(['STATION','DATE']).sum() $ df_daily.head()
plt.hist(p_diffs);
users['fees'] = 0 # broadcast to entire column $ print(users)
df[df['public']=='offline'].count()[0]/df.count()[0]*100
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=18000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
kick_projects.head()
merged_tickets = pandas_ds[pandas_ds["current_status"] == 'MERGED']; $ merged_tickets
df.head()
df2.loc[df2["user_id"] == 773192,]
df = df[df['lang'] == 'en'].drop('lang', axis=1) $ df.head()
p_old  = (df2['converted']).mean() $ print(p_old) $
state='FINISHED' $ %%sql $ SELECT :state as "bind_variable"
rdf_clf = RandomForestClassifier()
conv = df[df['converted']==1]['user_id'].shape[0] $ ratio = conv/total $ ratio
df3[['US','UK','CA']] = pd.get_dummies(df3['country'])
data['Created Date'][0:20]
def sort_dict_values(d, reverse=False): $     return sorted(d.items(), key=lambda x: x[1], reverse=reverse)
hpd["2015-06":"2015-08"]['Complaint Type'].value_counts().head(5)
df.query('converted==1')['user_id'].count()/df.shape[0]
company_vacancies.head(10)
y_preds = svc_grid.best_estimator_.predict(X_test) $ svc_scores = show_model_metrics('SVM', svc_grid, y_test, y_preds)
import pandas as pd $ stops = pd.DataFrame(data, columns=good_columns)
valid_news = news.loc[((news.messages.str.contains('http')) | (news.messages.str.contains('ht'))) & (~news.messages.str.contains('twitter.com'))] $ valid_news.shape
sentences_text = nltk.sent_tokenize(text) $ len(sentences_text)
ind[::2]
to_be_predicted_Day3 = 14.79963952 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
season_groups.aggregate(np.sum).sort_values(by = "Tm.3PA", ascending = False)
fb.head()
df.iloc[::-1].plot.barh(title="station observation") $ plt.tight_layout() $ plt.show()
ux.datasets.load_us_national_media_outlets()[:5]
retention_10.to_csv('retention.csv')
zipcodes = pd.read_csv('in/zbp13totals.txt', dtype={'zip': object}) $ zipcodes = zipcodes[['zip', 'city', 'stabbr']] $ zipcodes = zipcodes.rename(columns = {'zip':'zipcode', 'stabbr': 'state', 'city': 'city'}) $ zipcodes.city = zipcodes.city.str.title() $ zipcodes.zipcode = zipcodes.zipcode.astype('str')
autos.odometer_km.unique().shape
Maindf['Year'] = Maindf.index.year $ Maindf['Month'] = Maindf.index.month $
plt.boxplot(cc.close_ratio) $ plt.title("Close Ratio") $ plt.show()
breed_predict_df_clean.drop(['p2','p2_conf','p2_dog','p3','p3_dog','p3_conf'], axis=1, inplace=True)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='two-sided') $ print(str(z_score) + ", "+ str(p_value)) $ print("The significance of our z-score is " + str(norm.cdf(z_score))) $ print("Critical Z-score value at 95% confidence is " + str(norm.ppf(1-(0.05/2))))
len(df.query('converted == 1').index)/len(df.index)
sample = msftAC[:2] $ sample
%timeit df.set_index(['date', 'item']).unstack()
df_agg_click_rand.head()
originaldata.head()
full_dataset = full_dataset.sort_values('date')
df['screen_name'].value_counts()[:6]
df2.nunique()
% matplotlib inline $ import datetime $ import numpy as np $ import pandas as pd
from scipy.stats import norm $ norm.ppf(1-(0.05/2))
monthly_portfolio_average = round(np.sum(MonthlyReturns * stock_weights),2) $ monthly_portfolio_average
with open('new_reddit_topics.pkl', 'rb') as pick: $     submissions = pickle.load(pick)
print(prec_nc) $ for v in prec_nc.variables: $     print(prec_nc.variables[v])
to_be_predicted_Day1 = 47.75 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
df2[df2.converted==1].count()[0]/df2.count()[0]
df.shape
dftouse = flattened_df[~flattened_df['review_count'].map(np.isnan)] $ dftouse.shape
output_col = ['msno','days_since_the_last_expiration','days_since_the_last_subscription','is_subscribe_early'] $ df[output_col]
interpolate_acs_file(process_year, 1)
p_old = df2['converted'].mean() $ print("The convert rate for p_old under the null: ",p_old)
SEPARATOR = '__'
data.loc[:'Illinois', :'pop']
import requests
temp_series_freq_15min = temp_series.resample("15Min").mean() $ temp_series_freq_15min.head(n=10) # `head` displays the first n values
d = pd.DataFrame(index=pd.date_range(start, end))
df_new.rename(columns={'country_US':'intercept_us', 'country_UK':'intercept_uk', 'country_CA':'intercept_ca'}, inplace=True) $ df_new.head()
TrainData_ForLogistic.columns
list(soup.ul.children)
VMMmeans = {} $ for station, Id in stations.items(): $     temp = df.loc[setup.loc[Id, 'StartTimeStamp']:setup.loc[Id, 'EndTimeStamp'], station] $     VMMmeans[station] = temp.mean()
p_diffs = np.array(p_diffs) $
from sklearn import tree $ from sklearn.metrics import accuracy_score $ clf = tree.DecisionTreeClassifier() $ clf.fit(x_train,y_train) $ accuracy_score(clf.predict(x_test),y_test)
full_data = my_data.append(my_data2) $ len(full_data)
df.info()
df2_dummy.tail(1)
xs_df = valid_scores[['home team 41 game win%','away team 41 game win%', 'home team 8 game win%', $                       'away team 8 game win%']] $ xs_df.head()
print len(com311) $ print len(hpdcom) $ data = pd.merge(com311, hpdcom, how='outer', on='ComplaintID') $ print len(data)
import statsmodels.api as sm $ convert_old = df2[(df2['landing_page']=='old_page')&(df2['converted']==1)].shape[0] $ convert_new = df2[(df2['landing_page']=='new_page')&(df2['converted']==1)].shape[0] $ n_old = df2[(df2['landing_page']=='old_page')].shape[0] $ n_new = df2[(df2['landing_page']=='new_page')].shape[0]
pd_train_filtered['date'] = pd.to_datetime(pd_train_filtered['date']) $ pd_train_filtered['day'] = pd_train_filtered['date'].dt.weekday_name $ pd_train_filtered = pd_train_filtered.drop('date', axis=1)
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s') $ print(git_log.describe())
rec_items('Shirley_a_louis@yahoo.com', product_train, item_vecs, user_vecs, customers_arr, products_arr, item_lookup)
knn3.fit(X_train, y_train)
free_mo_churns = [ix for ix in USER_PLANS_df.index if USER_PLANS_df.loc[ix,'scns_array']==['Free-Month-Trial'] ]
print(AFX_X_06082017_r.json())
bow_df
df_joe
non_na_df = df.dropna()
list(Users_first_tran.dropna(thresh=int(Users_first_tran.shape[0] * .9), axis=1).columns)
print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)
pd.DataFrame(features['MONTH(joined)'].head())
weather.zip_code.unique()
df2.shape
from pandas.util.testing import assert_frame_equal $ assert_frame_equal(df1, df2, check_dtype=False)
train_df.head()
lr.drop('control',axis=1,inplace=True)
np.ones((3,4),dtype=np.int)
df_us_.drop(['category','creator','location','profile','deadline'], axis=1, inplace = True)
len(list_of_genre_1990s_1)
num_recs = df['Class'].count() $ num_zeros = df['Class'][df['Class']==0].count() $ num_ones = num_recs - num_zeros $ print("{}% of transactions are fraudunat and {}% are legitimate".format((num_ones/num_recs)*100, $                                                                        (num_zeros/num_recs*100))) $
vol = vol.div(vol['Volume'], axis=0) $ vol.head()
n_net.score(x_test,y_test > 0)
df.dtypes
hist(df.pre_clean_len,100) $ grid()
print(len(set(contract_history.INSTANCE_ID) & set(intervention_train.INSTANCE_ID))) $ print(len(set(contract_history.INSTANCE_ID) & set(intervention_test.INSTANCE_ID))) $ print(len(set(contract_history.INSTANCE_ID) & set(intervention_history.INSTANCE_ID)))
print (temperature_data)
df = pd.DataFrame.from_records(mylist) $ df.head()
from datetime import datetime, date, time $ dt = datetime(2011, 10, 29, 20, 30, 21)
d = docx.Document(downloadIfNeeded(example_docx, example_docx_save, mode = 'rb')) $ for paragraph in d.paragraphs[:7]: $     print(paragraph.text)
df3[df3['STATION'] == '103 ST'].groupby(['DATE']).sum()
weather_df.T
%time $ if 0 == go_no_go: $     lda_vis_serialized = pyLDAvis.gensim.prepare(lda, serial_corp, d) $     pyLDAvis.save_html(lda_vis_serialized, fps.pyldavis_fp)
rshelp.query("SELECT id, ev_charging FROM postgres_public.ratings_amenities LIMIT 100;")
dfM = df.copy()
nps.crs
pres_df.ix[362865] # how to get row info by index number
df_tweet_data.info()
inp = Input(shape=(train_source_emb.shape[1],)) $ x = Dense(train_target_emb.shape[1], use_bias=False)(inp) $ modal_model = Model([inp], x) $ modal_model.summary()
columns = ['date','favorites','id','retweets','source','text','user_flwr_count','user_name'] $ data = pd.read_csv(file_name,header = None,names = columns,parse_dates=['date'],dtype={'id':str,'favorites':str,'retweets':str,'user_flwr_count':str}) $ new_df = data.dropna(subset=['id','favorites','retweets','user_flwr_count']) $ new_df.to_csv(file_name, index=False,header=False) $ data = pd.read_csv(file_name,header = None,names = columns,parse_dates=['date'],dtype={'id':str,'favorites':int,'retweets':int,'user_flwr_count':int})
damaged_or_not_damaged = autos.groupby("unrepaired_damage").mean() $ damaged_or_not_damaged['price_$']
lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page','UK','US']]) $ results = lm.fit() $ results.summary()
plt.pie(fare_city, labels=type_city,explode=explode, colors=colors, $         autopct="%1.1f%%", shadow=True, startangle=140) $ plt.axis("equal")
twitter_archive_df_clean['name'].replace(to_replace=['None', 'a', 'the', 'an'], value="Unsure", inplace=True)
old = df2['landing_page'] $ old_converted_null = df2.query('converted == 1') $ p_old = old_converted_null.count()[0]/old.count() $ p_old
df2['intercept'] = 1 $ df2[['control','ab_page']] = pd.get_dummies(df2['group']) $ df2.drop('control', axis=1, inplace=True) $ df2.head()
print type(RandomOneRDD) $ print RandomOneRDD.collect()
naimp.get_isna_ttest('age', type_test='ttest')
sns.regplot(x=data['days_active'], y=data['ltv'])
df_a= df[(df.landing_page =="new_page") & (df.group !="treatment")]; $ print("The number of times the new_page and treatment don't line up: " ,len(df_a))
print('SQRT(X) = \n', np.sqrt(fruits))
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import seaborn as sns
df.groupby('Status').count().sort(desc("count")).show(10)
climate_vars.head()
y_pred = logreg.predict(X_test) $ print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))
fb.set_index('created_time', inplace=True)
weather = pd.read_csv("https://s3.amazonaws.com/leiwen/dmfa/austin_weather.csv")
births['day'] = births['day'].astype(int)
if os.path.isfile(pickle_full): $     print("You've already pickled!") $ else: $     sorted_stays.to_pickle(pickle_full)
lliberia = [pd.read_csv(filename) for filename in glob.glob("data/ebola/liberia_data/*.csv")] $ liberia_data = pd.concat(lliberia, join='outer')
m = np.mean(x, axis=0); m  #m has shape (3,).
print(model_CBoE.aic) $ print(model_CBoE.bic)
pd.merge(user_products, transactions, how='left', on=['UserID', 'ProductID']).groupby(['UserID', 'ProductID']).apply(lambda x: pd.Series(dict(Quantity=x.Quantity.sum()))).reset_index().fillna(0)
plot_confusion_matrix(cm=cm, classes=['COLLECTION', 'PAIDOFF'], $                           normalize=False, $                           title='Confusion matrix', $                           cmap=plt.cm.Blues)
npd = npy.NumpyDirectory('/data/readout/2017-12-02_152629_scans.npd/')
len(df_groups['group_id'].unique())
response = read_table_item('demographics', 'pmcid', '2842548')["Item"] $ print(response)
summary = summary.merge(outcomes, left_on='match_id', right_on='match_api_id', how='left').drop(['match_id', 'match_api_id'], 1)
ohlc = walk.resample("H").ohlc() $ ohlc
dates=pd.date_range(start='1/Oct/2020', periods=5, freq='B') $ print(dates)
count_polarity.reset_index(inplace=True)
all_weekdays = pd.date_range(start=start, end=end, freq='B')
cities = csvData['city'].value_counts().reset_index() $ cities.columns = ['cities', 'count'] $ cities[cities['count'] < 5]
closeSeriesP = closeSeriesQ.pct_change()
census_tracts_df['shapes'] = census_tracts_df['geometry'].map(loads)
ps
model = LinearRegression() $ model.fit(X_train, y_train) $ utils.cross_val_metrics(X_train, y_train, LinearRegression())
mean_rmse, rmse_list = cross_validate(working_data, get_split, train_model, get_rmse, workflow) $ print('Average RMSE: ', mean_rmse) $ print('RMSE list:', rmse_list)
fashion.groupby(fashion.handle).size()
n_new = (df2[df2['landing_page']=='new_page']).count()[0] $ n_new
df.head()
guineaDf = pd.concat([guineaCases,guineaDeaths],axis=1) $ guineaDf.index.name = 'Date' $ guineaDf.head()
df.groupby("pickup_year")["cancelled"].mean()
print(pandas_list_2d.iloc[:, 0])
df = df.set_index('email')
new_read = new_read.drop_duplicates(subset=['article_id', 'project_id', 'user_id', 'user_type'], keep='first') $ print(new_read.shape) $ new_read.head() 
top_rsvp = df.sort_values("last_rsvp", ascending=False).reset_index(drop=True) $ top_rsvp.head(5)
tmp_df.columns.sort_values().values
month_year_crimes.plot()
ids = df2["user_id"] $ df2[ids.isin(ids[ids.duplicated()])]
new_items = [{'pants': 30}] $ new_store = pd.DataFrame(new_items, index=['store 3']) $ new_store
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True) $ sns.boxplot(x="frcar", y="y", data = psy_prepro, ax=ax1).set_title("Anxiety driving/riding in a car") $ sns.boxplot(x="jmp2w", y="y", data = psy_prepro, ax=ax2).set_title("Feel jumpy and restless in past 2 weeks") $
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer $ analyzer = SentimentIntensityAnalyzer()
crime_and_permits_df = pd.DataFrame({'permit_count': permits_agg['permit_count'], 'crime_count': crime_agg['crime_count']}) $ crime_and_permits_df.head()
df.shape[0] $ print("Number of rows in the dataset is :{}".format(df.shape[0]))
twitter_df_clean['timestamp'] = pd.to_datetime(twitter_df_clean['timestamp'])
data.columns = data.columns.str.replace('_-_', '_') $ data.columns[data.columns != data.columns.str.extract(r'^(\w+)$')] 
out = train[['bathrooms','bedrooms','created','display_address','interest_level','latitude','longitude', $              'listing_id','building_id','manager_id','price','street_address']]
df_new['UK_page'] = df_new['country_UK'] * df_new['ab_page'] $ df_new['US_page'] = df_new['country_US'] * df_new['ab_page'] $ df_new.head()
merged1['Specialty'].value_counts()
us_temp = temp_fine.reshape(843,1534).T #.T is for transpose $ np.shape(us_temp)
addFarm(fileName,[491,484],'ssTemp2')
from sklearn.cluster import KMeans $ est = KMeans(n_clusters=12) $ est.fit(X) $ labels = est.labels_
df_c_merge['country'].astype(str).value_counts()
mask = (x > 0.5) & (y < 0.5)
Image(url= "https://pbs.twimg.com/media/C2tugXLXgAArJO4.jpg")
meta = [] $ for chunk in tqdm(chunks(video_ids, 50)): $     data = yt.get_video_metadata(chunk, key, verbose=1) $     meta.extend(data) $     time.sleep(.1)
scoresdf[1].head(20)
df2.head(3) $
df.shape $
p_treatment = df2.query("group == 'treatment'")['converted'].mean() $ p_treatment
df_protest.index[0]
pd.merge(d10, d11, on='city')
df_countries = pd.read_csv('./countries.csv') $ df_countries.head()
data.occupation_husb.unique()
le_indicators = wb.search("life expectancy") $ le_indicators.iloc[:3,:2]
day_ahead_price_df.columns = ['Date','DA-price', 'PTE'] $ DA_power_df = day_ahead_price_df.merge(solar_wind_df, on = ['Date', 'PTE'], how = 'inner') $ DA_power_df.head()
twitter_df_clean['favorite_count'].corr(twitter_df_clean['retweet_count'])
df.head()
cnf_matrix = confusion_matrix(y_test, yhat_tree, labels=['PAIDOFF','COLLECTION']) $ np.set_printoptions(precision=2) $ print (classification_report(y_test, yhat_tree)) $ plt.figure() $ plot_confusion_matrix(cnf_matrix, classes=['PAIDOFF','COLLECTION'],normalize= False,  title='Confusion matrix')
today = datetime.datetime.today() $ start_date = str(datetime.datetime(today.year, today.month-1, 1).date()) $ print('predictions for', start_date)
import os
dt.strftime('%m/%d/%Y %H:%M')
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US', 'ab_page']])
result = api.search(q='%23puravida') #%23 is used to specify '#' $ len(result)
df.withColumn("sentiment", get_sentiment_udf(array(SENTIMENT_KEYS))).head()
excelDF.Quantity.max()
df_joined[['UK', 'US', 'CA']] = pd.get_dummies(df_joined['country']) $ log_mod_2 = sm.Logit(df_joined['converted'], df_joined[['intercept', 'US', 'CA']]) $ results = log_mod_2.fit() $ results.summary()
import statsmodels.api as sm $ convert_old = len(df2[((df2['landing_page'] == 'old_page') & (df2['converted'] == 1))]) $ convert_new = len(df2[((df2['landing_page'] == 'new_page') & (df2['converted'] == 1))]) $ n_old = len(df2[df2['landing_page']=='old_page']) $ n_new = len(df2[df2['landing_page']=='new_page'])
len(df['lang'].value_counts())
df_new[['CA','UK','US']] = pd.get_dummies(df_new.country) $ df_new.head()
y=np.array(df_train['interest_level'])
group = df.groupby(['brand'])[['price', 'kilometer', 'powerPS', 'age', 'min_online_time']]
print(experience.sum(axis=0).sum()) $ print() $ print(experience.sum(axis=0))
bp.rename(columns={"value1num":"systolic","value2num":"diastolic"}, inplace=True)#inplace: overwrite new label in old label
sub1 = pd.merge(sub1, sub2, on = ['hacker_id', 'challenge_id'], how = 'left')
len(all_doc)
validation.analysis(observation_data, simple_resistance_simulation_0_25)
elms_all_0611.loc[range(1048575)].to_excel(cwd+'\\ELMS-DE backup\\elms_all_0611_part1.xlsx', index=False)
print df.set_index(['date', 'item']) $
active_marketing = clean_users[clean_users['active']==1]['enabled_for_marketing_drip'].sum()/active_count $ inactive_marketing = clean_users[clean_users['active']==0]['enabled_for_marketing_drip'].sum()/inactive_count_with_na
df_enhanced = df_enhanced.drop(['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'], axis=1)
random_numbers.groupby(pd.Grouper(freq='M')).mean() #calender months with each months mean values
finals['type'] = "normal"
fav_max = data['Likes'].max() $ fav_tweet = data[data['Likes']==fav_max].index $ print("The tweet with more likes is: \n{}".format(data.loc[fav_tweet,'Tweets'])) $ print("Number of likes: {}\n".format(fav_max)) $ print("{} characters.\n".format(data.loc[fav_tweet,'len']))
random_integers
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'uk', 'us']]) $ result = log_mod.fit() $ result.summary()
page.is_categorypage()
log_mod2 = sm.Logit(df2['converted'], df2[['intercept', 'CA', 'UK']]) $ results2 = log_mod2.fit() $ results2.summary()
address_indexes_arr = np.asarray(address_indexes) $ address_indexes_arr[:10]
%%time $ w2v_model = Word2Vec(tokenized_tweets, min_count = 1)
b_cal_q1.shape
manager.image_df.head()
print(type(s))
print "The mean of the preTest scores is " + str(np.mean(df.preTest)) $ print "The mean of the postTest scores is " + str(np.mean(df.postTest))
df.info()
df2.drop(2893, inplace=True) #Drop the second value from the output above
propiedades.sample()
print("Job status for " + jobId + ": " + sqlClient.get_job(jobId)['status'])
def percent_charged(l): $     greater_than_zero = [e > 0 for e in l] $     return float(sum(greater_than_zero))/len(greater_than_zero)
df_clean.text.str.extract(pat='(\d+)/10')
import praw $ import csv $ import pandas as pd $ from tqdm import tqdm_notebook, tqdm $ from datetime import datetime 
lgb_model.fit(X_train, y_train)
!apt-get install tree -qq $ !tree -d /tools/node/
df_archive_clean["name"].value_counts()[0:15]
7952925-7952784
get_all_tweets("rorypul", "M")
t=longitudinal.take(1)[0] $ print "One client id: {}".format(t.cid) $ print "First five ssd for this client id (unordered): {}".format(t.ssd[:5]) $ print "First five num_ssd for this c_id  (unordered): {}".format(t.num_ssd[:5])
df_ad_airings_5[df_ad_airings_5['location'].isnull()]
df.index
bst = xgboost.train(param, dtrain, 10)
for n in [tup[0] for tup in france_tops]: $     print (lda.print_topic(n)) $     print ("")
to_be_predicted_Day4 = 21.29191735 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
df_length = len(df)         $ print(df_length)
!cat /notebooks/.log/20170704/20170704-071619-0567.log
logit_countries = sm.Logit(df4['converted'], $                            df4[['country_UK', 'country_US', 'intercept']]) $ result2 = logit_countries.fit()
slist = [s1, s2, s3] $ for item in slist: $     item.name == 'NAME': $
Lab7_Equifax = Lab7_Redesign.loc[Lab7_Redesign['ENTITY'] == 'Equifax']
get_sentiment_udf = udf(get_sentiment_label, StringType())
crime_wea.head(10)
df2['dow']=df2['datetime'].dt.weekday_name
train = pd.merge(train, hpg_reserve, how='left', on=['air_store_id','visit_date']) $ test = pd.merge(test, hpg_reserve, how='left', on=['air_store_id','visit_date'])
df_hubs = df_avg_use.query('city != "non hub"').copy() $ df_lg_hubs = df_hubs.query('annual_avg > 500') $ df_lg_hubs
speakers = pd.read_json('speakers.json')
data2.dates = pd.to_datetime(data2.dates) $ data2.TRDATETIME = pd.to_datetime(data2['dates'],  dayfirst= True, format='%d%b%y:%H:%M:%S')
s.dt.minute # extract minute as integer
from pytrends.request import TrendReq $ pytrends = TrendReq(hl='en-US', tz=360) # initiate connection to google trends $ kw_list = ["bitcoin"] $ pytrends.build_payload(kw_list)
data.head()
exploration_airbnb.print_infos('consistency')
destinations_url = '~/Documents/Expedia/destinations.csv' $ dests = pd.read_csv(destinations_url) $ dests.shape
lr = LogisticRegression() $ cv_score = cross_val_score(lr, features_class_norm, overdue_transf, scoring='roc_auc', cv=5)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_autocorrelation(therapist_duration, params=params, lags=30, alpha=0.05, \ $     title='Weekly Therapist Hours Autocorrelation')
df_new['country'].value_counts()
new_page_converted = np.random.choice([0,1],n_new, p=(1-p_new, p_new))
df_delta = df_t1["Finish"].sub(df_t1["Start"], axis=0)
from sqlalchemy import create_engine
df['Grades'].astype('category').head()
p_conv_treat - p_conv_ctrl # !! Keep in mind that later, under the null, we are assuming this difference to be 0 anyways.
twitter_archive_df_clean['stage'] = twitter_archive_df_clean[['doggo','floofer','pupper', 'puppo']].replace('None','').sum(1)
y_predict = knn_model.predict(X_test)
url = 'http://stat.data.abs.gov.au/sdmx-json/data/CPI/1.1.40055+115524.10.Q/all?detail=DataOnly&dimensionAtObservation=AllDimensions&startPeriod=2015-Q2' $ response = requests.get(url)
df = df.set_index('date') $ month_plot = df.resample('d', how=sum).plot() $ plt.show()
total = df['user_id'].nunique() $ pct_convert = df.query('converted == 1').user_id.nunique()/total $ print(pct_convert)
tmdb_movies_production_countries_revenue.describe()
testurl = 'https://42hire.com/20-reasons-why-working-from-home-is-way-better-than-working-in-an-office-with-people-7590d54980c8' $ df_test = df_merged[df_merged[df_merged.columns[0]]==testurl] $ df_test.head()
full_data.dtypes
for route in routes: $     print('mode: {} | id: {} | route name: {}'.format(route['mode'], route['id'], route['longName']))
df_new['ca_intercept'] = df_new['country'].replace(('US','UK','CA'),(0,0,1)) $ lm = sm.OLS(df_new['converted'],df_new[['intercept','ca_intercept']]) $ lm.fit().summary()
dfWords.rename(columns={ 0:"date", 1:'word', 2:'latitude', 3:"longitude" }, inplace=True)
print("Number of NAN values before: ", df_usa.isnull().sum()) $ df_usa=df_usa.fillna(0) $ print("Number of NAN values after: ", df_usa.isnull().sum())
lr.head()
tagged_df.head()
df = datetime.now()
for file in s3_key_origs: $     os.environ["s3_key"] = "s3://wri-public-data/" + file $     os.environ["gs_key"] = "gs://resource-watch-public/" + file $     !gsutil cp $s3_key $gs_key
assert "Red" == "Blue"
unique_domains.sort_values('mentions', ascending=False).head()
price = autos['price'] $ ax = sns.boxplot(y= price) $
snow.select ("select count (distinct diagnosis_id) from st_angina")
df.dropna(axis=1, thresh=1000).shape
df_table = dff.loc[dff['Suburb'].isin(m_fire)] $ df_table = df_table.reset_index() $ len(df_table)
df_restaurants.select('name','categories').limit(10).toPandas()
df['user_id'].nunique()
print( gwt.ggPM(4,0.0) )
master_list.sort_values(by='Count', ascending=True).head(10)
failures['datetime'] = pd.to_datetime(failures['datetime'], format="%Y-%m-%d %H:%M:%S") $ failures['failure'] = failures['failure'].astype('category') $ print("Total number of failures: %d" % len(failures.index)) $ failures.head()
low = out_df.copy() $ low['low']=1 $ low['medium']=0 $ low['high']=0 $ low.to_csv('all_low.csv',index=False)
tweets[0].text
df.describe()
param_grid = [ $     {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, $     {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}, $   ] $ grid_search = GridSearchCV(forest_model,param_grid=param_grid,cv=5)
mention_df['tweet_id'].unique().size
y_pred = tuned_forest_model.predict(X_test) $ confusion_matrix(y_test,y_pred)
constructTables() $ valueInsertion()
tweets_df.tweet_place.unique()
autos.head(10)
session = Session(engine)
active_referred,inactive_referred,total_users_referred, SD_referred
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop(['CA'], axis=1) $ df_new.head()
pd.Period('2007-1-1', 'B')
df_predictions_clean['p1_dog'].unique()
df['car_type'] = df['Keyword'].str.extract("(bmw|audi|ford)", re.IGNORECASE, expand=False)
df[df['doggo'] == 'None'][df['floofer'] == 'None'][df['pupper'] == 'None'][df['puppo'] == 'None']
to_be_predicted_Day4 = 34.13664722 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
df1_clean.replace('None', np.NaN, inplace=True)
gaussian = GaussianNB() $ gaussian.fit(X_train, Y_train) $ Y_pred = gaussian.predict(X_test) $ acc_gaussian = round(gaussian.score(X_test, Y_test) * 100, 2) $ acc_gaussian
plt.figure(figsize=(15,6)) $ plt.title('Distribution of Crimes per day', fontsize=16) $ plt.tick_params(labelsize=14) $ sns.distplot(crimes.resample('D').size(), bins=60);
datAll['Block_range'] = np.where(~pd.isnull(datAll['Block Range']),datAll['Block Range'],np.nan) $ datAll['Block_range'] = np.where(~pd.isnull(datAll['BlockRange']),datAll['BlockRange'],datAll['Block_range'])
learn.fit(lr, 1, cycle_len=1)
ts
df.groupby('Agency')['Complaint Type'].value_counts().groupby(level=0, group_keys=False).nlargest(5)
dfgrouphc = df.groupby('hotel_cluster').count() $ serhc = dfgrouphc['date_time'] $ sns.set(font_scale=1.5) $ f = serhc.plot(figsize=(12,6)) $ _ = f.set_ylabel('Number of Examples')
df_uniname.rate.hist() $ plt.title('one-word name users (rate)') $ plt.show()
tweet_data.tweet_id.unique().size
xml_in_merged.tail(2)
df_concat_2["time"].str.split(':') $ df_concat_2["time"] = df_concat_2["time"].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1]))
df.head()
n_old = df_old.shape[0] $ print(n_old)
rsi = rs.index[120] $ rsi
full_contingency = np.array(pd.crosstab(index=intervention_train['target'], columns=intervention_train['ORIGINE_INCIDENT']))
round((timelog.seconds.sum() / 60 / 60 / 24), 1)
bus['postal_code_5'] = bus['postal_code'].str[:5] $ bus
etf_weights = calculate_dividend_weights(ex_dividend) $ helper.plot_weights(etf_weights, 'ETF Weights')
data = pd.DataFrame(json.load((open('ultimate_data_challenge.json')))) $ data.head()
Xforadding = np.expand_dims(X, axis=2) $ Xforadding_v = np.expand_dims(VX, axis=2)
classification_data = classification_data[classification_data.primary_role == 'company'].copy()
All_tweet_data_v2.name[All_tweet_data_v2.name.str.len() < 2].value_counts()
import time $ start_time = time.time() $ patient = 'pt1sz2' $ adjmats = main(patient) $ print("--- %s seconds ---" % (time.time() - start_time))
%%time $ grid_tfidf.fit(X_tfidf, y_tfidf)
df4[["old_page","new_page"]] = pd.get_dummies(df4["landing_page"]) $ df4.head()
Most_active_stations = session.query(Measurement.station).\ $                                      group_by(Measurement.station).order_by(func.count(Measurement.prcp).desc()).limit(1).scalar() $ print ( "Station which has the highest number of observations is  " + str(Most_active_stations))
print(type(x_tokens)) $ print(len(x_tokens)) $ print(x_tokens[83029])
sample.head(1)
austin[['driver_rating', 'rider_rating', 'charity_id', 'free_credit_used', 'round_up_amount','promocode_redemption_id','tipped_on', 'tip']]= austin[['driver_rating', 'rider_rating', 'charity_id', 'free_credit_used', 'round_up_amount', 'promocode_redemption_id','tipped_on', 'tip']].fillna(value=0)
hyper_parameters = {'ntrees':[5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], $                     'max_depth':[5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]} $
tweet_archive_clean.dtypes
events["speed"] = events.distance / events.time $ events.sort_values('speed', ascending = False).head()
prcp_df = pd.DataFrame(prcp_data, columns=['Date', 'Precipitation']) $ prcp_df.set_index('Date', inplace=True) $ prcp_df.head()
df2['intercept']= 1 $ df2['ab_page'] = 0
timelog.head()
print("At the moment, we have %d entries with NewsDesk=Nan." % len(joined.loc[joined['NewsDesk'].isnull()]))
end_of_period = pd.Timestamp("2018-07-30") $ datediff = lambda x: (end_of_period - x) $ df_r["Recency (1)"] = (df_r.groupby("CustID")["BookingDate"].transform(datediff)).dt.days $ df_r["Recency (3)"] = df_r["Recency (1)"]
pc = pd.DataFrame(tt1.groupby('ZIPCODE').size()) $ pc.columns=['count'] $ pc_gt100 = pc[pc['count']>100].sort('count', ascending = False) $ pc_gt100
labeled_features = final_feat.merge(failures, on=['datetime', 'machineID'], how='left') $ labeled_features.head()
(etsamples_grid,etmsgs_grid,etevents_grid) = be_load.load_data(algorithm='hmmnosmooth_') $ raw_large_grid_df = condition_df.get_condition_df(data=(etsamples_grid,etmsgs_grid,etevents_grid),condition='LARGE_GRID')
git_blame = pd.read_csv("C:/Temp/linux_blame.gz") $ git_blame.head()
vectorizer.vocabulary_
print('We double the amount of apples and oranges:\n', fruits[['apples', 'oranges']] * 2)
cur.execute('SELECT * FROM cable LIMIT 2;') $ for r in cur.fetchall(): $    print(r)
del df2['t'] $
dt_1 = pd.datetime(2016, 1, 1)
expiry = datetime.date(2015, 1, 17) $ aapl_calls = aapl.get_call_data(expiry=expiry) $ aapl_calls.iloc[0:5, 0:4]
popCon[popCon.content == 'video'].sort_values(by='counts', ascending=False).head(10).plot(kind='bar')
x = data2_df.BikeID.resample(sample_rate).count() $ x.name = 'Count' $ y =  data2_df.resample(sample_rate).mean() $ y = y.drop(columns=['BikeID', 'ID', 'Lat', 'Lon', 'Casual', 'Member', 'Fall', 'Spring', 'Summer', 'Winter']) $ R_trip = pd.concat([x, y], axis=1)
cat_pizza = tokenize_review(cat_pizza,'reviews_text' )
db = client.test_database
X = [html.unescape(string) for string in X]
import pandas as pd $ import numpy as np
testheadlines = test["text"] $ advancedtest = advancedvectorizer.transform(testheadlines) $ advpredictions = advancedmodel.predict(advancedtest)
suspects_file_path = '../../data/outbound/sample_219.txt' $ suspects_data = pd.read_csv(suspects_file_path, sep='|', header=None,names=header)
prob_new =len(df2.query("landing_page == 'new_page'"))/len(df2) $ print('The probality of an individual receiving a new page is {}'.format(prob_new))
np.nan == np.nan
import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib notebook $ import numpy as np $ from sklearn import datasets, linear_model
gcp_credentials = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' $ destination = './gcp_credentials.json' $ download_file_from_google_drive(gcp_credentials, destination)
offseason11["InorOff"] = "Offseason"
Grouping_Year_DRG_discharges_payments.groupby(['A']).sum() $ df.groupby(['A']).size()
n_new = df2.query('landing_page == "new_page"').converted.count() $ n_new
s[[2,3]]
payments_total_yrs.tail() $ payments_total_yrs.to_csv('Top Total Payees More than 1 Million Total.csv') $
frame['e'].map(format)
yeardf['TSRetAnn'] = yeardf['TSRet'] -1
cust_data2.info()
for i, score in enumerate(coherence_values): $     print(f'#{i}\tscore:{score}\t{model_list[i]}')
opening_price = [] $ for ele in r.json()['dataset']['data']: $     if ele[1] != None: $      opening_price.append(ele[1]) $
imp_date = datetime(2018, 4, 15, 23, 59, 59)
output= "select tag_id, count(tag_id) as pm from (select tag_id,date as dt from tweet_tags inner join tweet_details where tweet_tags.tweet_id=tweet_details.tweet_id) where dt='2018-02-26' group by tag_id order by pm desc limit 10 " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['tag_id','Count'])
letters = get_aflcio_alerts() $ letters[:2]
parsed_names = raw.name.str.extract(r"(?P<last_name>[\w ]+), (?P<first_name>[\S ]+)\2 \1", expand = True) $ parsed_names_simple = raw.name.str.extract(r"^(?P<last_name>[\w]+) (?P<first_name>[\w ]+)$", expand = True) $ q = parsed_names.first_name.isnull() & parsed_names_simple.first_name.notnull() $ parsed_names.loc[q] = parsed_names_simple.loc[q] $ parsed_names.assign(original = raw.name).sample(10)
authors = xml_in['authorName'].unique().tolist() $ random_authors = np.random.choice(authors, 100000) $ random_authors_final = random_authors.tolist()
all_complaints["Created Date"] = pd.to_datetime( $     all_complaints["Created Date"], $     format = "%m/%d/%Y %H:%M:%S %p"   $ )
much_data = np.fromfunction(lambda x,y: (x+y*y)%17*11, (10000, 26)) $ large_df = pd.DataFrame(much_data, columns=list("ABCDEFGHIJKLMNOPQRSTUVWXYZ")) $ large_df[large_df % 16 == 0] = np.nan $ large_df.insert(3,"some_text", "Blabla") $ large_df
pre_number = len( niners[niners['Jimmy'] == 'no']['GameID'].unique() ) $ print pre_number
data.loc[data.floor > 70, 'floor'] = np.NaN
df.groupby(['product_type','state'])['full_sq'].nlargest(5) $
plt.figure(2) $ df['GoldsteinScale'].plot.hist(bins=(20),legend=True) $ plt.figure(3) $ df['GoldsteinScale'].plot.kde(legend=True)
import pandas as pd
df['d']
list(archive_df)
embedding_model = Model(inputs=model.inputs, outputs=model.layers[-3].output)
df2[['ab_page', 'drop_it']] = pd.get_dummies(df2['landing_page']) $ df2.head()
from pysumma.utils import utils $ import os
train_age = train[(train.age.notnull()) & (train.age<150)] $ sn.distplot(train_age['age'])
tweet_json.info()
StockData.head()
dr_existing_data_plus_forecast
print(train_df[train_df.author.isnull()].shape[0]) $ print(train_df.shape[0])
sp500.tail()
model_filename = 'models/finalized_mpg_estimation_model.sav' $ loaded_mpg_estimation_model = pickle.load(open(model_filename, 'rb')) $
df_ml_56_01.tail(5)
pgh_311_data["2015-12-31 20:00:00":"2016-01-01 02:00:00"]
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new,(1-p_new)]) $ new_page_converted.mean()
sub_df.to_csv("xgb_unconverged.csv", index=False)
price2017['DateTime'] = pd.to_datetime(price2017['Date'] + ' ' + price2017['Time'])
df.head()
process_rt_column(raw_df)
url = "https://data.wprdc.org/datastore/dump/76fda9d0-69be-4dd5-8108-0de7907fc5a4" $ pgh_311_data = pd.read_csv(url) $ pgh_311_data.head()
tree = ET.parse(file) # mocht je een andere map hebben dan kan je het pad hier aanpassen $ root = tree.getroot() $ namespaces = {'xsd':"http://www.w3.org/2001/XMLSchema", 'xsi':"http://www.w3.org/2001/XMLSchema-instance" } $
best_rf.cross_validation_metrics_summary
date.month
for attr in ncTest.ncattrs(): $     print('%s: %s' % (attr, ncTest.getncattr(attr)))
df_final.drop(['doggo', 'floofer', 'puppo', 'pupper'], axis=1, inplace=True)
dfRegMet.info()
pd.merge(staff_df, student_df, how='right', left_index=True, right_index=True)
p_treatment = df2.query('group=="treatment"').converted.mean() $ p_treatment
df['ruling'].value_counts()
msft_cum_ret['2012-01'].mean()
with open("Valid_events_eps0.7_5days_500topics","rb") as fp: $     Valid_events = pickle.load(fp)
store_items.fillna(method='ffill', axis=1) # filled with previous value from that row
ds = web.DataReader("5_Industry_Portfolios", "famafrench") $ print(ds['DESCR'])
url_df=pd.DataFrame({'url':urls})
post_req = requests.post(post_url, data=json_dat, headers=headers) $ json_out = json.loads(post_req.content)
clicks.head(2)
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" & converted == 1').shape[0] $ convert_new = df2.query('landing_page == "new_page" & converted == 1').shape[0] $ n_old = df2.query('landing_page == "old_page"').shape[0] $ n_new = df2.query('landing_page == "new_page"').shape[0]
geo_df.head()
35173/float(290584) $
s1.iloc[0]
import pandas as pd
print u
np.any(x > 8)
encoded_df = pd.get_dummies(df, columns=['category', 'fileType']) $ encoded_df.shape
X = vectorizer.transform(react_texts)
ftp = ftplib.FTP("ftp.star.nesdis.noaa.gov")
rows_no=ab_file.shape[0] $ print('No. of rows in dataset are : ', rows_no)
pd.reset_option("display.max_rows") $ pd.get_option("display.max_rows")
autos.registration_year.describe()
all_39s_from_2011 = Grouping_Year_DRG_discharges_payments.loc[(slice(2011), slice(39)),:] $ len(all_39s_from_2011)
X = jdf.drop(['Journal','PubDays'],axis=1) $ y = jdf['PubDays']
click_condition_meta['dvce_type'] = np.where(click_condition_meta.user_id == "1f336e8c-d658-4656-bd87-aae8995e2725", 'Mobile', click_condition_meta.dvce_type)
twitter_goodreads_users_df.head(5)
pageviews_mobile_df = (all_data_as_dfs['pageviews_mobile-web']['views'] + all_data_as_dfs['pageviews_mobile-app']['views']).to_frame() $ pageviews_mobile_df['access'] = 'mobile' $ pageviews_mobile_df['api'] = 'pageviews' $ pageviews_mobile_df[:3]
merged_NNN.groupby("committee_name_x")
about
converted_users = df[df.converted == 1]['converted'].count() $ prop_conv = converted_users/unique_users $ prop_conv
browser.get("http://stats.nba.com/scores/#!/{}".format(date_url)) $ time.sleep(10) $ soup = bs(browser.page_source, "html5lib")
import pickle $ with open('house_regressor.pkl', 'wb') as f: $     pickle.dump(automl, f) $
all_text.head()
list(set(words) - set(clean_words))
control_rate = df2[df2['group'] == 'control']['converted'].value_counts()[1]/df2[df2['group'] == 'control']['converted'].count() $ control_rate
print topUserItemDocs.shape $ topUserItemDocs=topUserItemDocs.join(targetItemData['id'],on='item_index_corpus',how='left') $ print topUserItemDocs.shape $ topUserItemDocs.head()
t.index
%%sql mysql://admin:admin@172.20.101.81 $ CREATE USER 'user1'@'%' IDENTIFIED BY 'logger'; $ GRANT SELECT, INSERT, DELETE, UPDATE ON pidata.temps3 TO 'user1'@'%'; $ FLUSH PRIVILEGES; $
pokemon['Name'] = pokemon['Name'].fillna('Primeape') $ pokemon['Type 1'] = pokemon['Type 1'].replace('Fighting', 'Fight') $ pokemon['Type 2'] = pokemon['Type 2'].replace('Fighting', 'Fight') $ pokemon_df = pokemon.copy() $ pokemon.head() $
StockData.head()  # DataFrame.head(n) shows the first n rows
facilities = pd.read_csv('facilities.csv') $ facilities.info()
df2.head()
sample_x1,sample_y1,sample_value = testflow.next()['data']
dates = [datetime(2014, 8, 1), datetime(2014, 8, 2)] $ dti = pd.DatetimeIndex(dates) $ dti
grouped2 = df_cod3.groupby(["Death year", "Cause of death"]) $ grouped2.size()
df1 = df1[df1['Title'].str.contains(blacklist) == False] $ df1.shape
frame.apply(f)
store_states.head()
plt.hist(p_diffs); $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency')
df = pd.DataFrame(precipitation_2yearsago, columns = ["prcp","date"]) $ df = df.set_index("date") $ print(df)
campaigns['is_click'].sort_values(ascending = False)[0:5]
df2.reset_index(inplace=True) $ df2.drop(df2.index[df2[df2['user_id'].duplicated(keep=False)]['user_id'].index.values[:-1]], inplace=True); $
print('reduce memory') $ utils.reduce_memory(transactions) $ transactions.info()
tweet_df_clean.shape
df2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']) $ df2
prob_treatment = df2.query('group == "treatment"').converted.mean() $
logs
p_treat_conv = df2.query('group == "treatment"')['converted'].mean() $ p_treat_conv
df
active_stations = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all() $ active_stations $
df.groupby('Year').agg({'Points' : np.sum,'Rank' : np.mean})
nearest_id = lambda row: int(((pair.DATETIME - row).abs() / np.timedelta64(1, 's')).argsort()[:1])
label_freq[0:10]
import datetime $ date = datetime.date(year=2015,month=1,day=8) $ df[df['date'] > date]
import urllib.request $ url = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_train.csv' $ u = urllib.request.urlopen(url) $ rawdata = u.read()
regression = sm.Logit(df['converted'],df[['intercept','treatment']])
full_globe_temp.values
result_df['UK_page']=result_df.ab_page*result_df.UK $ result_df['US_page']=result_df.ab_page*result_df.US $ result_df.head()
result_1 = pd.concat([df1, df3], axis = 1) # concatenate one dataframe on another along columns $ result_1
def convert_time (time): $     new_time = datetime.datetime.strptime(time, "%Y-%m-%d %H:%M:%S") $     new_time = new_time.strftime('%Y-%m-%d') $     return new_time
pd.to_datetime(df['Timestamp']).head()
bikedataframe = bikedataframe.join(pd.Series(1, index=bike_events, name='bike_events')) $ bikedataframe['bike_events'].fillna(0, inplace=True)
autos["price"].value_counts().sort_index(ascending=True).head(20)
merged_df = (odds[odds.team.isin(agg_stats.team.unique())] $                 .pipe(pd.merge, match_results, on=['date', 'team', 'home_game']) $                 .pipe(pd.merge, agg_stats, on=['date', 'team', 'opponent']) $                 .sort_values(by=['game']))
staff['Name']  # Acts similar to dictionary; returns a column's Series
parser.HHParser.regexp.pattern
col('nonsenseColumnName')
pickle_it(logistic, 'logistic_classifier') $ logistic = open_jar('logistic_classifier')
d1 = count_hash_tags(df1['hashtags']) $ d2 = count_hash_tags(df2['hashtags']) $ d3 = count_hash_tags(df3['hashtags']) $ d4 = count_hash_tags(df4['hashtags'])
n_new = df2[df2['group']== 'treatment'].shape[0] $ n_new
lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])
autos.loc[autos["registration_year"].between(1950, 2018), "registration_year" ].shape
bruins_postgame.to_csv('../../../data/sports_analysis/bruins_postgame.csv',   index=False) $ celtics_postgame.to_csv('../../../data/sports_analysis/celtics_postgame.csv', index=False) $ sox_postgame.to_csv('../../../data/sports_analysis/sox_postgame.csv',         index=False)
datacamp.shape
df = pd.read_csv('ab_data.csv', parse_dates=['timestamp']) $ df.head()
wb_list = df_step1.VISS_EU_CD.unique() $ print('number of waterbodies in step 1: {}'.format(len(wb_list))) $ typeA_list = [row.split('-')[0].strip().lstrip('0') for row in df_step1.WATER_TYPE_AREA.unique()] $ print('number of type areas in step 1: {}'.format(len(typeA_list))) $
df7.describe() # basic stats from 600 rows $
history.head()
tree_tunned = GridSearchCV(DecisionTreeClassifier(), tuned_parameters, cv=5, scoring='roc_auc')
np.exp(0.0150)
tweets_df.head().T
so_score_10_or_more.shape
countries = pd.read_csv('countries.csv') $ df3 = pd.merge(df2, countries, on='user_id') $ df3.head()
SMOOTH.plot_init_latency(smoothresult,option="difference")
p_old = df2[df2.landing_page == 'old_page'].converted.mean() $ p_new = df2[df2.landing_page == 'new_page'].converted.mean() $ p_diff_observed = p_new - p_old $ p_diff_observed
top50 = pd.DataFrame(popCon.groupby(by='contact').sum()) $ top50.columns = ['counts'] $ top50.sort_values(by='counts', ascending=False).head(50)
LARGE_GRID.display_fixation_centered(raw_large_grid_df)+xlim((-10,10))+ylim((-10,10))
my_df_small_T
model.fit([train_word, train_char], train_labels,batch_size=20, epochs=20,callbacks=[metrics])
df_clean=df_clean.drop(['favorited','truncated'],axis=1) $
n_new = df2[df2.landing_page == 'new_page'].shape[0] $ n_new
plt.hist(p_diffs) $ plt.axvline(obs_diff, c='r');
descr_text = df.description.tolist() $ cleaned_description = [] $ for tweet in descr_text: $     cleaned_description.append(clean_tweet(tweet))
df_data.show()
w.cfg['indicators'] $ [item.strip() for item in w.cfg['indicators'].loc['din_winter'][0].split(', ')]
shows['release_weekday'] = shows['release_date'].dropna().apply(lambda x: x.strftime('%w')) $ shows['release_weekday'] = shows['release_weekday'].dropna().apply(lambda x: str(x)) $ shows['release_weekday'] = shows['release_weekday'].dropna().apply(lambda x: int(x))
print(autos["seller"].value_counts()) $ print(autos["offer_type"].value_counts())
from jupyterworkflow.data import get_fremont_data $ Fremont = get_fremont_data()
per_var = np.round(pca.explained_variance_ratio_*100, decimals=1) $ labels = ['PC' + str(num) for num in range(1, len(per_var) + 1)]
import folium $ from folium.plugins import MarkerCluster $ from folium import IFrame
z_score, p_value = sm.stats.proportions_ztest([convert_old * n_old, convert_new * n_new], [n_old, n_new], alternative = 'smaller') $ z_score, p_value 
station = combined_turnstile.groupby(["DATE"]) ["TOTALTRAFFIC"].sum().reset_index() # without this reset_index DATE column would be an index $ station
X_age_notnull = X_train[X_train['age'].notnull() & ((X_train['age'] > 10) & (X_train['age'] < 100))] $ print (X_age_notnull.shape[0]) $ X_age_null = X_train[X_train['age'].isna() | (X_train['age'] <= 10) | (X_train['age'] >= 100)] $ print (X_age_null.shape[0]) $
import pandas as pd $ import numpy as np $ import  matplotlib as plt $ %matplotlib inline
ign['Platform'].value_counts()
for topic in ldamodel.show_topics(num_topics=10, formatted=False, num_words=10): $     print("Top terms in topic {}: ".format(topic[0])) $     words = [w for (w, val) in topic[1]] $     print(words)
df_categories = pd.read_csv('categories.csv') $ df_categories.head()
tesla.head()
from scipy import stats $ ben_fin['reverted_mode'] = ben_final.groupby(['userid']).agg({'isReverted': lambda x:stats.mode(x)[0]})
sum(tweet_archive_clean['jpg_url'].isnull())
park = load_data('../../static/parkinson_1960tonow.csv')
X = cc.open $ X.shape $ X = X.values.reshape([785024,1]) $ print(X.shape) $ y= cc.close
train['is_weekend'] = train['start_timestamp'].map(lambda x: 1 if x.weekday() in [5,6] else 0) $ test['is_weekend'] = test['start_timestamp'].map(lambda x: 1 if x.weekday() in [5,6] else 0)
df.info()
temp_cat.value_counts()
print train_data['photos'].apply(len).describe()
lg = sm.Logit(df_new["converted"], df_new[[ "intercept", "ab_page", "CA", "UK",]]) $ res = lg.fit() $ res.summary()
female_journalists_mention_summary_df[['mention_count']].describe()
YS1517['Adj Close'].rank()
df2.head(5)
writers.groupby("Country").all()
df_all_wells_wKNN_DEPTHtoDEPT[0:1000]
images_predictions.info()
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
n_old=df2.query('landing_page == "old_page"').user_id.nunique() $ n_old
tweets_df.created_at
pd.set_option('max_colwidth',150) $ df_en.head()
train = train.replace(['Business', 'Economy'], [1, 0]) $ test = test.replace(['Business', 'Economy'], [1, 0])
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 4) & (jobs.GPU == 0) & (jobs.Group == 'p_meiler')][['Memory','Wait']].groupby('Memory').agg(['mean', 'median','count']).reset_index()
df2.drop_duplicates(subset = ['user_id'],  inplace=True) $ df2[df2['user_id']==773192]
raw = pd.read_excel( param.weather_folder + '/wind-forecast.xlsx') $ raw['TIMESTAMP_UTC'] = pd.to_datetime(raw['TIMESTAMP_UTC']) $ raw = raw.set_index('TIMESTAMP_UTC') $ raw.head()
pd.set_option('display.max_colwidth', 500) $ paired_df_grouped['n_best_co_occurence'] = paired_df_grouped.best_co_occurence.apply(lambda x: len(x)) $ paired_df_grouped[['dataset_1', 'all_co_occurence', 'best_co_occurence', 'n_best_co_occurence']].head(10)
ab_df2.converted.mean()
url_df.head()
percipitation_2017_df.describe()
maximum = df['time_open'].max() $ print(maximum)
fig3 = df['zone'].value_counts().plot('barh', title='Citations by Zone Category')
df_pol['domain'].value_counts().head(20).plot(kind='bar')
rtc = rtc.set_index('Export Region') $ rtc = rtc.drop(['Year'], axis=1)
grader.status()
store_items
df_enhanced = df_enhanced.rename(index=str, columns={"rating_numerator": "rating_10_scaled"})
flattened_df = pd.concat([indexed_df, review_count], axis=1, join_axes=[indexed_df.index]) $ flattened_df.rename(columns={'movie_name': 'review_count'}, inplace=True) $ flattened_df = pd.concat([flattened_df, star_avg], axis=1, join_axes=[indexed_df.index]) $ flattened_df.rename(columns={'stars': 'star_avg'}, inplace=True) $ flattened_df = pd.concat([flattened_df, positive], axis=1, join_axes=[indexed_df.index])
df_columns[ ~df_columns['Descriptor'].isnull() & df_columns['Descriptor'].str.contains('Pothole') ]['Day of the week'].value_counts().head() $
import statsmodels.api as sm $ convert_old = df2[df2['landing_page'] == 'old_page']['converted'].sum() $ convert_new = df2[df2['landing_page'] == 'new_page']['converted'].sum() $ print(convert_old, convert_new, n_old, n_new)
pd.datetime(2012, 5, 1)
from keras.utils import plot_model $ import pydot
vx, vy = lat_lng_to_pixels(v_lat, v_lng)
bloomfield_pothole_data.index = bloomfield_pothole_data['CREATED_ON'] $ bloomfield_pothole_data.info()
k = p.join(train.reset_index(drop=True).project_is_approved)
df_old = df.query('landing_page == "old_page"') $ p_old = df_old[df_old['converted'] == 1]['converted'].count() / df_old[['converted']].count() $ print (p_old)
releases.head()
tweets_pp = pd.concat([multi.reset_index(), pp.reset_index()], axis=1) $ tweets_pp.head(2)
df.drop(todrop1, inplace=True)
Base.classes.keys()
%%time $ intersections = gpd.sjoin(geo_segments_all, geo_TempJams, how="inner", op='intersects')
df=df[df['subreddit']=='Liberal'] $
stock["Date"] = pd.to_datetime(stock.Date, format="%Y-%m-%d", errors='ignore')
flight2.select('arr_time', 'arr_time_local').dtypes $ flight2.select('arr_time', 'arr_time_local').show(2, truncate=False)
tweet_archive.shape
tvecdata= tvec.transform(X_train,y_train)
bb_pivot_table.iloc[[0, -1]]
_ = ok.grade('q05')
df_columns[df_columns['Complaint Type']=='Street Condition']['Descriptor'].value_counts().head() $
Meter1.Errors
multi.handle.value_counts()/multi.shape[0]
import pandas as pd $ import numpy as np $ df=pd.read_csv("F:/Data Analysis process/twitter-archive-enhanced.csv") $ df.head()
text_clf
exp_budget_vote = sorted_budget_biggest.groupby(['original_title'])['vote_average'].mean()
top_10_authors = git_log.loc[:, 'author'].dropna().value_counts().head(10) $ top_10_authors
logit = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']]) $ result=logit.fit()
prcp_df.describe()
eSL['APP_STORE_BRANCH'] = [str.upper(x)[0:6] if str.upper(str(x)).find('CLACORP.COM')>-1 else np.nan $                            for x in eSL.APP_LOGIN_ID.values] $ bSL = f_ELMS_branch_info(pconn=pconn) $ bSL.drop_duplicates(inplace=True) $ aSL = eSL.merge(bSL, how='left', left_on='APP_STORE_BRANCH', right_on='BRANCH')
apple.index.duplicated().sum()
df_countries['CA_new'] = df_countries['new_page']* df_countries['CA'] $ df_countries['UK_new'] = df_countries['new_page']* df_countries['UK']
n_new = len(df2.query('group == "treatment"')) $ n_new
engine.build_classifier_by_key("nhtsa_classifier", # self: named in previous cell. $                                "cdescr", # subject: the text variable to be classified. $                                "injured", # classes: the target variable. $                                df.to_dict(orient='records'), # data dictionary. $                                "cmplid") # primary key: unique identifier for each complaint.
with tb.open_file(filename='data/my_pytables_file.h5', mode='a') as f: $     earray = f.root.my_earray $     earray.append(sequence=matrix[0:1000, :])
df3 = pd.read_csv("countries.csv") $ df3.head()
df_TempIrregular = pd.DataFrame(Irregularities_data,columns=['timeStamp','pubTimeStamp','speed','level','city','lineString'])
plt.hist(taxiData.Trip_distance, bins = 60, range = [0, 3]) $ plt.xlabel('Traveled Trip Distance') $ plt.ylabel('Counts of occurrences') $ plt.title('Histogram of Trip_distance') $ plt.grid(True)
bwd = df[['Store']+columns].sort_index().groupby("Store").rolling(7, min_periods=1).sum()
feables = pd.read_pickle('../data/feables.pkl') $ print(feables.columns) $ feables.head()
df_new.head()
Quandl_DF.info() $ Quandl_DF.tail(5)
extractor = twitter_setup() $ user = extractor.get_user('racheldyap') $ print(user.screen_name) $ print(user.id)
tb.reset_index()
import os $ import pandas as pd $ import numpy as np
def multi(arg0, arg1): $     return arg0*arg1
sponsors_df['Sponsor_Classification']['Reform America Fund']
np.save(CLAS_PATH/'tmp'/'trn_ids.npy', trn_clas) $ np.save(CLAS_PATH/'tmp'/'val_ids.npy', val_clas)
frame.head(3)
data.head(4)
test.columns = ['streamTweets', 'accountDuration', 'numDays', 'allTweets', 'followers', 'following'] $ candSplit.columns = ['streamTweets', 'accountDuration', 'numDays', 'allTweets', 'followers', 'following'] $ test.reset_index(inplace=True, drop=False) $ candSplit.reset_index(inplace=True, drop=False) $ candSplit.head()
os.getcwd() $ os.path.abspath(holding_file_name)
idx2 = pd.IndexSlice $ health_data_row.loc[idx2[2013:2015, 1, 'Bob':'Guido'], 'Temp']
df2.query('landing_page=="new_page"').user_id.count()/df2.shape[0]
df = pd.concat([pd.read_json('http://www.trumptwitterarchive.com/data/realdonaldtrump/%s.json' % (year)) $                 for year in range(2009, 2018)])
df_mes['tpep_pickup_weekday'] = pd.DatetimeIndex(df_mes['tpep_pickup_datetime']).weekday $ df_mes['tpep_dropoff_weekday'] = pd.DatetimeIndex(df_mes['tpep_dropoff_datetime']).weekday
pulledTweets_df.head(20)
1/np.exp(results.params[1])
!mkdir models
train_df = train_df.drop(['Name', 'PassengerId'], axis=1) $ test_df = test_df.drop(['Name'], axis=1) $ combine = [train_df, test_df] $ train_df.shape, test_df.shape
df_ml_6203_01.tail(5)
wrd_clean['expanded_urls'].str.contains('https://www.gofundme.com/').sum()
df.describe()
lr_fit.summary()
itos = [o for o, c in freq.most_common(max_vocab) if c > min_freq]
jobs.loc[jobs.FAIRSHARE == 2].groupby('ReqCPUS').JobID.count().sort_values(ascending= False)
import statsmodels.discrete.discrete_model as sm $ from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
dtm_df['news']
duration_df = duration_df[duration_df['AppointmentDuration'] <= 90]
df[df.location_id==0].head()
df2[df2.duplicated(subset='user_id', keep=False) == True]['user_id']
twitter_archive_master = pd.read_csv('twitter_archive_master.csv')
df_station = pd.DataFrame(list(station_zipcode.items()), columns = ['station', 'zipcode']) $ df_station['zipcheck']=df_station.zipcode.apply(lambda x:len(x)) $ df_station[df_station['zipcheck']!=5] $
df_R['Year']=df_R['Date'].str.slice(0,4) $
import sqlite3 $ import pandas as pd
pos_tweets = ioDF[ioDF.all_sent_x >= .5]
emoji_pattern = emoji.get_emoji_regexp()
df_all.first_affiliate_tracked.value_counts()
$$\mappingFunction(\inputScalar_i) = m\inputScalar_i + c$$
d
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=' + API_KEY + '&start_date=2017-1-1&end_date=2017-12-31') $
prison_talk = BeautifulSoup(response.content, 'lxml')
df_tweet_json_clean.created_at = pd.to_datetime(df_tweet_json_clean.created_at)
line change fron 1 days to just 1
ventas_por_tipo = prop_caba_gba.groupby(['year','property_type']).count()['created_on']
print("Output: Count of missing data\n") $ for col in missing_info: $     num_missing = df_users_first_transaction[df_users_first_transaction[col].isnull() == True].shape[0] $     print('number missing for column {}: {}'.format(col, num_missing))    
df_index = df_r[['price_date']+_factors].copy()
pdata={'Ohio':frame3['Ohio'][:-1],'Nevada':frame3['Nevada'][:2]} $ DataFrame(pdata)
df_new.groupby('group')['converted'].mean()
TestData_ForLogistic.columns
(df2[df2['landing_page'] == "new_page"].user_id.count())/290585
df.T
import matplotlib.pyplot as plt $ %matplotlib inline $ plt.hist(p_diffs); # A sample of 10,000, the Central Limit Theorem has started kicking in $
print('Shape : ', pd_train.shape, '\n') $ print('Type : ', '\n', pd_train.dtypes) $ pd_train.describe()
business_days = pd.to_datetime(business_days, format='%Y/%m/%d').date $ holidays = pd.to_datetime(holidays, format='%Y/%m/%d').date
throwaway_df.select(throwaway_df['count']).show()
df_master[df_master.favorite_count==144118].jpg_url
df2['intercept'] = 1 $ df2['ab_page'] = df2.group.map({'treatment':1,'control':0})
sdf = feature_set.df $ sdf.head()
rf_v1.score_history()
q_derived_count = c.submit_query( $                     ptoclient.PTOQuerySpec().time("2014-01-01","2018-01-01") $                                      .condition("ecn.stable.*") $                                      .condition("ecn.multipoint.*") $                                      .time_series_year()) $
sep2014 = aug2014 + 1 $ sep2014,sep2014.start_time, sep2014.end_time
print('Best features:', gs.best_estimator_.steps[0][1].k_feature_idx_)
Base.prepare(engine, reflect=True) $ Base.classes.keys()
df.printSchema()
set_themes.rename(columns = {'name_x': 'set_name', 'name_y': 'theme_name'}, inplace = True)
education_data_rows[-14:]
reddit = data.drop(['body','id','created', 'is_video', 'thumbnail', 'url', 'timestamp', $                     'time_up', 'time_up_sec', 'time_up_clean'], axis=1)
ann_ret_SP500[9].head(10)
autos['fuel_type'].value_counts()
merged_data['payment_day'] = merged_data['last_payment_date'].dt.day $ merged_data['payment_month'] = merged_data['last_payment_date'].dt.month $ merged_data['payment_year'] = merged_data['last_payment_date'].dt.year
df = pd.DataFrame({'From_To': ['LoNDon_paris', 'MAdrid_miLAN', 'londON_StockhOlm','Budapest_PaRis','Brussels_londOn'],'FlightNumber': [10045, np.nan, 10065, np.nan, 10085],'RecentDelays': [[23, 47], [], [24, 43, 87], [13], [67, 32]],'Airline': ['KLM(!)', '<Air France> (12)', '(British Airways. )', '12. Air France', '"Swiss Air"']}) $ df
df2[df2.duplicated(['user_id'],keep=False)]['user_id']
df2.head()
df['intercept'] = 1 $ df['ab_page'] = pd.get_dummies(df['group'])['treatment'] $ df.head()
df.groupby('author').score.agg(['min', 'max'])[:10]
first_commit_timestamp = git_log.iloc[-1, 0] $ last_commit_timestamp = pd.to_datetime('now') $ corrected_log = git_log[(git_log['timestamp']>=first_commit_timestamp)\ $                         & (git_log['timestamp']<=last_commit_timestamp)] $ corrected_log['timestamp'].describe()
df[['Close','Open']]   # Observe the change of order in column names 
df2.info()
All_tweet_data_v2.retweeted_status_timestamp=All_tweet_data_v2.retweeted_status_timestamp.str.replace(' \+0000','') $ All_tweet_data_v2.retweeted_status_timestamp $ All_tweet_data_v2.retweeted_status_timestamp.dropna().head()
df2[df2.duplicated('user_id',keep=False)] $
control = df2[df2['group']=='control']['converted'].mean() $ control
df.info() $ print("There are no missing values in the dataset")
gr_e2 = df.query("group == 'control' and landing_page == 'new_page'")
df3
df_tick_clsfd_sent = df_tick.join(df_amznnews_clsfd_2tick) $ df_tick_clsfd_sent.info()
order.shape
grades + bonus_points
print (df2[mask])
baseball[['r','h','hr']].rank(ascending=False).head()
session.query(func.count(measurement.date)).all()
by_party_type = candidates.groupby(["party_type", "election_year"]).size().reset_index()
data_after_first_filter = w.get_filtered_data(step=0) # level=0 means first filter $ print('{} rows matching the filter criteria'.format(len(data_after_first_filter)))
counts_df.sample(20)
data['SMA2'] = data['AAPL'].rolling(200).mean() $ data.tail()
googletrend.head()
live_weights.value_counts().sort_index()
df[df['status_type']=='photo'].groupby('dayofweek').status_id.count()
b_df.rename(columns={'TimeCreate':'Date'},inplace=True)
from sklearn.metrics import accuracy_score
data.L2.unique()
input_dir = '/Users/jeriwieringa/Dissertation/text/text-current/2016-11-16-corpus-with-preliminary-cleaning/'
ps['2011-01']
(own_star[~own_star['starred'].isnull() & ~own_star['owned'].isnull()]).shape
pumaPP.head()
tweets_clean.pupper.value_counts()
sample_size = 100000
print searchRange(5,4,7)
%matplotlib inline $ plt.hist(threeoneone_geo['fix_time_sec']/60,bins=100) $ plt.show() $
import pickle
df = pd.read_csv('twitter-archive-enhanced.csv')
df = pd.read_sql('SELECT COUNT(*) FROM city;', con=conn) $ df
dog1
sorted_ct.select('count').toPandas().hist(bins=15)
!file data/*
print_body(response)
df.columns = df.columns.str.strip() $ df.columns
print(r.json())
a=pd.DataFrame(general_volume_types.groupby('complaint_type').f1_.sum()) $ a.sort_values('f1_', ascending=False)[0:26]
open('data/wx/tmy3/proc/700197.csv').readlines()[:6]
df_d.describe()
caps1_output_tiled
users_not_odu.groupby(users['Asked'] > 0).mean()
question_2_dataframe_in_top_zips = question_2_dataframe_in_top_zips.size().to_frame(name='count')
df_master[df_master.rating_numerator==1776].jpg_url
autos["model"].value_counts()
p_old=df2['converted'].mean() $ p_old
def split1(elem): $     elem = elem.replace('POINT (', '') $     elem = elem.replace(')', '') $     return elem.split(' ')[0]
weather_df.shape
df2.query('group=="control"')['converted'].mean()
modelLGB.fit(X,y) $ submission =  pd.DataFrame( modelLGB.predict_proba(test_X)[:,1] , index = test['comment_id'] , columns = ["is_fake"] ) $ submission.to_csv('submission_Boosting.csv')
auto.describe().transpose()
session.query(Measurement.date, Measurement.prcp).filter(Measurement.date > '2016-08-23').all() $
client = MongoClient('ec2-34-198-246-43.compute-1.amazonaws.com', 27017) $ db = client.renthop $ collection = db.listings $ pp_bold('{} listings'.format(collection.count()))
df['time_detained'].describe()
rfmTable = df.groupby('cust_id').agg({'order_date': lambda x: (NOW - x.max()).days, 'count': lambda x: len(x), 'total_spend': lambda x: x.sum()}) $ rfmTable['order_date'] = rfmTable['order_date'].astype(int) $ rfmTable.rename(columns={'order_date': 'Recency', $                          'count': 'Frequency', $                          'total_spend': 'Monetary_value'}, inplace=True)
df1['io_state']= df1.io_state.apply(lambda x: x.zfill(8)) $ df2['io_state']= df2.io_state.apply(lambda x: x.zfill(8)) $ df3['io_state']= df3.io_state.apply(lambda x: x.zfill(8)) $ df4['io_state']= df4.io_state.apply(lambda x: x.zfill(8)) $ df5['io_state']= df5.io_state.apply(lambda x: x.zfill(8)) $
highestobs_station = active_station[0][0] $ highestobs_station
my_tree_one = tree.DecisionTreeClassifier() $ my_tree_one = my_tree_one.fit(features_one, target)
df2_treatment = df2.query('group == "treatment"') $ agg_df2_treatment = df2_treatment.query('converted == "1"').user_id.nunique() / df2_treatment.user_id.nunique() $ agg_df2_treatment
df[df.pre_clean_len > 140].head(10)
m.fit(lr, 3, metrics=[exp_rmspe], cycle_len=1)
cats = shelter_cleaned_df.loc[shelter_cleaned_df['AnimalType'] == 'Cat'] $ cats.describe() $
data1_new=data1_new.reset_index(drop=True) $ data1_new.head()
adopted_cats.loc[adopted_cats['Color']=='Blue/Blue','Color']='Blue' $ adopted_cats.loc[adopted_cats['Color']=='Blue/Tortie','Color'] = 'Blue Tortie' $ adopted_cats.loc[adopted_cats['Color']=='Blue Cream','Color'] = 'Blue/Cream' $ adopted_cats.loc[adopted_cats['Color']=='Tortie/Blue Tabby', 'Color'] = 'Tortie Blue Tabby' 
df
autos = autos[autos['registration_year'].between(1900,2016)] $ autos['registration_year'].describe()
hired = data.loc[data['hired']==1].tasker_id.value_counts() $ hired[:5]
act_diff = df2[df2['group']=="treatment"]["converted"].mean()-df2[df2['group']=="control"]["converted"].mean() $ (p_diffs > act_diff).mean()
support.amount.sum() / merged.amount.sum()
StockData.describe()  # pandas.DataFrame.describe() shows statistics on each numeric column
menus_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/menus.csv')['Body'].read().decode('utf-8') $ menus = pd.read_csv(StringIO(menus_csv_string), header=0)
sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK']]).fit().summary()
df_twitter_archive_master
sub2 = submissions[['hacker_id', 'challenge_id', 'created_at']]
import os $ path = os.getcwd() $ os.chdir(os.path.join('..', '..', 'notebook_format')) $ from formats import load_style $ load_style(css_style = 'custom2.css')
sl_data3 = sl_data1[['Description', 'Date', 'Totals']] $ sl_data3
print("Largest Change in any one day : " + str(max(daily_change)))
df2['tweet_id'][(df2['predict_2_breed'] == True)].count()/2075 $
orig_ct = len(dfd) $ dfd = dfd.query('capacity_5F_max >= 5000. and capacity_5F_max <= 70000.') $ print(len(dfd) - orig_ct, 'eliminated')
df2[["treatment", "control"]] = pd.get_dummies(df2["group"])
liquor2015_combined.head()
d311 = pd.read_csv(dic_inp["detroit-311.csv"], quotechar='"',converters={"lng":float,"lat":float}) $ d311.drop_duplicates(subset="ticket_id") $ d311.rename(columns={"lng":"long"},inplace=True)
df_yt_resolved = pd.DataFrame(meta)
intervention_history.columns
trump = api.user_timeline(id='realDonaldTrump') # last 20 tweets
dictionary = corpora.Dictionary(texts)
temp_series_paris_naive.tz_localize("Europe/Paris", ambiguous="infer")
tweet=results[16] $ for param in dir(tweet): $     if not param.startswith("_"): $         print "%s : %s" % (param, eval("tweet." + param))
autos["price"] = autos["price"].str.replace("$","") $ autos["price"] = autos["price"].str.replace(",","") $ autos.price = autos.price.astype(float)
df.query('converted==1')['user_id'].nunique()/df.user_id.nunique()
people.iloc[1:3]
data['Sales'] = data['Sales'].str.replace(',', '') $ data['Sales'] = pd.to_numeric(data['Sales'])          #or data['Sales'] data['Sales'].astype(int) $ data.dtypes
r = df['upvotes'].groupby(df['author']).sum() $ r.head()
print("Unique values from odometer_km column:", autos["odometer_km"].unique().shape[0]) $ print("Unique values from price column:",autos["price"].unique().shape[0])
nan_tables = {} $ overviews = {} $ for res_key, df in data_sets.items(): $     data_sets[res_key], nan_tables[res_key], overviews[res_key] = find_nan( $         df, res_key, headers, patch=True)
get_assignment('ariel/ariel3/vlf-receiver_fixed-frequency/signal_strength/DATA2_DR002106_DR002106_20080804_071839/dr002106_f00001.phys.1')
df = pd.read_csv('../../../Comments_FanofGame.csv')
xgb_learner.random_search(100, verbose=True)
with open('data.json', 'w') as f: $     json.dump(r.json(), f)
df_inds = pd.concat((df_inds1, df_inds2, df_inds3, df_inds4, df_inds5, $                      df_inds6, df_inds7, df_inds8, df_inds9, df_inds10), axis=1) $ df_inds.info()
columns = inspector.get_columns('measurement') $ for c in columns: $     print(c['name'], c["type"])
df_concensus_uaa = df_concensus_uaa.sort_index()
df = pd.read_sql('SELECT hotel_name, contact_email FROM hotel WHERE hotel_name=\'Hyatt Regency Sydney\'', con=conn_a) $ df
bets.drop('bet_either', 1, inplace=True)
df_uro_dd_dummies_no_sparse = df_uro_dd_dummies.drop(columns=ls_sparse_cols)
dfWeek = dfWeek.rename(columns = {'Weighted Value': 'Weighted Value (Weekly)', $                                  'Contract Value (Daily)': 'Contract Value (Weekly)'})
working_directory =r'<please provide a working directory on your machine here>' $ web_service_name = 'multitext2-classifier' $ web_service_full = full_text_classifier.deploy(web_service_name= web_service_name, $                        config_file_path=deployment_config_file_path, $                        working_directory= working_directory)  
watershed = gpd.GeoDataFrame.from_features(result['watershed'], crs={'init': 'epsg:4326'})
pd.read_sql(f'explain {sql}', engine).head()
pd.read_sql(f'explain {sql_sub}', engine).head()
combined_df[['CA', 'US']] = pd.get_dummies(combined_df['country'])[['CA','US']] $ combined_df.head()
kick_projects_ip[features].shape
df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], $              index=['Mon', 'Tues', 'Wed'], $              columns=['A', 'B', 'C'])
df2.drop(df2.index[2893], inplace=True) $ df2.head()
Y, X = dmatrices('Y ~ X', data=df) $ mod2 = sm.OLS(Y, X) $ res2 = mod2.fit() $ res2.summary2()
real_runtimes = shows[shows['fixed_runtime'] <= 100]
new_df=df[['Principal','terms','age','Gender','weekend']] $ new_df=pd.concat([new_df,pd.get_dummies(df['education'])], axis=1) $ new_df.drop(['Master or Above'], axis = 1,inplace=True)
WorldBankLimited =  sqlContext.sql("select * from worldbank limit 2") $ print type(WorldBankLimited) $ print "*" * 20 $ print WorldBankLimited
df = pd.DataFrame(np.random.randint(0, 50, 32).reshape(8, 4), columns=list('WXYZ'), index=list('abcdefgh')) $ df1 = df.ix[2:, ['W', 'X']] $ df2 = df.ix[:5, ['Y', 'Z']] $ print df, '\n\n', df1, '\n\n', df2
corr_aggs = [] $ corr_aggs.extend(manual_aggs) $ corr_aggs.extend(computed_aggs)
taxiData.head(10)
evaluator.get_metrics("macro_f1")
goodTargetUserItemInt=targetUserItemInt[targetUserItemInt['label']>0.0] $ print goodTargetUserItemInt.shape
np.vstack((a, a))[0:3,:]
tweet_2 = pd.read_json('tweet_json.txt') $
donations.groupby('Donor ID')['Project ID'].value_counts().head(10)
data = np.linspace(0,1,201) $ print(data)
df2.loc[df2['user_id'].duplicated(), :]['user_id']
engine.execute('SELECT * FROM Station').fetchall()
plt.figure(figsize=(16,10)) $ plt.clf() $ sns.boxplot(x='Journal',y='PubDays',hue='Publisher',data=df_less) $ plt.xticks(rotation=90)
null_desc = raw_data[(raw_data['desc'].isna())] $ null_desc.T
import pandas as pd $ import numpy as np
import requests $ google_request = requests.get("https://google.com") $ print("Status code: ", google_request)
tran_time_diff.info()
ab_data = pd.read_csv("ab_Data.csv") $ ab_data.head()
eTL['APP_STORE_BRANCH'] = [str.upper(x)[0:6] if str.upper(str(x)).find('CLACORP.COM')>-1 else np.nan $                            for x in eTL.APP_LOGIN_ID.values] $ bTL = f_ELMS_branch_info(pconn=pconn) $ bTL.drop_duplicates(inplace=True) $ aTL = eTL.merge(bTL, how='left', left_on='APP_STORE_BRANCH', right_on='BRANCH')
logit_results.summary()
def relevant_ping(p): $     parent = p.get('payload', {}).get('keyedHistograms', {}).get('IPC_MESSAGE_SIZE', {}).get('PLayerTransaction::Msg_Update') $     content = p.get('payload', {}).get('processes', {}).get('content', {}).get('keyedHistograms', {}).get('IPC_MESSAGE_SIZE', {}).get('PLayerTransaction::Msg_Update') $     return parent is not None and content is not None and parent['sum'] == content['sum'] $ relevant_pings = pings.filter(relevant_ping)
df1_after_df2 = df2.append(df1) $ df1_after_df2
liberia_data.Description
print(sl.mindate.min()) $ print(sl.maxdate.max())
df.all()  # checks if all the elements in each column are non-null
test = prep_data(test)
df2[mask]
top_headlines = newsapi.get_top_headlines(q='trump', sources='bbc-news', language='en') $ top_headlines['articles'][0]
df_2018.columns
pothole = df[df['Descriptor'] == 'Pothole'] $ pothole.groupby(pothole.index.weekday)['Created Date'].count().plot()
insert = users.insert() $ print (insert) $ insert = users.insert().values(name = 'kim', fullname = "Anonymous kin") $ print (insert)
fit1_test = fh_1.transform(test.device_model) $ fit2_test = fh_2.transform(test.device_id) $ fit3_test = fh_3.transform(test.device_ip)
preds = learn.predict(is_test=True)
df['body'] = df['body'].apply(lambda x: x.lower());
df.head()
tw_clean.rating_denominator.value_counts()
AAPL.index
print (r.text[0:500])
data = pd.concat([gdp, pop], axis=1); data  #Concatenate two Series to a DataFrame.
1.0*sum(df2['converted']==1)/ n_valid_users
old_page_converted= np.random.binomial(1, p_old, n_old)
conn.addtable(table='myclass', caslib='casuser', $               **mydmh.args.addtable)
if not os.path.isdir('output/wind_generation'): $     os.makedirs('output/wind_generation')
import pandas as pd
df_clean.rating_denominator = df_clean.rating_denominator.astype(str)
c_secondDigit3 = (df['c'].cast('string').substr(3, 1) == '3')
creations.ix[null_reg.index, "creator_autoconfirmed"] = True $ creations[ creations["user_registration"].isnull() ]["creator_autoconfirmed"]
contrib_state.amount = contrib_state.amount.astype(int)
issubclass(parser.BadLine, Exception)
preprocessor = PreProcessor(titanic, copy=True) $ print("We made a copy so id titanic :  {} different from id preprocessor.data {}".format( $         id(titanic),id(preprocessor.data)))
from bs4 import BeautifulSoup#Beautiful Soup is reading the HTML and making sense of its structure.
df2_control = df2[df2['group'] == 'control']['converted'].mean() $ print('The probability that an individual in the control group converted is: {}'.format(round(df2_control, 4)))
df_result = df[df.State.isin(['Failed', 'Successful'])] $ pd.crosstab(df_result.launched_year, df_result.State)
response = Query(git_index).get_min("author_date")\ $                                    .by_authors("author_name")\ $                                    .fetch_aggregation_results() $ print(buckets_to_df(response['aggregations']['0']['buckets']).head())
import pandas as pd $ data_1 = pd.read_csv(url_1) $ data_2 = pd.read_csv(url_2)
import sys $ import os $ from urllib.request import urlretrieve
barcelona.dtypes
series[1:4]
db.fetch_all_data()
cassession.builtins.serverstatus()
(null_values > pop_diff).mean()
df.groupby('episode_id').count().head()
autos['price'].value_counts().sort_index(ascending=True).head(20)
import requests
twitter_archive_master[(twitter_archive_master.iloc[:,8:12].sum(axis=1) == 1) & (twitter_archive_master['has_stage'] == 0)]
%%time $ PredClass.add_dummies()
print("Percentage of merchants not churning: %s %%" % "{0:.3}".format(100*start/(start+trial)))
users.email = users.email.apply(code_email) $ users.creation_source = users.creation_source.apply(code_creation)
index_group2 = user_group['Group'].apply(checkGroup, number=2) $ user_group2 = user_group[index_group2] $ print(len(user_group2)) $ user_group2.head()
dfMeta.head(5)
df4["intercept"] = 1 $ logit_mod = sm.Logit(df4["converted"], df4[["intercept","CA","UK"]]) $ results = logit_mod.fit() $ results.summary()
api = api_auth() $ tweets = [] $ for status in tweepy.Cursor(api.user_timeline, screen_name='@realDonaldTrump', count=200, max_id = 1017025848409550848).items(): $     tweets.append(status) $
measure_df.info() $
min(change_values)
control_new = df.query('group == "control" and landing_page == "new_page"') $ treatment_old = df.query('group == "treatment" and landing_page == "old_page"') $ unmatched = len(control_new) + len(treatment_old) $ print(unmatched)
df[df.bottles_sold==2508]
shortcodes[31]
np.savetxt('myfile.csv', a, delimiter=',')  #Save `a` as a CSV file (comma seperated values, can be read by MS Excel).
crime_data["OFNS_DESC"].sort_values().unique()
p_mean = np.mean([p_new, p_old]) $ print('The mean conversion rate of p_new and p_old:',p_mean)
def streak(team, month, day, year): $     team = teamtables[team] $     team = team[team["Date"]<datetime.datetime(year,month,day)] $     return team["Streak"][len(team["Streak"])-1]
%matplotlib inline
!python -c "import numpy; numpy.info(numpy.add);"
dt.time()
sns.distplot(df_predictions_clean.img_num) $ plt.xlabel("Image Number") $ plt.ylabel("Count")
autos["price"].value_counts().head()
url_speaker = 'https://api.ted.com/v1/speakers.json?api-key=ynw2u8e4h9sk8c2htp7vutxq'
result['portalId'].nunique()
startlist = ['2015', '2016', '2017', '2016-04', '2017-04'] $ for s in startlist: $     returns_vol_tbl(start=s, assets=assets_).plot.barh() $     plt.title("start @" + s) $     plt.show()
from sklearn.ensemble import RandomForestClassifier $ rf = RandomForestClassifier()
column_datasets = {'train': ColumnarDataset.from_data_frame(trn_df, cat_vars, trn_y), $                    'val': ColumnarDataset.from_data_frame(val_df, cat_vars, val_y)}
building_pa_prc.shape
grouped = df_providers.groupby(['year','drg3']) $ grouped_by_year_DRG =(grouped.aggregate(np.sum)['disc_times_pay']) $ grouped_by_year_DRG.head()
test= test.reset_index(drop = True) $ test['expenses'] = pd.Series(predictions) $ datatest = pd.concat([train, test]) $ datatest = datatest.reset_index(drop = True)
df = sqlContext.createDataFrame(rows)
na_df.notna() # check elements that are not missing
staff.Name 
strfilter1 = "CAPITAL|ASSET MANAGEMENT" $ strfilter2 = "CAPITAL|ASSET MANAGEMENT|ADVISORS|ADVISERS|HOLDINGS|FINANCIAL|HEDGE|SECURITIES" $ hedgeprop1 = bigdata[bigdata['Company Name'].str.upper().str.contains(strfilter2)].sort_values('CIK') $ hedgeprop1
skip.head(4)
df.if_fielding_alignment.value_counts()
cityID = '8e9665cec9370f0f' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Minneapolis.append(tweet) 
np.exp(results.params) 
cX_test.head()
df_new['CA_ab_page'] = df_new['CA'] * df_new['ab_page'] $ df_new['UK_ab_page'] = df_new['UK'] * df_new['ab_page'] $ df_new.head()
df_only_headline[df_only_headline["tags"].apply(lambda x: "rstats" in set(x))].groupby(["headline", "date"]).size()
from sklearn.decomposition import PCA $ PCA(2)
median = df['time_open'].quantile(q=0.5) $ print(median)
sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)] $ sheet_data[:2]
tier2 = getcrimesby_tier(store1,Property) $ tier2_df = tier2.copy()
log_mod_new=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA','UK']]) $ results_new=log_mod_new.fit() $ results_new.summary2()
test1.head(10)
df1 = pd.DataFrame({"A":["A1", "A2"], $                     "B":["B1","B2"]},index=[1,2]) $ df2 = pd.DataFrame({"A":["A3", "A4"], $                     "B":["B3","B4"]},index=[3,4]) $ pd.concat([df1,df2], axis="col")
w.get_step_object(step = 3, subset = subset_uuid).indicator_objects['din_winter'].get_ref_value(type_area = '1s', salinity = 25)
print('Scores') $ print("Mean is {:0.2f} and the Median is {:0.2f}".format(np.mean(df.score),np.median(df.score))) $ print('Number of coments') $ print("Mean is {:0.2f} and the Median is {:0.2f}".format(np.mean(df.comms_num),np.median(df.comms_num))) $
df2.drop(146212,axis=0,inplace=True);
order_data['OrderPeriod'] = order_data.created.apply(next_weekday)
cols = ['chanel', 'Costs'] $ chanel_costs = pd.merge(Relations, Costs, how='left',left_on=['name', 'id_partner'], right_on=['campaign', 'id_partner']) $ chanel_costs.Costs = chanel_costs.Costs.fillna(0) $ chanel_costs = chanel_costs[cols].groupby('chanel', as_index=False).sum() $ chanel_costs.head()
len(df[~(df.groups == {})])
lin_svc_clf = LinearSVC(random_state=2018, C = 0.055)
df.sample(20)
theft.iloc[0:5]
import matplotlib.pylab as plt $ fig, ax = plt.subplots(figsize=(12,8)) $ weather.hist( ax = ax); $ plt.show()
cold_tweets.head()
((df-df_from_csv)**2 < 1e-25).all()
sel1=[Measurement.date, $      func.sum(Measurement.prcp)] $ all_prcp=session.query(*sel1).group_by(Measurement.date).all() $
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old,n_new],alternative='smaller') $ z_score, p_value
[random_date(pd.datetime.now() - pd.offsets.Day(10), pd.datetime.now()) for _ in range(10)]
twitter_archive_full[twitter_archive_full.tweet_id.isin(duplicated_list)][['tweet_id','stage']].sort_values('tweet_id')
avg_comments = reddit['Comments'].mean() $ reddit['Above_Below_Mean'] = np.where(reddit['Comments']>=avg_comments, 'Above', 'Below')
df_onc.columns $ df_uro.columns
run txt2pdf.py -o 'Sepsis Payments as % of Total Payments.pdf'  'Sepsis Payments as % of Total Payments.txt' $
plt.boxplot(trips_data['duration']/3600) # convert to hours and we can observe that $ plt.show() $ plt.boxplot(reduced_trips_data['duration']/3600) # using outliers filtered dataset shows all $ plt.show()
tmdb_movies_production_countries_revenue.shape
print(train_data.notRepairedDamage.isnull().sum()) $ print(test_data.notRepairedDamage.isnull().sum())
s[s.between(11, 14)].head()
print(type(data.points)) $ data[['points']] $
locations.State.unique()
mpbrand_series = pd.Series(mean_price_by_brand).sort_values(ascending=False) $ mpbrand_series
(null_vals > obs_diff).mean()
known_shorteners = ux.constants.all_short_domains.copy() $ known_shorteners[:25]
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ print(z_score, p_value)
ds_train = FileDataStream.read_csv(train_file, collapse=False, header=False, names=columns, numeric_dtype=np.float32, sep='\t', na_values=[''], keep_default_na=False) $ print(repr(ds_train.Schema)) $ print(ds_train.Schema)
data.sample(5)
inter = mr * vol $ inter = inter.between_time('9:30', '16:00') $ lagged_inter = inter.tshift(1, 'min').between_time('9:30', '16:00') $ pd.ols(y=mr, x=lagged_inter)
n_new = df2.query('landing_page == "new_page"').shape[0] $ print('Number of occurrences for landing_page == "new_page": {}'.format(n_new))
filter_df.shape
print(teams1)
print "modelType: " + saved_model.meta.prop("modelType") $ print "trainingDataSchema: " + str(saved_model.meta.prop("trainingDataSchema")) $ print "creationTime: " + str(saved_model.meta.prop("creationTime")) $ print "modelVersionHref: " + saved_model.meta.prop("modelVersionHref") $ print "label: " + saved_model.meta.prop("label")
final_topbike = top_bike[top_bike['Distance'] != 0.000000]
timestamps = [] $ for k in range(len(bird_data)): $     timestamps.append(datetime.datetime.strptime(bird_data.date_time.iloc[k][:-3], '%Y-%m-%d %H:%M:%S')) $ print(timestamps[:5])
len(df_meta['collection'].unique())
cityID = '1d9a5370a355ab0c' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chicago.append(tweet) 
X_train, X_test, y_train, y_test = train_test_split(X_s_n, y_sc, test_size=0.5, random_state=42)
test = np.array(([3,3],[9,1])) $ print(test) $ print(test.shape) $ print(test.shape())
tweet_archive_clean = tweet_archive_clean.sort_values(by='dog_stage').drop_duplicates(subset='tweet_id', keep='last')
df.drop(df.loc[(df["group"]!="treatment") & (df["landing_page"]=="new_page") ,  ["group","landing_page"]].index, inplace=True) $ df.drop(df.loc[(df["group"]=="treatment") & (df["landing_page"]!="new_page") ,  ["group","landing_page"]].index, inplace=True)
dict2 = {k:g['DATETIMEENTRIESVAL'].tolist() for k, g in df.groupby('TUPLEKEY')} $ dict2[('A002','R051','02-00-00','LEXINGTON AVE')]
index_to_change = df3[df3['group'] == 'treatment'].index $ df3.set_value(index = index_to_change, col = 'ab_page', value = 1) $ df3.set_value(index = df3.index, col = 'intercept', value = 1) $ df3[['intercept', 'ab_page']] = df3[['intercept', 'ab_page']].astype(int) $ df3 = df3[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
def df_norm(df) : $         newdf = (df - df.mean()) /(df.max() - df.min()) $         return newdf - newdf.min()
from sklearn.metrics import accuracy_score, classification_report $ score = 100.0 * accuracy_score(y_test, predicted) $ print(f'Logistic Regression [Challenger Data] Score = {score:4.1f}%\n') $ print(classification_report(y_test, predicted))
windfield_matched_array=windfield_matched.ReadAsArray() $ print('windfield shape = '+ str(shape(windfield_matched_array))) $ print('ndvi_change shape = '+ str(shape(ndvi_change.values)))
by_party = candidates.groupby(["party_name", "election_year"]).size().reset_index()
converted = (len(df[df['converted']==1]))/df.shape[0] $ not_converted = (len(df[df['converted']==0]))/df.shape[0] $ converted, not_converted
print '"VersionNum"' $ checkObsValsAreIntegers(df,'VersionNum',['int16','int32','int64'])
tweetsF03 = pd.read_sql_query("SELECT created_at, extracted FROM tweets_info;", connF03, parse_dates=['created_at'] ) $ tweetsF03['created_at'] = tweetsF03['created_at'].dt.tz_localize("UTC").dt.tz_convert("Europe/Berlin") $ print("Number of Tweets: %s" %len(tweetsF03)) $ tweetsF03.head()
f_close_clicks_device_train.show(3)
plt.rcParams['axes.unicode_minus'] = False $ dta_52.plot(figsize=(15,5)) $ plt.show()
df2['landing_page'].value_counts()
outcome.shape
symbol = "SENSEX" $ stock_data = stockchart(symbol)
ts.info()
df.index
import pandas as pd $ import numpy as np $ import seaborn as sns $ import matplotlib.pyplot as plt $ %matplotlib inline
from numpy import median $ width, height = 12, 6 $ plt.figure(figsize=(width, height)) $ ax = sns.barplot(x="Memory", y="Wait", data=jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0) & (jobs.Group == 'cms')], estimator = median, ci= None) $ ax.set_yscale('log') $
'Backtest on training set: 2006 - 2015' $ print('Baseline RMSE: ',Baseline_training_RMSE) $ print('      M7_RMSE: ',M7_RMSE)
k = 15 $ kmeans = KMeans().setK(k).setSeed(10388876432723).setFeaturesCol("features") $ model = kmeans.fit(transformed) $ clustered_hashtags = model.transform(transformed).select('hashtag', 'prediction')
for k in range(10,20): $     the_small_df = find_sites_in_STATE_or_REGION_YEAR_DRG( States[k],2015,871,2500) $ print('Completed')
stanford_word_list.head()
success_classifier = DecisionTreeClassifier() $ success_classifier.fit(X_train, y_train) $ predictions = success_classifier.predict(X_test) $ accuracy_score(y_true = y_test, y_pred = predictions)
df.groupby('main_category')['ID'].count()
target = 'project_is_approved' $ sum(data.project_is_approved==1)/data.shape[0]
x=df2['degree']
df2 = df2.drop_duplicates(subset = ['user_id'])
autos.hist(column = 'price', bins = 100)
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline
def readData(startDate,endDate,compTicker): $     dataF = pandas_datareader.DataReader(compTicker,"yahoo",startDate,endDate) $     return dataF
overallGarageCars = pd.get_dummies(dfFull.GarageCars)
autos['price'].value_counts().sort_index().head(20)
df1.query("landing_page == 'new_page' and group == 'control'").shape[0] + \ $ df1.query("landing_page == 'old_page' and group == 'treatment'").shape[0]
for d in ['DOB_clean', 'Lead_Creation_Date_clean']: $     Train[d] = map(date.toordinal, Train[d]) $     Test[d] = map(date.toordinal, Test[d])
import sqlite3 $ import pandas as pd $ import matplotlib.pyplot as plt $ from textblob import TextBlob #https://textblob.readthedocs.io
df_links = df_links[df_links['link.domain'] != 'twitter.com'] $ print("N = {}".format(len(df_links))) $ df_links.head(3)
%load "solutions/sol_2_25.py"
for i in range(len(xmlData)): $     commas = xmlData.loc[i, 'address'].count(',') $     if (commas < 5): $         print 'Index: ' + str(i) $         print xmlData.loc[i, 'address']
merged1.columns
cl_ca.columns = ['XML_' + x if x!='APPLICATION_ID' else 'APP_APPLICATION_ID' for x in cl_ca.columns.values]
df.loc[index_name]
for i in check_cols: $     print(train[i].value_counts()) $     print('')
print(z_score) $ print(p_value)
August_deaths_Guinea = result1.iloc[0]['Guinea_deaths Mean'] $ September_deaths_Guinea = result1.iloc[1]['Guinea_deaths Mean'] $ October_deaths_Guinea = result1.iloc[2]['Guinea_deaths Mean'] $ October_deaths_Guinea
autos["seller"].value_counts()
j_pd = join_d.toPandas()
n_new,n_old = df2.landing_page.value_counts() $ print(n_new,n_old)
pd.crosstab(result['timestampUTC'].dt.year, result['type'])
print("Probability of user converting:", df2.converted.mean()) #using mean to find the  probability of an individual converting regardless of the page they receive
nt = nt[nt["sq_price_value"] != 0.0]
import pandas as pd
mgxs_lib.mgxs_types = ['nu-transport', 'nu-fission', 'fission', 'nu-scatter matrix', 'chi']
df_links.loc[:, 'link.domain'] = df_links.apply(resolve_shortened_link, axis=1)
overall_topics.head()
tweet_archive_clean = pd.merge(tweet_archive_clean, info, on = 'tweet_id', how = 'inner')
(float(new_page_converted)/n_new) - (float(old_page_converted)/n_old)
import statsmodels.api as sm $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ print(f"convert_old: {convert_old}") $ print(f"convert_new: {convert_new}")
countries = pd.read_csv('countries.csv') $ countries.sample(5) $ combined_df = countries.merge(df2, left_on = 'user_id', right_on = 'user_id', how = 'inner') $ combined_df[['UK', 'CA']] = pd.get_dummies(combined_df['country'])[['UK','CA']] $ combined_df.sample(5)
summary.head()
result1.summary2() $
df_unique.to_csv('./data/unique_feature_list.csv', encoding='utf-8')
merged_df_flight_cancels.head()
reddit_info.shape
hawaii_df.describe()
house_data.describe()
df.loc[df['last_name']=='Copening', 'age'] = df.age.median() $ df.loc[df['last_name'] == 'Copening']
HARVEY_WORDS_160 = ['water','helps','community','storm','call','prepared','informed','hurricane','rains','stay','phone','talking','evacuate','homes','news','leave','rescue','tornado','food','boat','family','neighborhood','church','neighbor','media','daughter','fema','bayou','freeway','warnings','trucks','facebook','weather','emergency','husband','lights','radio','supplies','plans','money','power','channel','stations','wind','damage','forecast','road','volunteers','decision','shelter','driving','ditches','county','situation','devastation','messaging','roofs','alerts','friends','gulf','police','rita','cell','texas','lake','television','seniors','survival','insurance','rising','worry','stuck','drain','electricity','fight','highway','announcing','drainage','governments','grocery','organization','pumps','elders','safe','decide','higher','officials','afraid','danger','fire','horrible','katrina','mayor','rainfall','surge','trapped','deep','disabled','leaders','strong','blowing','helicopter','identifying','relatives','bayous','blessed','contact','hospital','praying','residents','resources','batteries','canned','charge','drown','frustrating','hispanic','hurt','latino','mexican','nature','predict','response','risk','river','univision','attic','deaths','floating','generator','immigrant','preparedness','route','safety','tide','tropical','agency','coverage','disaster','doctors','faith','mandatory','medicine','meteorologist','reservoirs','authorities','broadcasting','canal','dark','engineers','hotel','inundated','landlines','levies','pets','screaming','stranded','underwater','escape'] $ HARVEY_WORDS_92  = ['water','helps','community','storm','call','prepared','hurricane','rains','stay','phone','talking','evacuate','homes','news','leave','rescue','tornado','boat','family','church','neighbor','media','fema','bayou','warnings','facebook','weather','emergency','lights','radio','plans','money','power','channel','wind','damage','forecast','decision','shelter','devastation','alerts','gulf','police','rita','cell','lake','seniors','rising','stuck','drain','electricity','drainage','governments','safe','decide','higher','officials','afraid','danger','katrina','mayor','rainfall','surge','trapped','deep','disabled','leaders','strong','bayous','blessed','hospital','praying','frustrating','mexican','risk','river','univision','generator','safety','tide','agency','disaster','faith','mandatory','medicine','reservoirs','hotel','levies','pets','stranded','underwater']
def lm (x, y): $     lm = sm.formula.ols(formula = 'y ~ x', data = data).fit() $     x_new = pd.DataFrame({'newdata' : range(1,len(x)+1)}) $     y_preds = lm.predict(x_new) $     print(lm.summary())
!ls
lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])
convo3
year_temp = session.query(Measurement.station, Measurement.date, Measurement.tobs).\ $         filter(Measurement.date.between('2016-01-01' , '2017-01-01')).\ $         filter(Measurement.station == most_active).all() $ year_temp
tweets = pd.read_csv('LaManada_new/tblposts.csv',sep=SEP) $ tweets.shape
tweets_df.retweeted.value_counts()
scores = cross_validate(lr2, X, y, cv=10, $                         scoring=['accuracy'], $                         return_train_score=True $                        ) $ scores
import pprint $ summary = learn.summary() $ print(str(pprint.pformat(summary))[:1000])
h2o.init() $
year_precip = session.query(Measurement.date, Measurement.prcp).\ $         filter(Measurement.date.between('2016-01-01', '2017-01-01')).all() $ year_precip
zipcodesdetail.zipcode = zipcodesdetail.zipcode.apply(fill_zipcode)
table = driver.find_element_by_xpath('//*[@id="body"]/table[2]') $ table.get_attribute('innerHTML').strip()
autos_real
goes_lightcurve =  lc.GOESLightCurve.create('2011-06-07 06:00', '2011-06-07 08:00') $ hsi_lightcurve = lc.RHESSISummaryLightCurve.create('2011-06-07 06:00', '2011-06-07 08:00')
n_treatment = df2.query('group == "treatment"').shape[0] $ n_treatment
df = pd.DataFrame(predict_result, columns = ["prob_0", "prob_1"])
bankID = pd.read_csv('data/bankID.csv', index_col=0) $ bankID
data.show() $ data.printSchema() $
X_train_scaled.shape
df[df > 0]
celtics = celtics[pd.to_datetime(celtics.date).isin(bad_dates) == False] $ celtics = celtics[pd.to_datetime(celtics.date) >= dt.datetime(2013,1,1)] $ celtics = celtics[pd.to_datetime(celtics.date) <= dt.datetime(2015,2,11)] $ celtics.reset_index(drop=True, inplace=True)
tweet_df[tweet_df['lang'] == 'en'].groupby('username')['lang']
complaints2016 = all_complaints[ $     all_complaints['Created Date'].dt.year == 2016 $ ]
to_be_predicted_Day2 = 17.7492913 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
!rm world_bank.json.gz -f $ !wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz
INC.shape
plot = song_tracker[["rockstar","Bad and Boujee (feat. Lil Uzi Vert)"]].astype(float).plot(figsize=(20,10)) $ plot.set_xlabel("Date") $ plot.set_ylabel("Chart Position")
plt.close('all')
Florida.head()
pr('Extracting hashtags... (2 min)') $ tw['hashtag'] = tw.text.apply(lambda x: extract_hashtags(str(x))) # Getting list of hashtag into new column $ twh = tw.ix[tw.hashtag.apply(lambda x: len(x) != 0)] # droping the rows (tweets) that contains no hashtags. $ pr('We have extracted {} rows with hashtags out of the {} rows of our initial dataframe.'.format(strNb(len(twh)),strNb(len(tw))))
df2.duplicated().sum()
twitter_merged_data.describe()
r_ord = {i[0]:i[1:] for i in r_lol}
cm_knn = confusion_matrix(y_final, knn_predicted)
import pandas as pd
conn_str = ( $     r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};' $     r'DBQ=C:\users\rws\google drive\mydocuments lenovo backup\sr24imported.mdb;' $     )
MNB = MultinomialNB() $ model3 = MNB.fit(x_train, y_train)
questions = questions.drop(questions.index[[9,22]])
bad_tc_isbns = pd.read_table("C:/Users/kjthomps/Documents/GOBI holdings reports/IDs to exclude/TC and Law invalid ISBNs.txt") $ bad_tc_isbns = bad_tc_isbns['ISBN'] $ print(bad_tc_isbns.size)
import statsmodels.api as sm $ logit = sm.Logit(df['converted'],df[['intercept','treatment']]) $ res = logit.fit()
!head -5 data-new.csv
dta.violations.head()
plt.scatter(y_test,predictions)
import pandas as pd $ import numpy as np
cityID = '3f3f6803f117606d' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Lubbock.append(tweet) 
!wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/900/186/905/GCA_900186905.1_49923_G01/GCA_900186905.1_49923_G01_feature_table.txt.gz
df.shape $ df.to_csv('ny_times_url_dataframep3.csv', index=False,encoding = 'utf-8') $
autos["price"].describe()
rf=RandomForestClassifier(labelCol="label", featuresCol="features") $ labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel", labels=labelIndexer.labels) $ pipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,labelIndexer,OH1,OH2,OH3,OH4,assembler,rf,labelConverter]) $
records3.loc[(records3['Graduated'] == 'Yes') & (records3['Age'].isnull()), 'Age'] = grad_age_mean $ records3.loc[(records3['Graduated'] == 'No') & (records3['Age'].isnull()), 'Age'] = non_grad_age_mean
user = pd.read_csv('users.csv')
df_goog.set_index('Date', inplace=True) $ df_goog.head()
converted_new = df2.query("group == 'treatment' and converted == '1'").count()[0]/df2.query("group == 'treatment'").count()[0] $ converted_new
from sklearn.neighbors import KNeighborsClassifier $ k = 4 $ neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train_knn,y_train_knn) $ neigh
df=pd.DataFrame([['frank','M',29],['mary','F',23],['tom','M',35],['ted','M',33],['jean','F',21],['lisa','F',20]]) $ df
X = Florida['Description'] $ stop = set(stopwords.words('english'))
type(students['weight'])
url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars" $ browser.visit(url) $ html = browser.html $ soup = BeautifulSoup(html, "html.parser")
def train(df): $     ar_model = sm.tsa.AR(df, freq='D') $     ar_model_res = ar_model.fit(ic='bic') $     return ar_model_res
df= pd.read_csv('ab_data.csv') $ df.head()
cityID = 'd98e7ce217ade2c5' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Stockton.append(tweet) 
df_archive_clean = df_archive_clean.replace("None",np.NaN)
cityID = 'fa3435044b52ecc7' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Newark.append(tweet) 
from pandas_datareader import wb $ all_indicators = wb.get_indicators() $ all_indicators.ix[:,0:1]
df
files_txt = glob.glob("*.txt")
person = "lucianwintrich" $ df[df["quoted_status_screen_name"].str.lower() == "lucianwintrich"]
ratedCars = pd.merge(cars, $                      coolCars, $                      on=['Model']) $ ratedCars[5:10]
b1[b1.base_twitter_count > 100000].shape
(df2['landing_page'] == 'new_page').mean()
newdf.yearmonth = newdf.yearmonth.str.replace('(','').str.replace(')','').str.replace(',','').str.replace(' ','')
predictions = pd.DataFrame(y_pred.view(len(y_pred), -1).data.numpy(), columns=['tavg_norm']) $ predictions['series'] = 'predicted'
groceries
train_df.info()
pumpT.drop(['From_ft','To_ft','Material','Gage_in'],axis=1,inplace=True)
pred_df_dtmodel = pd.concat([X_test, pd.DataFrame(dtmodel_predictions, columns = ['dtmodel_predictions'])], axis = 1) $ pred_df_dtmodel.head(10)
cust_demo.shape
len(df2.user_id.unique())
df_moved.sort_values(by='Count', ascending=False).head()
top_supporters = support.groupby(["contributor_firstname", "contributor_lastname"] $ ).amount.sum().reset_index().sort_values("amount", ascending=False).head(10)
geo = users['location'].str.split(r',', 1, expand=True) $ geo.columns = ['cityOrState', 'country'] $ geo['cityOrState'] = geo['cityOrState'].str.strip() $ geo['country'] = geo['country'].str.strip() $ geo.head()
!hdfs dfs -cat /user/koza/hw3/3.2/issues/frequencies_part3/* | sort -k1,1nr -k3,3 | head -n 20
df.name.value_counts()
response.headers
df_hi_temps = pd.concat(hi_temps)
converted_group = df2.query('converted == 1') $ print(converted_group.user_id.nunique() / df2.user_id.nunique())
print('Actual price of the 1st house: {:.1f}'.format(test_data['price'][0]))
print parsed_review
df = pd.read_csv('data/test1.csv', $                  parse_dates=['date'], $                  index_col='date') $ df
US_cases=match_id(merkmale, merkmale.Merkmalcode.isin(['US'])) $ US_cases.Merkmalcode.unique()
zip_counts = bus.fillna("?????").groupby("postal_code").size().sort_values(ascending=False) $ zip_counts.head(15)
txt_exception_folder = '/home/ubuntu/s3/comb/txt_exception/' $ print(txt_exception_folder) $ flightv1_1 = spark.read.json(os.path.join(txt_exception_folder, "flight_15_13_price_2017-05-11*.txt")) $ flightv1_1.count() $
kickstarter = kickstarter[kickstarter["usd pledged"].notnull()] $ assert not kickstarter["usd pledged"].isnull().values.any() $ kickstarter["country"].unique()
cursor.execute(sq7) $ cursor.execute(sq71) $ results1 = cursor.fetchall() $ results1
total_treatment = (df2['group'] == 'treatment').sum() $ treatment_converted = len((df2[(df2['group'] == 'treatment') & (df2['converted'] == 1)])) $ print((treatment_converted / total_treatment))
df_summary = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq='M')) $ df_summary['tubes'] = temp_df.set_index('date').resample('M').sum().num_tubes $ df_summary.fillna(inplace=True, value=0) $ df_summary=df_summary.join(df_summary.groupby(df_summary.index.year).cumsum(), rsuffix='_cumul') $ df_summary.head()
%%bash $ pip install cx_freeze
sendAttack(coords=[541,513],units={'axe':25})
df_archive_clean = df_archive_clean.drop(["in_reply_to_status_id", "in_reply_to_user_id", $                                           "retweeted_status_id","retweeted_status_user_id"],axis="columns")
FREEVIEW.plot_fixation_durations(raw_freeview_df)
string.punctuation
print("No rows have missing values.")
data['b']
tweet_image_clean.info()
df2.groupby('group').describe()
autos['price'].describe()
to_be_predicted_Day1 = 22.41 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
GenresString=Genres.replace('|',',') $ GenresString=GenresString.replace(',',',\n')
df_total['US_new'] = df_total['US']*df_total['ab_page'] $ df_total['UK_new'] = df_total['UK']*df_total['ab_page'] $ df_total['CA_new'] = df_total['CA']*df_total['ab_page'] $ df_total.head()
pivoted_data = popular_centers.pivot(columns="center_name", values="attendance_count") $ pivoted_data.head()
dat = wb.download(indicator='NY.GDP.PCAP.KD', country=['US', 'CA', 'MX'], start=2014, end=2017) $ print(dat)
conf_matrix = confusion_matrix(y_tfidf_test, svc.predict(X_tfidf_test), labels=[1,2,3,4,5]) $ conf_matrix $
a = np.arange(0.5, 10.5, 0.5) $ a
filt_cols=y>20
plt.rc('figure', figsize=(16.0, 10.0)) $ plt.rcParams.update({'font.size': 22}) $ np.set_printoptions(precision=3, formatter={'float': '{: 8.3f}'.format})
args = mfclient.XmlStringWriter("args") $ args.add("where", "namespace=/projects/proj-hoffmann_data-1128.4.49/libraries") $ args.add("action", "get-path") $ args.add("size", "infinity") $ libraries_query = con.execute("asset.query", args.doc_text())
df.plot(x='observ_time',y='observ_prcp',kind = 'bar',title = 'Precipitation Over Time') $ plt.show()
dr_new_patient_8_to_16wk_arima = dr_new_patient_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted Number of Patients']] $ dr_new_patient_8_to_16wk_arima.index = dr_new_patient_8_to_16wk_arima.index.date
pd.value_counts(RNPA_existing['ReasonForVisitName'])
from sklearn.metrics import confusion_matrix $ mat = confusion_matrix(y_test,y_pred) $ print('CONFUSION MATRIX') $ print(mat) $ print('Recall: {0:.2f}'.format(mat[1][1] / (mat[1][0] + mat[1][1])))
r = requests.get( $     'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')
last_word("hey, there are words here")
Dataset contains retweets.  Needs to be deleted.
pred_probas_rfc_under = gs_rfc_under.predict_proba(X_test)
test.shape
from IPython.core.display import HTML $ HTML(filename=DATA_FOLDER+'/titanic.html')
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=56000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
click_condition_meta.drop(['geo_timezone', 'browser_cookies', 'geo_region', 'browser_colordepth', 'geo_city', $                            'geo_region_name', 'useragent', 'browser_viewdepth', 'browser_viewheight'], axis = 1, inplace = True)
run txt2pdf.py -o '2018-06-22  2011 872 discharges.pdf'  '2018-06-22  2015 872 discharges.txt'
bills_votes['congress'] = bills_votes['congress'].map(float)
final_grades["hobby"] = ["Biking", "Dancing", np.nan, "Dancing", "Biking"] $ final_grades
df_subset.info()
movies.to_csv('..\\Output\\CleanedMovies.csv')
df.dtypes.index
hr[hr["icustay_id"]==14882].plot(x="new charttime", $                                  y="value1num") $ hr[hr["icustay_id"]!=14882].plot(x="new charttime", $                                  y="value1num", color='red') #this is time series
aa2 = np.array(eval(aa2)) $ aa2
autos.brand.value_counts(normalize=True) $
xml_in_sample1 = xml_in[xml_in['days_diff_to_publication'] < 100]
with open(home_dir + '/general_warehouse_key.json', 'rb') as a: $     Config = json.load(a) $ Historical_Raw_Data = Connect_Sql_Warehouse( $     Config, Historical_Demand_SG, "Historical Demand")
sum(users_visits.visits)
index_group3 = user_group['Group'].apply(checkGroup, number=3) $ user_group3 = user_group[index_group3] $ print(len(user_group3)) $ user_group3.head()
rank = cc.groupby(['ranknow', 'slug'])['market'].mean() $ rank
x_dict = {'text':X.values, 'joined_text':new_values}
print(month.sum(axis=0).sum()) $ print() $ print(month.sum(axis=0))
df2[['control', 'treatment']] = pd.get_dummies(df2['group']) $ df2.head()
1/np.exp(-0.0149)
noNulls.orderBy(sort_a_desc, sort_b_desc).show(5)
r['pf_ret']  = r['weight']*r['daily_return']
type(df) $
ffrM_resample = ffr.resample("MS") $ type(ffrM_resample)
recipes.name[np.argmax(recipes.ingredients.str.len())]
print 6000/5000.
rng = pd.date_range(start='6/1/2017', end = '6/30/2017', freq='B') $ rng $
treat_converted = df2[df2['group'] == 'treatment'].converted.mean() $ treat_converted
n=1 $ new_page_converted = np.random.binomial(n, p_new, n_new) $ new_page_converted
df2.head()
from IPython.core.interactiveshell import InteractiveShell $ InteractiveShell.ast_node_interactivity = "all"
bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')
name = 'jared'
likes.groupby('reaction').size()
sum(abs(np.subtract(y_val, val_pred)))/len(y_val)
current_len = len(youtube_urls.items()) $ print(current_len)
borough_population = question_3_dataframe.\ $     drop_duplicates(['borough', 'incident_zip', 'population']).\ $     groupby('borough')['population'].\ $     agg('sum') $ borough_population
cust_demo.Martial_Status.value_counts().plot(kind='bar', color='R', alpha=0.5)
np.linspace(0, 10, 5) #5 equally spaced points between 0 and 10
from sklearn.metrics import f1_score $ from sklearn.neighbors import KNeighborsClassifier
filter_df = filter_df[filter_df['start_time'] <= datetime(2016, 11, 8, 23, 59, 59)] $ filter_df.head(2)
logistic = SklearnClassifier(LogisticRegression()) $ logistic.train(train_set)
df2_by_day['intercept']=1 $ lm=sm.OLS(df2_by_day['old_rate'], df2_by_day[['intercept', 'day']]) $ results=lm.fit() $ results.summary()
mean_price_by_brand = {} $ for b in sel_brand: $     mean_price = autos.loc[autos["brand"] == b, "price"].mean() $     mean_price_by_brand[b] = int(mean_price) $ mean_price_by_brand
sns.factorplot('sex', data=titanic3, hue='pclass', kind='count')
joined_df.groupby(['id', 'churned'])[['accepted']].mean().boxplot(by='churned')
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=6QBbyTowfjjBzvYS8nXF')
y_score_test = gbm.predict_proba(X_test)[:,1] $ metrics.roc_auc_score(y_test, y_score_test)   # 0.69
df4['new_id'] = df4['name'].apply(lambda x : topicmap[x]) $ df5 = df4.drop(['name'], axis=1) $ print(df5['new_id'].unique()) $ print(df5.head())
score_100 = score[score["score"] == 100] $ score_100.shape[0]
records =  dict(((k,name_freq_table[k]),12) for k in name_freq_table) $ records
problem_combos.cache()
mean_temp = temp_long_df.groupby(['date'])['temp_c'].mean() $ mean_temp.plot(x='date', y='temp_c')
move_1_herald = sale_lost(breakfastlunchdinner.iloc[1, 1], 10) $ print('Adjusted total for route: ' + str(move_34p34h34h - move_1_herald))
s3 = boto3.client('s3') 
treatment_mean = (df2.query('group == "treatment"')['converted']==1).mean() $ treatment_mean
dates_by_tweet_count['Tweet_counts'].corr(dates_by_tweet_count['Percent_change'])
encoder_model_inference.predict(encoder_input_data[0:1]).shape
NewTotalUser = df2['user_id'].count() $ NoOfConverted = df2.query('converted == 1')['user_id'].count() $ print("Probability of Individual Converting: ",(NoOfConverted/NewTotalUser))
auth = tweepy.OAuthHandler(consumer_key=con_key, consumer_secret=con_secret) $ auth.set_access_token(acc_token, acc_secret) $ api = tweepy.API(auth)
try: $     d = docx.Document(downloadIfNeeded(example_docx, example_docx_save)) $ except Exception as e: $     print(e)
path="drive/capstone/"
print("Accuracy score: ", accuracy_score(y_true=y_test, y_pred=y_knn_predicted))
techmeme['date_time'] = pd.to_datetime(techmeme.date) $ techmeme.date = techmeme.date_time.apply(lambda x: x.date()) $ techmeme.extra_sources = techmeme.extra_sources.apply(lambda x: ast.literal_eval(x))
for idx, row in df_trips.iterrows(): $     pilot_created = df_pilots.loc[row["pilot"], "created"] $     passenger_created = df_passengers.loc[row["passenger"], "created"] $     min_trip = max(pilot_created, passenger_created) $     df_trips.loc[idx, "trip_requested"] = np.random.randint(min_trip, max_trip)
df_new.groupby('country')['user_id'].count() #see the distribution of three countries
total_number_of_usage_per_feature = df_usage['feature_name'].value_counts() $ total_number_of_usage_per_feature.shape
my_gempro.set_representative_structure() $ my_gempro.df_representative_structures.head()
joined_samp.head(2)
aux = image_clean.merge(dog_rates.loc[dog_rates.race == 'golden_retriever'], $                                                            how='inner', on='tweet_id') $ aux[['jpg_url','grade']].sort_values(['grade'], ascending=False).iloc[:,0].values[:3]
weather_mean.iloc[4:8]
future_dates_df = pd.DataFrame(index=future_dates[1:],columns=dfs.columns)
user.ix["Trump", [1,2,4,5]]
data.head(100)
msft.dtypes
sales = pd.read_csv('sales.csv') $ print(sales,'\n') $ targets = pd.read_csv('targets.csv') $ print(targets)
index_missin_hr0to6_before2016 = taxi_hourly_df.loc[(taxi_hourly_df.missing_dt == True) & $                                                     (taxi_hourly_df.index.hour.isin((0, 1, 2, 3, 4, 5, 6))), : $                                                    ].index
run txt2pdf.py -o"2018-06-19 2015 CLEVELAND CLINIC HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2015 CLEVELAND CLINIC HOSPITAL Sorted by discharges.txt"
adopted_cats = adopted_cats[adopted_cats['DaysInShelter']>=0] $ hist_cats = adopted_cats[adopted_cats['DaysInShelter']<=100]
def cleanPhoneNumber(number): $
all_data[:][all_data["Restaurant"]==186]
bnbA.date_first_booking.head()
segmentData = pd.read_excel('2017 08 take home data.xlsx')
data[0]
df_main.info()
data.head(10)
csvname = queryfile[:-3] + 'csv' $ print('The CSV, by default will be "{}".\n'.format(csvname)) $ print('To change this, uncomment and change "example path" below')
df_2002 = pd.DataFrame(rows)
!hdfs dfs -mkdir {HDFS_DIR}/3.2 $ !hdfs dfs -mkdir {HDFS_DIR}/3.2/input $ !hdfs dfs -mkdir {HDFS_DIR}/3.2/output
inputstrings =  ['Python2.7', 'Cpython', 'Python3.4', 'Perbl5.0', 'Lua', 'Python3.6', 'Powershell'] $ outputstrings = ["Hello, " + item for item in inputstrings] $ outputstrings
import pandas as pd $ import matplotlib.pyplot as plt
store_items
conn.fileinfo('data', caslib='casuser')
print(google_stock.max()) $ print('\n', google_stock.mean()) $ print('\n', google_stock['Close'].min())
import tweepy as tp $ import pandas as pd $ import json
r_json = r.json()
pprint(q_multi.metadata(reload=True))
def assistant(x): $     if 'Assistant' in x: $         return 1 $     return 0 $ df_more['Assistant'] = df_more['Title'].apply(assistant)
df_tweet.head()
elms_all_0611.ORIG_DATE.max()
datatest['rooms'] = datatest['rooms'].apply(lambda x: 1.0 if x<1 else x)
df.columns
q4 = "select distinct CONCAT(cast(s.id_nda as VARCHAR),cast(TO_DATE(s.dt_deb) as VARCHAR)) as id_ndaj1, \ $ p.age, p.dt_deces from icu_pat_info p, icu_sensor_24 s \ $ where p.id_nda = cast(s.id_nda as VARCHAR) \ $ and s.dt_cancel = ''" $ df_pat = df_from_query(conn, q4)
df_more = pd.read_csv('/Users/aakashtandel/Desktop/Indeed_Project_3_df_cleaned.csv', index_col=0) $ df_more = df_more.reset_index(drop=True) $ df_more.head()
stock_ids
print(survey.describe(include='all')) $ categoricos = survey.select_dtypes(include=['object']).copy() $ categoricosx = categoricos.astype(str) $
logit_control = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ result_control = logit_control.fit()
pt_weekly = pt.resample('W').sum() $ pt_weekly.head(10)
autos.loc[autos["registration_year"]>2018, "registration_year" ].shape
import datetime $ from pytz import timezone
%lsmagic $
autos['offer_type'].value_counts()
df.zone.fillna('Unknown', inplace=True) $ df.county_name.fillna('Alameda', inplace=True)
print('San Francisco' in cities) $ print('Kolkata' in cities)
movies['scoreRank'] = movies.score.rank(axis=0, method='average', numeric_only=None, na_option='keep', ascending=True, pct=False) $ movies['grossRank'] = movies.gross.rank(axis=0, method='average', numeric_only=None, na_option='keep', ascending=True, pct=False)
df_filtered = df[~df['is_rank_1']]
selected=features[features.importance>0.03] $ selected.sort_values(by="importance", ascending=False)
def get_exchanges_list(): $
pd.date_range('2005', periods=4, freq='Q-NOV')
def text_process(text): $     nopunc = [char for char in text if char not in string.punctuation] $     nopunc = ''.join(nopunc) $     return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]
t = datetime.time(1,30,5,2) $ print(t)
group=class_merged.groupby(['date'],as_index=False) $ daily_sales=pd.DataFrame(group['sum_unit_sales'].agg('sum')) $ pd.DataFrame.head(daily_sales)
loadedModelArtifact = ml_repository_client.models.get(saved_model.uid)
df2.drop_duplicates(subset='user_id', keep='first', inplace=True)
trip_data_sub = trip_data_sub.drop("Ehail_fee", axis = 1)
_index = 'SPX' $ index_ts = pd.read_sql("select * from daily_price where ticker='SPX' and instrument_type='index'", engine) $
airports_df = pd.read_csv("airports.csv") $ airports_df.head()
git_project_data_df_raw = git_project_data_rdd.map(lambda row: Row(**row)).toDF().persist() $ git_project_data_df = non_blocking_df_save_or_load(git_project_data_df_raw, "{0}/raw_git_data".format(fs_prefix))
stock.iloc[-2]['next_day_open'] - stock.iloc[1640]['next_day_open']
plt.rcParams['figure.figsize'] = [15, 10]
users=collections.Counter(fullDf.user.values) $ print 'Top users',users.most_common(5) $ print '--------------------' $ print '%.1fk total users' % (len(users.keys())/1000.0) $ print '%.1fm tweets in total' % (sum(users.values())/1000000.0)
1928+1965
filename = processed_dir+'pulledTweetsCleanedLemmaEmEnc_df' $ gu.pickle_obj(filename,pulledTweets_df)
em_sz = 200 # size of each embedding vector $ nh = 500    # number of hidden activations per layer $ nl = 3      # number of layers $ bs = 64
rain = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >= "2016-08-23").\ $        group_by(Measurement.date).all() $ rain
filter_android(orig_tweets)[filter_android(orig_tweets)['text'].str.contains("\?")]
time2 = '2017-04-05 00:00:00' $ df2 = df1.set_index(['id_container', 'time_in']) # , 'id_container', 'time_in', $ op_before = df2[df2['time_move']<time2] $ op_after = df2[df2['time_move']>time2] $ at_snapshot = op_before.index.intersection(op_after.index) $
n_old=len(df2.query('group=="control"')) $ print("n_old : {}".format(n_old))
with open(link.split('/')[-1], mode = 'wb') as outfile: $     outfile.write(r.content)
df2.drop('Adj Close', axis=1,inplace=True)
features
from alpha_vantage.timeseries import TimeSeries $ ts = TimeSeries(key='824R4DDDH2LAKPEL')
data = pd.read_json('json/Tweets_ss_24.json',lines=True)
df1 = df1.reindex_like(df2) #this will reindex same like df2 $ df1
attr_bench = 'prob_HDWPSRRating' $ df_result_payout = compute_simple_payout(dfX_hist, attr_model=attr_bench, ascending=False, bet_amount=1.0) $ advantage_HDWPSRRating = compute_advantage(df_result_payout)
sq74= "CREATE TEMPORARY TABLE  time_33 ( SELECT * FROM NBA_tweets where TweetCreated >='2018-04-16 21:00:00' and   TweetCreated <'2018-04-16 23:59:59' )" $ sq75="SELECT word, COUNT(*) total FROM ( SELECT DISTINCT Num, SUBSTRING_INDEX(SUBSTRING_INDEX(TweetText,' ',i+1),' ',-1) word FROM time_33, ints) x where word like'%#%'GROUP BY word HAVING COUNT(*) > 1 ORDER BY total DESC, word;"
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs) $ null_vals = np.random.normal(0,np.std(p_diffs),10000)#Converting to be a distribution under tha null
predictions = grid.predict(X_test) $ print('MAE:', metrics.mean_absolute_error(y_test, predictions)) $ print('MSE:', metrics.mean_squared_error(y_test, predictions)) $ print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))
(null_vals > obs_diff).mean()
fwd = sess.get_data(['gbp curncy','hkd curncy','eur curncy'],'fwd curve') $ fwd
df.loc[['a','b','e'],'A']
df_ad_airings_5['metro_area'].unique()
df = df.loc[(df['ruling']!='Full Flop')&(df['ruling']!='Half Flip')]
total_users = len(df.user_id.unique()) $ users_converted=float(df.query('converted == 1')['user_id'].nunique()) $ p1 = (users_converted/total_users) $ print("The proportion of users converted is {0:.2%}".format(p1)) $
glm_multi_v1 $
X = df[['region', 'tenure','age', 'marital', 'address', 'income', 'ed', 'employ','retire', 'gender', 'reside']] .values  #.astype(float) $ X[0:5] $
flattened_df.head()
mod = sm.Logit(df3['converted'], df3[['intercept', 'US', 'CA']])
print('Best Score: {:.3f}'.format(XGBClassifier.best_score)) $ print('Best Iteration: {}'.format(XGBClassifier.best_iteration))
df_proj[df_proj.duplicated(subset='ProjectId', keep=False)]
tweet_df_clean.head(2)
mode = "overwrite" $ url = "jdbc:postgresql://localhost:5432/postgres" $ properties = {"user": "benjarman","password": "","driver": "org.postgresql.Driver"} $ for dat in data_sets: $     dat[1].write.jdbc(url=url, table=dat[0], mode=mode, properties=properties)
my_gempro.genes.get_by_id('Rv1295').protein.representative_structure $ my_gempro.genes.get_by_id('Rv1295').protein.representative_structure.get_dict()
df3 = df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'], $                     'salary': [70000, 80000, 120000, 90000]}) $ df3
tweet_archive_enhanced_clean.loc[1627,'text']
yc_new3.describe()
account = authenticate_into_dropbox()
result_co.summary()
props.info()
counts.index
tf.reset_default_graph()
print(well_data.head())
fields_a = ['aid', 'tid','tname', 'title','pubdate','ctime', 'desc', 'state', 'duratioin','owner', 'stat'] $ fields_stat = ['aid', 'view', 'danmaku', 'reply', 'favorite', 'coin', 'share', 'now_rank', 'his_rank', 'like']
print ("#6b PCA and view scatter plot on 2D") $ PCAPlot(dfL, predictColumn)
%load "solutions/sol_2_41.py"
powerConsumption = create_dataframe()
df1.shape, df2.shape, df3.shape
image_path2 = "daffodils-2162825_1280.jpg" $ img2 = pil_image.open(image_path2) $ img2 = img2.convert('RGB')
df_staff = df_select.copy() $ df_staff = df_staff.groupby(['staff_pick'], as_index=False).mean() $ df_staff
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs); $ plt.axvline(diff, color = 'red');
id_of_tweet = '892420643555336193' $ tweet = api.get_status(id_of_tweet, tweet_mode='extended') $ print(tweet._json)
stock_change.plot(grid = True).axhline(y = 0, color = "black", lw = 2) $
autos['year_of_registration'].value_counts(normalize = True).sort_index()
trainx.shape
new_converted_simulation = np.random.binomial(n_new, new_page_converted.mean(), 10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, old_page_converted.mean(), 10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation
m = Prophet() $ m.fit(df1);
permits_df = permits_df[['LAT', 'LON', 'IssuedDate']] $ permits_df['YEAR'] = permits_df['IssuedDate'].apply(get_year) $ permits_df['LAT_LONG_COORDS'] = pd.Series(list(zip(permits_df['LON'], permits_df['LAT']))) $ permits_df.head()
num_features  = ['teacher_number_of_previously_posted_projects', $                  'total_quantity', 'mean_cost', 'total_cost','unique_items']
df[0:4]
newdf.to_csv('bitcoin_semscore.csv')
autos = autos[autos['odometer_km'].between(70000, 150000)] $ autos['odometer_km'].hist()
dfY.head()
dir(tweet_list[0])
startups_USA.head()
df_wm['sentiment'] = df_wm['cleaned_text'].apply(sentiment_calc)
teams_sorted_by_wins = df.sort_values(by=['wins']) $ teams_sorted_by_wins.head(10)
df_TempJams = pd.DataFrame(Jams_data,columns=['timeStamp','pubTimeStamp','speed','level','lineString'])
with open(csvFname, 'r') as ifile: $     print(ifile.readline(), '\n', ifile.readline() )  #Print a couple of lines $     ifile.seek(0);  # and reset the pointer. $     dbData = pd.read_csv(ifile)  # Read the file into a Panda dataframe $     ifile.close()
tweets_df.tail(3)
ab_data.shape[0]
bacteria2 = pd.Series(bacteria_dict, $                       index=['Cyanobacteria','Firmicutes','Proteobacteria','Actinobacteria']) $ bacteria2
twitter_archive_master['Rating_number'] = twitter_archive_master['rating_numerator']/twitter_archive_master['rating_denominator']
print('Preview dataset info') $ print('Unique questions:', df['question_id'].nunique()) $ print('Unique users:', df['user_id'].nunique()) $ print('Total predictions:', df.shape[0])
to_be_predicted_Day2 = 21.30837267 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
probability = df2["converted"].mean() $ print("The probability is {}.".format(probability))
weather_data['zip'].unique()
active_users[active_users['LastAccessDate'] > pd.to_datetime('01-01-2017')].shape # more than half of active users were active in the last year
vow.plot()
warnings.filterwarnings('ignore') $
raw_large_grid_df.iloc[1:2]
my_data.count()
pd.date_range('2014-08-01 12:10:01',freq='S',periods=10)
pd.read_pickle('data/city-util/proc/city.pkl', compression='bz2').head()
float(df.converted.sum())/df_length
feedex = pd.read_sql_query('select * from "events_feedexes"',con=engine)
cleaner_tweets = [] $ for tweet in tweets: $     cleaner_tweets.append({'id': tweet.id, 'text': tweet.text, 'created_at': tweet.created_at, 'profile': tweet.user.screen_name}) $
rsp.json()['code']==0
rfc = RandomForestClassifier() $ scores = cross_val_score(rfc,  X_test, y_test,  cv=5) $ np.mean(scores), np.std(scores)    # scoring on my Testing Data under performs also
records3.loc[(records3['Graduated'] == 'Yes') & (records3['Days_missed'].isnull()), 'Days_missed'] = grad_days_mean $ records3.loc[(records3['Graduated'] == 'No') & (records3['Days_missed'].isnull()), 'Days_missed'] = non_grad_days_mean
train, validation = train_test_split(df_train, test_size=0.3)
test_matrix = get_doc_matrix(model, test_clean_token) $ train_matrix = get_doc_matrix(model, train_clean_token) $
nan_sets
nitrodata['MonitoringLocationIdentifier'].nunique()
parts_sets.shape, parts.shape
n_new = (df2.landing_page == 'new_page').sum() $ n_new
max_ch_ol2 = max(abs(u.close-v.close) for u,v in zip(list(o_data.values()),list(o_data.values())[1:])) $ print('Another one liner using islice: {:.2f}'.format(max_ch_ol2))
Meter1.MakeMeasurment()
input =  df_MC_most_Convs.MC_mostConvs.tolist() $ c = Counter(input) $ df_MC_most_Convs = pd.DataFrame(c.items()) $ df_MC_most_Convs.rename(columns={1:'number_months'}, inplace=True) $ df_MC_most_Convs
np.log(0.025/0.28)
import base64
autos['price'].describe()
knn_10.score(X_train, y_train)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print("The z-score is: %.4f\nThe p-value is: %.4f" %(z_score, p_value))
ls_data_v2[ls_columns].to_excel(cwd+'\\LS_mail_0604_from_python_v2.xlsx', index=False)
top10_df_pd=top10_df.toPandas() $ top10_df_pd.head(10)
X= df_loantoVal.drop(['First_Payment_Date' ,'Maturity_Date','LoanToValue'], axis=1) $ y = df_loantoVal['LoanToValue']
df.groupby(['group', 'landing_page']).agg({'user_id':'count'})
ip_clean = ip.copy()
sunspots.iloc[10:20, :]
one_hot_domains_questionable = df_questionable.groupby('user.id')[media_classes].sum().fillna(0) $ one_hot_domains_questionable = one_hot_domains_questionable.apply(normalize, axis=1).fillna(0)
train.index
df_Providers = df_Providers.sort_values(['name','year'], ascending=[True,True]) $ todays_date = time.strftime("%b %d %Y %H%M") $ print('Date: ', todays_date) $ print('Dimensions of df_Providers:', df_Providers.shape) $ print(df_Providers.head()) $
print(d['entries'][0]['title']) $ print(d.entries[0]['link'])
print('Do any of the rows have missing values?  Answer:  ' + str(df.isnull().values.any()))
for row in df.itertuples(): $     print(row)
segments.info()
import statsmodels.api as sm
stars = pd.read_csv('data/new_subset_data/new_subset_stars.csv', sep='\t') $ stars.info() $ stars.head()
prob = x_/y_ $ prob $
print lesson_date + timedelta(hours=1) $ lesson_date - timedelta(days=3) $ print lesson_date + timedelta(days=-3) $ print lesson_date + timedelta(days=368, seconds=2)
weather
df3['intercept']=1
weekend = np.where(data.index.weekday < 5, 'Weekday', 'Weekend') $ by_time = data.groupby([weekend, data.index.time]).mean()
df['D']
random_crashes_df.shape
data_final['authorId'].nunique()
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, $                                                     y, $                                                     test_size=0.5)
graf_train['DETAILS3']=graf_train['DETAILS'].apply(review_to_wordlist) $ graf_test['DETAILS3']=graf_test['DETAILS'].apply(review_to_wordlist) $
print('renters only use style lend one time: {}'.format(sum(total_cost['number']==1))) $ total_cost = total_cost[total_cost['number']<40] $ total_cost['number'].plot(kind='hist', logy=True, $                          title='Histogram on renting counts',yticks=[]) $ plt.xlabel('Counts')
data_l1 = tmpdf.index[tmpdf[tmpdf.isin(DATA_L1_HDR_KEYS)].notnull().any(axis=1)].tolist() $ data_l1
print("The minimum value of artistID:") $ userArtistDF.agg(min("artistID")).show()
ss = StandardScaler() $ Xs = ss.fit_transform(X) 
df=pd.read_table("../../data/msft.csv",sep=',') $ df.head()
pd.set_option('display.max_columns', 100)
df = pd.read_sql('SELECT UPPER(first_name || \' \' || last_name) as "Actor Name" FROM actor', con=conn) $ df.head()
P_n=(len(treatment_df2)) / (df2.shape[0]) $ print("Probability that an individual received the new page is {}".format(P_n))
print(giss_temp.shape) $ print(giss_temp.dtypes)
kimanalysis.listids('model-driver', extended=True)
by_weekday = data.groupby(data.index.dayofweek).mean() $ by_weekday.index = ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun'] $ by_weekday.plot(style=[':', '--', '-']);
scoring_data = {'values': [image_1.tolist(), image_2.tolist()]}
to_be_predicted_Day4 = 21.28690176 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
txt = df.join(txt)
df_fireworks = df[(df['Complaint Type'] == 'Illegal Fireworks')] $ df_fireworks.groupby(df_fireworks.index.month).apply(lambda x: len(x)).plot(kind='bar')
orders_subset = orders[orders.user_id.isin(selected_users)]
data = item.data $ data
month = dd.read_csv('training/yellow_tripdata_2015-01.csv') $ print(month.columns)
import TwitterGEXF as TG #custom gexf saver $ today = date.today() $ filename = 'twitter_graph_data/%s_tweet_bigraphFULL.gexf' % date.today() $
lgb_cv = GridSearchCV(lgb_mdl, param_grid,verbose=1, cv=5, n_jobs=-1)# Fit it to the data
basicmodel = LogisticRegression() $ basicmodel = basicmodel.fit(basictrain, train["rise_nextday"])
Returning_df = Type_date.groupby(['New_or_Returning']).get_group('Returning ').reset_index() $ Returning_df.head()
ser = pd.DataFrame({'By': dates, 'key':[0] * len(dates)}) $ ser
proj_df = pd.read_csv('io/Projects.csv', engine='python')  # dataset available on kaggle/donorschoose
np.sum(jArr, 0)
pd.set_option('display.max_colwidth', 100) $ clinton_df.head()
for c in ccc: $     vwg[c] = vwg[vwg.columns[vwg.columns.str.contains(c)==True]].sum(axis=1)
sum_df = full_clean_df.groupby('type').sum()['retweet_count'] $ count_df = full_clean_df.groupby('type').count()['tweet_id'] $ retweet_per_doggy = sum_df/count_df
df2.groupby('group').mean()
tweets = pd.read_csv('tweets_mentioning_candidates.csv') $ tweets['set'] = 'test' $ tweets['polarity_value'] = np.NaN
df_final_.category.nunique()
from pyspark.sql.functions import collect_list $ dict_visited_by_user = ...
lq.columns.values
with open('dropbox/github/Thinkful/unit1/data/lecz-urban-rural-population-land-area-estimates_continent-90m.csv', 'rU') as inputFile: $     for line in inputFile: $         print(line)
import test_package.print_hello_class_container                          # Import module that the class is in $ classified = test_package.print_hello_class_container.Print_hello_class  # Extract the class from the module and assign it to a variable $ print_hello_instance = classified()                                      # Instantiate the class $ print(print_hello_instance.name)                                         # Print a property of the instance $ print_hello_instance.print_hello_method_within_class()                   # Call a method of the instance
tables[4].head()
df.count()
n_new = df2[df2.landing_page == 'new_page'].user_id.count() $ print(n_new)
ben_final.revtime = ben_final.revtime.str[:19] $
%%R $ head(flightsDB)
rename_list = ['aa','bb','cc','dd','ee','ff','gg','hh','ii','jj','kk','ll','mm','nn','oo','pp','qq','rr','ss']
import matplotlib.pyplot as plt $ df.drop('Volume', axis=1).plot(figsize=(10,7)) $
Numerator=df2.loc[(df2['group']=='control') & (df2['converted']==1),].shape[0] $ Denominator=df2.loc[df2['group']=='control',].shape[0] $ ControlConverted= Numerator/Denominator $ print("The probability is", Numerator/Denominator)
print('\noriginal string:\n', train_body_raw[0], '\n') $ print('after pre-processing:\n', train_body_vecs[0], '\n')
print(df.info()) $ pd.isna(df).sum() $
nltk.download('stopwords')
textToTable(text).head()
legos = {} $ for name in files: $     filename = 'legos/{}.csv'.format(name) $     file = pd.read_csv(filename) $     legos[name] = file
print('Events dataframe:') $ new_events.head(10)
columns = inspector.get_columns('station') $ for c in columns: $     print(c['name'], c["type"])
sp = openmc.StatePoint('statepoint.50.h5')
import joblib
df = pd.read_csv('../data/hash_rate_raw.csv', names=['Date', 'Hashrate'])
s.str.endswith('t')
with open('nmf_doc_top_5_29.pkl', 'rb') as pikkle3: $     doc_top = pickle.load(pikkle3)
from collections import Counter
prcp = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date > 2017).order_by(Measurement.date.desc()) $ prcp[:5] $
plot_contingency(train_users, "gender", "country_destination")
print('Average Daily trading volume during 2017 is :',np.mean(dtunit))
df2.info()
df_actor[df_actor.last_name.str.contains('GEN')]
tmax_day_2018.values
df.time.unique().shape
autos = pd.read_csv("autos.csv", encoding = "Latin-1") # Default encoding UTF-8 threw an error $ autos.info()  # printing information about the'autos' dataframe $ autos.head()  # printing first few rows of "autos"
import statsmodels.api as sm $ convert_old = df2.query('group=="control" and converted == 1')['converted'].count() $ convert_new = df2.query('group=="treatment" and converted == 1')['converted'].count() $ n_old = df2.query('group=="control"')['group'].count() $ n_new = df2.query('group=="treatment"')['group'].count()
n_old = df2[df2['landing_page']=='old_page'].count()[0] $ n_old
shift_entries['TimeElapsed'] = (shift_entries.SE_TIMESTAMP - shift_entries.SE_START).apply( $     lambda x: x.total_seconds() / 3600.)
df_wna.sort_values(by='n_faults').head(24)
cr_control = df2[df2['group'] == 'control']['converted'].mean() $ cr_control $
unique_speaker_id = speeches_cleaned['speaker_bioguide'].unique() $ print(len(unique_speaker_id))
len(b_cal_q1.groupby('listing_id').mean().index)
len(scr_churned_ix) + len(scr_active_ix) == len(SCR_PLANS_df)
autos[ ["date_crawled", "ad_created", "last_seen"] ].head()
print(speeches_cleaned.shape) $ print(len(speeches_cleaned['text'].unique())) $ unique_speeches = speeches_cleaned['text'].unique()
train_data, validation_data, test_data = np.split(df.sample(frac=1, random_state=1729), [int(0.7 * len(df)), int(0.9 * len(df))]) 
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old]) $ z_score,p_value
all_tables_df.query('OWNER == "ALMA"').OBJECT_NAME.nunique()
model_df = stock.iloc[915:].copy()
print(str(autos["price_euro"].unique().shape[0]) + " unique values") $ print(autos["price_euro"].describe())
e.instance_method
plt.rcParams['figure.figsize'] = (15, 5) $ tzs.plot(kind='bar') $ plt.xlabel('Timezones') $ plt.ylabel('Tweet Count') $ plt.title('Top 10 Timezones tweeting #puravida')
drop_cols = ['project_is_approved','id','teacher_id'] $ X = train.drop(drop_cols, axis=1) $ y = train['project_is_approved'] $ feature_names = list(X.columns)
np.exp(-.0099), np.exp(-.0507)
INT.loc[:,'M'] = INT['Create_Date'].apply(lambda x: "%d" % (x.month))
np.random.seed(123456) $ dates = pd.date_range('8/1/2014', periods=10) $ s1 = pd.Series(np.random.randn(10), dates) $ s1[:10]
%%time $ ent_count = defaultdict(int) # reset defaultdict $ for doc in nlp.pipe(texts): # ['parser','tagger','ner'] $     matcher(doc) # match on your text $ print(ent_count)
unique = len(df) - sum(df["user_id"].duplicated()) $ print(unique)
json.dumps(letters)[:1000]
df_new['interaction_ab_CA'] = df_new['ab_page']*df_new['CA'] $ df_new['interaction_ab_UK'] = df_new['ab_page']*df_new['UK']
airline_df = pd.read_csv('data/unitedAIRLINES.csv') $ airline_df
%%writefile butler/trainer/model.py $
pprint.pprint(posts.find_one({"reinsurer": "AIG"}))
model_artifact = MLRepositoryArtifact(model_rf, training_data=train_data, name="Product Line Prediction") $ model_artifact.meta.add("authorName", "Team XX")
autos["date_crawled"].str[:10].value_counts(normalize=True, dropna=False).sort_values()        
news_df.head(5)
pd?
p_old = df2['converted'].mean() $ print("{} is the convert rate for Pold under the null.".format(p_old))
htmldmh.args.addtable
df['created_at'] = df['created_at'].map(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))
df_state_victory_margins.plot(x='state', y='Percent margin', kind='bar')
obamaSpeechSoup = bs4.BeautifulSoup(obamaSpeechRequest.text, 'html.parser') $ print(obamaSpeechSoup.text[:200])
urls_to_shorten = [link for link in urls if ux.is_short(link)] $ urls_to_shorten
df_rows = df.shape[0] $ print("There are {} rows in the dataset.".format(df_rows))
for col in temp_columns: $     print(col) $     dat.loc[:,col]=dat[col].interpolate(method='linear', limit=3)
device = train_data.groupby(["device.browser","device.operatingSystem","device.isMobile"]).agg({'totals.transactionRevenue': 'sum'}) $ device.sort_values(by = ["device.browser","totals.transactionRevenue"],ascending=False)
pd.DataFrame({'trump':s_trump_ratio, 'hillary': s_hillary_ratio}).plot()
rdf = arcgis.SpatialDataFrame().from_featureclass(road_features_path)
contract_histo = import_contract_history(data_repo + 'contract_history.csv')
S_distributedTopmodel.decision_obj.hc_profile.options, S_distributedTopmodel.decision_obj.hc_profile.value
index_vector = df.source == 'GRCh38' $ gdf = df[index_vector] $ gdf.shape
d[0]  # this is the first dictionary inside the data
obs_diff = (df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean()) $ (p_diffs > obs_diff).mean()
plt.figure(figsize=(12,8)) $ sns.violinplot(x=trump.source, y= 'num_char', data= trump) $ plt.xlabel(' Source of Tweet', fontsize=12) $ plt.ylabel('Number of Characters', fontsize=12) $ plt.title('Number of Characters per Tweet by Source', fontsize=15)
df2 = df.drop(df[(df['group'] == 'treatment') == (df['landing_page'] != 'new_page')].index, axis = 0)
tweet_json.describe()
tweets_streamedDF = make_df(tweets_stream_data)
deaths['Totals'] = deaths['Totals'].astype(int) $ deaths['Date'] = deaths['Date'].apply(lambda d: datetime.strptime(d, '%Y-%m-%d')) $ grouped_months = deaths.groupby(deaths['Date'].apply(lambda x: x.month)) $
data[data.author == 'Scaryclouds'].comment_body.head()
data.describe()
np.sqrt(fruits)
df_pivot2['id_ndaj1'] = df_pivot2.index $ df_pivot2.head()
df.loc[:,"Date"] = 
shots_df.to_pickle('shots_df.pkl')
df_master[df_master.rating_numerator==1776]
(df['converted'].sum() / len(df))*100
import requests $
merged_NNN = pd.merge(committees_NNN, contributions, on="calaccess_committee_id")
twitter_archive_master.likes.value_counts()
len(tweets_list)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json')
pd.DataFrame ([{'a': 1}, {'a': 5, 'b': 10}], index = ['Y', 'Z'])
p_diffs=np.array(p_diffs) $ plt.hist(p_diffs); $ plt.title('simulation from 10000 samples') $ plt.xlabel('proportion difference (p_new - p_old)') $ plt.ylabel('proportion difference frequency')
from sklearn.model_selection import train_test_split #need to conduct a Train/Test split before creating features $ X_train, X_test, y_train, y_test = train_test_split(pd.get_dummies(reddit['Subreddits']), reddit['Above_Below_Median'], test_size=0.3, random_state=42)
train['answer_dt']=pd.to_datetime(train['answer_utc'], unit='s') $ test['answer_dt']=pd.to_datetime(test['answer_utc'], unit='s')
first_ten_thousand = df['Created Date'].head(10000) $ pd.to_datetime(first_ten_thousand)
convers_con['time_difference'] = convers_con.booked_at - convers_con.added
df.duration.describe()
!python3 -m spacy download en_core_web_md
austin[['distance_travelled', 'miles', 'driver_rating', 'rider_rating', 'total_fare', 'base_fare', 'tip']].describe()
print(confusion_matrix(y_test, rf_yhat))
titanic.isnull().head()
crimes.drop('LOCATION', axis=1, inplace=True)
df_onc_no_metac = df_onc.drop(columns=ls_metac_colnames) $ df_onc_no_metac = df_onc_no_metac.rename(columns = {'METAC_SITE_NM1': 'METAC_SITE'})
import numpy as np $ ok.grade('q03')
pkl_file = open('speeches_metadata_evidence.pkl', 'rb') $ speeches_metadata_evidence = pickle.load(pkl_file) $ speeches_all_bill_data = speeches_metadata_evidence.merge(bills_votes, left_on = ['bill_id', 'congress'], right_on = ['bill_slug', 'congress'], how = 'inner')
c = d6tstack.convert_xls.XLStoCSVMultiSheet('test-data/excel_adv_data/sample-xls-case-multisheet.xlsx', $                                             output_dir = 'test-data/output', logger=PrintLogger()) $ c.convert_all()
control = df2[df2['group'] == 'control'] $ size_control = control.shape[0] $ prop_conv_control = control[control['converted'] == 1].shape[0] / control.shape[0] $ prop_conv_control
token_sendavg["ID"] = token_sendavg.sender $ token_receiveavg["ID"] = token_receiveavg.receiver
archive_df_clean.info()
print(X_test.shape) $ print(X_sl_holdout.shape) $ print(Y_sl_holdout.shape)
def datetimeampm2datetime(text): $     try: $         return  pd.to_datetime(text,format='%d-%m-%Y %I:%M:%S %p') $     except AttributeError: $         return text 
total_daily_sales = data[['Sales']].resample('D').sum() $ total_daily_sales.expanding().sum()['2014-12'].head() $
wb.search('cell.*%').iloc[:,:3]
df2.dtypes
autos = autos[autos["registration_year"].between(1900,2016)] $ autos['registration_year'].value_counts(normalize=True)
ins.index.value_counts()
df2 = df2.drop_duplicates(subset=['user_id']) $ df2.info()
df_pol_matrix = tvec.transform(df_pol_t['text'])
df.head(3)
utility_patents_df = all_patents_df[all_patents_df.application_type == 'utility'] $ utility_patents_df.info()
details = client.repository.get_experiment_details(experiment_uid) $ print(details)
(df.location_id.nunique(), $  df.raw_location_text.nunique(), $  df.raw_location_text.str.lower().nunique())
calls.head().T
Imputation_columns = ['Paid at','Fulfillment Status','Fulfilled at','Accepts Marketing','Currency','Subtotal','Shipping','Taxes','Total','Discount Code','Discount Amount','Shipping Method']
final_topbikes.sort_values(by='Distance', ascending=False)
data1_new['timestamp']=data2['timestamp']
print(best_values)
mb = pd.read_table("Data/microbiome.csv", sep=',')
%%time $ df['closed_at'] = pd.to_datetime(df['Closed Date'], format='%m/%d/%Y %X %p') $
df.drop(df[df.amount == 0].index, axis=0, inplace=True)
contribs.head(10)
jail_census.drop("_id", axis=1, inplace=True) $ jail_census
results_distributedTopmodel, output_DT = S_distributedTopmodel.execute(run_suffix="distributedTopmodel_hs", run_option = 'local')
df.select('c', c_secondDigit3).show(5)
lr = 1e-3 $ learn.fit(lr, 20, cycle_len=1, use_clr=(10,10))
dfSummary = pd.concat([sumAll,sumPre,sumPost],axis=1) $ dfSummary.columns = ("all","before","after")
df2[df2.duplicated('user_id') == True]
token_sendReceiveAvg_month = pd.merge(token_sendReceiveAvg_month,empInfo[["ID","level"]],how ="left",on="ID")
twitter_data.rating_numerator.value_counts()
X = endometrium_data.drop(['ID_REF', 'Tissue'], axis=1).values $ y = pd.get_dummies(endometrium_data['Tissue']).values[:,1]
df[(abs(df['Open']-df['High'])<0.2 ) | (abs(df['Close']-df['Low'])<0.2)]
consumer_key = consumer_key $ consumer_secret = consumer_secret $ access_token = access_token $ access_token_secret = access_token_secret
cur.execute('SELECT material_type, COUNT(*), AVG(alpha), MAX(beta) FROM materials GROUP BY material_type') $ cur.fetchmany(2)  # use fetchmany() with size parameter, just for fun
!wget https://pjreddie.com/media/files/yolov3.weights $
dat_hcad = pd.read_table(dir_hcad+"real_acct.txt", sep='\t', encoding = "ISO-8859-1")
tmp.value_counts()
df2.user_id[df2.user_id.duplicated()]
df.columns = [dict_names[x] for x in df.columns]
raw_large_grid_df.query("subject=='VP4'&eyetracker=='pl'").duration-raw_large_grid_df.query("subject=='VP4'&eyetracker=='el'").duration
df_test[['id','visitors']].to_csv('h2o_automl_pubmine.csv',index=False)
df.describe()
test = Plot['mLayerLiqFluxSoil'].data $ dates = Plot['time'].data $ test = np.squeeze(test) $ df = pd.DataFrame(data = test, index=dates) $ df.replace(to_replace=-9999.0, value = 0, inplace=True)
df_questionable[df_questionable['bias'] == 1]['link.domain_resolved'].value_counts(25).head(25)
df_master[df_master['retweet_count']==61590]
cityID = 'f995a9bd45d4a867' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Memphis.append(tweet) 
update_1_query = ( $     " UPDATE signal_signal SET trend = -trend WHERE `signal` = 'ANN_simple' " $ ) $
df['Temperature'].unique()
print 'Can pass partial datetime values and get sensible output:' $ msft.loc['2012-02':'2012-03'] # Feb and March 
print(result.summary2())
countries = pd.get_dummies(df2['country']) $ df2 = df2.join(countries)
van_dummy = van_final.loc[:,['diffs','userid','pagetitle']]
df_person.merge(df_grades, how="inner", left_index=True, right_index=True)
print(df_pat.shape) $ df_pat.head()
lr = LogisticRegression(random_state=20, max_iter=10000) $ param_grid = { 'C': [1, 0.5, 5, 10,100], 'multi_class' : ['ovr','multinomial'], 'solver':['saga','newton-cg', 'lbfgs']} $ grid = GridSearchCV(lr, param_grid=param_grid, cv=10, n_jobs=-1)
search['trip_start_date'] = search['message'].apply(trip_start_date)
results.summary2()
print("The standard deviation is: $", donations['Donation Amount'].std()) $ print("The variance is: $", donations['Donation Amount'].var()) $ print("The 1st, 2nd, 3rd quantiles are: \n", donations['Donation Amount'].quantile([0.25, 0.50, 0.75]))
get_adjacency_matrix(57, as_np_array= True, verbose = False)
tweets.head()
prediction.sample(3)
plt.hist([guest['ama.week.diff'] for guest in ama_guests.values()]) $ plt.title("How many weeks previously was this AMA posted") $ plt.show()
import pandas as pd $ print pd
table_1c = table_1c.where(table_1c[u'Collected from port'] < store_time)
rf = ensemble.RandomForestClassifier(n_estimators=300) $ rf.fit(cX, cy) $ cm = confusion_matrix(cy, rf.predict(cX)) $ sns.heatmap(cm, annot=True)
df_countries = pd.read_csv('countries.csv') $ df_full = pd.merge(df_countries, df2, how='outer') $ df_full.head()
mean_new=df2[df2['converted']==1].count()[0]/df2.shape[0] $ mean_new
temp_df2.sort_values(by='timestamp', ascending=False).head()
print(data.iloc[[5396]])
y_pred = [randint(0, 2) for i in range(1000)] $ y_true = [randint(0, 2) for i in range(1000)] $ di = {0: 'actif', 1: 'churn', 2: 'lost'} $ y_pred = [di.get(n,n) for n in y_pred] $ y_true = [di.get(n,n) for n in y_true]
mp2013 = pd.period_range('1/1/2013', '12/31/2013', freq='M') $ mp2013
autos['vehicleType'].unique()
import numpy as np
trump['hour'] = trump['est_time'].dt.hour + (trump['est_time'].dt.minute)/60 + (trump['est_time'].dt.second)/(60**2) $ sns.distplot(trump['hour']) $ plt.show() $
countries_grouped = df4.groupby(["country"]) $ countries_grouped.count()
tweet_info.head()
sns.countplot(y="objecttype",data=firstWeekUserMerged) $ plt.show()
g8_aggregates.columns = ['_'.join(col) for col in g8_aggregates.columns] $ g8_aggregates
new_page_converted = np.random.choice([0,1], size = n_new, p = [1-p_new, p_new]) $ new_page_converted.mean() $ print(new_page_converted)
archive_clean.head(5)
age = pd.cut(titanic['age'], [0, 18, 80]) $ titanic.pivot_table('survived', ['sex', age], 'class')
autos['price'].describe()
df2['datetime']=pd.to_datetime(df2['timestamp'], errors='coerce')
tags = db.get_tags() $ tags.head()
session['action_detail'].value_counts().sort_values(ascending=False).head(20)
df2['intercept'] = 1 $ log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ result = log_mod.fit() $ result.summary()
stations = session.query(Measurement).group_by(Measurement.station).count() $ stations
html = browser.html $ mars_hemispheres = bs(html, 'html.parser')
detroit_census2=detroit_census.drop("Fact Note", axis=1) $ detroit_census2=detroit_census2.drop("Value Note for Detroit city, Michigan", axis=1)
merged['US_page'] = merged['US']* merged['ab_page'] $ merged['UK_page'] = merged['UK']* merged['ab_page'] $ merged['CA_page'] = merged['CA']* merged['ab_page']
print confMatrixLog $ print confMatrixRand
na_df.isna() # check elements that are missing 
os.chdir('code')
df[df['gathering'].isnull()]
journalist_retweet_gender_summary(journalists_retweet_df[journalists_retweet_df.gender == 'F']) $
pd.merge(left=city_loc, right=city_pop, on="city")
from sklearn.preprocessing import StandardScaler $ X_std = StandardScaler().fit_transform(X) $ pca = PCA(n_components=2) $ x_9d = pca.fit_transform(X_std) $
import pandas as pd $ import numpy as np $ from sklearn.neighbors import KNeighborsRegressor
print('Unique number of users notified: {}'.format(len(atdist_4x['vendorId'].unique())))
density = 1.32
int[1] = 0
vect = CountVectorizer() $ vect.fit(X_train)
import statsmodels.api as sm $ convert_old =  len(df2[(df2['landing_page'] == 'old_page')& (df2['converted'] == 1)]) $ convert_new = len(df2[(df2['landing_page'] == 'new_page')& (df2['converted'] == 1)]) $ n_old = n_old = df2[df2['landing_page']=='old_page'].user_id.count() $ n_new = n_new = df2[df2['landing_page']=='new_page'].user_id.count()
graf_train=graf_train.copy() $ graf_test=graf_test.copy()
train_holiday_oil_store_transaction_item_test_004 = train_holiday_oil_store_transaction_item_test_004.join(item_onpromotion, 'item_nbr', 'left_outer') $
sub = pd.DataFrame(np.column_stack((y_id, y_test_log_pred)), columns=['listing_id'] + le.classes_.tolist())
import test_package.print_hello_function_container $ import test_package.print_hello_class_container $ import test_package.print_hello_direct # note that  the paths should include root (i.e., package name) $
all_tweets.iloc[0:10,:]
pd.date_range('3/7/2012 12:56:31', periods=6, normalize=True)
a = "foo"
news_sentiments.to_csv("News_Sentiments.csv")
pd.to_datetime(['2005/11/23', '2010.12.31'])
metadata['wavelength'] = refl['Metadata']['Spectral_Data']['Wavelength'].value $ metadata
new_nodes[1]
pd.crosstab(df_result.launched_year, df_result.State).plot()
df = db.call_non_select_stored_proc(DBConnections.FRAME_USER, '[Frame_User].[FinFC].[USP_HHdataVarAnalysis]', params=(4463, '2017-10-01', '2018-01-01', 'Forecast', 'all'), print_sql=True)
pickle.dump(lda_tfidf, open('iteration1_files/epoch3/lda_tfidf.pkl', 'wb'))
ans = reviews.loc[reviews.country.notnull() & reviews.variety.notnull()] $ ans = ans.apply(lambda srs: srs.country+'-'+srs.variety, axis='columns') $ ans.value_counts()
week47 = week46.rename(columns={329:'329'}) $ stocks = stocks.rename(columns={'Week 46':'Week 47','322':'329'}) $ week47 = pd.merge(stocks,week47,on=['329','Tickers']) $ week47.drop_duplicates(subset='Link',inplace=True)
win_rows = inputs['label'].sum() $ total_rows = inputs['label'].size $ perc_wins = win_rows / total_rows * 100 $ print('there are {0}({1:.2f}%) wins out of a total of {2} rows/matches'.format(win_rows, perc_wins, total_rows))
m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), $                    0.04, 1, [1000,500], [0.001,0.01], y_range=y_range) $ lr = 1e-3
to_be_predicted_Day5 = 48.64925054 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
baseball1_df.drop(baseball1_df.index [8107], inplace=True) $ baseball1_df.loc[baseball1_df['ageAtDebut'].idxmin()]
(df_merged['p1'].iloc[228], df_merged['p1_conf'].iloc[228],df_merged['p2'].iloc[228],df_merged['p2_conf'].iloc[228],df_merged['p3'].iloc[228],df_merged['p3_conf'].iloc[228])
age.iloc[0:3]
for df in (train,test): $     df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32) $     df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32) $     df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32) $     df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)
cur.execute("CREATE DATABASE IF NOT EXISTS test;") $ cur.execute("USE test;") $ cur.execute("show tables;") $ for r in cur.fetchall(): $    print(r)
dftop2 = dftop.groupby(['complaint_type']).size().reset_index(name = 'days_top_complaint')
p_new = (df2['converted']).mean() $ print(p_new)
df = pd.DataFrame(airline_tw_collec) $ df.head(2)
one_star_token_count = nb.feature_count_[0, :] $ five_star_token_count = nb.feature_count_[1, :]
my_stream.filter(track=['data'])
records[0:3]
data3 = pd.concat(dfs, ignore_index=True) $ data3.shape
df = pd.DataFrame(np.random.randn(8, 4), $ index = ['a','b','c','d','e','f','g','h'], columns = ['A', 'B', 'C', 'D']) $ df
data[data['processing_time']<datetime.timedelta(0,0,0)]
new_page_converted = np.random.choice([0, 1], size=nnew, p=[1-pnewnull, pnewnull])
RandomForestReg= RandomForestRegressor(max_depth= 25, min_samples_leaf= 1, n_estimators= 100,  random_state=42)
hour_of_day15.to_excel(writer, index=True, sheet_name="2015")
graphlab.__VERSION__
model = Ridge(alpha = 1) $ model.fit(X_tr, y_tr) $ preds = model.predict(X_val) $ np.sqrt(metrics.mean_squared_error(y_val, preds))
size_t = df2.query("group=='treatment'").count()[0] $ new_page_converted =  np.random.choice([0,1], size=(size_t,1), p=[1-pn,pn]) $
indexes=df.query('group=="treatment" and landing_page!="new_page" or group=="control" and landing_page!="old_page"').index.values $ indexes
dinw_filter_set = w.get_step_object(step = 2, subset = subset_uuid).get_indicator_data_filter_settings('din_winter')
import netCDF4 $ import numpy as np
postings.head(2).transpose()
print(df["YEAR"].dtype,df["YEAR"].dtype,df["DAY_OF_MONTH"].dtype,df["DEP_HOUR"].dtype) $ print(df_weather_origin["YEAR"].dtype,df_weather_origin["YEAR"].dtype,df_weather_origin["DAY_OF_MONTH"].dtype,df_weather_origin["DEP_HOUR"].dtype)
by_year = candidates.groupby("election_year").size().reset_index()
temp_cat_more = temp_cat.add_categories(['susah','senang']) $ temp_cat_more
print list(label_encoder.inverse_transform([0,1])) $ model.predict_proba(np.array([0,0,1,0,0,0,0,0,0,5,0])) 
reviews.info() $ reviews=pd.read_csv("ign.csv",index_col=['Unnamed: 0','score_phrase']) $ reviews.head()
clean_appt_df.groupby('Gender')['No-show'].value_counts(normalize=True).plot.bar()
df2.head()
test_norm.head(3)
transactions
grouped = df.groupby('Team') $ score = lambda x: x*10 $ grouped.transform(score)
control_conversion = df2[df2['group'] == 'control']['converted'].mean() $ control_conversion
from sklearn.ensemble import RandomForestClassifier $ rnd_clf = RandomForestClassifier(random_state=42)
co.steady_states.tail()
for col in var_cat_num: $     cats = taxi_sample[col].astype(np.int64).unique()[1:]  # drop first $     taxi_sample = taxi_sample.one_hot_encoding(col, prefix=col, cats=cats) $     del taxi_sample[col]
raw.rename(columns = lambda x: x.lower().replace(' ', ''), inplace = True) $ raw.drop(['ref', 'unnamed:2'], axis = 1, inplace = True, errors='ignore') $ raw.head(2)
list = [] $ lat = (festivals.at[1,'latitude']) $ long = (festivals.at[1,'longitude']) $ list.append((lat, long)) $ len(list)
bool(login_page.find_all(string=re.compile('redirect')))
l = []
df = pd.read_csv(datafile) $ print(df.as_matrix().shape)
print('X_train:', len(X_train)) $ print('y_train:', len(y_train)) $ print("Start:", datetime.datetime.now()) $ pipeline.fit(X_train,y_train) $ print("End:", datetime.datetime.now())
joined=join_df(joined, oil_prices, 'date')
df = df.set_index('datetime') $ df = df.resample('H').sum()
1/np.exp(-0.0150)
aml.leader
df=pd.read_csv("dataset_quora/info_test.csv")
pd.Period('3/5/2016')
df.loc[:, 'comments'] = df.loc[:, 'comments'].str[1:] 
df
length_unit = 'nm' $ area_unit = 'nm^2' $ energy_unit = 'eV' $ energyperarea_unit = 'mJ/m^2'
states = pd.DataFrame(locations['State'].value_counts()) $ top10states = states[0:10] $ top10states.plot(kind='bar') $ plt.show()
print('Scatter plot distribution of the error') $ sb.regplot(y_pred, errors, fit_reg=False) $ plt.xlabel('y_pred') $ plt.ylabel('errors')
bottom_views = doctype_by_day.loc[:,doctype_by_day.max() < 10] $ ax = bottom_views.plot() $ ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.plot(top_bc, label ='Top BC') $ plt.plot(bot_bc, label = 'Bot BC') $ plt.legend();
pd.isna(df).any()
print(train.isnull().sum()) $ train_att = train[train['is_attributed']==1] $ print(train_att.isnull().sum())
pos_dic.id2token.items()
import nltk $ from nltk.corpus import stopwords $ print(stopwords.words('english'))
print(df[df.goal_usd.isnull()].count()) $ describe = df.describe() $ print(describe) $ sns.boxplot( y=np.log(df.backers))
df_clean.sample(3)
def check_duplicate_rows(df, onColumns): $     duplicate = df.duplicated(subset = onColumns) $     return duplicate[duplicate == True] $ check_duplicate_rows(complete_wind_df, ['DateTime']) $ check_duplicate_rows()
brands
archive_copy['timestamp'].dtype
ax = pre_analyzeable['prio_sims_other'].value_counts().plot(kind='bar') $ for p in ax.patches: $     ax.annotate(str(int(p.get_height())), (p.get_x() * 1.005, p.get_height() * 1.005), fontsize=25)
convert_new = df2.loc[(df2.landing_page == "new_page") & (df2.converted == 1)].user_id.nunique() $ convert_new
df = spark.read.csv('../NYC311/nyc311_sample.csv', inferSchema=False, header=True) $ df.printSchema()
df_ok_samples.iloc[0:7,:12]
df_columns = pd.read_csv("/data/311_Service_Requests_from_2010_to_Present.csv", error_bad_lines=False, usecols=['Created Date','Closed Date','Agency','Complaint Type','Descriptor','Borough']) $
(null_vals > actual_diff).mean()
soup = bs(response.text, 'html.parser')
n_new = len(df2.query("group == 'treatment'")) $ n_new
df_archive_clean = df_archive_clean.drop(labels = errors_index,axis = "index")
crime_2015 = df[cols][df["DATE_TIME"].dt.year == 2015] $ by_month_percent_2015 = crime_2015["DATE_TIME"].dt.month.value_counts().sort_index() / crime_2015.shape[0] $ by_month_percent_2015.plot.bar() $ print(by_month_percent_2015.std())
future = m.make_future_dataframe(periods= int(len(test_p) * 1.1),freq= '1MIN')
cust_data['MonthlySavings1'] = cust_data['MonthlyIncome'] * 0.15 $ cust_data.head(2)
print(labels_np.shape) $ print(labels_np)
import matplotlib.pyplot as plt
data.dtypes
population.apply(lambda val: val > 1000000)
stoplist = set('for a of the and to in it be'.split())
Aust_result['tweetText']
dfrecent.sort_values(by = 'count_n', ascending = 0, inplace = True)
df_A.groupby('Gender').mean()
kick_projects_ip.head()
with model: $     idx = np.arange(n_count_data) $     lambda_ = pm.math.switch(tau > idx, lambda_1, lambda_2)
california_house_dataframe.head()
for tweet in trump_df.text: $     clean_tweet(tweet)
temp_df.to_csv('users_gh_emailHash_All.csv')
len(word_freq_df)
%%time $ ent_count = defaultdict(int) # reset defaultdict $ for doc in nlp.pipe(texts, batch_size=100, disable=['parser','tagger','ner']): $     matcher(doc) # match on your text $ print(ent_count)
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[n_new, n_old], alternative='larger') $ print('z-score: ', z_score) $ print('p-value: ', p_value)
mean_abs_dev = lambda x: np.fabs(x-x.mean()).mean() $ pd.rolling_apply(hlw,5,mean_abs_dev).plot();
autos['fuel_type'].cat.categories
lon_us = lon[lon_li:lon_ui]-360 $ lat_us = lat[lat_li:lat_ui] $ print(np.min(lon_us), np.max(lon_us), np.min(lat_us), np.max(lat_us))
optimizer_col= optim.Adam(model.parameters(),lr=0.001)
Ralston.head()
airquality_dup = pd.concat([airquality_melt, airquality_melt])
target_pf[target_pf['date'] == inception_date]
financial_crisis.size
countries_df.info()
ser.mean()
df["DISPOSITION_TYPE"].value_counts().sort_values(ascending=True).plot(kind='barh') $
data.head()
clean_appt_df.to_csv('processed_data/clean_appt_df.csv', index=False)
print "The day size is:", len(df_day_pear['tripduration']) $ print "The night size is:",len(df_night_pear['tripduration']) $
stem_porter = IndexedText(porter, chat_posts)
df2.query('group == "control"').converted.sum()/df2.query('group == "control"')['user_id'].count()
lastyear_day = session.query(Measurement.date).order_by(Measurement.date.desc()).first() $ lastyear_day $
Test.SetFlowStats(Ch1State=True, Ch2State=True, Ch3State=True)
jpn = ['Japan', 'Tokyo', 'JAPAN', 'Tokyo-to', 'Osaka', 'Okinawa', 'Chiba', 'Okinawa-ken', 'Kyoto', 'Miyazaki'] $ notus.loc[notus['country'].isin(jpn), 'country'] = 'Japan' $ notus.loc[notus['cityOrState'].isin(jpn), 'country'] = 'Japan' $ notus.loc[notus['country'] == 'Japan', 'cityOrState'].value_counts(dropna=False)
new_dems.Clinton.isnull().sum()
lm_country = sm.OLS(df_new['converted'], df_new[['intercept', 'US', 'UK']]) $ country_results = lm_country.fit() $ country_results.summary()
df['intercept']=1 $ df[['treatment','control']]=pd.get_dummies(df['group'])
print(points) $ print(points2) $ print(points+points2) $ (points+points2).dropna()
df2['intercept'] = 1 $ df2[['control','treatment']] = pd.get_dummies(df2['group'])
trace1 = go.Scatter(x=df['Date'], y=df['Likes'], name='Likes') $ trace2 = go.Scatter(x=df['Date'], y=df['Re_Tweets'], name='Re-Tweets') $ data = [trace1, trace2] $ py.iplot(data, filename='2line')
example_tweets[0] $
top_bc = df["0"].values[0:n_iterations] $ bot_bc = df['20'].values[0:n_iterations]
commiters_by_month = commiters['first_commit']\ $                      .groupby(commiters.first_commit.dt.to_period('M'))\ $                      .agg('count') $ pprint (commiters_by_month)
np.dtype({'names':('name', 'age', 'weight'), $           'formats':((np.str_, 10), int, np.float32)})
n_new = df2.query('landing_page == "new_page"').user_id.count() $ n_new
print(np.shape(a)) $ print(np.size(a))
df_test_index.action_type.value_counts()
yhat = LR_model.predict(X_test) $ yhat
len(nullCity2014)
df = pd.read_sql('SELECT * from payment', con=conn_b) $ df
import pydotplus $ from sklearn import tree $ dot_data = tree.export_graphviz(clf, out_file=None,feature_names= features,class_names = ["high","medium","low"]) $ graph = pydotplus.graph_from_dot_data(dot_data)   $
pd.datetime.now()
ggplot(aes(x="failure"), failures) + geom_bar(fill="blue", color="black")
learner.save_encoder('adam1_20_enc')
print("Probability of treatment group converting:", $       ab_file2[ab_file2['group']=='treatment']['converted'].mean())
help('modules')
expenses_df.drop(expenses_df.index[-1], inplace = True) $ expenses_df
type(Mars_tables)
import matplotlib.pyplot as plt $ import matplotlib.dates as mdates $ plt.gca().xaxis.set_major_locator(mdates.DayLocator()) $ plt.plot(df['order_date'],total_spend) $ plt.gcf().autofmt_xdate()
dfs = amzn.loc['AMZN']
df.to_csv('Crimes_-_2001_to_present_v2.csv', index=False)
weather_df.groupby(["weather_main", "weather_description"]).sum()
print bnb.first_affiliate_tracked.shape $ print pd.Series(bnb.first_affiliate_tracked).value_counts()
prob_convert_c = df2_control.converted.mean() $ prob_convert_c
shows1 = pd.read_csv('scraped_data4.csv') $ shows1.shape
rescue_code(normalize)
BASE = 'http://' + IP +  ':' + str(PORT_NUMBER) + '/v1/' $ HEADERS = {'Content-Type': 'application/json'} $ requests.delete(BASE + 'session') $ def pp(json_data): $     print(json.dumps(json_data, indent=4))
calls = pd.read_csv("311_Calls__2012-Present_.csv")
folds = 5 $ n = training_data.shape[0] $ kf = KFold(n,folds,random_state=123)
df.converted.sum()/df.converted.count()
print (featured_image_url)
df14 = pd.read_csv('2014.csv')
df_new.head()
df_questionable_3 = pd.merge(left= df_questionable, left_on= 'link.domain_resolved', $                              right= df_usnpl_one_hot_state, right_on= 'domain', how= 'left')
from matplotlib.pyplot import figure $ figure(num=None, figsize=(17, 4), dpi=80, facecolor='w', edgecolor='k') $ places = list(tweet_place_hist.index) $ frequencies = list(tweet_place_hist.place_freq) $ plt.bar(places, frequencies)
dog_ratings.rating_numerator.value_counts()
data2 = r.json()['dataset_data']['data']
max_open = [] $ for entry in d["dataset_data"]["data"]: $     max_open.append(entry[1]) $ print("The highest opening price was $"+str(max(max_open)))
height.apply(np.sqrt)
df.loc[df['ruling']=='Full Flop']
invoice_link_dropper = ['deleted_at', 'fk_s_change_context_id_cr', $                         'fk_s_change_context_id_dl', 'fk_x_invoice_hub_id', 'fk_x_invoice_item_hub_id', 'sk_id' $                        ] $ check_cols(invoice_link, invoice_link_dropper)
df_train[df_train['labels'] != 2].to_csv(CLAS_PATH / 'train.csv', header=False, index=False) $ df_val.to_csv(CLAS_PATH / 'test.csv', header=False, index=False) $ (CLAS_PATH / 'classes.txt').open('w').writelines(f'{o}\n' for o in CLASSES) $ df_train.head()
imp_cols = ['explain', 'info_found', 'url', 'created_at', 'node_id'] $ draft_df = all_df[imp_cols]
trips.dtypes
df2_control = df2.query('group == "control"') $ agg_df2_control = df2_control.query('converted == "1"').user_id.nunique() / df2_control.user_id.nunique() $ agg_df2_control
(details['Average Rating'] == 0.0).value_counts()
ls
df.loc[:,['A','B']]
plt.figure(figsize = (5,5)) $ plt.scatter(X_std[:,0],X_std[:,1], label='True Position') $ kmeans = KMeans(n_clusters=2)  $ kmeans.fit(X_std)  
loans_act_20150430_xirr=cashflows_act_investor_20150430.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
autos['last_seen'].str[:10].value_counts(normalize = True, dropna = False).sort_index()
df_questionable_3[df_questionable_3['state_NJ'] == 1]['link.domain_resolved'].value_counts()
data_full = pd.read_csv('invoices_full.csv')
df_group_by = df_transactions.groupby('msno').agg(f)
barcelona['GMT'] = to_datetime(barcelona['GMT']) $ barcelona = barcelona.sort('GMT')
df2[df2.duplicated(subset="user_id", keep=False)] 
recommendation_df = recommendation.to_dataframe()
query = { $     'type' : 'fill_vesting_withdraw', $     'timestamp' : {'$gte': dt.now() - datetime.timedelta(days=360)}} $ proj = {'deposited.amount': 1, 'withdrawn.amount': 1, 'timestamp': 1, 'from_account': 1, 'to_account':1, '_id': 0} $ sort = [('timestamp', -1)]
train_users_pd.loc[train_users_pd['gender'] == '-unknown-','gender'] = 'UNKNOWN' $ train_users_pd.loc[train_users_pd['gender'] == 'OTHER','gender'] = 'UNKNOWN'
df1 = pd.DataFrame(data, index=['rank1','rank2','rank3','rank4']) $ print(df1)
convRate = pd.concat([hired,shown],axis=1) $ convRate['rate'] = convRate['hired']/convRate['tasker_id']
tlen_k1.plot(figsize=(16,4), color='r'); $ tlen_k2.plot(figsize=(16,4), color='b');
mod.fit(trainx, trainy)
df = pd.read_csv('data/test1.csv', parse_dates=['date'], index_col='date') $ df
ac['Eligibility Date'].groupby([ac['Eligibility Date'].dt.year]).agg('count')
fin_r_monthly.shape
filter_frame = event_list['n_frames'] > -1 $ event_list[filter_frame]
beirut = beirut.rename(columns={'WindDirDegrees<br />' : 'WindDirDegrees'}) $ beirut['WindDirDegrees'] = beirut['WindDirDegrees'].str.rstrip('<br />') $ beirut['WindDirDegrees'].head(3)
autos = pd.read_csv('autos.csv', encoding='Latin-1')
final_df.corr()["ground_truth_crude"][names]
sample_text = "Hey there! This is a sample review, which happens to contain punctuations." $ print(text_process(sample_text))
sdof_resp(1,0.1,1,.1,10,10)
pd.Period('2012', freq='A') - pd.Period('2002', freq='A')
logit_model = sm.Logit(df3['converted'],df3[['intercept','ab_page','CA','UK']]) $ results = logit_model.fit() $ results.summary()
df2.nunique()
df['J']
import urllib $ url = "https://www.google.co.uk/search?q=%22search+example%22&oq=%22search+example%22&aqs=chrome..69i57j0l5.5927j0j7&sourceid=chrome&ie=UTF-8" $ url
leadConvpct.tail()
to_be_predicted_Day3 = 55.22955286 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
print('{:6}Hi'.format(42)) $ print('{:<6}Hi' .format(42))
prob_rf1000 = rf1000.predict_proba(Test) $ prob1_rf1000 = pd.Series(x[1] for x in prob_rf1000) $ Results_rf1000 = pd.DataFrame({'ID': Test.index, 'Approved': prob1_rf1000}) $ Results_rf1000 = Results_rf1000[['ID', 'Approved']] $ Results_rf1000.head()
print('(Rows, Columns):', df.shape) $ df.head()
archive.floofer.value_counts()
multi_col_lvl_df.to_csv('multi_col_lvl_output.csv') $ read_multi_df = pd.read_csv('multi_col_lvl_output.csv', header=[0, 1], index_col=[0, 1, 2, 3, 4, 5], $                             skipinitialspace=True, parse_dates=[0]).head(3) $ read_multi_df
learner.load_encoder('adam3_10_enc')
for idx, row in df_trips.iterrows(): $     df_trips.loc[idx, "trip_ended"] = row["trip_started"] + np.random.randint(60 * 5, 60 * 60)
cols=['text', 'title', 'url', 'id', 'subreddit', 'meta', 'time', 'author', $       'ups', 'downs', 'authorlinkkarma', 'authorcommentkarma', 'authorisgold']
hour_of_day14.to_excel(writer, index=True, sheet_name="2014")
df.groupby(['group', 'landing_page']).nunique()
old_page_converted = np.random.choice([1,0], size=n_old, p=[p_old, (1-p_old)]) $ old_page_converted
print(new_page_converted/n_new - old_page_converted/n_old)
aru_df.shape
filepath2 = "data/kickstarter-projects/" $ df2 = pd.read_csv(filepath2 + "ks-projects-201801.csv", encoding = 'ISO-8859-1', low_memory=False)
path = "/Users/Yujiao/Desktop/twitter_Ikea/Data/IKEA_database/" $ Stockholm_data = pd.read_csv(path + "stockholm.csv")
frame2
Manhattan_gdf = newYork_gdf[newYork_gdf.boro_name == 'Manhattan'] $ Manhattan_gdf.crs = {'init': 'epsg:4326'}
p4_result = p1_table.sort_values('Profit', ascending=True) $ p4_result.head()
from shapely.geometry import Point $ data3['geometry'] = data3.apply(lambda x: Point((float(x.lon), float(x.lat))), axis=1)
cluster_label_group = df.groupby("cluster_labels").mean()
job_requirements.values
m + 1 # ie we are adding a month to the one that we started with
df['month'] = df.activity_date_time_c.dt.to_period('M')
class_items=pd.DataFrame(group['item_nbr'].agg('count')) $ pd.DataFrame.head(class_items)
df.isnull().sum().sum()
tweet_hour.drop(columns=tweet_hour.columns[3:],axis=1,inplace=True)
glm_binom_v1 = H2OGeneralizedLinearEstimator( $     model_id='glm_v3', $     solver='L_BFGS', $     family='binomial') $ glm_binom_v1.train(covtype_X, covtype_y, training_frame=train_b, validation_frame=valid_b)
second_cluster = LDACluster(num_topics=20) $ second_cluster.fit(docs) $ r1 = second_cluster.model.print_topics(10)
event_num = suspects_data.groupby('imsi')['event_type'].unique().apply(len)
g['Created Date'] = pd.to_datetime(g['Created Date']) $ g['created_year'] = g['Created Date'].dt.year $ g.head(2)
from sklearn import linear_model $ lm = linear_model.LinearRegression() $ lm.fit(x_train, y_train)
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=24000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
from sklearn.feature_extraction.text import TfidfTransformer $ tfidf = TfidfTransformer() # l2 normalization by default $ np.set_printoptions(precision=2) $ tfidf.fit_transform(count.fit_transform(docs)).toarray()
autos["price"] = (autos["price"] $                       .str.replace("$", "") $                       .str.replace(",", "") $                       .astype(float) $                  )
submission_counts.describe()
full_nan = full.replace('@NA',np.NaN) # replace @NA $ full_nan = full_nan.replace('',np.NaN) # didn't find any empty strings
pd.to_datetime(['2009/07/31', 'asd'], errors='ignore')
impressions_products.info()
resdf.iloc[:,114:129].to_sql('demotabl',conn)
df_twitter_copy.head()
df_raw.head()
age_median = df_titanic_temp['age'].median() $ print(age_median) $
results = session.query(measurement.station, measurement.tobs).\ $     filter(measurement.station == 'USC00519281').\ $     filter(measurement.date >= prev_year).all()
obs_diff = obs_conv_new - obs_conv_old $ print(obs_diff)
airlines_day_unstacked = airlines_day.unstack().reset_index() $ airlines_day_unstacked.rename(columns={'level_0': 'airline', 0: 'count'}, inplace=True)
print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $
print(saved)
sean = relevant_data[relevant_data['User Name'] == 'Sean Hegarty'] $ sean['Event Type Name'].value_counts().plot(kind='barh')
wb =  'SE654470-222700' $
donors.loc[donors['Donor Zip'] == 606 , 'Donor City'].value_counts()
print(df.shape) $ print(df2.shape) $ print(both_dfs.shape)
for i in np.range(1,1e5): $     with open('test_string', mode) as f: $             f.write(  )
df_afc_champ2018['playId']
df=pd.DataFrame(np.random.randn(1000,3),columns=['A','B','C'],index=pd.date_range('1/1/2012',periods=1000,freq='h'))
own_star.drop(['created_at_own'], axis=1, inplace=True)
from IPython.display import HTML, display
start = df['versionCreated'].min().replace(hour=0,minute=0,second=0,microsecond=0).strftime('%Y/%m/%d') $ end = df['versionCreated'].max().replace(hour=0,minute=0,second=0,microsecond=0).strftime('%Y/%m/%d') $ Minute = ek.get_timeseries(["IBM.N"], start_date=start, interval="minute") $ Minute.tail()
from scipy.stats import norm $ z_sig = norm.cdf(z_score) $ crit_val = norm.ppf(1-(0.05/2)) $ print(z_sig, crit_val)
def code_email(s): $     return mail[s] $ def code_creation(s): $     return creation[s]
fig = acc.plot_history(what='amount')
dir = '/Users/sumad/Documents/DS/Python/UM Spcialization/DS_with_Python/' $ with open(dir + 'census.csv') as con: $     cs = pd.read_csv(con)
s_median = s.resample('5BM').median() $ s_median
import guipyter
cog_simband_times
findname('This is a Dasani Kingfisher from Maine. His name is Daryl. Daryl doesnt like being swallowed by a panda.  ')
cols = ['GP', 'GS', 'MINS', 'G', 'A', 'SHTS', 'SOG', 'PKG', 'PKA', 'SC%', 'Year'] $ shots_df[cols] = shots_df[cols].apply(pd.to_numeric)
from sklearn.preprocessing import StandardScaler $ ss = StandardScaler() $ Xs = ss.fit_transform(X) $ type(Xs) $ Xs
recommend.head()
with open('test.csv') as f: $     size=len([0 for _ in f]) $     print("Records in test.csv => {}".format(size))
s.data_filter.all_filters
df[df['Descriptor'] == 'Loud Music/Party']['Unique Key'].groupby(df[df['Descriptor'] == 'Loud Music/Party'].index.dayofweek).count().plot()
df = pd.DataFrame.from_records(sf_json)
lims_query = "SELECT specimens.name AS cell_name, donors.id, specimens.donor_id, specimens.external_specimen_name, donors.full_genotype, donors.name, specimens.patched_cell_container \ $ FROM donors JOIN specimens ON specimens.donor_id = donors.id \ $ WHERE specimens.ephys_roi_result_id IS NOT NULL" $ lims_df = get_lims_dataframe(lims_query) $ lims_df
tweet_archive_df.head(3)
tweet_archive_master = tweet_archive_clean.merge(image_predictions_clean, on='tweet_id').merge(tweet_json_clean, on='tweet_id')
user.head(3)
k_means_labels = k_means.labels_ $ k_means_labels
no_of_obs_by_station = engine.execute('SELECT station, count(*)  FROM measurement group by station order by count(*) desc').fetchall() $ no_of_obs_by_station
print(pos) $ print(neg)
crimes.PRIMARY_DESCRIPTION.head()
calls_df["time"]=calls_df["call_date"].dt.hour $ calls_df["call_time"]="NULL"
DataSet_sorted[['tweetText', 'prediction']].head(10)
def validate_block(block): $     if isinstance(block, int): $         block = hex(block) $     return block
kickstarter.isnull().sum()
pc_order
nt_price.to_csv(path+"datas/avg_house_price.csv")
words = ['Data Science','DataScience','datascience','Data Scientist','data science','data','data scientist','Data scientist'] $ pat = '|'.join(words) $ datascience_tweets = megmfurr_tweets[megmfurr_tweets['text'].str.contains(pat)] $ megmfurr_tweets[megmfurr_tweets['text'].str.contains(pat)]['text'].count()
pred2 = nba_pred_modelv1.predict(g2) $ prob2 = nba_pred_modelv1.predict_proba(g2) $ print(pred2) $ print(prob2)
diff_weekly_mean_comp = ((day_counts['count'] - functions.avg('count').over(weekly_hashtag_window))**2)
all_scores = [] $ for k in range(10,200,10): $     knn_reg = KNeighborsRegressor(n_neighbors = k) $     knn_reg.fit(x_train,y_train) $     all_scores.append(knn_reg.score(x_test,y_test))
ab_dataframe = pd.read_csv('ab_data.csv') $ ab_dataframe.head()
com_grp.ngroups
all_noms[all_noms["agency"] == "Foreign Service"]["nom_count"].sum()
plot_data(200)
city_holidays_df = ph.loc[ph['city_hol']==1, ['date', 'city', 'city_hol']].copy().reset_index(drop=True) $ state_holidays_df = ph.loc[ph['state_hol']==1, ['date', 'state', 'state_hol']].copy().reset_index(drop=True) $ nat_holidays_df = ph.loc[ph['nat_hol']==1, ['date', 'nat_hol']].copy().reset_index(drop=True) $ nat_events_df = ph.loc[ph['nat_event']==1, ['date', 'nat_event']].copy().reset_index(drop=True)
df_master.shape
tweets.text[0]
del rows
results_3 = pd.DataFrame(list(results3), columns=['tags', 'values'])#, index=[1,2,3,4]) $ results_3.head(20)
trumpint = ddf[(ddf['status_published'] > '2016-07-19') & (ddf['status_published'] < '2016-11-08')] $ trumpint
cb
pregnancies.topics.loc[0]
richard_simulation = ET_Combine ["Baseflow = 1D Richards'"] $ lumped_simulation = ET_Combine ['Baseflow = Topmodel(lumped)'] $ distributed_simulation = ET_Combine ['Baseflow = Topmodel(distributed)']
stemmed_dict_list = list(stanford_word_list.Word_stem)
S_1dRichards.decision_obj.simulStart.value, S_1dRichards.decision_obj.simulFinsh.value
usersDf.hist(column=['listed_count'],bins=50) $
df.resample('D', how='sum')
imgp_clean.info()
all_df.describe()
df.dtypes
df['goal_date_ratio'] = df['date_diff'] / df['goal'] $ df.loc[df['date_diff'] == 0] = 1 $ df['goal_pledge_date'] = (df['pledged'] / df['goal']) / df['date_diff']
df_unique_users_treatment = df2[df2['group'] == 'treatment'] $ n_unique_users_treatment = len(df_unique_users_treatment) $ n_conversion_treatment = len(df_unique_users_treatment[df_unique_users_treatment['converted'] == 1]) $ probability_treatment = n_conversion_treatment/n_unique_users_treatment $ print ("The probability of an individual converting from the treatment group: {:.4f}".format(probability_treatment))
new_page_converted = np.random.choice([1, 0], size=nnew, p=[pnew, (1-pnew)])
ad_group_performance.drop(columns=['TheAnswer']) $ ad_group_performance
data_numeric = auto_new.select_dtypes(exclude="object")
sub_df = sub_df.merge(op_add_comms, on='id', how='outer')
sum(tweetsDF.location.value_counts())
import os
yhat = LR.predict(X_test) $ yhat
def get_twitter_api(consumer_key,consumer_secret, access_token, access_token_secret): $     auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $     auth.set_access_token(access_token, access_token_secret) $     return tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)
new.Purchased.value_counts()/len(new)*100 $
col = ['msno','date','num_25','num_50','num_75','num_985','num_100'] $ df = user_logs[col] $ df.loc[:,"num_25":"num_100"] = df.loc[:,"num_25":"num_100"].div(df.loc[:,"num_25":"num_100"].sum(axis=1), axis=0) $ df
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
data[ $     ["song_freq", "user_freq", "genre_freq", "target"] $ ].astype(float).corr().abs()
fig, ax = plt.subplots(figsize=(12,12)) $ xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax) $ plt.show()
for i in range(-5, 0, 1) : $     vol[f'Volume {i}d'] = vol['Volume'].shift(-i) $ vol = vol.dropna() $ vol.head()
!head ../data/olympics.1996.txt
df.describe()
control_group = df2.group == 'control' $ control_group_and_converted = control_group & (df2.converted == 1) $ len(df2[control_group_and_converted]) / len(df2[control_group])
to_be_predicted_Day3 = 50.95463678 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
df_predictions_clean
corn["price"].max() $ corn[["price", "sold"]].max()
counts_by_campaign_date = ad_groups_zero_impr.groupby( $     ['CampaignName', 'Date'] $ ).count() $ counts_by_campaign_date
import numpy as np $ print(np.info(np.tile))
df2['intercept'] = 1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group'])
duplicate_events_df = pd.DataFrame(duplicate_check_df['event_id'].value_counts()) # count values for events $ duplicate_events_df.reset_index(inplace=True)  # abstratc the event_id to be a column $ duplicate_events_df.columns = ['event_id', 'frequency']  # rename columns with more clear names $ duplicate_events_df[duplicate_events_df['frequency'] > 1] # filter the duplicate events and show the results
merged_df = pd.merge(left=g,right=p,how='inner',left_on='customer',right_on='customer',copy=True) $ print(merged_df.shape) $ print(merged_df['validOrders'].sum()) $ merged_df.head() $
cat 1st_flask_app_2/templates/_formhelpers.html
cityID = '18810aa5b43e76c7' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Dallas.append(tweet) 
merge.to_csv('Data/Incidents.csv') $
df['timestamp']=pd.to_datetime(df['timestamp'],format='%Y-%m-%d')
np.histogram(s2)
df.groupby(['userid', 'website']).sum()
DataSet.head()
ser.loc[:1]
data_2017_subset.head(5)
control = df2.query('group == "control"') $ converted = control.query('converted == 1') $ p_control_convert = converted.count()[0]/control.count()[0] $ p_control_convert
search2 = search2.sample(frac=1)
train_dum_clean.shape
IBMspark_df.registerTempTable("IBM") $ print sqlContext.sql("select * from IBM limit 10").collect()
doctype_grouped
twitter_final['year'] = twitter_final['date'].apply(lambda x : x.year) $ twitter_final['month'] = twitter_final['date'].apply(lambda x : x.month) $ twitter_final['day'] = twitter_final['date'].apply(lambda x : x.day) $ twitter_final['dayofweek'] = twitter_final['date'].apply(lambda x : x.dayofweek)
import statsmodels.api as sm $ convert_old = df.query("landing_page == 'old_page'")['converted'].sum() $ convert_new = df.query("landing_page == 'new_page'")['converted'].sum() $ n_old = df.query("landing_page == 'old_page'")['user_id'].count() $ n_new = df.query("landing_page == 'new_page'")['user_id'].count() $
del df0
df_new = ct_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
df_leiden = df_city[(df_city.Lon>=4.438) & (df_city.Lon<=4.525) & (df_city.Lat>=52.118) & (df_city.Lat<=52.187)] $ sns.pairplot(df_leiden, x_vars='Lon', y_vars='Lat', size=6) $ plt.show()
h + 1 # If we add one hour to our time period object, we get the expected result
pd.to_datetime([1349720105, 1349806505, 1349892905, 1349979305, 1350065705], unit='s')
X = df.drop('num_comments', axis = 1)
for key,value in trends_per_year_avg_ord.items(): $     print( "Deaths and Trend average in " + str(key) + " = " + str(deaths_per_year[key]) + ", " + str(trends_per_year_avg[key]))
own_star_2csv = own_star.drop(['forked_from_repo_id', 'owned', 'created_at', 'starred'], axis=1) $ own_star_2csv = own_star_2csv.rename(columns={'repo_id': 'item', 'user_id': 'user'}) $ own_star_2csv[['user', 'item', 'rating']].to_csv('data/new_subset_data/ratings_data.csv', sep='\t', index=False)
sp.head()
total = df.shape[0] $ print('The dataset consists of {} rows.'.format(total))
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.choice([0, 1], size=n_new, p=[1-p_new, p_new]) $     old_page_converted = np.random.choice([0, 1], size=n_old, p=[1-p_old, p_old]) $     p_diffs.append(new_page_converted.mean()-old_page_converted.mean()) 
gatecount.head()
p_diffs = np.array(p_diffs) $
type(my_town.sentiment)
import tensorflow as tf $ tf.logging.set_verbosity(tf.logging.ERROR)
desc_order_active_stations = session.query(Measurement.station ,func.count(Measurement.station)).\ $                     group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all() $ desc_order_active_stations
PATTERNS2 = {k:re.compile(v) for k, v in PATTERNS.items()}
df_twitter[df_twitter['retweet_count'] == 84387]
dates = session.query(Measurement.date, Measurement.prcp).order_by(Measurement.date.desc()).limit(365).all() $ s_dates = pd.DataFrame(dates) $ s_dates.head(1)
miss_grp1 = df.query("group =='treatment' and landing_page == 'old_page'") $ print('The number of times a user from the treatment group lands on the old page is {}'.format(len(miss_grp1)))
import pandas as pd $ git_log = pd.read_csv("datasets/git_log.gz", header=None, sep="#", encoding="latin-1", names=["timestamp", "author"]) $ git_log.head(5)
df_goog.sort_values('Date', inplace=True)    # This is a good idea to sort our values so the indexes ultimately line up $ df_goog.set_index('Date', inplace=True)      # also df_goog.index = df_goog['Date'] works well here $ df_goog.index = df_goog.index.to_datetime()  # Convert to datetime
tweets_clean['retweet_count'] = 0 $ tweets_clean['favorites_count'] = 0 $ tweets_clean.head() $
autos=autos.drop('nrOfPictures',1)
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US']]) $ results = lm.fit() $ results.summary()
preprocessor.print_infos('consistency')
len(train[simple_features][train[simple_features].price_per_bedroom==np.inf])
numofstations=session.query(Station.station).count() $ numofstations
df2[df2['landing_page']=='new_page'].count()
lda_tfidf.print_topics(num_topics=10, num_words=7) $
token_sendcnt = token.groupby(["sender","month"]).size().reset_index(name= "sendcount")
grouped_dpt["Revenue"].filter(lambda x: x.sum() > 1000)
import cx_Oracle $ import pandas as pd
MA_final.head(10)
crs = {'init': 'epsg:4326'} $ geometry = df_TempIrrs['lineString'] $ geo_TempIrrs = gpd.GeoDataFrame(df_TempIrrs, crs=crs,geometry = geometry)
print(autos['odometer_km'].unique().shape) $ print(autos['odometer_km'].describe()) $ print(autos['odometer_km'].value_counts().sort_index(ascending=True))      
sh_max_df.dtypes
control=df2.query('group=="control"') $ con=df2.query('group=="control" and converted==1').count()[0]/control.count()[0] $ con
serc_plot = plt.imshow(b56,extent=serc_ext,cmap='Greys') 
inv.shape
act_p_new = df2.query('group=="treatment"')['converted'].mean() $ act_p_old = df2.query('group=="control"')['converted'].mean() $ act_diff = np.array(act_p_new - act_p_old) $ (p_diffs > act_diff).mean()
print(df.shape) $
tl_2050 = pd.read_csv('input/data/trans_2050_ls.csv', encoding='utf8', index_col=0)
df_new.groupby('country').mean()
bnbA['date_first_booking'] = bnbA.date_first_booking.fillna(0)
print(stockP['apple'])
p_diffs = np.array(p_diffs) $ (p_diffs > diff).mean()
opportunities_not_lost = opportunities[opportunities['StageName'] != 'Closed - Lost'].groupby('Account ID')['Building ID'].count().reset_index()
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_axial_brace_crown[1][0])) $ perr = np.sqrt(np.diag(pcov_axial_brace_crown[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
sl_pf_v2.sample()
prods_user2 =pd.merge(priors_product_purchase_spec,priors_product_reordered_spec,how="outer") $ prods_user2['reorder_ratio']=prods_user2['reordered_count_spec']/prods_user2['purchase_count_spec'] $ prods_user2.head()
p_control_obs = df2.query('group == "control"')['converted'].mean() $ p_control_obs
np.eye(5)
df_new['country'].value_counts()
Base = automap_base() $ Base.prepare(engine, reflect=True)
df.Category.value_counts()
cityID ='5c62ffb0f0f3479d' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Phoenix.append(tweet) 
outcome=[0,1] $ new_page_converted=np.random.choice(outcome,n_new, p=(1-p_new,p_new)) $ new_page_converted
autos = autos[autos['price'].between(500, 14000)] $ autos['price'].hist()
start = '2016-01-02' $ end = '2018-06-01'
print train_data[['building_id','display_address','listing_id','manager_id', $                  'street_address']].apply(lambda col: col.nunique())
returns.corrwith(returns.IBM)
subwaydf['4HR_Entries'] = subwaydf['ENTRIES'] - subwaydf['ENTRIES'].shift(1) $ subwaydf['4HR_Exits'] = subwaydf['EXITS'] - subwaydf['EXITS'].shift(1)
list(pn_and_qty.iteritems())
names = pd.Series(['Alice', 'Bob', 'Carol']) $ phones = pd.Series(['555-123-4567', '555-987-6543', '555-245-6789']) $ dept = pd.Series(['Marketing', 'Accounts', 'HR']) $ staff = pd.DataFrame({'Name': names, 'Phone': phones, 'Department': dept})  # 'Name', 'Phone', 'Department' are the column names $ staff
twitter_Archive.info()
temp = ['low','high','medium','high','high','low','medium','medium','high'] $ temp_cat = pd.Categorical(temp, ordered=True) $ temp_cat
zip_1_df = two_zips_df[two_zips_df.Zip==70117] $ zip_1_df = zip_1_df.groupby(zip_1_df.TimeCreate).size().reset_index() $ zip_2_df = two_zips_df[two_zips_df.Zip==70119] $ zip_2_df = zip_2_df.groupby(zip_2_df.TimeCreate).size().reset_index()
brands = brand_counts[brand_counts > 0.05].index $ brands
cols = ['tweetId', 'screenName', 'text', 'processed', $         'replyToSN', 'truncated',  'isRetweet', 'retweetCount', 'favoriteCount',    $         'created', 'longitude', 'latitude', 'sentiment_score', 'sentiment'] $ tweets = tweets[cols]
tweets_df.head()
year13 = driver.find_elements_by_class_name('yr-button')[12] $ year13.click()
reqs.index.set_names('num', level=0, inplace=True) $ reqs.head()
data1=data1_new.loc[:69,:]
results.summary()
!wget https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv $ !head -n 2 Consumer_Complaints.csv
sns.regplot(x=df2['age'], y=df2['ltv'])
trains_fe1=pd.merge(trains_fe,users_fin) $ trains_fe1.head()
data['yyyymm'].head()
merged_data['inv_creation_day'] = merged_data['invoices_creation_date'].dt.day $ merged_data['inv_creation_month'] = merged_data['invoices_creation_date'].dt.month $ merged_data['inv_creation_year'] = merged_data['invoices_creation_date'].dt.year
liberia_data.Description.replace('Newly Reported Cases in HCW', 'Total new cases registered so far', inplace=True) $ liberia_data.Description.replace('Newly Reported deaths in HCW', 'New deaths registered', inplace=True)
df = pd.DataFrame({"number": [2,5,4,7,3,0], "raw_grade": ['a', 'b', 'c', 'd', 'e', 'f']}) $ df
webmap = gis.map('Meades Ranch, KS') $ webmap
final_topbikes.to_csv('final_top_bikes.csv')
autos["price_dollars"].value_counts().sort_index(ascending=False).tail(200)
merge_DA_imb_power_df.to_excel(data_folder_path + '/final-merged-df.xlsx', index = False)
data.simple_features
predict_sentiment(post_clean_wnl) #Lemmatized sentences lead to lower accuracy
df_2015 = pd.DataFrame(rows)
train.describe()
authors_with_name = authors_grouped_by_id_saved.select( $     "*", parse_name_info_udf("Author").alias("parsed_info")).cache()
tia[tia['duration']>dt.timedelta(30)]
df.info()
ss=StandardScaler()
df_mes = df_mes[(df_mes['extra']==0)|(df_mes['extra']==0.5)|(df_mes['extra']==1)|(df_mes['extra']==1.5)] $ df_mes.shape[0]
np.mean(df2.days_active)
%%time $ df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', nrows=400000)
con = sqlite3.connect('db.sqlite') $ pd.read_sql_query("SELECT * from sqlite_master", con)
df.to_csv('tab_delim_clean.csv', sep='\t')
day_of_year15.to_excel(writer, index=True, sheet_name="2015")
ab_diff = df[df['landing_page'] == 'new_page']['converted'].mean() -  df[df['landing_page'] == 'old_page']['converted'].mean() $ p_diffs = np.array(p_diffs) $ (ab_diff < p_diffs).mean()
float(trips_sorted_pilot["overlap"].sum()) / len(trips_sorted_pilot)
show_crosstab('paymeth_code')
df = pd.DataFrame(np.random.randn(25).reshape(5,5), $                 index=list('abcde'), $                 columns=list('vwxyz')).round(2); $ df
print('Examples of tweets (with only text and hashtag column):') $ twh[['text', 'hashtag']].head(3)
learn.sched.plot()
dog_breeds = pd.Series({index: get_probable_breed(row) for index, row in image_predictions_clean.iterrows()})
from bokeh.io import output_notebook $ output_notebook()
train[train['is_weekend'] == 1].head(3)
y_cm.shape
autos["odometer_km"].value_counts().sort_index(ascending=True).head(15)
X_train, X_test, y_train, y_test = model_selection.train_test_split(X_sl_training, Y_sl_training, $                                                                     test_size=.2, random_state=7, $                                                                     stratify=Y_sl_training) $
model.show_topic(0)
df = pd.read_csv('ab_data.csv') $ df.head(10)
pd.describe_option("display.max_rows")
flights = pd.read_csv("./data/flight_sample.csv")
topics_doc_count = docs_by_topic.count()[['TopicNum']] $ topics_doc_count
people.insert(1, "height", [172, 181, 185]) $ people
train_session_v2.head()
weather_data_null.groupby([weather_data_null.index.month]).agg('count').head(12)
autos["date_crawled"].str[:10].\ $ value_counts(normalize=True,dropna=False).\ $ sort_index(ascending=True).describe()
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365) $ print(last_year) $
w.get_filtered_data(step = 1)[['DIN', 'SALT_CTD']].apply(pd.to_numeric).dropna(thresh=2)
old_page_df = df2.query('landing_page =="old_page"')
nba_df.info()
HARVEY_92_USERS_AC[5:10]
article_divs = [item.find('div',{'class':'article--container'}) for item in soups]
score_b.shape[0] / score.shape[0]
compound_sub2 = compound_sub2.append(compound_wdate_df4)
unique_words_sk = set(words_sk) $ corpus_tweets_streamed_keyword.append(('unique words', len(unique_words_sk))) # update corpus comparison $ print('Number of unique terms: ', len(unique_words_sk))
autos['price'].hist()
corpus_tfidf = tfidf[corpus]
points = points.reindex(mindex) $ points
df.groupby("pickup_week")["cancelled"].mean()
SVPOL(data/'realdata'/'MM.dat').to_dataframe().head()
px.isnull().sum()
prediction_plot = prediction.plot(x= 'Prediction 1', y= 'difference 1', style=".", label = "prediction 1", color = 'blue') $ prediction.plot(x= 'Prediction 2', y= 'difference 2', style=".", color = 'red', ax=prediction_plot, label = "prediction 2") $ prediction.plot(x= 'actual order date', y= 'difference 1', style=".", color = 'green', ax=prediction_plot, label = "actual")
import os $ %matplotlib inline $ import pandas as pd $ import seaborn as sns $ import matplotlib.pyplot as plt
index_df = index_df[['SPX']]
plt.hist(p_diffs) $ plt.xlabel("p_diffs") $ plt.ylabel("frequency") $ plt.title("plot of simualted p_diffs")
d311_gb.to_csv(processed_path+"c311_incident_count.csv")
MergeWeek = Merge.copy(deep=True)
df_combined['CA_ind_ab_page'] = df_combined['CA']*df_combined['ab_page'] $ df_combined['US_ind_ab_page'] = df_combined['US']*df_combined['ab_page']
df2 = df.drop(control_wrong.index) $ df2.drop(treatment_wrong.index, inplace = True)
incl_Ss = QUIDS_wide["subjectkey"]
train['visit_date']=pd.to_datetime(train['visit_date'])
dsg.axes('T')
adj_close_acq_date['Date Delta'] = adj_close_acq_date['Date'] - adj_close_acq_date['Acquisition Date'] $ adj_close_acq_date['Date Delta'] = adj_close_acq_date[['Date Delta']].apply(pd.to_numeric)  $ adj_close_acq_date.head()
date_now = datetime.today() $ date_now
top_allocs = hist_alloc.loc[pd.to_datetime(intervals)].sum(axis=0).sort_values(ascending=False) $ top_allocs[:10], top_allocs[-10:]
seeds_setting_simulation(seeds,network_simulation) $ seeds_setting_features(seeds,users)
set(w_counts)
oppose_NNN.sort_values("amount", ascending=False).head()
consumer_key = os.environ.get('TWEEPY_API_KEY') $ consumer_secret = os.environ.get('TWEEPY_API_SECRET') $ access_key = os.environ.get('TWEEPY_ACCESS_TOKEN') $ access_secret = os.environ.get('TWEEPY_TOKEN_SECRET')
output= "SELECT date, count(*) as srt from tweet_details group by date order by srt desc limit 10 " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['Tweet_id','Count of Tweets'])
tstLoc1 = gMapAddrDat.getGeoAddr(40.699100, -73.703697, test=True)  ## use .raw on output during testing $ print(type(tstLoc1))                                                ## to view JSON structure of Location obj $ tstLoc1.raw
countries_df = pd.read_csv('countries.csv') $ reg_df = reg_df.merge(countries_df, on='user_id') $ reg_df[['is_CA', 'is_UK', 'is_US']] = pd.get_dummies(reg_df['country']) $ reg_df.head()
results = model.transform(test) $ results=results.select(results["ID"],results["CHURN"],results["label"],results["predictedLabel"],results["prediction"],results["probability"]) $ results.toPandas().head(6)
df_tweet = pd.read_json('./WeRateDogs_data/tweet_json.txt', lines=True)
df.shape
coins_thisplot = ['Monero'] + coins_top10 # incl monero so BTC is orange in fig $ w[coins_thisplot].plot(kind='area', legend=True) $ plt.title('Fund weights for different coins \n mostly BTC before 2017') $ plt.ylabel('Weight') $ plt.show()
autos = autos.drop(["seller", "offer_type"], axis=1)
detroit_census2.head(1)
print df.shape $ df.index
df.isnull().sum()
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=10000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
fx.head()
top_10_complaint_types = data_2017_subset.complaint_type.value_counts().nlargest(10) $ top_10_complaint_types
start_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first() $ start_date
deltadf.to_csv('exports/trend_deltas.csv')
rGraphData.groupby(['from', 'to']).sum().tail()
np.random.seed(123) $ b = pd.Series(np.round(np.random.uniform(0,1,10),2)) $ c = b.copy() $ b.index = np.random.permutation(np.r_[0:10]) $ b
stations = session.query(Measurement.station).distinct().all() $ print(len(stations))
import pandas as pd $ url = 'http://raw.githubusercontent.com/chrisalbon/simulated_datasets/master/data.csv' $ df = pd.read_csv(url) $ df.head(2)
cell_df = pd.read_csv("cell_samples.csv") $ cell_df.head()
import sys $ sys.path.append('/content/models/research/') $ sys.path.append('/content/models/research/slim/')
Image(user['profile']['image_192'])
match.iloc[:,:11].head()
keff = res_esc * fast_fiss * therm_util * eta * p_fnl * p_tnl $ keff.get_pandas_dataframe()
closePrice.sort_values()[-1] $ closePrice.sort_values(ascending=False)[0]
test_id_ctable = df.groupby(['test_id', 'pass_test']).size().unstack('test_id').fillna(0) $ _, p, _, _ = chi2_contingency(test_id_ctable) $ p
df_new['CA'] = pd.get_dummies(df_new['country'])['CA']
cand_date_df = cand_date_df[cand_date_df['state'] != 'NY']
data_tokenized = [] $ for item in data_splitted: $     item = item.split(",") $     data_tokenized.append(item) $ print(data_tokenized[0][0])
legHouse["first_name"] = legHouse["first_name"].fillna(indexed_df["first_name_y"])
df_final.columns = ['user_id','partner_name', 'first_trx', 'last_trx', 'total_profit', 'monetary', 'cluster', 'frequency']
datatest.loc[datatest.surface_total_in_m2.isnull(),'surface_total_in_m2'] = datatest['surface_covered_in_m2']/datatest['covered/total'] $ del datatest['covered/total']
sentlex_analysis(df['text'][432673])
twitter_archive_clean['score'] = twitter_archive_clean['rating_numerator']/twitter_archive_clean['rating_denominator'] $ twitter_archive_clean.drop(columns=['rating_numerator','rating_denominator'],inplace=True)
df_new.loc[df_new['last_active'] <= 7 , 'score_activity_1'] = 5 $ df_new.loc[ (df_new['last_active'] > 8) & (df_new['last_active'] <= 15) , 'score_activity_1'] = 3 $ df_new.loc[ (df_new['last_active'] > 15) & (df_new['last_active'] <= 30) , 'score_activity_1'] = 1 $ df_res['score_activity'] = df_new['score_activity_1'] $
precip_df.describe()
tc_final = tc2.rename(columns={'ISBN RegEx':'ISBN'}) $ tc_final.to_csv(folder + "\\" + 'TC-all.txt', sep="\t", index=False)
(autos["date_crawled"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index().head() $ )
likes.drop(columns='actor')
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']]) $ results = mod.fit() $ results.summary()
train_source_emb = encoder_model.predict(source_proc.transform(source_docs))
prop= df['converted'].mean() $ prop
plt.figure(figsize=(12,8)) $ sns.violinplot(x=trump['source'], y= 'num_punc', data= trump) $ plt.xlabel(' Source of Tweet', fontsize=12) $ plt.ylabel('Amount of Punctuation', fontsize=12) $ plt.title('Amount of Punctuation per Tweet by Source', fontsize=15)
df_new["page_country"] = df_new["treatment"]*df_new["us"] $ df_new.head()
pp.barplot(df=df, filters={'technology': tecs, 'variable': ['CAP']}, $            title='CAP - light')
training_set, test_set = newdf[newdf['date']<split_date], newdf[newdf['date']>=split_date] $ training_set = training_set.drop('date', 1) $ test_set = test_set.drop('date', 1)
df['y'].plot.hist()
print(data[:5]) $ print(len(data)) $ print ('\n') $ print(tokenized_tweets[:5]) $ print(len(tokenized_tweets))
dfFull.BsmtFinSF1 = dfFull.BsmtFinSF1.fillna(dfFull.BsmtFinSF1.mean())
predicted_talks_vector = classifier.predict( ... )
df2.reindex_like(df1) #Padding Nan
np.exp(0.0099)
mean = ... $ print("The average length of the tweets: {}".format(mean))
top_songs.info()
trainheadlines = train["text"].values $ basicvectorizer = CountVectorizer() $ basictrain = basicvectorizer.fit_transform(trainheadlines) $ print(basictrain.shape)
X_train.shape
counts_by_campaign_date.loc[('Sport', '2018-03-18')]
df2.query("converted==1").shape[0]/df2.shape[0]
finals.to_csv("player_season_statistics.csv", index=False)
n_new = ab_file2[ab_file2['landing_page']=='new_page'].shape[0] $ print(n_new)
del archive_df_clean['retweeted_status_id']
obj = pd.Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])
len(B4JAN18)
tweet_json_clean['id'] = tweet_json_clean['id'].astype('str')
merged_feature_df.columns
print("The propotion of users converted are",sum(df['converted'])/df.shape[0])
adopted_cats['Y'].value_counts()
datetime.now()
ts.index
apply_cats(joined_test, joined)
df.loc['Alabama']
df1 = pd.merge(left=dfWQ_annual,right=dfQ1_annual,how='inner',left_index=True,right_index=True) $ df2 = pd.merge(left=dfWQ_annual,right=dfQ2_annual,how='inner',left_index=True,right_index=True) $ df3 = pd.merge(left=dfWQ_annual,right=dfQ3_annual,how='inner',left_index=True,right_index=True)
most_retweeted = grouped.iloc[:5,:] $ most_retweeted
sum(twitter_archive.tweet_id.duplicated())
merged_portfolio_sp_latest_YTD_sp = pd.merge(merged_portfolio_sp_latest_YTD, sp_500_adj_close_start $                                              , left_on='Start of Year', right_on='Date') $ merged_portfolio_sp_latest_YTD_sp.head()
description = pd.read_csv('description.txt', sep = '\n', header=None)
df_master_select = df_master_select.dropna() $ df_master_select.head()
df['rating_denominator'].value_counts()
df_goog.Open.resample('M').plot() $
s3_path = 's3://smapp-nyu/projects/ideology_estimation/congress/'
Logit_mod = sm.Logit(dfX['converted'],dfX[['intercept','ab_page']]) $ results = Logit_mod.fit() $
len(df_user[df_user['collection'] == 'us_media_accounts_2016'])
a,b=train,(train+validation) $ x,y=X[a:b],Y[a:b] $ scores_cv=model.evaluate(x,y) $ print ("\n%s: %.2f%%" %(model.metrics_names[1],scores_cv[1]*100))
street_conditions = df[df['Complaint Type'] == 'Street Condition'] $ street_conditions['Descriptor'].value_counts()
click_condition_meta.info()
treatment_converted = (df2.query('group == "treatment" and converted == 1').count()[0]) $ treatment_total = (df2.query('group == "treatment"').count()[0]) $ treatment_prob = float(treatment_converted) /  float(treatment_total) $ print ("Probability of Treatment Group converted is {}".format(treatment_prob))
pd.date_range(start, periods=5, freq=pd.tseries.offsets.BDay())
type(df.date[0]) $
promo_df.drop('onpromotion', 1, inplace=True)
hist(df1.tripduration, bins = 20, color = "Grey", label = "Total trips", normed = 1) $ plt.xlabel("Trip Duration in seconds", fontsize=12) $ plt.ylabel("Amount of trips", fontsize=12) $ plt.title("Trip Duration Histogram by gender", weight='bold', fontsize=14) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
lift = attack_client()
p_mean=np.mean([p_new, p_old]) $ new_page_converted= np.random.choice([1, 0], size=n_new, p=[p_mean, (1-p_mean)]) $ new_page_converted.mean()
comps_count = pd.DataFrame(comps.groupby('entity_uuid').size()).reset_index() $ comps_count.columns = ['entity_uuid','competitor_count']
process = CrawlerProcess({ $     'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'  $ }) $ process.crawl(XRateSpider) $ process.start()
result = tf.multiply(x1,x2) $ print(result)
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline $
data_out = eval_data.dropna().drop('tavg_norm', axis=1).to_dict(orient='records')
goo2.head()
type(xpdraft)
twitter_data.rating_denominator.value_counts()
countries = [] $ for i in places2.index: $     countries.append(places2[i]['country'])
old_page_converted = np.random.binomial(1, p_old, n_old) $ old_page_converted.mean()
mgxs_lib.load_from_statepoint(sp)
s.resample('Q', label='left', loffset='-1D').head()
snow.select("select count(distinct patient_id) from nk_albatross_psp where left(patient_id, 5) != 'XXX -'")
weblogs = spark.table("weblogs") $ weblogs.limit(10).toPandas()
prob = df2.converted.mean() $
words = ['stately', 'controlled', 'pulsing', 'channeling', 'sprectral', 'seamlessly', 'cracked', 'characteristic', 'sustained', 'passable', 'emphasizes', 'rollicking', 'fluttering', 'suck', 'insistent'] $ importance = [9.9, 8.5, 7.8, 6.6, 6.5, 6.2, 6.2, 6.1, 5.8, -5.7, 5.7, 5.7, 5.6, -5.5, 5.4]
twitter_df.sort_values("created_at", inplace=True) $ twitter_df.reset_index(drop=True, inplace=True) $ twitter_df.head(10)
testObj.buildOutDF(tst_lat_lon_df[0:600])
np.array(p_diffs) $ plt.hist(p_diffs); $ plt.xlabel('p_diffs') $ plt.ylabel('size of p_diffs') $ plt.title('Sampling distribution of difference in means') $
consumerKey = "XXXXXXX" $ consumerSecret ="XXXXXX"
print("Number of Groups in Mobile ATT&CK") $ groups = lift.get_all_mobile_groups() $ print(len(groups)) $ df = json_normalize(groups) $ df.reindex(['matrix', 'group', 'group_aliases', 'group_id', 'group_description'], axis=1)[0:5]
sns.barplot(x="MILLESIME", y="target", data=df)
td = td.fillna(0)
data=datasets.load_iris()
df_precep_dates_12mo.set_index('date')
import sentlex $ import sentlex.sentanalysis
from src.pipeline import pipeline_json $ pj = pipeline_json(s1) $
df, y, nas, mapper = proc_df(joined_samp, 'Sales', do_scale=True) $ yl = np.log(y)
df['converted'].mean()
columns = inspector.get_columns('Station') $ for c in columns: $     print(c['name'], c["type"])
acc.find(t_type='46')
pd.value_counts(merged1['Specialty'])
from bs4 import BeautifulSoup $ import requests $ import os
day = lambda x: datetime.date(int(x[0]),int(x[1]),int(x[2])).isocalendar()[1] $ emotion_big_df['date']=emotion_big_df['date'].apply(day)
autos.price.unique()[-10:]
paragraphs = tree.xpath('//p') $ for p in paragraphs: $     print(p.text)  
d = ddf.groupby('key').value.sum() $ %timeit d.compute()
df_new =df_country.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
from sklearn.model_selection import train_test_split $ x_train, x_test, y_train, y_test = train_test_split(xs, ys) $ y_train = y_train.ravel() $ y_test = y_test.ravel() $ print(len(x_train),len(x_test))
df['Duration'].plot.hist(xlim=(0,100)) # doesn't do what we want
train_df,test_df = getQuant(train_df,test_df,['latitude','longitude'],'gbm_quant_lat_long') $ fealist_quant_gbm = ['gbm_quant_lat_long']
def mean(x): $     return x.mean() $ mean_age_per_component = git_blame.groupby('component').age.agg([mean, 'count']) $ mean_age_per_component.head()
X = pd.merge(X, expAndCoinByUser, on="userid") $ X.head(5)
df_con=pd.concat([df_con, Xs], axis=1)
raw.head()
PredClass = ForecastModel('list_dol.sql', last_dt='2017-12-16')
nold = sum(df2['landing_page'] == 'old_page') $ nold
tweets_df.entities.describe()
Q3 = pd.DataFrame(avg_values, columns=['Average Temperature'])
notes.head()
USER_PLANS_df.loc[str(np.array(['e08b8a17', 'f6dd6544']))]
clean_users
!git clone https://github.com/RaulMedeiros/test_cpp_cuda_codes.git $ !ls test_cpp_cuda_codes
baseball.drop(['ibb','hbp'], axis=1)
new_page_converted = np.random.binomial(n_new,p_new)
df.groupby('converted')['user_id'].nunique()[1]/df.groupby('converted')['user_id'].nunique().sum()
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY $ r = requests.get(url)
df['timestamp'] = pd.to_datetime(df['timestamp']) $ df['EST'] = df['timestamp'] - pd.Timedelta(hours=5) #Convert to EST
X.drop('title', axis=1, inplace=True) $ X = pd.get_dummies(X, drop_first=True)
taxi_hourly_df.index = pd.DatetimeIndex(taxi_hourly_df.index)
from sklearn.neighbors import KNeighborsClassifier
log_mod_interact = sm.Logit(df_new_npage['converted'], df_new_npage[['intercept', 'US', 'UK']]) $ log_mod_interact_results = log_mod_interact.fit() $ log_mod_interact_results.summary()
StockData.head()
X_extra = Train_extra.drop('Approved', axis=1) $ y_extra = Train_extra['Approved'] $ kNN500.fit(X_extra, y_extra)
join_d.show(5)
cityID = 'ac88a4f17a51c7fc' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Portland.append(tweet) 
cats_df['number of vet visits'] = cats_df['number of vet visits'].apply(lambda x: np.nan if x < 0 else x) $ cats_df[cats_df['number of vet visits'].isnull()]
baseball.reindex(baseball.index[::-1]).head()
import pandas_datareader.data as dr $ from pandas_datareader import data $ import pandas as pd $ import datetime $
summed.fillna(method='pad', limit=1)  # No more than one padded NaN in a row
BroncosBillsPct = winpct.loc[winpct['Game Title Date'] == 'Broncos vs. Bills  2017-09-24'][['playId','text','homeWinPercentage']]
duplicated_user = df2[df2.user_id.duplicated()] $ duplicated_user
stats['total_commits'] = len(commit_df)
y_train.head()
np.exp(0.0408)
cats_df['date of last vet visit'] = cats_df['date of last vet visit'].apply(lambda c: pd.to_datetime(c, errors='coerce'))
house_data.describe()
pd.read_html?
station.head()
p_converted = df['converted'].mean() $ print('Proportion of converted users: ',p_converted)
shows.isnull().sum()
print pd.concat([s1, s2, s3], axis=1)
import tensorflow as tf $ sess=tf.Session()    $ saver = tf.train.import_meta_graph('/tmp/testing/stock_prediction_12_21/model.ckpt-1000.meta') $
from IPython.display import Image $ Image("http://www.dofactory.com/Images/sql-joins.png")
indices = np.argsort(vectorizer.idf_)[::-1] $ top_n = 100 $ top_features = [feature_names[i] for i in indices[:top_n]] $ print(top_features)
all_data.reindex(all_data.index[::-1])
feature_set = layer.query(out_sr={'wkid': 4326}) $
apply_cats(joined_test, joined)
PYR.head()
engine =create_engine("mysql://root:1234@localhost:3306/db?charset=utf8", encoding="utf-8")
for i in default.index: $     percentdonationoptional(i)
len(scr_churned_ix)
file = 'https://assets.datacamp.com/production/course_2023/datasets/tips.csv' $ tips = pd.read_csv(file) $ tips.info()
df.to_feather(f'{PATH}df')
count_non_null(geocoded_df, 'Judgment.Date')
m4 = m3.flatten()       #converting the array in 1-D $ m5 = np.sort(m4, axis = None)[::-1]      #arranging in descending order $ sorted_m3 = np.reshape(m5, (2, 4))      #reshaping it to 2-D $ print("sorted m3: ", sorted_m3)
df['MeanFlow_mps'] = df['MeanFlow_cfs'] * 0.028316847
bool(np.nan)
with open('tweet_json.txt', 'w') as outfile: $     for tweet_json in tweet_list: $         json.dump(tweet_json, outfile) $         outfile.write('\n') #add a newline character at the end of each json
twitter_archive_clean.describe()
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
data_issues.columns
seats_per_hour = seats_per_hour.reindex(taxi_weather_df.index)
image_clean.info()
pd.options.display.max_columns
7952784
print(type(r.json())) $ json_dict = r.json() $
df_tick = pd.read_pickle('tweet_data/amzn_ohlcv_may18.pkl') $ df_tick.set_index(df_tick.index.to_datetime(), inplace=True) $ df_tick.info()
print ("Results", len(result['hits']['hits'][0]) ) $ print ("Score = ", result['hits']['hits'][0]['_score'] ) $ print ("\nScore is the boolean model to find matching documents and the formula to computea a relevance met\n") $ result['hits']['hits'][0][u'_source'] $
Z = np.tile( np.array([[0,1],[1,0]]), (4,4)) $ print(Z)
matplotlib.pyplot.hist(y_pred)
df.to_csv('/Users/digitalmarketer1977/Desktop/620trumptweets.csv',index = False,encoding='utf-8')
numUsers = firstWeekUserMerged[['userid']].drop_duplicates().count() $ print("The number of users is :",int(numUsers))
text = nltk.Text(tokens) $ print(type(text)) $ pp.pprint(text[976:991]) $ pp.pprint(text.collocations())
print("The highest opening prices",df.Open.max()) $ print("The highest opening prices",df.Open.min())
df_l_s=df.sample(n=2000)
extract_deduped..groupby('application_month').size()
df1.head(5)
n_old=df2[df2['group'] == "control"].shape[0] $ print(n_old)
problems = pd.DataFrame(problems, columns=['srx', 'srr'])
token = token[["sender","date","receiver"]] $ token.index = range(len(token))
print(gs.best_estimator_)
df.shape[0]
dfjoined.sort_values(['count_type_day'], ascending = 0).head()#very cold days have heat complaints
joblib.dump(clf, 'gbc200th.pkl', compress=True)
df['2016-09']
model_arima121 = ARIMA(AAPL_array, (1,2,1)).fit() $ print(model_arima121.params)
data[data.density > 10]
airline_df['sentiment'] = airline_df['tweet'].apply(lambda tweet: NBClassifier.classify(extract_features(getFeatureVector(processTweet2(tweet))))) $
print [(i,v) for i, v in enumerate(simple_features)]
gMapAddrDat.set_statusMsgGrouping(12)
countries["country"].unique()
prices.drop(cols_to_drop, axis=1, inplace=True) $ prices.iloc[:, 0:10].head()
df_onc_no_metac[ls_other_columns].head()
session_v2 = session_feature_engineer(session_v1)
dr_mod_id_str = "ff_50_15p"
aliases
a = np.array([1, 2, 3, 4], dtype='float64')  #or np.array([1., 2., 3., 4.]) $ a.dtype
df_3 
m.lr_find()
df.groupby(['education'])['loan_status'].value_counts()
distance_list = [] $ for i in range(0, len_start_coord_list): $     distance_list.append(get_distance())
users
monthlyDF
station_count.loc[(station_count['Count'] >= 2000)]
df7 = pd.read_csv('2007.csv')
null_reg = creations[ creations["user_registration"].isnull() ] $ null_reg
giss_temp.Aug.interpolate().tail()
subwaydf.iloc[81762:81768] #this low number seems to be because entries and exits resets.
clf = svm.SVC(C = 1.0) $ clf.fit(trainDataVecs, train["rating"]) $ predictions = clf.predict(testDataVecs) $ output = pd.DataFrame(data={"id":test["id"], "rating":predictions}) $
model.add(Conv1D(filters=10, kernel_size=10, $                  activation='relu', padding='same')) $ model.add(MaxPooling1D(pool_size=10)) $ model.add(Dropout(rate=0.25))
df = df[df.BEDRM >= 2.0]
week39 = week38.rename(columns={273:'273'}) $ stocks = stocks.rename(columns={'Week 38':'Week 39','266':'273'}) $ week39 = pd.merge(stocks,week39,on=['273','Tickers']) $ week39.drop_duplicates(subset='Link',inplace=True)
dump["country"].value_counts() / dump["country"].value_counts().sum() $
twitter=pd.read_csv('twitter-archive-enhanced.csv')
def extract_year(s): $     return s[0:4]
word_freq_df = word_freq_df.reset_index()
cust_data.loc[:10,'ID']
loan_stats = loan_stats[no_missing_values_response] $ print('how many missing values do we have now:' , loan_stats['loan_status'].isna().sum())
rng = np.random.RandomState(23) $ sample_size = 50 $ rng.choice(sizes, sample_size, replace=True)
df = pd.DataFrame(np.array([ht,h0,cv]).T, columns = ['ht','h0','cv'], index = folder_names) $ dfPeDa = pd.DataFrame(list(df.apply(lambda x: list(PeDa(1,33,x.h0/1000)), axis = 1)), index = folder_names, columns = ['Pe','Da']) $ df = pd.concat([df,dfPeDa], axis = 1) $
th = table.find('th', text='Indebtedness:') $
del(StockData['Date-1st']) $ StockData = StockData.drop(['Date-DaysInMonth'], axis=1) $ StockData.drop(['Date-MonthStart', 'Date-MonthEnd'], axis=1, inplace=True)
a=['BLOCK','LOT','APT','BUILDING CLASS CATEGORY','CONDOLOT'] $ df[a][1:100]
latest_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()[0] $ format_latest_date = dt.strptime(latest_date,"%Y-%m-%d") $ format_latest_date # Display the date
df_final_edited['p1'].value_counts() $
gold_customers.show()
df.select(a / 100).show(5)
i_unique_user2 = df2.user_id.nunique() $ i_unique_user2
ac.describe()
!du -h train.csv
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=16000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
count_vect = CountVectorizer(vocabulary=significant_features, tokenizer=lambda x: x.split(',')) $
print len(df.City.unique())    $ print len(df.County.unique())
df
dummy_IntServ = pd.get_dummies(df['InternetService'], prefix='IntServ') $ dummy_Contract = pd.get_dummies(df['Contract'], prefix='Contract') $ dummy_PayMethod = pd.get_dummies(df['PaymentMethod'], prefix='PayMethod') $ dummy_Contract = pd.get_dummies(df['Contract'], prefix='Contract') $ print(dummy_IntServ.head()) $
model.fit(x_train, fb_train.popular)
many_bdays
aapl_opt.loc[(aapl_opt.Expiry=='2018-03-16') &(aapl_opt.Type=='call') & (aapl_opt.Strike==180)].JSON[638]
autos.head(1)
from sklearn.linear_model import LogisticRegression $ LR_model = LogisticRegression(C=0.01).fit(X_train,y_train) $ LR_model
Fields = ['aerospace', 'oil & gas', 'alternative energy', 'engine technology', 'construction', 'research', 'water systems', 'distributor', 'solutions provider', 'other', 'instrumentation manufacturer', 'valve manufacturer', 'material science', 'chemical', 'food', 'health']
df = pd.read_csv('data_test_2m.esc', sep='\3')
converted_control = df2.query('group == "control"')['converted'].mean() $ print("Given that an individual was in the control group, the probability they converted is{0: .4} ".format(converted_control))
m3.load('split_model_v1')
lammps_command = 'lmp_serial' $ mpi_command = None
doc = "Human computer interaction" $ vec_bow = dictionary.doc2bow(doc.lower().split()) $ vec_lsi = lsi[vec_bow] # convert the query to LSI space $ print(vec_lsi)
 df.apply(lambda x: sum(x.isnull()),axis=0) 
definition_details = client.repository.store_definition(filename_keras, model_definition_metadata)
dfleavetimes.head(5)
n_new = len(df2.loc[new_page]) $ n_new
df2.head()
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)])
train_data = gl.SFrame(submissions[submissions['solved']==1])
prec_pred.head()
df.groupby('Borough').count().sort(desc("count")).show(10)
pros_std_dev = utility_patents_subset_df.prosecution_period.std() $ pros_median = utility_patents_subset_df.prosecution_period.median() $ utility_patents_subset_df = utility_patents_subset_df[utility_patents_subset_df.prosecution_period <= (pros_median + 3*pros_std_dev)] $ sns.distplot(utility_patents_subset_df.prosecution_period, color="green") $ plt.show()
df.plot(column='cluster_labels', cmap='Reds', legend=True) $ plt.title("Clusters: K Means")
np.exp(result2.params)
data
df.nsmallest(1,'rating')
dfDay['Likelihood of Win'].unique()
Group=pd.get_dummies(df2['group'],drop_first=True)  #Forming dummy Variables and dropping the first Cloumn $ Group=Group.rename(columns={'treatment':'ab_page'})   $ df2['Intercept']=1 $ df2=pd.concat([df2,Group],axis=1)       #Joining by column $ df2.head(3)
result['uid'].notnull().sum()
from sklearn.ensemble import GradientBoostingClassifier
lesson_date = datetime(2017, 5, 25, 9, 38, 15, 844089)
for e in res: $     print e.text
crimes.columns = crimes.columns.str.replace('__', '_') $ crimes.columns
train, test = train_test_split(df, test_size=0.3)
bwd = df[['Store']+columns].sort_index().groupby("Store").rolling(7, min_periods=1).sum()
datatest.loc[datatest.place_name == "Buenos Aires Interior",'lat'] = -34.708663 $ datatest.loc[datatest.place_name == "Buenos Aires Interior",'lon'] = -58.973401
grinter1[grinter1.number_int > 50]
dtc = DecisionTreeClassifier(max_depth=3) $ cv_score = cross_val_score(dtc, features_class_norm, overdue_transf, scoring='roc_auc', cv=5)
cr_under_null = df2['converted'].mean()    # Under the null, use both groups. $ cr_under_null $
archive_clean[archive_clean['in_reply_to_status_id'].notnull()]
type(panel_data) $ panel_data.ix['Close'].head()
df2.drop([1899] , inplace=True )
features = df[list(feature_dict.keys())] $ feat_type = list(feature_dict.values()) $ feature_types = feat_type $ labels = df[label_name]
weather.shape
y_pred_class = nb.predict(X_test_dtm) $ from sklearn import metrics $ metrics.accuracy_score(y_test, y_pred_class)
predictions = rfModel.transform(testData)
twitter_archive_clean=twitter_archive_clean[~twitter_archive_clean.expanded_urls.isnull()]
autos['ad_created'].str[:7].value_counts().sort_index()
capa2017offshore = capa2017[capa2017['Production Type'] == 'Wind Offshore'] $ capa2017onshore = capa2017[capa2017['Production Type'] == 'Wind Onshore']
sc.stop()
mean_mileage = pd.Series(brand_mean_mileage).sort_values(ascending=False) $ mean_price = pd.Series(brand_mean_prices).sort_values(ascending=False)
strategy.trades(start_date = '2018-03-23', end_date = '2018-03-24')
list(tweets_df)
len(users) $ users.drop(users[users['friends_count'].isna()].index, inplace=True) $ len(users)
age.iloc[[1, 3, 0]]
y_test_over[rfc_bet_over].mean()
df2['intercept'] = 1 $ df2[['drop', 'ab_page']] = pd.get_dummies(df2['group']) $ df2.drop(['drop'], axis=1, inplace=True) $ df2.head()
sample.asfreq("H")
df2 = df1.groupby(['Destination', 'Protocol'], sort=False).sum()
df.loc['20180102':'20180104', ['B', 'C']]  # Selecting by label
log_reg_over.score(X_train, y_train_over)
autos = autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].value_counts(normalize=True).head(10)
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ output = round(act_diff, 4) $ output
len(station_tobs) $ print(" There are {} tobs within the data set".format(len(station_tobs)))
plots.proto_distribution(traffic_type = 0,scale = "log") #Normal traffic log scale
for row in selfharmm_topic_names_df.iloc[0]: $     print(row)
pickle_it(MNB, 'mnb_classifier') $ MNB = open_jar('mnb_classifier')
df_yt.head(2)
target_data = pd.read_csv('data/kaggle_data/train-targets.csv', sep=",") $ target_data.head(5)
archive_clean.info()
fig = plt.figure() $ ay = fig.add_subplot(111, projection='3d') $ ay.scatter(lat_27_1.values, lng_27_1.values, hour_27_1.values, c=day_27_1.values) $ plt.show()
Helper().what_is_on_the_fly_selector()
missing_values = missing_values_table(perf_train) $ missing_values.head(20)
event_list = pd.DataFrame() $ event_list['event_id'] = df_events.event_id $ event_list['event_start_at'] = df_events.event_start_at $ event_list
stock[['target', 'forecast']].sort_values('target').head() $
stats = prices.describe().transpose() $ cols_to_drop = list(stats[stats['count'] < min_days_traded].index) $ len(cols_to_drop)
with open('obj/specsUrl21062018.pkl', 'rb') as handle: $     specsUrl = pickle.load(handle)
class_merged_hol=pd.merge(class_merged_hol,regional_hol,on=['date','state'],how='left') $ print("Rows and columns:",class_merged_hol.shape) $ pd.DataFrame.head(class_merged_hol)
import pandas as pd $ data = pd.read_csv("data/stream_lunch.csv", parse_dates=['created_at'], index_col='created_at').sort_index() $ data.head()
a = criteria.values $ so.iloc[a].head()
fb_tokens
au.save_df(df_city, 'data/city-util/proc/city') $ au.save_df(df_util, 'data/city-util/proc/utility') $ au.save_df(misc_info, 'data/city-util/proc/misc_info')  # this routine works with Pandas Series as well
states_table = page_soup.find_all("table") $ states_table
sentdata_beginning.plot(kind='area')
pd = %sql select * from runtime.queries limit 1 $ pd.DataFrame().head()
print('Number of rows in the dataset is {}.'.format(df.shape[0]))
im_clean["p1"] = im_clean["p1"].str.lower() $ im_clean["p2"] = im_clean["p2"].str.lower() $ im_clean["p3"] = im_clean["p3"].str.lower()
news_list = []
print("regarding attrition:") $ print("Yes: " + str(ibm_hr.filter(ibm_hr['Attrition'] == "Yes").count())) $ print("No: " + str(ibm_hr.filter(ibm_hr['Attrition'] == "No").count()))
api_copy.info()
load_dotenv('.env')
df_never_moved.info() #remove Time
data["name"] = data["name"].str.replace("\d", "")
df = data[(data['Shop_Existence_Days']>=30) & (data['Shop_Status']=='disabled') | (data['Merchant_Plan_Offer']=='Start')] $ trial, start = df['Merchant_Plan_Offer'].value_counts() $ print('Interest merchant plan offer distribution: ') $ print('Trial: '+str(trial)) $ print('Start: '+str(start))
len(M7_pred),len(M7_actual),len(dfM.DATE[16:-12])
twitter_archive_enhanced.head()
sampleDF = tweetsDF
nvidia.take(1)
first_commit_timestamp = '2005-04-16 22:20:36' $ last_commit_timestamp = '2017-10-03 12:57:00' $ corrected_log = git_log[(git_log['timestamp'] >= first_commit_timestamp) & (git_log['timestamp'] <= last_commit_timestamp)] $ corrected_log.describe() $
pd.DataFrame(np.random.rand(3, 2), $              columns=['foo', 'bar'], $              index=['a', 'b', 'c'])
temp_df = df.copy() $ temp_df.index = df.index.set_names('Desc.', level='Description') $ temp_df.head(3)
gc.collect()
X_loadtest = Feature1 $ X_loadtest[0:5]
n_old = control.shape[0] $ print(n_old)
the_frame["time"] = the_frame.set_index("local_15min").index.tz_localize(pytz.timezone('America/Chicago'), ambiguous = True) $ the_frame["date"] = [ dt.datetime(d.year,d.month,d.day,0,0,0,0) for d in the_frame['time'] ] $
from google.colab import auth $ auth.authenticate_user()
df_ml_50_01.tail(5)
df_pageviews_mobile_web.head()
soup.find_all('div', class_='schedule-container')[0].select('.active')
q1_url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31?api_key="+API_KEY $ r1 = requests.get(q1_url) $ r1 = r1.json() $
speeches_df3.head()
from sklearn import svm $ clf = svm.LinearSVC() $ clf.fit(X_train, Y_train) $ print (clf) $ print(clf.score(X_test, Y_test))
pd.Series(np.random.randn(1000)).plot(kind='hist', bins=20, color='Y');
grouped = df_cod2.groupby(["Death year", "Cause of death"]) $ grouped.size()
response = requests.get("http://api.open-notify.org/iss-now.json") $ pd.read_json(response.content)
ratings.count()
turnaround_planes=get_turnaround_planes(planevisits, start_t, end_t, end_t1)
n_new = df2[df2.group == "treatment"].count()[0] $ print("The population of user under treatment group: %d" %n_new)
print(atoms)
vol = vol.groupby(vol.index.day).transform(lambda x: x/x.sum()) $ vol.head()
price_series = pd.Series(branding_price)
plt.savefig('average_scores.png')                                            # saves plot to file
result['timestampDate'] = result['timestampUTC'].dt.date
count_by_insertid = Counter(df.insert_id) $ insertid_freq = {x : count_by_insertid[x] for x in count_by_insertid if count_by_insertid[x] > 1 } $ print('Duplicated insert_ids: {}'.format(len(insertid_freq.keys())))
ben_final['revtime'] = pd.to_datetime(ben_final['revtime'])
run_augmented_Dickey_Fuller_test(series=dr_num_new_patients, num_diffs=2) $ run_augmented_Dickey_Fuller_test(series=dr_num_existing_patients, num_diffs=2)
options_frame = pandas.read_pickle('options_frame.pickle')
margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name="margin_loss")
trading.coint_test()
ind = pd.read_csv('170418_3_cbg_w_foursquare_prep.csv')
df["days_to_pickup"] = (df["pickup"] - df["created_at"]).dt.total_seconds() / 86400 $ df["trip_duration"] = (df["dropoff"] - df["pickup"]).dt.total_seconds() / 86400
states = pd.read_csv('in/state_table.csv') $ states.rename(columns={'abbreviation': 'state'}, inplace=True)
contractor_clean=contractor_clean[contractor_clean['contractor_id'].isin([139,140,228,236,238]) & $     contractor_clean['contractor_version']!=1 ] $ contractor_clean=contractor_clean.loc[~contractor_clean.contractor_id.isin([373,374,378])]
df_new[['CA','UK','US']] = pd.get_dummies(df_new['country']) $ df_new.drop(columns = ['UK'], axis = 1, inplace = True) $ lm = sm.Logit(df_new['converted'], df_new[['US', 'CA', 'intercept']]) $ results = lm.fit() $ results.summary()
twitter_archive_df_clean.drop(['doggo', 'puppo', 'pupper', 'floofer'], axis=1, inplace=True)
binary_sensors_list = [entity[0] for entity in entity_id_list if entity[0].split('.')[0] == 'binary_sensor'] # Print only the sensors $ binary_sensors_list
lbl = preprocessing.LabelEncoder() $ lbl.fit(list(drace_df['manager_id'].values)) $ drace_df['manager_id'] = lbl.transform(list(drace_df['manager_id'].values))
df_user[df_user['user.name'] == 'Marco Rubio']
df['converted'].mean() $
fuel_mgxs = mgxs_lib.get_mgxs(fuel_cell, 'nu-fission')
for key, grp in raw_data.groupby('objective'): $     print "{0: <20}{1}".format(key, percent_charged(grp.charged.values))
eval_RGF_tfidf_tts = clf_RGF_tfidf.score(X_testcv_tfidf, y_testcv_tfidf) $ print(eval_RGF_tfidf_tts)
items
soup.prettify
import statsmodels.api as sm $ convert_old = len(df2.query("landing_page == 'old_page' and converted == True")) $ convert_new = len(df2.query("landing_page == 'new_page' and converted == True")) $ n_old = len(df2.query("landing_page == 'old_page'")) $ n_new = len(df2.query("landing_page == 'new_page'")) $
result.summary()
autos["ad_created"].str[:10].value_counts(normalize = True, dropna = False).sort_index(ascending = True)
sample_fractions = filtered_active_sample_sizes.withColumn( $     "sample_fraction", $     (filtered_active_sample_sizes.sample_size_1+0.5) / filtered_active_sample_sizes.author_count)
data_full.head()
from sklearn.feature_extraction.text import CountVectorizer $ cvec = CountVectorizer() $ cvec.fit(my_df.text)
len(pd.unique(f['user_id']))
X, y = make_blobs(n_samples=5000, centers=[[4,4], [-2, -1], [2, -3], [1, 1]], cluster_std=0.9)
%matplotlib inline $ import matplotlib.pyplot as plt
df[(df['group'] != 'treatment') & (df['landing_page'] != 'new_page')].sum()
df_archive["source"].value_counts()
df2['intercept'] = 1 $ df2[['intercept','ab_page']]= pd.get_dummies(df2['group']) $ df2.head()
random_non_crashes_df.shape
dates = ['150905', '150912'] $ df = pd.DataFrame() $ for date in dates: $     df = df.append(pd.read_csv("turnstile_{0}.txt".format(date)), ignore_index = True) $
day_of_year15 = uber_15["day_of_year"].value_counts().sort_index().to_frame() $ day_of_year15.head()
np.exp(results.params)
user_actions = get_action_counts(session,top_actions).reset_index() $ user_actions.head()
all_sets.cards["XLN"].shape
print np.arcsin(1./2)*180./np.pi $ print np.sin(35*np.pi/180.)
import seaborn as sns
df.T
drawPoliGraph(multiG,0.01)
gis = arcgis.gis.GIS(os.environ['ESRIFEDERAL_URL'], username="Anieto_esrifederal")
X_new = model.transform(X_tfidf)
drugTree = DecisionTreeClassifier(criterion="entropy", max_depth = 4) $ drugTree # it shows the default parameters
departureZip = departureZip[['Unnamed: 0', 'ZIPCODE']] $ departureZip.rename(columns={'ZIPCODE':'zip_depart'}, inplace=True) $ departureZip.head()
adds.to_csv(folder + "\\" + law_adds_file, sep="\t", index = False)
pred = session.run(Ypred, feed_dict={X:x_data}) $ plt.plot(x_data, pred) $ plt.plot(x_data, y_data, 'ro') $ plt.xlabel("# Chirps per 15 sec") $ plt.ylabel("Temp in Farenhiet") $
atloc_opp_loc_count_prop_overall = compute_count_prop_overall(atloc_opp_loc, 'remappedResponses') $ atloc_opp_loc_count_prop_overall
train = df[df.index < '2017-01-01'] $ test = df[df.index > '2016-12-31']
ux.expand(urls[0])
store[['StoreType_a', 'StoreType_b', 'StoreType_c', 'StoreType_d']] = pd.get_dummies(store['StoreType']) $ store[['Assortment_a', 'Assortment_b', 'Assortment_c']] = pd.get_dummies(store['Assortment']) $ store.drop('StoreType', axis=1, inplace=True) $ store.drop('Assortment', axis=1, inplace=True)
openLastOnly = pd.merge(legLastOnly, openHouse.reset_index(), how='left', on=['last_name'])
weedmaps = 'https://weedmaps.com/deliveries/westsideorganic/menu_items.json' $ df = pd.read_json(weedmaps) $ df
from sklearn.linear_model import Lasso
store_items.interpolate(method = 'linear', axis = 1)
import calendar $ list(calendar.day_name)
most_informative_features_top_and_bottom(vectorizer=vectorizer, classifier=clf, binary=True, n=15)
df.drop(["city","lat","lon"], inplace = True, axis = 1)
prob=df2['converted'].mean() $ print("Probability of an individual converting regardless of the page they receive is "+str(prob))
mentioned_bills_all['last_vote'] = pd.to_datetime(mentioned_bills_all['last_vote'], format = '%Y-%m-%d') $ mentioned_bills_all['votes_date'] = pd.to_datetime(mentioned_bills_all['votes_date'], format = '%Y-%m-%d')
data_repo = "/home/julien/src/DSC_HOME_SERVICE/data/" $ import_params = {'sep': '|', 'encoding': 'latin-1'}
health_data.loc[:, ('Bob', 'HR')]
data.show()
my_data.shape $
df_users_products=pd.merge(left=df_user_product_ids,right=transactions,how="left",left_on=["UserID","ProductID"],right_on=["UserID","ProductID"])[["UserID","ProductID","Quantity"]]
twitter_df.head()
train_view.sort_values(by=1, ascending=False)[0:10]
tweet_image = pd.read_csv('image_predictions/image-predictions.tsv', sep='\t')
txns.head()
posGroups = list(pos_tweets.group_id_x) $ num_convos = len(set(posGroups)) $ print(f'Working with {num_convos} conversations') $ companyPos = filtered[filtered.group_id.isin(posGroups)]
df["Dif 50-day"] = df["50-day moving average"] - df["Closing Price"]  $ df["Dif 50-day"].min()  $ df[df["Dif 50-day"] <=0].head(3) $ df[df["Dif 50-day"] <=0].shape
churned_ordered = ordered_df.loc[churned_ord]
from sklearn.linear_model import LogisticRegression $ from sklearn.metrics import confusion_matrix $
data.tail(3)
tweetsS17 = pd.read_sql_query("SELECT created_at, extracted FROM tweets_info;", connS17, parse_dates=['created_at'] ) $ tweetsS17['created_at'] = tweetsS17['created_at'].dt.tz_localize("UTC").dt.tz_convert("Europe/Berlin") $ print("Number of Tweets: %s" %len(tweetsS17)) $ tweetsS17.head()
new_page_converted / nnew - old_page_converted / nold
collected_clientid_reasons = clients_with_dupes.collect()
poptweets = dict(df[['text', 'retweetCount']].groupby('text')['retweetCount'].sum()) $ poptweets = pd.DataFrame(list(poptweets.items()), columns=['text', 'Popularity']).sort_values('Popularity', ascending=False)[0:20] $ poptweets.to_csv('poptweets.csv', index=False)
new_page_converted = np.random.choice([0,1], n_new, p=[1-pnew,pnew]) $ new_page_converted.mean()
oil_nulls_after_interpolation=pd.DataFrame(oil_interpolation[pd.isnull(oil_interpolation.dcoilwtico)]) $ pd.DataFrame.head(oil_nulls_after_interpolation)
print(len(df_DRGs.drg_description.unique())) $ print('So all DRGs in df_DRGs are unique')
tm_2040 /= 1000 $ tm_2040_norm = tm_2040 ** (10/11) $ tm_2040_norm = tm_2040_norm.round(1) $ tm_2040_alpha = tm_2040 ** (1/3) $ tm_2040_alpha = tm_2040_alpha / tm_2040_alpha.max().max()
learner.save('lm5')
scoring_data = {'values': [x1.tolist(), x2.tolist()]}
autos["registration_year"].describe()
temperature_df = pd.DataFrame(temperature, columns=['date', 'tobs']) $ temperature_df.set_index('date', inplace=True) $ temperature_df.head()
df2.drop_duplicates(subset='user_id', keep="last", inplace=True) $
X = preprocessing.StandardScaler().fit(X).transform(X.astype(float)) $ X[0:5]
data_df[['clean_desc','tone']][data_df.tone == 'neutral'][2:7]
store_items.interpolate(method='linear', axis=0)
data_df
pivot_ui(df_users_6_after)
automl.leaderboard
soup.find_all('a', 'tltle')
import sys $ reload(sys) $ sys.setdefaultencoding('utf-8')
datatest.info()
twitter_df_clean.dtypes
obs_diff = prob_treatment - prob_control $ (p_diffs > obs_diff).mean()
kNN5 = KNeighborsClassifier()
pd.Series(sales, index=index)
df4=df_new.copy()
lm=smf.ols('tripduration ~ gender',data=df1).fit() $ lm.summary()
pickle_full = "Data Files/urinal-data-28-nov_clean.p"
results = pd.read_csv("../01-data/processed/curieuzeneuzen-results.csv")
df2 = df2.drop(df.index[[2893]]) $ df2.shape
from sklearn.metrics import accuracy_score $ y_pred = lin_clf.predict(X_train) $ accuracy_score(y_train, y_pred)
query2 = feature_layer.query(where="POP2010 > 1000000") $ query2.sdf
stats = pd.DataFrame([i[1:] for i in summary.tables[1].data[1:]], $                      columns=summary.tables[1].data[0][1:], $                      index=[i[0] for i in summary.tables[1].data[1:]]) $ stats = stats[stats.columns].astype(float) $ stats
from sklearn.preprocessing import MultiLabelBinarizer
candidate_gender = pd.Series(gender_top.candidate_id, index=['Female', 'Male', 'Unknown'], name='Gender in Company') $ plt.rcParams['patch.edgecolor'] = 'white' $ candidate_gender.plot.pie(figsize=(6, 6), colors=['lightskyblue', 'peru', 'skyblue'], autopct='%.2f%%')
df_ab.converted.sum()/df_ab.count()[0]
import pyspark.sql.functions as F $ flight2.filter(flight2.price <= 0).count() / flight2.count() $ flight2 = flight2.filter(flight2.price > 0)
print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
well_data.head()
ab_page = pd.get_dummies(df2['group']) $ df2 = df2.join(ab_page) $ df2.drop(['control'],axis=1,inplace=True) $ df2.rename(columns={'treatment': 'ab_test'}, inplace=True) $ df2.head() $
df2.drop(labels = 1899, axis=0, inplace=True)
z_values, _ = create_latent(nn_aae.nn_enc, test_loader) $ print(z_values[:,0].mean(), z_values[:,0].std()) $ print(z_values[:,1].mean(), z_values[:,1].std()) $ recon_x = create_sample(nn_aae.nn_dec, z_values) $ plot_mnist_sample(recon_x)
df_goog.Open.asfreq('D',method='backfill')
top_supporters.apply(combine_names, axis=1)
df.target.value_counts() $
dfgts.head()
PCA(2).fit(X)
submission = reddit.submission(url='https://www.reddit.com/r/funny/comments/3g1jfi/buttons/') $ submission.comment_sort = 'new'   #sort from newest $ submission.comments.replace_more(limit=None)
major_cities_l1_features[0].geometry
import pandas as pd $ import numpy as np $ import seaborn as sns $ import matplotlib.pyplot as plt $ %matplotlib inline
y = op_ed_articles['byline'] $ X = op_ed_articles.drop(['byline','subjects'],axis=1) $ from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.25) $
test.to_pickle('../data/merged_data/test.pkl')
journalists_mention_summary_df[['mention_count']].describe()
joined_test['Sales']=pred_test
df.loc[df['edition']=='Global News Service'].head()
atloc_opp_loc_tabledata = atloc_opp_loc_count_prop_byloc.reset_index() $ create_study_table(atloc_opp_loc_tabledata, 'locationType', 'remappedResponses', $                    location_remapping, atloc_response_list)
iris_data_path = "http://h2o-public-test-data.s3.amazonaws.com/smalldata/iris/iris.csv" # load demonstration data
full['DaysSinceAdmission'] = full[full['Readmitted'] == 1].groupby(['Patient']).diff()['AdmitDate']
result1 = (df1 < 0.5) & (df2 < 0.5) | (df3 < df4) $ result2 = pd.eval('(df1 < 0.5) & (df2 < 0.5) | (df3 < df4)') $ np.allclose(result1, result2)
bp.rename(columns ={"value1num":"systolic", "value2num" :"diastolic"}, $          inplace=True)
df2.all(axis=1)
events = pd.get_dummies(weather.events)
new_article_project = article_project.drop_duplicates(subset=['article_id', 'project_id'], keep='first') $ new_article_project.shape
sns.boxplot(x='rating', y='text length', data=dataset) $
now = datetime.now() $ now.hour, now.year
o_diff = df2.query('group=="treatment"').converted.mean() - df2.query('group=="control"').converted.mean() $ (p_diffs > o_diff).mean()
del people["body_mass_index"] $ people
image_predictions.info()
jail_census.groupby('Gender')['Age at Booking'].mean()
!open resources/html_table_marsfacts.html
print(bus.head(5)) $ print(ins.head(5)) $ print(vio.head(5)) $ print(q1e_answer)
df = pd.read_sql_query("SELECT * FROM Indiana.sesiones_page_screen_w_categL1 where platform = 'iOS' and date >='20170701' and date <= '20170712' and page = '/VIP/ITEM/MAIN/' and site = 'MLA' and site = 'MLA'limit 100", cnx) $ df.tail(10)
df['Updated Created diff'] = (df['Updated At'] - df['Created At']).astype('timedelta64[h]') $ df['Updated Shipped diff'] = (df['Updated At'] - df['Shipped At']).astype('timedelta64[h]') $ df['Shipped Created diff'] = (df['Shipped At'] - df['Created At']).astype('timedelta64[h]')
example1 = BeautifulSoup(train["review"][0])
logit_mod2 = sm.Logit(df_new.converted, df_new[['intercept', 'CA', 'US', 'new_page']]) $ result2 = logit_mod2.fit() $ result2.summary()
treatment_group_user_count = df2[df2['group'] == 'treatment']['user_id'].count() $ converted_treatment_user_count = df2[(df2['group'] == 'treatment') & (df2['converted'] == True)]['user_id'].count() $ p_treatment_converted = converted_treatment_user_count / treatment_group_user_count $
pd.read_csv("../data/microbiome/microbiome.csv", skiprows=[3,4,6]).head()
dates = [pd.Timestamp('2012-05-01'), pd.Timestamp('2012-05-02'), pd.Timestamp('2012-05-03')]
score_0 = score[score["score"] == 0] $ score_0.shape[0]
df = df[pd.isnull(df['retweeted_status_id'])] $ df.shape[0]
import xlrd
(df2.query("group == 'control'")['converted'] == 1).mean()
df_EMR = df_EMR.drop(columns=ls_wrong_dates)
amps = measures / (2 * ureg.ohm)  # I = V/R $ amps.dimensionality
print("max:",time_series.index.min(), "min:",time_series.index.max()) $
autos[["odometer_km", "price_euro"]].head()
sq7= "CREATE TEMPORARY TABLE  time_11 ( SELECT * FROM NBA_tweets where TweetCreated >='2018-04-16 16:50:00' and   TweetCreated <'2018-04-16 19:00:00' )" $ sq71="SELECT word, COUNT(*) total FROM ( SELECT DISTINCT Num, SUBSTRING_INDEX(SUBSTRING_INDEX(TweetText,' ',i+1),' ',-1) word FROM time_11, ints) x where word like'%#%'GROUP BY word HAVING COUNT(*) > 1 ORDER BY total DESC, word;"
dfs['Stock First Difference'] = dfs['Close'] - dfs['Close'].shift(1)
response_cpi_all = requests.get(url_cpi_all)
result_set =session.query(Adultdb).filter(Adultdb.education.in_(['Masters', '11th'])).all()
CoolPlot(df,'dayofweek','terms')
df2_treatment = df2[df2['group'] == 'treatment']['converted'].mean() $ print('The probability that an individual in the treatment group converted is: {}'.format(round(df2_treatment, 4)))
tb.loc['All'] # column total, return a Series
fit = log_model.fit()
spp_plot.set_index('term', inplace=True) $ ved_plot.set_index('term', inplace=True) $ vhd_plot.set_index('term', inplace=True) $ vwg_plot.set_index('term', inplace=True)
len(enrollments)
tags['Count'].agg(['count', np.mean, np.std, np.median])
baseball_newind[baseball_newind.ab>500]
df_estimates_true = df_estimates_true[df_estimates_true['release_id']==122711] $ print(df_estimates_true)
url = "www.nytimes.com/2015/09/11/us/politics/looking-to-score-with-republican-debate-viewers-not-floor-donald-trump.html?_r=0"
logodds[logodds.index.str.contains('you')]
df['Cat'] = pd.cut(df.Size, bins=[0, 1024, 1024*1024, 40*1024*1024, 1.6e+09], $               labels=['zeros', 'small', 'moderate', 'oversized'])
cityID = '3b77caf94bfc81fe' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Los_Angeles.append(tweet) 
sp500.ix[[10, 200, 450]]
print(''.join(open("vader_lexicon.txt").readlines()[:10]))
model = gensim.models.Word2Vec(sentences, min_count=10)
contribs = pd.read_csv("http://www.firstpythonnotebook.org/_static/contributions.csv")
predictions = final_pipeline.predict_proba(df_target.text.values) $ df_target['BlockChainPredictions'] = [j for i,j in predictions] $
pnew = df2[df2['landing_page']=='new_page']['converted'].mean() $ print("the convert rate for  pnew =", pnew)
page_soup.title.string
d.groupby(d['pasttweets'].str.len())['tweet_id'].count()
df.iloc[-1].name
ax = plt.subplot(111) $ ax.bar(date_frequency.index, date_frequency.data) $ ax.xaxis_date() $ plt.show()
new = df2.query('landing_page == "new_page"').converted.mean() $ old = df2.query('landing_page == "old_page"').converted.mean() $ differ = new - old $ (p_diffs > differ).mean()
df2[df2['user_id'].duplicated()].user_id
station = session.query(Station).count() $ station
df_new[['UK', 'US', 'CA']] = pd.get_dummies(df_new['country']) $ lm = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'UK']]) $ results = lm.fit() $ results.summary()
top_20_breed_stats['favorite_count'].sort_values(ascending=False).plot(kind='bar', subplots=True)
data = df.copy()
from sklearn.metrics import mean_squared_error $ print("R^2: {}".format(logreg.score(X_test, y_test))) $ rmse = np.sqrt(mean_squared_error(y_test,y_pred_lgr)) $ print("Root Mean Squared Error: {}".format(rmse))
appleNegs.shape
df3=pd.read_csv('countries.csv') $ print(df3['country'].unique())
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) $ import matplotlib.pyplot as plt # Plotting $ import numpy as np $ import re # regular expressions --> Finding Postal Code $
df.head()
weather.head()
df.head()
extract_all.loc[(extract_all.APPLICATION_DATE_short>=datetime.date(2018,6,1)) $                &(extract_all.APP_PRODUCT_TYPE.isin(['TL','RL'])) $                &(extract_all.CLA_RiskScore==1)].groupby('app_branch_state').size()
sample = msft_cum_ret[1:3] $ sample
obs_diff = c_trt-c_ctrl $ obs_diff
min_date, max_date = session.query( $ func.min(Measurement.date), $ func.max(Measurement.date)).first() $ print(min_date) $ print(max_date)
merged_data['overdue'].value_counts()
train.tail()
df['col1'].map(lambda x:x*100)
df.text[175]
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative = 'larger') $ 'Z-score: {}, p-value: {}'.format(z_score, p_value)
week7 = week6.rename(columns={49:'49'}) $ stocks = stocks.rename(columns={'Week 6':'Week 7','42':'49'}) $ week7 = pd.merge(stocks,week7,on=['49','Tickers']) $ week7.drop_duplicates(subset='Link',inplace=True)
sns.distplot(p_diffs) $ plt.title('Simulation of 10,000  p_new-p_old') $ plt.xlabel('p_diffs') $ plt.ylabel('Frecuency')
ts = pd.Series(np.random.randint(0,500,len(rng)), index=rng) $ ts5min = ts.resample('20S') $ print(ts5min,'\n')  # Resampler object $ print(ts5min.groups,'\n') # dict $ print(ts5min.sum(), '\n') # series of sum() to each 1min bin $
with open('glove.twitter.27B.200d.txt', "rb") as lines: $     w2v = {line.split()[0]: $                       np.array(map(float, line.split()[1:])) $                for line in lines}
p_diffs= np.array(p_diffs) $ p_observed= (prob_t-prob_c) $ p_observed= np.array(p_observed)
del twitter_df_clean['timestamp']
print(cross_val_score(forest_model,X_train,y_train,cv=3,scoring='recall'))
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs) $ plt.axvline(x=obs_diff, color='green'); $
sqlContext.sql("select person,count from pcs where count >50 order by count desc").show()
p_tnl = therm_abs_rate / (therm_abs_rate + thermal_leak) $ p_tnl.get_pandas_dataframe()
df = pd.merge(train,user_logs, on = 'msno', how = 'left') $ df.sort_values(by = ['msno','date'], inplace = True)
pd.read_pickle('data/wx/tmy3/proc/703950.pkl', compression='bz2').head()
categorised_df.head(5)
import pandas_datareader.data as web
df.info() $ total_rows = df.shape[0] $ print("Number of rows are: {}".format(total_rows))
crimes_by_type.sort_values("Offense_count",ascending=False)
df2 = gbq.read_gbq(querry, project_id = project_id, dialect = "standard") $ df2.head()
Test.MakeConnection(UnLockedPort)
df = df.drop('log_price', axis=1)
print ("Number of unique users:",df.nunique()['user_id'])
output_notebook()
%%time $ pd.to_datetime(df_10000['Created Date'], format="%m/%d/%Y %I:%M:%S %p")
df3 = tier1_df.reset_index() $ df3 = df3.rename(columns={'Date':'ds', 'Incidents':'y'}) $ df_orig = df3['y'].to_frame() $ df_orig.index = df3['ds'] $ n = np.int(df_orig.count())
from scipy.stats import norm $ norm.cdf(z_score) # Tells us how significant our z-score is
X = X.rename(columns = {'C(occupation)[T.2.0]':'occ_2','C(occupation)[T.3.0]':'occ_3','C(occupation)[T.4.0]':'occ_4','C(occupation)[T.5.0]':'occ_5','C(occupation)[T.6.0]':'occ_6','C(occupation_husb)[T.2.0]':'occ_husb_2','C(occupation_husb)[T.3.0]':'occ_husb_3','C(occupation_husb)[T.4.0]':'occ_husb_4','C(occupation_husb)[T.5.0]':'occ_husb_5','C(occupation_husb)[T.6.0]':'occ_husb_6'})
writer = pd.ExcelWriter('NaN_table.xlsx') $ for res_key, df in nan_sets.items(): $     df.to_excel(writer, res_key) $ writer.save()
print(df2['user_id'].nunique())
df['intercept'] =1 $ df[['control', 'treatment']] = pd.get_dummies(df['group'])
df.isnull().values.any()
df.dropna(inplace=True) $ df['County Number'] = df['County Number'].astype(int) $ df['Category'] = df['Category'].astype(int)
import sqlite3
questions['VIP_YES'] = questions['vip'].map({'Yes':1, 'No':0}) $ questions['VIP_YES'].value_counts()
rmse_ebay
print(global_mean) $ app_ver_map.head() $ app_ver = joined_df.groupby(['id'])['initial_app_version'].first().to_frame() $ app_ver = app_ver.applymap(lambda x: app_ver_map[x] if x in app_ver_map.index else global_mean) $ app_ver.head()
df2_copy.head()
news_df = news_df.loc[news_df['topic'] != 'commentisfree',:]
fashion.text[0]
tweets_clean.doggo.value_counts()
token_sendreceiveCnt = token_sendreceiveCnt.fillna(0)
git_blame.sha = pd.Categorical(git_blame.sha) $ git_blame.path = pd.Categorical(git_blame.path) $ git_blame.author = pd.Categorical(git_blame.author) $ git_blame.info(memory_usage='deep')
print sqlContext.sql(query).toPandas()
HYB, STD = set(HYB_customer_order_intervals), set(STD_customer_order_intervals) $ for customerID in HYB.intersection(STD): $     print(customerID)
results = model_selection.cross_val_score(gnb, X_test, Y_test, cv=kfold) $ results.mean()
plt.hist(taxiData.Trip_distance, bins = 100, range = [7, 10])
new_df = pd.DataFrame(tweet_archive_clean.loc[tweet_archive_clean['new'].apply(find_dot) == True].new.str.split('/').tolist(), columns=['rating_numerator', 'rating_denominator'])
soup('img')
def remove_no_reaction(df): $     tmp = df[(df['highest_reaction'].notnull())] $     return tmp[tmp['fb_total_reactions'] >= 200] $ dataset_test = remove_no_reaction(dataset_test) $ dataset_test.loc[:,"highest_reaction"] = dataset_test.loc[:,"highest_reaction"].astype(int)
dfn = df.loc[df.period].copy() $ dfo = df.loc[~df.period].copy() $ dfn.shape, dfo.shape
autos = autos[autos["price"].between(1, 350000)]
lmscore.pvalues[lmscore.pvalues >0.05]
auto_new.CarYear.value_counts()
df4  = df[df['lambda'] == 0.09] $ df4.shape
pa_max_date = prcp_analysis_df["date"].max().date() $ pa_today = dt.date.today() $ pa_min_date = (pa_max_date - dt.timedelta(days=365)) $ print("Date Range: "+str(pa_min_date)+" to "+str(pa_max_date))
bike_events = pd.DatetimeIndex(['2010-05-21', '2011-05-20', '2012-05-18', '2013-05-17', '2014-05-16', '2015-05-15', '2016-05-20', '2017-05-19']) $ bike_events
len(itos), counter
twitter_archive_master[twitter_archive_master.img_num != 1].head()
stem_porter.concordance('damn')
p_old = df2.converted.sum()/len(df2) $ p_old
inactive_count = len(clean_users[clean_users['active']==0].dropna(how='any')) $ inactive_mean = clean_users[clean_users['active']==0].dropna(how='any')['account_life'].dt.days.mean() $ inactive_sd = clean_users[clean_users['active']==0].dropna(how='any')['account_life'].dt.days.std()
from textblob import TextBlob
test_ind["Pred_state_XGB"] = xgb_model.predict(test_ind[features]) $ train_ind["Pred_state_XGB"] = xgb_model.predict(train_ind[features]) $ kick_projects_ip["Pred_state_XGB"] = xgb_model.predict(kick_projects_ip_scaled_ftrs)
def next_weekday(d): $     day_of_week = d.weekday() $     if day_of_week !=0: $         d = d + datetime.timedelta(7 - day_of_week) $     return d.strftime('%Y-%m-%d')
autos.loc[autos["registration_month"] == 0, :].describe(include='all')
education.reset_index(drop=True, inplace=True)
temps_df.iloc[1]
dfDay = df.copy(deep=True)
df2 = df2.drop(dup_user.index) $ df2[df2['user_id'].duplicated()]
auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) $ auth.set_access_token(ACCESS_TOKEN, TOKEN_SECRET) $ api = tweepy.API(auth)
stopWords = set(stopwords.words('english')) | set(stopwords.words('spanish'))
k = pd.concat([j, c], axis=1, join_axes=[a.index])
df.query('converted == 1').nunique()['user_id']/unique_user
logging.basicConfig(level=logging.INFO) $ logger = logging.getLogger(__name__) $ home_dir = '/Users/mtaylor/Desktop' $ log_file_name = home_dir + '/Haiku_Home_Forecast.log'
location = loc_date_str.split(',')[0].strip() $ location
click_condition_meta.reset_index(inplace = True, drop = True) $ click_condition_meta.info()
f = open("json_example.json","r") $ print(json.load(f)) $ f.close()
parameter = '00060' $ Shoal_Ck_15min = pull_nwis_data(parameter=parameter, site_number='08156800', start_date='2018-02-01', end_date='2018-02-18', site_name='08156800') $ Shoal_Ck_15min.head(10)
six_months_ago = pd.Timestamp('now') - pd.DateOffset(months=6) $ six_months_ago
full['LOS'].hist(bins=15,figsize=(12,4)) $ plt.title('Distribution of length of stay',size=15) $ plt.xlabel('days') $ plt.ylabel('count')
dfs
trip_result = pd.DataFrame(normals, columns=["tmin","tavg","tmax"]) $ trip_result["date"] = date_range $ trip_result.set_index("date", inplace=True) $ trip_result.head()
df_ml_692 = df.copy() $ df_ml_692.index.rename('date', inplace=True) $ df_ml_692_01=df_ml_692.copy()
df2['user_id'].value_counts() > 1
df2[(((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page'))) == False].shape[0]
from statsmodels.stats.diagnostic import acorr_ljungbox $
data_overdue_dur = (data['last_payment_date'] - data['due']).astype('timedelta64[D]') $ data_overdue_dur[data_overdue_dur > 0].hist(bins=200, figsize=(20, 10))
adv_stats.head()
print ("Jaccard Similarity Score", jaccard_similarity_score(y_test, yhat)) $ print("F1-score Accuracy: ",  metrics.f1_score(y_test, yhat, average='weighted')) 
del c_df['Date']
typesub2017.dtypes
data = data.dropna() $ data.info()
df = pd.merge(df, tran_time_diff, on='msno', how='left').drop_duplicates(['msno','order_number']) $ df = df.dropna()
liberia_data3['Date'].apply(pd.to_datetime)
adf_check(dfs['Close'].dropna())
season09["InorOff"] = "In-Season"
from sklearn.linear_model import LogisticRegression
writer = pd.ExcelWriter("../visualizations/uber_day_of_year.xlsx")
oldfile.shape
pd.merge(pd.concat([df_a, df_b]), df_c, left_on = "mathdad_id", right_on = "mathdad_id") # same as above 
cust_demo.age.plot.box()
convo_frame.head()
grouped = df_providers.groupby(['year','drg3']) $ grouped_by_year_DRG_max =(grouped.aggregate(np.size)['id_num']) $ grouped_by_year_DRG_max.head()
h6
train['Date'] = pd.to_datetime(train['Date']) $ test['Date'] = pd.to_datetime(test['Date'])
a = t[(t['group'] == 'treatment') & (t['landing_page'] == 'old_page')] $ b = t[(t['group'] == 'control') & (t['landing_page'] == 'new_page')] $ c=len(a)+len(b) $ c_t = pd.concat([a, b]) $ c
a.keys()-b.keys()
df_anomalies.loc[:, 'novel_tweets'] = df_anomalies.apply(lambda row: novel_tweets(row['tweets']), axis=1) $ display(df_anomalies['novel_tweets'].head())
len(pd_austin)
h.ix['ibm us equity'].plot()
tweet_archive_clean =tweet_archive_clean[tweet_archive_clean['retweeted_status_user_id'].isnull() == True]
np.unique(np.concatenate(l2.map(lambda amns: amns.split("|"))))[1:]
score_fs = score[(score["score"] < 70) & (score["score"] >= 50)] $ score_fs.shape[0]
bar.head()
user_gh_dict
DummySwapRepricing = ((df_old['DummySwapMW'] / -2) * delta(df_new, df_old, ['RealPrice'])).rename('DummySwapRepricing') $ Exposure = (df_old['ExposureMWH'] * delta(df_new, df_old, ['RealPrice']) - df_new['ExposureMWH'] * df_new['CapCurve']).rename('Ex[psure]') $ DummyCapRepricing = ((df_old['DummyCapMW'] / -2) * delta(df_new, df_old, ['CapCurve'])).rename('DummyCapRepricing') $ CapVolMvt = ((df_new['DummyCapMW'] + df_new['CapMW'] - df_old['DummyCapMW'] - df_old['CapMW']) * df_new['CapCurve'] *-1).rename('CapVolMvt')
sex_dummy = pd.get_dummies(fraud_data_updated['sex']) $ fraud_data_updated = pd.concat([fraud_data_updated,sex_dummy],axis=1)
df_variables.to_csv("../01_data preprocessing/data new/variables.csv", encoding="utf-8", sep=",", index=False)
grid_id = pd.DataFrame(grid_id_flat).astype('str') $ grid_id.columns = ['grid_id'] $ print(grid_id.head(), grid_id.tail())
df_enhanced['name'].describe()
df.isnull().sum()
print(df.shape) $ print(df['user_id'].nunique())
img = mpimg.imread('input/c(DE-DK-NO-SE).png')
AAPL.columns  # returns the names of its columns
test_predictions = model.transform(df_test) $ test_rmse = evaluator.evaluate(test_predictions) $ print(test_rmse)
mask_XX_century = (deaths_by_decade.reset_index()['decade'] >= 1900) & (deaths_by_decade.reset_index()['decade'] <= 2000)
All_tweet_data_v2.name[(All_tweet_data_v2.name.str.contains('^[(a-z)]'))].value_counts()
prec_fine = interp_spline(grid_lat, grid_lon)
type(df_min_max)
standalone_series = df['instagrams'] != 10.0 $ standalone_series
p_diffs.mean()
fraud_data = pd.read_csv("Fraud_Data.csv") $ ipAddress_to_country =pd.read_csv("IpAddress_to_Country.csv") $
twitter_archive_enhanced = pd.read_csv('twitter-archive-enhanced.csv')
lgb_model.fit(X_train, y_train)
full_df.info()
station_cnt = session.query(Measurement.station).distinct().count() $ station_cnt
df2_treatment.query('converted == 1')['user_id'].count()/df2_treatment['user_id'].count()
rows_exp = parts.map(parse_explicit) $ df_exp = sql.createDataFrame(rows_exp)
db.summarize(load_buf_array)[:]
dill.dump(TEXT, open(f'{PATH}models/TEXT.pkl', 'wb'))
print(parser.HHParser.boundaries)
list(data.dropna(thresh=int(data.shape[0] * .9), axis=1).columns)
guido_title = soup.title $ print(guido_title)
a.append(4) $ print b
CONSUMER_KEY    = 'Uu3D2hHGljVnU2vchDYmHZtGw' $ CONSUMER_SECRET = 'aUfG03L1ZUQjojGK1dQ6dKFC8nktUZQZ4eZDU3p23hEA8ZQus3' $ ACCESS_TOKEN  = '955416286477082625-85nByhWARuuQQJt2QyfFwublVbSE28L' $ ACCESS_SECRET = 'DWARKyhsQKwm1aVWiTsS8RjKQGy778iaERGHeKnauB9Sb'
dtrump = miner.mine_user_tweets(user='realDonaldTrump', max_pages=10) $ print (dtrump[0]['text']) $ dtrump_df = pd.DataFrame(dtrump)
df.head()
px.info()
session.query( $     Country.country_id, Country.country $ ).filter( $     Country.country.in_(["Afghanistan", "Bangladesh", "China"]) $ ).frame()
all_market_data_as_df.size
soup.find("span", {"class":'next-button'}).a['href']
calc_temps1('2017-01-17','2017-03-03')
overall3SsnPorch = pd.get_dummies(dfFull['3SsnPorch'])
class_merged=pd.merge(class_merged,transactions,on=['date','store_nbr'],how='left') $ print("Rows and columns:",class_merged.shape) $ pd.DataFrame.head(class_merged)
obamaSpeechRequest = requests.get('https://en.wikisource.org/wiki/Barack_Obama%27s_Eighth_State_of_the_Union_Address') $ print(obamaSpeechRequest.text[40000:41000])
'my string my'.rfind('my')
lead = df[df["stamp"] <= "2018-02-20 00:00:00"] $ lead.shape
from scipy import stats $ stats.describe(MaxPercentage)
n_new = df2[df2['group']=='treatment'].shape[0] $ n_new
data.head()
df_subset['Existing Zoning Sqft'].describe()
url = form_url(f'organizations/{org_id}/teams', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
p_new = df2.converted.sum() / df2.shape[0] $ p_new
df_country=pd.get_dummies(data=countries_df, columns=['country']) $ df_country.head()
open_weekly = open_prices.groupby([open_prices.index.year, open_prices.index.week]).mean() $ open_weekly
g_sorted.head(10)
pd.DataFrame({'count':user_summary_df[user_summary_df.gender == 'F'].verified.value_counts(), 'percentage':user_summary_df[user_summary_df.gender == 'F'].verified.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})
fav_max = np.max(data['Likes']) $ rt_max = np.max(data['RTs']) $ fav = data[data.Likes == fav_max].index[0] $ rt = data[data.RTs == rt_max].index[0]
sum(df["user_id"].isin(rl))
pprint(q_mine.metadata(reload=True))
df2.info() $ df2['user_id'].unique() $ print('There are {} unique user ids'.format(len(df2['user_id'].unique())))
help(scipy.optimize.root)
display_code('models.py',[208,210])
trump_calc_freq(trump_pivoted) $ trump_pivoted
df.at['20180101','D'] = 0.3 $ df
df3[['CA', 'UK', 'US']] = pd.get_dummies(df3['country']) $ df3 = df3.drop(df3['CA']) #or could also drop doing df3 = df3.drop('CA', axis=1)
cols_to_remove = ["multiverseid", "imageName", "border", "mciNumber", "foreignNames", $                   "originalText", "originalType", "source"] $ all_sets.cards = all_sets.cards.apply(lambda x: x.loc[:, list(set(x.columns) - set(cols_to_remove))])
logreg_sentiment.fit(Xs, y).coef_[0].round(3)
%pylab inline
rng_pytz.tz
pk_planes.count()
data['CONSDIAG'] = scaler.fit_transform(data['CONSDIAG']) $ print_stats('CONSDIAG') $ print '\n' $ data['TRATCONS'] = scaler.fit_transform(data['TRATCONS']) $ print_stats('TRATCONS')
fundret.index
visit_num = grouper.apply(lambda df: (df.shift(1).results == 'Fail').cumsum())
data = settlement_lin_df $ data = data[data["Income"]>=1] $ holdout = data.sample(frac=0.05) $ training = data.loc[~data.index.isin(holdout.index)]
building_pa_prc_shrink=building_pa_prc.drop(columns=cols_filt) $ building_pa_prc_shrink.head()
df_archive_clean["doggo"].add(df_archive_clean["floofer"], $                               fill_value="").add(df_archive_clean["pupper"], $                                                  fill_value="").add(df_archive_clean["puppo"],fill_value="").unique()
import json
interestlevel_df.to_csv('interest_level_df.csv',index=False)
df1.index = range(len(df1)) $ df2.index = range(len(df1))
print(lyra_lightcurve.data.keys())
xmlData = pd.DataFrame(recordSeries)
i_c = df3[df3['group']=='treatment'].index $ df3.set_value(index=i_c, col='ab_page', value=1) $ df3.set_value(index=df3.index, col='intercept', value=1) $ df3[['intercept', 'ab_page']] = df3[['intercept', 'ab_page']].astype(int) $ df3 = df3[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
df4.describe()
timelog.tail()
my_query = "SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES" $ my_result = limsquery(my_query) $ my_result
from datetime import datetime $ birthday = datetime(1983, 9, 27) $ print(birthday)
data3
pandas_list = pd.DataFrame(list) $ print("New data type: %s" % (type(pandas_list)))
url = "https://d5tgnm602g.execute-api.us-east-1.amazonaws.com/beta/la?long="\ $       + var_lon + "&lati=" + var_lat
sns.distplot(master_list['Count'])
user_dropped = df2.user_id.duplicated().sum() $ print('There are {} duplicate rows in df2.'.format(user_dropped))
str(datetime.now())
driver = webdriver.Chrome(executable_path = "C:/libs/web_driver_for_crawl/chromedriver") $ driver.wait = WebDriverWait(driver, 10)
df.head(2)
np.std(diffs)
ripple_market_info.drop(['Date'],inplace=True,axis=1) $ scaler_rip = MinMaxScaler(feature_range=(0, 1)) $ scaled_rip = scaler_rip.fit_transform(ripple_market_info) $
df = pd.read_csv(data_location) $ df.head(5)
serc_reflArray = serc_refl['Reflectance_Data'] $ print(serc_reflArray)
_ = pd.DataFrame(df_trips["pilot"].value_counts()).plot(kind="hist", bins=11)
plt.figure(figsize=(15,6)) $ plt.title('Item Family Distribution', fontsize=20) $ sns.countplot(data=df_items, x='family') $ plt.xticks(rotation=75, fontsize=12)
digits.images[200], digits.target[200]
def filtering(s): $     s = s.lower() $     s = ''.join([c for c in s if c in string.printable]) # Get rid of non ascii whitespace chars, e.g. japanese $     s = s.strip() # Get rid of whitespace AFTER removing chars $     return s
candidates.election_date = pd.to_datetime(candidates.election_date)
joined_hist.reset_index(level=0, inplace=True)
likes.head(5)
df_concat_2["message_likes_dummy"] = np.where(df_concat_2.message_likes_rel > 500, 1, 0)
feature_importances.head(20)
df.shape
data2017_list = [revenue, cost, profit]
print(''.join(re_split_raw[100:150])) $ print() $ print('|'.join(re_split_raw[100:150]))
dfn.to_csv('News.csv')
tweets.head()
pickle.dump(tfidf_data, open('iteration1_files/epoch3/tfidf_data', 'wb'))
no_hyph = df_nona[df_nona['variety']\ $                     .apply(lambda x: len(x.split('-')) < 2)]['variety'].str.lower() $ no_hyph = no_hyph[no_hyph.apply(lambda x: x.split()[-1] != 'blend')].replace(repl_dir)
print(loan_stats["revol_util"].na_omit().min()) $ print(loan_stats["revol_util"].na_omit().max()) $
class_merged=pd.merge(class_merged,class_onpromotion,on=['date','store_nbr','class','family'],how='left') $ print("Rows and columns:",class_merged.shape) $ pd.DataFrame.head(class_merged)
url = "https://en.wikipedia.org/wiki/List_of_world_records_in_swimming" $ tables = pd.read_html(url, header = 0, encoding='utf-8') $ tables[0].head(5)
archive.info()
obs_diff = df_treatment['converted'].mean()- df_control['converted'].mean() $ obs_diff
df_complete = pd.merge(left=merged_predictions_archive, right=df_json_tweet, how='inner', on=['tweet_id']).copy()
autos.describe(include='all')
df['year_built'] = df['year_built'].iloc[:, 0].fillna(-1) + df['year_built'].iloc[:, 1].fillna(-1)
train.readingScore[train.male==0].mean()
df = pd.DataFrame(['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D', 'D-'], $             index = [3*['excellent'] + 3*['Good'] + 3*['Avg'] + 2*['Poor']], columns = ['Grades']) $ df
print (train[["Sex", "Survived"]].groupby(['Sex'], as_index=False).mean())
y = X.dollar_chg_opencls $ X = X.drop('dollar_chg_opencls', axis = 1) $ X_train, X_test = X[:2800], X[2800:]
df = pd.get_dummies(df, columns = ["max_dog_size", "min_dog_size", "requester_gender", "provider_gender", "experience"]) $ df.head(5)
fashion['date_time'] = pd.to_datetime(fashion.created) $ fashion.drop(labels='created', axis=1, inplace=True) $ fashion.info()
year=requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=7up7a3-xNp8abz4VJGsq&start_date=2017-01-01&end_date=2018-01-01')
calls_df.loc[(calls_df["time"]>=4) & (calls_df["time"]<10),["call_time"]]="Morning" $ calls_df.loc[(calls_df["time"]>=10) & (calls_df["time"]<16),["call_time"]]="Noon" $ calls_df.loc[(calls_df["time"]>=16) & (calls_df["time"]<22),["call_time"]]="Evening" $ calls_df.loc[(calls_df["time"]>=22) & (calls_df["time"]<4),["call_time"]]="Night" $ calls_df["call_time"].unique()
data.shape
table3= table1.copy() $ for elem in table3['source'].unique(): $     table3[str(elem)] = table3['source'] == elem $ table3.head(3)
subway2_df.info()
top_songs['Date'].min()
converted = df.query('converted == 1').count() $ converted_prop = converted/total $ converted_prop[0]
likes.dtypes
user1 = df[df.In == people[0]] $
distance = pd.Series(distance_list)
date + pd.to_timedelta(np.arange(12), 'D')
loaded_text_classifier = TextClassifier.load(model_file) $ from tatk.feature_extraction import NGramsVectorizer $ word_ngram_vocab = NGramsVectorizer.load_vocabulary(word_vocab_file_path) $ char_ngram_vocab = NGramsVectorizer.load_vocabulary(char_vocab_file_path)
show_unique(df)
scoring_ind['response_x'][(scoring_ind['response_root'].isnull())].unique()
start_date_sample = '2017-07-25' $ end_date_sample = '2017-07-25' $ sample_data = requests.get('https://www.quandl.com/api/v3/datasets.json?api_key='+API_KEY+\ $                            '&start_date='+start_date_sample+'&end_date='+end_date_sample) $
mnb = MultinomialNB() $ param_grid = {'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], $               'fit_prior':[True, False]} $ mnb_gd = GridSearchCV(estimator=mnb, param_grid=param_grid, cv=5, scoring='f1', n_jobs = -1) $ mnb_gd.fit(X_train, y_train)
randomdata1 = randomdata.describe() $ print randomdata1
tmp_input = pd.DataFrame(tail.iloc[-1,lookforward_window:]).T.values $ tmp_input[0] = scaler.transform(tmp_input[0].reshape(-1,1)).reshape(1,-1) $ print(tmp_input.shape) $ tmp_input = np.reshape(tmp_input, (tmp_input.shape[0], 1, tmp_input.shape[1])) $ print(tmp_input.shape)
posts_answers_df.head()
station.dtypes
(df.set_index('STNAME').groupby(level=0)['CENSUS2010POP'] $     .agg({'avg': np.average, 'sum': np.sum}))
charge_counts.head()
%store -r extract_all
station_bounds[station_bounds.station_id == choice]
df[df.isnull().any(axis=1)]
results.summary()
build_visual_explainer(r_t_test, relevance_scores_df, highlight_oov=True, enable_plot=True)
model = convModel1() $ model.summary()
filtered_brewery[['beer_name', 'brewery_name', 'rating_score']][filtered_brewery.brewery_name == bottom_three[0][0]]
merged_data['due_day'] = merged_data['due'].dt.day $ merged_data['due_month'] = merged_data['due'].dt.month $ merged_data['due_year'] = merged_data['due'].dt.year
db = SteemData()
model._model_json['output']
new_page_converted = np.array(df2.sample(received_new_page, replace=True).converted)
autos = pd.read_csv("autos.csv", encoding="Latin-1")
d_aux=[] $ for i in range(0,l2): $     if i not in seq: $         d_aux.append(diversity[i]) $ col.append(np.array(d_aux))
temp_df['email']
tweet_df.head()
print(All_tweet_data.shape) $ All_tweet_data.head()
lda = LatentDirichletAllocation(n_topics=20, max_iter=5, random_state=1) $ lda.fit(X)
tweets_df.iloc[3, 10]
tz_cat.describe()
results = log_mod1.fit() $ results.summary()
plt.subplots(figsize=(6, 4)) $ sn.barplot(train_session_v2['isNDF'],train_session_v2['reviews'])
import pandas as pd $ import re $ from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
dir='../../Data' $ parquet_file=dir+"/users.parquet" $ !ls $dir
sum(tw_clean.expanded_urls.isnull())
question_2_dataframe = data_2017_subset[data_2017_subset['complaint_type'].isin(top_10_complaint_types.index)] $ question_2_dataframe.head(2)
a+a
plt.rcParams['axes.unicode_minus'] = False $ dta_55.plot(figsize=(15,5)) $ plt.show()
mydata=pd.read_csv("Data-5year-2012-2017.csv")#,index_col="Date") $ mydata[['Date']] = mydata[['Date']].apply(pd.to_datetime, errors='ignore') $ mydata=mydata.set_index(mydata["Date"]) $ mydata = mydata.drop("Date",axis=1)
bin_data.head()
datetime.date(datetime(2014,12,14))
print(ozzy.name)
'-'.join(np.array('asdf-tre-asf-paoa-pamf'.split('-'))[[0,1,2,4]])
c_df['CUSTOMER_ID_CAT'].value_counts()
store_items.interpolate(method = 'linear', axis = 0)
df = pd.DataFrame(np.random.randn(1000, 4), index=random_series.index, columns=list('ABCD')) $ df.head()
df3['DETAILS']=df3['DETAILS'].fillna("")
df_inner_users_sess = pd.merge(users,sessions,how='left',on='UserID') $ df_inner_users_sess[df_inner_users_sess['Registered']== df_inner_users_sess['SessionDate'] ].reset_index().drop('index',axis=1)
df_onc = pd.read_csv(r'F:\Projects\Pfizer_mCRPC\Data\pre_modelling\EMR_Oncology\Pfizer_mCRPC_ONCEMR_update.csv')
jackard_udf = F.udf(jaccard_similarity,types.DoubleType()) $ join_b = join_a.withColumn("name_similairity", jackard_udf(join_a["party_name_orig"],join_a["party_name"])) $ join_b = join_b.withColumn("address_similairity", jackard_udf(join_a["address1_orig"],join_a["address1"])) $ join_b = join_b.withColumn("st_name_similairity", jackard_udf(join_a["street_name_orig"],join_a["street_name"]))
new_df_left = train_df.merge(prop,how='left',on = 'parcelid')
trump.columns
df2[df2['user_id'] == 773192][:-1]
reddit_master.shape
top_10_authors = pd.value_counts(git_log['author']) $ top_10_authors = top_10_authors.head(10) $ print(top_10_authors)
db.collection_names(include_system_collections=False)
random_numbers=pd.Series(np.random.randn(len(ind_date)),index=ind_date) # indexed dates with normal distributed random numbers $ random_numbers
df_h1b_nyc_ft.pw_1.hist(bins=20,figsize=(8,8))
data = [['Alex',10],['Bob',12],['Clarke',13]] $ df = pd.DataFrame(data,columns=['Name','Age'],dtype=float) $ print(df)
(3).__add__(4)
df.head()
temp = (X.describe().T ==len(X)) $ temp.columns = ['a','b','c','d','e','f','g','h'] $ col_with_na = temp.index[~temp.a]
twitter_archive.rating_denominator.value_counts()
df_usnpl_one_hot_state = pd.get_dummies(df_usnpl[['domain', 'state_level_media', 'state']], columns=['state'])
sns.pairplot(iris, hue='species');
ss_scaler = StandardScaler()
dataframe.pct_change()
scidat = pandas.read_csv('./scientists_1.csv') $ print(scidat.columns) $
loansvm = svm.SVC(kernel='rbf') $ loansvm.fit(X, y) 
height.dropna()
df['usr_lat'] = df['user_timezone_coord'].apply(lambda x: None if type(x) is float else x['lat']) $ df['usr_lng'] = df['user_timezone_coord'].apply(lambda x: None if type(x) is float else x['lng'])
df = pd.DataFrame(np.random.randn(5, 3), index=['a', 'c', 'e', 'f','h'],columns=['one', 'two', 'three']) $ df
from datetime import datetime $ sdate = datetime.strptime("2016-10-23 21:50:00.000", "%Y-%m-%d %H:%M:%S.%f") $ print(sdate)
for remove in map(lambda r: re.compile(re.escape(r)), $                   [",", ":", "\"", "=", "&", ";", "%", "$", "@", "%", "^", "*", "(", ")", "{", "}", $                    "[", "]", "|", "/", "\\", ">", "<", "-", "!", "?", ".", "'", "--", "---", "#", "..."] $                  ): $     test.replace(remove, " ", inplace=True)
house_data['price'].hist(bins = 20)
obs_diff = prob_treat - prob_contr $ obs_diff # difference in conversion rate between the treatment and control group
ffr.resample("MS").first().head()
df_new['country'].astype(str).value_counts()
archive_clean.info()
c.execute(create_users_table) $ con.commit()
car_data.describe(include='all')
ebola_melt_1 = ebola_melt.iloc[:,0:4] $ print(ebola_melt_1.head()) $ status_country = ebola_melt.iloc[:,5:] $ print(status_country.head())
df_all_loans.rename(columns={0:'irr'},inplace=True)
predictions = rfr.predict(test[['expenses', 'floor', 'lat', 'lon', 'property_type',\ $                                 'rooms', 'surface_covered_in_m2', 'surface_total_in_m2']])
del sales['County Number'] $ true_sales = sales.dropna() $ true_sales.shape
autodf.monthOfRegistration = autodf.monthOfRegistration.astype('category')
scr_churned_bool = pd.Series([USER_PLANS_df.loc[uid]['status'][np.argmax(SCR_PLANS_df.loc[uid]['scns_created'])] =='canceled' for uid in SCR_PLANS_df.index],index=SCR_PLANS_df.index)
df[df['converted'] == 1]['user_id'].nunique() / df['user_id'].nunique()
from IPython.display import Image $ Image("/Users/jamespearce/repos/dl/data/dogscats/train/cat.2150.jpg")
df2[(df2['group'] == 'control')]['converted'].mean()
witf = open("latlong_test2.txt","w", encoding="utf-8") $ for i in range(len(test_kyo2)): $     witf.write("{location: new google.maps.LatLng(%f, %f)},\n" % (test_kyo2['ex_lat'][i],test_kyo2['ex_long'][i])) $ witf.close()
filename = 'turnstile_180303.csv' $ subwaydf = pd.read_csv(filename)
df.select_dtypes(include='bool').head()
click_condition_meta.os_timezone = click_condition_meta.os_timezone.str.lower()
capitalizer = lambda x: x.upper() $
files = [] $ for filename in glob.glob('data/*.csv'): $     print filename $     files.append(filename)   
df.to_csv('btime2.csv')
test = test.drop('teacher_id',axis=1)
contractions_df = pd.read_csv('data/contractions.csv', sep=' -') $ contractions = [word for word in contractions_df['from']] $ contractions[18] = "mightn't" $
full_globe_temp.dropna()
df_cprc = df_uro_dd_dummies[ls_cprc_columns] $ df_cprc.head()
optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss) $ tf.summary.scalar("loss", loss) $ merged_summary_op = tf.summary.merge_all()
autos.describe(include='all')
iter_visits = pd.read_csv(visits_path, iterator=True, chunksize=1000000) $ cols = ['id', 'id_partner', 'name', 'visits'] $ users_visits = Users.assign(visits=lambda x: 0) $ users_visits[cols] $ users_visits = count_visits(iter_visits, users_visits, 0)
p_diff = np.array(diffs) $ plt.hist(p_diff); $ plt.axvline(x = ab_data_diff, color = 'r');
thisWeek.head()
ann_ret_SP500[0].head(10)
Amazon = web.DataReader('AMZN', 'google', start, end) $ Amazon.head()
scoreListOfList = pd.Series(scoreListOfList)
convert_mean = df2.query('converted == 1')['user_id'].nunique() / total_users $ convert_mean
tfav_b.plot(figsize=(16,4), label="Likes", legend=True) $ tret_b.plot(figsize=(16,4), label="Retweets", legend=True);
top_songs['Country'].unique()
for col in ['bottle_vol_ml', 'bottles_sold', 'sale_dollars', 'volume_sold_lt', $             'state_bottle_retail', 'state_bottle_cost']:    $     df = iowa[(iowa[col] < np.mean(iowa[col]) + 3 * np.std(iowa[col])) $               & (iowa[col] > np.mean(iowa[col]) - 3 * np.std(iowa[col]))]
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.binomial(N_new, P_new) $     old_page_converted = np.random.binomial(N_old, P_old) $     p_diffs.append((new_page_converted/N_new) - (old_page_converted/N_old))
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs);
type(df_arch_clean["tweet_id"])
print(len(content))
appl["20d"] = np.round(appl["Close"].rolling(window = 20, center = False).mean(), 2) $ pandas_candlestick_ohlc(appl.loc['2016-01-19':'2017-01-19',:], otherseries = "20d")
tobs_data
def dist(a, b): $     return np.power((np.power((a[0] - b[:, 0]), 2) + np.power((a[1] - b[:, 1]), 2)), 1./2)
gdax_trans['Balance']= 0.00
null_mean = np.mean([p_new,p_old]) $ null_mean
df = pd.read_csv('train.csv') $ breedMap = pd.read_csv('breed_size.csv') $ breedMap['Size'] = breedMap.Size.apply(lambda x: x.strip())
n_new = ab_df2.query('landing_page == "new_page"').shape[0] $ n_new
df2 = df2.drop_duplicates(['user_id']) $ df2[df2.duplicated(['user_id'])]
we_rate_dogs = pd.read_csv('twitter-archive-enhanced.csv')
sm.graphics.plot_partregress
logit = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit.fit()
%bash $ gsutil cat "gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*" | head
autos["ad_created"].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True)
len(liberiaFileList) == len(liberiaDf)
print cust_data1.columns $ cust_data1=cust_data1.rename(columns={'RevolvingUtilization':'Rev_Utilization', 'SeriousDlqin2yrs':'SeriousDlq'}).head(100)
g_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ p_diff = np.array(p_diff) $ (g_diff < p_diffs).mean() $
import statsmodels.api as sm
d = docx.Document(downloadIfNeeded(example_docx, example_docx_save, mode = 'rb')) $ for paragraph in d.paragraphs[:7]: $     print(paragraph.text)
loanTree.fit(X,y)
d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), $      'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])} $ df = pd.DataFrame(d) $ df
features = daily_df[['Company','Number_of_Tweets', 'Number_of_Users','Mean_Volume','Clean_text']] $ classification_price = daily_df['Price_Change'] $ classification_open = daily_df['Open_Price_Change'] $ regression_price = daily_df['Price_Percent_Change'] $ regression_open = daily_df['Price_Percent_Open']
fig = plt.figure(figsize=(10,4)) $ ax = fig.add_subplot(111) $ ax = resid_713.plot(ax=ax);
fb['2012':].resample('W')['share_count'].sum().sort_values(ascending=False)[:5]
for info in zf_train.infolist(): $     print("File Name         -> {}".format(info.filename)) $     print("Compressed Size   -> {:.2f} {}".format(info.compress_size/(1024*1024), "MB")) $     print("UnCompressed Size -> {:.2f} {}".format(info.file_size/(1024*1024), "MB"))
advcoeffdf.tail(10)
challenges = pd.read_csv('challenges.csv')
autos['last_seen'].str[:10].value_counts(normalize = True, dropna = False).sort_index()
train['StateHoliday']=train['StateHoliday']!='0' $ test['StateHoliday']=test['StateHoliday']!='0' $ add_datepart(train,"Date",drop=False) $ add_datepart(test,"Date",drop=False)
seller
import numpy as np $ data['position'] = np.where(data['SMA1'] > data['SMA2'], 1, -1) $ data.dropna(inplace=True) $ data['position'].plot(ylim=[-1.1, 1.1], title='Market Positioning')
results.mean()
df2 = df.copy()
df['intercept']=1 $ df[['control', 'treatment']] = pd.get_dummies(df['group']) $ df.rename(columns={'treatment':'ab_page'},inplace=True)
reddit_comments_data.select('parent_id').distinct().count()
young.groupBy("gender").count()
lower_case = letters_only.lower()        # Convert to lower case $ words = lower_case.split()               # Split into words $ words[:10]
Grouping_Year_DRG_discharges_payments.xs(2015, level='year').head()
from credentials import *    # This will allow us to use the keys as variables $
corpus = createCorpus(tokens)
from sklearn.ensemble import RandomForestRegressor $ rf_reg = RandomForestRegressor(max_depth=25, random_state=0) $ rf_reg.fit(x_train,y_train)
print("DataFrame existing column name (before rename):") $ df.head(2)
import pandas as pd
wikiMeritSoup = bs4.BeautifulSoup(wikiMeritRequest.text, 'html.parser') $ print(wikiMeritSoup.text[:200])
top_songs['Artist'].dtype
gt_enrollment_data['semester'] = gt_enrollment_data['semester']
len(train_data[train_data.notRepairedDamage == 'ja'])
df.rename(columns={'Year_of_Release':'Year_of_Release_Sales'}, inplace=True) $
tbl $
shifted_backwards = msftAC.shift(-2) $ shifted_backwards[:5], shifted_backwards.tail(5)
print('number of observations:',len(data)) $ col_names = list(data.columns) $ print('features:', col_names)
print type(rdd_example2) $ print rdd_example2.collect() $
train_small_data.head(1)
calls_df.head()
daily_norm = [] $ for trip_date in trip_list: $     daily_norm.append([trip_date[0], sq.daily_normals(trip_date[1])[0][0], $                        round(sq.daily_normals(trip_date[1])[0][1]), $                        sq.daily_normals(trip_date[1])[0][2]])
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 1), 'type'] = 'defenders'
datatest.loc[datatest.expenses.str.isdigit() == False,'expenses'] = np.NaN
log_mod_dweek = sm.Logit(df_new['converted'], df_new[['intercept', 'Friday', 'Monday', 'Saturday', 'Thursday', 'Tuesday', 'Wednesday']]) $ log_mod_dweek_results = log_mod_dweek.fit() $ log_mod_dweek_results.summary()
data_archie = data_archie[data_archie['cur_sell_price'].notnull()]
df_tweet_clean2.info()
Y_tweet.idxmax()
with pd.option_context('display.max_colwidth', 130): $     print(news_title_docs_high_freq_words_df['high_freq_words'])
Replace instances of "& amp" with just "&"
class_merged.isnull().sum()
images_copy.columns $
df2.drop(labels=1899, axis=0, inplace=True)
Pnew = df2.query('converted == 1').user_id.nunique()  / df2.user_id.count() $ Pnew
col_names = data["dataset"]['column_names']
from pyspark.ml.feature import VectorAssembler $ from pyspark.ml.clustering import KMeans
print(df5['new_id'].value_counts())
len(labels)
tweet = api.get_status(892420643555336193) $ print(tweet.text)
education=pd.read_csv('data/crime/final_education.csv')
X_train, y_train = make_x_y(df_train) $ X_dev, y_dev = make_x_y(df_dev) $ X_test, y_test = make_x_y(df_test)
print('labeling data...') $ df_labeled = pp.label_data(df_data=df_raw, df_label=df_label) $ print('saving labeled file to the path:'+out_file_path +'...') $ df_labeled.to_csv(out_file_path)
authors_to_github_username_saved.schema
!miniasm -h
!head -n 2 evalme/predict_results_eval.csv
clf = svm.SVC(kernel='rbf')
get_first_ten = "SELECT * FROM coindesk LIMIT 3;"
dfWords.head()
df_final['Scorepoints'] = df_final.R + df_final.F + df_final.M + df_final.P + df_final.J
df2 = pd.concat([df2a, df2b], axis=0)
sum(df['converted'])/df.shape[0]
%%time $ with tb.open_file(filename='data/NYC-yellow-taxis-2017-12.h5', mode='a') as f: $     table = f.get_node(where='/yellow_taxis_2017_12') $     table.cols.trip_distance.create_index() $     table.cols.passenger_count.create_index()
doc_duration.head()
ab_file2[ab_file2['user_id']==773192]
temp_dir = tempfile.TemporaryDirectory() $ output_path = Path(temp_dir.name, 'fake_patient_data.h5') $ output_path
df_repub.iloc[2984]
test.head(1)
list(c.find({}, {'name.first': 1, $                  'born': 1, $                  '_id': 0}))
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response = requests.get(url)
twitter_Archive.dropna(subset = ['rating']) $ twitter_Archive = twitter_Archive.drop(twitter_Archive[twitter_Archive.rating > 3].index)
fig, ax = plt.subplots(figsize=figsize) $ ax.plot(daily_deltas.rolling(window=30, min_periods=0, center=False).mean()) $ ax.set_xlabel('Date') $ ax.set_ylabel('Delta notebook count') $ _ = ax.set_title('30-day rolling mean of daily-change')
autos = autos.drop(['seller', 'offer_type', 'num_photos'], axis=1) $ autos.columns
len(df[df.isnull().any(axis=1)])
lrs.shape
autos['date_crawled'].str[:10].head()
df_country_join[['UK','US']] = pd.get_dummies(df_country_join['country'])[['UK','US']] $ df_country_join.head()
df.expanding(min_periods=2).sum()
not_in_dsi = not_in_dsi.copy()[not_in_dsi.copy()['full_name_y'].isnull()][['name', 'forks_url_x', 'forks_count_x', 'created_at_x']] \ $     .rename(columns={'forks_url_x': 'forks_url', 'forks_count_x': 'forks_count', 'created_at_x': 'created_at'})
tipsDF = pd.read_csv('yelp_tips.csv', index_col=False, encoding='UTF-8') $
long_list = list(top_bike['Long']) $ last_elem = long_list[-1] $ long_list.append(last_elem) $ long_list.pop(0) $ top_bike['newLong'] = long_list
session.query(func.count(Measurements.date)).all()
centers_df = pd.read_csv('./data/chicago-workforce-centers.csv') $ centers_df.head()
df.resample('D', how='count')
rescue_code(get_weights)
3175 + 17378 # Not matched + "doi_pid" size == distinct DOIs
df1_clean.sample(5)
print X_lately
potholes['Unique Key'].groupby(by= potholes.index.dayofweek).count().sort_values(ascending = False).head(1)
len(fb)
df.head(3) # only 3 data rows
len(test_dict.keys())
extract_all.loc[(extract_all.application_month=='2018-04') $                 &(extract_all.app_branch=='NV0848') $                 &(extract_all.APP_PRODUCT_TYPE=='PL'),['APPLICATION_DATE_short','DEC_LOAN_AMOUNT1','DEC_FINAL_DECISION']]
test_features = ['usd_goal_real', 'Main_Cat_Nums', 'Sub_Cat_Nums', 'City-Nums', 'State-Nums', 'month_launched', 'Length_of_kick', 'Days_spent_making_campign', 'City_Pop','staff_pick'] $ X = df_master_select[test_features].copy() $ y = df_master_select[['Status']].copy() $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=324)
df2 = df2.drop_duplicates()
print(r.json()['dataset_data']['column_names'])
sponsors_df = pd.read_csv('data/sponsors_list_final.csv', $                           header=None, $                           names=['Sponsors', 'Sponsor_Classification'], $                           index_col='Sponsors') $ sponsors_df.head()
df2.loc[1899] #check if first index has the same userid
df_final_edited_10 = df_final_edited.loc[df_final['rating_denominator']==10] $
from sklearn.metrics import auc, roc_curve
del df_t['Postal Code Int'] $ del df_t['Postal Code_y']
compound_final['Date'] = pd.to_datetime(compound_final['Date'])
pd.DataFrame({'like_plaintiff': like_plaintiff.sample(100)}).join(pd.DataFrame({'like_defendant':like_defendant})).join(geocoded_df[['Plaintiff.Name','Defendant.Name','Judgment.In.Favor.Of']])
suspects_with_25_1['in_cp'].value_counts()
min([len(h.tweets) for h in heap])
nbsvm_models
import nltk $ nltk.download('stopwords')
print 'Most Onion-like:', tweets_pp[tweets_pp.handle == 'TheOnion'].sort_values('The_Onion_Prob', ascending=False).text.values[0] $ print 'Least Onion-like:', tweets_pp[tweets_pp.handle == 'TheOnion'].sort_values('The_Onion_Prob', ascending=True).text.values[0]
export1['chi_squared'] = export1['1'].apply(lambda x: eval(x)[1])
lda_model_filepath = paths.lda_model_filepath
print(dt + td)
p_diffs = [] $ new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation $
print 'The result of the operation is a new Period object that mimics the intial one,' $ print 'as if the new object were created by pd.Period(_new_time_, freq=_freq_).' $ print 'Adding 1 to aug2014 gives start_time and end_time of:' $ sep2014 = aug2014 + 1 $ sep2014.start_time, sep2014.end_time
plt.hist(p_diffs) $ plt.axvline(x=df2.converted[df2.group == "treatment"].mean() - df2.converted[df2.group == "control"].mean(), color = "red" )
%bash $ ls -ls preproc_tft $ ls -ls preproc_tft/metadata $
for tweet in islice(tweets, 20): $     if tweet != None: $         print json.loads(tweet)['text'][:20] $     else: $         print 'Timeout.'
reddit = praw.Reddit(client_id='OPV1wsaqbio8yA', $                      client_secret='xM_zLZjDaqwex_rH4vcfpoEvCqc', $                      password='capstone', $                      user_agent='testscript by /u/capstone33', $                      username='capstone33')
merged_df['frequency'] = merged_df['validOrders'] - 1 $ merged_df.head()
data.drop(['ceil_10min_x','ceil_10min_y'], axis=1,inplace=True)
vlen_t = ncfile.createVLType('i4', 'phony_vlen')
autos['fuelType'].unique()
top_brands = (autos["brand"].value_counts() $               .sort_values(ascending=False).head(6).index) $ print(top_brands)
df_titanic_temp = pd.read_excel(DATA_FOLDER + "/titanic.xls") $ sns.barplot(x="sex", y="survived", hue="pclass", data=df_titanic_temp, estimator=sum, ci=None) $ df_titanic_survived = df_titanic.loc[df_titanic['survived'] == 1] $ titanic_class_sex_obj = df_titanic_survived.groupby(['sex','pclass']) $ titanic_class_sex_obj['survived'].describe()
experiment_run_details
reddit_comments_data.orderBy('score').select('score').show(10)
edfconvert = EDFConverter(os.path.join(edfDir, 'pt1sz2_0001.edf'), 'pt1sz2.log') $ edfconvert.edfconvertlogger()
from Classes.TweetsProcessor import TweetsProcessor $ allImages = TweetsProcessor.getAllImages(tweetsDuringEvent, 'image_url')
All_tweet_data_v2.name[(All_tweet_data_v2.name.str.len() < 3) & (All_tweet_data_v2.name.str.contains('^[(a-z)]'))]
inspector = inspect(engine) $ columns = inspector.get_columns('stations') $ for c in columns: $     print(c['name'], c["type"]) $
null_vals = np.random.normal(0, np.std(p_diffs), 10000) $ plt.hist(null_vals);
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ WHERE twitter_day = 7 \ $ GROUP BY tag_text ORDER BY count DESC LIMIT 1;
dot818_dup = [486782526] $ fields = ['APP_APPLICATION_ID','APPLICATION_DATE','APP_SOURCE','APP_SSN','APP_LAST_NAME','APP_FIRST_NAME','DEC_FINAL_DECISION'] $ extract_all.loc[(extract_all.APP_SSN.isin(dot818_dup)), fields]
len(df)
plot_series_save_fig(series=RN_PA_duration, figsize=(12,6), xlabel='Date', ylabel='Appointment Time (hours)',\ $                      plot_name='RN/PAs', figname='./images/RNPA_weekly_time_series.png')
from sklearn.model_selection import StratifiedShuffleSplit $ split = StratifiedShuffleSplit(n_splits=1, test_size=20000, random_state=1234) $ for train_index, test_index in split.split(clean_appt_df, clean_appt_df['No-show']): $     train_set = clean_appt_df.iloc[train_index] $     test_set = clean_appt_df.iloc[test_index] $
train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)
boolean_test_frame = ((loan_stats['loan_status'] == 'Charged Off')|(loan_stats['loan_status'] == 'Default')) $ loan_stats['loan_status'] = boolean_test_frame.ifelse('Default','Fully Paid')
df_newlen = len(df2.query("group =='treatment'")) $ df_newlen 
df.query("group == 'treatment' and landing_page != 'new_page'").shape[0] + df.query("group != 'treatment' and landing_page == 'new_page'").shape[0]
tree = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=2, splitter='random')
train_labels = perf_train['Default'] $ perf_train, perf_test = perf_train.align(perf_test, join = 'inner', axis = 1) $ perf_train['Default'] = train_labels $ print('Training Features shape: ', perf_train.shape) $ print('Testing Features shape: ', perf_test.shape)
question_1_dataframe = question_1_dataframe.groupby(['borough', 'complaint_type']) $ question_1_dataframe
print(df_raw.loc[(df_raw.year==1993) & (df_raw.data.str.match(r'\s*\d+\s*\d+\s*NEVER KEEPING SECRETS')),:]) $
len(retweet_lst_names)
S_2017 = active_2017_06.drop_duplicates('PropID')[['PropID', 'Lat', 'Lng']] $ S_2016 = active_2016_06.drop_duplicates('PropID')[['PropID', 'Lat', 'Lng']] $ S_2015 = active_2015_06.drop_duplicates('PropID')[['PropID', 'Lat', 'Lng']]
df.tail()
resultvalue_df = pd.DataFrame.from_records(resultvalue_records)
smap
theft = crimes[crimes['PRIMARY_DESCRIPTION']=='THEFT'] $ theft.head()
indexed_return(fundret['2017-04':]).head()
logit_mod4 = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'UK', 'ab_UK']]) $ results4 = logit_mod4.fit() $ results4.summary()
plot_hist_dict(signup_uncompleted, feature='birthPlace', top_k=20) 
X_train, X_test, y_train, y_test = train_test_split(data.drop('Class',axis=1), data['Class'], test_size=0.33) $ print(X_train.shape) $ print(y_train.shape) $ print(X_test.shape) $ print(y_test.shape)
df.median() - 1.57 * (df.quantile(.75) - df.quantile(.25))/np.sqrt(df.count())
df2.converted[df2.group == 'control'].mean()
va = VectorAssembler(inputCols=(input_features), outputCol='features') $ df = va.transform(df).select('deviceid','date','label','features') $
df.dropna(how='any', inplace=True) $
for name, in session.query(User.name).\ $ ...             filter(User.fullname=='Ed Jones'): $ ...    print(name)
df['price'].max()
pnew = df2[df2['landing_page']=='new_page']["converted"].mean() $ pnew
?pd.read_csv()
f_lr_hash_test = (prediction_test $                   .withColumn('f_lr_hash_inter2_2p18_noip', get_pred_udf(col('probability'))) $                   .select('id', 'f_lr_hash_inter2_2p18_noip')) $ f_lr_hash_test.show(3)
gdf['SiteTypeCV'].value_counts()
indeed[indeed['summary'].isnull()] $
reader.load_entire_table()
pd.Timedelta('2D 3H')
from sklearn.model_selection import train_test_split
df_new_log = pd.get_dummies(df_new,columns = ['landing_page', 'group','country']) #create dummies $ df_new_log = df_new_log.drop(['landing_page_old_page','landing_page_new_page','group_control','country_CA'],axis=1) #drop unnecessary columns $ df_new_log['intercept'] = 1 $ df_new_log.head()
import pandas as pd
df['acct_type'].unique()
joined.reset_index(inplace=True)
providers_schedules = pd.read_csv('./data/ProvidersSchedulesLastest.csv')
sns.heatmap(users.corr())
df_R.head()
train.StateHoliday.head()
wrd_clean['expanded_urls'][0]
create_database_query='CREATE DATABASE twitter' $ engine.execute(create_database_query) $ pd.read_sql('Show Databases',con=engine)
corn.size().index.values
donald_tweets_df = transform_tweets( donald_tweets, $                                      "Trump - Vader Sentiment Score" ) $ donald_tweets_df.head()
twitter_Archive.head()
dates=pd.date_range(start='1/Oct/2020', periods=5, freq='M') $ print(dates)
df_treatment = df2[df2.group == 'treatment'] $ sum(df_treatment.converted==1) / df_treatment.shape[0]
df[df.duplicated(subset='user_id', keep=False)].groupby(['group', 'landing_page']).count()
lmscore.predict(X)
twitter_data[twitter_data.tweet_id.duplicated()]
nb.fit(X_train_dtm, y_train)
lok = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','CA','US']]) $ result2 = lok.fit() $
grid.fit(X_trainfinaltemp, y_train)
priceData = priceData.loc[:,('Name', 'BestBuyYesCost', 'BestSellYesCost', 'BestBuyNoCost', 'BestSellNoCost','TickerSymbol')] $ priceData.sort_values(by = 'TickerSymbol', ascending=True, inplace=True) $ priceData.reset_index(drop=True, inplace=True)
df = pd.read_csv(wd + 'all_w_label.csv') $ pred = pd.read_csv(wd + 'output/myoutput-2017-09-11-13-21-54-361501.csv') $ geog = pd.read_csv(INPUT_FOLDER + 'beh_nyc_walkability.csv') $ df['index_col'] = df.index $ pred['index_col'] = pred.index
from datetime import datetime $ from dateutil import parser $ ind_date= pd.bdate_range('2015-01-01','2015-12-31') 
model = GradientBoostingRegressor(verbose=True) $ model.fit(X_train, y_train) $ model.score(X_test, y_test) # This is the R^2 value of the prediction
combined_df[combined_df['classifier_summary'].isnull() == True]
result = log_mod.fit() $ result.summary()
first_result = results[0]  $ first_result 
autos['last_seen'].str[:10].value_counts().sort_index()
dummy_var_df.head()
oppose.sort_values("amount", ascending=False).head(10)
df_vow.describe()
new_page_convertered = np.random.binomial(1, p=p_new, size=n_new)
weather.dtypes
df[(df['outcome']=='Exposed to Potential Harm') | (df['outcome']=='No Negative Outcome')].count()[0]
news_titles_sr = news_period_df.resample('D', on='news_collected_time')['news_title'].apply(lambda x: '\n'.join(x))
df['month'] = df.order_date.dt.month.T $ df['day'] = df.order_date.dt.day.T $ df['year'] = df.order_date.dt.year.T
neigh = KNeighborsClassifier(n_neighbors = 9).fit(X_train,y_train) $ yhat_knn = neigh.predict(X_test)
def var(scenarios, level=99, neutral_scenario=0): $     pnls = scenarios - neutral_scenario $     return - np.percentile(pnls, 100-level, interpolation='linear')
log_reg_under.score(X_train, y_train_under)
df.tail(3)
miss = df.isnull().values.any() $ print ("Missing Values : {}".format(miss))
pattern = re.compile('AA') $ print([x for x in pattern.finditer('AAbcAA')]) $ print([x for x in pattern.finditer('bcAA')])
sentiDf[sentiDf.index > '2014-01-01'].rolling(window=30, min_periods=0).mean().corr()
print ('Keywords:') $ print (keywords(text))
final_word_df[final_word_df['Word_stem']=='the']
df.select('post_creation_date').show()
loans_df.earliest_cr_line = pd.to_datetime(loans_df.earliest_cr_line) $ loans_df.earliest_cr_line.value_counts()
print requests.__package__+':'+requests.__version__ $ print etree.__package__+':'+etree.VERSION $ print pd.__package__+':'+pd.__version__ $ print matplotlib.__package__+':'+matplotlib.__version__
clean_appt_df = appt_df.copy() $ clean_appt_df.describe()
sim_diff = new_page_converted.mean() - old_page_converted.mean() $ sim_diff
grouped = df.groupby('Year') $ for name,group in grouped: $     print(name) $     print(group)
info.describe()
lm = sm.Logit(df2['converted'], df2[['intercept', 'UK','US']]) $ results = lm.fit() $ results.summary()
z_score, p_value = sm.stats.proportions_ztest([convert_new[0], convert_old[0]],[n_new[0], n_old[0]],alternative = 'larger') $ z_score, p_value
bptt,em_sz,nh,nl = 70,400,1150,3 $ vs = len(itos) $ opt_fn = partial(optim.Adam, betas=(0.8, 0.99)) $ bs = 48
print(r.json())
for col in users: $     print(col) $     print(users[col].describe()) $     print('-----------------')
tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['dog_stage']=='floofer']
import requests $ import datetime
df = df[['id','name','state','category_main','category_name','backers_count','pct_goal_achieved','usd_pledged','usd_goal','country','currency','campaign_length', $          'deadline','launched','created','staff_pick','spotlight','creator_name','blurb_length']]
df2.head()
apple.groupby(pd.TimeGrouper(freq = 'M')).agg(np.mean).index
weather = weather.sort_values(by='created_date') $ weather.head()
plt.figure(figsize=(8, 5)) $ train_df.groupby('domain').favs.median().plot.bar() $ plt.title('Median of the #favs by domain') $ plt.xticks(rotation='horizontal');
sf_small = sf_subset.iloc[1:100000]
data['Time'] = pd.to_datetime(data['Time'], format='%H:%M').dt.time
impressions_products.head(2)
master_df.name.value_counts().hist();
merged_df['recency'] = (merged_df['last'] - merged_df['first']).astype('timedelta64[W]') #D for days, W for weeks etc $ print(merged_df.dtypes) $ merged_df.head() $ merged_df[merged_df['recency'] <0] 
complete_df.loc[complete_df['Totals'].apply(lambda x: int(x)) < 0, 'Totals'] = 0
ab = fb['2012':].resample('W')['share_count'].sum().sort_values(ascending=False)[:5].index
staff = staff.set_index('Name') $ staff
weather_features = weather_features['20130101':'20171231'] $
calls_df[calls_df["call_type"]=="Ignore"].count()
req.text
data.set_index('zipcode', inplace=True) $ data.to_json('heating.json')
todaysTweets_json = todaysTweets.to_json(orient='records') $ with open('Data/todaysTweets_'+date+'.json','w') as fp: $     json.dump(todaysTweets_json,fp)
%load "solutions/sol_2_12.py"
ocsvm_stemmed_bow.fit(trump_stemmed_bow, y = y_true_stemmed_bow) $ prediction_stemmed_bow = ocsvm_stemmed_bow.predict(test_stemmed_bow) $ prediction_stemmed_bow
%matplotlib inline
new_page_converted = np.random.choice([0,1], size = n_new, p=[1-p_new,p_new])
(df_state_votes.hill_trump_diff > 0).value_counts()
building_pa_prc_shrink.dtypes
df[(df.amount == 0)].amount_initial.unique()
%%time $ data = pd.read_csv('/Users/srdas/GoogleDrive/Papers/DeepLearning/DLinFinance/CreditCardFraud/creditcard.csv') $ print(data.shape)
float(result2[result2['dt_deces'].notnull()].shape[0]) / float(result2.shape[0]) * 100.
scores, metrics = pipeline.test(ds_train, 'Label') $ print("Performance metrics on training set: ") $ display(metrics)
uniqueArtists = userArtistDF.select("artistID").distinct().count() $ print("Total n. of distinct artists: ", uniqueArtists) $
list(zip(feature_cols,lr.coef_))
skf = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)
train.age.value_counts().head()
pd.Categorical(titanic.pclass) $ pd.Categorical(titanic.survived) $ pd.Categorical(titanic.embarked) $ pd.Categorical(titanic.pclass) $ pd.Categorical(titanic.sex)
companyNeg.shape
import pickle $ df = pickle.load(open("kicks.pkl",'rb'))
engine = create_engine("sqlite:///hawaii.sqlite")
first_result.contents[1]
highlow_range = [float('nan') if item[2] is None or item[3] is None else item[2]-item[3] for item in afx['dataset_data']['data']]
len(pd.unique(tag_df.values.ravel()))
fcc_nn.plot(y='score', use_index=True)
df['user_id'].value_counts().index $ newdf = df.drop_duplicates('user_id') $ len(newdf[newdf["converted"]==1])/len(newdf.index)
train = train.sort_values(['Store','Date']) $ test = test.sort_values(['Store','Date'])
plt.title("rating_numerator over favorite_count") $ plt.xlabel("favorite_count") $ plt.ylabel("rating_numerator") $ plt.ylim(9,15) $ plt.scatter(data=df_merge,x='favorite_count',y='rating_numerator',alpha=0.2);
pd_builder = builder.toPandas()
pd.DataFrame([[7.75, 8.75, 7.50]]*4, index=grades.index, columns=grades.columns)
svc = SVC() $ svc.fit(X_train, Y_train) $ Y_pred = svc.predict(X_test) $ acc_svc = round(svc.score(X_test, Y_test) * 100, 2) $ acc_svc
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs))\ $     .filter(Measurement.station == 'USC00519281').all()
archive_copy.head()
df.shape
df['log_time_detained'] = np.log(df['time_detained'])
for index, text in enumerate(all_text): $     cv = re.compile('crossvalidation') $     all_text[index] = re.sub(cv,'cross validation',text) $     all_text[index] = re.sub(quote,'',text)
import json $ emp_records_json_str = json.dumps(EmployeeRecords) $ df = pd.read_json(emp_records_json_str, orient='records', convert_dates=['DOJ']) $ print(df)
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ WHERE twitter_day = 5 \ $ GROUP BY tag_text ORDER BY count DESC LIMIT 1;
engine = create_engine("sqlite:///./Resources/hawaii.sqlite", echo=False)
autos['odometer_km'].value_counts()
y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled) $ mse = mean_squared_error(y_train, y_pred) $ np.sqrt(mse)
top5_best_fan = success_order.groupby(['CUSTOMER_ID']).size().sort_values(ascending=False)[0:5] $ top5_best_fan
"Rural" in df_protest.columns
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.binomial(1,p_new,n_new).mean() $     old_page_converted = np.random.binomial(1,p_old,n_old).mean() $     p_diffs.append(new_page_converted - old_page_converted) $
sns.barplot(x=top_sub['id'], y=top_sub.index) # challenge: annotate values in the plot $ plt.xlabel("number of posts") $ plt.title("Top 5 active subreddits by # of posts");
b_list.head(2)
weather_missing = weather_all.isnull() $ weather_missing.head()
null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size)
sum(twitter_df_clean.rating_numerator > 10)
def qualConversion(x): $     p = '' $     if type(x.opportunity_qualified_date) == pd.tslib.Timestamp: p = 'convertedQual' $     return p
article_urls = ['http://www.nhm.ac.uk/discover/the-cannibals-of-goughs-cave.html','http://www.nhm.ac.uk/discover/how-we-became-human.html','http://www.nhm.ac.uk/discover/the-origin-of-our-species.html'] $
print(autos.groupby('seller').size())
req_test.text
b = pd.read_sql_query(q, conn) $ b
measurement_results = session.query(Measurements.station,Measurements.date,Measurements.prcp, Measurements.tobs).all() $
"my string".find('s')
Recent_Measurements = pd.DataFrame(Recent_Measurements) $ Recent_Measurements.set_index(Recent_Measurements['date'], inplace=True) $ Recent_Measurements = Recent_Measurements.sort_values('date') $ Recent_Measurements.plot()
festivals.set_index('Index', inplace=True)
tweet_df.retweeted.value_counts() $
df_twitter_archive[df_twitter_archive.text.duplicated()]
ws= wb['test_sheet']
save_n_load_df(df, 'rolled_filled_elapsed_events_df.pkl')
information_ratio - 2 $
highlandpark_potholes = data_311[potholes & highland_park] $ print(highlandpark_potholes.shape) $ highlandpark_potholes.head()
d = pd.read_csv('twitter_archive_master.csv')
df.to_csv('superbowl_playbyPlay.csv')
loan_stats[1:3,['member_id','term']]
rng = pd.date_range('2015-12-01', periods=100, freq='S') $ rng[:5]
vip_crosstab = pd.crosstab(vip_df['Month'], vip_df['Finish_Type']) $ vip_crosstab
gs_from_model.score(X_test, y_test_over)
bigdf.head()
data.to_json('nytimes_oped_articles.json')
faa_data_pandas["DAMAGE"].value_counts().rename(index=damage_lookup_dictionary)
obs_diffs = treatment_convert_rate - control_convert_rate $ p_val = (obs_diffs < p_diffs).mean() $ print('The p value is {}'.format(p_val))
from __future__ import division $ import csv $ from datetime import datetime $ from collections import Counter
%matplotlib inline
inspector = inspect(engine) $ inspector.get_table_names() $ columns = inspector.get_columns('Measurement') $ for c in columns: $     print(c['name'], c["type"])
temps_df.Difference[1:4]
labels = tf.one_hot(targets, len(vocab)) $ loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels) $ loss = tf.reduce_mean(loss) $ train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)
def getHour(x): $     return x.split()[1].split(':')[0] $
df[df.speaking_line].groupby('episode_id')['id'].nunique().head()
twitter_df_clean.drop(twitter_df_clean[twitter_df_clean.retweeted_status_id.notnull()].index, inplace=True)
df = pd.read_csv("detected.csv") $ df['META_b_cov'] = df['META_b'].where(df['metaREF']!=df['glmmREF'].str.lower(), -df['META_b'])
merge_DA_imb_power_df = imb_df.merge(merged_DA_power_df, on = ['Date', 'PTE'],\ $                                      how = 'inner')
plt.hist(traindata.rating) $ plt.title("Training Data Star to Own Distribution");
for u in session.query(User).order_by(User.id)[1:3]: $ ...    print(u)
plt.figure() $ plt.title('Team count') $ plt.scatter(filterdf['teamCount'], filterdf['best'], marker= 'o', s=20) $ plt.show() $
check_cols = ['teacher_prefix', 'school_state', $        'project_grade_category', $        'project_subject_categories', 'project_subject_subcategories', $        'teacher_number_of_previously_posted_projects', 'project_is_approved']
norm.ppf(1-(0.05/2))
youthUser4 = youthUser3.astype(object).fillna("UNKNOWN") $ youthUser4.head()
df1.device_id[0],df2.device_id[0],df3.device_id[0],df4.device_id[0],df5.device_id[0],df6.device_id[0]
n_new=len(df2.query('group=="treatment"')) $ print("n_new :{}".format(n_new))
vectorizer = CountVectorizer(stop_words=custom_stop_words) $ X = vectorizer.fit_transform(dfn['News'])
np.exp(-1.9888)
len(less_hacker_list)
twitter_df_clean.info()
num_rows = df.shape[0] $ num_rows
scores[3.5:].sum()/total
s = w.get_subset_object(subset_uuid).indicator_objects['din_winter'] $ s.get_filtered_data(subset = subset_uuid, step = 'step_2')
import numpy as np
import pandas as pd $ df = pd.read_csv("autos2.csv") $ df.dropna(how='any',inplace=True)    #to drop if any value in the row has a nan $ df.head()
knn = KNeighborsClassifier() $ pipe_knn = make_pipeline(cvec, knn) $ pipe_knn.fit(X_train, y_train) $ pipe_knn.score(X_test, y_test)
!ls
merged_data['invoices_creation_date'] = pd.to_datetime(merged_data['invoices_creation_date']) $ merged_data['customer_creation_date'] = pd.to_datetime(merged_data['customer_creation_date']) $ merged_data['due'] = pd.to_datetime(merged_data['due']) $ merged_data['last_payment_date'] = pd.to_datetime(merged_data['last_payment_date'])
sensor.system
print("zscore: {}, pvalue: {}".format(z_score, p_value));
for school in bigten: $     c = tp.Cursor(api.search, q=school, lang="en") $     for status in c.items(100): $         tempDict = {"School":school, "Text": status.text, "Location":status.geo, "Time":status.created_at} $         df = df.append(tempDict, ignore_index=True) $
week19 = week18.rename(columns={133:'133'}) $ stocks = stocks.rename(columns={'Week 18':'Week 19','126':'133'}) $ week19 = pd.merge(stocks,week19,on=['133','Tickers']) $ week19.drop_duplicates(subset='Link',inplace=True)
times.ncattrs()[0]+': '+times.units
pickle.dump(best_dt, open('xgb_model.pkl', 'wb'))
print(1/np.exp(-0.0408)) $ print(1/np.exp(0.0099))
data.sort_values('fix_duration', ascending=False)
etf_weights = calculate_dividend_weights(dividends) $ project_helper.plot_weights(etf_weights, 'ETF Weights')
df['state'].value_counts()
_ = ok.grade('q08a') $ _ = ok.backup()
df[df['RT']==False][['op','text','time','rtcount']].sort_values(by='rtcount',ascending=False).head()
req_body = None $ with open(datafile, "r") as f: $     req_body = json.loads(f.read()) $ log.info("loaded request_file={} which will be POST-ed as an HTTP JSON dictionary={}".format(datafile, ppj(req_body)))
psy.shape
data.dtypes
autos["odometer_km"].value_counts().sort_index(ascending = True)
helpthispersonpls297 = df5['HT'].where(df5['andrew_id_hash'] == '21cb1f9adbacb7cf4bfb15085966652648310c05').dropna() $ plt.hist(helpthispersonpls297, bins=20)
bow_corpus  = [dictionary.doc2bow(text) for text in list(repos)] $ index = SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12418)
import os $ REGION = 'us-central1'    # region of cloud storage bucket $ BUCKET = 'qwiklabs-gcp-0bb127b38c8a3a29'    # bucket name on the cloud ML engine $ PROJECT = 'qwiklabs-gcp-0bb127b38c8a3a29'    # project name
df2.query('user_id == 773192').index.get_values()
data.groupby('Agency').size().sort_values(ascending=False)
potential_accounts_buildings_info_tbrr = pd.merge(accounts, potential_accounts_buildings_info,\ $          on=['Account ID'],\ $          how='inner')
frames = [df_1, df_2] $ btc_price_df = pd.concat(frames)
f_top_domains = '/scratch/olympus/projects/ideology_scaling/congress/top_domains.json' $ with open(f_top_domains , 'w+') as f: $     f.write(json.dumps(domain_counter)) $ shutil.chown(f_top_domains, group='smapp')
tweets_clean.favorites_count.mean()
df2.info()
frame3
df['fullVisitorId'] = df['fullVisitorId'].astype('str')
data2= data.assign(SYMBOL=SYMBOLs, SOTERM=SOTERMs, FXNCLASS=FXNCLASSs) $ forIdx = data2["POS"].str.split(":", expand=True).applymap(int).sort_values(by=[0, 1]) $ df = data2.reindex(forIdx.index) $ df.to_csv("detected.csv", index=False)
data['2015']
plt.scatter(e_pos['Polarity'], e_pos['Subjectivity'], alpha=0.3, color='purple') $ plt.title('Positive tweets #Election2018, Subjectivity on Polarity') $ plt.ylabel('Subjectivity') $ plt.xlabel('Polarity') $ plt.show()
cohort_retention_sum = pd.Series([sum(cohort_retention_df[col]) for col in cohort_retention_df.columns],index=cohort_retention_df.columns,name='Total')
df_EMR_with_dummies = pd.concat([df_not_categorical, df_dummies], axis=1)
tokens = [] $ for t in nltk.regexp_tokenize(text.lower(), '[a-z]+'): $     if t not in sr: $         tokens.append(t) $ tokens[:10]
dfRegMet = df[df["latitude"] > -34.164060]
df_parsed.head()
df_merge.columns
stock_data.Date
executable_path = {'executable_path': 'chromedriver.exe'} $ browser = Browser('chrome', **executable_path, headless=False)
length = len(df2.query("group == 'control'")) $ pr=len(df2.query("group=='control' & converted == 1")) $ prob = pr/length $ prob
vectorizer = TfidfVectorizer(stop_words=stops, use_idf=True, ngram_range=(1,3), max_df=0.2) $ X = vectorizer.fit_transform(testParsedPD) $ X[0]
temp_df = officers[(officers.type == 'Person') & (officers.position == 'director')].groupby(['first_name','last_name','partial_date_of_birth','address.postal_code'])[['company_number']].agg(unique_company_count).sort_values(by='company_number',ascending=False) $ temp_df = temp_df[temp_df.company_number > 100] $ temp_df = temp_df.reset_index() $ temp_df.dropna(inplace=True) $ len(temp_df)
merged1 = merged1.rename(columns={'Name':'OfficeName', 'id_x':'id'})
unitech_df.dtypes
rfc.fit(features_class_norm, overdue_transf)
turnstiles_df.sort_values(["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"], inplace=True, ascending=False) $ turnstiles_df.drop_duplicates(subset=["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"], inplace=True)
def func8(x): $     return pd.Series([x.min(), x.mean(), x.max()], $                   index=['MIN.', 'MEAN.', 'MAX.']) $ df.apply(lambda x: func8(x))
df
data.info()
access_key = os.environ['AWS_ACCESS_KEY_ID'] $ secret_access_key = os.environ['AWS_SECRET_ACCESS_KEY'] $ conn = boto.connect_s3(access_key, secret_access_key) $ conn.get_all_buckets()
wrd_clean['tweet_id'] = wrd_clean['tweet_id'].astype('str') $ wrd_clean['in_reply_to_status_id'] = wrd_clean['in_reply_to_status_id'].astype('str') $ wrd_clean['in_reply_to_user_id'] = wrd_clean['in_reply_to_user_id'].astype('str') $ wrd_clean['retweeted_status_id'] = wrd_clean['retweeted_status_id'].astype('str') $ wrd_clean['retweeted_status_user_id'] = wrd_clean['retweeted_status_user_id'].astype('str')
top_20_breeds = tweet_archive_master['dog_breed'].value_counts().index[:20]
pre_strategy = people_person.date < '2017-04-12'
num_of_converted = df2[df2.converted == 1] $ p_old = len(num_of_converted)/len(df2) $ p_old
libraries_metadata_df = pd.DataFrame(libraries_metadata_dicts).T $ libraries_metadata_df.head()
american_ratings, american_counts = topic_ratings_all(american_dictionary, american_train_model, cat_american, 'reviews_without_rare_words', 10)
samp311.head(4)
sn.distplot(train_binary_dummy['unique_device'])
bowie = {'name': {'first': 'David', $                   'last': 'Bowie', $                   'middle': 'Robert'}, $          'born': datetime(1949, 1, 8)}
loading_scores = pd.DataFrame(pca.components_[0], index = pivoted.index.values) $ sorted_loading_scores = loading_scores.abs().sort_values( by = [0],ascending=False) $ top_ten_genes = sorted_loading_scores[0:10].index $ loading_scores[loading_scores.index.isin(top_ten_genes)].sort_values(by=0,ascending=False)
result_concat = pd.concat([city_loc, city_pop]) $ result_concat
users_num = Users[['Reg_date', 'id_partner', 'name']].assign(num_users=lambda x: 1) $ users_num = users_num.groupby(['id_partner', 'name', 'Reg_date'], as_index=False).sum() $ users_cost = pd.merge(Users, users_num, how = 'left', on=['Reg_date', 'id_partner', 'name']) $ del users_num
smart_authors.head()
id_pickup_label.shape, id_dropoff_label.shape
vis = final[['BTC Price', 'BTC Price Change', 'BTC Volume', $        'ETH Price', 'ETH Price Change', 'ETH Volume', $       'Crypto Compound','Crypto Positive', 'Crypto Negative', 'Crypto Neutral']] $ viscorr = vis.corr() $ viscov = vis.cov()
os.chdir(root_dir + "data/") $ df_fda_drugs_reported.to_csv("filtered_fda_drug_reports.csv", index=False)
X_test.shape
df_new['US_ab_page'] = df_new['US']*df_new['ab_page'] $ df_new['CA_ab_page'] = df_new['CA']*df_new['ab_page']
y = K.dot(x, W) + b $ loss = K.categorical_crossentropy(y, target)
fe.bs.csv2ret??
from ipywidgets import interact, interactive, fixed, interact_manual $ import ipywidgets as widgets $ from IPython.display import display $ import numpy as np $ from scipy.optimize import curve_fit as cf
screen_name = 'seanhannity'
clean_archive.columns
old_page_converted = np.random.choice([1,0], size = n_old, p = [0.1196,1-0.1196]) $ old_page_converted
indices = df.query('group != "treatment" and landing_page == "new_page" or group == "treatment" and landing_page != "new_page"').index $ df2 = df.drop(indices)
set(train_data['gearbox'])
o = pd.read_sql_query(QUERY, conn) $ o
df_favored = df_stars[df_stars.stars > 4] $ user_favored = df_favored[df_favored.user_id == df_favored.user_id.iloc[0]] $ df_similar_items[df_similar_items['business_id'].isin(user_favored.business_id)].sort_values('score',ascending=False).similar[:5]
c.execute('SELECT city, max(average_high) FROM weather') $ print(c.fetchone())
one_test_pvalue = 1 - 0.19/2 $ one_test_pvalue
df2.drop(mismatch_treatment_to_old_pg.index, inplace=True) $ df2.drop(mismatch_control_to_new_page.index, inplace=True)
train['age'] = train['age'].apply(lambda x: ageTransform(x))
df1.describe()
Meter1=MultiMeter34401A('MM1') $ Meter1.MakeConnection(UnLockedPort)
pred_probas_over_k150 = gs_k150.predict_proba(X_test)
df_group_by2 = copy.copy(df_group_by)
df2.shape
print("The average daily trading volume is %3.3f"%(df["Traded Volume"].mean()))
df.info()
l_norm=(l-np.min(l))/(np.max(l)-np.min(l))
pc_cz_fl.index.values
stop_words = set(stopwords.words('english')) $ for i in range(0,len(twitter_final)): $     data= ''.join(twitter_final['text']) $ words = data.split()
print(testObj)
bufferdf.Fare_amount[(bufferdf.Fare_amount==2) | (bufferdf.Fare_amount==3) | (bufferdf.Fare_amount==4)].apply(int).size
countries_df['country'].unique()
df_merged = pd.merge(df_merged, pred_clean, on='tweet_id', how='inner') $ df_merged.info()
extract_field(tmp["results"]).head()
dr = webdriver.Chrome()
pd.DataFrame(cats['SexuponOutcome'].value_counts()).plot(kind='bar')
import pandas as pd $ country_data = pd.read_csv('countries_all_2.csv') $ country_data.shape
clean_rates.sample()
df2 = pd.DataFrame([[2,3],[4,5]],index=['a','c']) $ df = df2.append(df2) $ print(df)
df_protest.describe()
df.info() #.Additional check to note the null values. 
dates = pd.date_range('2010-01-01', '2010-12-31') $ symbols = ['GOOG', 'IBM', 'GLD']  # SPY will be added in get_data() $ df = get_data(symbols, dates) $ sd,ed = '2010-05-01','2010-09-30'
total_sales = total_sales.dropna() $ total_sales.head()
sns.barplot(y = X.columns, x = list(logr.coef_.reshape(-1)))
print('No, there are no rows with missing values.')
tweets_per_hour=convert_index_hour(df_tweet_hour_count) $ tweets_per_hour_df=pd.DataFrame(tweets_per_hour) $ tweets_per_hour_df
df['created_at'].head()
import test_package.print_hello_function_container $ test_package.print_hello_function_container.print_hello_function()
joined_samp.head(2)
df['SALEDATE'] = pd.to_datetime(df['SALEDATE'], errors='coerce')
df['Injury_Type'].value_counts()
!mkdir data
df
print(df.user_answer.nunique()) $ print(sorted(df.user_answer.unique())[:5])
for row in session.query(User, User.name).all(): $ ...    print(row.User, row.name)
knn.predict(X)
vwg['season'] = vwg.index.str.split('.').str[0] $ vwg['term'] = vwg.index.str.split('.').str[1]
autos.brand.value_counts().head(20)
player['events'].unique()
week38 = week37.rename(columns={266:'266'}) $ stocks = stocks.rename(columns={'Week 37':'Week 38','259':'266'}) $ week38 = pd.merge(stocks,week38,on=['266','Tickers']) $ week38.drop_duplicates(subset='Link',inplace=True)
filtered.sort('streamTweets', ascending=False, inplace=True) $ filtered.head()
(group.shift(1).results == 'Fail').cumsum()
test = pd.read_csv('test.csv', sep=',') $ print(type(test)) $ test.head(15)
import time $ from datetime import datetime $ at = datetime.strptime("2016-10-23 21:50:00.000", "%Y-%m-%d %H:%M:%S.%f") $ current_time = at.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ') $ datetime.timestamp(at)
print(get_price(COIN)) $ print(get_price(COIN_LIST)) $ print(get_price(COIN_LIST, ['USD', 'ETH', 'LTC']))
experiment_details = client.experiments.get_details(experiment_uid)
poo = df[df.Year_of_Release.isin(df.release_year)==False] $ poo[['Year_of_Release','release_year', 'Name','Platform']].head()
StockData.columns
%time pd.to_datetime(df_columns['Created Date'].head(10000), format="%m/%d/%Y %H:%M:%S %p") $
vals = Inspection_duplicates.index.values $ vals = list (vals) $ vals
t['Counts'] = t['Counts'].apply(lambda x: x*100/202)
fm = pd.merge(crime_df, weather_df, how='left', on=None, left_on='Date_Time', right_on='Date_ TimeCST', $       left_index=False, right_index=False, sort=True, $       suffixes=('_x', '_y'), copy=True, indicator=False)
tweet_image_clean.head(2)
linkNYC = pd.read_csv('linkNYClocations.csv') $ linkNYC = linkNYC.iloc[:,1:]
to_be_predicted_Day3 = 22.24302433 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
archive_df.rating_denominator.value_counts()
print('No of repeated entries: {}'.format(sum(df.duplicated()))) $ n_unique_users = df['user_id'].unique().shape[0] $ print (n_unique_users)
%matplotlib inline $ import sys $ import pandas as pd $ from sklearn import preprocessing $ from matplotlib import pyplot as plt $
df.to_pickle('data/df.pkl')
flatten = lambda l: [item for sublist in l for item in sublist]
archive_copy = pd.merge(left=archive_copy, right=tweet_data_copy, left_on='tweet_id', right_on='id', how='inner')
condos = pd.merge(condos, mar, left_on='FULLADDRESS',  right_on='full_address')
import requests $
california_averages = california.groupby('FIRE_YEAR')['FIRE_SIZE'].mean() $ california_averages.plot(y= 'FIRE_SIZE', x= 'FIRE_YEAR')
m.sched.plot(100)
tw = tweets_in_period(datetime(2015, 9, 18, 16, 0, 0)) $ tw
test_portfolio = ew_portfolio_ts[ew_portfolio_ts['ticker'].isin(['AMD', 'UA'])].copy()
df=pd.DataFrame({"Source":source,"Date":date,"Text":text,"Positive Score":posScore,"Negative Score":negScore, $                 "Neutral Score":neutralScore,"Compounded Score":compoundedScore}, $                 columns=["Source","Date","Positive Score","Negative Score","Neutral Score","Compounded Score","Text"]) $ df.head()
np.sin(1)
from bs4 import BeautifulSoup as bs $ import requests
import statsmodels.formula.api as smf $ lm = smf.ols(formula='sales ~ TV', data=data).fit() $ lm.params
print(df2['landing_page'].value_counts()[0]/len(df2))
(autos["last_seen"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index() $ )
tuna_neg_cnt = tuna_neg.count()*100 $ print "{:,} users have no activity after {} ({:.2%} of DAU)"\ $       .format(tuna_neg_cnt, D0.isoformat(), tuna_neg_cnt*1./dau)
show_author('fossasia')
1/np.exp(-0.0150)
df_joined = df2.join(df3.set_index('user_id'),on ='user_id') $
pax_raw = pax_raw.merge(n_user_days, on='seqn', how='inner')
data.loc[data.surface_covered_in_m2 > data.surface_total_in_m2, 'surface_covered_in_m2'] = np.NaN
mars_html_table = mars_table.to_html() $
LinkNYCpLagQ10 = ps.Quantiles(LinkNYCpLag,k=5)
p_new = df2['converted'].mean() $ print("The converted rate for P(new) = ", p_new)
sorted(CheckNull.items(), key=operator.itemgetter(1), reverse=False)
suspects_with_25_1 = suspects_with_1T_25[suspects_with_1T_25['imsi'] == '525016DD781A0783B4BD6F8D2BE9489D48675B']
so_hashlist.columns = ["emailhash"]      $ gh_hashlist.columns = ["emailhash"]      
df_new[['ca', 'uk', 'us']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop('ca', axis = 1) $ df_new.head()
points2=pd.Series(points_dic,index=["Spain","India","China","France"]) $ points2
df2_control = df2.query("group == 'control'")
df = pd.read_csv('data/btc-market-price.csv')
deltat.days
movies.isnull().sum() #checking for missing values
tau_p = 150 $ K_p = 0.33 $ theta = 15
plt.hist(p_diffs); $
import pandas as pd $ import numpy as np $ import seaborn as sns $ import matplotlib.pyplot as plt $ from datetime import datetime
so.loc[:, col_bools].head()
support=merge[merge.committee_position=="SUPPORT"]
df = pd.read_csv("training.1600000.processed.noemoticon.csv", $                  header=None, names=cols, encoding='latin-1')
%%R $ head(flightsDB)
GBR.score(X_train,Y_train)
users.shape
topics = pd.DataFrame(interests)
tips.groupby(["sex","size"]).mean().loc[:,"total_bill"].loc["Female",3:5]
autos['price']=autos['price'].str.replace('$','').str.replace(',','').astype(int) $
all_data_as_dfs = {f'{apiname}_{access}' : get_cleaned_dataframe(apiname, access, start, end) for $                    (apiname, access, start, end) in data_specs} $ len(all_data_as_dfs)
ans = df.groupby('A').sum() $ ans
facts_metrics.id.nunique()
df2[df2.landing_page == 'new_page'].shape[0]/ df2.shape[0]
df2[df2['user_id'] == 773192].index $
response = requests.get( $   'https://panel.sendcloud.sc/api/v2/parcels/statuses', $     auth=('key', 'secret_key'))
soup.li.parent.name
myplot_parts = [go.Scatter(x=counts["stamp"],y=counts["count"],mode="line",name="Total"), $                 go.Scatter(x=person_counts["stamp"],y=person_counts["count"],mode="line",name="KianMcIan")] $ mylayout = go.Layout(autosize=False, width=1000,height=500) $ myfigure = go.Figure(data = myplot_parts, layout = mylayout) $ iplot(myfigure,filename="crisis")
writers['Age'].iloc[0]
left = pd.DataFrame({'key' : ['foo', 'bar'], 'lval': [1,2]}) $ left
recent_prcp_data = session.query(Measurement.date, Measurement.prcp).\ $     filter(Measurement.date >= YrFrombd).\ $     order_by(Measurement.date).all() $ print(recent_prcp_data) $
pulledTweets_df = gu.read_pickle_obj(processed_dir+'pulledTweetsProcessedAndClassified_df')
stats['deep_review_post_submission_authors'] = len(new_authors)
%%time $ sl_pf_v2 = f_import_suchi_perf('06-11')
df_final.shape[0]
import pandas as pd $ import numpy as np
all_df.columns
df_clean = df.copy()
(df2.converted).mean()
import scipy.optimize
lag_list = list(df.columns.values)
cov_df.head(7)
n_duo = pd.Categorical((df.landing_page.astype(str)+df.group.astype(str))).value_counts() # number of each duo (newpage_control, new_pagetratment, etc) $ dontlineup = n_duo['new_pagecontrol']+n_duo['old_pagetreatment'] $ print ("The number of times the new_page and treatment do not line up is: {}".format(dontlineup))
df_R.info()  #274 rows
df.tail(3) # See the bottom 3 rows
df = pd.DataFrame() $ print(df)
one_df = df.query("group == 'treatment' & landing_page == 'new_page'") $ other_df = df.query("group == 'control' & landing_page == 'old_page'") $ df2= one_df.append(other_df) $ df2.head()
group_techniques[0]
Pipeline.head()
cc['name'].describe()
firstday_df.dtypes
prcp_data_df.head()
tizibika[tizibika['likes']==tizibika['likes'].max()]
df= pd.read_csv("D:/Aditya/datasets/used-cars-database/autos.csv", encoding='cp1252') $ df.sample(10)
local_folder = "C:/Users/Max81007/Desktop/Python/Resource_Watch/Raster/ene_018/" $ file_name = "ene_018_wind_energy_potential.tif" $ local_orig = local_folder + file_name $ orig_extension_length = 4 #4 for each char in .tif $ local_edit = local_orig[:-orig_extension_length] + "_edit.tif" 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) $ regr = LinearRegression() $ regr.fit(X_train, y_train)
difference = [] $ for entry in d["dataset_data"]["data"]: $     difference.append(entry[2]-entry[3]) $ print("The maimum difference in a given was $" + str(max(difference)))
len(df[~(df.data == {})])
ts[::2].index
df3_month.plot(x='Donation Received Month-Year', y='Total Donations', kind='line') $ df3_month.plot(x='Donation Received Month-Year', y='Mean Donation', kind='line') $ df3_month.plot(x='Donation Received Month-Year', y='Count of Donations', kind='line') $ df3_month.plot(x='Donation Received Month-Year', y='Count of Unique Donors', kind='line') $ plt.show() $
autos["price"].values
from sklearn.metrics import accuracy_score $ accuracy_score(y_test, y_predict)
fig, ax = plt.subplots(figsize=(12, 8)) $ sar.plot_diagnostics(fig=fig);
start = datetime.now() $ modelxg5 = xgb.XGBClassifier(max_depth=10, learning_rate=.05, n_estimators=250, n_jobs=-1) $ modelxg5.fit(Xtr.toarray(), ytr) $ print(modelxg5.score(Xte.toarray(), yte)) $ print((datetime.now() - start).seconds)
index_df = index_df[start:end]
elms_all = elms_sl.append(elms_pl, ignore_index=True) $ elms_all = elms_all.append(elms_tl, ignore_index=True) $ elms_all['SSN'] = [float(x) for x in elms_all.SSN.values]
def get_integer6(s): $     return notRepairedDamage_list.index(s)
plt.hist(null_vals, bins=21) $ plt.axvline(p_convert_obs_diff, color='green') $ plt.axvline(np.percentile(null_vals, 95), color='red');
tf.logging.set_verbosity(tf.logging.INFO) $ shutil.rmtree('taxi_trained', ignore_errors=True) # start fresh each time $ model = tf.contrib.learn.LinearRegressor( $       feature_columns=make_feature_cols(), model_dir='taxi_trained') $ model.fit(input_fn=make_input_fn(df_train), steps=10);
first_mscore = first_movie.find('span', class_ = 'metascore favorable') $ first_mscore = int(first_mscore.text) $ print(first_mscore)
convert_old = df2.loc[(df2.landing_page == "old_page") & (df2.converted == 1)].user_id.nunique() $ convert_old
df1.sort_values(by=['date'], ascending=True, inplace=True)
cnx = mysql.connector.connect(user=cred['username'], password=cred['password_indiana'], $                               host=cred['host_indiana'],port='6612', $                               database='')
for df in (joined, joined_test): $   df['CompetitionMonthsOpen']=df['CompetitionDaysOpen']//30 $   df.loc[df['CompetitionMonthsOpen']>24,'CompetitionMonthsOpen']=24 $ joined['CompetitionMonthsOpen'].unique()   
df_new.head()
items2 = [{'bikes': 20, 'pants': 30, 'watches': 35}, $           {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5}] $ store_items = pd.DataFrame(items2) $ store_items
%matplotlib inline $ mpl.rcParams["font.size"] = 16 $ mpl.rcParams["figure.figsize"] = (14, 7)
sub_dataset.groupby(["NewsDesk", "SectionName", "Popular"]).size()
autos['ad_created'].str[:10].value_counts().sort_index()
pd_review["text"][0]
cityID = 'c7ef5f3368b68777' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Baton_Rouge.append(tweet) 
import pandas as pd $ import numpy as np $ topUserItemDocs=pd.read_pickle('datasets/topUserItemDocsCB_27Jun_vFULL300.pkl')
def to_pickle(filename,objname): $     with open(filename, 'wb') as f: $         pickle.dump(objname, f)
df = df[df['type']=='User'] $ df.drop(['type'], axis=1, inplace=True)
Google_stock
crimes.describe()
df_log.num_of_people.value_counts()
df.rename(index={'Goose Island - Honkers Ale - 6 Pack': 'We changed this'}).head(15) $
obs_diff_mean = new_page_converted.mean() - old_page_converted.mean() $ obs_diff_mean
np.mean(p_new1) - np.mean(p_old1) $
job_status = sqlClient.wait_for_job(jobId) $ print("Job " + jobId + " terminated with status: " + job_status) $ if job_status == 'failed': $     details = sqlClient.get_job(jobId) $     print("Error: {}\nError Message: {}".format(details['error'], details['error_message']))
def pysqldf(q): $     return sqldf(q, globals())
noNulls = df.dropna(how='any')
dat['NY.GDP.PCAP.KD'].groupby(level=0).mean()
height.isna()
p.asfreq('M', how='end')
rain_df.describe()
cum_cont = pd.DataFrame(data={"cum_contr":np.cumsum(sorted_by_date.contb_receipt_amt),"date":get_date(sorted_by_date.contb_receipt_dt)}) $ cum_cont.head()
social_disorder = pd.DataFrame(social_disorder.groupby("Census Tract").mean().mean(axis=1)) $ social_disorder.columns = ["Social Disorder"] $ social_disorder
uber_15["day_of_week"] = uber_15["Pickup_date"].apply(lambda x: getDayofWeek(x, 2015)) $ uber_15.head()
df['stemmed'] = df["message_tokens"].apply(lambda x: [stemmer.stem(y) for y in x])
import pandas as pd
log_price = test_df['Log_Price_Sqft'] $ lpmu, lpstd = np.mean(log_price), np.std(log_price) $ X_cols = test_df.as_matrix()
m3.clip=25. $ lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])
import pandas as pd $ import numpy as np $ from datetime import datetime
import requests $ from collections import OrderedDict $
pd.isnull(obj4)
from scipy.stats import norm
df = pd.read_csv('../input/crime.csv') $ df.head()
excelDF.columns
re.sub('https?://[A-Za-z0-9./]+','',df.text[0])
pr = nx.pagerank(G) $ def prf(xs): $     return np.array([pr[x] for x in xs]) $ prDf = pd.DataFrame.from_dict(pr, orient='index') $ prDf.describe()
df['btime'] = df.apply(lambda row: phimage(row.name).BTime(), axis = 1)
res_Mix
print(131/0)
len(options_data)
pd.DataFrame ([[101,'Alice',40000,2017], $                [102,'Bob',  24000, 2017], $                [103,'Charles',31000,2017]] )
old_page_converted = np.random.choice(2, size = n_old, p=[0.8805, 0.1195]) $
archive_clean.stage = archive_clean.stage.astype('category')
df_aggregate = df_list[0] $ for i in range(1, 9): $     df_aggregate = pd.concat([df_aggregate, df_list[i]])
d = np.array([0, -1]) $ print(a * d) $ for i in range(2): $     print(a[i] * d)
data.index = pd.to_datetime(data.index) $ data.interpolate(method='time',inplace=True) $
LATENT_DIM = 5 $ BATCH_SIZE = 32 $ EPOCHS = 50
rs.best_score_
np.save(embedding_save_path, embeddings_matrix) $
df_2013['bank_name'] = df_2013.bank_name.str.split(",").str[0] $
tia['salary_clean'] = tia['salary'].apply(clean_salary) $ tia.head()
data_archie['cur_sell_price'].min()
coin_data.head(5)
df2["intercept"] = 1 $ df2[["not_ab", "ab_page"]] = pd.get_dummies(df2["group"]) $ df2 = df2.drop(["not_ab"], axis=1) $ df2.head(1) $
df_final = pd.concat([df_us_, df_cat, df_crea, df_loc], axis=1, join='inner')
obj4
from pyspark.sql import Row $ rdd_example3 = rdd_example2.map(lambda x: Row(id=x[0], val1=x[1], val2=x[2])) $ print rdd_example3.collect() $
for sent in fullData['change'].value_counts().keys(): $     subset = fullData[fullData['change'] == sent] $     plt.plot(subset['compound']**2, label=sent) $ plt.legend()
graf_counts2['precinct'] = graf_counts2['precinct'].astype(int)
entries = requests.get('https://api.apis.guru/v2/list.json')
df_json_tweet = pd.DataFrame(extended_tweet_data, columns=['tweet_id', 'favorite_count','retweet_count']) $ df_json_tweet.head()
type(df_clean['date'].iloc[0]) $ type(df_clean['time'].iloc[0])
n_control = df2.query('group == "control"').shape[0] $ n_control
with open('hashtags/hashtags.csv', 'w') as f: $     [f.write('{0},{1}\n'.format(tag, val)) for tag, val in tag_cloud.items()]
from IPython.display import Image $ from IPython.core.display import HTML $ Image(url= "https://pbs.twimg.com/media/CmgBZ7kWcAAlzFD.jpg")
to_be_predicted_Day4 = 14.52772653 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
for v in squares:  # calls .__iter__() $     print(v)
df_dateorder = df.sort_values('timestamp', ascending=True) $ df_dateorder.head(), df_dateorder.tail()
os.getcwd()
day_of_month14 = uber_14["day_of_month"].value_counts().sort_index().to_frame() $ day_of_month14.head()
data.info()
sample.head()
to_be_predicted_Day1 = 37.79 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
tweet_counts_by_hour.plot(subplots=True)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ print("The z score calculated is = ", z_score) $ print("The P value calculated is = ", p_value)
liquor['Category'] = liquor['Category'].map(lambda x: int(x)) $ liquor['County Number'] = liquor['County Number'].map(lambda x: int(x))
pd.read_pickle("baseball_pickle")
events = df.sort_values(['game_date', 'at_bat_number'], ascending=True).groupby(['atbat_pk']).last().reset_index() $ events = events[['atbat_pk', 'events', 'woba_value']]
users.columns
tweets['user_id'].value_counts()[:10]
np.random.seed(1) $ rs = 1
df2['user_id'].nunique()
lr2 = LogisticRegression() $ params2 = {'penalty': ['l1', 'l2'], 'C':np.logspace(-5,0,100)} $ gs_2 = GridSearchCV(lr2, param_grid=params2, cv=10, verbose=1) $ gs_2.fit(X2, y2)
max(X_test_pred)-min(X_test_pred)
doctors = doctors[columns] $ RNPA = RNPA[columns] $ therapists = therapists[columns]
chart = top_supporters.head(5).amount.plot.barh()
df2[df2['user_id'] == 773192]
result['modifiedBy'].value_counts()
df2['intercept']=1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group']) # create dummy variable columns for 'group' $ df2['ab_page'] = df2['treatment']
n_old=df2.landing_page.value_counts()[1] $ n_old
old_page_converted=np.random.choice([1,0],size=n_old,p=[p_old,1-p_old]) $ old_page_converted.mean()
print("\nTopics in LDA model:") $ tf_feature_names = tf_vectorizer.get_feature_names() $ print_top_words(lda, tf_feature_names, n_top_words)
!pip install patsy $ from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt $ import matplotlib.pyplot as plt $ import pandas as pd
p_diffs = [] $ for _ in range(10000): $     p_new = np.random.choice([0,1], size=n_new, p=[1-p_new, p_new]).mean() $     p_old = np.random.choice([0,1], size=n_old, p=[1-p_old, p_old]).mean() $     p_diffs.append(p_new - p_old)
X_train = train.drop(['Patient','Readmitted','DaysSinceAdmission','<=30Days'],axis=1) $ y_train = train['<=30Days'] $ X_test = test.drop(['Patient','Readmitted','DaysSinceAdmission','<=30Days'],axis=1) $ y_test = test['<=30Days']
with_condition_heatmap_quer5 = folium.Map([41.9029,-87.7070],zoom_start=11) $ print("Filtered records for query 5 :  ",len(list_query_5))
df['dateCrawled'] = df['dateCrawled'].astype('datetime64') $ df['dateCreated'] = df['dateCreated'].astype('datetime64') $ df['lastSeen'] = df['lastSeen'].astype('datetime64[ns]')
afl_data = afl_data.append(next_week_df).reset_index(drop=True) $ match_results = afl_data_cleaning_v2.get_cleaned_match_results().append(next_week_df) $ odds = (afl_data_cleaning_v2.get_cleaned_odds().pipe(lambda df: df.append(next_week_df[df.columns])) $        .reset_index(drop=True))
log_reg = LogisticRegression() $ log_reg.fit(X_train_counts, y_train) $ log_reg.score(X_test_counts, y_test)
predictions_clean.tweet_id = predictions_clean.tweet_id.astype(str)
pd.read_sql('select * from sessions limit 5;',cnx)
measure_nan = measure[measure.isnull().any(axis=1)]
print len(liveonly_live_woc)
z_score, p_value = sm.stats.proportions_ztest([convert_new,convert_old],[n_new,n_old], alternative = 'larger') $ print('z score is '+ str(z_score)) $ print('p_value is '+str(p_value))
tbl
print("Number of duplicated rows: "+ str(df.duplicated().sum())) #There are no duplicated rows $ print("Number of duplicated user id's: " + str(df["user_id"].duplicated().sum())) $ df.loc[df["user_id"].duplicated(keep = False), :] 
df_con.to_csv('titles_scores', index=False)
raw_df.workflow_version.value_counts()
dir(scipy.optimize)
z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new],[n_old,n_new])
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results = logit_mod.fit() $
df2['user_id'].drop_duplicates(inplace=True) $ df2['user_id'].duplicated().count()
result.index
df_goog['Year'] = df_goog.index.year $ df_goog['Month'] = df_goog.index.month $ df_goog['Day'] = df_goog.index.day
np.exp(0.0149)
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US']]); $ results2 = logit_mod.fit() $ results2.summary()
5/9*(5.88)
df_artist.isna().sum()
from sklearn.model_selection import train_test_split $ from sklearn.linear_model import LogisticRegression $ from sklearn.linear_model import LinearRegression
df = pd.DataFrame({'text':tweets, 'createdTime':createdTime})
active_station = session.query(Measurements.station,func.count(Measurements.date)) \ $              .group_by(Measurements.station)
print pd.pivot_table(data=cust_demo, index='Location', columns='Gender', values='age',aggfunc='mean' )
df.plot(subplots=True) $ plt.show()
X.head()
df.converted.values.sum()/row_num
run_augmented_Dickey_Fuller_test(doc_duration, num_diffs=2)
no_specialty.columns
train_doc = doc_id_list[['training' in doc_id for doc_id in doc_id_list]] $ print("train_doc is created with following document names: {} ...".format(train_doc[0:5]))
df_mes['average_speed'] = df_mes['trip_distance'] / df_mes['travel_time'] *3600
subrcvec1.head()
bd = birth_dates.set_index("Name").head(3) $ bd
Z = np.random.random(10) $ Z.sort() $ print(Z)
[ list(calendar.day_name)[i] for i in customer_visitors.DateCol.dt.dayofweek]
students[3:7]
df_never_moved = df_never_moved.drop_duplicates(subset='id')
github_data.head(2)
min_avail = df.cust_avail_v3.min() $ max_avail = df.cust_avail_v3.max() $ size = df.cust_avail_v3.size $ cust_avail_v3 = np.random.uniform(min_avail, max_avail, size) $ cust_avail_v3
departure_datetimes = pd.to_datetime(df2['FlightDate'] + ' ' + df2['DepTimeStr']) $ df2['DepDateTime'] = departure_datetimes
token = pd.merge(token,empInfo[["sender","senderlevel"]],how="left",on = "sender") $ token = pd.merge(token,empInfo[["receiver","receiverlevel"]],how="left",on = "receiver")
diabetes_conditions.limit(10).toPandas()
 twitter_merged_data = pd.read_csv('twitter_archive_master.csv') 
BroncosBillsTweets['text30'] = BroncosBillsTweets['text'].apply(lambda x: x[:30])
import IPython  # for displaying parse trees inline $ for tree in parser.parse(sentence): $     IPython.display.display(tree)  # instead of tree.draw()
str(containers[0].find("li", {"class":"transaction"}).a["title"])
po= df2.query("converted==1").count()[0]/len(df2) $ po
features.head()
df.T.describe().T["count"].plot()
p_new = df2['converted'].mean() $ print("The convert rate for p_new under the null: ",p_new)
data = pd.Series(["quick", "brown", "fox"], name="Fox") $ data
converted = df2.query('converted == 1') $ converted.user_id.nunique()/df2.user_id.nunique()
df_o = df2.query('landing_page == "old_page"') $ df_o.shape[0]
stops_per_crime_per_month.index
df.dtypes
setup
for i in range(100,110): $     print reddit.title.values[i], analyzer.polarity_scores(reddit.title.values[i]) $     print 'Length: ', len(reddit.title.values[i].split())
for i in gen: $     print(i)
plt.figure(figsize=(16, 5)) $ plt.plot(news.date, news.bullets.apply(len));
df = pd.read_csv('./data/FB.csv')
df.isnull().sum() #Counts all null values
import seaborn as sns
pd.get_dummies(cust_demo['Martial_Status'], prefix="D").head(10)
df_dates_final = df_merged.groupby(df_merged.columns[0]).min() $ df_dates_final.head()
print(len(pos_tweets)) $ print(len(neu_tweets)) $ print(len(neg_tweets))
iowa.describe()
print(mbti_text_collection_filler.drop('text', axis=1)[mbti_text_collection_filler.text_count>50])
anomaly_df.Date = anomaly_df.Date.apply(lambda d: datetime.strptime(d, '%Y-%m-%d') )
df.dtypes
df_usa['GDP/capita'] = df_usa['GDP']/df_usa['Total population']
pd.merge(pd.concat([df_a, df_b]), df_c, on = "mathdad_id") # concatenate 2 mathdad df and then merge with skill level df
print ("Filtered records for all match :  ", len(final_location_ll))
data = {} $ data['name'] = 'pratap' $ json_obj = json.loads('{"names": {"name1":"pratap","name2":"swetha"}}') $ json_data = json.dumps(data) $ print (json_obj['names'])
stx = ha.accounts.manual_current('dbxtrack_stoxx_50', path=os.path.join('data', 'manual_accounts'), $                                  currency='LU0380865021') $ stx.add_transaction('13.08.2013', 'me', 'buy', 139, t_type='buying rate: 34.52 EUR')
sm.Logit(complete_df['converted'], complete_df[['intercept', 'ab_page', 'CA', 'UK']]).fit().summary()
mb.swaplevel('Patient', 'Taxon').head()
df_adjusted['adjusted_numerator']=  df_adjusted.rating_numerator_y.iloc[0]* df_adjusted.rating_numerator_x/df_adjusted.rating_numerator_y
import json $ with open('jallikattu.json', 'r') as f: $     line = f.readline() # read only the first tweet/line $     tweet = json.loads(line) # load it as Python dict $     print(json.dumps(tweet, indent=4)) # pretty-print
integratedData = pd.merge(xmlData, csvData, how = 'outer', indicator = 'True', on = [u'date', u'price', u'bedrooms', u'bathrooms', u'floors', u'waterfront', $        u'view', u'yr_built', u'yr_renovated', u'sqft_above', u'sqft_basement', $        u'sqft_living', u'sqft_lot', u'street', u'city', u'statezip', $        u'country'])
es['payments']
plt.figure(figsize=(16,8)) $ dendrogram(features['cnn']['linkage'], orientation='top', $           p=300, truncate_mode='lastp', no_labels=True, color_threshold=0) $ plt.axes().get_yaxis().set_visible(False) $ plt.show()
df_new['intercept'] =1 $ log_mod2= sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US']]) $ result2 = log_mod2.fit() $ result2.summary()
max_tweets=1 $ for tweet in tweepy.Cursor(api.search,q="Dreamers").items(max_tweets): $     print(tweet)
from pyspark.ml.evaluation import MulticlassClassificationEvaluator $ evaluator = MulticlassClassificationEvaluator( $     labelCol="label", predictionCol="prediction", metricName="accuracy") $ accuracy = evaluator.evaluate(predictions) $ print("Accuracy = %g " % (accuracy))
df_new[(df_new['group'] == 'control')]['converted'].mean() $
plt.figure(figsize=(12,8)) $ sns.violinplot(x='source', y= 'num_words', data= trump) $ plt.xlabel(' Source of Tweet', fontsize=12) $ plt.ylabel('NUmber of Words', fontsize=12) $ plt.title('Number of Words per Tweet by Source', fontsize=15)
live_weights.plot.hist(rwidth=.9, bins=np.arange(0,16,1)) $ plt.xlabel('weight')
train.popular.value_counts(normalize=True)
log_loss(y_test, clf.predict_proba(X_test))
weights_A=pd.Series([85.1, 90.2, 76.8, 80.4,78.9],index=['s1','s2','s3','s4','s5'])
p_old = df2['converted'].mean() $ p_old
import pandas as pd $ disaster_tables = pd.read_html("https://en.wikipedia.org/wiki/List_of_accidents_and_disasters_by_death_toll", header = 0)
cv_score.mean()
import pickle $ with open('/tmp/mtuberculosis_gp_atlas/model/mtuberculosis_gp_atlas.pckl', 'rb') as f: $     my_saved_gempro = pickle.load(f)
average_polarity.to_csv('polarity_results_LexiconBased/monthly/polarity_avg_monthly_2012_2016.csv', index=None) $ count_polarity.to_csv('polarity_results_LexiconBased/monthly/polarity_count_monthly_2012_2016.csv', index=None)
archive_clean = archive_df $ image_clean = image_file $ tweet_clean = tweet_df $
mydmh = MyDMH()
np.exp(-0.0150)
control = df2['group'] == 'control' $ obs_conv_old = df2[control]['converted'].mean() $ print(obs_conv_old)
pt_weekly.drop(pt_weekly.index[[3, 5, 8]], inplace=True) $ pt_weekly.head(7)
min_max_dict_model = Model.register(model_path = MIN_MAX_DICT_PATH, $                        model_name = MIN_MAX_DICT, $                        tags = [TICKER, "MinMaxDict"], $                        description = "MIN_MAX dictionary use to normalization of "+ TICKER +" stock data", $                        workspace = ws)
grouped_authors_by_publication.tail()
df.num_comments = df.num_comments.apply(lambda x: x.replace(' comments', ''))
bb_df = bb_df.dropna(how='any') $ len(bb_df)
kate_df = df[katespadeseries]
df = pd.read_csv('Reddit06022018.csv',index_col ='Unnamed: 0' , engine='python')
autos["gearbox"].value_counts()
learner= md.get_model(opt_fn, em_sz, nh, nl, $     dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4]) $ learner.metrics = [accuracy] $ learner.freeze_to(-1) # why are we unfreezing this one? Don't we want to unfreeze FIRST layer? $
import requests $ from collections import namedtuple $
((loans.originated_since_date<datetime.date(2015,2,28)) & (loans.payback_state=='payback_complete')).sum()
roc_auc_score(predictions, fb_test.popular)
tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['retweet_count']==tweet_archive_enhanced_clean['retweet_count'].max()]
df2_conv = df2.converted.mean() $ df2_conv
count_polarity
def process_abstract(abstract, latex_regex, latex_repl, whitespace_regex): $     abstract = latex_regex.sub(latex_repl, abstract) $     abstract = whitespace_regex.sub(' ', abstract) $     stop_words_removed = [ word for word in abstract.strip().split() if word.lower() not in ENGLISH_STOP_WORDS ] $     return ' '.join(stop_words_removed)
suburban_summary_table = pd.DataFrame({"Average Fare": suburban_avg_fare, $                                     "Total Rides": suburban_ride_total}) $ suburban_summary_table.head()
season_type_groups.ngroups
week27 = week26.rename(columns={189:'189'}) $ stocks = stocks.rename(columns={'Week 26':'Week 27','182':'189'}) $ week27 = pd.merge(stocks,week27,on=['189','Tickers']) $ week27.drop_duplicates(subset='Link',inplace=True)
weights = np.array([.25, .25, .25, .25]) $
from sklearn import linear_model
details.shape
train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1) $ test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1) $ combine = [train_df, test_df] $ train_df.head()
df = pd.read_csv('./ab_data.csv') $ df.head(5)
df = wrangle_df(df_merged)
Train.shape
len(df.query('converted == 1'))/df['user_id'].nunique()
dump["country"].value_counts().sum() # Calculates sum of users in every country listed in the Series
USvideos['category_id'] = USvideos['category_id'].astype(str) $ USvideos['category_name'] = USvideos.category_id.map(id_to_category)
json_data = req.json()
data_archie = data_archie[data_archie.cur_purchase_price != 0.0]
df_dummy = pd.get_dummies(data=df_countries, columns=['country']) $ df_new = df_dummy.merge(df3, on='user_id') $ df_new = df_new[['user_id', 'timestamp', 'group', 'landing_page', $                  'ab_page', 'intercept', 'converted', 'country_CA', 'country_UK', 'country_US']] $ df_new.head()
df_unique.to_csv('./data/unique_feature_list.csv', encoding='utf-8')
weather_mean.loc['HALIFAX':'OTTAWA', 'Rel Hum (%)':'Wind Spd (km/h)']
print(f"Now {urls[2]} returns:") $ ux.is_short(urls[2], list_of_domains=known_shorteners)
taxi_hourly_df = pd.read_csv('../clean_data/Aggregated_TaxiData_14-17.csv', index_col=0)
lsi_tf.save(os.path.join(outputs, 'model_tf.lsi')) $ corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lsi_tf.mm'), corpus_lsi_tf) $ lsi_tfidf.save(os.path.join(outputs, 'model_tfidf.lsi')) $ corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lsi_tfidf.mm'), corpus_lsi_tfidf)
dtm_df['news'].sum()
ll ../input/
pred = predict_class(np.array(theta), X_train_1) $ print ('Train Accuracy: %f' % ((y_train[(pred == y_train)].size / float(y_train.size)) * 100.0))
bigdf.loc[bigdf['comment_body'].str.contains('\r\r')]
df_input.toPandas().info()
import requests $ import json $ url = "https://www.dcard.tw/_api/posts?popular=false&before=228084271" $ resp = requests.get(url, headers=headers)
df.dropna(subset=['model','brand'],axis=0, how='any', thresh=None, inplace=False)
embeddings_index2 = dict() $ for i in range(len(word_list)): $     word = word_list[i] $     embeddings_index2[word] = np.asarray(model_ft[word], dtype='float32') $
datatest['floor'] = datatest['floor'].apply(lambda x: (float)((int)(x)))
summary = Logit(y, X).fit().summary() $ summary
n_old = len(df2.query('landing_page != "new_page"')) $ n_old
import pandas as pd $ %matplotlib inline
(new_page_converted/n_new)-(old_page_converted/n_old)
Results_kNN100.head()
new_page_converted = np.random.choice(2, n_new, p=[1-p_new,p_new]) $ new_page_converted
my_df["user_create"].plot() $ my_df["user_active"].plot() $ plt.ylabel("Nuser") $ plt.legend() $ plt.show()
data_compare['SA_textblob_de'].mean()
result2 = sm.ols(formula="port_ret ~ mkt_ret", data=tbl).fit() $ result2.summary()
df_coor.head(20)
autos.price.hist(bins = 30)
grouped_dpt.head(1)
glm_multi_v2.hit_ratio_table(valid=True)
dfv = pd.Series(documents)
zero_rev_acc_opps.rename(columns={'Building ID': 'Total Buildings'}, inplace=True)
df.pct_chg_opencls.describe()
print('Detecting language') $ large_post_df.loc[:, 'language'] = large_post_df.filtered_body.apply(lambda x: detect_language(x))
londonGeoDF = gpd.read_file('OA_2011_BGC_london.json') $ londonDFSubset = londonGeoDF.ix[:,['OA11CD', 'POPDEN', 'WD11NM_BF', 'geometry']]
all_data = pd.concat([concat, ti_mar[ti_mar['store'] == 'Suning']]) $ all_data.groupby('store').size() $
c[c['days_to_next_churn'] < 30]
import sys, os $ import numpy as np $ import urllib as ul $ import gwTools as gwt $ import pandas as pd
yhat.shape, X_test.shape,y_test.shape
pipeline = Pipeline([('posf', PartOfSpeechFilter()), $                      ('cv', CountVectorizer()) $                    ]) $ pipeline.set_params(cv__lowercase=True,cv__max_df=0.95, cv__min_df=0.01, cv__stop_words='english')
df = pd.DataFrame(rng.randint(0, 10, (3, 4)), $                  columns=['A', 'B', 'C', 'D']) $ df
california_house_dataframe = pd.read_csv("https://storage.googleapis.com/mledu-datasets/california_housing_train.csv", sep=',') $ california_house_dataframe.describe()
run txt2pdf.py -o '2018-07-09-2015-871 - SEPTICEMIA OR SEVERE SEPSIS Without MV 96+ HOURS W MCC.pdf'  '2018-07-09-2015-871 - SEPTICEMIA OR SEVERE SEPSIS Without MV 96+ HOURS W MCC.txt'
text = ','.join(wordlist) $ wordcloud = WordCloud().generate(text) $ plt.imshow(wordcloud, interpolation='bilinear') $ plt.axis("off") $ plt.show()
df_clean.name.value_counts()
pd.Series({2:'a', 1:'b', 3:'c'})
twitter_archive_enhanced[twitter_archive_enhanced.rating_denominator != 10][['rating_numerator','rating_denominator','text']]
print("The probability of converted:", df.converted.mean())
samp_size = n $ joined_samp = joined.set_index("Date")
print(contribs.info())
check_wait_corr().groupby('wait').mean()
import pandas as pd $ from pandas_datareader import data $ import numpy as np $ import matplotlib.pyplot as plt $ import datetime as dt    
df['Close'].pct_change(n).rolling(21)
plt.semilogx(alphas, scores)
import pandas as pd $ list = [11,22,33,44,55,66,77,88,99]
from sklearn.feature_extraction.text import TfidfVectorizer $ tfidf = TfidfVectorizer()
zip_1_df.head()
rGraphData = pd.DataFrame({'to': MultData.to_account, $                            'from': MultData.from_account, $                            'deposit': MultData.deposited, $                            'withdraw': MultData.withdrawn})
plt.hist(taxiData.Trip_distance, bins = 50, range = [10,20]) $ plt.xlabel('Traveled Trip Distance') $ plt.ylabel('Counts of occurrences') $ plt.title('Histogram of Trip_distance') $ plt.grid(True)
df_new = df_new.join(pd.get_dummies(df_new['country'])) $ df_new.head()
lq.columns = lq.columns.str.replace(' ','')
fig, ax = plt.subplots() $ investors_df.groupby('investor_type').size().plot(ax=ax,kind = 'bar',figsize = [10,6]) $ ax.set_xticklabels(set(investors_df.investor_type), rotation=0)
df['timestamp'] = pd.to_datetime(df.timestamp) $ df['ts_year'] = df['timestamp'].dt.year $ df['ts_dayofweek'] = df['timestamp'].dt.weekday_name $ df18 = df.loc[0:5,['timestamp','ts_year','ts_dayofweek']] $ df18
findM = re.compile(r'm[ae]n', re.IGNORECASE) $ for i in range(0, len(postsDF)): $ 	print(findM.findall(postsDF.iloc[i,0]))
(p_diffs > orig_diff).mean()
Data.tail()
test.show(5)
store_info.info()
p_old = df2['converted'].value_counts()[1]/len(df2) $ p_old
training_data = query_training_data(model) $ model.train_model(training_data) $ df_backcast = swag.control.training.run.create_backcast(model, start_date, end_date) $ backcast = df_backcast[0].pivot_table(index="Gas_Date", columns='Line_Item', values="Daily", aggfunc=np.sum).reset_index()
list(twitter_Archive.columns.values)
input_data['categories'] = pd.cut(input_data['normalized_commits'], bins=[0,0.95,3.7], labels=[0,1])
prec_group.precip_in.fillna(method='bfill', inplace=True)
festivals.dtypes
req = requests.get('https://api.coinmarketcap.com/v1/ticker/?limit=50') $ res = req.text $ cryptos = pd.read_json(res) $ cryptos.head() $
columns2=['eval_set','purchase_count_spec','reordered_count_spec'] $ test_orders_prodfill.drop(columns2, inplace=True, axis=1)
predictions = lrmodel.transform(testData)
df_clean.rating_numerator = df_clean.text.str.extract(pat='((?:\d+\.)?\d+)\/10')
stats_cols = ['Backers', 'Pledged (USD)', 'Goal (USD)', 'Funding %'] $ desc_stats = df[df.State.isin(['Successful', 'Failed'])].groupby('State')[stats_cols].describe() $ desc_stats
from spacy.pipeline import Pipe
analyze_set.loc[analyze_set['favorites']==162332]
train['diff_lng'] = train['end_lng'] - train['start_lng'] $ train['diff_lat'] = train['end_lat'] - train['start_lat'] $ test['diff_lng'] = test['end_lng'] - test['start_lng'] $ test['diff_lat'] = test['end_lat'] - test['start_lat']
df.isnull().sum()
irisRDD = SpContext.textFile("iris.csv") $ print (irisRDD.count())
A = data['Race'].value_counts() $ B = data['Race'].value_counts() $ B.plot(kind='bar') $ plt.show()
df
%run indebtedness.py
df.rename(columns={"Indicator":"Indicator_Id"},inplace=True)
suspects_with_25_2 = suspects_with_1T_25[suspects_with_1T_25['imsi'] == '5250168CED24C9D727C8DAD9CFDECBB3574414']
closePrice.max()
val=17.5 $ size = np.exp(val)/(1024*1024) $ percentage = 100*df.loc[df.Size < np.exp(val)].Size.count()/len(df) $ print('Setting mark for size at {0:.2f} MB will still give us {1:.2f} % of data objects'.format(size, percentage))
df_new['night']= pd.get_dummies(df_new['day_part'])['night']
df2=df2[['TRANSACTION_CURRENCY','VOL','FLOW','REVENUE']] $ df2.head()
dfFull['BsmtUnfSFNorm'] = dfFull.BsmtUnfSF.fillna(dfFull.BsmtUnfSF.mean())/dfFull.BsmtUnfSF.max()
ser[:1]
plt.figure(figsize=(15, 7)) $ df.public_repos.hist(log=True, bins=80);
autos.registration_year.describe()
session = Session(engine)
roc_auc_score(y_test, y_pred_rf)
ttarc.describe()
df_subset.plot(kind='scatter', x='Initial Cost', y='Total Est. Fee', rot=70) $ plt.show()
print(df_A.loc['s1']) $ print(df_A.iloc[1]) $
df_day['Forecast'] = bound_prediction(sarima_mod.predict()) $ df_day $ df_day.plot(figsize=(14, 6));
duration_df = merged2[['Provider', 'Specialty', 'AppointmentCreated', 'AppointmentDate', 'AppointmentDuration', $        'ReasonForVisitName', 'DurationHours', 'ReasonForVisitDescription','MeetingStatusName', 'MeetingStatusDescription', $        'OfficeId']]
try: $     temp_series_paris_naive.tz_localize("Europe/Paris") $ except Exception as e: $     print(type(e)) $     print(e)
full_globe_temp = pd.read_table(filename, sep="\s+", names=["year", "mean temp"], $                                 index_col=0, parse_dates=True) $ full_globe_temp
df_prep3 = df_prep(df3) $ df_prep3_ = pd.DataFrame({'date':df_prep3.index, 'values':df_prep3.values}, index=pd.to_datetime(df_prep3.index))
aux = image_clean.merge(dog_rates.loc[dog_rates.cuteness == 'puppo'], $                                                            how='inner', on='tweet_id') $ aux[['jpg_url','grade']].sort_values(['grade'], ascending=False).iloc[:,0].values[:3]
df[treatment]['converted'].mean()
print(f"Station which has the highest number of observations: {data_stations[0]}")
print(applications.shape) $ print(applications.dtypes) $ print(applications.head(5))
new_TC_file = folder + "\TC-all.txt" $ new_TC_file
ncTest
tvec_df.head(25)
master = master() $ master = master[['bbrefID', 'bats']] $ names = names.merge(master, left_on='key_bbref', right_on = 'bbrefID', suffixes=('_chadwick', '_lahman')) $ names = pd.DataFrame(names[['key_mlbam', 'batter_name', 'bats']])
score_merkmale.to_clipboard(encoding='UTF-8')
for file in my_zip.namelist(): $     print(ds100_utils.head(data_dir/file, 5))
months = original_months $ months.head(5) $ print(months.shape)
lq.dtypes
loan_stats["loan_result"].head(rows=2)
n_new = df2.query('landing_page=="new_page"').shape[0] $ n_new
date_null_agg=pd.DataFrame(nulls['date'].value_counts()) $ date_null_agg.columns=['frequency'] $ date_null_agg['date']=date_null_agg.index $ pd.DataFrame.head(date_null_agg)
non_grad_GPA_mean = records3[records3['Graduated'] == 'No']['GPA'].mean() $ non_grad_GPA_mean
df1.describe()
df_concat_2.message_likes_rel = np.where(df_concat_2.message_likes_rel > 10000, 10000, df_concat_2.message_likes_rel)
from nltk.tokenize import word_tokenize $ words = word_tokenize(text) $ print(words)
intake['IntakeDate'] = pd.to_datetime(intake['DateTime'],format ='%m/%d/%Y %I:%M:%S %p' ) $ intake = intake.drop(['DateTime','MonthYear','Found Location'],axis=1)
new_page_converted = np.random.choice(2, n_new, p= [1-p_new,p_new]) $ new_page_converted
with open("Politician_and_Events","wb") as f: $     pickle.dump(NameEvents,f) $
import statsmodels.api as sm $ log = sm.Logit(df['converted'], df[['intercept', 'treatment']])
Lab7_Redesign.tail()
dates_list=list(dates_list) $ dateset=pd.DataFrame({'date':dates_list,'type':'train'}) $ dateset=dateset.sort_values(ascending=True,by="date") $ dateset.head()
impressions.head(2)
secret_company_at_top_of_chain = pd.DataFrame(graph.run("MATCH p=(c1:Company)<-[:CONTROLS*0..]-(c2:Company)\ $ WHERE c2.uid IN {list_of_secretly_controlled_companies}\ $ RETURN DISTINCT (c1.company_number)",list_of_secretly_controlled_companies=list_of_secretly_controlled_companies).data()) $ len(secret_company_at_top_of_chain)
members = pd.read_csv(f'{PARTITION_DIR}/members.csv', $                       parse_dates=['registration_init_time'], infer_datetime_format = True) $ trans = pd.read_csv(f'{PARTITION_DIR}/transactions.csv', $                    parse_dates=['transaction_date', 'membership_expire_date'], infer_datetime_format = True) $ logs = pd.read_csv(f'{PARTITION_DIR}/logs.csv', parse_dates = ['date'])
import bs4 as bs #importing beautiful script $ import urllib.request # to request a url $ import time $ import requests $ from bs4.element import Comment
mit.describe(include='all')
cust_demo.sort_values(by=['Location', 'Gender'], ascending=[False, True]).head(50) $
archive_clean['stage'].value_counts()
flowerData = irisRDD.map( lambda x: ( x.split(",")[4], \ $     x.split(",")[0])) $ print (flowerData.take(5)) $ print ("\n") $ print (flowerData.keys().collect()) $
df_temp_diff_redL["Position"] = ["left" for x in range(len(df_temp_diff_redL))] $ df_temp_diff_redM["Position"] = ["middle" for x in range(len(df_temp_diff_redM))] $ df_temp_diff_redR["Position"] = ["right" for x in range(len(df_temp_diff_redR))]
df_c=pd.read_csv('countries.csv') $ df_c.head()
df=pd.read_json('tweet_json.txt', lines=True) $
%writefile /tmp/test.json $ {"dayofweek": "Sun", "hourofday": 17, "pickuplon": -73.885262, "pickuplat": 40.773008, "dropofflon": -73.987232, "dropofflat": 40.732403, "passengers": 2}
autos["odometer_km"].value_counts().sort_index(ascending=False)
pd.set_option('display.mpl_style', 'default') $ plt.rcParams['figure.figsize'] = (15, 5)
simResist_rootDistExp_1 = simResist_rootDistExp[0] $ simResist_rootDistExp_0_5 = simResist_rootDistExp[1] $ simResist_rootDistExp_0_25 = simResist_rootDistExp[2]
chart = top_supporters.head(5).amount.plot.barh() $ chart.set_yticklabels(top_supporters.contributor_lastname)
dfPre = df['1930-01-01':'1979-12-31'] $ dfPost = df['1984-01-01':'2017-12-31']
collocate = lambda model, obs: model.load().sel( $     lon=obs.lon, lat=obs.lat, time=obs.time, method='nearest' $ )
neuron_no = 10 $ source_indices_L23exc_L23fs = np.where(np.array(conn_L23exc_L23fs.i)==neuron_no) $ target_indices_L23exc_L23fs = np.array(conn_L23exc_L23fs.j)[source_indices_L23exc_L23fs]
df.iloc[2893]
talks['text'] = text
com_grp
data_donald_replies.to_csv("compared_sentiments.csv")
w = np.zeros(shape=(2, 1)) $ w[0] = m $ w[1] = c
vals2 = np.array([1, np.nan, 3, 4]) $ vals2.dtype $
lr = LogisticRegression() $ train = lr.fit(X_train, y_train) $ print(X_train.shape, y_train.shape) $ print('Training set score:', lr.score(X_train, y_train)) $ print('\nTest set score:', lr.score(X_test, y_test))
del databreach_2017['Unnamed: 0']
merged1 = merged1[ordered_columns]
df_test_2.set_index('user_id') $ df_test_2.head()
auto_new.Body_Type.unique()
sgd = SGDClassifier() $ sgd.fit(X_train, Y_train) $ Y_pred = sgd.predict(X_test) $ acc_sgd = round(sgd.score(X_test, Y_test) * 100, 2) $ acc_sgd
df.groupby('userid')['price'].max()
ScoreLabel = 'HDWPSRRating' $ A = ScoreToProbViaIntegral(Score, ScoreLabel) $ dfX_hist['prob_'+ScoreLabel] = dfX_hist.groupby('race_id')[ScoreLabel].transform(lambda x:A(x))
print(np.info(np.ubyte))
actual_converted_diff = p_treatment_converted - p_control_converted $ plt.hist(p_diffs) $ plt.axvline(x=(actual_converted_diff), color='red') $
pd.Series(bnb.first_affiliate_tracked).isnull().sum()
active_raw_sample_sizes = active_num_authors_by_project.withColumn( $     "sample_size_1", $     compute_num_required_sample_1("author_count")).persist()
s[s > 100].head()
df_ml_6201_01.tail(5)
segmentData.opportunity_conversion.value_counts()
import pickle
new = len(df2.query("landing_page == 'new_page'")) $ new
pd.Period('2012-1-1 19:00', freq='H')
tweets.to_csv('tweets.csv')
autos = autos.drop(['seller','offer_type','nr_of_pictures'],axis=1)
corpusDF = corpusDF.groupby("URI").first() $ len(corpusDF)
df = pd.read_csv("tmdb2years.csv") $ df.head()
list(soup.children)
import requests $ import numpy as np $
dr.index
next_week_df = get_next_week_df(afl_data) $ game_ids_next_round = create_next_weeks_game_ids(afl_data) $ next_week_df
extractor = twitter_setup()
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_mean, (1-p_mean)]) $ old_page_converted.mean()
temp_df2['timestamp'].max() - temp_df2['timestamp'].min()
100.0*df_outcomes.sum()/len(df_outcomes)
import ffn $ df_portfolio_value = data['cumreturns'] $ perf = df_portfolio_value.calc_stats() $ perf.plot()
trunc_df.shape
groupby_example
temp_stations = pd.DataFrame(session.query(Measurement.date, Measurement.tobs).\ $                               filter(Measurement.date >= '2016-08-23').\ $                               filter(Measurement.station == 'USC00519281').order_by(Measurement.date).all()) $ temp_stations.set_index('date', inplace = True) $ temp_stations.hist(bins=12) $
autos["price"].unique().shape
plt.hist(full_clean_df['type']) $ plt.ylabel('Number of Doggies') $ plt.xlabel('Dog Type') $ plt.title('Which Dog Type is the Most Common')
height.iloc[0] = float('nan')
segments = pd.read_csv("transit_segments.csv", parse_dates=['st_time', 'end_time']) $
cityID = 'a84b808ce3f11719' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Omaha.append(tweet) 
control_converted = df2[df2['group'] == 'control']['converted'].mean() $ control_converted
print(full_df.shape)
df_arch_clean['source'] = df_arch_clean['source'].str.replace('<a href="http://twitter.com/download/iphone" rel="nofollow">Twitter for iPhone</a>', 'iPhone') $ df_arch_clean['source'] = df_arch_clean['source'].str.replace('<a href="http://vine.co" rel="nofollow">Vine - Make a Scene</a>', 'Vine') $ df_arch_clean['source'] = df_arch_clean['source'].str.replace('<a href="http://twitter.com" rel="nofollow">Twitter Web Client</a>', 'Web') $ df_arch_clean['source'] = df_arch_clean['source'].str.replace('<a href="https://about.twitter.com/products/tweetdeck" rel="nofollow">TweetDeck</a>', 'TweetDeck')
df.isnull().sum() $
spark = SparkContext() $ sql_context = SQLContext(spark)
final_dataframe.plot(kind='bar',stacked=True) $ import matplotlib.pyplot as plt $ plt.legend(bbox_to_anchor=(1.04,1),loc='upper left')
trump_tfidf = vect_tfidf.fit_transform(df_train) $ trump_cleaned_tfidf = vect_cleaned_tfidf.fit_transform(df_cleaned_train) $ trump_stemmed_tfidf = vect_stemmed_tfidf.fit_transform(df_stemmed_train)
pt_download=pd.DataFrame.pivot_table(df_download_node,index=['uid'],values=['nid'],aggfunc='count',fill_value=0) $ pt_download=pt_download.reset_index() $ pt_download=pt_download.rename(columns={'nid':'no_of_downloads'}) $ pt_download['downloaded_or_not']='Downloaded after 1st July 2018'
len(news_sentiment)
grades["dec"] = np.nan $ final_grades = grades + better_bonus_points $ final_grades
total_df['Res_id'].value_counts().head()
cassession.loadactionset('datastep') $
data['cumreturns']=1+(data['strategy'].cumsum()) $ data.head()
copy['Tokenized'] = copy['Text'].apply(preprocess) $ copy = copy[copy['Tokenized'].notnull()]
metrics.recall_score(y_valid, y_pred)
index_weights = generate_dollar_volume_weights(close, volume) $ helper.plot_weights(index_weights, 'Index Weights')
the_data = tmp_df.applymap(lambda x: 1 if x > 3 else 0).as_matrix() $ print(the_data.shape)
replace = {'I': 0, 'E': 1, $            'N': 0, 'S': 1, $            'F': 0, 'T': 1, $            'P': 0, 'J': 1, $            'A': 0, 'T': 1}
ddf.max().sort_values(ascending=False)
df_all_wells_wKNN.info()
pkl_file = open('think_tanks.pkl', 'rb') $ think_tank_party_dict = pickle.load(pkl_file) $ pkl_file = open('speeches_metadata.pkl', 'rb') $ speeches_cleaned = pickle.load(pkl_file)
cities = pd.DataFrame({ 'City name': city_names, 'Population': population }) $ cities['Area square miles'] = pd.Series([46.87, 176.53, 97.92]) $ cities['Population density'] = cities['Population'] / cities['Area square miles'] $ cities['Is wide and has saint name'] = (cities['Area square miles'] > 50) & cities['City name'].apply(lambda name: name.startswith('San')) $ cities
df['LOT'].value_counts()
autos['registration_year'].value_counts(normalize=True)
plt.violinplot(resampled1_groups['Sales_in_CAD'])
df_new = df_countries.set_index('user_id').join(df2.set_index('user_id'), how='inner') #joining countries_df with df2 $ df_new.head()#displays first 5 rows of df_new
training_data_path = 'train.csv' $ test_data_path = 'eval.csv' $ df_train.to_csv(training_data_path, header=False, index=False) $ df_eval.to_csv(test_data_path, header=False, index=False)
dfETHPriceC.corr() $
from sklearn.linear_model import LogisticRegression
pres_df['subjects'].nunique() # unique combinations of ad topics
! head failed_samples.txt
datesStr=dates.strftime('%Y-%m-%d')
parsed_locations = raw.location.str.extract(r'(?P<country>[\w ]+), (?P<city>[\w ]+) ! \2, \1', expand = True) $ parsed_locations.assign(original = raw.location).sample(5)
print "The percentage the total NetOut of the total NetIn in 2016 was " + str((df2['NetOut'].sum() / df2['NetIn'].sum()) * 100) + "%"
conn.droptable('data.iris', caslib='casuser')
np.exp(-0.0408), np.exp(0.0099)
df_new.groupby('country').nunique()
discrp = austin[ (austin['miles'] > austin['total_fare']) & (austin['miles'] > 100) & (austin['total_fare']!=0) ] $ discrp[['started_on', 'completed_on', 'distance_travelled', 'miles', 'total_fare', 'tip', 'rate_per_mile']]
tickers = ['AAPL', 'AMZN', 'MSFT', 'AA', 'KO'] $ start_date = '2016-01-01' $ closes = get_historical_closes(tickers, start_date=start_date) $ closes.plot(figsize=(8,6));
features, feature_names = ft.dfs(entityset=es, target_entity='clients', $                                  max_depth = 2)
ridge3 = linear_model.Ridge(alpha=1) $ ridge3.fit(x3, y) $ (ridge3.coef_, ridge3.intercept_)
git_log.index = git_log['timestamp']
prop = props[props.prop_name=="PROPOSITION 064- MARIJUANA LEGALIZATION. INITIATIVE STATUTE."]
lab.info()
metadata['spatial_extent'] = refldata.attrs['Spatial_Extent_meters'] $ metadata
df2.drop_duplicates(['user_id'], keep='first', inplace=True)
moving_average = close_ys.rolling(center=False,window=10).mean() $ moving_average
y_train_hat = model.predict(X_train) $ y_train_hat = squeeze(y_train_hat) $ CheckAccuracy(y_train, y_train_hat)
import pandas as pd $ import numpy as np $ import pickle $ from alpha_vantage.timeseries import TimeSeries $ ts = TimeSeries(key='NXY0VT9AHBRYGKKC',output_format='pandas') $
data_df = pd.read_csv('classif_is_high_val.csv', na_values=['NA']) $ data_df.info()
album_name = [playlist['tracks']['items'][i]['track']['album']['name'] for i in range(0,num_songs)] $ album_release_date = [playlist['tracks']['items'][i]['track']['album']['release_date'] for i in range(0,num_songs)]
ab_df.converted.mean()
highmeans = cc.groupby(['name'])['high'].mean()
plt.scatter(cc['open'],cc['high']) $ plt.title("Opening vs High Value") #Change x and y axis scale $ plt.show()
scores = [('Minnesota','Washington'),('Washington', 'Celtics'),('LA Lakers', 'Warriors'),('Cleveland','Portland'),('New Orleans', 'San Antonio'), ('Cleveland', 'Chicago'),('Oklahoma City', 'Thunder'), ('Oklahoma City', 'Boston'), ('Atlanta', 'Houston'),('Toronto','Boston')] $ X_test = build_test_set(scores, 3, 13, 2018) $ predictions(X,Y,X_test,True)
y_id = test['listing_id'].astype('O')
bathrooms = train_df['bathrooms'].value_counts() $ x = bathrooms.index $ y = bathrooms.values $ sns.barplot(x, y )
endDate = datetime.strptime("2017-07-08 00:00:00.000000", '%Y-%m-%d %H:%M:%S.%f') $ firstWeekUserMerged = userMerged[userMerged['time_stamp2'] < endDate] $ print("the most recent activity is :",max(firstWeekUserMerged.time_stamp2)) $ firstWeekUserMerged.shape $
df_lm.keys()
n_new=df2[df2['landing_page']=='new_page']['user_id'].count() $ n_new
df.head()
from kipoi_veff.parsers import KipoiVCFParser $ vcf_reader = KipoiVCFParser("example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf") $ print(list(vcf_reader.kipoi_parsed_colnames.values()))
Multiplication = df.loc['Total']['AveragePrice'] * df.iloc[18248]['AveragePrice'] $ print(Multiplication)
plt.figure(figsize=(15, 7)) $ df.following.hist(log=True, bins=80);
plt.hist(np.log(house_data['price']))
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA', 'US']] $ df_new.head()
new_model.wv.vocab
tweet_train_data = df_train['Tweet_text'].tolist() $ tweet_train_target = df_train['Attribute'].tolist() $ tweet_test_data = df_test['Tweet_text'].tolist() $ tweet_test_target = df_test['Attribute'].tolist()
treat_oldp = df[(df['group'] == 'treatment') & (df['landing_page'] == 'old_page')] $ ctrl_newp = df[(df['group'] == 'control') & (df['landing_page'] == 'new_page')] $ print(treat_oldp.shape) $ print(ctrl_newp.shape)
tweets_prediction.info()
warnings.filterwarnings("ignore", 'This pattern has match groups') $ faulty_rating_id = archive_clean[archive_clean.text.str.contains( r"(\d+\.?\d*\/\d+\.?\d*\D+\d+\.?\d*\/\d+\.?\d*)")].tweet_id $
ice_df.dtypes
preproc_reviews = pipeline.fit_transform(review_body) $ pipe_cv = pipeline.named_steps['cv']
test_df_01.loc[test_df_01['predict'] != test_df_01['label'], 'imagePath']
logit4 = sm.Logit(df3['converted'], df3[['intercept','new_page','UK_new_page','US_new_page','UK','US']]) $ result4 = logit4.fit() $ result4.summary()
bnb[bnb['age']<100].plot(kind='hist', y='age') $
%matplotlib inline $
train_data.fuelType.fillna('benzin', inplace = True) $ test_data.fuelType.fillna('benzin', inplace = True)
ind = 9
yar = train.project_subject_categories.apply(lambda x:yup(x)) $ p = pd.get_dummies(yar.apply(pd.Series).stack()).sum(level=0).reset_index(drop=True) $ del p['Warmth']
RandomTwoDF = RowedRDD.toDF() $ RandomTwoDF.registerTempTable("RandomTwo") $ print type(RandomTwoDF)
df_questionable_3[df_questionable_3['state_NY'] == 1]['link.domain_resolved'].value_counts()
tweet1 = result[1] #get the data of the first tweet... $ dir(tweet1)
from sklearn.feature_extraction.text import TfidfVectorizer $ vect_tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words = 'english') $ vect_cleaned_tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words = 'english') $ vect_stemmed_tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words = 'english') $
A = pd.DataFrame(np.random.randint(0, 20, (2, 2)), $                  columns=list('AB')) $ A
pd.Series({2:'a', 1:'b', 3:'c'}, index=[3, 2])
df2 = df2.join(countries_df.set_index('user_id'), on='user_id') $ df2.head()
loan_requests_indebtedness.head()
groceries * 2
df_ab.info() #From the output we know that the answer is no, all the rows are complete
df_goog.head(3)
mmx = MinMaxScaler() $ %time train_4_reduced = mmx.fit_transform(train_4_reduced)
df2.corr()
df_final.sort_values(by='Pct_Passing_Overall', ascending=False).tail(5)
p_old = df2.converted.mean() $ p_old $
top5_days= df['Unique Key'].resample('D').count().sort_values(ascending=False).head(5) $ top5_days
autos["registration_year"].describe()
data = pd.read_csv("MSFT.csv", index_col='Date') $ data.index = pd.to_datetime(data.index)
bwd.head()
df.replace({'city': {'': np.nan}, 'state': {'': np.nan}}, inplace=True) $ df.set_index(['zipcode'], inplace=True) $ zipcodes.set_index(['zipcode'], inplace=True) $ df.update(zipcodes, join='left', overwrite=False, raise_conflict=False)
users.location.fillna('\\N',inplace = True) $ users.company.fillna('\\N',inplace = True)
dat_wkd = data.groupby(['admission_type','inday_icu_wkd'])['hospital_expire_flag'].mean().reset_index() $ dat_wkd = dat_wkd.pivot(index='inday_icu_wkd', columns='admission_type', values='hospital_expire_flag') $ dat_wkd.head()
add_datepart(date_info, "calendar_date", drop=False)
import yaml
df2.query('group=="control"').converted.mean()
userByCountry_df  = youtube_df[youtube_df["channel_title"].isin(channel_namesC)]
atdist_opp_dist_noinfo_count_prop_overall = compute_count_prop_overall(atdist_opp_dist[~atdist_opp_dist['infoIncluded']], 'emaResponse') $ atdist_opp_dist_noinfo_count_prop_overall
df_all_wells_basic = turnDictOfWellDfs_to_SingleDfOfAllWells(dictOfWellDf) $ print(type(df_all_wells_basic))
df2_copy.drop(['landing_page_old','ab_page_control'], axis=1, inplace=True)
summed.fillna(method='pad')  # The NaN column remained the same, but values were propagated forward $
call_hist.iloc[-1]
frame.reindex(['a', 'b', 'c', 'd'], ['Texas', 'Utah', 'California'])
linkNYC['days'] = pd.to_datetime(linkNYC.date_link_)
from bs4 import BeautifulSoup $ soup=BeautifulSoup(_html,"html.parser")
df = pd.DataFrame([]) $ for i in range(0,1000000): $     x = next(meta) #next item in the generator $     df2 = pd.DataFrame.from_dict(x,orient='index').transpose() $     df = df.append(df2,ignore_index=True)
evaluator.get_metrics('micro_avg_accuracy')
data.head()
dogs.dropna(how='any')
Tradeday = namedtuple('Tradeday', ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ex_Dividend', $                                    'Split_Ratio', 'Adj_Open', 'Adj_High', 'Adj_Low', 'Adj_Close', 'Adj_Volume']) $ data = {d[0] : Tradeday(*d) for d in json_data['dataset_data']['data']}
print 'See correlation with actual: ',test_case.select('party_name').take(1) $ actual_acct_id.select('party_name').distinct().show(10,False)
fires = pd.read_sql_query("SELECT * from fires", conn) # I get a no such table error here
df_twitter_archive_copy.rating_numerator.describe()
df2.query('landing_page=="new_page"').count()[0]/df2.count()[0]
iris_df.describe()
df_trends = pd.read_sql(query, con=engine) $ df_trends.set_index('name')
autos['odometer_km'].describe()
words_scrape, corpus_tweets_scraped = count_freq(tweets_l_scrape) $ wordfreq = FreqDist(words_scrape) $ print('The 100 most frequent terms, including special terms: ', wordfreq.most_common(100))
order_data['CohortGroup'] = order_data.groupby(level =0)['created'].min().apply(next_weekday)
ten = pd.merge(left=free_data.groupby('educ')['v1','v2','v3','v4','v5','v6'].mean().idxmax(1).to_frame(), right = free_data.groupby('educ')['v1','v2','v3','v4','v5','v6'].median().idxmax(1).to_frame(), left_index=True, right_index=True) $ ten.columns = ['mean', 'median'] $ ten
df_btc.head()
twitter_archive_master.rating_numerator.plot(kind='box');
vio2016 = vio2016.rename(index=str, columns={"description_y": "num_vio"}) $ ins2016 = pd.merge(ins2016, vi, on=["business_id", "date"]).rename(index=str, columns={"description": "num_vio"}) $ ins2016
print(len(pbptweets))
print(cross_val_score(tuned_forest_model,X_train,y_train,cv=3,scoring='recall'))
df2['converted'].mean()
raw_df = data.import_raw_data() $ raw_sample_df = raw_df.sample(4, random_state=42) $ display(raw_sample_df.set_index('Name').T)
df_train= Daily_Price[len(Daily_Price)-days_look-days_from_end:len(Daily_Price)-days_from_train] $ df_test= Daily_Price[len(Daily_Price)-days_from_train:] $ print(len(df_train), len(df_test))
lr = 5e-4 $ learn.fit(lr, 10, cycle_len=1, use_clr=(10,10))
df_q = pd.read_sql(query, conn, index_col='work_order_id') $ df_q.head(5)
index_to_change = df3[df3['group']=='treatment'].index $ df3.set_value(index=index_to_change, col='ab_page', value=1) $ df3.set_value(index=df3.index, col='intercept', value=1) $ df3[['intercept', 'ab_page']] = df3[['intercept', 'ab_page']].astype(int) $ df3 = df3[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
df_prep14 = df_prep(df14) $ df_prep14_ = pd.DataFrame({'date':df_prep14.index, 'values':df_prep14.values}, index=pd.to_datetime(df_prep14.index))
dfFull.TotalBsmtSF = dfFull.TotalBsmtSF.fillna(dfFull.TotalBsmtSF.mean())
df3[df3['group']=='treatment'].head()
mean_sea_level = pd.DataFrame({"northern_hem": northern_sea_level["msl_ib(mm)"].values, $                                "southern_hem": southern_sea_level["msl_ib(mm)"].values}, $                                index = northern_sea_level.year) $ mean_sea_level
df_uv = df.query('landing_page != "new_page"') $ df_vu = df_uv.query('group == "treatment"') $ df_vu.count() $
im_clean.query('p1_dog == True').p1.value_counts()[0:3]
to_be_predicted_Day1 = 22.00 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
dfFull['GarageAreaNorm'] = dfFull.GarageArea/dfFull.GarageArea.max()
df2[df2['user_id'] == 773192]
twitter_archive_df_clean['expanded_urls'].fillna("Unsure", inplace=True)
data_sets = {} $ data_sets['1min'] = pd.read_pickle('final_1min.pickle') $ data_sets['3min'] = pd.read_pickle('final_3min.pickle') $ data_sets['15min'] = pd.read_pickle('final_15min.pickle') $ data_sets['60min'] = pd.read_pickle('final_60min.pickle')
leg
df_con_treat = df_con1.query('group =="treatment"') $ x_treat = df_con_treat["user_id"].count() $ x_treat $
itos2 = pickle.load((WT_PATH / 'itos_wt103.pkl').open('rb')) $ stoi2 = collections.defaultdict(lambda: -1, {v: k for k, v in enumerate(itos2)})
merged1['DaysFromAppointmentCreatedToVisit'] = (merged1['AppointmentDate'] - merged1['AppointmentCreated']).dt.days
%load "solutions/sol_2_36.py"
print(get_historical_price_timestamp(COIN))
bad_tc_e_isbns = pd.read_table("C:/Users/kjthomps/Documents/GOBI holdings reports/IDs to exclude/TC electronic ISBNs to exclude.txt") $ bad_tc_e_isbns = bad_tc_e_isbns['ISBN'] $ bad_tc_e_isbns.size
store_items.count()
dfss.head()
x = np.random.rand(5e7) $ %timeit np.power(x,3) $ %timeit x*x*x $ %timeit np.einsum('i,i,i->i',x,x,x)
states.columns
import pandas as pd $ import matplotlib as plt
all_sites_with_unique_id_nums_and_names.head()
matt_tweets = pd.DataFrame(Matt) $ matt_tweets
test_features = spark.read.csv(os.path.join(mungepath,"model_data/20180504/rf_lr_lasso_inter2_noip/test_features/*"), header=True) $ print("Number of observations in test :", test_features.count())
result['type'].value_counts()
df3.groupby('OriginCityName')['Origin'].count().head() # number of flights per Origin
result5 = sm.ols(formula="HPr_RF ~ Mkt_RF", data=tbl3).fit() $ result5.summary()
df_h1b_mv_ft.pw_1.hist(bins=20,figsize=(8,8))
model_path = DATAPATH + 'models/'
adopted_cats['Y']=0 $ adopted_cats.loc[adopted_cats['DaysInShelter']<29,'Y']=1
energy_cpi = abs_to_df(response_json) $ energy_cpi
total1 = pd.read_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_1/target_winter2017_night2.csv',\ $                     keep_default_na=False, na_values=[""]) $ total1[80:]
df_image_clean2.info()
beirut['Mean Humidity'].std(), summer['Mean Humidity'].std()
new_page_converted = np.random.binomial(n_new,p_new) $ new_page_converted
tips.columns
resultvalue_df['valuedtoff'] = resultvalue_df.apply(lambda x: pd.to_datetime(x['valuedatetime']).tz_localize(dateutil.tz.tzoffset(None, $                                                                                                                                int(x['valuedatetimeutcoffset'])*60*60)).astimezone(pytz.UTC), axis=1)
scaled_X = Df_X.drop(['id', 'author', 'text', 'spacy_text','lemma', 'created_at'], 1) $ scaled_X = scalar.fit_transform(scaled_X) $ y = Df_X.author $ X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, random_state=24)
Pold = df2.query('converted == 1').user_id.nunique()  / df2.user_id.count() $ Pold
geocoded_df.loc[idx,['Case.File.Date','Judgment.Date','Case.Duration']].head()
df2[df2['user_id'].duplicated(keep=False)]
autos = autos[autos["year_of_registration"].between(1910,2016)] $ autos["year_of_registration"].value_counts(normalize=True).head(10)
dr_new = doctors[doctors['ReasonForVisitDescription'].str.contains('New')] $ dr_existing = doctors[~doctors['ReasonForVisitDescription'].str.contains('New')] $ keep_cols = ['Follow up Telepsychiatry', 'Follow up', 'Therapy Telepsychiatry', 'Returning Patient', 'Returning Patient MD Adult'] $ dr_existing = dr_existing[dr_existing['ReasonForVisitName'].isin(keep_cols)]
df6.stack()
popt_axial_brace_saddle, pcov_axial_brace_saddle = fit(d_axial_brace_saddle)
states.set_index(['location', 'day'], inplace=True) $ states
df2.drop([2893], inplace=True)
potential_accounts_buildings_info_tbrr_temp = potential_accounts_buildings_info_tbrr[potential_accounts_buildings_info_tbrr['# Buildings not on Net'] > 0] $ potential_accounts_buildings_info_tbrr_temp.sort_values(by='# Buildings on Net', ascending=False, inplace=True)
loans_plan_all_xirr=cashflows_plan_investor_all.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
type(g_geo)
july = goodWeather.ix[datetime(2014,7,1) : datetime(2014,7,31)] $ july[['Precipitationmm', 'Apparent Mean Temp C', 'Apparent Max Temp C']].plot(grid=True, figsize=(10,5))
data_archie.columns.values
shows.iloc[row:row+50,:]
nar5=nar4.merge(loans[['fk_loan','rating_base','rating_switch']],on='fk_loan')
cheap_budget_vote = sorted_budget_cheapest.groupby('original_title')['vote_average'].mean()
tweet_dict = TweetMiner(twitter_keys, api).mine_user_tweets()
y_pred_cont_norm = (y_pred_cont == y_pred_cont.max(axis=1)[:,None]).astype(int) $ y_pred_cont_norm_df = pd.DataFrame(y_pred_cont_norm,copy=True) $ y_test_cont_df = pd.DataFrame(y_test_cont,copy=True)
df_birth.population.plot('hist') $ plt.show()
raw_test_df.shape
from scipy.stats import norm $ norm.cdf(z_score)
columns_to_drop
dict_campaign_per_list = {} $ for i in list(set(df_campaigns['List'])): $     temp_var = i.lower() $     dict_campaign_per_list[temp_var] = df_campaigns.groupby("List").count()['Title'][i]
df['user_id'].nunique()
corr_matrix = telecom3.corr().abs() $ upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)) $ to_drop = [column for column in upper.columns if any(upper[column] > 0.80)] $ print(to_drop) $ telecom3 = telecom3.drop(set(to_drop), axis=1)
sns.distplot(dfz.favorite_count, color = 'red', label = 'Favorites') $
mars_fact_url = "https://space-facts.com/mars/" $ mars_facts = pd.read_html(mars_fact_url) $ mars_facts_df=pd.DataFrame(mars_facts[0]) $ mars_facts_df.columns=["description", "value"] $ mars_facts_df
data_vi[['VIOLATIONS']].plot(figsize=(8,4), $                                style=['-']);
dcaggr = dc.groupby(['YearWeek'])['text'].apply(lambda x: ' '.join(x)).reset_index(['YearWeek']) $ tmaggr = tm.groupby(['YearWeek'])['text'].apply(lambda x: ' '.join(x)).reset_index(['YearWeek']) $ dcaggr.head(2)
y=pd.DataFrame(y)
autos["registration_year"].describe()
df[df.tstamp > '2018-01-07']
fin_r_monthly = fin_r_monthly.iloc[:-1]
us_tax = quandl.get("OECD/REV_NES_TOTALTAX_TAXUSD_USA") $ df_info(us_tax)
price = autos['price'] $ odometer = autos['odometer_km'] $ price.unique().shape
articles.to_csv('data_table/article.csv', index = False)
ins.describe()
france_tops = lda.get_term_topics(d.token2id['france'], minimum_probability=0.001) $ france_tops, get_topic_desig(france_tops)
df.of_fielding_alignment.value_counts()
to_be_predicted_Day3 = 31.34422603 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
import pandas as pd
msg
coarse_fuel_mgxs = coarse_mgxs_lib.get_mgxs(fuel_cell, 'nu-fission') $ coarse_fuel_mgxs.get_pandas_dataframe()
eia_total['type'].unique()
colormap = plt.cm.RdBu $ plt.figure(figsize=(14,12)) $ plt.title('Pearson Correlation of Features', y=1.05, size=15) $ sns.heatmap(train_df.filter(filtered_columns).astype(float).corr(),linewidths=0.1,vmax=1.0, $             square=True, cmap=colormap, linecolor='white')
np.mean(shows['first_year'].dropna())
nltk.help.upenn_tagset()
scale = StandardScaler() $ classify_df[['IntakeAge']] = scale.fit_transform(classify_df[['IntakeAge']])
surveys2001_df = pd.read_csv("survey2001.csv", index_col=0, keep_default_na=False, na_values=[""]) $ surveys2002_df = pd.read_csv("survey2002.csv", index_col=0, keep_default_na=False, na_values=[""])
order_data.head()
n_new = (df2[df2['landing_page'] == 'new_page']).shape[0] $ n_new $
result_df[['CA','UK','US']]=pd.get_dummies(result_df['country']) $ result_df=result_df.drop('CA',axis=1) $ result_df.head()
m_test = merged_test[['customer','frequency','recency','T']].set_index('customer') $ m_test.shape
autos["price"] = autos["price"].str.split('$',expand=True).iloc[:,1].str.replace(',','').astype(float) $ autos["odometer"] = autos["odometer"].str.replace("km",'').str.replace(',','').astype(float) $ autos.rename(columns={'odometer':'odometer_km'}, inplace=True) $ autos[["price","odometer_km"]].describe() $
pm_data.dropna(inplace=True) $ pm_data.status.value_counts()
%%time $ test2 = model.predict_generator(custom_generator(), steps = 100, $                                workers=1, use_multiprocessing=False)
print(r.json())
sqlContext.sql("select * from RandomOne").toPandas()
dow_columns=pd.get_dummies(df2['dow']) $ dow_rate=pd.DataFrame([(lambda x:(df2[x] * df2.converted).sum()/df2[x].sum()) (x) for x in dow_columns], index=list(pd.get_dummies(df2['dow']).columns), columns=['conversion_rate']) $ dow_rate
print(d)
exiftool -csv -createdate -modifydate cisrol12/cycle1_MVI_0032.mp4 cisrol12/cycle1_MVI_0033.mp4 cisrol12/cycle1_MVI_0034.mp4 cisrol12/cycle2_MVI_0035.mp4 cisrol12/cycle2_MVI_0036.mp4 cisrol12/cycle2_MVI_0037.mp4 cisrol12/cycle2_MVI_0038.mp4 cisrol12/cycle2_MVI_0039.mp4 cisrol12/cycle2_MVI_0042.mp4 cisrol12/cycle2_MVI_0043.mp4 cisrol12/cycle2_MVI_0044.mp4 cisrol12/cycle2_MVI_0045.mp4 cisrol12/cycle2_MVI_0046.mp4 cisrol12/cycle2_MVI_0047.mp4 cisrol12/cycle2_MVI_0048.mp4 cisrol12/cycle2_MVI_0049.mp4 cisrol12/cycle2_MVI_0050.mp4 cisrol12/cycle2_MVI_0051.mp4 cisrol12/cycle2_MVI_0052.mp4 cisrol12/cycle2_MVI_0053.mp4 cisrol12/cycle2_MVI_0054.mp4 cisrol12/cycle2_MVI_0055.mp4 cisrol12/cycle4_MVI_0075.mp4 cisrol12/cycle4_MVI_0076.mp4 cisrol12/cycle4_MVI_0078.mp4 cisrol12/cycle4_MVI_0079.mp4 cisrol12/cycle4_MVI_0080.mp4 cisrol12/cycle4_MVI_0081.mp4 cisrol12/cycle4_MVI_0082.mp4 cisrol12/cycle4_MVI_0083.mp4 cisrol12/cycle4_MVI_0084.mp4 cisrol12/cycle4_MVI_0085.mp4 cisrol12/cycle4_MVI_0086.mp4 cisrol12/cycle4_MVI_0089.mp4 cisrol12/cycle5_MVI_0095.mp4 cisrol12/cycle5_MVI_0096.mp4 cisrol12/cycle5_MVI_0097.mp4 cisrol12/cycle5_MVI_0098.mp4 cisrol12/cycle5_MVI_0100.mp4 cisrol12/cycle5_MVI_0101.mp4 cisrol12/cycle5_MVI_0102.mp4 cisrol12/cycle5_MVI_0106.mp4 > cisrol12.csv
df['Symbol'] = finalSymbolsList.values
df1.plot(lw=2) $ plt.legend(loc="upper left") $ plt.tick_params(axis='both', labelsize=10) $ plt.xticks(rotation=70)
avg_per_seat_price_seasonsandteams = seasons_and_teams["Per Seat Price"].mean() #Takes the average of those transactions
ved = pd.read_excel('input/Data.xlsm', sheet_name='51', usecols='A:W', header=12, skipfooter=4)
new_page_converted = np.random.choice([1,0], size = nNew, p=[pMean,oneMinusP]) $ new_page_converted.mean()
pres_df['location'].unique()
dfs = pd.read_html('https://en.wikipedia.org/wiki/Timeline_of_programming_languages', header=0) $ dfs[4]
train.ORIGINE_INCIDENT.value_counts()
solar_wind_df = complete_solar_df.merge(complete_wind_df, on=['DateTime', 'Date', 'PTE'], how='inner') $ solar_wind_df.to_excel(weather_folder + '/complete_solar_wind.xlsx', index = False)
%%time $ X_test_term = vectorizer.transform(X_test['text'])
git_blame.timestamp = pd.to_datetime(git_blame.timestamp) $ git_blame.info(memory_usage='deep')
b = splinter.Browser('chrome')
len(train_data[train_data.fuelType == 'hybrid'])
image_predictions_copy['tweet_id'] = image_predictions_copy['tweet_id'].astype(str)
the_year=2011
df_graph = new_df_dummies[new_df_dummies['click_rec_menues']==1] $ df_graph.head()
type(twitter_archive_df_clean.timestamp[0])
fb.index = fb.index.tz_localize('utc').tz_convert('Asia/Singapore')
daily_sales.plot('date','dcoilwtico',kind='bar', color='r', figsize=(15,5))
usersDf.head(2) $
print(temp1.shape, temp1.ndim)
unknown_users[unknown_users['total_price']!=9999]
joined.to_feather(f'{PATH}joined')
w = 'jew' $ model.wv.most_similar (positive = w)
ffr.resample("M").last().head()
media_classes = [c for c in df_os.columns if c not in ['domain', 'notes']] $ breakdown = df_questionable[media_classes].sum(axis=0) $ breakdown.sort_values(ascending=False)
tweet_archive_clean['timestamp'] = pd.to_datetime(tweet_archive_clean['timestamp'])
print(cc['open'].describe()) $ print(cc['spread'].describe())
from sklearn.metrics import classification_report $ labels = ['Setosa', 'Versicolor', 'Virginica'] $ y_pred = nbc.predict(d_test_sc) $ print(classification_report(l_test, y_pred, \ $                             target_names = labels))
reddit_info = reddit_info.drop('index', axis = 1)
data.sort_values(by='Likes',ascending=False).head(4)
archive_copy.info()
autos["odometer_km"].unique().shape[0]
temp_df2.shape
autos.rename({'yearOfRegistration':'registration_year', $              'monthOfRegistration':'registration_month', $              'notRepairedDamage':'unrepaired_damage', $              'dateCreated':'ad_created'}, $             axis=1, inplace=True)
df_users.gender.value_counts()
match_results.tail(3)
content_values = cached.map(lambda x: (x[1]['content_input'], x[1]['content_bhr'])).countByValue()
b_cal.available.unique()
print(aa)
UK_treatment_conversion = df_c_merge[(df_c_merge['group']== 'treatment') & (df_c_merge['country'] == 'UK')]['converted'].mean() $ UK_treatment_conversion
count_non_null(geocoded_df, 'Disposition.Date')
plt.hist(test_residuals['255_elec_use'])
twitter_df_clean.head(10)
for col in ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']: $      df_raw[col].replace({'Yes': 1, 'No': 0, 'No internet service': 2}, inplace=True)
tweets_df = pd.read_csv(tweets_filename, $                         converters={'tweet_place': extract_country_code, $                                     'tweet_source': extract_tweet_source})
eval_RF_tf_tts = clf_RF_tf.score(X_testcv_tf, y_testcv_tf) $ print(eval_RF_tf_tts)
d = tran_time_diff[tran_time_diff.msno == '++1Wu2wKBA60W9F9sMh15RXmh1wN1fjoVGzNqvw/Gro='] $ d
pd.value_counts(ac['Issues']).head(10)
sns.pairplot(df, x_vars=['Bottles Sold','Volume Sold (Liters)'], y_vars='Sale (Dollars)', size=7, aspect=0.7)
data = data[(data['Latitude'].notnull())& $             (data['Longitude'].notnull())& $             (data['Closed Date'].notnull())]
df_bug[u'Service Location Type'].value_counts()
inoroffseason = ALLbyseasons.groupby("InorOff") # This groups our sample by whether transactions took place in-season or during $
df_cs = pd.read_csv("costco_all.csv", encoding="latin-1")
date.
import pymysql
import statsmodels.api as sm $ df2['intercept'] = 1 $ df2[['old_page', 'new_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop(['old_page'], axis=1) $ df2.head()
logit_mod_2 = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'UK','US']]) $ results_2 = logit_mod_2.fit() $ results_2.summary()
type(new_cust_without_discover['Time To Buy'][0])
pystore.set_path('./pystore_demo') $ pystore.get_path()
class_merged=pd.merge(class_sales,class_items,on=['date','store_nbr','class','family'],how='left') $ class_merged=pd.merge(class_merged,class_perishables,on=['date','store_nbr','class','family'],how='left') $ class_merged.columns=['date','store_nbr','class','family','sum_unit_sales','no_items','no_perishable_items'] $ print("Rows and columns:",class_merged.shape) $ pd.DataFrame.head(class_merged)
print (" The text: %s \n The grade in rating_numerator: %.1f \n" % (df['text'].ix[695], df['rating_numerator'].ix[695]))
csvData['country'].value_counts()
collection.delete_snapshot('snapshot_name') $
merged = pd.merge(prop, contribs, on="calaccess_committee_id")
sub = pd.read_csv('../input/sample_submission.csv') $ sub['Tag'] = le.inverse_transform(test_pred_svm) $ sub.to_csv('../submissions/svc.csv', index=False)
df2['intercept'] = 1 $ df2['a/b_page'] = df2['group'].replace(('control','treatment'),(0,1)) $ df2.head()
plt.title('Overall ngram', fontsize=18) $ overall_ng.plot(kind='barh', figsize=(20,16)); $ plt.savefig('../visuals/overall_ngram.jpg')
y_pred = lr_pred $ print('precision: {:.2f}\nrecall: {:.2f}\naccuracy: {:.2f}'.format(precision_score(y_test,y_pred), $                                                        recall_score(y_test,y_pred), $                                                        accuracy(y_test,y_pred)))
out = conn.addtable(table='iris_sql', caslib='casuser', $                     **sqldmh.args.addtable) $ out
hour_distributedTopmodel_average = (hour_distributedTopmodel[0]*78300 + hour_distributedTopmodel[1]*32700 + hour_distributedTopmodel[2]*18600 + hour_distributedTopmodel[3]*32800 + hour_distributedTopmodel[4]*168200 + hour_distributedTopmodel[5]*45400)/(78300+32700+18600+32800+168200+45400)
p_sort=p_sort.rename(columns = {'Product_x':'Product'}) $ p_sort.drop(['Product_y'], axis = 1, inplace = True) $ p_sort
cur = conn.cursor()
extractor = connectToTwitterAPI() $ tweets = extractor.search(q="#BlackPanther", count=50) $ print("Number of tweets extracted: {}.\n".format(len(tweets)))
html_table_marsfacts = df.to_html() $ html_table_marsfacts
ekos.copy_workspace(user_id = user_id, source_alias = 'default_workspace', target_alias = 'lena_newdata')
len(list(set(have_seen_two_versions) & set(df_click.user_session.unique())))
fixed_bonus_points = bonus_points.fillna(0) $ fixed_bonus_points
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results_1 = logit_mod.fit() $
mb = pd.read_excel('Data/microbiome/MID2.xls', sheetname='Sheet 1', header=None) $ mb.head()
sorted(problem_combos.map(lambda c: (c[0], 1)).countByKey().iteritems(), key=lambda x: x[1], reverse=True)
data.index = pd.to_datetime(data.index)
p = gpd.read_file('Police Precincts/geo_export_00b06dd5-82ff-48a1-9401-f80ee795bc70.shp')
containers[0].find("div",{"class":"key"}).a['title'].split()[1].replace(',',"")
plot_confusion_matrix(cm_dt, classes=['COLLECTION', 'PAIDOFF'],normalize=False, title="Confusion matrix for knn", cmap=plt.cm.Blues)
totals['pct_offline'] = round(totals['offline']/totals['total']*100)
authors.head()
%%ml batch_predict --model train/evaluation_model --output evalme $ format: csv $ prediction_data: $     csv: './eval.csv'
import openpyxl
jobs_data1 = json_normalize(json_data1['page']) $ jobs_data1.head(5)
refldata = refl['Reflectance_Data'] $ refldata
news5 = ('The oil and gas industry of Texas continued to recover in April, with strong oil production growth last month, the Federal Reserve Bank of Dallas said in its Energy Indicators monthly release.' $ 'The Permian Basin continued to be the driver of the U.S. rig count growth. Rig counts in the Permian increased from 310 in March to 337 in April, while rig counts in the Eagle Ford rose from 80 in March to 89 in April, the Dallas Fed said.' $ 'In job figures for a month lagging production figures, total Texas oil and gas employment rose in March by 3,500 jobs to around 211,700 jobs, with oil and gas extraction employment up slightly to 92,500 jobs, and payrolls in support activities for mining rising to 119,200. March was the third consecutive month of increases in total Texas oil and gas employment, the Dallas Fed said.' $ 'Earlier this month, the Dallas Fed said in its Texas Economic Update that positive job growth and rising rig counts indicate an ongoing energy sector recovery.')
dbData.head(10)  # NaN's show up in the field with no data!  Want to drop these from the dataframe.
df_transactions = pd.read_csv('C:/Users/ajayc/Desktop/ACN/2_Spring2018/ML/Project/WSDM/DATA/transactions.csv', parse_dates=['transaction_date','membership_expire_date'], dtype={'payment_method_id': np.int8, 'payment_plan_days': np.int16, 'plan_list_price': np.int16, 'actual_amount_paid': np.int16, 'is_auto_renew': np.int8, 'is_cancel': np.int8}) $ df_transactions= pd.concat((df_transactions, pd.read_csv('C:/Users/ajayc/Desktop/ACN/2_Spring2018/ML/Project/WSDM/DATA/transactions_v2.csv', parse_dates=['transaction_date','membership_expire_date'], dtype={'payment_method_id': np.int8, 'payment_plan_days': np.int16, 'plan_list_price': np.int16, 'actual_amount_paid': np.int16, 'is_auto_renew': np.int8, 'is_cancel': np.int8} )), axis=0, ignore_index=True).reset_index(drop=True)
list(c.find({}, {'_id': 0}))
kickstarter["country"].unique()
twitter_archive_clean.loc[twitter_archive_clean['tweet_id']==666287406224695296]
map(lambda x: x.shape,dfs_loan)
sm.stats.proportions_ztest([convert_old,convert_new], [n_old,n_new])
print("replacing null value to zero is done") $ compiled_data['botometer']=compiled_data.botometer.replace(np.NaN, 0) $ print("there is null value on column botometer, this null indicated this account already deactivate their account :") $ len(compiled_data[pd.isnull(compiled_data['botometer'])==True])
print(deep_match_model.summary())
km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100) $ km.fit(X)
print("") $
df.group.unique()
df = pd.merge(train,user_logs, on = 'msno', how = 'left')   $
selected=features[features.importance>0.01] $ selected.sort_values(by="importance", ascending=False)
call.iloc[-1]
from urllib import request $ response = request.urlopen('http://httpbin.org') $ print(response.status) $ print(response.getheaders()) $ print(response.getheader('server'))
gbm_model.plot()
main_tables = dict() $ for i in files_to_manage: $     print(download_file_from_dropbox(account, '{}_2017.csv'.format(i), folder="nba games/test_ws_workflow/")) $     main_tables[i] = pd.read_csv('{}_2017.csv'.format(i))
plt.show()
import seaborn as sns $ import matplotlib as mpl $ import matplotlib.pyplot as plt $ %matplotlib inline
df.plot()
log_mod = sm.Logit(df3['converted'], df3[['intercept', 'CA' , 'UK']]) $ results = log_mod.fit() $ results.summary() $
sep2014.start_time, sep2014.end_time
intervention_train.sort_index(inplace=True) $ intervention_history.sort_index(inplace=True)
occurrences[occurrences['job_id'] == 68551].to_csv('abcd.csv')
varianceTable
print('the one user_id repeated is {}'.format(df2[df2.duplicated(['user_id'], keep=False) ]['user_id'].iat[0])) $
X = pivoted.fillna(0).T.values $ X.shape
liberia_data.columns
n_new = df2.query('group == "treatment"').count()[0] $ n_new
final_test_pred_nbsvm1 = test_probs.idxmax(axis=1).apply(lambda x: x.split('_')[1])
pd.options.display.max_columns = 100 $ X_train.head(2)
sns.countplot(x="month",data=twitter_final)
train, test = pd.read_csv('train.csv'), pd.read_csv('test.csv')
import pandas as pd $ import MySQLdb $ db = MySQLdb.connect("localhost","root","","mysql_project" ) $ print("connected")
people = ['hillary', 'trump', 'obama', 'mueller', 'comey', 'kim', 'putin', 'merkel', 'trudeau', 'sessions'] $ people_dict = {} $ for person in people: $     people_dict.update({person: df.text.str.extractall(r'({})'.format(person), re.IGNORECASE).size}) $ people_dict = dict(sorted(people_dict.items(), key= lambda x: x[1], reverse=True))
df2 = df2.drop_duplicates(subset='user_id', keep='first') $ df2[df2['user_id'] == 773192] $
subtes = pd.read_csv('datasets/estaciones-de-subte.csv', sep=',', error_bad_lines=False, low_memory=False) $ subtes.info()
df_episodes[~df_episodes.id.isin(df.episode_id)]
run txt2pdf.py -o"2018-06-18  2015 470 discharges.pdf"  "2018-06-18  2015 470 discharges.txt"
average_temp = temperature[175][2] $ average_temp
obs_diff=df2[(df2.group == 'treatment')].converted.mean() - df2[(df2.group == 'control')].converted.mean() $ (p_diffs > obs_diff).mean()
con = sqlite3.connect('db.sqlite') $ df=pd.read_sql_query("SELECT * from tbl", con) $ con.close() $ df
df.party.value_counts()
stops.head()
n_new = df2.loc[(df2.landing_page == "new_page")].user_id.nunique() $ n_new
autos['price'].describe()
march_2016 = pd.Period('2016-03', freq='M') $ print march_2016.start_time $ print march_2016.end_time
dummy = pd.get_dummies(df3['country'], prefix='country_code') $ dummy.head()
print("\nThere are {} data points in our data set.".format(df.shape[0]))
dummy_Contract = pd.get_dummies(df['Contract'], prefix='Contract') $ print(dummy_Contract.head())
train_data = pd.read_csv('train.csv') $ test_data = pd.read_csv('test.csv') $
import datetime $ import pandas as pd $ speeches_df3['month'] = [datetime.datetime.strptime(month, '%B').strftime('%m') for month in speeches_df3['month']] $ speeches_df3['date'] = pd.to_datetime(speeches_df3[['year','month','day']])
sl[sl.status_binary==0][(sl.today_preds==1)].shape
old_page_converted = np.random.choice(2, n_old, p= [1-p_old,p_old]) $ old_page_converted
metadata2 $
data_df.iloc[535]
tweet_archive_clean.dtypes
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END)
journalist_mention_gender_summary(journalists_mention_df) $
soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]
df19.to_csv("AppleTweets19.csv",index=False)
len(df[df.converted==1])/len(df)
social_disorder = class_data.iloc[:, 3:13] $ social_disorder.head()
country_with_least_expectancy = le_data.idxmin(axis=0) $ country_with_least_expectancy
import pandas as pd
calls_df[calls_df["phone number"]==5600724140].head()
led_pin = 18 $ GPIO.setmode(GPIO.BCM) $ GPIO.setwarnings(False) $ GPIO.setup(led_pin,GPIO.OUT)
test_data.tail()
round(df2[df2['group'] == 'control']['converted'].mean(), 4)
click_condition_meta['geo_country'] = np.where((pd.isnull(click_condition_meta['geo_country'])), $                                                'NG', click_condition_meta.geo_country)
idx = pd.IndexSlice $ health_data.loc[idx[:, 1], idx[:, 'HR']]
    spark.sql(query).toPandas()
doctype
users = pd.read_csv('takehome_users.csv', encoding='ISO-8859-1')
type(rj)
from sklearn.cross_validation import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(data[features], data["OutcomeType"], stratify=data["OutcomeType"], random_state=100) $ X_train.head()
ins['new_date'] = pd.to_datetime(ins['date']) $ ins.head(5)
len(clean_tweet_texts)
for i in ['Shipped Created diff','Updated Shipped diff']: $     print df[df[i]>500].shape
df_new[['US', 'CA', 'UK']] = pd.get_dummies(df_new['country']) $ new_log_m = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'US', 'CA']]) $ new_results = new_log_m.fit() $ new_results.summary()
n_old = old.count() $ n_old
spencer_bday + thirty_years
X = [string.replace('\n', ' ') for string in X]
c.find_one({'albums.released': 1982})
grouped = df_providers.groupby(['year','drg3']) $ grouped_by_year_DRG_max =(grouped.aggregate(np.max)['medicare_payment']) $ grouped_by_year_DRG_max.head()
itm_add_elapsed('onpromotion', 'before_')
print('Bill id repeat id', df_bill_id[df_bill_id.duplicated(['patient_id'])].shape) $ print('Bill repeat id', df_bill[df_bill.duplicated(['bill_id'])].shape)
stocks = sp_companies[0] $ stocks.head()
def stageWonMap(x): $     p = 0 $     if x.opportunity_stage == 'Closed Won': p = x.opportunity_amount $     return p
simm['test'] = siim['pred']
seen.shape
inv_count,inv_bin=np.histogram(investor_fut_bucket_30360_xirr_orig,bins=np.linspace(-.5,.5,101)) $
df['y'].plot()
print topUserItemDocs.shape $ print topUserItemDocs.columns $ topUserItemDocs.head()
with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f: $     f.create_group(where='/', name='my_group')    
temp = pd.read_table('vader_lexicon.txt', names=('word', 'polarity', 'idk', 'idk1')) $ sent = pd.DataFrame({'polarity':temp['polarity']}) $ sent.index = temp['word'] $
df_madrid = pd.read_csv('madrid_df.csv') $ replace_set = stopwords.words('english') $ df_madrid['text'] = df_madrid['text'].str.split(' ').apply(lambda x: ' '.join(k for k in x if k not in replace_set)) # replace stop-words
from collections import Counter
df.isnull().any()
df_time = df.groupby('userTimezone')[['tweetRetweetCt', 'tweetFavoriteCt']].mean() $ df_time
autos["odometer"] $
display('df2', 'df2.groupby(str.lower).mean()')
df_twitter_archive_copy['gender'] = df_twitter_archive_copy.apply(extract_gender, axis=1) $ df_twitter_archive_copy['gender'] = df_twitter_archive_copy.gender.astype('category')
actions = [r['FeatureAction']['Action'] for r in results]
cnf_matrix = confusion_matrix(y_test, yhat_SVM, labels=['PAIDOFF','COLLECTION']) $ np.set_printoptions(precision=2) $ print (classification_report(y_test, yhat_SVM)) $ plt.figure() $ plot_confusion_matrix(cnf_matrix, classes=['PAIDOFF','COLLECTION'],normalize= False,  title='Confusion matrix')
movies.head(5)
test['Month']     = test["date"].dt.month $ test['Day']       = test["date"].dt.day $ test['DayOfWeek'] = test["date"].dt.dayofweek
n_new = df2[df2['landing_page'] == 'new_page']['user_id'].count() $ print('n_new: ', n_new)
gaussian = GaussianNB() $ gaussian.fit(X_train, Y_train) $ Y_pred = gaussian.predict(X_test) $ acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2) $ acc_gaussian
cryptos['name']
df_user_extract_copy.info()
df['ReceivTurnover'] = df['ReceivTurnover'].map(transform1)
cumulative_returns.plot()
plotdf['forecast'] = plotdf['currentCount'] $ plotdf['forecastPlus'] = plotdf['currentCount'] $ plotdf['forecastMinus'] = plotdf['currentCount']
top_supports.amount.plot.bar()
def prob2logodds(prob): $     odds = prob / (1 - prob) $     logodds = np.log(odds) $     return logodds
from nltk.corpus import stopwords $ portuguese_stop_words = stopwords.words('portuguese')
table_info = con.execute('PRAGMA table_info(samples);').fetchall() $ table_info
USER_PLANS_df.head()
print('Installed Offshore Capacity:',capa2017offshore['Current Installed Capacity [MW]'].sum(),'MW') $ print('Installed Onshore Capacity:',capa2017onshore['Current Installed Capacity [MW]'].sum(),'MW') $ print('Total:', capa2017offshore['Current Installed Capacity [MW]'].sum()+capa2017onshore['Current Installed Capacity [MW]'].sum())
trainDF.drop('UNIQUE_CARRIER', axis = 1, inplace = True)
c.execute(query) $ results = c.fetchall() $ print "Accommodation that has a postcode tag:", len(results)
df.groupby('key').transform(lambda x: x - x.mean())
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']] $ df_new['country'].value_counts()
df = pd.read_csv(pump_data_path, index_col = 0) $ df.head(1)
d = ts.resample('1D') $ d
obj[mask]
c_df.rename(columns={'TimeCreate':'Date'},inplace=True)
movies2000to2014=movies.loc[(movies['year']>=2000) & (movies['year']<=2014)]
!hadoop fs -ls -h stocks.json
stockdftest.tail(3)
top_supports.head(10)
USER_PLANS_df['start_date'] = pd.to_datetime(USER_PLANS_df['scns_created'].apply(lambda x:x[np.argmin(x)])).dt.strftime('%Y-%m')
old_page_converted = np.random.choice([0,1], size = n_old, replace=True,p=[1-p_old, p_old])
list_feature_names = list(feature_data['feature_names']) $ train_data = pd.read_csv('data/kaggle_data/train.csv', header=None, sep=" ", names=list_feature_names) $ train_data.head(5)
sorted_budget_biggest = df.sort_values(by=['budget_adj'], ascending = False).head(200)
autos['registration_year'].hist()
top_topics = (news_df['topic'] $               .value_counts() $               .sort_values(ascending = False) $               .head(10)) $ top_topics
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=38000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
fig, axs = plot_partial_dependence(clf, X_test[X_test.age_well_years != 274], [ 7,8], feature_names=X_test.columns, grid_resolution=70, n_cols=7) $ fig, axs = plot_partial_dependence(clf, X_test[X_test.age_well_years != 274], [ 10], feature_names=X_test.columns, grid_resolution=70, n_cols=7) $
scores = pd.read_sql("SELECT subreddit, author, score FROM comments", con=engine) $ scores.head()
individuals_metadata_df.to_excel(writer, "individuals", index=False)
df.isnull().sum()
dfss.head()
df_events.head()
twitter_archive_enhanced_clean = twitter_archive_enhanced_clean.drop(["doggo", "floofer", "pupper", "puppo"], axis = 1) $ twitter_archive_enhanced_clean.head()
tweets['created'] = pd.to_datetime(tweets['created'])
%load "solutions/sol_2_5.py"
df_questionable_2 = pd.merge(left= df_questionable, left_on= 'link.domain_resolved', $                              right= df_usnpl_one_hot, right_on= 'domain', how= 'left')
pd.pivot_table(more_grades, index=("name", "month"), margins=True)
image_predictions_clean.info()
import pickle $ import pandas as pd
shows['release_date'] = shows['release_date'].dropna().apply(lambda x: datetime.strptime(x, '%d %b %Y'))
n_new = len(df2.query("landing_page == 'new_page'")) $ print('N_new is {}'.format(n_new))
%%time $ df['closed_at'] = pd.to_datetime(df['Closed Date'], format='%m/%d/%Y %X %p')
qualConvDataGrouped = qualification.groupby(['lead_source', 'qual_conversion']).qual_conversion.count() $ qualConvpct = qualConvDataGrouped.groupby(level=[0]).apply(lambda x: 100* x / float(x.sum())); 
sys.stderr.write("This is stderr text\n") $ sys.stderr.flush() $ sys.stdout.write("This is stdout text\n") $ print(sys.argv) $
weeks = list(range(0,365,7))  # Compiling list of 7 day increments $ weeks # Inspecting progress
data = data.rename(columns={0:'time',1:'sentiment'})
df_new['new_US'] = df_new['ab_page']*df_new['US'] $ df_new['new_CA'] = df_new['ab_page']*df_new['CA']
with pm.Model() as model: $     alpha = 1.0/summary.mean()[0] $     lambda_1 = pm.Exponential("lambda_1", alpha) $     lambda_2 = pm.Exponential("lambda_2", alpha) $     tau = pm.DiscreteUniform("tau", lower = 0, upper = n_count_data - 1)
print(tipsDF.describe())
X_train, X_test, y_train, y_test = train_test_split(stock.drop(['target'], 1), stock['target'], test_size=0.3, random_state=42)
xml_in[xml_in['publicationDate'].isnull()].count()
autos[['date_crawled','ad_created','last_seen']][0:5]
airport_count.to_excel('airport_count.xlsx') $ print("Done Writing!")
sub1.head(10)
joined.head()
autos['price'] = autos['price'].str.replace('$', '').str.replace(',', '').astype(int)
raw , y = clean_test_data('data/raw_data.json')
drace_df = drace_df.fillna(mean_values)
%%time $ prices = data.DataReader(stocks, 'yahoo', start, end)['Adj Close']
prices.plot()
image_predictions.describe()
df.rename = df $ df
df.dot(df.T)
california['bins'] = pd.qcut(california['FIRE_SIZE'],4) #used .qcut to cut the fire sizes from california into $ california['bins'].value_counts()                       #quartiles
np.random.seed(500) $ x=np.linspace(1,10,100)+ np.random.uniform(low=0,high=.5,size=100) $ y=np.linspace(1,20,100)+ np.random.uniform(low=0,high=1,size=100) $ print ('x = ',x) $ print ('y= ',y)
(loans_fut_bucket_xirr_groups*100)
temp = tokendata.drop("ID",axis=1) $ g = sns.pairplot(tokendata[temp.columns].dropna())
df_clean[df_clean.duplicated()]
run txt2pdf.py -o '2018-06-22 2014 FLORIDA HOSPITAL Sorted by payments.pdf'  '2018-06-22 2014 FLORIDA HOSPITAL Sorted by payments.txt'
menu_about_latent_features = pd.DataFrame(menu_about_latent_features_with_menu_ids, columns=menu_about_latent_features_column_names)
tmax_day_2018.coords
sns.distplot(df['num_comments']) $ plt.title("Distribution - Number of comments");
flights2.loc[[(1949,"December"),(1950,"January")]]
tweets_predictions_all.info()
df_h1b_nyc.pw_1.describe()
df_raw['list_date'] = pd.to_datetime(df_raw.list_date, errors='coerce')
print('Full:') $ print('Polarity:', fullDF.Polarity.mean()) $ print('Subjectivity:', fullDF.Subjectivity.mean())
train_view.sort_values(by=6, ascending=False)[0:10]
http = credentials.authorize(httplib2.Http())
active_station_date.plot(kind='hist',bins=12)
n_old = df2.query('group == "control"').shape[0] $ n_old
crimes['month'] = crimes.DATE_OF_OCCURRENCE.map(lambda x: x.month)
df_archive_clean.shape
df2[df2['landing_page'] == 'new_page'].count()/df2.shape[0]
dfJobs.ix[40]
random_integers
criteria = so['score'] >= 10 $ criteria.head(10)
X = pd.get_dummies(X, columns=['subreddit'], drop_first = True)
adopted_cats.loc[adopted_cats['Color']=='Chocolate','Color'] = 'Brown' $ adopted_cats.loc[adopted_cats['Color']=='Chocolate Point','Color'] = 'Brown Point' $ adopted_cats.loc[adopted_cats['Color']=='Chocolate/White','Color'] = 'Brown/White' $ adopted_cats.loc[adopted_cats['Color']=='Chocolate Point/White','Color'] = 'Brown Point/White'
tweet_archive_clean.rating_denominator.value_counts()
grouped = data[['processing_time','Borough']].groupby('Borough')
sqlContext.registerFunction("TimesTen", TimesTen)
sites.shape
X = tfidf.transform(text)
df.sample(10)
analyze_set.describe()
data = sat_lin_df $ data = data[data["satisfied"]<=10] $ holdout = data.sample(frac=0.05) $ training = data.loc[~data.index.isin(holdout.index)]
index_outliers_age = cserie(outliers_age.is_outlier==1, index=True)
zipincome.head()
All_tweet_data_v2.info()
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([17489, 17264], [145274, 145310], alternative = 'smaller') $ print(z_score, p_value)
df.plot(y ='rating', ylim=[0,14], style = '.', alpha = 0.4) $ plt.title('Rating with Time') $ plt.xlabel('Date') $ plt.ylabel('Rating');
autos = autos.loc[(autos["registration_year"] > 1900) & (autos["registration_year"] < 2006)] $ autos["registration_year"].value_counts(normalize=True)
session.query(Measurement.tobs).order_by(Measurement.tobs).first()
print('source embedding shape on training set: ', train_source_emb.shape) $ print('target embedding shape on training set: ', train_target_emb.shape)
wheels = autos[['make', 'drive_wheels']] $ wheels.head()
df_tsv_clean = df_tsv.copy() $ df_archive_csv_clean = df_archive_csv.copy() $ df_json_tweets_clean = df_json_tweets.copy()
pd.value_counts(ac['Bank'].values, sort=True, ascending=False)
( diff_between_group < p_diffs).mean()
n_new = df2.query('landing_page == "new_page"')['user_id'].count() $ n_new
pd.to_datetime('4.7.12', dayfirst=True)
archive_copy = archive_copy.drop('id', axis=1)
df_new.country.value_counts() $ df_new[['CA','UK','US']] = pd.get_dummies(df_new['country'])
Total_number_stations=session.query(Station.station).count() $ print(f"Total number stations: {Total_number_stations}") $
airlines_day = df.resample('D').apply({'iberia':'sum', 'spanair': 'sum', 'jetblue': 'sum',\ $                                        'vueling': 'sum', 'ryanair': 'sum', 'norwegian': 'sum', 'aireuropa': 'sum'})
more_200 = tweet_df['username'].value_counts() > 200 $ dublicated_users = more_200[more_200].index
tsd = gcsfs.GCSFileSystem(project='inpt-forecasting') $ with tsd.open('inpt-forecasting/Transplant Stemsoft data -040117 to 061518.xlsx') as tsd_f: $   tsd_df = pd.read_excel(tsd_f)
tweets_original['full_text'] = tweets_original['full_text'].str.decode('utf-8') $ tweets_original['created_at'] = tweets_original['created_at'].str.decode('utf-8')
indexed_df = movie_df.set_index("title")
predictor_cols = ['Wins','Playoffs','Conference']
X_train.to_csv('X_train.csv') $ X_test.to_csv('X_test.csv')
plt.figure() $ fitA = SMOOTH.plot_single_trial(etsamples,etmsgs,None,'VP3','el',10,4,smooth_stanmodel) # trial 1, block 1 $ fitB = SMOOTH.plot_single_trial(etsamples,etmsgs,None,'VP3','pl',10,4,smooth_stanmodel) # trial 1, block 1
%%bash $ wget "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz" $ ls
taxi_hourly_df.index.min()
mis_match = [] $ for i in range(len(tweet_json)): $     if (tweet_json['id'][i]) not in list(twitter_archive['tweet_id']): $         mis_match.append(tweet_json['id'][i]) $ print(len(mis_match))
full_act_data.plot(figsize=(20,8));
auto_new.info()
first_movie = movie_containers[0] $ first_movie
k_var['launched_at'] = launch_date $ k_var['state_changed_at'] = change_date $ k_var['days_to_change'] = (k_var['state_changed_at'] - k_var['launched_at'])
actor = pd.read_sql_query('select last_name, first_name from actor \ $                            where last_name like "%%LI%%" \ $                            order by last_name, first_name', engine) $ actor.head()
adj_close_latest.set_index('Ticker', inplace=True) $ adj_close_latest.head()
dups = df2.user_id.value_counts() $ dups[dups > 1] $
import os $ files = os.listdir("/data/measurements") $ print('\n'.join(files))
from sklearn.feature_selection import RFE $ rfe = RFE(classifier,7 ) $ rfe = rfe.fit(data,affair) $ print(rfe.support_) $ print(rfe.ranking_) $
len(database)
data[1:4:2]
df_data.visa_class.value_counts()
merged2.to_pickle("data/merged_2018.pkl")
df2.drop(df2.index[1899])
cabs_df_byday = cabs_df.loc[cabs_df.index.weekday == weekday] $ cabs_df_byday.info() $ cabs_df_byday.head()
df_new = transactions.join(users.set_index('UserID'), on='UserID', how = 'left') $ df_new[df_new["User"].isnull()]
mb = pd.read_csv("Data/microbiome.csv", index_col=['Patient','Taxon']) $ mb.head()
fraq_volume_m_coins[['Bitcoin', 'Litecoin']].plot() $ plt.ylabel('Fraction of volume') $ plt.show()
fulldf['sentiment'] = np.array([sentiment(tweet) for tweet in fulldf['tweetText'] ])
(df.query('(group == "treatment" & landing_page != "new_page") |' $           '(group == "control" & landing_page != "old_page")') $  .groupby(['group', 'landing_page']).count())
grouped_dpt.agg(np.sum) # note: we can also just write agg instead of aggregate
ser4[np.isnan(ser4)] #numpy has isnan function to identify Not a Number values
rows = df.shape[0] $ print("Number of rows - {}".format(rows))
weather_json['data'][0]
len(" ".join(dc['text']).split(" "))
new_page_converted = np.random.binomial(1,pnew,nnew) $ plt.hist(new_page_converted)
Maindf.head(5)
stop = stopwords.words('english') $ tweetering['Text'] = tweetering['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) $ pd.Series(' '.join(tweetering['Text']).lower().split()).value_counts()[:50]
scores
import datapackage $ package = datapackage.Package()
df.loc[df.following.idxmax()]
import gc
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new.head()
print ('Shape of the dataset after transformation:' + str(master_file.shape)) $ master_file.head()
np.full((2,2),7)
plt.scatter(df_2015['state_bottle_retail'], df_2015['state_bottle_cost']) $ plt.xlabel('Sales $'); $ plt.ylabel('Bottles Sold'); $ plt.title('Bottles Sold x Sales $')
images_clean.index
s.index
qry_url = '{}index/feeds'.format(base_url) $ response = requests.get(qry_url) $ response = response.text $ print(response)
df_total[['CA','UK','US']] = pd.get_dummies(df_total.country)
GSW_2017["Tm.3PA"].mean()
tips.index
s4p = combined.reset_index() $ s4p[:5]
p_diffs = np.array(p_diffs) $ (p_diffs > p_diff).mean() $ print('As a percentage: {}%'.format((p_diffs > p_diff).mean()*100))
df_final.shape
df2 = df2.join(countries_df.set_index('user_id'),on='user_id')
chambers['red_state'] = chambers.index.isin(colors_states_abbr[0]) $ chambers['blue_state'] = chambers.index.isin(colors_states_abbr[1]) $ chambers['swing_state2'] = chambers.index.isin(colors_states_abbr[2])
df.injured.value_counts()
last_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first() $ print(last_date)
stories.head()
b_new_order_item.min()
twitter_archive_full['timestamp'] = pd.to_datetime(twitter_archive_full['timestamp'])
autos['odometer_km'].value_counts().sort_index(ascending=False)
myplot_parts = [go.Scatter(x=person_counts["stamp"],y=person_counts["count"],mode="line")] $ mylayout = go.Layout(autosize=False, width=1000,height=500) $ myfigure = go.Figure(data = myplot_parts, layout = mylayout) $ iplot(myfigure,filename="crisis")
season_type_groups.groups
df3 = df2.drop(['FlightDate', 'DepTime', 'DepTimeStr'], axis=1)
access_logs_df.cache()
cur.execute(query)
aux = image_clean.merge(dog_rates.loc[dog_rates.cuteness == 'doggo,puppo'], $                                                            how='inner', on='tweet_id') $ aux[['jpg_url','grade']].sort_values(['grade'], ascending=False).iloc[:,0].values[:3]
status.retweeted, status.favorited
predict, model = runXGB(train_X, train_y, train_X, num_rounds=400) $ out_train = pd.DataFrame(predict)
autos['last_seen'].str[:10].value_counts()
d={'c1':pd.Series(['A','B','C']),'c2':pd.Series(np.random.randint(0,4,4))} $ pd.DataFrame(d)
all_preds = np.stack([m.predict(X_test, batch_size=256) for m in models])
cond_1 = df.query("group == 'treatment' and landing_page == 'old_page' ") $ cond_2 = df.query("group == 'control' and landing_page == 'new_page' ") $ event_count = len(cond_1)+ len(cond_2) $ print ("Total mismatch events:",event_count)
not_avail_price = set(b_cal[b_cal['available'] == 'f']['listing_id']) - set(b_cal[b_cal['price']!= 'nan']['listing_id']) $ print(not_avail_price) $ print([b_cal[b_cal['listing_id'] == x] for x in not_avail_price])
data = pd.read_csv('dump.csv')
crimes_all.info(null_counts=True)
p_diffs = [] $ for i in range(10000): $     new_conv_rate = df.sample(len(df.query("group == 'treatment'")), replace = True)['converted'].mean() $     old_conv_rate = df.sample(len(df.query("group == 'control'")), replace = True)['converted'].mean() $     p_diffs.append(new_conv_rate-old_conv_rate)
import xlwt
Meter1.RemoteSetup()
percipitation_2017_df = pd.DataFrame(percipitation_2017[(percipitation_2017['Date'] >= '2016-08-01')\ $                                 & (percipitation_2017['Date'] <= '2017-08-23')].set_index('Date')) $ percipitation_2017_df $
with tf.name_scope("loss"): $     xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) $     loss = tf.reduce_mean(xentropy, name="loss") $     loss_summary = tf.summary.scalar('log_loss', loss)
data['timestamp'] = pd.to_datetime(data['Date'], unit='s')
obj.index
cols = list(dfs) $
raw_df.replace({'3wordweather': ''}, inplace=True, regex=True) $ raw_df.replace({'threewordweather': ''}, inplace=True, regex=True) $ raw_df.replace({'and': ''}, inplace=True, regex=True) $ raw_df.shape
cust_data1['ID']
rf_v2.hit_ratio_table(valid=True)
age.iloc[[True, False, True, False, True, False]]
convert_new_ct = df2.query("group=='treatment'and converted==1")['user_id'].count() $ print(convert_new_ct) $
dfsunrise = pd.read_pickle('sunrise120120102017.pkl')
df_enhanced['in_reply_to_status_id'] = df_enhanced.in_reply_to_status_id.map(lambda x: '{:.0f}'.format(x)) $ df_enhanced['in_reply_to_user_id'] = df_enhanced.in_reply_to_user_id.map(lambda x: '{:.0f}'.format(x))
df['city'] = df['location'].map(get_city) $ df['country'] = df['location'].map(get_country) $ df["type"] = df["type"].fillna("UnkownType") $ df = df.drop(axis= 1, labels=  ["location"]) $ df.to_csv('attacks.csv', index = False)
treatment = df2[df2["group"] == 'treatment'] $ treatment_conv = treatment[treatment["converted"] == 1] $ treatment_conv_prob = treatment_conv.shape[0]/treatment.shape[0] $ treatment_conv_prob
lagged = minute_return.tshift(1, 'min').between_time('9:30', '16:00') $ lagged.at_time('9:30')
store_items.pop('new watches') $ store_items
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_partial_autocorrelation(doc_duration, params=params, lags=30, alpha=0.05, \ $     title='Weekly Doctor Hours Partial Autocorrelation')
print(data['Year']) $
result['teamId'].nunique()
pd.read_sql_query("SELECT id,last_name FROM person WHERE id > 2;", conn, index_col="id")
df_twitter = pd.read_csv('data/twitter_archive_master.csv') $ user_extract = pd.read_csv('data/user_extract_master.csv')
OR = np.exp(-1.9888-0.0150) / np.exp(-1.9888) $ print (OR)
festivals.at[2,'longitude'] = -87.7035663 $ festivals.head(3) $
calories_df = df_summary.groupby('Date').sum( $ ).reset_index().sort_values(by='Date', ascending=1) $ calories_df = calories_df.filter(items=['Date', 'Calories']) $ calories_df.rename(columns={'Calories': 'ttl_cal_burnt'}, inplace=True) $ calories_df['day_no']= calories_df.index + 1
df_sentiment.withColumn( $         "value_count", get_log_round_udf(add_1=False)(grouping) $     ).head()
month_df = pd.get_dummies(df_total.index.month, prefix='month') $ month_df.index =  df_total.index
up_bias_x10
enroute_4x_count_prop_overall = compute_count_prop_overall(enroute_4x, 'remappedResponses') $ enroute_4x_count_prop_overall
data = data.apply(lambda x: x.str.lower() if(x.dtype == 'object') else x) $ data.loc[data['opinion'].str.contains("best"), 'pos_count'] = data['pos_count'] + 1 $ data.loc[data['opinion'].str.contains("worst"), 'pos_count'] = data['pos_count'] + 1 $ data
df2.query('landing_page == "new_page"').shape[0]/df2.shape[0]
cityID = '944c03c1d85ef480' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fresno.append(tweet) 
plt.figure(3) $ appl_plot = plt.subplot() $ appl_plot.plot(appl['Close'],color='red') $ plt.legend(['Apple Close Value'],loc="upper left") $ plt.title('Valores de Cierre Apple')
education_data.drop('Category', axis=1, inplace=True)
n_new=df2[df2['landing_page']=="new_page"].shape[0] $ n_new
df.plot()
df['x'] = df.index $ df.plot.scatter(x='x', y='y')
y = df_series#pd.Series(y, index=dates) $ arma_mod = sm.tsa.ARMA(y, order=(2,2)) $ arma_res = arma_mod.fit(trend='nc', disp=-1)
sc = SparkContext(appName="Spark Streaming mini project") $ stc = StreamingContext(sc, 10)
to_be_predicted_Day2 = 21.39111393 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
print("Number of unique users: %d, thus %d entry in the set is duplicate." %(df2.user_id.nunique(), df2.user_id.count() - df2.user_id.nunique()))
accuracies=cross_val_score(estimator=svc,X=X_train,y=y_train,cv=10,n_jobs=1)
norm_features = ['diff_lng', 'diff_lat']
pd.options.display.max_colwidth = 130 $ ttarc.expanded_urls.head(16)
print(rmse_scores.mean())
!apt-get install -y -qq protobuf-compiler python-pil python-lxml
for i in range(1000): $     new_page_converted = np.random.choice([1,0], size=df_newlen, p=[pnew,(1-pnew)]).mean() $     old_page_converted = np.random.choice([1,0], size=df_oldlen, p=[pold,(1-pold)]).mean() $     diff = new_page_converted.mean() - old_page_converted.mean() $     p_diffs.append(diff)
df_cal.head()
experience.columns = ['RATE_'+str(col) for col in experience.columns]
df['Hour']=pd.DatetimeIndex(df['lpep_pickup_datetime']).hour
np.arange(5, 10)
date_clean(TestData, 'DOB') $ date_clean(TestData, 'Lead_Creation_Date') $ TestData[['DOB_clean', 'Lead_Creation_Date_clean']]
properati[properati['zone'] == "Tigre"]
def chunks(list_, n): $
pp = pd.DataFrame(pp, columns=['dril_pp', 'laziestcanine_pp', 'ch000ch_pp'])
review = !cat {TRN}{trn_files[7]} $ review[0]
df.dtypes
df[df['converted']==1].count()[0]/df.shape[0]
xml_in['authorId'].nunique()
joined_store_stuff.show()
bus['text'].iloc[1]
c.find_one({'born': {'$lt': datetime(2000, 1, 1)}, # AND $             'born': {'$gt': datetime(1900, 1, 1)}})
dict_category['position'] # position of category or position of project in category?
df.groupby('character_id').raw_character_text.unique().head()
indexes_insert_1 = df3.query('group == "treatment"').index $ df3.set_value(index = indexes_insert_1, col ='abs_page', value=1) # sets value 1 only for the selected indexes $ df3.set_value(index = df3.index, col ='intercept', value=1) # sets value 1 for all rows with no preselected indexes $ df3.head() $ df3.isnull().sum()
print(sl.two_measures.sum()) $ print(sl.second_measurement.sum())
def remove_punct(text): $     text_nopunct = "".join([char for char in text if char not in string.punctuation]) $     return text_nopunct $ infinity['text_clean'] = infinity['text'].apply(lambda x: remove_punct(x))
def cost_computation(theta, X, y): $     hx = sigmoid(np.dot(X, theta)) # predicted probability of label 1 $     cost = (-y)* np.log(hx) - (1-y)*np.log(1-hx) # log-likelihood vector $     J = cost.mean() $     return J
plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS,  color='blue') $ plt.xlabel("Engine size") $ plt.ylabel("Emission") $ plt.show()
df_repub['created_at'].dtypes
idx_names = all_sites_with_unique_id_nums_and_names['name'].unique() $ print (color.RED + color.BOLD + 'check_df ' + color.END) $ print('Contains: \n',len(idx_names),'unique site names') $ idx_id_nums = check_df['id_num'].unique() $ print('and \n',len(idx_id_nums),'unique site ID numbers') $
df2_control = df2.query('group == "control"') $ p_control = df2_control['converted'].sum() / df2_control.shape[0] $ p_control
top_bike['Distance'] = distance_list
archive_df[archive_df.tweet_id.duplicated()] $
df3['intercept'] = pd.Series(np.zeros(len(df3)), index = df3.index) $ df3['ab_page'] = pd.Series(np.zeros(len(df3)), index = df3.index)
data.info()
df_2009['bank_name'] = df_2009.bank_name.str.split(",").str[0] $
import pandas as pd $ import matplotlib.pyplot as plt
forecast_df['employed'] = 0 $ for ind, row in SANDAG_jobs_df.iterrows(): $     forecast_df.loc[ind, 'employed'] += row['JOBS'] $ forecast_df.head()
df_2018_talks
frame3
data = raw.copy() $ data = data.div((data['Close']/data['Adj Close']), axis=0) $ del data['Adj Close'] $ del data['Volume'] $ data.head()
cryptos.percent_change_7d > 25
bymin = walk.resample("1Min") $ bymin.resample('S').mean()
contractor_final.info()
print('Starting script at: ', datetime.datetime.now())
df2_new = df2.query('landing_page == "new_page"')['landing_page'].count() $ total = df2['landing_page'].count() $ prob = (df2_new / total) $ print(prob)   
df_index.head()
import pandas as pd $ import numpy as np $ df = pd.read_csv('census.csv') $ df = df[df['SUMLEV']==50] $ df
databreach_2017 = databreach_2017.dropna(axis=0,how='any')
round(len(df_twitter[df_twitter.dog_label == 'pupper']) / len(df_twitter.dog_label), 2)
autos['price'] = autos['price'].str.replace("$","").str.replace(',', '') $ autos['price'] = autos['price'].astype(int) $ autos['price'].head()
import os
for fashion_index, label in enumerate(model.classes_): $     fashion['%s-proba' % label] = np.round(probas[:, fashion_index], 4)
ts = pd.to_datetime('2015-01-15 08:30') $ ts
todaysFollowers[todaysFollowers[date].isnull()] #check nulls
(train_4.shape, y_train.shape)
import pandas as pd $ df_from_pd = pd.read_clipboard() $ df_from_pd
model_preds['prob_off'] = abs(model_preds['actual_class'] - model_preds['predic_prob'])
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key={}".format(API_KEY), $                  auth=('user', 'pass'))
data[data['Processing Time']>datetime.timedelta(148,0,0)]
score = cross_val_score(lr, Xs, y, cv = cv, verbose = 1)
ts.resample('5Min', how='sum')
CMVolMvt = (delta(df_new, df_old, ['CMDemandMW'], ConvertToMWH=True) * (df_old['CM_TP'] - df_new['RealPrice'])).rename('CMVolMvt') $ CMTPMvt = (delta(df_new, df_old, ['CM_TP']) * (df_new['CMDemandMW'] / 2)).rename('CMTPMvt')
nri = dftemp[(dftemp['Variable Name'] == "National Rainfall Index (NRI)")] $ nri = nri[(nri.Value>950) |(nri.Value<900) ] $ nri.head(10)
users.email = users.email.apply(email_clean)
plt.figure(figsize=(12,12)) $ sns.countplot(cars['brand']) $ plt.xticks(rotation=90)
full['<=30Days'].value_counts()#.mean()
calls_df.pivot_table(["length_in_sec"],["call_day"],aggfunc="mean")
store_items = store_items.rename(index = {'store 2': 'last store'}) $ store_items
import matplotlib.pyplot as plt $ plt.hist(p_diffs) $ plt.axvline(x = prob_convert_given_treatment-prob_convert_given_control,color='red') $
df_clean.columns
df.dropna(how='any',inplace=True)
chinadata["NFCR_change"] = chinadata.Non_Financial_Credit_Ratio.pct_change()
df_mod = countries_df.set_index('user_id').join(df.set_index('user_id'), how='inner') $ df_mod.head()
hp.listprice = hp.listprice.astype('float', inplace=True)
workbook = xlrd.open_workbook(datapath / datafile) $ sheet = workbook.sheet_by_index(0)
l = []
client.get_list_users()
import pandas as pd $ import facepy $ from facepy import GraphAPI $ from urllib.parse import urlparse
from rl.callbacks import ModelIntervalCheckpoint $ save_path= 'outputs/agent_portfolio-ddpg-keras/agent_{}_weights.h5f'.format('portfolio-ddpg-keras-rl')
dfCountry = pd.read_csv('countries.csv') $ dfCountry.head()
train['Events'].unique()
print(df2.info())
wgts['0.encoder.weight'] = T(new_w) $ wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w)) $ wgts['1.decoder.weight'] = T(np.copy(new_w))
temps_df[['Missouri', 'Philadelphia']]
my_employee
df_sale_price=pd.read_csv('Sale_Prices.csv',index_col=0) $ df_sale_price.head(5)
top100ratings=ratings.groupby('movieId').count().nlargest(100,'rating').reset_index()
display(data.head(4))
confusion_matrix(y_test,rf_pred)
food.dtypes
df2 = df.copy() $ df2.columns
mis_match = [] $ for i in range(len(image_predictions)): $     if (image_predictions['tweet_id'][i]) not in list(tweet_json['id']): $         mis_match.append(image_predictions['tweet_id'][i]) $ print(len(mis_match))
print('Maximum values of each column:\n', google_stock.max())
!cp ../submissions/nvsvm.csv ../../drive/ColabNotebooks/AV_innoplexus_html
df_transactions['membership_duration'] = df_transactions['membership_duration'].clip_lower(0)
df_new[['CA','UK','US']] = pd.get_dummies(df_new['country'])
autos[autos["price"] > 1000000]
datatest.loc[datatest.place_name == "Gregorio de Laferrere",'lat'] = -34.743953 $ datatest.loc[datatest.place_name == "Gregorio de Laferrere",'lon'] = -58.592342
store_items = store_items.drop(['store 1'], axis=0) $ store_items
dicttagger_price = DictionaryTagger(['price.yml'])
from pyspark.sql.types import * $ from pyspark.sql import SQLContext $ sc.stop() $ sc = pyspark.SparkContext(appName="ml") $ sqlContext = SQLContext(sc)
db.collection_names()
jobs.loc[(jobs.FAIRSHARE == 180) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
archive_clean.info()
exportparams = urllib.parse.urlencode({ $     'action': 'tweet-export', $     'format': 'csv'})
twitter_archive[twitter_archive.duplicated()]
print(pd.isnull(df).any(axis = 1).sum()) $ df.info() #there are no missing values
!du -h html_data.csv
!ls -lh store.h5
index # Note the frequency information
old_compiled_data.describe()
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $ p_new_page_converted = new_page_converted.mean() $ print(p_new_page_converted)
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=10000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
[list(np.ravel(item)) for item in normals] 
df.drop(bad_indices, inplace=True)
for index, row in df.iterrows(): $     df.loc[index,'is_ch_company'] = row.postcodes in row.ch_postcodes
sp = openmc.StatePoint('statepoint.082.h5')
wine_reviews.head()
tweet_archive_clean[tweet_archive_clean.text.str.contains(r"\d+\.\d*\/\d+")]
df_Tesla = pd.DataFrame.from_dict(dataset) $ df_Tesla[['created_at','text','hashtags','username','user_followers_count','topic']].head()
sp_500_adj_close_start = sp_500_adj_close[sp_500_adj_close['Date']==end_of_last_year] $ sp_500_adj_close_start
invalid_name_list = twitter_archive_full[twitter_archive_full['name'].str.islower()].name.unique() $ twitter_archive_full.loc[twitter_archive_full.name.isin(invalid_name_list), 'name'] = 'None'
loc_date_str = soup.find('p', attrs={'class' : 'article-location-publishdate'}).get_text() $ loc_date_str
run txt2pdf.py -o '2018-06-22 2014 FLORIDA HOSPITAL Sorted by discharges.pdf'  '2018-06-22 2014 FLORIDA HOSPITAL Sorted by discharges.txt'
start = timer() $ partition_to_labels(50) $ end = timer() $ print(f'{round(end - start)} seconds elapsed.')
sales = pd.read_csv('sales.csv') $ targets = pd.read_csv('targets.csv') $ men_women = pd.read_csv('men_women_sales.csv') $ print(men_women)
c.execute('SELECT city FROM weather where cold_month = "January" LIMIT 2 OFFSET 3') $ print(c.fetchall())
import regex
all_data[all_data['price_change'].isnull().values]
discover_first = discover_data.drop_duplicates(subset = ['email'], keep='first') $ discover_first['timestamp2'] = pd.to_datetime(discover_first['timestamp'].map(lambda x: x.replace('th', ""))) $ discover_first.drop('timestamp', axis=1,inplace=True) $ discover_first.head() $
tables = pd.read_html(facts_url) $ tables
df.head()
pct_outliers = len(pax_raw[pax_raw.paxstep>step_threshold]) / len(pax_raw)*100 $ print(f'Percent of minutes with > {step_threshold} steps: {pct_outliers}')
cursor = connection.cursor() $
X_test = test.drop(axis=1, labels='loan_status')
df['fetched time'] = df['fetched time'].astype('datetime64[s]') $ df['created_utc'] = df['created_utc'].astype('datetime64[s]')
cp311.columns = [ x.lower().replace(' ','_') for x in cp311.columns]
df_new.groupby(['country','landing_page']).converted.mean().plot(kind='bar');
s.index = ['a', 'b', 'c', 'd', 'e'] $ s
!mkdir {PATH}models
images.head()
%%time $ with tb.open_file(filename='data/NYC-yellow-taxis-100k.h5', mode='a') as f: $     table = f.get_node(where='/yellow_taxis_2017_12') $     table.cols.trip_distance.create_index() $     table.cols.passenger_count.create_index()
df_test_index = event_list $ df_test_index $
cols = tweet_json_df.columns.tolist() $ cols
some_id = patient_ids[-1] $ some_id
layer = item.layers[0] $ layer
df2[df2.duplicated('user_id')]['user_id']  #finding the duplicated users
prob_treatment = df2[df2.group == 'treatment']['converted'].mean() $ prob_treatment 
ab_df.nunique() $
rf = RandomForestClassifier(n_estimators = 50, min_samples_leaf = 3) $ rf = rf.fit(train_data_features, y_train)
articles = soup.find_all('article', class_='carousel_item')
Raw_Forecast.set_index("Date_Monday").groupby([pd.TimeGrouper('M'),"Product_Motor","Part_Number"]).sum().fillna(0)[["Qty"]]
test_bow = vect_bow.transform(test) $ test_tfidf = vect_tfidf.transform(test)
xyz = json.dumps(youtube_urls, separators=(',', ':')) $ with open('youtube_urls.json', 'w') as fp: $     fp.write(xyz) $
pd.DataFrame(records4.describe().loc['mean', ['Age', 'GPA', 'Days_missed']])
df.Opened  = pd.to_datetime(df.Opened , format="%m/%d/%Y %H:%M:%S %p") $ df.Closed  = pd.to_datetime(df.Closed , format="%m/%d/%Y %H:%M:%S %p") $ df.Updated = pd.to_datetime(df.Updated, format="%m/%d/%Y %H:%M:%S %p")
vectors = np.array([t.tolist() for t in train_set[0]]).astype('float32') $ labels = np.array([t.tolist() for t in train_set[1]]).astype('float32') $ buf = io.BytesIO() $ smac.write_numpy_to_dense_tensor(buf, vectors, labels) $ buf.seek(0)
ad_groups_zero_impr = ad_group_performance[ $     ad_group_performance['Impressions'] == 0 $ ] $ ad_groups_zero_impr
df.info()
autos['year_of_registration'].describe()
adopted_cats.loc[adopted_cats.Color.str.contains('Lynx Point'),('white')] = 1 $ adopted_cats.loc[adopted_cats.Color.str.contains('Lynx Point'),('gray')] = 1 $ adopted_cats.loc[adopted_cats.Color.str.contains('Seal Point'),('white')] = 1 $ adopted_cats.loc[adopted_cats.Color.str.contains('Lynx Point'),('black')] = 1
all_data_merge['review_text'] = all_data_merge.apply(fix_text,axis=1) $ all_data_merge['username'] = all_data_merge.apply(fix_name,axis=1) $ all_data_merge.shape
i
API_KEY = ''
yt.get_subscriptions(channel_id, key)
df_vow.head()
pd.DataFrame(data, columns=['year','state', 'pop'])
df2[df2['user_id']==773192]
!wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb
plt.plot(ages, weights,'.', alpha = 0.1) $ plt.xlabel("mother's age") $ plt.ylabel("birth weight") $
import statistics as s # Aliasing $ from statistics import variance as v, mode as m # Import methods from modules $ print("Mean =",s.mean(exampleList)) $ print("Variance =",v(exampleList)) $ print("Mode = ",m(exampleList))
os.chdir('input')
train.age.isnull().sum()
full_url = "https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced" #change this to loop through all $ high_res_soup = BeautifulSoup(requests.get(full_url).text, "html.parser") $ hem_hr = [x.attrs.get('href') for x in high_res_soup.find_all('a') if ".tif" in x.attrs.get('href')] $ hem_hr[1] $
data.text.notnull().resample("1T").sum().plot()
df = pd.read_csv('data.csv', parse_dates=['Date']) $ df.sample(10)
nfl.interest_over_time()
pd.DataFrame(result)
model.most_similar("mary") 
controlp = df2[df2['group'] == 'control']['converted'].mean() $ controlp
train['text'] = train['text'].apply(process_tweet)
psy_df5 = HAMD.merge(psy_df4, on='subjectkey', how='right') # I want to keep all Ss from psy_df $ psy_df5.shape
twitter_Archive.drop(['timestamp'], axis=1,inplace=True) $ twitter_Archive.info()
obj4
pd.Timestamp(pd.datetime(2012, 5, 1))
stock['next_day_open'] = stock.open.shift(-1) 
df_clean.dog_stage.value_counts()
top_songs.columns
experiment_run_details = client.experiments.get_run_details(experiment_run_uid) $ print(experiment_run_details)
columns = [u'bathrooms', u'bedrooms', u'created', u'description', u'features', u'interest_level', u'latitude', u'longitude', u'price'] $
type(mod.model)
accuracy = metrics.r2_score(y_test, predictions_sm) $ print accuracy
cp311.info()
data = data.dropna(subset=['name'], how='any') $ data.info()
fm_confident_over = fm_confident_over.rename('bet_won_over_pred')
import statsmodels.api as sm $ convert_old = num_of_converted_in_control $ convert_new = num_of_converted_in_treatment $ n_old = len(control_group_sample) $ n_new = len(treatment_group_sample)
sst = f.variables['sst'] $ sst
pd.merge(df1,df3,how='left')
df_new['intercept'] = 1 $ df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']]
file = 'https://assets.datacamp.com/production/course_2023/datasets/ebola.csv' $ ebola = pd.read_csv(file) $ ebola.info()
merged2.shape
print(festivals.at[2,'latitude'])
print(d['feed']['title']) $ print(d['feed']['link']) $ print(len(d['entries']))
log_with_day.select('dateTime', 'dayOfWeek').show(10)
import os $ import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib notebook
df2.dtypes
prob_kNN500x = kNN500.predict_proba(Test_extra) $ prob1_kNN500x = [x[1] for x in prob_kNN500x] $ Results_kNN500x = pd.DataFrame({'ID': Test.index, 'Approved': prob1_kNN500x}) $ Results_kNN500x = Results_kNN500x[['ID', 'Approved']] $ Results_kNN500x.head()
weather_yvr['Datetime']
df_vow.head()
highest_station = active_stations_data[0][0] #Storing the highest station as a variable $
df_NOTCLEAN1A.isnull().sum()
1-autos["registration_year"].between(1900,2016).sum()/autos.shape[0]
20550 - 20364 + 2985
wrd_clean['expanded_urls'].isnull().sum()
df_new[['CA','UK', 'US']]= pd.get_dummies(df_new['country']) $ df_new.head()
common_names.plot(kind = 'bar', figsize = (8, 8)) $ plt.title ('Top 5 most common dog names') $ plt.xlabel ('Dog name') $ plt.ylabel ('Amount of names');
cold_tweets = all_raw_tweets[all_raw_tweets['tweet'].str.lower().str.contains("cold")]
d_stream_profiles = pd.read_json('Twitter_SCRAPING/profile_tweets.json', lines=True) $ tweets_l_stream_profiles = d_stream_profiles['text'].tolist() # create a list from 'text' column in d dataframe $ print(tweets_l_stream_profiles[-1:])
df['kw-cliton'] = df['text'].apply(lambda t: 'cliton' in str(t).lower()) $ df['kw-debate'] = df['text'].apply(lambda t: 'debate' in str(t).lower()) $ df['kw-blacklivesmatter'] = df['text'].apply(lambda t: 'blacklivesmatter' in str(t).lower())
outlier_detection = OutliersDetection(titanic)
newdf.groupby([newdf.Hour_of_day]).Trip_distance.median()
a = a.reshape(4, 5) $ a
transactions.merge(users, how='outer', on=['UserID'])
combined_df = test.join(prediction_df) $ combined_df.head(15)
le_data_all.index.levels[0]
pd.merge(df_a, df_b, right_index = True, left_index = True) # merge based on indexes 
example1_df = sqlContext.read.json("./world_bank.json.gz")
from datetime import datetime
archive_clean = archive.copy() $ images_clean = images.copy() $ popularity_clean = popularity.copy()
df4.dropna(how = 'any')
df_clean.drop(df_clean[df_clean['retweeted_status_id'].notnull()== True].index, inplace= True) $ df_clean.shape[0]
convert = df['converted'].mean() $ print('The proportion of users converted is {}'.format(round(convert, 4)))
df2[(df2['group']=='control') & (df2['converted']==1)].converted.sum()/df2[df2['group']=='control' ].group.count()
beijing = beijing.rename(columns={'WindDirDegrees<br />' : 'WindDirDegrees'})
%%time $ project_id = 'mlab-194421' $ df = acquire_mlab_data(project_id, "dtp", "New York", 174, "01/01/13", "01/01/14") $ df.head()
weather.describe(include='all')
len(calls_nocontact)
df2 = pd.DataFrame(np.array([[10, 11], [20, 21]]), columns=['a', 'b']) $ df2
svc = SVC(random_state=20) $ param_grid = { 'C': [1, 0.5, 5, 10,100], 'decision_function_shape':['ovo', 'ovr'], 'kernel':['linear', 'rbf']} $ grid_svc = GridSearchCV(svc, param_grid=param_grid, cv=10, n_jobs=-1)
p_conv = df2['converted'].mean() $ print('The probability of someone converting, disregaring the page he/she visited is {:2f}'.format(p_conv))
tweet_data_df["date"]=pd.to_datetime(tweet_data_df["date"]) $ tweet_data_df.head() $
tmp = tweets.groupby(['snsuserid','text']).size().reset_index() $ tmp.rename(columns={0:'counts'},inplace=True) $ tmp.sort_values(by=['counts'],ascending=False).query("counts > 2").counts.sum()
dataframe.columns = column_names
articles = list(data['article']) $ authors = list(data['author']) $ topics = list(data['topic'])
unique_users = set(orders.user_id.unique()) $ selected_users = random.sample(unique_users, 20000)
df['launched_year'] = df['launched'].dt.year $ df['deadline_year'] = df['deadline'].dt.year
df_first = train.loc[:, ['gender', 'age', 'language', 'first_device_type', 'first_browser', 'country_destination']] $ df_first.tail()
test_pred = model.predict(val_iter).asnumpy() $ print np.mean((test_pred - testY)**2) $ test_plot = scaler.inverse_transform(test_pred) $ test_plot[:5], testY[:5]
%matplotlib inline $ import matplotlib.pyplot as plt $ plt.style.use('ggplot')
len(digits.target)
! ls -lrt
mean_for_each_weekday = mean_for_each_weekday[['weekday', 'duration(min)']] $ mean_for_each_weekday
df3.head()
alias_tokens = word_tokenize(unicode.decode('utf-8').lower()) $ print(alias_tokens)
image_predictions_df.describe()
twitter_archive_enhanced.head()
e = driver.find_element_by_id('date_arrival')
df_cat.head()
cleanedAutos=autos $ cleanedAutos.isnull().sum()
cashflows_act_investor_20150430_def=add_loan_rating(cashflows_act_investor_20150430_def,loans1)
rsi = relative_strength(fulldata_copy)
b = [str(i).split(';')[1] for i in a]
def get_weight(event, hours_left): $
ls -l *.csv
resturaunt_set = set(data_test["Restaurant"]) $ for resturaunt in resturaunt_set: $     data_test = add_next_order_days (data_test,resturaunt,38)
train_df = pd.DataFrame(train_df_dict) $ test_df = pd.DataFrame(test_df_dict)
df_length = len(df)         $ print(df_length)
df = pd.DataFrame(video_ids) $ df.head()
information_ratio.iloc[1] - 2
df.set_index('Country').reset_index().head(5)
target = ('../datasets/CSVs/1500000 Sales Records.hdf') $ target
df=pd.read_csv("outputs/allV_info.csv", sep='\t', index_col=0, keep_default_na=False, na_values="") $ df.shape
(data_archie['cur_purchase_price'] == 0.0).sum()
odm_organizations.head() $
def pivot_count(df, vertical_column, horizontal_column): $
Grouping_Year_DRG_discharges_payments.head()
merge_features = ['id', 'category', 'main_category', $        'state', 'country', 'usd_goal_real', $        'City', 'State', 'launched_atYM', 'Length_of_kick', $        'Days_spent_making_campign', 'City_Pop', 'staff_pick']
new_x.head()
df2[df2.user_id.duplicated(keep=False)].user_id
train_view.sort_values(by=0, ascending=False)[0:10]
np.random.random((5,5))*np.identity(5)
get_all_tweets('elonmusk')
ayush = relevant_data[relevant_data['User Name'] == 'AYUSH JAIN'] $ ayush['Event Type Name'].value_counts().plot(kind='barh')
change = [] $ for i in data: $     change.append(abs(i[2] - i[3])) $ max_change = round(max(change), 2) $ print('Max daily price change in 2017: ', max_change)    
Google_stock['Open'].max()
get_roc_auc_score(naive_bayes_classifier)
support.amount.sum()/merged.amount.sum()
m3 = m3.round(decimals=2) $ print("m3: ", m3)
dfEPEXbase = pd.DataFrame() $ dfEPEXbase['Price'] = EPEXprices $ dfEPEXbase['Volume'] = EPEXvolumes $ dfEPEXbase.head() # verify generated data frame
Quantile_95_disc_times_pay = df.groupby(['drg3','year']).quantile([0.1,0.9]) $ Quantile_95_disc_times_pay.head()
import re
from templates.invoicegen import create_invoice
commits_per_hour.plot.bar();
df.groupby("cancelled")["awards_referral_bonus"].value_counts()
clean_tokens = [] $ for t in tokens: $     if t not in sr: $         clean_tokens.append(t) $ clean_tokens[:10]
df_raw = pd.read_csv("./datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv") $ df = pd.read_csv("./datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv") $ print df_raw.head()
df[df.msno == '++1Wu2wKBA60W9F9sMh15RXmh1wN1fjoVGzNqvw/Gro=']
print(len(keys)) $ print(len(key_ids)) $ print(len(time))
df1 = pd.DataFrame({'full_text': full_text}) $ combined_df = pd.concat([df_urls, df1], axis=1, join_axes=[df_urls.index]) $ data = combined_df.drop(['source','type_material','headline','url'],axis=1)
print("GLM AUC on training = " + str(glm_model.auc(train = True)) + " and GLM AUC on validation = " + str(glm_model.auc(valid = True))) $ print("GBM AUC on training = " + str(gbm_model.auc(train = True)) + " and GBM AUC on validation = " + str(gbm_model.auc(valid = True)))
autos.head()
last_values = grouped_months.last() $ last_values=last_values.rename(columns = {'Totals':'last_v_T'}) $
df.dropna(subset=['insert_id'], how='all') $ df = remove_duplicate_index(df)
%%bash $ gsutil ls -l gs://$BUCKET/taxifare/ch4/taxi_preproc/
df.set_index('State', inplace=True) $ df.head()
training_data = data[data['loan_status']!='Current'].copy() $ training_data.ix[training_data['loan_status']=='Current', 'target']=-1 $ training_data.ix[training_data['loan_status']=='Charged Off', 'target']=1 $ training_data.ix[training_data['loan_status']=='Fully Paid', 'target']=0
tweetering = pd.read_csv('victim.csv',names=['Date', 'Text']) $ tweetering.Text.replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
tmp_df.to_file('geocoded_evictions.shp')
df.rating_denominator.value_counts()
league = pd.read_sql_query('select * from League', conn)  # don't forget to specify the connection $ print(league.shape) $ league.head()
g = df.groupby('start_station_name').count() #sorting by start station names $ g_sorted = g.tripduration.sort_values(ascending=False)#arranding in descending order
varlist = ['Monthly_Income', 'Existing_EMI', 'Loan_Amount', 'Loan_Period', 'Interest_Rate', 'EMI'] $ for v in varlist: $     for df in Train, Test: $         df[v + '_NAind'] = pd.isnull(df[v]) * 1 #make 0/1 integer instead of T/F $ Train.head(10)
from IPython.display import FileLink
ts.index.month
r.summary()
df['range'] = (df.max(axis='columns') - df.min(axis='columns'))
b = 1 $ def my_function(): $     b = 10  # This is not a new value assignment to the file-level 'b'... $ my_function() $ print("local_b: " + str(b)) $
my_tag={'name':'img','title':'Sunset boulevard', $        'src':'sunset.jpg','cls':'framed'} $ tag(**my_tag)
df.to_json("json_data_format_columns.json", orient="columns") $ !cat json_data_format_columns.json
new_page_converted = df_treatment_and_new_page.sample(no_of_samples_treatment) $ p_new = new_page_converted.converted.mean() $ p_new
rider_rat = austin['rider_rating'].value_counts().sort_index() $ driver_rat = austin['driver_rating'].value_counts().sort_index() $ print(rider_rat) $ print(driver_rat)
model = tree.DecisionTreeClassifier() $ print ('Decision tree') $ reg_analysis(model,X_train, X_test, y_train, y_test)
df.num_comments.median()
print(discharge_list[0].date) $ print(discharge_list[len(discharge_list)-1].date)
for c in df_test.columns: $     print(c, df_test[c].dtype)
X = stock.iloc[915:-1].drop(['target', 'target_class', 'news_sources', 'news_text', 'tesla_tweet', 'elon_tweet'], 1) $ y = stock.iloc[915:-1].target_class
dfTemp=df.rename(columns={'Indicator':'Indicator_ID'}) $ print(dfTemp.info()) $ print ("----------------------df not reflecting the change-------------") $ print(df.info())
np.exp(0.0469), np.exp(0.0783), 1/np.exp(-0.0674), np.exp(0.0118), np.exp(0.0175)
goog = web.DataReader("MSFT", "google", start, end) $ goog.head()
n_new = df2.query('group == "treatment"').shape[0] $ n_new
result1 = -df1 * df2 / (df3 + df4) - df5 $ result2 = pd.eval('-df1 * df2 / (df3 + df4) - df5') $ np.allclose(result1, result2)
df_nd101_d_b = df_pivot_days_b[df_pivot_days_b['nd_key_formatted'] == 'nd101'] $ df_nd101_d_b.head()
DataSet.tweetText[0]
reddit = praw.Reddit(client_id='d7f3uorplpvBCg', \ $                      client_secret='dNHe-zhXT7EqLp8f8vnxcMZ633U', \ $                      user_agent='terrible hack', \ $                      username='rsaavy', \ $                      password='19943303ras')
len(df)
rankings_USA['year'] = pd.DatetimeIndex(rankings_USA['rank_date']).year $
ben_out['type']='Benign' $ van_out['type']='Vandal'
autos["odometer"].value_counts()
null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size) $ null_vals
no_mismatch_oldpage = df[(df['landing_page'] == 'old_page') & (df['group'] == 'treatment')] $ print(len(no_mismatch_oldpage)) $ no_mismatch_newpage = df[(df['landing_page'] == 'new_page') & (df['group'] == 'control')] $ print(len(no_mismatch_newpage)) $ print(len(no_mismatch_oldpage)+len(no_mismatch_newpage))   
cat_vars = ['hour', 'dayOfWeek', 'month', 'year'] $ contin_vars = ['priceOpen', 'tradesCount', 'volumeTraded', 'priceHigh', 'priceLow'] $ n = len(joined); n
from sklearn.model_selection import cross_val_score, StratifiedKFold $ from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier
import pandas as pd $ import numpy as np $ import csv $ import ast
converted = df2.query('converted == 1').count()[0]/df2['landing_page'].count() $ converted
import pickle $ with open('weather_data.pkl', 'rb') as fp: $     df = pickle.load(fp)
print(f"Fit shape: {fit.shape}, Fit non-nulls: {fit.nnz}") $ print(f"Non-null fraction of total: {'{:.10f}'.format(fit.nnz/(fit.shape[0] * fit.shape[1]))}")
train = df[df.index < '2014-01-01'] $ test = df[df.index > '2013-12-31']
d + 1
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=' + API_KEY) $
obs_beginning_date = df3['Donation Received Date'].max() - dt.timedelta(days=365*2) $ obs_end_date = df3['Donation Received Date'].max() - dt.timedelta(days=365) $ print("The focus period begins at:", obs_beginning_date) $ print("The focus period ends at:", obs_end_date) $ df_survival = df3.loc[(obs_beginning_date <= df3['Donation Received Date']) & (df3['Donation Received Date'] <= obs_end_date)] $
new_converted_simulation = np.random.binomial(n_new, p_new, 10000)/n_new $ new_converted_simulation.mean()
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn as sns
tzs = tweets_df['userTimezone'].value_counts()[:10] $ print(tzs) $
print("Jaccard  Similarity Score: ", metrics.accuracy_score(y_test, yhat)) $ print("F1-score Accuracy: ",  metrics.f1_score(y_test, yhat, average='weighted')) 
cm5 = ConfusionMatrix(y5_test, predicted5) $ cm5.print_stats()
control_df = df2.query('group == "control"') $ ctrl_cr = control_df.converted.mean() $ ctrl_cr
properati['zone'].value_counts(dropna=False)
countries.country.nunique()
brand_counts = autos.brand.value_counts(normalize=True) $ brand_counts
str(datetime.now())
n_new=df2.landing_page.value_counts()[0] $ n_new
sample = pd.read_csv('../assets/sampleSubmission') $ test = pd.read_csv('../assets/test')
customer_visitors_new.index
df_group
df = pd.read_csv('ab_data.csv') $ df.head(5)
df_pr.sort_values('sentiment').iloc[:10]
ltc.info() $ eth.info() $ xrp.info()
len(predictor_names)
import tweepy $ import credentials $ import bts_classifier $ import pandas
list(squares.items())
dfData['rqual_score10'].hist()
le_indicators = wb.search("life expectancy") $ le_indicators.iloc[:3, :2]
autos = autos[autos["price"].between(1,350000)] $ autos["price"].describe()
df3.summary()
mr = minute_return.between_time('9:30', '16:00') $ mr.head()
BID_PLANS_df.head()
rGraphData.groupby(['from', 'to']).sum().head()
X = breast_cancer_df.drop('malignancy', axis=1) $ y = breast_cancer_df['malignancy'] $ from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y)
hr[hr["icustay_id"]==14882][:100].plot(x="new charttime", $                                  y=["chart delta"])
biology.plot()
df1 = clinton_df[clinton_df['source'] == "Twitter Web Client"].head(10) $ df2 = clinton_df[clinton_df['source'] == "TweetDeck"].head(10) $ pd.concat([df1, df2])
landcover = {'vegetation':1,'urban':2,'earth':3,'water':4} $ study_areas = ['mtbarker', 'swmelb', 'gunghalin', 'goldengrove', 'molonglo', 'nperth', 'swbris', 'swsyd', 'custom'] $ sat_bands = ['blue','green','red','nir','swir1','swir2'] $ dc_bands = sat_bands.copy() + ['cloud_mask'] $ colours = ['r', 'b', 'm', 'c']
autos_pr['brand'].value_counts()
containers[0].find("span",{"class":"date"}).contents[0]
info.info()
bins = [70, 72, 74, 76, 78, 80] $ temp_freq_dict={"Temperature":temp_list, "Frequency":frequency } $ temp_freq_df=pd.DataFrame(temp_freq_dict)
twitter_archive[twitter_archive['rating_denominator']<10]
future_pd_30360_arrears=make_future_pd(payment_plans_combined, loans1, arrears_dict,True,'2015-04-30') $ future_pd_30360_origpd=make_future_pd(payment_plans_combined, loans1, arrears_dict,False,'2015-04-30') $ future_pd_30360_arrears_latest_paid=make_future_pd(payment_plans_combined, loans1, arrears_dict,True,'2015-04-30',latest_paid_interval_investor) $ future_pd_30360_origpd_latest_paid=make_future_pd(payment_plans_combined, loans1, arrears_dict,False,'2015-04-30',latest_paid_interval_investor) $ future_pd_30360_origpd_all=make_future_pd(payment_plans_combined, loans1, arrears_dict,False)
def run(agent, env, num_episodes=20000, mode='train'): $
week42 = week41.rename(columns={294:'294'}) $ stocks = stocks.rename(columns={'Week 41':'Week 42','287':'294'}) $ week42 = pd.merge(stocks,week42,on=['294','Tickers']) $ week42.drop_duplicates(subset='Link',inplace=True)
X_train.info()
data[data.state == 'open'].sort_values('updated_at')
invoice_sat_dropper = ['description', 'fk_s_change_context_id','fk_x_invoice_hub_id','row_hash'] $ check_cols(invoice_sat, invoice_sat_dropper)
corpora.SvmLightCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.svmlight'), corpus) $ corpora.BleiCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.lda-c'), corpus) $ corpora.LowCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.low'), corpus)
forcast_col='Adj. Close'
Celsius._temperature = 'spag'
df_2017 = df[(df.year == 2017)]
ben_dummy.head(20)
df_comment.liked_by = df_comment.liked_by.apply(ast.literal_eval) $ s = df_comment.apply(lambda x: pd.Series(x['liked_by']),axis=1).stack().reset_index(level=1, drop=True) $ s.name='liker'
df2.shape
fin_pivot_table_ni = pd.pivot_table(fin_df, values = 'netIncome', index = ['symbol'], aggfunc = np.mean) $ fin_pivot_table_ni = fin_pivot_table_ni.rename(index=str, columns={"netIncome": "Avg Net Income"}) $ fin_pivot_table_ni.sort_values(by = ['Avg Net Income'], ascending = False)
learn.save('clas_1')
house_data['renovated'] = house_data['yr_renovated'].apply(lambda x: 1 if x > 0 else 0)
df.to_html('resources/html_table_marsfacts.html')
df_valid = pd.read_json("valid.json", orient="records")
sep2014.start_time, sep2014.end_time
lReligion = list(db.osm.find({"religion":{"$exists":1}})) $ print 'length of the list = ', len(lReligion) $ lReligion[:5]
iris_fromUrl.describe()
len(df2['user_id'].unique())
bldg_data_0 = bldg_data[bldg_data['255_elec_use']==0] $ bldg_data_0.groupby([bldg_data_0.index.year,bldg_data_0.index.month]).agg('count').head(100) $
df2 = df
plt.plot(pipe.tracking_error)
df.hist(bins=50, figsize=(15,15)); $
movies.overview[6]
size_c = df2.query("group=='control'").count()[0] $ size_c
display(output.count(), flight6.count(), flight7.count())
y_test_over[fm_bet_over].sum()
tweets_master_df.ix[309, 'expanded_urls']
lm_uk = sm.OLS(df_new['converted'], df_new[['intercept_uk', 'ab-page']]) $ results_uk = lm_uk.fit() $ results_uk.summary()
ttest_ind(monthly_medication_df.first_month_adherence, monthly_medication_df.second_month_adherence)
new_page_conv = np.random.binomial(n_new,p_new) $
print('Total number of error records: {}'.format(len(errors.index))) $ errors.head()
y_train.shape
val logs = ssc.socketTextStream(hostname, port)
df.groupby("three_day_reminder_profile_incomplete_sent")["cancelled"].mean()
groceries[[0, 1]]
SOCIAL_NETWORKS = ['mother','father','brother','sister','grandmother','grandfather','neighbor ','friend','community','church','school ','pastor','principal','kids','cousin','neice ','nephew','son ','daughter','group','facebook','twitter','tweet','help','neighborhood','call','NextDoor','manager','children','knock','gang','peeps'] $ DECISION_MAKING = ['stay','leave','shelter','ride','evacuate','flood','deep','higher','attic','get out','up to ','decision ','rob','theives','pet','dog','cat','motel','hotel','traffic','highway','road','dirty side','clean side','vacancy','go','shutters','stairs','elevator','power','electricity','order','mayor','Irene','NYCHA','car','subway','bus','basement'] $ ADAPTIVE_CAPACITY = ['Volunteer','cajun navy','boat','help','last time','phone','cell','pump','message','donations','rescue','organize','information','1979','built','TV','Radio ','government ','supplies ','wading','canoe','kayak','food','meals','cooking','light','strong','cash','money','assist','mission','generator','roof','gas','proud','cost ','milk','bread','eggs','store']
test_df = test_df.loc[test_df['lang'] == 'en'] $ en_test_df = test_df[['created_at', 'text']] $ print(en_test_df.shape) $ en_test_df.head()
train.head(1)
symbols = re.compile("[^a-zA-Z]") $ word_freq_df = word_freq_df[~word_freq_df.Word.str.contains(symbols)]
autos['ad_created'].str[:10].value_counts(normalize=True, dropna=False).head(10)
logit_mod = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ results = logit_mod.fit() $
distinct_user_values = df.nunique()['user_id'] $ print("Unique users are : {}".format(distinct_user_values))
my_cryptory.get_google_trends(kw_list=['bitcoin'])
megmfurr_tweets = pandas.read_csv('@megmfurr_tweets.csv') $ megmfurr_tweets
import pickle $ pkl_file = open('speeches_metadata_evidence.pkl', 'rb') $ speeches_metadata = pickle.load(pkl_file)
print('The number of unique users:', df['user_id'].nunique())
forked.to_pickle('data/pickled/new_subset_forks.pkl')
df_json_tweets['tweet_id'] = df_json_tweets['tweet_id'].astype(str) $ df_json_tweets.info()
ab_df2['intercept'] = 1 $ ab_df2['ab_page'] = pd.get_dummies(ab_df2.landing_page)['new_page'] $ ab_df2.head() 
most_informative_features_top_and_bottom(vectorizer=vectorizer, classifier=lasso2, binary=False, n=15)
mr = vwap / bars.open - 1 $ mr = mr.between_time('9:30', '16:00') $ lagged = mr.tshift(1, 'min').between_time('9:30', '16:00') $ pd.ols(y=mr, x=lagged)
goog.set_index('Date', inplace=True)
data2
merged2[["article_publisher_id", "PID"]] \ $     [(merged2["article_publisher_id"] != "") & $     (merged2["article_publisher_id"] != merged2["PID"])] \ $     .drop_duplicates()
segmentData.count()
serc_pixel_df['wavelength'] = metadata['wavelength']
pd.value_counts(ac['Dispute Resolution Status'])
df_dummy = pd.get_dummies(data=df_country, columns=['country']) $ df3 = df2.set_index('user_id').join(df_dummy.set_index('user_id')) $ df3.head()
db_4 = DBSCAN(eps=50, min_samples=4).fit(crosstab_transformed)
from sklearn.model_selection import train_test_split $ from sklearn.linear_model import LinearRegression $ from sklearn.ensemble import RandomForestRegressor
mobile = nvidia.filter(lambda p: devices[int(p['adapter']['deviceID'], 16)].endswith("M")) $ mobile.map(lambda p: len(p['adapters'])).countByValue()
test_sentence_stemmed = test_sentence.apply(lambda sequence: ' '.join(stemmer.stem(word) for word in sequence.lower().split())) $ test_sentence_stemmed[0]
df.groupby('state')['ID'].count()
hundred_stocks_df.tail()
high_polarity = df2[df2['polarity']>0.3] $ mid_polarity = df2[(df2['polarity']<0.3) & (df2['polarity']>-0.3)] $ low_polarity = df2[df2['polarity']<-0.3] $ print("The weighted mean low polarity is: {0}".format(np.mean(low_polarity['polarity'])*len(low_polarity))) $ print("The weighted mean high polarity is: {0}".format(np.mean(high_polarity['polarity'])*len(high_polarity)))
df2.query("landing_page=='new_page'").shape[0]/df2.shape[0]
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ logit = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ result = logit.fit() $ result.summary()
pax_raw[pax_raw.paxstep > step_threshold].tail(20)
e.map_rain(basemap=True)
vectorizer = TfidfVectorizer(max_df=1.0, min_df=0.0, sublinear_tf=True, $                              ngram_range=(1,3), stop_words=stoplist) $ X = vectorizer.fit_transform(train) $ X.shape
mean_price_by_brand = {} $ for brand in brands: $     price_by_brand = autos.loc[autos["brand"] == brand, "price"] $     mean_price_by_brand[brand] = price_by_brand.mean() $ mean_price_by_brand
df_total = pd.concat([df_uro_no_cat, df_dummies], axis=1)
odds_W = ['B365W', 'EXW', 'PSW', 'LBW', 'CBW', 'SJW', 'IWW', 'UBW', 'SBW', 'GBW', 'B&WW'] $ odds_L = ['B365L', 'EXL', 'PSL', 'LBL', 'CBL', 'SJL', 'IWL', 'UBL', 'SBL', 'GBL', 'B&WL']
cohort = sts.query(qry)
s = pd.Series([99, 32, 67],list('abc')) $ s.isin([67,32])
print(16 in squares) $ print('square of 4' in squares)  # calls .__contains__()
referee_1_groups = nba_df.groupby(["Referee1", "Referee2", "Referee3"]) $ referee_1_groups.size().sort_values(ascending = False).head(30)
from pandas.tseries.holiday import USFederalHolidayCalendar $ cal = USFederalHolidayCalendar() $ holidays = cal.holidays('2012', '2016') $ daily = daily.join(pd.Series(1, index=holidays, name='holiday')) $ daily['holiday'].fillna(0, inplace=True)
df.describe(include="all")
profit_table_simple = data_df[['ID','Profit','Month Name','Year']].copy() $ profit_table_simple.head()
new_page_converted/n_new - old_page_converted/n_old
unique_urls.query('mentions >= 10').sort_values('avg_payout', ascending=False)[0:20][['url', 'total_payout']]
df.head(2)
field_streams = list(field_stream_occurances_df.field_stream.unique())
df.head()
year_averages = session.query(Measurement.date, Measurement.prcp).\ $     filter(func.strftime("%Y-%m-%d", Measurement.date) > "2016-08-23").\ $     order_by(Measurement.date).all() $
data_with_dates['post'] = 1 $ data_with_dates.head()
file = root + 'gas/2018-2008_monthly_gas_NYC.csv' $ gas_df = pd.read_csv(file, header=0, skipinitialspace=True) $ gas_df.info() $ gas_df
df2 = pd.DataFrame(np.random.randint(1,100,size=(7,4)), $                    columns=['Carbs', 'Fats', 'Proteins', 'Other'], $                    index=["M","Tu","W","Th","F","Sa","Su",]) $ df2.plot(kind="bar")
g8_groups['area'].mean()
print(people.groupby(lambda x: GroupColFunc(people, x, 'a')).mean()) $ print(people.groupby(lambda x: GroupColFunc(people, x, 'a')).std())
tx, ty = tsne[:,0], tsne[:,1] $ tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx)) $ ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty)) $ pd.DataFrame(list(zip(tx,ty))).plot.scatter( $     x=0, y=1, alpha = .4);
stocks_info_df.to_sql(con=conn_helloDB, name='company_info', if_exists='replace', index=False)
twitter_archive_df = pd.read_csv('twitter-archive-enhanced.csv')
cpq.columns
doctors.index
p=[p_new for i in range(n_new)] $ len(p)
!earthengine task info 4VBSBIT62BXDT6JYVCIDLFHK
stories.dropna(thresh=9, axis=1).shape
df2t = df.query('group == "treatment" and landing_page == "new_page"')
roberts = nobel[nobel['Full Name'].map(lambda x: x.startswith('Robert'))] $ print("There were {} winners named Robert".format(len(roberts)))
p_old = df2[df2['landing_page']=='old_page']['converted'].mean() $ print("Probability of conversion for old page (p_old):", p_old)
traindf = bq.Query(query + " AND MOD(ABS(FARM_FINGERPRINT(reviews)),4) > 0").execute().result().to_dataframe() $ evaldf  = bq.Query(query + " AND MOD(ABS(FARM_FINGERPRINT(reviews)),4) = 0").execute().result().to_dataframe() $ traindf.head()
pred6 = nba_pred_modelv1.predict(g6) $ prob6 = nba_pred_modelv1.predict_proba(g6) $ print(pred6) $ print(prob6)
def wins(team, month, day, year): $     team = teamtables[team] $     team = team[team["Date"]<datetime.datetime(year,month,day)] $     return team["Wins"][len(team["Wins"])-1]
p_control_converted = df2[df2['group'] == 'control']['converted'].mean() $ print('The probability of an individual in the control group converting: ', p_control_converted)
continentdict = {'China': 'AS', 'Korea': 'AS', 'Canada': 'NA', 'France': 'EU', 'BRAZIL': 'SA', 'Russia': 'EU'}
Y, X = dmatrices('Y ~ X', data=df2) $ mod3 = sm.OLS(Y, X) $ res3 = mod3.fit() $ res3.summary2()
components1
inspector.get_table_names()
df = pd.read_csv('data/goog.csv') $ df
days = pd.date_range('2014-08-29','2014-09-05',freq='B') $ for d in days: print(d)
primitives = ft.list_primitives() $ print(primitives.shape) $ primitives[primitives['type'] == 'aggregation'].head(10)
pmol.df.head(3)
ddf = dd.read_csv('test-data/output/sample-xls-case-multisheet.xlsx-*.csv') $ ddf.compute().head()
for item in duration_data: $     index_to_datetime(item)
breaches.DataClasses.value_counts()
pd.set_option('max_columns',None)
! head readme.html 
active_stations[0][0]
c.execute('SELECT city FROM weather where cold_month = "January"') $ print(c.fetchall())
faa_data_phil_pandas = faa_data_pandas[faa_data_pandas['AIRPORT_ID'] == "KPHL"] $ print(faa_data_phil_pandas.shape) $ faa_data_phil_pandas.head()
df_b
df_madrid = pd.read_csv('madrid_df.csv') $ demo = df_madrid['text'][:5] # demo testing with 10 texts $ demo
print ("benchmark all zero: %s" %log_loss(y_val,np.zeros_like(y_val))) $ print ("benchmark all ones: %s" %log_loss(y_val,np.ones_like(y_val))) $ print ("benchmark all half: %s" %log_loss(y_val,0.5*np.ones_like(y_val))) $ randY=np.random.randint(2,size=y_val.shape) $ print ("benchmark random: %s" %log_loss(y_val,randY))
cd
rows_in_data = len(df2.index) $ rows_converted = (df2['converted'] == 1).sum() $ rows_converted/rows_in_data
print ('First Five Rows in the Training Data:\n\n',training_pending_ratio[0:5])
weather.loc[weather.NAME == 'RALSTON RESERVOIR, CO US'].boxplot(column="TMAX");
df['man_on_first'] = np.where(df['on_1b'] > 0 , 1, 0) $ df['man_on_second'] = np.where(df['on_2b'] > 0 , 1, 0) $ df['man_on_third'] = np.where(df['on_3b'] > 0 , 1, 0) $ df['men_on_base'] = df['man_on_first'] + df['man_on_second'] + df['man_on_third']
reduced_trips_data = trips_data.loc[abs(trips_data.duration - trips_data.duration.mean()) <= (3*trips_data.duration.std())] $
ratio = result["Fail"].div(result["Pass"]) $ ratio.sort_values(ascending=False, inplace=True) $ ratio $
print (evals_mean)
old_converted = np.random.choice([1,0], size=nold, p=[pmean, (1-pmean)]) $ old_converted.mean()
trump['text'] = trump['text'].str.lower() $
final_grades_clean = final_grades_clean.dropna(axis=1, how="all") $ final_grades_clean
df['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['UK', 'US']]) $ result = log_mod.fit() $ result.summary()
top_10_authors = git_log['author'].value_counts()[:10] $ top_10_authors
import time $ twitter_archive_master.timestamp = twitter_archive_master.timestamp.apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(x,'%Y-%m-%d %H:%M:%S +0000'))) $ twitter_archive_master.timestamp = np.array(twitter_archive_master.timestamp,dtype='datetime64[ns]')
df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'}, $                     'B': {0: 1, 1: 3, 2: 5}, $                     'C': {0: 2, 1: 4, 2: 6}}) $ df
from PIL import Image as pil_image $ import numpy as np
s = c.retrieve_set(setid=0xb) $ pprint(s.metadata())
df_ml_6208.tail(5)
!hadoop fs -ls -h stocks.csv.gz
Measurement = Base.classes.measurement $ Station = Base.classes.station
with open('dropbox/github/thinkful/unit1/data/lecz-urban-rural-population-land-area-estimates_codebook.\ $ csv', 'r', encoding = 'windows-1252') as inputFile: $     for line in inputFile: $         line = line.rstrip().split(',') $         print(line)
len(df_links)
df['Descriptor'].value_counts().head()
df[['Price Increase Count M/M']].tail(100) $ df[['Price Increase Count Y/Y']].tail(100) $ df[['Pending Listing Count M/M']].tail(100) $ df[['Pending Listing Count Y/Y']].tail(25) $
datetime.now().strftime('%A')
print("Unique users:", len(ab_file2.user_id.unique())) $ print("Non-unique users:", len(ab_file2)-len(ab_file2.user_id.unique()))
News_outlets_df.dtypes
np.std(p_diffs)
q = [] $ for row in c.execute('SELECT * from weather'): $     q.append(row[4]) $     print(row) $
n_old = df2.query('landing_page == "old_page"')['landing_page'].count() $ n_old
frames = [df,df2]
os.mkdir('input')
baseball.drop([89525, 89526])
y_col = 'variety_cleaned'
pd.to_numeric(new_dems.Sanders).describe() $
word = 'Data' $ it = iter(word) $ print(*it) $ print('again?...') $ print(*it)
X_valid = mnist.validation.images $ y_valid = mnist.validation.labels
proj_df['Project Grade Level Category'].value_counts()
print(ds_issm.geospatial_lat_max) $ print(ds_issm.geospatial_lon_max)
kwargs = {'alg': xgb.XGBRegressor, 'metric': mean_squared_error $           , 'X_train': X_train, 'y_train': y_train, 'X_dev': X_dev, 'y_dev': y_dev}
df_clean = df_clean[df_clean['retweeted_status_id'].isnull()]
!ls -la
session.query(Stations.station).count()
parameters = {'n_neighbors':[1,2,3,4,5,6,7,8,9,10], 'weights':['uniform', 'distance'], 'p':[1,2,3,4,5]}
newdf.info()
all_data_wide['year'] = pd.Series(all_data_wide.index.map(lambda dt: pd.Timestamp(dt).year), index=all_data_wide.index) $ all_data_wide['month'] = pd.Series(all_data_wide.index.map(lambda dt: f"{pd.Timestamp(dt).month:02}"), index=all_data_wide.index) # format to pad with leading zero
print('Number of results for {0}: {1}'.format(varcode, len(results)))
sales = pd.read_csv('sales_train.csv') $ item_categories = pd.read_csv('item_categories.csv') $ items = pd.read_csv('items.csv') $ shops = pd.read_csv('shops.csv') $ test = pd.read_csv('test.csv')
writers.groupby("Country").apply(sum)
data_donald_replies = pd.read_csv("compared_sentiments.csv", index_col=0, dtype={'reply_id':str}) $ data_donald_replies.head()
y_pred = rnd_reg.predict(X_test)
graph.number_of_edges()
df = pd.read_excel("../01-data/external/NO2_apr_mei_2016_notval.xlsx", na_values=[-9999])
n_rows = len(df) $ n_rows
emails_dataframe['dayofweek'].value_counts()
len(crimes['2011-06-15'])
dframe_team['cut_year'] = dframe_team['Start'].dt.year + 1 $ dframe_team['end_cut'] = dframe_team.cut_year.map(draftDates) $ dframe_team
importances=model_tree_6_b.feature_importances_ $ features=pd.DataFrame(data=importances, columns=["importance"], index=x.columns) $ features
talks_train = pd.read_json('train.json')
sensor_clean['ls8']
state_party_df['National_D_neg_ratio']['2016-08-01':'2016-08-07'].sum() / 7
twitter_df_merged.info()
tweets_raw = pd.read_table(filepath_or_buffer='tweets_terror2.txt', names=["lan","id","date", "user_name", "content"])
recommended_vids[:2]
df_CLEAN1A.info() $
performTimeSeriesCV(X_train, Y_train, 10, mdl1, mae)
df['Feedback'] = ['Positive', None, 'Negative'] $ df
obj.sort_index()
df.drop(['id','date','query_string','user'],axis=1,inplace=True)
test_orders_prodfill_finale=test_orders_prodfill_final2[['order_id','products']] $ test_orders_prodfill_finale.head()
other_stopwords = ['http://', 'https://www.', '\xe2\x80\xa6'] $ stop_words = set(stopwords.words('english')) $ stop_words |= set(other_stopwords)
data = pd.read_csv('price_data/coinbaseUSD_1-min_data_2014-12-01_to_2018-06-27.csv')
p_new=df2.converted.mean() $ p_new
concat_3 = pd.concat([df1, df2, df5], axis=1) $ concat_3
!ls ema*
display(data2.head(10))
st_columns = inspector.get_columns('stations') $ for c in st_columns: $     print(c['name'], c["type"]) $
import random $ L=[random.random() for i in range(100000)] $ print('sorting an unsorted list:') $ %time L.sort()
document = [i for i in tweets['text']]
from scipy.stats import norm $ norm.cdf(z_score), norm.ppf(1 - (0.05/2))
print("Logistic Regression") $ param_grid = {'solver':['newton-cg'], 'n_jobs':[-1], 'max_iter':[100, 300, 400], 'C':[0.01, 0.1, 1.0, 10.0, 100, 1000]} $ train_give_me_a_value(LogisticRegression(solver='newton-cg', n_jobs=-1), param_grid)
summary_all = df['MeanFlow_cms'].describe(percentiles=[0.1,0.25,0.75,0.9]) $ summary_all
model_128_name = 'Esri_Classification_CobbCounty_Cam128_v2' $ model_129_name = 'Esri_Classification_CobbCounty_Cam129_v2' $ model_130_name = 'Esri_Classification_CobbCounty_Cam130_v2'
import seaborn as sns $ %matplotlib inline $ sns.countplot(data.OutcomeType, palette='Set3')
print(X.todense()) $ print("") $ print("Words for each feature:") $ print(vectorizer.get_feature_names())
query = session.query(Measurement) $ rain = query.filter(Measurement.date >= year_ago_str).all() $ print(len(rain))
autos.info() $ print(autos.head(10))
df.convtd.min(), df.convtd.max()
joined = customers.join(transactions.set_index('CustomerID'), on='CustomerID', how='inner').reset_index(drop=True)
fb['2015-03']
rows,columns= df.shape #. To return the number of cells $ print('There are {} total records in the dataset with {} no.of columns in it.'.format(rows-1, columns))
df['intercept']=1 $ df[['control', 'treatment']] = pd.get_dummies(df['group'])
for df in (train,test): $     df["CompetitionMonthsOpen"] = df["CompetitionDaysOpen"]//30 $     df.loc[df.CompetitionMonthsOpen>24, "CompetitionMonthsOpen"] = 24 $ train.CompetitionMonthsOpen.unique()
df.head()
data = Fraud_Data.copy() $ for variable in ['country','sex','source','browser']: $     data[variable] = data[variable].factorize()[0] $ data.head()
try: $     c.loc[0] $ except: $     print("nah") $ c["j"]
happiness_df
merged_df = pd.merge(stool_df, tissue_df, how='outer', on=['Description', 'GROUP']) $ merged_df = pd.merge(merged_df, na_df, how='outer', on=['Description', 'GROUP']) $ merged_df.head()
output.to_csv( "randomforest.csv", index=False, quoting=3 )
df_repub['text'].dtypes
df_ad_airings_4.columns
result.index.names
len(chefdf.name)
y = df_new.converted $ X_cols = ['intercept'] $ for i in range(unique_countries.size - 1): $     X_cols.append(unique_countries[i]) $ X = df_new[X_cols]
stat_info_merge
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative= 'smaller') $ z_score, p_value
import matplotlib.pyplot as plt $ import seaborn as sns
lm = smf.ols(formula='sales ~ TV + radio + newspaper', data=data).fit() $ lm.params
old_page_converted=np.random.binomial(n=1,p=P_old,size=n_old) $ old_page_converted $
df_short=df=df[["start station latitude","start station longitude"]] $
baseball.sum()
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country']) $ df_new.head() $
df['clean_text'] = df['text'].apply(lambda x: preProcessing(x))
df_clean3.loc[979, 'text']
possible_bots = users.query("created_at > '2018-04-26'") $ possible_bots.shape
students.head()
keras_entity_recognizer.set_step_params_by_name("learner", $                                                  model__recurrence_type = 'RNN', $                                                  model__bidirectional= True, #False, $                                                  batch_size = 50, $                                                  n_epochs = 5) $
df2.query('landing_page == "new_page"').user_id.count()/df2.user_id.count()
aapl_opt.loc[:, 'Type']
old_page_converted = np.random.binomial(1,p_old,n_old) $ old_page_converted.mean()
from pyspark.sql import SQLContext $ from pyspark.context import SparkContext $ sc = SparkContext.getOrCreate() $ sqlContext = SQLContext(sc) $ type(sqlContext)
df_twitter_archive_copy.drop(['doggo','pupper','floofer','puppo'], axis=1, inplace=True)
stationcount = session.query(measurement).distinct(measurement.station).group_by(measurement.station).count() $ stationcount
size_c = df2.query("group=='control'").count()[0] $ old_page_converted = np.random.choice([0,1], size=(size_c,1), p=[1-po,po]) $
import numpy as np $ import pandas as pd $ %matplotlib inline
twitter_master1 = pd.merge(tweet_info, image_pred, left_on='id', right_on='tweet_id')
ts.plot(figsize=(12, 6))
df2[df2.duplicated(['user_id'], keep=False)]
def reorder_weekday(day): $   if day==1: $     return 7 $   else: $     return day-1
print ('training_pending_ratio: ', training_pending_ratio.shape)
bacteria_data[['value']]
index_weighted_cumulative_returns = calculate_cumulative_returns(index_weighted_returns) $ etf_weighted_cumulative_returns = calculate_cumulative_returns(etf_weighted_returns) $ helper.plot_benchmark_returns(index_weighted_cumulative_returns, etf_weighted_cumulative_returns, 'Smart Beta ETF vs Index')
baseball_newind.query('ab > @min_ab')
print (df.loc[100,'year1'])
df2['Agency'].value_counts()
df_birth['Country '].value_counts(dropna=False).head()
df_mas['rating_numerator'] = df_mas['rating_numerator'].astype(float) $ df_mas['rating_denominator'] = df_mas['rating_denominator'].astype(float)
df_merge = pd.merge(df_schoo11, df_students1, on='school_name') $ df_merge.drop(['School ID', 'Student ID'], axis = 1, inplace=True)
from selenium import webdriver
df[df.converted==1].user_id.count()/df.shape[0]
img_url_valles = soup.find('div', 'downloads').a['href']
pd.DataFrame.to_csv(merged)
print("filled in", 245 - click_condition_meta[pd.isnull(click_condition_meta['refr_source'])].shape[0], "NaN values")
print(type(dict_of_well_df['00/11-04-067-03W4/0']))
data_donald_replies_with_following_feature = pd.read_csv("trump_replies_with_following_feature.csv") $ data_donald_replies["following"] = data_donald_replies_with_following_feature["following_feature_vector"]
store_items = store_items.drop(['store 3'], axis = 0) $ store_items
temp_series_ny = temp_series.tz_localize("America/New_York") $ temp_series_ny
tweets.loc[tweets['sentiment_score'] == 0,'sentiment'] = 'neutral' $ tweets.loc[tweets['sentiment_score'] > 0,'sentiment'] = 'positive' $ tweets.loc[tweets['sentiment_score'] < 0,'sentiment'] = 'negative'
df.loc['20180103','A']
total_differences = df.sales - df.new_sales # create a series of all of the differences $ random_differences = generate_random(total_differences) $ random_differences
print('waiting...', end='') $ time.sleep(2) $ print('done!', end='')
dr_new_patient_data_plus_forecast $
df2.query("converted==1").count()[0]/len(df2)
for  feature  in  ['country']: $     print("{}: {}".format(feature, countries_df[feature].unique()))
total.iloc[-3:]
X_final_test_3 = X_test_best_coef[[c for c in X_test_best_coef.columns if "foreign born" not in c]] $ X_training_3 = X_training_best_coef[[c for c in X_training_best_coef.columns if "foreign born" not in c]]
fashion.columns
ordered_df = USER_PLANS_df.iloc[ordered_timelines]
import pandas as pd $ df1 = pd.read_csv("drive/NBA_Data_Hackathon/nba1617_gt.csv") $ df2 = pd.read_csv("drive/NBA_Data_Hackathon/nba1718_gt.csv") 
ts_split = TimeSeriesSplit(n_splits=3).split(X_train)
train_commits, test_commits = train_test_split(commits_per_repo, test_size=0.33)
tweet_data.groupby('stage')['tweet_id'].count()
autos["last_seen"].str[:10].value_counts(normalize=True, dropna=False).sort_index()
pickle.dump(nmf_tfidf_data, open('iteration1_files/epoch3/nmf_tfidf_data.pkl', 'wb'))
treat = ab_data.query('group == "treatment"') $ contr = ab_data.query('group == "control"') $ contr['landing_page'].value_counts(), treat['landing_page'].value_counts()
conversions_bydate = pd.DataFrame(totalConvs_month_byMC) $ conversions_bydate = conversions_bydate.groupby(['year', 'month', 'day'], as_index=False).sum() $ print 'DataFrame conversions_bydate', conversions_bydate.shape $ conversions_bydate.head()
result = [] $ for chunk in pd.read_csv('data.csv', chuncksize = 1000): $     result.append(sum(chunk['x'])) $ total = sum(result) $ print(total)
i = random.randint(0, X_train.shape[0]) $ rec = X_train.iloc[[i]] $ label = y_train.iloc[[i]] $ print("Actual help time: " + str(label['HT'].values[0])) $ print("Predicted help time: " + str(results.predict(rec).values[0]))
country_dummy = pd.get_dummies(fraud_data_updated['country']) $ fraud_data_updated = pd.concat([fraud_data_updated,country_dummy],axis=1)
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == 1').shape[0] $ convert_new = df2.query('landing_page == "new_page" and converted == 1').shape[0] $ n_old = sample_size_old_page $ n_new = sample_size_new_page
df1_clean.drop(['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'], axis = 1, inplace=True)
import pandas as pd $ url = "https://space-facts.com/mars/"
ticker, ticker_okay, name = list(zip(tickers, ticker_okays, names))[1] $ ticker, ticker_okay, name
autos_VW = autos[autos["brand"] == "volkswagen"] $ autos_VW["price"].describe()
cityID = '7c01d867b8e8c494' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Garland.append(tweet) 
events[['title', 'price']]
from sklearn.model_selection import train_test_split # Import the function that makes splitting easier. $ X_train_count, X_test_count, y_train_count, y_test_count = train_test_split(count_vectorized, df_train.author, train_size=0.75) $ X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_vectorized, df_train.author, train_size=0.75)
previous_month_date = end_date - timedelta(days=30) $ pr = PullRequests(github_index).get_cardinality("id").since(start=previous_month_date).until(end=end_date) $ get_aggs(pr)
monte.str[0:3]
df_new['CA_int_ab_page'] = df_new['CA'] * df_new['ab_page'] $ df_new['UK_int_ab_page'] = df_new['UK'] * df_new['ab_page'] $ df_new['US_int_ab_page'] = df_new['US'] * df_new['ab_page'] $ df_new.head()
trans.head()
! 7za x ./data/train-jpg.tar.7z $
final_topbikes['Timestamp index'] = final_topbikes['Timestamp +2'].apply(pmam) $ final_topbikes['Timestamp index'] = final_topbikes['Timestamp index'].apply(lambda x: $                                     dt.datetime.strptime(x,'%d/%m/%Y %H:%M %p')) $ final_topbikes.index = final_topbikes['Timestamp index']
df_25year=df[df['date']>'1991-02-24'] #last 25 years $ df_25year.groupby('origin')['description'].agg('count').sort_values(ascending=False).head(1)
orig_ct = len(dfd) $ dfd = dfd.query('in_pwr_5F_max >= 0.75 and in_pwr_5F_max <= 10.0') $ print(len(dfd) - orig_ct, 'eliminated')
all_years_by_DRG =Grouping_Year_DRG_discharges_payments['discharges'].groupby(level=['year','drg3']).sum() $ all_years_by_DRG.tail() 
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $
countries_df = pd.read_csv('./countries.csv') $ df4 = countries_df.set_index('user_id').join(df3.set_index('user_id'),how = 'inner') $ df4.head()
print('Prediction for index 0: {:.2f}'.format(example_model.predict(train_data)[0]))
gold_access.show(10*7, False)
date = datetime.datetime.strptime(   ) $ mask = 
data = pd.read_csv('time_series.csv', index_col = 'time')
autos = autos[autos["price"].between(200, 500000)]
S.decision_obj.simulStart.value = "2007-07-01 00:00" $ S.decision_obj.simulFinsh.value = "2007-08-20 00:00"
1/np.exp(results.params[3])
df=df[df['subreddit']=='Conservative'] $
df_numbers = pd.DataFrame({'Number':np.arange(5)}) $ df_numbers
treatment_converted = df2[df2['group'] == 'treatment']['converted'].mean() $ treatment_converted
tokendata = pd.merge(token_sendReceiveAvg_month,token_send_add_receiveAvg_month,how="left",on="ID")
print ('RMSE of age imputation via h2o model is {}'.format(np.sqrt(((y_age_predict - y_age_valid)**2).mean())))
t = ('RHAT',) #tuple with just one element $ c.execute('SELECT * FROM stocks WHERE symbol=?', t) $ print(c.fetchone())
gdax_trans_btc['Year'] = gdax_trans_btc['Timestamp'].dt.year $ gdax_trans_btc['Month'] = gdax_trans_btc['Timestamp'].dt.month $ gdax_trans_btc['Day'] = gdax_trans_btc['Timestamp'].dt.day $ gdax_trans_btc = pd.merge(gdax_trans_btc, usd_gbp_rate.iloc[:,1:], on=['Year','Month','Day'], how="left")
new_page_converted = np.random.choice([1, 0], size = n_new, p = [p_mean, (1-p_mean)], replace = True)
df_afc_champ2018['playnumber'] = df_afc_champ2018['playId'].apply(lambda x: int(str(x).replace('400999172', ' ')))
os.path.exists("classB.csv")
print('Train...') $ model.fit(x_train, y_train, $           batch_size=batch_size, $           epochs=n_epoch, $           validation_data=(x_test, y_test))
n_old = df2.query('landing_page == "old_page"')['user_id'].count() $ n_old
df = pd.read_csv(wiki_file_path)
tw.query('rating_denominator != 10')
df2 = df2.drop(temp.head(1).index)
users.rename(columns={'id': 'user'}, inplace=True) $ users.head()
or_list1 = df.query("group=='control'| landing_page=='old_page'").index
reddit.shape
pd.concat([d1, d3], axis=1)
d = r.json() $ d
df_test = text_classifier.predict(df_test) $ df_test.head()
display(monthly_medication_df.describe())
precipitation_df.sort_values("date", inplace = True) $ precipitation_df.head(15)
countries_df['country'].unique()
train.info()
pd.pivot_table(expenses_df, values = "Amount", index = "Buyer", aggfunc = np.sum)
my_list = [1,2,3,4,5,6,7] $ my_list[:-3]
df_closed = df_bug[date_cols] $ df_closed.head()
x = np.array([[1, 2, 3], [4, 5, 6]], np.int32)
df2.query('converted == 1').shape[0] / df2.shape[0]
a = np.random.randn(50, 600, 100) $ a.shape
manager.image_df.loc[0,'p_hash']
raw_large_grid_df.groupby(["subject","eyetracker"],as_index=False).duration.agg("median").groupby("eyetracker").agg("mean")
s_nonulls = s.dropna() $ s_nonulls
medals_data.to_csv("../data/medals.csv", index=False)
def id_annotation(row): $     new_row = row.copy() $     new_row['annotations']['classification_id'] = new_row['classification_id'] $     return new_row
df[df['Complaint Type'] == 'Noise - Residential']['Descriptor'].unique()
df3 = pd.merge(df1, df2) $ df3
dates = pd.to_datetime([datetime(2015, 7, 3), '4th of July, 2015', $                        '2015-Jul-6', '07-07-2015', '20150708']) $ dates
df_countries.head(10)
word = 'convolution' $ fig = w2v.create_2d_tsne_plot(word, number_closest_words=25)
last_year = calc_temps('2017-08-05', '2018-08-05') $ last_year = last_year[0] $
for col in data_df.columns: $     if np.unique(data_df[col].dropna().astype(str)).shape[0] <= 1: $         print(col)
notus.loc[notus['country'].isin(brazil), 'country'] = 'Brazil' $ notus.loc[notus['cityOrState'].isin(brazil), 'country'] = 'Brazil' $ notus.loc[notus['cityOrState'].isin(['Brasil', 'Brazil']), 'cityOrState'] = 'Brazil' $ notus.loc[notus['country'] == 'Brazil', 'cityOrState'].value_counts(dropna=False)
df2.head()
print('Retweet tweets: ' + str(len(tweets_clean.query('retweeted_status_id == retweeted_status_id')))) $ print('Total tweets: ' + str(len(tweets_clean))) $ tweets_clean = tweets_clean[tweets_clean.retweeted_status_id != tweets_clean.retweeted_status_id]
df = pd.read_csv('../data/raw_running_data.csv') $ print(type(df)) $ df.head(10)
n_old = len(df2_control)
cust_demo.duplicated().value_counts() $
df1 = pd.DataFrame(last_year, columns=['date', 'precipitation'])
metrics.accuracy_score(y_valid, y_pred)
vol = volume.between_time('10:00', '16:00') $ vol.head(20)
twitter_data_v2[~twitter_data_v2.tweet_id.isin(tweet_data_v2.tweet_id)]
top_songs.shape
session_top_subset[session_top_subset.user_id == '00023iyk9l']
def combine_names(row): $     if row.contributor_fullname.startswith('SEAN PARKER'): $         return 'SEAN PARKER' $     return row.contributor_fullname
input_ids = [n.node_id for n in inputNetwork.nodes()] $ from bmtk.utils.io.spike_trains import PoissonSpikesGenerator $ psg = PoissonSpikesGenerator(gids=input_ids, firing_rate=0.5, tstart=0.0, tstop=10000.0) $ psg.to_hdf5(file_name='network/source_input/poission_input_spk_train.h5')
complete_df[42:47]
kDistArray_6 = kDist(crosstab_transformed, 6)
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=00) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
liquor2016 = liquor[liquor.Date.dt.year == 2016]
season17["InorOff"] = "In-Season"
train.pivot_table(values = 'Fare', index = 'Title', aggfunc=np.mean)
df2[['intercept', 'ab_page']] = pd.get_dummies(df2['group']) $ df2.head()
Trip_Output_list = list(Trip_Output)
sales = np.random.randn(10) * 10 + 500 $ sales
import re $ spice_df = pd.DataFrame(dict((spice, recipes.ingredients.str.contains(spice, re.IGNORECASE)) $                             for spice in spice_list)) $ spice_df.head()
df.groupby(df.index.hour).apply(lambda x: len(x)).plot()
get_daily_close = lambda key: get_float_value( r_dict, key, 'Close') $ get_nextday_chg = lambda key: abs( get_daily_close(key) - get_daily_close( int(key)+1 ) )
pd.value_counts(ac['Description']).head()
last_date = df.iloc[-1].name $ last_unix = last_date.timestamp() $ one_day = 86400 $ next_unix = last_unix + one_day $ next_unix
df_msg[df_msg["attachments"]] $ "attachments", "edited", "reactions" $ mentions_with_id = pd.io.json.json_normalize(resp.json(), record_path='mentions', meta='id', $                                      record_prefix='mentions.') $ mentions_with_id.head()
df_journey['lat'] = pd.to_numeric(df_journey['lat']) $ df_journey['lon'] = pd.to_numeric(df_journey['lon'])
df10['country'] = df_loc10['country'] $ df10['input_string'] = df_loc10['input_string']
X_valid.shape
bool_sel = autos['registration_year'] > 2016 $ autos.loc[autos['registration_year'] > 2016,'registration_year'].value_counts() $
df[df['Descriptor'] == 'Pothole'].groupby(by=df[df['Descriptor'] == 'Pothole'].index.dayofweek).count().plot(y='Agency') $
df2_cntrl = df2.query('group == "control"') $ df2_cntrl_conv = df2_cntrl[df2_cntrl['converted'] == 1].count() $ total = df2_cntrl['converted'].count() $ prop_cntrl = (df2_cntrl_conv['converted'] / total) $ print(prop_cntrl)                   $
n_new = df2.query('landing_page=="new_page"').count()[0] $ n_new
df_ml_52_01.tail(5)
df.head(1)
values = pd.DataFrame(dataSeries.values) $ df = pd.concat([values.shift(1), values], axis=1) $ df.columns = ['t-1', 't+1'] $ result = df.corr() $ print(result)
trial_df = df.loc[(df.label==0)] $ trial_df['label'] = trial_df.label.apply(lambda a: int(a))
index = dframe_team['Draft_year'] $ dframe_team.set_index(index, inplace = True) # sets the column 'Draft_year' as the index' $ dframe_team.head()
df = populate_tweet_df(tweets)
X = vectorizer.fit_transform(clean_train_reviews)
data.shape
sample=train.head(100).copy() $
X_sl_training = X2[X2.index.isin(sl_train.index)] $ Y_sl_training= Y[Y.index.isin(sl_train.index)] $ X_sl_holdout = X2[X2.index.isin(sl_holdout.index)] $ Y_sl_holdout = Y[Y.index.isin(sl_holdout.index)]
(details['Number of Ratings'] == 0).value_counts()
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, inspect, func
result = pd.DataFrame(alg.predict_proba(test[features]), index=test.index, columns=alg.classes_) $ result.head()
vlc = df[(df.country == "es") & (df.city == "Valencia") & (df.status == "active")]
df.head().to_json("msft.json") $ !cat msft.json
total = sessions_sample.shape[0] $ top_actions = pd.DataFrame(data=sessions_sample.action.value_counts()) $ top_actions = top_actions.reset_index().rename(columns={"index":"action", "action":"count"}) $ top_actions["cumul_pct"] = top_actions["count"].cumsum() / total $ list_top_actions = top_actions.loc[0:19,"action"].tolist()
categoricalize(df) $ df
sess = tf.InteractiveSession()
engine.execute("select * from measurement limit 5").fetchall()
grid_search.best_score_
volume = [row[6] for row in r_data if row[6] is not None] $ avg_volume = sum(volume)/len(volume) $ print('The Average Daily Trading volume was: ' + str(avg_volume))
plt.figure(figsize=(20,6)); $ plt.plot(df['datetime'],df['MeanFlow_cms'],linewidth=0.5,color='orange') $ plt.ylim((0,700)) $ plt.show;
c = dft.groupby(['stamp_round'])['log_followers_count'].sum()
mb1 = mb_file.parse("Sheet 1", header=None) $ mb1.columns = ["Taxon", "Count"] $ mb1.head()
rfc_feat_sel = RandomForestClassifier(n_estimators=4000, max_depth=45, n_jobs=18)
generate_chart(precision_recall_curve, $                "", $                "injured", $                [('a', our_nb_classifier), ]) $ plt.show() $
np.exp(-0.0408)
twitter_archive_clean.info()
df.columns
pm_data.dropna(inplace = True) $ pm_data.shape
def clean_stopwords(text): $     stopwords = set(nl.corpus.stopwords.words('portuguese')) $     words = [i for i in text.split() if not i in stopwords] $     return (" ".join(words))
df.count()
dfss.price.mean()
tweets['created_at'] = pd.to_datetime(tweets['created_at'])
twitter_master2 =  pd.merge(twitter_master1, twitter_master, left_on='tweet_id', right_on='tweet_id')
output = pd.DataFrame(data={"id":test.id, "rating":predictions}) $ output.to_csv( "new_naive.csv", index=False, quoting=3 ) $
df_subset.dtypes
archive_clean.drop(['retweeted_status_id', $                     'retweeted_status_user_id', $                     'retweeted_status_timestamp'], $                   axis=1, inplace=True)
ab_new['country'].value_counts() $
smpl_join = smpl.join(contest_cr, F.col('end_customer_party_ssot_party_id_int_sav_party_id')==F.col('party_id'))#.select('party_id','decision_date_time','party_name','postal_code','address1','street_name','sales_acct_id').cache()
! head -n 5 ../../data/msft.csv # OS/Linux $
class_merged=class_merged[class_merged.date>'2014-03-31']
print('Groceries has shape:', groceries.shape) $ print('Groceries has dimension:', groceries.ndim) $ print('Groceries has a total of', groceries.size, 'elements')
svm_parameters = [ $                   {'C': [1, 10, 100, 1000], 'kernel': ['linear']}, $                   {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']} $                   ]
df.message.apply(word_tokenize)
df = pd.read_csv('data3.csv') $ df.head(10)
mgxs_lib.dump_to_file(filename='mgxs', directory='mgxs')
autos
country_lm = sm.Logit(df3.converted, df3[['intercept','ab_page','US','UK']]) $ country_result = country_lm.fit() $ country_result.summary()
plt.figure() $ precip_df.plot(kind = 'line', x = 'Date', y = 'Precip') $ plt.legend(loc='best') $
len(itos), len(stoi)
df_final.head()
bacteria2
import random $ metric_series_type_id = random.choice(metric_series_type_ids) $ url = form_url(f'metricTypes/{metric_series_type_id}') $ response = requests.get(url, headers=headers) $ print_body(response, max_array_components=3)
df2.converted.mean() $ print("Probability of individual converting:", df2.converted.mean())
df3[df3['STATION']=='1 AVE'].pivot_table(index = 'DAY', columns = 'WEEK', values = 'INCR_ENTRIES').plot(figsize=(10,3))
figs_std_dev = utility_patents_subset_df['number-of-figures'].std() $ figs_median = utility_patents_subset_df['number-of-figures'].median() $ utility_patents_subset_df = utility_patents_subset_df[utility_patents_subset_df['number-of-figures'] <= (figs_median + 3*figs_std_dev)] $ sns.distplot(utility_patents_subset_df['number-of-figures'], color="green") $ plt.show()
sl['second_measurement'] = np.where((sl.new_report_date==sl.maxdate) & (sl.mindate!=sl.maxdate),1,0)
learner_id = challange_1["learner_id"] $ len(learner_id)
nyc_311_data_filename = './data/rows.csv' $ nyc_311_data_file = Path(nyc_311_data_filename) $ if not nyc_311_data_file.is_file(): $     nyc_311_data_url = 'https://data.cityofnewyork.us/api/views/erm2-nwe9/rows.csv' $     download_file(nyc_311_data_url, nyc_311_data_filename)
countries_log_reg = sm.Logit(df4['converted'], df4[['country_UK', 'country_US', 'intercept']]) $ country_result = countries_log_reg.fit()
df_main.p3.head(3)
print(df.info())
learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)
df.groupby('Clus_km').mean()
rchillipivot = pd.pivot_table(files8, index='shortlistCandidateId', $                      aggfunc={'Tenure' : 'max', 'ExperienceId':'count'}) $ rchilli = pd.DataFrame(rchillipivot.to_records()) $ rchilli = rchilli.rename(columns={'shortlistCandidateId':'candidateid','ExperienceId':'cv_total_jobs','Tenure':'cv_max_tenure'})  $ rchilli.head()
nar4=nar3.merge(avg_yield[['avg_investment_apr']],left_on='fk_loan',right_index=True)
bnbA= bnb[(bnb['age']<80) & (bnb['age']>=18)]
lv_workspace.apply_subset_filter(subset='A') # Not handled properly by the IndexHandler
import requests $ res = requests.get("https://www.skyscanner.net/transport/flights/tpet/cts/180118/180130?adults=1&children=0&adultsv2=1&childrenv2=&infants=0&cabinclass=economy&rtn=1&preferdirects=false&outboundaltsenabled=false&inboundaltsenabled=false&qp_prevProvider=ins_month&qp_prevCurrency=GBP&qp_prevPrice=178&pricesourceid=b3ms-SG1-2#results")
X_today = X2.copy() $ X_today['age_well_years'] = X_today.age_well_years + X_today.time_since_meas_years
df_regression.rename(columns={'treatment': 'ab_page'}, inplace=True)
mortraffic_byday = (targettraffic.groupby(('STATION', 'weekdayname')) $                      .mean() $                      .reset_index())
new_items = [{'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4}] $ new_store = pd.DataFrame(new_items, index=['store 3']) $ new_store
twitter_archive_full[(twitter_archive_full.in_reply_to_status_id_x.isna()==False) | $                      (twitter_archive_full.retweeted_status.isna()==False)]
df2 = df.fillna(0) $ df2
df.head()
clean_review = review_to_words( train["review"][0] ) $ print (clean_review)
df_measures = pd.read_csv('../data/interim/df_measures.csv', encoding="utf-8")
print("Un ejemplo de un registro de Like: \n") $ print(json.dumps(l[0], indent=4))
click_condition_meta.dvce_type.unique()
print("Number of rows and features", tipsDFslice.shape)
loan_requests1['indebt:actual'].notnull().sum()
df2.drop_duplicates(subset="user_id" , inplace=True)
store_items.fillna(method='ffill', axis=0) # filled with previous value from the column
dfLikes.dropna(subset=["created_time"], inplace=True)
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin' $ S_lumpedTopmodel.executable = excutable +'/summa.exe'
df_subset.boxplot(column='Initial Cost', by='Borough', rot=90) $ plt.show()
user_df = stories['submitter_user'].apply(pd.Series)
fit.summary()
df_twitter_archive.loc[np.random.randint(0,df_twitter_archive.shape[0],40), ['text','name']]
print(np.isfinite(testDataVecs).all())
def parse_sensor_domain(parsedDF): $
list_tokens = [] $ for idx in range(len(message)): $     list_tokens.append([str(x) for x in message[idx].split(' ')])
from sklearn.model_selection import train_test_split
import requests $ from bs4 import BeautifulSoup
col['bike_involved'] = list(map(lambda x: 'BICYCLE' in x, col['all_vehicle']))
tipsDFslice.tail()
new_page_converted = np.random.choice([1,0], size=sample_size_new_page, p=[p_new_null,1-p_new_null])
df.isnull().any(axis=1).any()
sales_diff = sales_change1.merge(sales_change2, on = 'Store Number') $ sales_diff.head() $ sales_diff['2016_sales'] = total_sales['2016_q1_sales'] $ sales_diff.dropna(inplace = True) $ sales_diff.tail()
X.reset_index(inplace=True, drop=True)
sns.jointplot(x = "positive_ratio", y = "negative_ratio", data = news_df)
!rm microbiome.sqlite3
movies.shape
(ggplot(all_lum,aes(x="td",y="gx",color="subject"))+stat_summary(geom="line"))
clf.fit(X_train, y_train)
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
model_df['is_top_user'] = False $ model_df.is_top_user[model_df.author.isin(top_25_users)] = True $ model_df = model_df.drop(['author'], axis=1)
Lab7.loc[Lab7['ENTITY'] == 'OVH', 'RECORDS LOST'] = 5000000 $ Lab7.loc[Lab7['ENTITY'] == 'UbiSoft', 'RECORDS LOST'] = 10000000
p_control = df2[df2['group'] == 'control']['converted'].mean() $ p_control
stories.dtypes
i = 0 $ users_visits = check_visits(iter_visits, users_visits, i)
pgh_311_data["2017-06-13"]
n_old = (df2.landing_page == 'old_page').sum() $ n_old
np.dtype([('name', 'S10'), ('age', 'i4'), ('weight', 'f8')])
parser.BadLine
pd.concat([train_data['lifetime_revenue'], train_data.drop(['lifetime_revenue'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False) $ pd.concat([validation_data['lifetime_revenue'], validation_data.drop(['lifetime_revenue'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)
import copy $ newFormat = copy.deepcopy(formats.defaultFormat) $ newFormat['recorddefinition']['granularity'] = '1H' $ newFormat['recorddefinition']['minrecordcount'] = newFormat['recorddefinition']['maxrecordcount'] = 24 $ pprint.pprint(newFormat['recorddefinition'])
sc_cut = parks_sc.groupby(['ParkID'])[['Score']].mean() $ sc_cut.reset_index(inplace=True) $ sc_cut.rename(columns={"ParkID": "parkid"}, inplace=True) $ sc_cut.head()
data['register_date'] = data.register_date.apply(pd.to_datetime) $ data['register_month'] = data.register_date.apply(lambda date: date.month) $ data['register_year'] = data.register_date.apply(lambda date: date.year) $ data['register_year_week'] = data.register_date.apply(lambda date: str(date.year)+'/'+str(date.week)) $ data['is_weekend'] = data.register_date.apply(lambda date: True if date.weekday() in [4,5,6] else False)
stop_words_list = ['flashlight', 'light'] $ pipeline.set_params(posf__stop_words=stop_words_list, cv__stop_words=stop_words_update)
df3_results.summary()
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=123,stratify=y)
data = [] $ for f in fileList: $     with open(f) as data_file:    $         data.append(json.load(data_file)["results"])
print(df.apply(lambda x: abs(x)),'\n')  # example of element-by-element lambda fx $ print(df.abs()) # same result (to confirm)
imp_words = np.argwhere(rfc_feat_sel.feature_importances_ >= 1e-6) $ blurb_to_vect_red = blurbs_to_vect[:, imp_words.flatten()] $ print(blurb_to_vect_red.shape)
rows = session.query(Adultdb).filter_by(occupation="?").all() $ print("-"*100) $ print("Count of rows having occupation '?' after delete: ",len(rows)) $ print("-"*100)
df2[['control','ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('control', axis=1) $ df2.head()
params_file_path = os.path.join(data_dir, "entity_extraction_params.tsv") $ keras_entity_recognizer.export_params(params_file_path)
from sklearn.manifold import TSNE
a = 5 $ isinstance(a, int)
stations = [] $ for value in session.query(Measurement.station).distinct(): $     stations.append(value) $ len(stations)
conv_mean
for x in tweets_clean.dog_class.unique(): $     print('Mean favorites for ' + str(x) + ' class:' + str(tweets_clean[tweets_clean.dog_class == x].favorites_count.mean()))
corr_matrix['REVENUE']
uber_14["day_of_week"].value_counts()
!pip install -q matplotlib-venn $ from matplotlib_venn import venn2 $ _ = venn2(subsets = (3, 2, 1))
engine = create_engine("sqlite:////Users/daryarudych/Desktop/repos/SQL-Python/hawaii.sqlite")
median_comments = reddit['num_comments'].median() $ median_comments
df3.head()
tweets = pd.read_pickle('cleaned_tweets_data.pkl')
print list(label_encoder.inverse_transform([0,1,2])) $ model.predict_proba(np.array([0,50])) #first value is the intercept
s = pd.Series(gs.best_estimator_.feature_importances_,index=X.columns) $ (s.sort_values() $   .plot $   .barh(figsize=(5,8)) $ )
df4 = df3[df3['average_activity_count_per_liaison'] > 50]
    from pyspark.sql import Row $     rdd_example3 = rdd_example2.map(lambda x: Row(id=x[0], val1=x[1], val2=x[2])) $     print rdd_example3.collect() $     df_example3 = rdd_example3.toDF()
df_actuals = io.cache.prince.gas.demand.query_segment_demand(start_date, end_date, region='WA') $ actuals = df_actuals[df_actuals.Customer_Type=='MM'].groupby('Gas_Date').Daily.sum().reset_index()
df5 = make_df('ABC', [1, 2]) $ df6 = make_df('BCD', [3, 4]) $ display('df5', 'df6', 'pd.concat([df5, df6])')
tweet_counts_by_month.plot(subplots=True)
calls_df["call_day"]=calls_df["call_date"].dt.weekday_name $ calls_df["month_day"]=calls_df["call_date"].dt.day
n = -100 $ similar = LSI_model.get_dists(corpus[n]) $ print(corpus[n]) $ df.loc[list(similar.keys())]['desc'].values.tolist()
geo.head()
autos[:10]
tweets_l_scrape = d_scrape['text'].tolist() # create a list from 'text' column in d dataframe $ print(tweets_l_scrape[-1:])
autos["price"] = autos["price"].str.replace("$", "").str.replace(",", "").astype(int) $ autos["odometer"] = autos["odometer"].str.replace("km", "").str.replace(",", "").astype(int) $ autos.rename({"odometer": "odometer_km"}, axis = 1, inplace = True)
df_joy.shape
m[0].bs=1 $ m.eval() $ m.reset() $ res,*_ = m(t) $ m[0].bs=bs
transactions.merge(users,how="left",on="UserID")
print(train["comment_text"][0])
yc_new2.describe()
import datetime as dt $ listStart=dt.datetime(2016,11,19) $ dwk=dt.timedelta(days=7) $ asdf['WkEnd']=asdf.apply(lambda x: listStart+dwk*(x.wk-1),axis=1)
the_drg_number = 66 $ idx = df_providers[ (df_providers['year']==2011) & \ $                   (df_providers['drg3']==the_drg_number)].index.tolist() $ print('There are',len(idx),'sites for DRG',the_drg_number) $ print('Max payment:',np.max( df_providers.loc[idx,'medicare_payment'] ))
plot_data(df[~df.index.isin(df.query('state == "YY" and amount > 5000').index)])
df.message.head()
df['user_id'].unique().size
pokemon.drop(['Generation'],inplace=True,axis=1) $ pokemon.rename(columns={'#':'id'},inplace=True) $ pokemon['Type 2'] = pokemon['Type 2'].fillna('None') $ pokemon.info()
cg_u= df[df['group']=='control'] $ cg_u_prob_of_conv = cg_u['converted'].mean() $ print('The probability of a control group user converting is:  ' + str(cg_u_prob_of_conv))
weather_yvr.plot()
twitter_archive_full.loc[(twitter_archive_full.stage == 'None') & (twitter_archive_full.text.str.contains('doggos')), 'stage'] = 'doggo'
sns.barplot(x='hour',y='number_of_tweets',data=tweets_per_hour_df)
for col in ['Partner','Dependents','PhoneService','PaperlessBilling','Churn']: $      df_raw[col].replace({'Yes': 1, 'No': 0}, inplace=True)
tweet_df.head(2)
import pandas as pd $ import numpy as np
from bmtk.analyzer.visualization.spikes import plot_rates_popnet $ cells_file = 'network/recurrent_network/node_types.csv' $ rates_file = configure['output']['rates_file'] $ plot_rates_popnet(cells_file,rates_file,model_keys='pop_name')
l
print(q3d_answer)
pred_clean.info()
plt.plot(W,RMSE_list) $ plt.xticks(np.arange(min(W),max(W)+1,6.0)) $ plt.ylabel('RMSE') $ plt.xlabel('Window (months)') $ plt.title('RMSE by window')
plt.hist(counts, bins=100)
print(y[0])
help(ogh.getDailyWRF_salathe2014)
data = pd.concat([data, data_scraped], axis = 1) $ data.head()
md_keys = ['   ' + s + ':' for s in METADATA_KEYS] $ md_idx = tmpdf_md.index[tmpdf_md[tmpdf_md.isin(md_keys)].notnull().any(axis=1)].tolist() $ md_idx
df.head(3)
users_visits.visits =(users_visits.visits.fillna(0)/ np.timedelta64(1, 'D')).astype(int) $ users_visits.head()
tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['name']!='None']['name'].value_counts().head(10)
station_df = (all_turnstiles $            .groupby(['STATION'], as_index=False) $            .sum() $            .drop(['ENTRIES','EXITS'], axis=1)) $ station_df.sort_values('D_ENTRIES', ascending=False).head()
dftop2.head()
data.info()
nc_file = netCDF4.Dataset('../OISST/sample_nc_file.nc',mode='w',format='NETCDF4_CLASSIC') $ print(nc_file)
result = lo.fit()
df_grouped.columns.tolist()
df['converted'].sum()
actual_diff = p_conv_treat - p_conv_ctrl $ plt.hist(p_diffs) $ plt.axvline(actual_diff, c='red');
api = get_twitter_api(twitter_api['CONSUMER_API_KEY'], $                       twitter_api['CONSUMER_SECRET'], $                       twitter_api['ACCESS_TOKEN'], $                       twitter_api['ACCESS_TOKEN_SECRET'])
ser = pd.Series(d) $ ser.plot.bar(figsize=(18,10)) $ plt.show()
df2 = df2.drop_duplicates(subset=['user_id']) $
loans_df = loans_df.query('loan_status == "Fully Paid" | loan_status == "Charged Off" | loan_status == "Default"')
df_new.ab_page.mean()
sum(bet_over)
local_2 = pd.read_csv('mar_quarterly_raw_data/local_Nestle.reviews.adhoc.F180319T180331.2018_0411_1610.xlsx.csv') $ local = pd.concat([local_1, local_2]) $ local.rename(columns={'title':'product_description', 'creation_date':'review_date'}, inplace= True) $ local_mar = local[local_clm] $ local_mar.shape
n_new = len(df2.query('group == "treatment"')) $ print(n_new)
X_train_vals = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1])) $ X_valid_vals = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))
dfJobs['DTSTART'] = pd.to_datetime(dfJobs['DTSTART']) $ dfJobs['DTEND'] = pd.to_datetime(dfJobs['DTEND']) $ dfJobs['DTSTAMP'] = pd.to_datetime(dfJobs['DTSTAMP']) $ dfJobs['CREATED'] = pd.to_datetime(dfJobs['CREATED']) $ dfJobs['LAST MODIFIED'] = pd.to_datetime(dfJobs['LAST MODIFIED'])
!pip install python-twitter
df.toPandas().info()
msftAC.tshift(1,freq='D')
age_range_breakdown.sum()
raw_train_df.shape
merged_df['Month'] = merged_df.index.month
user.loc["Trump", "location"]
data.head()
autos_df = pd.DataFrame({"price_comp":price_dict, "odom_comp":odom_dict}) $ autos_df
df_main = pd.merge(df_main, preds_clean,on='tweet_id', how='inner')
days[days>"Sat"]
cleaned2 = pd.DataFrame(tweets)
tweet_clean.info()
notus['country'].value_counts(dropna=False)
grouped_dpt.first() # first row of each group 
tl_2050 /= 1000 $ tl_2050_norm = tl_2050 ** (10/11) $ tl_2050_norm = tl_2050_norm.round(1) $ tl_2050_alpha = tl_2050 ** (1/3) $ tl_2050_alpha = tl_2050_alpha / tl_2050_alpha.max().max()
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new.tail(10)
secondary_temp_columns=list(set(temp_columns)-set([primary_temp_column])) $ passive_temp_columns=[s for s in temp_columns if "passive" in s]
print train.shape
nodes_file = directory_name + 'nodes.h5'              # Contains information about every node $ node_models_file = directory_name + 'node_types.csv'   # Contains information about models
dfRegMet2016.shape
from sklearn import model_selection $ kfold = model_selection.KFold(n_splits=10, shuffle=True) $ loocv = model_selection.LeaveOneOut()
df.loc[df.sex.isin(sex2NA),'sex']=np.nan $ df.loc[df.sex.isin(sex2m),'sex']='m' $ print(df.sex.loc[df.sex.isin(sex2NA)==True].count()) $ print(df.sex.loc[df.sex.isin(sex2m)==True].count())
top_10_authors = git_log.groupby('author').count().apply(lambda x: x.sort_values(ascending=False)).head(10) $ top_10_authors
retweets['id'].groupby(pandas.to_datetime(retweets['created_at']).dt.date).count().mean() # 2.86
import lightgbm as lgb $ mdl7 =lgb.LGBMClassifier(boosting_type ='gbdt',objective ='binary', num_leaves =80, learning_rate =0.1) $ mdl7.fit(X_train,y_train)
contract.info()
df.asfreq('H', method='pad') $
print(r"'\xba\xba'.decode('mbcs'):",repr('\xba\xba'.decode('mbcs')))
print(airquality_melt.head())
menus_dishes_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/menu_dishes.csv')['Body'].read().decode('utf-8') $ menu_dishes = pd.read_csv(StringIO(menus_dishes_csv_string), header=0)
trump = trump.drop(["id_str"], axis = 1)
session.query(Stations.station, Stations.name, Stations.latitude, $               Stations.longitude, Stations.elevation).all()
deaths_Sl_concat['Total_deaths_Sl'] = deaths_Sl_concat['death_suspected'] + deaths_Sl_concat['death_probable'] + deaths_Sl_concat['death_confirmed'] $ deaths_Sl_concat.head()
search['flight_type_m'] = search['flight_type'].apply(flight_type_m)
df = df[df.year >2005]
sc.stop()
ls -n data.*
C_Group = df.query('group == "control" and landing_page == "new_page"').count()[0] $ T_Group = df.query('group == "treatment" and landing_page == "old_page"').count()[0] $ print ("Number of times not lined up is {}".format(C_Group+T_Group))
df_test.shape
old = df2.query('landing_page == "old_page"') $ new = df2.query('landing_page == "new_page"')
df.describe()
movies.dtypes
id_list = df_archive["tweet_id"]
print("dataframe df row names as follows : ") $ df.index.values
f=table.find(text='Fatalities').find_next('td').text $ fatalities=re.search(r'\d+', f).group() $ fatalities
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == True].shape[0]
dfNiwot = df.loc[df["NAME"] == 'NIWOT, CO US'].copy() $ dfNiwot.head() 
opath = "/Volumes/GDATA/data/CRW/outlook/"
X= preprocessing.StandardScaler().fit(X).transform(X) $ X[0:5]
yhat = loanTree.predict(X_test) $ print (yhat[0:5]) $ yhat
data_read = pd.read_csv("data_music_replay_train") $ data = data_read.copy()
df_raw_fb = pd.read_csv('./Datasets/Facebook_Training_Data.csv', encoding='latin1') $ print (df_raw_fb.head())
names = pd.Series(data) $ names
S.modeloutput_obj.remove_variable('scalarCanopyTranspiration')
import matplotlib.pyplot as plt $ plt.matshow(ibm_hr_target_small.select("*").toPandas().corr())
df_new['new_page'] = pd.get_dummies(df_new['landing_page'])['new_page'] $ df_new.head()
count_polarity
n_bandits = gb.size().shape[0] $ test_labels = [i for i in gb.size().index] $ mcmc_iters = 25000
(p_diffs >= obs_diff).mean()
df15 = pd.read_csv('2015.csv')
pm_final[(pm_final.obs_date < 40) & (pm_final.status == 1)]
ad_group_performance.loc[5]
df.converted.mean()
print(df.tail())
predicted_table.head()
df2['ab_page'] = df2.apply (lambda row: label_abpage (row),axis=1) $ df2['intercept'] = 1 $
print "The probability of an individual converting in the control group is {0:.4f}%".format(float(control_con[0])/sum(df2['group']=='control')*100)
P_old=df2['converted'].mean() $ print("convert rate for P_old under the null is {}" .format(P_old))
df_treat = df2.query('group == "treatment"') $ treat_convert = df_treat.query('converted == 1').shape[0] / df_treat.shape[0] $ treat_convert
from sklearn.metrics import confusion_matrix $ cm = confusion_matrix(y_true=y, y_pred=model.predict_proba(X)[:,0]>0.98) $ cm
test_centroids = np.zeros((test["review"].size, num_clusters), dtype="float32") $ counter = 0 $ for review in clean_test_reviews: $     test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map) $     counter += 1
df_all_wells_basic.isnull().sum()
from pyspark.sql import Row $ test_df = spark.createDataFrame(sc.range(180).map(lambda x: Row(degrees=x)), ['degrees']) $ sin_rad = functions.sin(functions.radians(test_df.degrees)) $ test_df.select(sin_rad).show()
ufos_df.plot(kind='bar', x='Reports', y='Count', figsize=(12, 5))
X_train['building_id'].unique().shape
fuel_type_dict = {"lpg": "lp_gas", "benzin": "gasoline", "cng": "natural_gas", $                   "hybrid": "hybrid", "elektro": "electric", "andere": "other"} $ autos["fuel_type"].replace(fuel_type_dict,inplace=True) $ autos["fuel_type"].fillna("not_specified", inplace=True)
workspaces = pd.DataFrame.from_dict(workspaces_list)
elon = pd.read_csv('twitter/elonmusk_tweets.csv')
payload_online = {"name": "Product Line Prediction", "description": "My Cool Deployment", "type": "online"} $ response_online = requests.post(endpoint_deployments, json=payload_online, headers=header) $ print response_online $ print response_online.text
import pandas as pd $ import numpy as np $ import re $ import statistics $ %matplotlib inline
daily_returns['SPY'].hist(bins=20,label='SPY') $ daily_returns['XOM'].hist(bins=20,label='XOM') $ plt.legend(loc='upper right') $ plt.show()
events.iloc[241]  # par index de la ligne
df['simple_job'].value_counts()
file_dir = os.path.dirname(os.path.abspath("__file__")) $ parent_dir = os.path.dirname(file_dir) $ newPath = os.path.join(parent_dir, 'bus_lines/bus_66_lt.csv') $ dfleavetimes = pd.read_csv(newPath, delimiter=',', index_col=False)
week28 = week27.rename(columns={196:'196'}) $ stocks = stocks.rename(columns={'Week 27':'Week 28','189':'196'}) $ week28 = pd.merge(stocks,week28,on=['196','Tickers']) $ week28.drop_duplicates(subset='Link',inplace=True)
df.columns
for c in ccc: $     if not os.path.isdir('output/' + c): $         os.makedirs('output/' + c)
import pandas as pd
nt["catfathername"].unique()
df2.user_id[df2.user_id.duplicated() == True]
S_1dRichards.decision_obj.groundwatr.options, S_1dRichards.decision_obj.groundwatr.value
dframe_team.drop([12], inplace=True) $ dframe_team.set_value(11, 'Executive', 'Vinny Del Negro, Andy Roeser, Gary Sacks') $ dframe_team.set_value(13, 'Start', '2012-09-04')
plt.rcParams['axes.unicode_minus'] = False $ dta_53.plot(figsize=(15,5)) $ plt.show()
col='Case.Status' $ tmp_df.loc[tmp_df[col]=='null',col] = float('nan')
grouped = options_data.groupby(['MATURITY', 'STRIKE'])['PRICE', 'IMP_VOL']
na_df.isnull() # check elements that are missing 
train.shape, validation.shape
url = "ftp://ftp.star.nesdis.noaa.gov/pub/sod/mecb/crw/data/outlook/v4/nc/v1/outlook/{:%Y}/{}".format(today_UTC,lfiles[-1])
data.loc[((data.surface_total_in_m2 > 70000) & (data.property_type == "apartment")), 'surface_total_in_m2'] = np.NaN
df.rolling(2).sum() # rolling sum with window length of 2 (sum of every 2 values) - ignored NaN
data = [['Alex',10],['Bob',12],['Clarke',13]] $ df = pd.DataFrame(data,columns=['Name','Age']) $ print(df)
calls_df["phone number"].nunique()-840
def print_time_range(col_name): $     print('--- {} ---'.format(col_name)) $     print('min: {}'.format(min(df[col_name]))) $     print('max: {}'.format(max(df[col_name]))) $     print()
corpus = [dictionary.doc2bow(text) for text in texts] $ corpora.MmCorpus.serialize('bible.mm', corpus) $ print(corpus)
Imagenes_data.tweet_id.unique().size
y_pred = log_clf2.predict(X_test_reduced) $ accuracy_score(y_test, y_pred)
df.loc['r_five']=[5,10,15,20,False] $ df
yt.get_video_metadata(video_id[0], key, parser=P.parse_video_metadata)
num_of_mentions_by_day = mentions_df["date"].value_counts().sort_index()
df2.query("group == 'treatment'")['converted'].mean()
df_master = pd.merge(df_clean, image_clean, on ='tweet_id', how= 'inner' ) $ df_master = pd.merge(df_master, tweet_clean, on = 'tweet_id', how = 'inner' )
cylData = autoData.map( lambda x: ( x.split(",")[0], \ $     x.split(",")[7])) $ print (cylData.take(5)) $ print ("\n") $ print (cylData.keys().collect())
rng.tz_convert('US/Eastern')
rfecv.ranking_
siteds
diff_ = pumaBB.merge(bbpc, right_on="gid", $                      left_on="public use microdata area")[["pcBB", "HC01"]] $ diff_["diff"] = np.abs(diff_["pcBB"] - diff_["HC01"].astype(float)) $ diff_.describe()
X_train.columns.values
print(11 * (5 * 3 - 5) + 4 / 3 ** 2 - 1) $ print (8 + (7 + 6 * 5)    # use parentheses $        + 4 / 3 ** 2 - 1) $ print(8 * (7 + 6 - 5) \ $       + 4 / 3 ** 2 - 1)       # use backslash
len(MATTHEW_92_USERS_SN),len(MATTHEW_92_USERS_DM),len(MATTHEW_92_USERS_AC)
print "First tweet timestamp (UTC)", df['created_at'][0] $ print "Last tweet timestamp (UTC) ", df['created_at'][9374]
df['MeanFlow_cfs'].describe()
autos['price'].value_counts().sort_index(ascending=False).head(30)
to_be_predicted_Day3 = 38.49419985 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
for col in b_list.columns: $     print(f'{col}...{len(b_list[col].unique())}')
n_old=(df2['landing_page']=='old_page').sum() $ n_old
d $ X = reddit[['Work Day Hours', 'Morning Hours', 'Upvotes']] $ y = reddit['Above_Below_Median'] $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
p_diff = p_new-p_old $ p_diff
%%time $ parsedDF = parse_allquery(allqueryDF)
todaysFollowers.head()
    def file_to_module(path): $         with loader: $             name = path.relative_to(Path(deathbeds.__file__).parent).stem.split('.', 1)[0] $             return getattr(__import__('.'.join(('deathbeds', name))), name)
rejected.columns = ['loan_amnt', 'issue_d','purpose', $                     'dti','zip_code','addr_state','emp_length']
df.columns
planets.dropna().describe()
df1.info()
df2.query('user_id == 773192')
indata_dir = 'data' $ indata     = 'hmeq' $ result = cassession.loadTable(indata_dir + '/' + indata + '.sas7bdat', casout = indata)
df.groupby(['landing_page', 'group']).count()
weather['created_date'] = pd.to_datetime(weather['created_date'], errors = 'coerce')
feature_names = ('sepal length', 'sepal width', 'petal length', 'petal width') $ efs1 = efs1.fit(X, y, custom_feature_names=feature_names) $ print('Best subset (corresponding names):', efs1.best_feature_names_)
model.doesnt_match("edit user wikipedia man fuck".split())
df_final_edited = pd.read_csv('twitter_archive_master_edited.csv', sep=',')
closes = s4p.pivot(index='Date',columns='Symbol',values='Adj Close') $ closes[:4]
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK']]) $ results=mod.fit() $ results.summary()
number_of_commits = len(git_log) $ number_of_authors = len(git_log.query("author != ''").groupby('author')) $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
df2.duplicated('user_id') $
to_match = pd.Series(['c', 'a', 'b', 'b', 'c', 'a'])
londonDFSubsetWithCounts = londonDFSubset.merge(grouped, on='OA11CD')
n_old=df2[df2['group']=='control'].shape[0] $ print(n_old)
predictions.printSchema()
df_tweets.info() $ print() $ print ('we could double check the current state of the new data frame by using "info" command')
c = count_noun_tags(corpus_blob)
!ls taxi_trained/export/exporter/
prophet_df.sample(4)
train_words = cv.fit_transform(fb_train.message)
hawaii_measurement_df = hawaii_measurement_df.replace(np.nan, 0)
year17 = driver.find_elements_by_class_name('yr-button')[16] $ year17.click()
fig, ax = plt.subplots() $ sns.stripplot(x='State', y='VN_Entropy', data=df_one, ax=ax) $ ax.set_ylim(0.0, 0.7) $ ax.set_ylabel('von Neumann Entropy') $ plt.savefig('../output/g_entropy_vs_states.pdf', bbox_inches='tight')
print(highest_open)
df.tail(3)
weekly_window = Window.partitionBy('week').orderBy(functions.desc('variance')) $ weekly_rank = functions.rank().over(weekly_window).alias('rank')
christian|57 $ jewish|1
excel_file_Path
len(get_comments(html))
autos.rename(columns={"odometer":"odometer_km"},index=str,inplace=True) $ autos.columns
len(train_data[train_data.fuelType == 'diesel'])
from scipy.stats import norm $ z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ print('z_score', z_score) $ print('norm.cdf', norm.cdf(z_score)) $ print('norm.ppf', norm.ppf(1-(0.05/2)))
ys=[5,4,6,5,6]
run txt2pdf.py -o '2018-07-09-2015-870 - SEPTICEMIA OR SEVERE SEPSIS W MV 96+ HOURS.pdf'  '2018-07-09-2015-870 - SEPTICEMIA OR SEVERE SEPSIS W MV 96+ HOURS.txt'
import clipboard $ clipboard.copy('A wild zebra') $ clipboard.paste()
pickle.dump(lda_cv_df, open('iteration1_files/epoch3/lda_cv_df.pkl', 'wb'))
images_predictions.nunique()
yhat = SVM.predict(X_test) $ yhat
fullDf[fullDf.country=='NP'].levelIndices.value_counts()
import seaborn as sns $ sns.set(style="darkgrid") $ ax = sns.countplot(x="AGE_groups", data=df_CLEAN1A) $
to_be_predicted_Day2 = 81.77623296 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
details = details[~details['Popularity'].str.contains('E')]
missing_values = missing_values_table(pay_train) $ missing_values.head(20)
nold = df2[df2['group'] == 'control'].shape[0] $ print(nold)
users= pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/users.csv') $ sessions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/sessions.csv') $ products = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/products.csv') $ transactions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/transactions.csv')
plt.scatter(y=dftouse_four.opening_gross, x=dftouse_four.star_avg,s=dftouse_four.review_count)
dict(idx_set)
ben_final['diffs'] = ben_final.groupby(['userid'])['revtime'].transform(lambda x: x.diff()) / np.timedelta64(1, 'm')
brandValues= addOne \ $     .reduceByKey(lambda x, y: (int(x[0]) + int(y[0]), \ $     x[1] + y[1])) $ print (brandValues.collect()) $
rodelar.gender()
results = reg1.fit() $ type(results)
wrd_full2 = wrd_full.copy() $ prediction_clean3 = prediction_clean.copy() $ wrd_prediction = wrd_full2.merge(prediction_clean3, on='tweet_id', how = 'left') $ wrd_prediction.drop(['source','in_reply_to_status_id','in_reply_to_user_id', 'retweeted_status_id', 'retweeted_status_timestamp','retweeted_status_user_id'],axis=1,inplace= True) $ wrd_prediction.head()
type(status.user)
for index, dtype in enumerate(df.dtypes): $     if dtype==np.int64 and df.columns[index] in x+y: $         df[df.columns[index]] = df[df.columns[index]].astype(float)
data_for_model = pd.concat([data, raw_data['final_status']], axis=1)
for c in ccc: $     vwg[c] /= vwg[c].max()
p_diff_actual = df2.query('landing_page=="new_page"')['converted'].mean() - df2.query('landing_page=="old_page"')['converted'].mean() $ p_diff_actual
rdd_from_df = sqlContext.sql("SELECT * FROM dataframe_name")
red.shape
df_agg = tmdb_movies_production_countries_revenue.groupby('production_countries').agg({'revenue': np.sum}) #aggregate
cell_log(11, "Generate new classifiers") $ forest = RandomForestClassifier(n_jobs=-1) $ svc = SVC() $ log_reg = LogisticRegression(n_jobs=-1)
df.drop(labels = ['text_no_urls', 'text_no_urls_names', 'text_no_urls_names_nums'], axis = 1, inplace = True)
dfData['rqual_score5'].value_counts()
treaty_id_as_str = str(treaty_id) $ treaty_id_as_str  ## output is a string
click_condition_meta.os_name = click_condition_meta.os_name.str.lower()
mars_df.columns = ['Name','Value']; $ mars_df = mars_df.set_index('Name'); $ mars_html = mars_df.to_html(); $ mars_html
odds.tail(3)
events.sort_values('speed', ascending = False).tail()
stories[['comment_count', 'score']].corr()
planets.head()
df_reader = pd.read_csv(file_wb, chunksize = 10) $ print(next(df_reader)) $ print(next(df_reader)) $
print('Avg Words Per Snt Brown Corpus: '\ $       , len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents()))
df2 = gdf_gnis[(gdf_gnis['FEATURE_ID'] >= 2300000)] $ df2.rename(columns={'_id': 'identifier'}, inplace=True)
twitter_archive_master=prediction_and_counts.merge(twitter_archive_clean, how='inner',on='tweet_id')
pd.value_counts(ac['If No Monitoring, Why?'])
start_df.head()
affordability['Ratio'] = affordability.Median_Sales_Price/affordability.Median_HH_Income $ affordability
twitter_data['date'] = twitter_data.index.date $ tweets_per_day = twitter_data['date'].value_counts() $ twitter_mean = twitter_data.groupby(twitter_data.index.date).mean() $ twitter_mean = twitter_mean.join(tweets_per_day)
all_news.to_pickle("all_news.pkl")
target_pf.shape
columns = inspector.get_columns('measurements') $ for c in columns: $     print(c['name'], c["type"]) $
fig, ax = plt.subplots(figsize=(10, 5)) $ fig.suptitle('Histogram Simulated Durations (Days)', fontsize=14, fontweight='bold') $ ax.set_xlabel('Days') $ ax.set_ylabel('Frequency') $ plt.hist(simulated_total);
probs = model2.predict_proba(x_test) $ print probs
top_tracks = df_track.merge(df_artist, on = ['artist_id','artist_name']).sort_values( $     by = 'artist_followers', $     ascending = False)[['track_name','artist_name','artist_followers']].head(5) $ top_tracks
tags = tweets[['hour','hashtag','text']][tweets.hashtag != 'No Hashtag'] $ item = tags.groupby(['hour','hashtag']).agg('count').unstack(1) $ yt = np.arange(0,500,50) $ xt = np.arange(0,24) $ item.plot(subplots=True, figsize=(10,10),yticks=yt, xticks=xt)
pop = get( m4pop )
[pts[7], pts[8], sidelensq(pts[7], pts[8])]
df_archive_clean.info()
test_post_words = test_post.split() $ print(test_post_words[:10]) # tokenize and lower case $ print(len(test_post_words))
<blockquote class="instagram-media" data-instgrm-captioned data-instgrm-permalink="https://www.instagram.com/p/BeBwOoND4Cc/" data-instgrm-version="8" style=" background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 1px; max-width:658px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);"><div style="padding:8px;"> <div style=" background:#F8F8F8; line-height:0; margin-top:40px; padding:62.5% 0; text-align:center; width:100%;"> <div style=" background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAsCAMAAAApWqozAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAMUExURczMzPf399fX1+bm5mzY9AMAAADiSURBVDjLvZXbEsMgCES5/P8/t9FuRVCRmU73JWlzosgSIIZURCjo/ad+EQJJB4Hv8BFt+IDpQoCx1wjOSBFhh2XssxEIYn3ulI/6MNReE07UIWJEv8UEOWDS88LY97kqyTliJKKtuYBbruAyVh5wOHiXmpi5we58Ek028czwyuQdLKPG1Bkb4NnM+VeAnfHqn1k4+GPT6uGQcvu2h2OVuIf/gWUFyy8OWEpdyZSa3aVCqpVoVvzZZ2VTnn2wU8qzVjDDetO90GSy9mVLqtgYSy231MxrY6I2gGqjrTY0L8fxCxfCBbhWrsYYAAAAAElFTkSuQmCC); display:block; height:44px; margin:0 auto -44px; position:relative; top:-22px; width:44px;"></div></div> <p style=" margin:8px 0 0 0; padding:0 4px;"> <a href="https://www.instagram.com/p/BeBwOoND4Cc/" style=" color:#000; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none; word-wrap:break-word;" target="_blank">Tattoo done by Bunny. #bunnydontinstagram #deluxetattoochicago</a></p> <p style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;">A post shared by <a href="https://www.instagram.com/deluxetattoochicago/" style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px;" target="_blank"> Deluxe Tattoo</a> (@deluxetattoochicago) on <time style=" font-family:Arial,sans-serif; font-size:14px; line-height:17px;" datetime="2018-01-16T22:38:44+00:00">Jan 16, 2018 at 2:38pm PST</time></p></div></blockquote> <script async defer src="//platform.instagram.com/en_US/embeds.js"></script>
crimes.info(null_counts=True)
autos['last_seen'] = autos['last_seen'].str[:10].str.replace('-', '').astype(int) $ autos['last_seen'].head()
x = calc_temps('2016-05-10', '2016-05-20') $ x
posts.plot(kind='scatter',x='postsCount',y='a')
df_subset['Total Est. Fee'].head()
print result
got_sentiment = rdf_model.predict(Y_tfidf) $ got_sentiment_df = pd.DataFrame(got_sentiment) $ got_sentiment_df.columns = ['sentiment']
experiment_run_details = client.experiments.run(experiment_uid, asynchronous=True)
rdf.loc[pd.isna(rdf.accident_counts),'accident_counts'] = 0.0
print(autos['ad_created'].str[:10].value_counts(normalize = True, dropna = False).sort_index()) $ print(autos['ad_created'].str[:10].value_counts(normalize = True, dropna = False).shape)
with open(outputs / 'data.pkl', 'rb') as f: $     new_data = pickle.load(f) $ print(new_data)
X_test_reduced = pca.transform(X_test) $ y_pred = rnd_clf2.predict(X_test_reduced) $ accuracy_score(y_test, y_pred)
df2_dummy.set_index('user_id', inplace=True)
plt.scatter(X2[:, 0], X2[:,1])
print len(nodes) $ for i,node in enumerate(nodes): $     if i<20: $         print i, node.get('href'), node.text
Imagenes_data.jpg_url.value_counts() $ Imagenes_data.jpg_url.unique().size
title = soup.title.contents[0][10:] $ title
%time tsvd = tsvd.fit(train_4)
news.dates=pd.to_datetime(news.dates) $ news['month'] = news.dates.dt.month $ news['year'] = news.dates.dt.year $ news.loc[news.dates.dt.date.astype(str) == '2017-10-06']
nfnf,nlnl=getFilteredClasses(data_for_embedding[0],data_for_embedding[1],100)
gh_hashlist
rh = ['real', 'estate'] $ real_estate = wk_output[wk_output.explain.str.contains('|'.join(rh))] $ real_estate.shape $ real_estate.to_csv('real_estate_feedback.csv')
pol_tweets.head()
tweets['hour'] = tweets['created_at'].dt.hour $ tweets['day_of_week'] = tweets['created_at'].dt.dayofweek $ days_of_week = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']
tlen = pd.Series(data=data['len'].values, index=data['Date']) $ tfav = pd.Series(data=data['Likes'].values, index=data['Date']) $ tret = pd.Series(data=data['Retweets'].values, index=data['Date'])
payload = {'api_key': 'apikey'} $ r = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json',params=payload) $
print sqlContext.sql("select * from ufo_sightings limit 10").collect()
df_group_by.head()
transactions.merge(users, how='left')
d = datetime.date(2016, 7, 8) $ d.strftime("On %A %B the %-dth, %Y it was very hot.")
Algorithms = ['KNN','Decision Tree','SVM','Logistic Regression'] $ Metrics = ['Jaccard','F1-score'] $ for i, yhat in enumerate([yhat1, yhat2, yhat3, yhat4]): $     for j, metric in enumerate([jaccard_similarity_score(y_test, yhat),f1_score(y_test, yhat,average='micro')]): $         print('Accuracy of ' + Algorithms[i] + ' algorithm using ' + Metrics[j] + 'metric:' + str(round(metric,3)))
ind = np.arange(len(feature_importances)) $ plt.bar(ind, feature_importances, .35)
session.query(Measurement.id, func.avg(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
s = select([employee]) $ result = conn.execute(s) $ row = result.fetchall() $ print(row)
val = val.split(b'\r\n\r\n',1)[1] $ print(val.decode('utf-8'))
p_old = df2.query('converted == 1').user_id.nunique() / df2.user_id.nunique() $ p_old
df.plot(kind = "scatter", x = "doc freq", y = "topic freq")
p_old = df2.converted.mean() $ print("{:.4f}".format(p_old))
feature_layer.properties.extent
numeric_df.agg(*corr_aggs).show()
df.info()
teams_df = pd.DataFrame.from_dict(teams_data).T
prcp_df.head() # Display the top 5 records of the dataframe
witf = open("latlong_test1.txt","w", encoding="utf-8") $ for i in range(len(test_kyo1)): $     witf.write("{location: new google.maps.LatLng(%f, %f)},\n" % (test_kyo1['ex_lat'][i],test_kyo1['ex_long'][i])) $ witf.close()
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=25000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
click.shape
labels2idx = {label: i for i, label in enumerate(clf.classes_)} $ sub = pd.DataFrame() $ sub["listing_id"] = test_df["listing_id"] $ for label in ["high", "medium", "low"]: $     sub[label] = test_prediction[:, labels2idx[label]]
df['converted'].value_counts()[1]/df['converted'].count()
sum(df['converted'] == 1) / 294478
df.price_doc.hist(bins=100) $
df = pd.read_csv("msft.csv", skiprows=[0, 2, 3]) $ df
pd.value_counts(ac['If No DR, Why?'])
control_converted = df2[(df2['group'] == 'control')]['converted'].mean() $ treatment_converted = df2[(df2['group'] == 'treatment')]['converted'].mean() $ diff = treatment_converted - control_converted $ p_diffs = np.asarray(p_diffs) $ (p_diffs > diff).mean() $
df2 = df2.drop(df2.index[2893])
absorption.export_xs_data(filename='absorption-xs', format='excel')
df = df[df['Updated At'] >= df['Shipped At']]
transactions.merge(transactions, left_on=['UserID'], right_on=['UserID'], how='inner') $
type(INQbyMONTH)
vocab = {v: k for k, v in vectorizer.vocabulary_.items()} $ vocab
archive_df.rating_numerator.value_counts()
df[['Principal','terms','age','Gender','education']].head()
A.add(B, fill_value=0)
plt.plot(close_series) $ plt.show()
station_distance.info()
nuevo_df =  df.loc[:,['seller','offerType','vehicleType','yearOfRegistration','gearbox','powerPS', $                      'kilometer','monthOfRegistration','fuelType','notRepairedDamage','abtest']] $ nuevo_df = pd.get_dummies(nuevo_df, columns=['brand','seller','gearbox','fuelType','offerType','vehicleType','notRepairedDamage']) $ print "Nuevas dimensiones del dataset:" ,nuevo_df.shape $ nuevo_df.head()
data_2017_12_14_iberia_negative = (data_2017_12_14_iberia.\ $                                    loc[data_2017_12_14_iberia.airline_sentiment == "negative"])["text_2"]
df2 = df.query('group != "treatment" and landing_page == "new_page"') $ df3 = df.query('group == "treatment" and landing_page != "new_page"') $ df2.shape[0] + df3.shape[0]
for sheet in sheets_with_bad_column_names.values(): $     sheet.rename(mapper=first_word, axis='columns', inplace=True)
dbname = 'prediction_db' $ username = 'xingliu' # change this to your username
knn_grid.fit(X_train,y_train)
plt.scatter(bnbAx.age, bnbAx.target) $ plt.xlabel('age') $ plt.ylabel('target')
temp = pd.DataFrame(temp) $ data.drop('date', axis=1, inplace=True) $ data = pd.concat([temp, data], axis=1) $ data.rename(columns={0: 'date'}, inplace=True)
clf.fit(X_train, y_train)
from bs4 import BeautifulSoup as Soup $ with open("./sample.html", "r") as sample: $     sample_contents = sample.read() $ sample_soup = Soup(sample_contents) $ print(sample_soup.prettify())
df_json_tweet.info()
features_to_use=[cols for cols in train.columns if cols not in ['readingScore','index']] $ features_to_use
MergeMonth['Sales Target'] = Targets
print(s['Movie1'])
svc.predict(X)
unique_df = df2.drop_duplicates('user_id') $ unique_df.nunique()
print("P-tG-converting:", $       df2[df2['group']=='treatment']['converted'].mean())
capitalize('fooBar',lower_rest=True)
df.dropna(axis='columns', how='all')
y_test.value_counts().head(1)/y_test.shape
pax_raw.head()
for idx in featuresToPreprocessByMean: $     store[idx] = store[idx].fillna(np.mean(store[idx]))
target_date = tmp_cov.index.get_level_values(0).unique() $ target_date = target_date[0] $ target_date
mars_current_weather
pyLDAvis.enable_notebook() $ vis = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary) $
auto.tail(15)
import pandas as pd $ import numpy $ import dill $ import sqlite3
print('Endpoint name: {}'.format(rcf_inference.endpoint))
guardian_data.head()
logit_mod = sm.Logit(df3['converted'], df3[['intercept','ab_page', 'CA', 'UK']]) $ results = logit_mod.fit() $ results.summary()
dict_creator['name']
data[['Sales']].resample('D').mean().rolling(window=4, center=True).mean().head()
np.sum((inches > 0.5) & (inches < 1))
%matplotlib inline $
pd.merge(df_a, df_b, on='mathdad_id', how='inner')
titanic_df['cabin_floor'] = titanic_df['cabin'].map(lambda x: find_floor(x))
inspector = inspect(engine) $ columns = inspector.get_columns('measurement') $ for column in columns: $     print(column["name"], column["type"])
df_new[df_new['country'].unique()] = pd.get_dummies(df_new['country'])
iphone = trump.loc[trump['source'] == "Twitter for iPhone", :] $ android = trump.loc[trump['source'] == "Twitter for Android", :] $ ax = sns.distplot(iphone['year'], label="Twitter for iPhone") $ sns.distplot(android['year'], label="Twitter for Android") $
convert_rate_old = np.mean([p_new, p_old]) $ convert_rate_old
y_train = train_shifted[[y_col]].as_matrix() $ X_train = train_shifted[X_cols].as_matrix()
print len(hpdpro) $ data = pd.merge(data, hpdpro, how='outer', on='ComplaintID') $ print len(data)
import tensorflow as tf $ import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ mydata  = pd.read_csv(path, sep ="\s+") $ mydata.head(5) $
data = fat.add_sma_columns(data, 'Close', [6,12,20,200]) $ data.tail()
df = pd.read_csv("Example Car SERPs.csv")
graf_counts2 = pd.DataFrame(graffiti2['graffiti_count'].groupby(graffiti2['precinct']).sum())
f = plt.figure(figsize=(12, 5)) $ ax1 = f.add_subplot(1, 1, 1) $ ax1.plot(4 + 2 * np.sin(np.arange(50)), 'g--', label='4 + 6*sin(x)')
autos[['date_crawled','ad_created','last_seen']][0:5]
to_be_predicted_Day2 = 21.38060307 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
base_url = "http://shop.oreilly.com/category/browse-subjects/data.do?sortby=publicationDate&page=1"
details.sort_values(by='Released')
predict.predict_score('Wikipedia')
quarterly_revenue.plot(kind="line") $ plt.show()
loans_df.head()
service_path = 'https://internal-nginx-svc.ibm-private-cloud.svc.cluster.local:12443' $ ml_repository_client = MLRepositoryClient()
cityID = '0a0de7bd49ef942d' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Scottsdale.append(tweet) 
lst = "University of Pretoria".split() $ for i in lst: $     print(i)
tests = pd.read_csv("tests.csv") $ combats = pd.read_csv("combats.csv")
plt.hist([x['pct.questions'] for x in list(ama_guests.values())] )
resDir  = "results/basic weekly move predict quaterly train results.csv" $
dem = dem[dem["subjectkey"].isin(incl_Ss)]
print len(clintondf)
np.exp(-0.408), np.exp(0.0099), np.exp(0.0149)
def format_cust_campaign(x): $     file = re.split("[_\-.]",x.lower()) $     file = file[1:-1] $     return file
def datetimemdyampm2datetime(text): $     try: $         return  pd.to_datetime(text,format='%m-%d-%Y %I:%M:%S %p') $     except AttributeError: $         return text 
plt.hist(taxiData.Trip_distance, bins = 50, range = [3, 7]) $ plt.xlabel('Traveled Trip Distance') $ plt.ylabel('Counts of occurrences') $ plt.title('Histogram of Trip_distance') $ plt.grid(True)
X_train_all = pd.concat([X_train_df, X_train.drop('title', axis=1)], axis=1) $ X_test_all = pd.concat([X_test_df, X_test.drop('title', axis=1)], axis=1)
print d.variables['trajectory']
In the previous case the the difference between the convertion rate for new website and old website is very less. $ By statistics we proved that old method has better conversion rates than the new method.
plt.barh(ai['title'], ai['quantity'], align='center') $ plt.show()
        print(df_everything_about_DRGs.loc[x[j],'year'],\ $                     df_everything_about_DRGs.loc[x[j],'discharges']['count']) $
xmlData.set_value(296, 'zipcode', ' 98011') $ print xmlData.loc[296]
Z = np.arange(50) $ Z[::-1]
df.to_sql('toronto_health_inspections', conn, if_exists='append', index=False) $ inspector = inspect(conn) $ print(inspector.get_table_names())
cat_sz = [(c, len(full_data[c].unique())) for c in cats] $ emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz] $ n_conts = len(full_data.columns) - len(cats)
findNumbers = r'\d+' $ regexResults = re.search(findNumbers, 'not a number, not a number, numbers 2134567890, not a number') $ print(regexResults.group(0))
S_distributedTopmodel.meta_basinvar.filename
gb = experiment_df.groupby('treatment_group_key')
def max_n(lst,n=1,reverse=True): $     return sorted(lst,reverse=reverse)[:n]
raw.head()
final_data.to_pickle('D:/CAPSTONE_NEW/jobs_data_final.pkl') $ gc.collect()
for node in nodes: $     offer_group_desc_dict[node[0]] = node[1] $     first_genre_dict[node[0]] = node[2] $     entity_type_dict[node[0]] = node[3]
list1[list1>=3]
plate_appearances = plate_appearances.loc[plate_appearances.events.isnull()==False,]
z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new], [n_old,n_new], alternative = 'smaller') $ z_score, p_value
pipe_lr_2 = make_pipeline(hvec, lr) $ pipe_lr_2.fit(X_train, y_train) $ pipe_lr_2.score(X_test, y_test)
merged2['Specialty'].isnull().sum(), merged2['Specialty'].notnull().sum(), merged2.shape
df2= pd.read_csv('https://query.data.world/s/bmpdbk2e2ags6f5mkrvup3tir7xuax')
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?&start_date=2017-01-01&end_date=2017-12-31&collapse=monthly&transform=rdiff&api_key=amQDZBVZxsNFXgn8Dmpo')
f1 = w.get_data_filter_object(subset = subset_uuid, step=1) $ f1.include_list_filter
learner.sched.plot()
print(df2[df2['group'] == 'control']['converted'].mean())
popt_ipb_brace_crown, pcov_ipb_brace_crown = fit(d_ipb_brace_crown)
df2.query('group == "control" and converted == 1').count()['user_id']/df2.query('group == "control"').count()['user_id']
p_old = df2.converted.mean() $ p_old
rcParams['figure.figsize']= (9, 5)   # set Chart Size $ rcParams['font.size'] = 14            # set Font size in Chart
df_2004.dropna(inplace=True) $ df_2004
sentiments.tail()
df.converted.sum()/df.user_id.nunique()
pos_tags = ["CC","CD","DT","EX","FW","IN","JJ","JJR","JJS","LS","MD","NN","NNS","NNP","NNPS","PDT","POS","PRP","PRP$","RB","RBR","RBS","RP","SYM","TO","UH","VB","VBD","VBG","VBN","VBP","VBZ","WDT","WP","WP$","WRB"]
zip_1_sns =  s_n_s_df[s_n_s_df.Zip==70117] $ zip_2_sns  = s_n_s_df[s_n_s_df.Zip==70119] $ zip_1_sns = zip_1_sns.groupby(zip_1_sns.Date).size().reset_index() $ zip_2_sns = zip_2_sns.groupby(zip_2_sns.Date).size().reset_index()
df = pd.read_csv("311-2014.csv", nrows=200000)
to_be_predicted_Day1 = 25.18 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
df['cleaned_description'] = cleaned_description $ cleaned_description = df.cleaned_description.apply(lambda x: ' '.join(x))
user_frequency = my_df["user"].value_counts() $ print("Shape:", user_frequency.shape) $ print(user_frequency.head(10)) $ print(user_frequency.tail(10))
precipitation_df.describe()
SGD = SklearnClassifier(SGDClassifier()) $ SGD.train(train_set)
metadata['bad_band_window2'] = refl.attrs['Band_Window_2_Nanometers'] $ metadata['bad_band_window2']
ds_issm = xr.open_dataset(data_url1) $ ds_issm = ds_issm.swap_dims({'obs': 'time'}) $ ds_issm
t = len(np.concatenate(trn_lm)) $ t, t // 64
features_rolling_averages.tail()
weekly_variance = diff_weekly_mean.groupBy('week','hashtag')\ $                                   .sum('sq_diff')\ $                                   .withColumnRenamed('sum(sq_diff)', 'variance')\ $                                   .orderBy('week','variance', ascending=[True, False])\ $                                   .cache()
pd.MultiIndex.from_arrays([['a','a','b','b'], [1,2,1,2]])
learn.sched.plot_loss()
np.exp(-0.0149)
users_visits = users_visits.groupby('chanel', as_index=False).sum() $ users_visits = users_visits.assign(logins=lambda x: x.visits/x.user) $ users_visits
import statsmodels.api as sm $ convert_old = df2.query("group=='control' & converted==1").count()[0] $ convert_new = df2.query("group=='treatment' & converted==1").count()[0] $ n_old = size_c $ n_new = size_t
s1.sample(3)
cp311 = cp311.dropna(axis = 0).copy()
theta_0 = 0.1* np.random.randn(X_train_1.shape[1]) $ theta = gradient_descent(X_train_1, y_train, theta_0, 0.1, 100)
elm.nanosecond
countries_df = pd.read_csv('countries.csv')
merkmale.xs(96439,level='id')
active_count = len(clean_users[clean_users['active']==1]) $ active_mean = clean_users[clean_users['active']==1]['account_life'].dt.days.mean() $ active_sd = clean_users[clean_users['active']==1]['account_life'].dt.days.std()
ab_df.shape[0]
dtm_df['news']
def get_date(dates): $     new_dates = [] $     for date in dates: $         new_dates.append(dt.datetime.strptime(date, '%d-%b-%y')) $     return np.array(new_dates)
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country']) $ df_new.head(10)
confederations = pd.read_csv("confederations.csv")
experiment_run_details = client.experiments.get_run_details(experiment_run_uid) $ training_run_uids = client.experiments.get_training_uids(experiment_run_details) $ for i in training_run_uids: $     print(i)
print(train_trees[random.randrange(len(train_trees))])
writer = pd.ExcelWriter("../visualizations/uber_avg_day_of_month.xlsx")
df['TOTAL_PAYMENT'].describe().astype(int)
p_new = df2["converted"].mean() $ print("Convert rate for P-new is: {}".format(p_new))
train.loc['p081434']
right = pd.DataFrame({'key': ['yes', 'no'], 'rval': [3, 0]}) $ right
favorite_count_3q = master_df.favorite_count.quantile(0.75) $ favorite_count_iqr = master_df.favorite_count.quantile(0.75) - master_df.favorite_count.quantile(0.25) $ outlier_df = master_df.loc[master_df.favorite_count >= (favorite_count_3q+(1.5*favorite_count_iqr))]
print(results)
tweet_length = pd.Series(data=data['len'].values, index=data['Date']) $ tweet_favourite = pd.Series(data=data['Likes'].values, index=data['Date']) $ tweet_retweet = pd.Series(data=data['RTs'].values, index=data['Date'])
len([earlyPair for earlyPair in BDAY_PAIR_qthis.pair_age if earlyPair < 3])
df_eve =df3.query('evening==1')
%%script false $ data = get_dummy(ibm_hr_final2, categorical_no_target ,numerical, label) $ data.show(5)
total1=total
df_questionable['link.domain'].value_counts()
df_inter_2 = pd.DataFrame({'A': [1, 2.1, np.nan, 4.7, 5.6, 6.8], $                            'B': [.25, np.nan, np.nan, 4, 12.2, 14.4]}) $ df_inter_2
content_performance_bytime.groupby('_merge').size()
n_old = df2.query("landing_page == 'old_page'").shape[0] $ n_old
output= "SELECT count(*) from tweet where urls='NO_URL'" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['Number of NULL Columns'])
min_home_score =2
mtcars = h2o.import_file(path = os.path.realpath("../data/mtcars.csv"))
display_topics(nmf, features, no_top_words)
c = c.applymap(lambda x: x if isinstance(x, list) else [])
!wget -O loan_train.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_train.csv
monthly_sales = sales.groupby(['date_block_num', 'year', 'month', 'shop_id', 'item_id']).agg({'item_cnt_day':'sum'}) $ monthly_sales.rename(columns={'item_cnt_day': 'item_cnt_month'}, inplace=True) $ monthly_sales.reset_index(level=['date_block_num', 'year', 'month', 'shop_id', 'item_id'], inplace=True) $ monthly_sales.head()
All_tweet_data_v2.name[All_tweet_data_v2.name.str.len() < 3].value_counts()
df2[df2['landing_page']=='new_page'].count()/df2.count()
indeed1 = indeed[['company','location','skills','title','salary_clean','category','duration_int','summary_clean']] $ indeed1.shape
diff = today - birthday $ print(diff)
delta_tickets = create_delta_tickets_time(df_responded) $ ax = plt.figure(figsize=(15, 8)) $ ax = sns.barplot(x="assigned_group", y="delta", hue="created_at", data=delta_tickets)
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s') $ git_log.describe() $
image_predictions_df[image_predictions_df['tweet_id'].duplicated()]
pivot_discover_first = pd.melt(discover_first, $                                id_vars='email', $                                value_vars=list(discover_first.columns[-11:-1]), $                                var_name='step_no', $                                value_name= 'SKU')
df2.head()
week24 = week23.rename(columns={168:'168'}) $ stocks = stocks.rename(columns={'Week 23':'Week 24','161':'168'}) $ week24 = pd.merge(stocks,week24,on=['168','Tickers']) $ week24.drop_duplicates(subset='Link',inplace=True)
ad_source = questions['ad_source'].str.get_dummies(sep="'")
plotSent(grouped_df)
df.describe() # will describe the only numerical variable present in the dataset
primary_temp_null_indx=dat[primary_temp_column].isnull()
df['year3'] = df.year1 $ df
vertices.printSchema() $ edges.printSchema() $
P_Treatment_Converted = (df2.query('converted == 1')['group'] == 'treatment').mean() $ P_Treatment = (df2.group=='treatment').mean() $ P_Converted_Treatment = (P_Treatment_Converted * P_Converted) / P_Treatment $ P_Converted_Treatment
l=len(col) $ for i in range(1,l): $     X=np.column_stack((X,col[i]))
from pandas import read_csv # for files downloaded from ThingSpeak $ from pandas import datetime # for date/time calculations $ from pandas import DatetimeIndex # for converting strings into date/time stamps $ import matplotlib.pyplot as plt # to plot graphs
pivoted.T[labels==1].T.plot(legend=False, alpha = 0.1);
test_clean_token = tc.clean_corpus(test_corpus, string_line=False) $ train_clean_token = tc.clean_corpus(train_corpus, string_line=False) $ test_bow, test_word_freq = tc.get_bow(test_clean_token) $ train_bow, train_word_freq = tc.get_bow(train_clean_token)
clf = clf.partial_fit(X_test, y_test)
df_subset['Total Est. Fee'].describe()
tweet_archive_clean.info()
org_member_id = '791f91db-ae04-46ba-ab6d-42a71058f5f6' $ url = form_url(f'organizationMembers/{org_member_id}/actions') $ response = requests.get(url, headers=headers) $ print_body(response, max_array_components=3)
data[0] $
print(df['State'].value_counts(dropna=False))
df['intercept']=1 $ df[['control', 'treatment']] = pd.get_dummies(df['group']) $
full_pipeline.fit(clean_df) $ appt_mat = full_pipeline.transform(clean_df)
prob_c= df2.query('group == "control"').converted.mean() $ prob_c
z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new], [n_old,n_new],alternative='smaller') $ print('z-score is:{}'.format(z_score)) $ print('p-value is:{}'.format(p_value))
f, ax = plt.subplots(figsize=(12,10)) $ dset['CRW_BAA_Week_04'].plot(ax=ax)
sorted(users.age.unique())
bigram_converter = CountVectorizer(ngram_range=(2,2), token_pattern='(?u)\\b\\w+\\b') $ x2 = bigram_converter.fit_transform(review_df['text']) $ x2
duplicated_user_id = df2.set_index('user_id').index.get_duplicates() $ duplicated_user_id
df2.query('group == "treatment"').converted.sum()/df2.query('group == "treatment"').converted.count()
X = df_train[features_to_use] $ y = df_train["interest_level"] $ X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)
train = timeseries.loc[:'2018-05-31'] ## creating training data without the last month $ test = timeseries.loc['2018-06-01':]  ## Will test the model using the last month data
ti_mar = pd.concat([ti_jd, ti_tmall, ti_suning]) $ ti_mar.shape
dfChile.head()
apiv.head()
small_df = df.head(10) $ small_df.iloc[0]['account'] $ list(small_df.index)
stop_words = nltk.corpus.stopwords.words('portuguese')
y_hat_test = model.predict(X_test)
pd.date_range(start, end, freq='BM')
df_new[['CA','US','UK']]=pd.get_dummies(df_new['country']) $ df_new.head() $
estimators = range(1, 15) $ param_grid = dict(max_depth = estimators) $ dt = RandomForestClassifier() $ grid = GridSearchCV(dt, param_grid, scoring='roc_auc') $
sdf.rdd.first()  # ["host"]
(counts == 1).mean()
subred_counts.head()
state_party_df['National_R_neg_ratio']['2016-08-01':'2016-08-07'].sum() / 7
df.index = pd.to_datetime(df.index)
negative.head()
(fe.bs.SPXmean, fe.bs.SPXsigma)
pumashp = pumashp.merge(pumaPop,on='public use microdata area')
my_df[my_df.isnull().any(axis=1)].head()
df2 = pd.read_csv('ab_edited.csv', sep=',')
purchases = [('2006-03-28', 'BUY', 'IBM', 1000, 45.00), $              ('2006-04-05', 'BUY', 'MSFT', 1000, 72.00), $              ('2006-04-06', 'SELL', 'IBM', 500, 53.00), $             ] $ c.executemany('INSERT INTO stocks VALUES (?,?,?,?,?)', purchases)
stream = tweepy.Stream(auth, l)
plt.hist(df['log_price'], bins=100) $ plt.show()
turnstiles_df.DESC.value_counts()
df2.head()
from pandas import Series, DataFrame $ import pandas as pd $ import numpy as np
DT_feature_impt = pd.DataFrame({'features':features.columns, 'importance':model_dt.feature_importances_}).sort_values('importance', ascending=False) $ DT_feature_impt.head(20)
users.to_csv('data_table/user.csv', index = False)
! aws s3 ls s3://olgabot-maca/lung_cancer/sourmash/ | wc -l
clean_measure.info()
clean_archive.head(10)
def get_integer2(s): $     return vehicleType_list.index(s)
prob_rf = rf.predict_proba(Test) $ prob_rf
from nltk.corpus import stopwords $ stop = stopwords.words('english') $ stop.extend([u'put', u'take', u'one', u'i', u'got', u'us', u'hello', u'hi', u'going', u'go', u'set', u'1drv', u'come']) $ print stop
tweet_df['metadata'] = tweet_df['created_at'] $ tweet_df['category'] = 'tweet' $ sou_df['metadata'] = sou_df['president'] + ': ' + sou_df['date'] $ sou_df['category'] = sou_df.president.apply(lambda x: 'trumpsou' if x == 'Donald J. Trump' else 'othersou') $ df = pd.concat([tweet_df, sou_df])
df.iloc[ [0,3] ]    # by row numbers
conv_users = df2.query('converted == 1').shape[0] $ p1 = float(conv_users/df2.shape[0]) $ print("The probability of an individual converting regardless of the page is {:.4f}".format(p1)) $
classifier_comparison_df.to_pickle('classifier_comparison_table.pkl')
print("Unique bikes that are in this dataset: {}".format(len(data.imei.unique())))
df.iloc[[11,24,37]]
keras_entity_recognizer.save(pipeline_zip_file, compress_archive = False)
test_path = '../../Data/test_users.csv' $ test_users = pd.read_csv(test_path) $ print(len(test_users)) $ test_users.head()
nnew = (df2['landing_page'] == 'new_page').sum() $ print(nnew)
cal_dict
secret_corporate_pscs['address.country'].value_counts().head(20)
causes = pd.DataFrame(data['Cause'].value_counts()) $ causes
meets_credit_policy = doesnt_meet_credit_policy.logical_negation() $ meets_credit_policy.head(rows=2)
T = price_mat.shape[0] $ nr_coins = price_mat.shape[1] $ price_mat.shape
samsung = tweets[tweets['samsung'] == True] $ samsung.head()
tweetsIRMA = pd.read_sql("SELECT t.tweet_id,(SELECT MAX(DateTime) FROM irma as i WHERE i.DateTime <= t.created_at AND MINUTE(i.DateTime) = 0) as irmaTime FROM tweetCoords as t WHERE t.created_at >= '2017-09-10 13:00:00'", Database().myDB)
df.to_csv('movie_data.csv', index=False, encoding='utf-8')
mlb = MultiLabelBinarizer(classes=list(set(all_labels))) $ train_labels_bin = mlb.fit_transform(train_labels) $ test_labels_bin = mlb.transform(test_labels)
%%timeit $ with tb.open_file(filename='data/NYC-yellow-taxis-100k.h5', mode='r') as f: $     table = f.get_node(where='/yellow_taxis_2017_12') $     rows = table.where('(passenger_count == 1) | (passenger_count == 3)') $     amounts = [x['total_amount'] for x in rows]
support.amount.sum()/merge.amount.sum()
sns.regplot(x=reddit['Upvotes'], y=reddit['Comments'], $             line_kws={"color":"r","alpha":0.7,"lw":5}) 
import requests $ import io $ import pandas as pd
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page' and converted == 1").shape[0] $ convert_new = df2.query("landing_page == 'new_page' and converted == 1").shape[0] $ print (convert_old , convert_new , n_old , n_new)
psy_df4 = PDSQ.merge(psy_df3, on='subjectkey', how='right') # I want to keep all Ss from psy_df $ psy_df4.shape
merge.info()
experience = pd.get_dummies(questions['rate_experience'])
a=len(df.query("group =='treatment' & landing_page =='new_page' | group != 'treatment' & landing_page !='new_page'")) $ n_dont_line_up = n-a $ print(n_dont_line_up) $
articles = db.get_sql(sql) $ articles.head()
twitter_archive_master[twitter_archive_master.iloc[:,8:12].sum(axis=1) > 1]
dataset = pd.read_csv('fashion-mnist_train.csv') $ dataset = dataset.sample(frac=data_sampling_rate) #take a sample from the dataset so everyhting runs smoothly $ num_classes = 10 $ classes = {0: "T-shirt/top", 1:"Trouser", 2: "Pullover", 3:"Dress", 4:"Coat", 5:"Sandal", 6:"Shirt", 7:"Sneaker", 8:"Bag", 9:"Ankle boot"} $ display(dataset.head())
miss_vals = df.isnull().any(axis=1).sum() $ print('The number of rows with missing values is {}'.format(miss_vals))
c = pd.concat([base1, base2, base3], axis=1)
model.doesnt_match("france england germany berlin".split())
with open("sentiment_comments.json") as f: $     occurances = json.load(f)
CONSUMER_KEY    = 'YuFqL7bCSDHhLlbAzFvF02gcE' $ CONSUMER_SECRET = 'lr4MIHgghusgPSBZQ7VroKYaw3dDKfRvr4EzxJ1LS9QlrengWz' $ ACCESS_TOKEN  = '241206928-9tsIA2LyOqKOaLG5IHmswv09ghiubyBh4xfkJRXh' $ ACCESS_SECRET = 'mVXf7ds0iPSkOkB0FK5RiUBu25qZGpxXxBD7AJ7wAItdJ'
df = df.join(cluster.Cluster_Labels_GaussianMixture)
autos.price.value_counts().sort_index(ascending = False).head(10)
!wget https://data-ppf.github.io/labs/lab4/Residuals.jpeg $ !wget https://data-ppf.github.io/labs/lab4/Star.obs.jpeg
rejected.purpose = [purpose_map[tit] $                     if tit in purpose_map else tit for tit in rejected.purpose]
summary_ms = df_measures_users[df_measures_users['ms']==1][score_variable].describe() $ print('Average Score of MS Group: {} (SD {})'.format( $     round(summary_ms.loc['mean'],2), round(summary_ms.loc['std'],2)))
df['converted'].mean()*100
import test_package.print_hello_direct
train = energy.copy()[energy.index < valid_start_dt][['load']]
rent_db3 = rent_db2[rent_db2.price < 7000] $ rent_db3.boxplot(column='price')
country_sort = averages[averages.Country == 'Canada'] $ country_sort
for col in list(df.columns) : $     k = sum(pd.isnull(df[col])) $     print(col, '{} nulls'.format(k))
df2.set_index('user_id').index.get_duplicates()
grouped_dpt["City"]
with SAS7BDAT('./data/in/adtteos.sas7bdat') as file: $     df_dm = file.to_data_frame() $ df_dm.head()
b_cal = pd.read_csv('boston/calendar.csv') $ b_list = pd.read_csv('boston/listings.csv') $ b_rev = pd.read_csv('boston/reviews.csv')
df[df.handle == 'Jim Cramer'].head()
df.info()
df_western.genres.str.contains(r'Western').sum() == df_western['id'].count()
max_mean_km = price_vs_km["mean_odometer_km"].max() $ price_vs_km.loc[price_vs_km["mean_odometer_km"] == max_mean_km, :]
tweets.groupby("related_attack").sum()["frequency"]
idx = payments_all_yrs[ payments_all_yrs['Num_DRGs']>0].index.tolist() $ len(idx) $
commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq='AS')).count() $ commits_per_year.columns = ['commits'] $ commits_per_year.head()
df2['new_sales'] = df2.new_sales.apply(add_dollar) $ df2['sales'] = df2.sales.apply(add_dollar) $ df2['cust_avail_v3'] = df2.cust_avail_v3.apply(make_perc) $ df2['new_conv'] = df2.new_conv.apply(make_perc) $ df2['new_sales_perc'] = df2.new_sales_perc.apply(make_perc)
app_pivot['Total'] = [0 for i in range(len(app_pivot))] $ app_pivot['Total'] = app_pivot.Application + app_pivot['No Application'] $ app_pivot
df_list_rand = pd.read_csv(master_folder + lists +  "unsubscribed/" + files_lists[0] ,\ $                   low_memory=False)
dti1 = pd.to_datetime(['8/1/2014']) $ dti2 = pd.to_datetime(['1/8/2014'], dayfirst=True) $ dti1[0], dti2[0]
res = vectorizer.fit_transform(answer['tweet'].values)
wqYear = dfHawWQ.groupby('Year')['TotalN'].mean() $ dfAnnualN = pd.DataFrame(wqYear)
combined_df4['empty_prop']=combined_df4['empty_prop'].fillna(0) $ combined_df4['empty_prop']=combined_df4['empty_prop'].replace(['Y','V'],1) $ combined_df4.head()
archive_copy['full_text'][archive_copy['full_text'].str.contains('&amp;')]
month = pd.get_dummies(questions['month_bought'])
intervention_history['next_incident_type'] = groups.INCIDENT_TYPE_NAME.transform(lambda s: s.shift(-1))
y_range = (0, 200) $ x_range = bokeh.models.ranges.Range1d(start = datetime.datetime(2017,1,1), $                                       end = datetime.datetime(2017,1,31))
autos.drop("nr_of_pictures",axis=1, inplace=True) # this column does not contain useful info
stock_weights = np.asarray([0.5,0.5,0.5,0.5])
cc['logopen'] = np.log(cc['open']) $ plt.hist(cc['logopen']) $ plt.show()
brand_pct = dict(autos["brand"].value_counts(normalize=True))
talks.drop('name',axis = 1,inplace = True)
shots_df
df.tail(2)
np.save(LM_PATH / 'tmp' / 'train_ids.npy', trn_lm) $ np.save(LM_PATH / 'tmp' / 'val_ids.npy', val_lm) $ pickle.dump(itos, open(LM_PATH / 'tmp' / 'itos.pkl', 'wb'))
prob_control = (df2['group'] == 'control').sum()/unique_users_2 $ prob_control_and_converted = (df2[  (df2['group'] == 'control')& (df2['converted'] == 1)].shape[0])/unique_users_2 $ prob_convert_given_control = prob_control_and_converted/prob_control $ prob_convert_given_control
os_url = 'https://raw.githubusercontent.com/yinleon/fake_news/master/data/sources_clean.tsv?flush=true' $ df_os = pd.read_csv(os_url, sep='\t', index_col=0) $ len(df_os)
lda_tfidf.show_topics(formatted=False, num_words=20)[0:2]
archive_copy.pupper.unique()
sns.lmplot(x="Debt", y="Income", data=training, x_estimator=np.mean, order=1)
globalCityContent = readPDF(globalCityBytes) $ globalCitySentences = globalCityContent.replace('\n','').split('.') $ type(globalCitySentences)
sdsw = sd[(sd['JOB_TITLE'].str.contains('SOFTWARE')) | (sd['JOB_TITLE'].str.contains('PROGRAMMER')) | (sd['WAGE_UNIT_OF_PAY'] == 'Year')] $ sdsw.sample(50)
for c in ccc: $     ved[c] = ved[ved.columns[ved.columns.str.contains(c)==True]].sum(axis=1)
control_group_user_count = df2[df2['group'] == 'control']['user_id'].count() $ converted_control_user_count = df2[(df2['group'] == 'control') & (df2['converted'] == True)]['user_id'].count() $ p_control_converted = converted_control_user_count / control_group_user_count $
len(tweets)
(p_diffs > act_diff).mean()
df['token_count'] = df['body_tokens'].apply(lambda x: len(x)) $ print(df[['body_tokens','token_count']])
sc=SparkContext.getOrCreate() $ sqlContext = SQLContext(sc)
grouped_by_dow_df.to_csv('grouped_by_day_of_week_df.csv',index=False)
print("dataframe df1 row names as follows : ") $ df1.index.values
label_map
test
weather_df.head()
terrorism = text_file.filter(lambda t: is_interesting(t,bds)).take(4000)
X = pd.get_dummies(X, drop_first = True) $ X.head()
sum(image_predictions.tweet_id.duplicated())
all_data_vectorized = body_pp.transform_parallel(all_data_bodies)
dfFull['OverallCondNorm'] = dfFull.OverallCond/dfFull.OverallCond.max()
df_CLEAN1A.tail()
dataframe = pd.read_csv("stateratios.csv") $ dataframe = dataframe.sort_values('Ratio') $ dataframe.head(n=10)
from sklearn.linear_model import SGDClassifier
cutoff_times = pd.read_csv('s3://customer-churn-spark/p1/SMS-14_labels.csv') $ cutoff_times.head() $ cutoff_times.tail()
yt.get_subscriptions(channel_id, key, descriptive=True)[:2]
df_transactions.head()
pd.to_datetime(3e9)
aqmdata.to_excel('aqmdata.xlsx', index=False)
headers= ({'User-agent': 'kiros Bot 0.1'})
json_data
my_model_q6 = SuperLearnerClassifier(clfs=[clf_base_knn,clf_base_svc, clf_base_nb], stacked_clf=clf_stack_lr, training='label') $ my_model_q6.fit(X_train, y_train) $ y_pred = my_model_q6.predict(X_test) $ accuracy = metrics.accuracy_score(y_test, y_pred) $ print("Accuracy: " +  str(accuracy))
contribs.iloc[imax].committee_name
!gunzip -c GCA_002079225.1_ASM207922v1_feature_table.txt.gz | head -n 4
print("Cross-Validation Score") $ print("Decision tree: ", np.mean(cross_val_score(model_tree, x, y, cv=100)*100)) $ print("Logistic Regression: ", np.mean(cross_val_score(model_logit, x, y, cv=100)*100)) $ print("Random Forest: ", np.mean(cross_val_score(model_rf, x, y, cv=100)*100)) $ print("Neural Network: ", np.mean(cross_val_score(model_nn, x, y, cv=100)*100))
df.to_html('table.html')
dateindex = pd.date_range(taxi_hourly_df.index.min(), taxi_hourly_df.index.max(), freq="1H")
mod_model = ModifyModel(run_config='config/run_config.yml', model='MESSAGE_GHD', scen='hospitals baseline', $                         xls_dir='scen2xls', file_name='data.xlsx', verbose=False)
loans_df = loans_df.loc[:, pd.notnull(loans_df).sum() > int(len(loans_df)*(1 - frac))]
md = column_data.ColumnarModelData.from_data_frame(PATH, val_idx, df, y_log, cat_flds=cat_vars, bs=128)
conn.upload('/u/username/data/iris.csv')
df2 = df.set_index('key') $ mapping = {'A': 'vowel', 'B': 'consonant', 'C': 'consonant'} $ display('df2', 'df2.groupby(mapping).sum()')
df_uro.shape $ df_uro.head()
import yaml $ with open('../../conf/config.yaml') as f: $     dataMap = yaml.safe_load(f) $ from sqlalchemy import create_engine $ engine = create_engine('mysql+pymysql://%s:%s@localhost/securities_master' % (dataMap['securities_master']['id'], dataMap['securities_master']['pw']), echo=False)
df = pd.read_csv('https://raw.githubusercontent.com/jackiekazil/data-wrangling/master/data/chp3/data-text.csv') $ df.head(2)
print(plan["plan"]["to"])
from sklearn import linear_model $ lin_reg = linear_model.LinearRegression() $ lin_reg.fit(x_train,y_train)
predictions.info()
submissions = pd.read_csv('submissions.csv')
average_daily_sales = data[['Sales', 'Open']].resample('D').mean() $ average_daily_sales['DiffVsLastWeek'] = average_daily_sales[['Sales']].diff(periods=7) $ average_daily_sales.sort_values(by='DiffVsLastWeek').head() $ average_daily_sales[average_daily_sales.Open == 1].sort_values(by='DiffVsLastWeek').head() $
plt.scatter(master_copy.rating_numerator, master_copy.retweet_count) $ plt.xlim(0,25) $ plt.xlabel('Rating Numerator') $ plt.ylabel('Retweets') $ plt.title('Retweeting based on numerator score')
print ("Test Accuracy :: ",accuracy_score(test_dep[response], xgb_model.predict(test_ind[features]))) $ print ("Train Accuracy :: ",accuracy_score(train_dep[response], xgb_model.predict(train_ind[features]))) $ print ("Complete Accuracy  :: ",accuracy_score(kick_projects_ip[response], xgb_model.predict(kick_projects_ip_scaled_ftrs))) $ print (" Confusion matrix of complete data is", confusion_matrix(kick_projects_ip[response],kick_projects_ip["Pred_state_XGB"]))
area.index | population.index
tweets_gametitle.merge(winpct[['playId','text','Game Title Date']], $                        how='left', $                        left_on=['Game Title Date','text'], $                        right_on=['Game Title Date','text'])
data_archie.isnull().sum()
retweet_pairs[retweet_pairs["Weight"]>1].shape
np.empty((1,4))
print(new_model) $ print(model.wv.vocab)
points = pd.read_csv("points.csv") $ points.head()
data.whitelist_status.unique()
df_bkk = load_obj(path+"/translated_bkk_tweet")
newest=pd.merge(detroit_census2, portland_census2, how='inner', $                          left_on = "Fact", right_on = "Fact", indicator = True) $ population=newest.drop(newest.index[1:]) $ population.head()
DummyDataframe = pd.DataFrame(columns=['Date', 'hashtag_count', 'mention_count']) $ dummyDates = [ pd.Timestamp('20180101'),  pd.Timestamp('20180201'),  pd.Timestamp('20180301'),  pd.Timestamp('20180401'),  pd.Timestamp('20180501')] $ for i in range(25): $     DummyDataframe.loc[i] = [dummyDates[i % 5], np.random.randint(150), np.random.randint(150)] $ DummyDataframe
cityID = '7f061ded71fdc974' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Montgomery.append(tweet) 
def qnt(x): $     return (x*2)/3
my_model_q9 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='label') $ my_model_q9.fit(X_train, y_train) $ base_model_relation, base_accuracy_comparison = my_model_q9.base_model_eval()
!cat data/kaggle_data/features.txt
average_trading = statistics.mean([day[6] for day in data]) $ print ('Average daily trading volume for 2017:', round(average_trading,2))
df[['Genre','genre']]
df2 = pd.DataFrame(q2_results,columns=['station_name','observ_ct']).sort_values(by = 'observ_ct') $ df2
dfL.boxplot(figsize=(1150,1150), rot=50 , return_type='axes');
null_vals = np.random.normal(0, np.std(diffs), 10000) # Here are 10000 draws from the sampling distribution under the null
mean_newsorg_sentiment = grouped_newsorgs['Compound'].mean() $ mean_newsorg_sentiment.head()
archive_df.head(5)
columns = ['tweet_id','timestamp','tweet_text','user_id', $            'tweet_coords','tweet_coords_list','tweet_long','tweet_lat','location', $            'enc_url','tweet_lang','hashtags']
MostHourlyExits = subway3_df.nlargest(100,'Hourly_Exits')
exclude_year = [1985, 1986, 1987] $ lv_workspace.get_subset_object('A').set_data_filter(step=1, filter_type='exclude_list', filter_name='YEAR', data=exclude_year) 
min(close, key=close.get)
CHI = pd.read_excel(url_CHI, $                     skiprows = 8)
scn_genesis = pd.to_datetime(min(BID_PLANS_df['scns_created'])[0])
titanic.loc[index_strong_outliers, :].head()
combined_df[combined_df['nndr_prop_ref']=='1943742']
agg_churned_plans_counts = [(key,sum(num for _,num in value)) for key, value in groupby(uniq_sorted_churned_plans_counts,lambda x:x[0].tolist())]
df2[(df2['landing_page']=='old_page') & (df2['converted']==1)].count()[0]
len(df2[df2['landing_page'] == 'new_page']) / len(df2)
date.strftime("%A")
df_archive["text"][2]
df = df.loc[df['offerType'] == 'Angebot'] $ df = df.drop('offerType',axis=1) $ df.shape
conditions_counts.plot(kind='barh');
vocabulary_expression = pd.DataFrame(svdtrun.components_, $                                      index=components_names, $                                      columns=tfidf_vectorizer.get_feature_names()).T $
df['bound_at'].tail(10)
len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)
dfData['rqual_score10'].value_counts()
xmlData['build_year'] = pd.to_datetime(xmlData['build_year'], format = '%Y', errors = 'raise') $ xmlData['renovate_year'].replace({'0':''}, inplace = True) $ xmlData['renovate_year'] = pd.to_datetime(xmlData['renovate_year'], format = '%Y', errors = 'coerce') $ xmlData['build_year'] = [d.strftime('%Y') if not pd.isnull(d) else '' for d in xmlData['build_year']] $ xmlData['renovate_year'] = [d.strftime('%Y') if not pd.isnull(d) else '' for d in xmlData['renovate_year']]
btc_wallet = get_fiat_currencies(btc_wallet, 'BTC')
totals.sort_values('total',ascending=False)
!./flow --imgdir "../darkflow_data/MY20173c_test/" --model cfg/tiny-yolo-voc-3c.cfg --load -1 --lr 0.0001 --batch 32 --gpu 0.8
df_cont.head()
mars_weather = weather.text.strip() $ mars_weather
df = reduce(lambda left, right: pd.merge(left, right, on='tweet_id'), dfs)
notus['country'].value_counts(dropna=False)[:45]
unique_users = df['user_id'].nunique() $ unique_users
train.date.value_counts().shape
words_mention_sp = [term for term in words_sp if term.startswith('@')] $ corpus_tweets_streamed_profile.append(('mentions', len(words_mention_sp))) # update corpus comparison $ print('List and total number of mentions: ', len(set(words_mention_sp))) #, set(terms_mention_stream))
clf.fit(X_train, y_train)
print(dfd.in_pwr_47F_min.describe()) $ dfd.in_pwr_47F_min.hist()
data.info()
!ls ../data/imsa-cbf/ | grep _w.cbf | wc -l
import ta # technical analysis library: https://technical-analysis-library-in-python.readthedocs.io/en/latest/ $ features['f13'] = ta.momentum.money_flow_index(prices.high, prices.low, prices.close, prices.volume, n=14, fillna=False) $ features['f14'] = features['f13'] - features['f13'].rolling(200,min_periods=20).mean() $
df.groupby('Updated Shipped ranges').aggregate({'Updated Shipped diff':['count','mean','std','min','max']}) $ df_updated_stats = df.groupby('Updated Shipped ranges').aggregate({'Updated Shipped diff':['mean','std']})
goodTargetUserItemInt.head()
google = data.DataReader(name='GOOG', data_source='iex' $                         , start='2016-01-01', end='2018-05-01') $ google.head()
print(autos["price"].unique().shape) #Find out the number of different prices in the dataset
from pyspark.sql.functions import concat, col, lit $ file4 = file4.withColumn('date', concat(col('trans_start_year'), $                                         col('trans_start_month'), $                                        col('trans_start_day'))) $ file4.show(3)
php_xml_output = get_scielo_php("S1414-32832017000200349") $ dict(php_xml_output.xpath("//ARTICLE")[0].attrib)
lrs = [0.0001, 0.0001, 0.0001, 0.0001, .001]
import numpy as np
image_predictions_clean.p1.value_counts()
bacteria_data
word_ids = vocab.loc[tokens].values $ word_ids
move_1_union = sale_lost(breakfastlunchdinner.iloc[1, 1], 30) $ move_2_union = sale_lost(breakfastlunchdinner.iloc[5, 2], 30) $ adjustment_2 = move_1_union + move_2_union $ print('Adjusted total for route: ' + str(move_34p14u34p - adjustment_2))
sentdata_beginning = sentdata[(sentdata.day > '2017-10-25')]
df['Lunch'] # select by column 'Lunch'
if not os.path.exists('training_data'): $     os.mkdir('training_data')
merged_NNN.committee_position.value_counts()
from sklearn.model_selection import KFold $ cv = KFold(n_splits=250, random_state=None, shuffle=True) $ estimator = Ridge(alpha=32000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
draw_tree(m.estimators_[0], raw_train, precision=3)
df2 = df.query("(group == 'treatment' and landing_page == 'new_page') or (group == 'control' and landing_page == 'old_page')")
df8 = df[df['hired'] ==0] $ df8.groupby('category')['position','hourly_rate','num_completed_tasks'].agg({'median','mean','min','max'}).reset_index() \ $ .rename(columns={'category': 'category', 'position': 'position_stats_not_hired', \ $                     'hourly_rate':'hourly_rate_stats_not_hired',  'num_completed_tasks':'num_completed_tasks_stats_not_hired'}) $
et_helper.plot_around_event(etsamples_grid,etmsgs_grid,etevents_grid,raw_large_grid_df.query("eyetracker=='el'&subject=='VP4'&block==1").iloc[9],plusminus=(-2,5))
df['org_twitter'].nunique()
model.summary()
machin.shape
data_df[['clean_desc', 'tone', 'topic']][20:40]
df.head
writers.groupby("Country").groups
df = pd.read_csv('trump_state_of_union_2018.csv.gz') $ df['parse'] = df['text'].apply(unescape).apply(nlp)
df_questions.to_csv("BaseQuestionsPSE")
if not os.path.isdir('output/pv_production'): $     os.makedirs('output/pv_production')
dividends = yahoo_finance.download_dividends("GILD") $ print dividends[0]
logins = pd.DataFrame(json.load((open('logins.json')))) $ logins.head()
print len(df) $ df
%%read_sql df_test3_promotions -c engine $ SELECT * from PromotionRuleInstance $ where promotionID in ({promotionID_test3}) $ and activityTimestamp < '{endDate}' $
titanic.pivot_table('survived', index=['sex', age], columns='class')
rf_v1
sym.dsolve(eqn, C(t)) # Resolver
url = 'https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2015-01-01&end_date=2015-01-01&api_key=' $
plt.figure(figsize=(8, 5)) $ plt.scatter(prepared_train.favs_lognorm, prepared_train.comments); $ plt.title('The distribution of the favs_lognorm and number of the views without outliers');
API_KEY = 'AnxQsp4CdfgzKqwfNbg8'
c['code'] = c['code'].fillna('0000').astype(str) $ c['code'] = c['code'].apply(lambda x: x[:4]).astype(int) $ c['code'].unique()
autos["brand"].value_counts(normalize=True)
import re $ letters_only = re.sub("[^a-zA-Z]",           # The pattern to search for $                       " ",                   # The pattern to replace it with $                       example1.get_text() )  # The text to search $ print (letters_only)
from sqlalchemy import func
crimes['year'] = crimes.DATE__OF_OCCURRENCE.map(lambda x: x.year)
automl = pickle.load(open(filename, 'rb'))
df1.join(df2)
twosample_sub = scipy.stats.ttest_ind(locationing.subjectivity, tweetering.subjectivity) $ twosample_sub
df.info()
pb = progress_bar(len(accounts_df)) $ print('Collecting Todays Tweets') $ todaysTweets[date] = todaysTweets['Account'].apply(get_TodaysTweets)
np.unique(noaa_data[noaa_data['AIR_TEMPERATURE'] < -273.15]['AIR_TEMPERATURE'].index.date)
df6=base[(base['panstar']==True) & (base['pisco']==False) & (base['dec']>-27) & (base['dec']<30)] $ test2=df6
events.schema
f, ax = plt.subplots() $ ax.set_ylim(ymax=400); $ ax.set_xlabel('Shift duration [h]'); $ ax.set_ylabel('Count'); $ df['duration'].astype('timedelta64[h]').hist(bins=75, ax=ax);
p_new_abtest=df2.query('group == "treatment"')['converted'].mean() $ p_new_abtest
!pip install getorg --upgrade $ import pandas as pd $ import os
grouped_dpt.describe() # summary statistics 
errors.mae_vals.idxmin(errors.mae_vals.min())
print("Number of Mitigations in Enterprise ATT&CK") $ print(len(all_enterprise['mitigations'])) $ df = all_enterprise['mitigations'] $ df = json_normalize(df) $ df.reindex(['matrix','mitigation', 'mitigation_description', 'url'], axis=1)[0:5]
df_treatment = df.query("group == 'treatment'") $ df_treatment.converted.mean()
import pandas as pd
import datetime $ import pandas as pd $ from zoomtools import rdb
m2 = m[:,1:3].reshape(5,2) $ print("m2: ", m2)
output.count() $ output.printSchema() $ output.show(2)
from IPython.display import HTML $
X = np.arange(8) $ Y = X + 0.5 $ C = 1.0 / np.subtract.outer(X, Y) $ print(np.linalg.det(C))
twitter_count = pd.concat([twtter_count1, twtter_count2, twtter_count3], axis=1).fillna(0).apply(lambda x: np.sum(x), axis=1)
dftops.plot(kind='bar')
affair=data.pop('affair')
data.head()
patient_times = pd.DataFrame(patient_dict).T.rename_axis('patient').add_prefix('time').reset_index()
df_inds.columns.value_counts()[:5]
sns.countplot(x="y",data=X,palette='hls') $ plt.show()
names = top_allocs[:10].index.tolist()
nfl.build_payload(kw_nfl, geo="US", timeframe="2016-01-01 2016-09-01") #2016-09-01 2016-12-31 $
columnsToClean = ['body', 'title'] $ cleanDataset(dfTickets, columnsToClean, getRegexList()) $
np.exp(result4.params)
ab_data.landing_page.value_counts()
df2 = df2.join(countries.set_index('user_id'), on='user_id', how='left') $ df2.head()
result = Valid_events.copy() $ result.insert(loc=1, column='valid_result', value=Vy_pred) $ result
site
potential_accounts_buildings_info_tbrr = pd.DataFrame(potential_accounts_buildings_info_tbrr\ $                                                       [potential_accounts_buildings_info_tbrr[' Total BRR '] > 0]) $ potential_accounts_buildings_info_tbrr.drop([' AnnualRevenue ','NumberOfEmployees',\ $                                              ' DandB Revenue ', 'DandB Total Employees'], axis=1, inplace=True)
f_counts_week_channel.show(1)
tweet1.user.screen_name
fairshare['Is_Account'] = fairshare.ACCOUNT.str.contains('account') $ for i in range(4): $     fairshare.iloc[:,5-i] = fairshare.iloc[:,4-i] $ fairshare.loc[fairshare.Is_Account == False, 'MAXCPUS'] = np.NaN $ fairshare.MAXCPUS.fillna(method='ffill', inplace=True)
headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'} $ r = requests.get(url, headers=headers) $
def tokenize(text): $     tokens = re.split('\W+', text) $     return tokens $ infinity['text_tokenized'] = infinity['text_clean'].apply(lambda x: tokenize(x.lower()))
user_actions_with_profile = user_actions.join(user_profile)
autos['ad_created'].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True).hist()
close_day1 = [] $ for day in data: $     close_day1.append(day[4]) $ close_day1.pop(-1) $
youthUser2["cityName"] = youthUser2['name'] $ youthUser3 = youthUser2[['_id_x','creationDate','namefirst','namelast','contactInfoemail','contactInfophone', $                          'demographicInfodob','demographicInfoethnicity','demographicInfogender', $                         'demographicInfozipcode','loginDetailslastLoginTimestamp1','cityName']].copy() $ youthUser3.head()
data.iloc[[1, 2], [3, 0, 1]]
log_reg = sm.Logit(df2['converted'],df2[['intercept','ab_page']])
files8.head()
orgs.shape
df_pivot.head(5)
import pandas as pd $ df = pd.read_csv('census.csv') $ df
volume_weekly = vol.groupby([vol.index.year, vol.index.week]).sum() $ volume_weekly
chefdf = pd.merge(chefdf, chef02df,  how='left', left_on=['name','user'], $                   right_on = ['name','user'], suffixes=('','_02'))
joblib.dump(rf_gridcv, '../../data/shelter cats/models/random_forests_grid_model2.pkl') 
%timeit MyList4 = MyList.copy(); MyList4.sort() $ MyList4 = MyList.copy(); MyList4.sort() $ MyList4[:5]
print('There are',len(idx_places_less_million),'sites with <$1,000,000 for the time frame and they will be discarded') $ idx_places_less_million[:10]
plot_contingency(train_users, "age_bucket", "country_destination")
m['predicted_purchases'].sum()
control_converted = df2[(df2.group == "control") & (df2.converted == 1)].shape[0] $ total_control = df2[(df2.group == "control")].shape[0] $ p_control_convert = control_converted/total_control $ p_control_convert
users_conditions.drop_duplicates(subset = ['user_id'], keep = 'first', inplace = True) $ users_conditions.reset_index(drop = True, inplace = True)
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page' and converted == 1").shape[0] $ convert_new = df2.query("landing_page == 'new_page' and converted == 1").shape[0] $ n_old = df2.query('group == "control"').shape[0] $ n_new = df2.query('group == "treatment"').shape[0]
df_merged.describe()
print(np.min(ndvi), np.mean(ndvi), np.max(ndvi))
hdf.head()
pmol.df['element_type'] = pmol.df['atom_type'].apply(lambda x: x.split('.')[0]) $ pmol.df['element_type'].value_counts().plot(kind='bar') $ plt.xlabel('element type') $ plt.ylabel('count') $ plt.show()
test_scores=test_join.filter('p_repo_id is not null')
type(tips)
STD_customer_order_intervals
wrd_clean['doggo'].value_counts()[:10]
pd.DataFrame(lostintranslation.credits()['cast'])[["character","name", "id"]].head()
morning_rush.iloc[:5,6:8]
cr_new_data = len(df2.query('landing_page == "new_page" & converted == 1')) / n_new $ cr_old_data = len(df2.query('landing_page == "old_page" & converted == 1')) / n_old $ actual_diff = cr_new_data - cr_old_data $ print('The actual difference observed in ab_data.csv was: {}'.format(actual_diff))
com_grp.count()  # return panda DataFrame object
result = clf_LR_tfidf.predict(test_data_features_tfidf) $ result_prob = clf_LR_tfidf.predict_proba(test_data_features_tfidf) $ output = pd.DataFrame(data={"id":test["id"], "sentiment":result,})# "probs":result_prob[:,1]}) $ output.to_csv(os.path.join(outputs,'LR_tfidf_model.csv'), index=False, quoting=3)
calls_nocontact.council_district.value_counts()
dealer_avg_order_intervals = copy.deepcopy(result) $ %store dealer_avg_order_intervals
s2[1]
obs_diff = treatment['converted'].sum() / treatment.shape[0] - control['converted'].sum() / control.shape[0] $ obs_diff
data.loc[data.surface_total_in_m2 < 1, 'surface_total_in_m2'] = np.NaN
co.steady_states.head() $
np.exp(-0.0150)
df['Forecast'] = np.nan
testPredict = scaler.inverse_transform(testPredict) $ testY = scaler.inverse_transform([testY]) $ print (testPredict) $
control_mean = df2.query('group == "control"')['converted'].mean() $ print(control_mean)
to_be_predicted_Day2 = 52.31714025 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
df_from_csv.equals(df)
sorted(data['data'].keys())     # sorting  the data dictionary
intersections_final_for_update = intersections_final[['Segment ID','speed','level','avg_traffic_flow','updateTimeStamp','isWeekend','date_time_hour','estimated_number_vehicles']]
twitter_dataset['name'].sample(50)
user_extract.info()
tweets = pd.read_csv("tweets.csv")
rfc = RandomForestClassifier(random_state = 42) $ param_grid = {'n_estimators' : [100, 200, 300], $               'max_features' : ['log2', 'sqrt']} $ rf_gd = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5, scoring='f1', n_jobs = -1) $ rf_gd.fit(X_train, y_train)
m = RandomForestRegressor(n_estimators=40, max_features=0.99, min_samples_leaf=2, $                           n_jobs=-1, oob_score=True) $ m.fit(trn, y_trn);
plate_appearances = df.sort_values(['game_date', 'at_bat_number'], ascending=True).groupby(['atbat_pk']).first().reset_index()
pk_planes = hc.table('asm_wspace.parking_planes') \ $ .where("pk_start between '{0}' and '{1}'".format(start_date, end_date))
logit_mod = sm.Logit(df_new2['converted'], df_new2[['intercept', 'ab_page', 'CA_pages']]) $ results = logit_mod.fit() $ results.summary()
subwaydf['4HR_Entries'].idxmax()
guineaCases = guineaCases.rename(columns = {'Totals':'New cases'}) $ guineaDeaths = guineaDeaths.rename(columns = {'Totals':'New deaths'})
day_of_week = pd.DatetimeIndex(pivoted.columns).dayofweek
len(df2['user_id']) - len(df2['user_id'].unique())
shmuel = relevant_data[relevant_data['User Name'] == 'Shmuel Naaman'] $ df = shmuel['Event Type Name'].value_counts() $ df_shmuel = pd.Series.to_frame(df) $ df_shmuel.columns = ['Shmuel N'] $ df_shmuel
df.query('converted==1')['user_id'].nunique()/df['user_id'].nunique()
y = df['target'] $ X = df['subreddit'] $ cvec = CountVectorizer(stop_words = 'english') $ X  = pd.DataFrame(cvec.fit_transform(X).todense(), $              columns=cvec.get_feature_names())
jan_2015_groupby.head()
df.tail(5)
suburban_ride_total = suburban_type_df.groupby(["city"]).count()["ride_id"] $ suburban_ride_total.head() $
merge[merge.columns[37:40]].head(3)
from scipy.stats import norm $ print(norm.cdf(z_score)) #Tells us how significant our z-score is
sqlContext.sql("select sum(count) from pcs").show()
df.info()
liberia_data3 = liberia_data1[['Description', 'Date', 'Totals']] $ liberia_data3
type_driver=pd.Series(driver_count["type"]) $ count_driver=pd.Series(driver_count["driver_count"]) $ colors = ["gold","lightskyblue","lightcoral"] $ explode = (0, 0, 0.1)
df.head()
number_of_commits = len(git_log) $ number_of_authors = git_log['author'].nunique() $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
txt.reset_index() $ txt['Date'] = pd.to_datetime(txt['Time'] , unit = 's').dt.date $
classify_df.columns
import google.datalab.ml as ml $ import tensorflow as tf $ from tensorflow.contrib import layers $ print tf.__version__ $
df = df[df.sentiment != 'Neutral'] $ df['text'] = df['text'].apply(lambda x: x.lower()) $ df['text'] = df['text'].apply(lambda x: x.replace('rt', ' ')) $ df['text'] = df['text'].apply(lambda x: re.sub('[^a-zA-z0-9\s]', '', x))
pd.read_sql_query("SELECT * from person;", conn, index_col="id")
df_h1b_mv.pw_1.describe()
cur_b, conn_b = connect('FIT5148B', my_id, my_pass)
remove_index = treat_oldp.append(ctrl_newp).index $ remove_index.shape
subwaydf.iloc[114433:114439] #this low number seems to be because entries and exits resets
twitter_archive_master.stage.value_counts().plot(kind='bar');
df2.shape
poldnull = df2.query('converted == 1').shape[0] / df2.shape[0] $ poldnull
linearReg = LinearRegression() $ linearReg.fit(X_train, y_train) $ predicted_linear_y = linearReg.predict(X_test) $ print("OLS Mean Squared error:",mean_squared_error(y_test, predicted_linear_y )) $ print("OLS R2 Score:",linearReg.score(X_test, y_test))
X_s=X_d.drop(['title','text'],axis=1)
austin= pd.merge(left= df, right= df2, on='RIDE_ID')
ekos.workspaces
sl[(sl.age_well_years!= 274) & (sl.second_measurement==0)].age_well_years.describe()
df.sort_values('dealid', ascending = True, inplace = True)
df_clean = df.copy() $ preds_clean = pred.copy() $ api_clean = api_df.copy()
uniqueUsers = userArtistDF.select("userID").distinct().count() $ print("Total n. of users: ", uniqueUsers) $
xml_in_sample = xml_in_merged[xml_in_merged['countPublications'] > 3]
df_TempJams.info()
n_old = len(df2[df2['group']=='treatment']) $ print("Total number of n(old) = ", n_old)
df2 = df.drop(df[(df.group == 'control') & (df.landing_page == 'new_page')].index)
obj
titanic_class_sex_age = df_titanic_temp.groupby(['age', 'sex','pclass']) $ print(titanic_class_sex_age.describe())
df['age'] = df['fetched time'] - df['created_utc'] $ df['age'] = df['age'].astype('timedelta64[m]') $ df.head()
grid_clf.best_estimator_
plt.savefig('Sentiment_Analysis_News_Organizations_Tweets.png')
adaboost.fit(trainx,trainy)
convert_old,convert_new,n_old,n_new
ts_df = site_series_values_to_df(site_values, variable_name) $ ts_df.tail()
expiry = datetime.date(2015, 1, 5) $ msft_calls = pd.io.data.Options('MSFT', 'yahoo').get_call_data(expiry=expiry) $ msft_calls.iloc[0:5, 0:5]
my_gempro.uniprot_mapping_and_metadata(model_gene_source='TUBERCULIST_ID') $ print('Missing UniProt mapping: ', my_gempro.missing_uniprot_mapping) $ my_gempro.df_uniprot_metadata.head()
train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])   
out = conn.addtable(table='banklist', caslib='casuser', $                     **htmldmh.args.addtable) $ out
import statsmodels.api as sm $ df_new['intercept'] = 1 $ logit3 = sm.Logit(df_new['converted'], df_new[['intercept','new_page','US','CA']]) $ results3= logit3.fit() $ results3.summary() $
lm.summary()
deaths = pd.read_csv('data/uk_driver_deaths.csv') $ deaths.head()
subreddit = [x.text for x in soup.find_all('a', {'class':'subreddit hover may-blank'})]
df1 = pd.DataFrame(np.arange(12.).reshape((3, 4)), columns=list('abcd'))
g8_groups.mean()
classification_ex = (train['Percent Change'] > 0).astype(int)
df_clean.drop(['retweeted_status_id', 'retweeted_status_user_id','retweeted_status_timestamp'], axis= 1 , inplace= True)
cats_df[cats_df.isnull().any(axis=1)]
df.isnull().sum()[df.isnull().sum()>0]
from lifetimes.plotting import plot_frequency_recency_matrix $ plot_frequency_recency_matrix(bgf)
historicalPriceEFX.to_csv('EFX_stock.csv')
df1.fillna(value=5.)
df = pd.DataFrame({"counts1" : counts1, "counts2" : counts2}, index = ind) $ df
dem.describe()
df = sqlContext.inferSchema(my_data)
crsr.execute("describe targets")
health_data_row.xs(key=(2013 , 1), level=('year', 'visit'))
df['DATE'] = pd.to_datetime({'year':df['YEAR'], 'month':df['MONTH'], 'day':df['DAY']})
cv_folds = 5
sanfran.layers
del df_index_demo
open_users_test = open_users[['USERNAME', 'TEST']] $ open_users_test
revs.head()
df_goog.index + timedelta(days=1)
scr_activated_df.head()
s.__class__
dci['weekFriendliness'].max()
churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']] $ churn_df['churn'] = churn_df['churn'].astype('int') $ churn_df.head()
df.query('converted == "1"').user_id.nunique() / df.user_id.nunique()
speakers.head()
df2 = df.query("(group == 'control' and landing_page == 'old_page') or (group == 'treatment' and landing_page == 'new_page')")
ruid = df2['user_id'].mode()
ax=dat[['orig_T1']+temp_columns].plot() $ ax.vlines(dat.index[dat.TAspirated1-dat.Tpassive1>2], ymin=-30, ymax=15)
pd.Timestamp('2018-01-01') + timedelta(hours=3)
all_cards.describe()
df_train = pd.read_csv('C:/Users/ajayc/Desktop/ACN/2_Spring2018/ML/Project/WSDM/DATA/train.csv', dtype={'is_churn' : np.int8})
writers.groupby('Country').first()
serc_ext = (xMin, xMax, yMin, yMax) $ print('serc_ext:',serc_ext) $ print('serc_ext type:',type(serc_ext))
df2 = df $ mismatch_index = mismatch_df.index $ df2 = df2.drop(mismatch_index)
fig = goes_lightcurve.peek() $ fig = lyra_lightcurve.peek()
sb.heatmap(components3)
discGrouped.head(10)
os.listdir('..')
(null_vals > obs_diff).mean()
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt
pd.crosstab(calls_df['call_day'], calls_df['call_type'], margins=True)
import pandas as pd $ import glob $ import matplotlib.pyplot as plt $ %matplotlib inline
y_test_under[k150_bets_under].sum()
af.index.names = ['id']
if not os.path.exists('new_data_files'): $     os.mkdir('new_data_files') $ records2.to_csv('new_data_files/Q3A.csv')
cvec_df.head(25)
z,p = sm.stats.proportions_ztest([convert_old,convert_new], [n_old,n_new]) $ z,p
import pprint $ pprint.pprint(treaties.find_one())
df.to_json(data_file_path+'convo_df.json') $ print("... saved as json")
game_info = create_game_info_df("data/weekly_game_info.csv")
avg_per_seat_price_inoroffseason_teams = inoroffseason_teams["Per Seat Price"].mean() $ avg_per_seat_price_inoroffseason_teams # Acts as a benchmark for how expensive a team's PSLs are during in or off-season.
from keras.models import Sequential, Model $ from keras.layers import LSTM, Dense, Conv1D, Input, Dropout, AvgPool1D, Reshape, Concatenate
df['response'] = df.len_convo.apply(lambda x: 1 if x > 1 else 0)
data_df.head()
tweets.tail()
details.dropna(subset=['Runtime'], inplace = True)
df2[df2['landing_page']=='new_page'].shape[0]/df.shape[0]
lr=3e-3 $ lrm = 2.6 $ lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])
pothole = df[df['Descriptor'] == 'Loud Music/Party'] $ pothole.groupby(pothole.index.weekday)['Created Date'].count().plot()
print(dftemp['Year'][(dftemp['Year']>950) | (dftemp['Year']<900)])
1/np.exp(result_c.params)
twitter_archive.tail()
autos['num_photos'].value_counts() $ autos = autos.drop(['num_photos', 'seller', 'offer_type'], axis=1)
%sql SELECT * FROM temps3;
ja_en= pd.read_csv(BASE + "/ja-en-clean.csv", sep=" ") $ ja_en.head()
user_tweets= [] $ for tweet in tweepy.Cursor(api.user_timeline, screen_name=screen_name).items(): $     user_tweets.append(tweet._json) $ len(user_tweets)
import re $ a= '2010-06-28' $ m=re.search('\d+',a) $ m.group() $
print("The number of rows in the dataset is:  " + str(df.shape[0])) $
logistic_mod_country = sm.Logit(df3['converted'], df3[['intercept', 'UK','US']]) $ results_country = logistic_mod_country.fit() $ results_country.summary()
pos_tweets.shape
df_others = pd.DataFrame(list_others) $ df_others.head()
name =contractor.groupby('contractor_bus_name')['contractor_number'].nunique() $ print(name[name>1])
from dateutil import parser $ date = parser.parse("4th of July, 2015") $ date
max_day = df.created_at.dt.date.value_counts().sort_values(ascending=False).index[0] $ max_day_beers = df.created_at.dt.date.value_counts().sort_values(ascending=False).values[0] $ max_day_string = max_day.strftime('%B %d, %Y') $ print('On {} I drank a whopping {} beers! Let\'s hope they were just samplers!'.format(max_day_string, max_day_beers))
pd.Timestamp('1/2018')
' '.join(tok_trn[0])
stocks_pca_m
m.plot(forecast) $ plt.title('Kotak Bank', fontsize=20)
(df[df.converted==1].count())/df.count()
if os.path.isfile(pickle_full): $     print("loading pickle") $     df = pd.read_pickle(pickle_full) $ else: $     print("Did you run 1- Raw Data Visualisation?")
temp["time"]=temp.year.apply(lambda x: str(x)+ "-") +temp.month.apply(lambda x: str(x))
events_df['utc_offset'].head(5)
df_new['intercept'] = 1 $ logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'UK']]) $ results = logit_mod.fit() $ results.summary()
new_time = [] $ for index, row in tweets_df.iterrows(): $     new_time.append(ceil_dt(parser.parse(str(row[4])))) $ se = pd.Series(new_time) $ tweets_df['Time'] = se.values
rhum_us = rhum_nc.variables['rhum'][1, lat_li:lat_ui, lon_li:lon_ui] $ np.shape(rhum_us)
df.loc[[11,24,37]]
n_new = df2.query('landing_page == "new_page"') $ len(n_new)
print("example of a shift: NEM out, IOTA in") $ print(b[b>0].iloc[-3,].sort_values()[0:11]) $ print(b[b>0].iloc[-4,].sort_values()[0:11])
from sklearn.ensemble import RandomForestClassifier $ forest = RandomForestClassifier(n_estimators = 110) $ forest = forest.fit( X, y )
df.groupby("one_day_reminder_sent")["cancelled"].mean()
import feedparser
ethPrice = ethPrice.sort_values(by='date') $ ethPrice.set_index('date',inplace=True)
df=pd.read_csv('twitter-archive-enhanced.csv')
df_country = pd.read_csv('countries.csv')
drop_columns = ['CRE_DATE_date', 'UPD_DATE_date']
df2.head()
countries_df = pd.read_csv('countries.csv') $ df_joined = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_joined.head()
logit = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']]) $ result=logit.fit() $
Sort1 = stores.sort_values(by = "TotalSales")
mars_facts_df.head() $ mars_facts_html = mars_facts_df.to_html() $ Html_file= open("mars_facts.html","w") $ Html_file.write(mars_facts_html) $ Html_file.close()
xgb_learner.fit_best_model(dtrain)
total_ttop = len(mismatch_treatment_to_old_pg) $ total_ctnp = len(mismatch_control_to_new_page) $ print(total_ttop+total_ctnp)
import numpy as np $ import pandas as pd $ !wget -nv -O china_gdp.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/china_gdp.csv $ df = pd.read_csv("china_gdp.csv") $ df.head(10)
retweets_100 = api.retweets(deep_learning_tweet_id, count=100) $ len(retweets_100)
np.shape(ndvi_coarse)
df_G = pd.DataFrame({'key': list('bbacccb'), $                  'val': np.random.randn(7) }).round(2) $ df_G $ print df_G,'\n' $ print df_G.join(pd.get_dummies(df_G['key'], prefix='dummy')).drop('key', axis=1).drop('dummy_c', axis=1)
giss_temp.index.dtype
bag_of_words_vectorizer = CountVectorizer(min_df=2) $ bow_feature_vector = bag_of_words_vectorizer.fit_transform(corpus)
twitter_archive_clean = twitter_archive_clean.dropna(subset=['expanded_urls'])
total = mgxs.TotalXS(domain=cell, groups=groups) $ absorption = mgxs.AbsorptionXS(domain=cell, groups=groups) $ scattering = mgxs.ScatterXS(domain=cell, groups=groups) $
spark = SparkSession.builder.appName('nyc311').getOrCreate()
plt.figure(figsize=(12,6)) $ sns.barplot(x='lead_source', y= 'discConversionPercent', data=discConvpct) $ plt.xticks(rotation=90) $ plt.title('Discovery to Closed Won Percentage');
price_data.iloc[0:5]
df=df.rename(columns={"product": "item"}) $ df
%matplotlib inline $ word_freqs.plot(30)
X_svd = np.hstack((id_dense,blurb_SVD2, goal_dense, duration_dense))
print(autos['price_$'].unique().shape) $ print(autos['price_$'].describe()) $ print(autos['price_$'].value_counts().sort_index(ascending=True)) 
n_new = df2.query('landing_page == "new_page"').landing_page.count()
df2.rename( columns= { "comments.summary.total_count" : "total_comments"} , inplace=True) $ df2.rename( columns= { "likes.summary.total_count" : "total_likes"} , inplace=True)
import requests, collections, quandl
prob_treatment = (df2['group'] == 'treatment').sum()/unique_users_2 $ prob_treatment_and_converted = df2[  (df2['group'] == 'treatment')& (df2['converted'] == 1)].shape[0]/unique_users_2 $ prob_convert_given_treatment = prob_treatment_and_converted/prob_treatment $ prob_convert_given_treatment
svc_grid.fit(X_train, y_train)
taxi_hourly_df.head()
dt = DecisionTreeClassifier(labelCol = "Attrition_numerical", featuresCol = "features", \ $                            maxDepth = 7, minInstancesPerNode = 20, impurity = "gini")
graffiti['created_year'] = graffiti['created_year'].astype(int)
room_temp = Celsius()
profit_calculator(stock.iloc[1640:], 'model_predict',-1)
convert_old =  sum(df2.query("group == 'control'")['converted']) $ convert_new =  sum(df2.query("group == 'treatment'")['converted']) $ n_old = len(df2.query("group == 'control'")) $ n_new = len(df2.query("group == 'treatment'"))
resp = requests.get('https://lobste.rs/hottest.json') $ stories = pd.read_json(resp.content) $ stories = stories.set_index('short_id') $ stories.to_json('hottest.json') $
knn.fit(data[['expenses', 'floor', 'lat', 'lon', 'property_type', \ $               'rooms', 'surface_covered_in_m2', 'surface_total_in_m2']], \ $         data[['price_aprox_usd']])
df1 = pd.DataFrame(data01.load_data()) #load data into df $ df2 = pd.DataFrame(data02.load_data()) #load data into df
df_schools.shape
control = df2[df2["group"] == 'control'] $ control_conv = control[control["converted"] == 1] $ control_conv_prob = control_conv.shape[0]/control.shape[0] $ control_conv_prob
df_concat_2 = pd.concat([df_bild, df_spon]) #concats a list of dfs to one df. $
payments_total_yrs = (df_providers.groupby(['id_num'])[['disc_times_pay']].sum()) $ payments_total_yrs = payments_total_yrs.sort_values(['disc_times_pay'], ascending=[False]) $ payments_total_yrs = payments_total_yrs.reset_index() $ print('payments_total_yrs.shape',payments_total_yrs.shape) $ payments_total_yrs.head()
pd.Timestamp('now', tz='US/Eastern')
train['NTACode'] = train.NTACode.fillna('Non-NYC')
pd.options.display.max_columns = 60 $ df_protest.head()
learner.save_encoder('adam3_20_enc')
df_list_rand.columns
print('There are {} unique user ID\'s.'.format(df2.user_id.nunique()))
cust_new.head(3)
bgf_test = BetaGeoFitter(penalizer_coef=0.0) $ bgf_test.fit(m_test['frequency'], m_test['recency'], m_test['T']) $ print(bgf_test)
pos_tweets = [tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] > 0] $ neu_tweets = [tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] == 0] $ neg_tweets = [tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] < 0]
df['closed_at'] = pd.to_datetime(df['Closed Date'], format="%m/%d/%Y %I:%M:%S %p")
users['verified'].value_counts(dropna=False)
train_holiday_oil_store_transaction = train_holiday_oil_store.join(transactions, ['date', 'store_nbr'], 'left_outer') $ train_holiday_oil_store_transaction.show()
from sklearn.model_selection import KFold $ cv = KFold(n_splits=20, random_state=None, shuffle=True) $ estimator = Ridge(alpha=3500) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
df3.groupby(['TUPLEKEY2', 'DATE']).sum()
print("Print content for df_CLEAN1A:",'\n', df_CLEAN1A.isnull().sum()) $ print('\n') $ print("Print content for df_CLEAN1B:",'\n', df_CLEAN1B.isnull().sum()) $ print('\n') $ print("Print content for df_CLEAN1C:",'\n', df_CLEAN1C.isnull().sum())
data.describe().transpose()
from pandas_datareader.famafrench import get_available_datasets $ import pandas_datareader.data as web $ len(get_available_datasets()) $
session.query(Measurement.station).group_by(Measurement.station).count() $
run txt2pdf.py -o "CENTRA  HEALTH, INC  Sepsis.pdf"   "CENTRA  HEALTH, INC  Sepsis.txt"
pickle_in = open("neuralNet.pickle","rb") $ ANNModel = pickle.load(pickle_in)
es_url = 'twiceSpark1:9200'
df.sort_values(by=['title','num_comments'],inplace =True) $ df.head()
to_be_predicted_Day4 = 38.291767 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
X_d.shape
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_axial_chord_crown[0][0])) $ perr = np.sqrt(np.diag(pcov_axial_chord_crown[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
bp.head()
msft.loc['2012-02':'2012-02-09'][:5]
def get_integer5(s): $     return brand_list.index(s)
Test.dtypes
fullProduct.shape
y_preds = random_forests_grid.best_estimator_.predict(X_test) $ random_forests_scores = show_model_metrics('Random Forests', random_forests_grid, y_test, y_preds)
not_lined_up_df = df.query("(group == 'treatment' and landing_page != 'new_page') or (landing_page == 'new_page' and group != 'treatment')") $ print("The number of times the new_page and treatment don't line up: {}".format(len(not_lined_up_df)))
train_df8 = train_df7.copy() $ train_df8['fn_categorical'] = ['fna' if x<=2 else( 'fnb' if x<6 else 'fnc') for x in train_df7['features_number']] $ del train_df8['features_number'] $ train_df8.head()
recortar_tweet(tweets_data_all[0])
df2['Title'] = df2.Title.fillna(' N.A.') $ df2['Title'] = [x[1:] for x in df2.Title.values] $ df = pd.merge(df2, df1, on=['Cinema'], how='left') $ df = pd.merge(df, df3, on=['Title'], how='left') $ df.head()
import gym $ from gym import error, spaces, utils $ from gym.utils import seeding
model = Sequential() $ model.add(Dense(LATENT_DIM, activation="relu", input_shape=(T,))) $ model.add(Dense(HORIZON))
def str_to_utc(string): $
print(preprocessor(df.loc[0, 'review'][:-500]), '\n') $ print(preprocessor("</a>This :) is :( a test :-)!"))
postsDF
stocks['Date'] = stocks['Date'].dt.week
c = count_noun_tags(TextBlob(regular_season_corpus))
BASEBALL_SWING_ACTION_TYPE_ID = 1 $ url = form_url(f'actionTypes/{BASEBALL_SWING_ACTION_TYPE_ID}/metricTypes', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_body(response, max_array_components=3)
df_new2 = df_new $ df_new2['UK_pages'] = df_new2['ab_page']*df_new2['UK'] $ df_new2['US_pages'] = df_new2['ab_page']*df_new2['US'] $ df_new2['CA_pages'] = df_new2['ab_page']*df_new2['CA']
p_diff = p_new - p_old
(details.Runtime == 0).value_counts()
age.sort_values()
training_active_listing.transform(X_test)
print(voters.PermCategory.unique()) $ voters.PermCategory.value_counts(dropna=False)
df_d=pd.DataFrame(day_prcp,columns=['date','Prcp']) $ df_d['date'].head()
substr='BRANCH' $ for each_field in extract_all.columns.values: $     if str.upper(each_field).find(substr)>-1: $         print(each_field)
df.loc[:, ['B', 'D']]
url_cpi_all = 'http://stat.data.abs.gov.au/sdmx-json/data/CPI//all?detail=DataOnly&dimensionAtObservation=AllDimensions' $ print url_cpi_all
df2.head(1) $ df2.converted.mean()
usage_400hz_filter.registerTempTable('tmp_400hz_usage_final')
df_daily = df_daily.groupby(['C/A', 'UNIT','STATION','DATE']).sum() $ df_daily.head()
df_predictions_clean.p1_dog.value_counts()
train[~train.date_first_booking.isnull()]['country_destination'].value_counts()
sub_dataset['NewsDesk'].value_counts(sort=True, normalize=True)
t0 = time() $ model = MatrixFactorizationModel.load(sc,"lastfm_model.spark") $ t1 = time() $ print("finish loading model in %f secs" % (t1 - t0))
tweet_archive_clean.dog_stage = tweet_archive_clean.dog_stage.where(tweet_archive_clean.dog_stage != 'None')
pax_raw.to_hdf(os.path.join(data_dir, hdf_path), 'pax_raw')
delineate_in_weekdays = temp.groupby(['weekday'])
tweets_df['Sentiment'] = tweets_df['Text'].apply(lambda x: sa.get_sentiment_intensity(x)) $ tweets_df
yc_merged_drop.shape
subway2_df.columns = subway2_df.columns.str.strip()
station_availability_df['tot_docks'].plot(kind='hist', rot=70, logy=True) $ plt.show() $ station_availability_df = station_availability_df[station_availability_df.tot_docks != 2727] $ station_availability_df['tot_docks'].plot(kind='hist', rot=70, logy=True) $ plt.show()
for table in cur.fetchall(): $     print(table)
df.drop_duplicates(subset=["C/A", "UNIT", "SCP", "STATION", "DATETIME"], inplace=True)
sns.set_style('whitegrid') $ sns.distplot(data_final['countPublications'], kde=False,color="red") # bins=30, $
test_sentence = 'To all the little girls watching...never doubt that you are valuable and powerful & deserving of every chance & opportunity in the world.' $ test = ['Come on and kill Kenny!', $         'Make America great again!', $         'Beer... Beeeeer... Beeeeeeeeer... WOO-HOO!']
len(df_proj.ProjectId.unique())
from sklearn.datasets import fetch_20newsgroups $ dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes')) $ documents = dataset.data
from statsmodels.tsa.stattools import adfuller as ADF $ print() $ print(ADF(resid_6203.values)[0:4]) $ print() $ print(ADF(resid_6203.values)[4:7])
total_payments = np.sum(sortdf['disc_times_pay'])/1000000 $ percent_of_total = lambda x:round( x*100/total_payments,1) $ percent_of_total_list = list(map(percent_of_total, my_list)) $ percent_of_total_list[:10]
image_predictions_clean[image_predictions_clean.jpg_url.duplicated()].jpg_url $
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old,n_new], alternative='smaller') $ print(z_score, p_value)
import pandas as pd $ import numpy as np $ import matplotlib as plt $ %matplotlib inline
swinbounds.to_csv('../data/swPositive.csv')
df.info()
sub_set['DEVICE_MODEL'].value_counts()[:30].plot(kind='barh')
tizibika['time_date']=pd.to_datetime(tizibika['timestamp'])
plantlist.loc[plantlist['bnetza_id'] == 'BNA0834', 'fuel'] = 'Natural gas' $ plantlist.loc[plantlist['bnetza_id'] == 'BNA0662a', 'fuel'] = 'Hard coal' $ plantlist.loc[plantlist['bnetza_id'] == 'BNA0662b', 'fuel'] = 'Hard coal'
def create_validation(df, start_date, gap_size, train_size, test_size): $     train = np.arange(start_date, start_date+train_size) $     validate = np.arange(start_date+train_size+gap_size, start_date+train_size+gap_size+test_size) $     return train, validate
import matplotlib.pyplot as plt $ import numpy as np $ import pandas as pd $ %matplotlib inline
submissions['date_created'].apply(lambda x: x.week)
pd.__version__
merged2['Specialty'].value_counts()
sns.distplot(filter_iphone(orig_tweets[(orig_tweets['date'].dt.year==2017)&(orig_tweets['date'].dt.month==1)])['hour'], color='b') $ sns.distplot(filter_android(orig_tweets[(orig_tweets['date'].dt.year==2017)&(orig_tweets['date'].dt.month==1)])['hour'], color='g')
Train_extra['ID'].head()
df_stars['business_id'].nunique(), df_stars['user_id'].nunique()
input_node_types_DF = pd.read_csv('network/source_input/node_types.csv', sep = ' ') $ input_node_types_DF
first_commit_timestamp = pd.to_datetime('2005-04-16 22:20:36') $ last_commit_timestamp = pd.to_datetime('today') $ corrected_log = git_log[(git_log['timestamp']>first_commit_timestamp) & (git_log['timestamp']<last_commit_timestamp)] $ print(corrected_log.describe())
CONSUMER_KEY = 'kIfyYdxTpAdn201jTngWDG06E' $ CONSUMER_SECRET = 'PGELl3ticAMTOMtZm2qbpF2EK8jpp1vnRLIA8P53V8Ctp99sUx' $ ACCESS_TOKEN = '500966533-Ojcp3bcWdtXZiG8eGR5N1Y3514eZCTcKsaFVT3pv' $ ACCESS_SECRET = '38jdrYT2eSefS5GrU4q6sw8oJiYmQ5FkProK7abMdUzIK'
reg_traffic_with_flags.head()
import catboost as ctb $ ctb_model = ctb.CatBoostClassifier(iterations = 40, learning_rate=0.05, depth=4, eval_metric='Logloss', $                                        random_seed=42, rsm=0.8, l2_leaf_reg=2, od_wait=50, od_type='Iter' , verbouse = 0)
print(s[0]) $ print(s[3:5]) $ print(s[-5:])
len(url_data)
(df.set_index('STNAME').groupby(level=0)['POPESTIMATE2010','POPESTIMATE2011'] $     .agg({'POPESTIMATE2010': np.average, 'POPESTIMATE2011': np.sum}))
Celsius._temperature
DD= df['domain'].value_counts().head(30).index.tolist() $ df['domain_d'] = [type_ if type_ in DD $                       else "OTHER" for type_ in df['domain']] $ df['domain_d'].value_counts()
y_resampled.shape
greater_than_diff = [i for i in p_diffs if i > p_diff]
date.month
%%bash $ pwd $ ls
print(cbow_m1.wv.similarity('love', 'like')) $ print(cbow_m2.wv.similarity('love', 'like')) $ print(cbow_m3.wv.similarity('love', 'like')) $ print(cbow_m4.wv.similarity('love', 'like')) $ print(cbow_m5.wv.similarity('love', 'like'))
w.get_step_object(step = 2, subset = subset_uuid).allowed_data_filter_steps
top_apps = df_nona.groupby('app_id').accounts_provisioned.sum() $ top_apps.sort(inplace=True, ascending=False) $ top_apps.head(10)
focount.to_csv('followers.csv', index=False)
pizza_poor_reviews.head(2)
speakers[0].keys()
valid_scores = scores[scores['date']<'2018-03-12'] $ valid_scores.shape
dti = pd.to_datetime(['Aug 1, 2014', '2014-08-02', '2014.8.3', None]) $ dti
df2["intercept"] = 1 $ logit_mod = sm.Logit(df2["converted"], df2[["intercept","ab_page"]]) $ results = logit_mod.fit() $ results.summary()
import pandas as pd $ df = pd.DataFrame(data) $ df
tweets_df.retweet_count.describe()
pd.Series(data=y5_train).hist()
df_2013 = pd.DataFrame(rows)
df[df['Complaint Type'] == 'Homeless Encampment'].sort_index().resample('M').count().plot(y='Complaint Type')
vid_list_return = list(rsp.json()['data'].keys())
import pandas as pd $ flight_pd=pd.read_csv("/home/ubuntu/parquet/flight_pd.csv", delimiter='\t')
sessions_summary = sessions_summary.merge(right=train_users, how="left", left_on=["user_id"], right_on=["id"]) $ sessions_summary.shape[0]
offset.rollforward(d)
!ps auxww
df2 = df2.drop(df.index[1899])
purchase_history = pd.merge(orders_subset, $                             order_details_prior, $                             on=['order_id'])[['user_id', 'product_id']]
dat_before_fill=dat.copy() $ for temp_col in temp_columns: $     dat.loc[:,temp_col]=dat[temp_col].interpolate(method='linear', limit=3)
np.outer(u, v)
x_train = train_data.values
chk=pd.read_csv('sub_average_5models_nocv.csv') $ chk.head(10)
plt.figure(figsize=(10,10)) $ labels = pokemon['Type 1'].value_counts().index $ pokemon['Type 1'].value_counts().plot(kind='pie', labels=labels, autopct='%1.0f%%') $ plt.show()
tc1 = tc[~tc['ISBN RegEx'].isin(bad_tc_isbns)] $ tc1['ISBN RegEx'].size
sns.factorplot('SA','len',data = data, kind = 'point', size = 6)
revenue
'Despite traditionally high dividends for oil sector investors, current markets require shareholders to review corporate financial records to determine if a company has the resources for payouts, according to Marco Scherer of Deutsche Asset Management.' $ 'Superbly productive oilfields are a major pre-requisite for investments in the upstream U.S. oil sector, Scheder added. The consolidation of oilfield services providers has led to the emergence of several well-diversified multinational OFS companies that will be more resilient to oil price changes in the future, he noted.' $ 'As drilling activity dried up following the oil price crash of 2014, OFS companies had no choice but to slash their prices, charging much less for rigs, equipment, and services.' $ 'Now, drilling in the U.S. is coming back quickly, shifting leverage back in favor of OFS companies, who are starting to hike their prices. According to S&P Global Platts, services costs are expected to rise by about 20 percent on average this year. That could offset some of the efficiency gains that shale drillers are accruing as they improve their drilling techniques.')
X[0] $ pred, W, b = model(X_train, Y_train, word_to_vec_map) $ print(pred)
wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)
ofav.plot(figsize=(16,4), label="Likes", legend=True,title="Popularity of Barack Obama over time") $ oret.plot(label="Retweets", legend=True) $ orep.plot(label="Replies", legend=True) $ plt.xlabel('Time') $ plt.savefig("fig/obama_popularity_time.png");
eligible_by_week = {} $ for key in set([x['ama.week.diff'] for x in ama_guests.values()]): $     eligible_by_week[key] = [x['author'] for x in ama_guests.values() if $                              x['ama.week.diff'] == key]
lowest = session.query(func.min(Measurement.tobs)).\ $     filter(Measurement.station == "USC00519281").all() $ print(f"Lowest Temp: {lowest}")
with open(os.path.join(folder_name,url.split('/')[-1]) ,mode='wb') as file: $     file.write(response.content)
df2['intercept'] = 1 $ df2[['drop', 'ab_page']] = pd.get_dummies(df2['group']) $ df2.drop(['drop'], axis=1, inplace=True)
for v in d.variables: $     print(v)
df_train.to_csv("data/df_1.csv", index=False) $ df_test.to_csv("data/df_1_test.csv",index=False)
df.shape
%matplotlib inline $ pandas_df.plot.bar()
pickup_demand = pickup_orders.groupby(['ceil_10min','pickup_cluster'], as_index=False).apply(my_agg).reset_index() $ dropoff_demand = dropoff_orders.groupby(['ceil_10min','dropoff_cluster'], as_index=False).apply(my_agg).reset_index() $ ride_demand = ride_orders.groupby(['ceil_10min','ride_cluster'], as_index=False).apply(my_agg).reset_index()
validation.analysis(observation_data, BallBerry_resistance_simulation_1)
watch_table = events[['repo_id', 'user_id', 'login', 'created_at', 'archive_id']][events['type'] == 'WatchEvent']
nitrogen['ResultSampleFractionText'].unique()
churned_unordered.head()
blah = sub_df['text'].apply(sentiment_scores)
n_rows = ab_data.shape[0] $ n_rows
s[(s > 10) & (s < 15)].head()
psy_native = psy_df5 $ psy_native = psy_native.set_index('subjectkey', verify_integrity=True)
filepath = os.path.join(os.getcwd(), 'static', 'pythonlog.log')
df = df.reindex(np.random.permutation(df.index))
baseball_newind[['h','ab']]
ex4 = pd.DataFrame({"vegetable": ["lettuce", "brocolli", "carrot"], $                    "corn": ["beef", "tofu", "fish"], $                    "stable": ["rice", "pasta", "soba"], $                    "remark": ["sold out","sold out", None]}) $ ex4
joined_samp.head(2)
sl_data.Description.replace('new_confirmed', 'Total new cases registered so far', inplace=True) $ sl_data.Description.replace('etc_new_deaths', 'New deaths registered', inplace=True)
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=26500) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
students.weight.value_counts()
autos = autos[autos['registration_year'].between(1900,2018)] $ autos['registration_year'].value_counts(normalize=True).head(20)
date_convertor_udf = udf(epoch_to_datetime_convert, StringType()) $ reddit_comments_data = reddit_comments_data.withColumn('created_date',date_convertor_udf(reddit_comments_data.created_utc))
print(len(pmcids_list)) $ print (elapsed_time) $ print(pmcids_list[:20])
print("Number of Mitigations in ATT&CK") $ mitigations = lift.get_all_mitigations() $ print(len(mitigations)) $ df = json_normalize(mitigations) $ df.reindex(['matrix', 'mitigation', 'mitigation_description', 'url'], axis=1)[0:5]
base_mention_df.head()
col_totals.head()
b_cal_q1 = pd.DataFrame(b_cal[b_cal['available'] == 't'])
conn.caslibinfo()
fdist.plot(100, cumulative=True)
to_be_predicted_Day3 = 43.4200283 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
!head -3 train.csv
print("mean: \n",df_usa.mean()) $ print("Standard deviation: \n",df_usa.std())
np.exp(output.params)
df.describe()
violation_counts = restaurants["VIOLATION CODE"].value_counts(); $ violation_counts[0:10]
big_df_count=big_df_count.reset_index() $ big_df_avg=big_df_avg.reset_index()
paired_df_grouped.sort_values('co_occurence', ascending=False, inplace=True) $ paired_df_grouped.loc[:, ['dataset_1', 'all_co_occurence']].head(20) $ paired_df_grouped['top10'] = paired_df_grouped.co_occurence.apply(lambda x: x[:10]) $
mnb = MultinomialNB() $ mnb.fit(X_train, y_train) $ print('Training set score:', mnb.score(X_train, y_train)) $ print('\nTest set score:', mnb.score(X_test, y_test)) $ print('\nCross Val score:',cross_val_score(mnb, X_test, y_test, cv=5))
train = pd.read_csv("../data/wikipedia_train3.csv") $ test = pd.read_csv("../data/wikipedia_test3.csv") $ test['date'] = test['date'].astype('datetime64[ns]') $ train['date'] = train['date'].astype('datetime64[ns]')
df2.loc[2893]
def full_DRG_name(drg_number): $     idx = df_DRGs[ (df_DRGs['drg_num']==drg_number)].index.tolist() $     assert len(idx) > 0, 'Length of IDX is NULL' $     return( df_DRGs.loc[idx[0],'DRG_name'] )
item_item_rec_result = item_item_rec.recommend(k=10, verbose=False) $ item_item_rec_result
new_df.head(5)
new_log_mod = sm.Logit(df3['converted'], df3[['intercept', 'new_page', 'UK', 'US']]) $ new_results = new_log_mod.fit() $ new_results.summary()
pd.merge(staff_df, student_df, how='outer', left_index=True, right_index=True)
broadcast_file = "broadcast.txt" $ broadcast_list = list(map(lambda l: l.strip(), open(broadcast_file)))
df2 = df2[df2['timestamp'] != (duplicated.iat[1,1])] $ df2.info()
df.tail()
df_l = df_vu.append(df_cb) #creating a new dataset
print(type(plan))
%load_ext sql
zstat1_score, p_value_twosided = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='two-sided') $ print("The Pvalue for Two sided for alternate Hypothesis is {}".format(p_value_twosided))
tweet.user.location $ tweet.created_at $ tweet.user.name $ tweet.user.screen_name
merged_df_flight_cancels = pd.concat([merged_df_flightdelays, flight_cancels], axis=1)
class_test.shape
explore_text('worldnews')
import gensim $ import logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) $ model = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2)
new=df2.query('landing_page=="new_page"') $ new_samp=new.sample(new.shape[0],replace=True) $ new_page_converted=new_samp.query('converted==1')['user_id'].count()/new_samp['user_id'].count() $ new_page_converted
jobs_in_tech = jobs[jobs.category_name=='Technology'] $ by_day = jobs_in_tech.groupby('created_date')
import json # (dong) $ movieSaved = {feature: movie_df[feature].values.tolist() for feature in movie_df.columns.values} $ fp = open("allMovies_new.json","w") $ json.dump(movieSaved, fp) $ fp.close()
n_new=df2.query('landing_page=="new_page"').count()[0] $ n_new $
quarters.asfreq("M", how="start")
from google.cloud import storage $ import os
(autos["last_seen"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index().tail() $ )
print(f"Fit2 shape: {fit2.shape}, Fit2 non-nulls: {fit2.nnz}") $ print(f"Non-null fraction of total: {'{:.10f}'.format(fit2.nnz/(fit2.shape[0] * fit2.shape[1]))}")
df.as_matrix() # not a matrix, however, just numpy array
scoresdf.set_index('n_estimators')['score'].plot(figsize=(16,8))
df = pd.read_csv('tweet_csvs/realDonaldTrump_tweets.csv', index_col = None, header = 0, $                      parse_dates=['created_at'], infer_datetime_format = True, dayfirst = True)
df['order_date'].max()
cFrame = cumFrame.groupby(('Client','Date')).sum() $ cFrame.head(20)
data['returns'] = np.log(data['AAPL'] / data['AAPL'].shift(1)) $ data['returns'].hist(bins=35)
pd.read_sql(f'explain {sql_on}', engine).head()
df2[df2.groupby('user_id')['user_id'].transform('size') > 1]
sum(series**2)
class_merged.to_csv('class_merged_excl_hol.csv',sep=',')
f_ip_os_minute_clicks.show(1)
check_cols(item_hub, item_hub_dropper)
daily_returns = (port_cum_ret - 1).resample('D').last() $ daily_returns.index = pd.to_datetime(daily_returns.index.astype(str))
ol.data['funding_rounds']
obs_diff = obs_new - obs_old $ obs_diff[0]
sns.pairplot(subset_sessions_summary, hue="country_destination") 
claims.head()
for row in my_df_small.itertuples(): $     print(row)
print(f'dataframe shape: {match.shape}') $ match.head(3)
consumerKey = 'XXXXXXXXXXXXXXXXXXXXXXX' $ consumerSecret = 'XXXXXXXXXXXXXXXXXXXXX' $ auth = tweepy.OAuthHandler(consumer_key=consumerKey, consumer_secret=consumerSecret) $ api = tweepy.API(auth)
percent_top_dollar = 0.2 $ high_volume_symbols = helper.large_dollar_volume_stocks(df, 'adj_close', 'adj_volume', percent_top_dollar) $ df = df[df['ticker'].isin(high_volume_symbols)]
movies.isnull().any()
diff=new_page_converted/n_new-old_page_converted/n_old $ diff
toy['encrypted_customer_id'].nunique()
print("Check the number of records") $ print("Number of records: ", train.shape[0], "\n") $ print("Null analysis") $ empty_sample = train[train.isnull().any(axis=1)] $ print("Number of records contain 1+ null: ", empty_sample.shape[0], "\n")
df['TMAX']= df['TMAX'] * 0.10 $ df['TMIN']= df['TMIN'] * 0.10 $ df['TOBS']= df['TOBS'] * 0.10 $ df['PRCP'] = df['PRCP'] * 0.10
test
df_ctrl = df2[df2['group'] == 'control'] $ df_ctrl['converted'].mean()
daily_returns.hist(bins=100) $ plt.show()
%timeit [name.endswith('bacteria') for name in data.phylum] and [val > 1000 for val in data.value]
df2_no_outliers = df2.copy() $ df2_no_outliers['y'] = np.log(df2_no_outliers['y'])
normals_df = pd.DataFrame(normals, columns=["Date","Min Temp","Avg Temp","Max Temp"]) $ normals_df = normals_df.set_index('Date') $ normals_df
plt.close();plt.close();plt.close();plt.close(); $ plt.close();plt.close();plt.close();plt.close();
help(testObjDocs)
prec_long_df = pd.melt(prec_wide_df, id_vars = ['grid_id', 'glon', 'glat'], $                       var_name = "date", value_name = "prec_kgm2") $ prec_long_df.head()
purchases = pd.read_csv('./raw_data/purchases.csv') $ purchases['Number of Add Ons'].fillna(0.0, inplace=True) $ purchases['Add On Amount'].fillna(0.0, inplace=True)
dat=pd.read_csv(pth) $ print(pth)
data[-1]
len(churned_ordered) + len(churned_unordered)
model.fit(x_train, $           y_train, $           batch_size=batch_size, $           epochs=epochs, $           validation_data=(x_test, y_test))
result_df.head(10)
dat_hcad_zip.drop_duplicates(subset='blk_range',keep="last",inplace=True) $ dat_hcad_zip.columns = ['blk_range','zip'] $ dat_hcad_zip.shape
df_pol.sample(10)
news_tweets_pd = pd.DataFrame.from_dict(news_tweets) $ news_tweets_pd.head()
df.iloc[18248]['AveragePrice']
scattering_to_total = scattering.xs_tally / total.xs_tally $ scattering_to_total.get_pandas_dataframe()
a = df.head(2) $ a.set_index('object_id',inplace=True)
print(reviews_recent20.shape) $ reviews_recent20.head()
df_new.country.unique()
excelDF['Ship Mode']
exiftool -csv -createdate -modifydate cisuabg7/cisuabg7_cycle1.MP4 cisuabg7/cisuabg7_cycle4.MP4 cisuabg7/cisuabg7_cycle6.MP4 > cisuabg7.csv
tweet_json_clean.rename(index=str, columns={"id": "tweet_id"}, inplace=True)
df2[['ab_page', 'intercept']] = pd.get_dummies(df2['group']) $ df2['intercept'] = 1 $ df2.head()
print(users['Registered'].dtype) $ print(users['Cancelled'].dtype) $ print(sessions['SessionDate'].dtype) $ print(transactions['TransactionDate'].dtype)
high = max(v.Open for v in data.values() ) $ low = min(v.Open for v in data.values()) $ print('=>The high and low opening prices for this stock in 2017 were {:.2f} and {:.2f} '.format(high, low) + 'respectively.')
gjw.store.window = TimeFrame(start='2015-09-03 00:00:00+01:00', end='2015-09-05 00:00:00+01:00') $ gjw.set_window = TimeFrame(start='2015-09-03 00:00:00+01:00', end='2015-09-05 00:00:00+01:00') $ elec = gjw.buildings[building_number].elec $ mains = elec.mains() $ mains.plot() $
archive_clean.drop(['retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'],axis=1,inplace=True) $
np.random.seed(123) $ a=np.random.randint(-100,100,size=(5,20)) $ plt.plot(a) $ plt.show()
list(db.fs.files.find())
autos['registration_year'].max()
van15_fin.head()
import azureml.core $ print("SDK Version:", azureml.core.VERSION)
for col in train.columns: $     print(col,':',train[col].dtype)
np.exp(-0.0099), np.exp(-0.0507)
blame.to_csv("C:/Temp/linux_blame.gz", encoding='utf-8', compression='gzip', index=None)
cursor.execute(sq81) $ cursor.execute(sq82) $ results = cursor.fetchall() $ results
%%sql $ use pidata; $ show tables;
facts_url = "https://space-facts.com/mars/"
pd.to_datetime(df['Date']).dt.day
print("State space:", env.observation_space) $ print("- low:", env.observation_space.low) $ print("- high:", env.observation_space.high)
nold = len(df2[df2['group'] == 'control']) $ nold
fin_pivot_table = pd.merge(fin_pivot_table_tr, fin_pivot_table_ni, left_index=True, right_index=True) $ fin_pivot_table['Avg Profit Margin'] = fin_pivot_table['Avg Net Income']/fin_pivot_table['Avg Total Revenue'] $ fin_pivot_table.sort_values(by = ['Avg Profit Margin'], ascending = False)
lr_stack = LogisticRegression() $ stack_score = cross_val_score(lr_stack,stack_X_test,stack_y_test,cv=10)
df.describe()
dataset.to_csv(output_path, sep=",")
ocsvm_cleaned_bow.fit(trump_cleaned_bow, y = y_true_cleaned_bow) $ prediction_cleaned_bow = ocsvm_cleaned_bow.predict(test_cleaned_bow) $ prediction_cleaned_bow
autos["vehicle_type"] = autos["vehicle_type"].map(vehicle_type_eng) $ autos["gearbox"] = autos["gearbox"].map(gearbox_eng) $ autos["fuel_type"] = autos["fuel_type"].map(fuel_type_eng)
users['Registered'] = pd.to_datetime(users['Registered']) $ users['Cancelled'] = pd.to_datetime(users['Cancelled']) $ sessions['SessionDate'] = pd.to_datetime(sessions['SessionDate']) $ transactions['TransactionDate'] = pd.to_datetime(transactions['TransactionDate']) $
ts.index[0]
prob_temp = df2.converted.mean() * 100 $ print("Probability of an individual converting regardless of the page they receive: {}%".format(round(prob_temp, 2)))
json_data = r.json() $ json_data
testmx2.shape
rng_utc.tz
display_all(train_data.head(5).transpose())
articles[0]
sentiment_df = sentiment_df.sort_values(["Date"]) $ sentiment_df
pr('Starting to read file... (3 min)') $ tw = pd.read_csv(filename, sep='\t', encoding='utf-8', escapechar='\\', names=columns_header, $                       quoting=csv.QUOTE_NONE, na_values='N', header=None) $ pr('File is loaded.')
learn.save("dnn60")
max_dif = max(dif_dict.values()) $ for key, value in dif_dict.items(): $     if value == max_dif: $         print key, value
df2.head()
clf = LogisticRegression(fit_intercept=True) $ clf.fit(X_transform, y)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ mydata = pd.read_csv(path, sep ='\s+', nrows=80, skiprows=(1,2,5,10,60), usecols=(1,3,5)) $ print("mydata.head(5)\n",mydata.head(5)) $ print("Length of mydata = ",len(mydata))
df_enhanced.info()
station = pd.DataFrame(hawaii_measurement_df.groupby('Station').count()).rename(columns={'Date':'Count'}) $ station_count = station[['Count']] $ station_count $
corpora.BleiCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.lda-c'), corpus)
train['number_of_features'] = train['features'].map(len)
df_goog.sort_values('Date', inplace=True) $ df_goog.set_index('Date', inplace=True) $ df_goog.index = pd.to_datetime(df_goog.index)
tweets092315 = tweets092315[['id','text']]
hs=hydroshare.hydroshare()
%%time $ X_df.iloc[:, 5] = X_df['text'].apply( lambda rev: rev.replace('_', '') )
speakers.reset_index(inplace = True)
df_clean = ea.copy()
ttTimeEntry.drop_duplicates(subset = mainkey + ['DT'], inplace = True)
from sklearn.model_selection import train_test_split
import spacy
breed_predict_df[breed_predict_df.tweet_id.duplicated()]
records['Gender'] = records['Gender'].apply(str.capitalize)
logins['datetime'] = pd.to_datetime(logins.login_time)
tweetering.corr()
datatest.loc[datatest.floor > 100, 'floor'] = np.NaN
most_confident_predictions = pd.merge(left=highest_confidence, right=stacked_image_predictions, how='left',on=['tweet_id','confidence'])
data.text
taxi_hourly_df.shape
import numpy as np $ import datetime as datetime
import pandas as pd $ import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
df_user_extract = pd.read_csv('data/df_user_extract.csv') $ df_user_extract_copy = df_user_extract.copy() $ df_user_extract_copy.head()
mentions = read_csv("mentions.csv") $ mentions.head()
df_test_attempt = df_test_attempt[df_test_attempt["user id"] != mvoid_to_bson_id('000000000000000000000000')]
mms = MinMaxScaler() $ import copy $ columns_to_scale =['payment_plan_days','not_auto_renew','msno_count','is_cancel','is_discount','amount_per_day','membership_duration','long_time_user','bd']
df_country = pd.get_dummies(df_new['country']) $ df_new = pd.concat([df_new, df_country], axis=1) $ df_new.head()
dta.violations.unique().shape
merged_df["T"] = (pd.datetime.now().date() - merged_df['first']).astype('timedelta64[W]') $ merged_df.head()
data.fillna(method='ffill')
tweets
cum_cont = pd.DataFrame(data={"cum_contr":np.cumsum(contributions_by_day.contb_receipt_amt),"date":contributions_by_day.contb_receipt_dt}) $ cum_cont = cum_cont.sort_values('date', ascending=True).reset_index() $ cum_cont $ contributions_by_day
best_model = h2o.get_model(gbm_grid_cart.model_ids[0]) $ best_model
from textblob import TextBlob $ from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer $ analyzer = SentimentIntensityAnalyzer()
df2['wind_mph'] = [5, 4, 5, 10, 4, 7, 5] $ df2
austin[austin['total_fare'].isnull()].index.tolist()
print(type(coin_data.index)) $ coin_data.columns
a = dat.ward.unique() $ a.sort() $ a
(df_providers['year']==year | df_providers['region']=='CA - Chico' )
df2.head()
total_stations
dinw_filter_set = w.get_step_object(step = 2, subset = subset_uuid).get_indicator_ref_settings('din_winter') $ dinw_filter_set.allowed_variables
sns.violinplot(autos["registration_year"])
from sklearn.preprocessing import StandardScaler $ scaler = StandardScaler() $ X_train_scaled = scaler.fit_transform(X_train) $ X_test_scaled = scaler.transform(X_test)
import pandas as pd $ import numpy as np $ from datetime import datetime
temps_df[temps_df.Missouri > 82]
data_grid3
df_n = df2.query('landing_page == "new_page"') $ df_n.shape[0]
df.head()
X_test.shape
df['is_member'] = df.purchase_date.apply(lambda x: "Not Member" if x == None else "Member") $ df.head(5)
result.loc[(result['timestamp'] >= 1.4e+13) & (result['timestamp'] <= 1.6e+15), 'timestampCorrected' ] = result['timestamp']/1000
countries.shape[0] == df2.shape[0]
errors['datetime'] = pd.to_datetime(errors['datetime'], format="%Y-%m-%d %H:%M:%S") $ errors['errorID'] = errors['errorID'].astype('category')
tweets3.text[0]
sample = sample[sample['polarity'] != 2] $ sample['sentiment'] = (sample['polarity'] ==4).astype(int)
B = pd.DataFrame({'team':['cle','cle','bos'],'ast':[10,9,8]}) $ B
sp.head()
plt.scatter(x,y, color = 'lightgreen') $ plt.plot(x, np.dot(x3, model3.coef_) + model3.intercept_, color = 'dimgrey', linewidth = '5.0')
MICROSACC.plot_default(microsaccades,subtype="amplitude mean")
df['country code'] = df['country code'].astype(str) $ df.info()
ABT_tip.head()
pizza_ratings.head()
im_clean["p1"] = im_clean["p1"].str.replace("-", "_") $ im_clean["p2"] = im_clean["p2"].str.replace("-", "_") $ im_clean["p3"] = im_clean["p3"].str.replace("-", "_")
mean_year_first = df.groupby(df.date.dt.year)['first_day_pctchg'].mean()
print('Num entries where name is >= 2 and < 3:\t{}'.format( $         org_counts[(org_counts >= 2) & (org_counts < 3)].sum()))
crimes_in_window, stops_window = stops_vs_crime(zip_1_df,zip_1_sns,'MS','MS')
df_clean.rating_denominator = df_clean.rating_denominator.str.replace('(\d*)', '10')
import os $ print('\n'.join(os.environ['PATH'].split(':')))
ab_df_new['treatment_US'] = ab_df_new.ab_page * ab_df_new.US $ ab_df_new['treatment_CA'] = ab_df_new.ab_page * ab_df_new.CA $ ab_df_new.head()
df_top_cat_2015=df.groupby(df_2015.category_name,as_index=False).agg({ $         "sale_dollars": np.sum, $         "category_name": lambda x: x.iloc[0] $                 }).sort_values(by='sale_dollars', ascending=False)
user_project_grouped = user_project.groupby(['user_id','project_id']).count() $ user_project_grouped.head()
lego_color_cat.head()
import torch $ import torch.autograd $ from torch.autograd import Variable
print("5 recent tweets:\n") $ for tweet in tweets[:5]: $     print(tweet.text, '\n')
df.loc[df.sex.isin(sex2m)]
n_unique_users = df['user_id'].nunique() $ print ("Number of unique users in the dataset: {}".format(n_unique_users))
twitter_ar.expanded_urls[100]
eth = pd.read_csv('data/eth-price.csv', parse_dates=True, index_col=0) $ print(eth.info()) $ eth.head()
twoWordReviews = twoWordReviews[twoWordReviews["Improvements_lemma"] != "REMOVEPLEASE"] $ print("Total reviews:  ", len(data)) $ print("2-word reviews: ", len(twoWordReviews))
elms_all.ORIG_DATE.min()
df1 = pd.DataFrame({"A":["A1", "A2"], $                     "B":["B1","B2"]},index=[1,2]) $ df2 = pd.DataFrame({"A":["A3", "A4"], $                     "B":["B3","B4"]},index=[3,4]) $ pd.concat([df1,df2], axis=1)
1/_
fullDf[fullDf.country=='NP'].levelIndices.value_counts()
!hadoop fs -ls -h stocks.parquet
total_data_rows = len(df.index) $ df.dropna(subset = ['UNIQUE_CARRIER','ORIGIN','DEST','CRS_DEP_TIME','CRS_ARR_TIME','ARR_DELAY','CRS_ELAPSED_TIME','DISTANCE'],inplace=True) $ data_retained = len(df.index)/total_data_rows $ print('Data Retained: '+str(round(data_retained*100,2))+' %')
SVM_yhat = SVM_model.predict(test_X) $ print("SVM Jaccard index: %.2f" % jaccard_similarity_score(test_y, SVM_yhat)) $ print("SVM F1-score: %.2f" % f1_score(test_y, SVM_yhat, average='weighted') )
from sklearn.neighbors import KNeighborsRegressor $ knn = KNeighborsRegressor(n_neighbors = 3, p = 1, weights = 'distance')
for row in nocachedf.itertuples(): $     do_ifcformant(row, indir=srcdir, outdir=cachedir)    
fin_r_monthly = fin_r.resample('M').asfreq()
prices = prices.join(announcement_dates)
?df.head()
docs = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names()) $ docs.sum()
for key, value in think_tank_party_dict.items(): $     think_tank_party_dict[key] = [item for sublist in value for item in sublist]
sales = graphlab.SFrame('Data/kc_house_data.gl/') $ sales.head()
print('reduce memory') $ utils.reduce_memory(df) $ df.drop(['t-1_date'], axis = 1, inplace = True)# saving memory $ df = df.tail(n = df.shape[0] -1 )
city_data1 = "city_data.csv" $ ride_data1 = "ride_data.csv" $ city_data_df = pd.read_csv(city_data1) $ ride_data_df = pd.read_csv(ride_data1) $ ride_data_df.head(5) $
sum(tw_clean.duplicated(subset=['expanded_urls']))
print(merged.info())
pn_counter
data_df = pd.read_csv('../datasets/flight_delays_data.csv') $ data_df.shape
df.to_json("json_data_format_index.json", orient="index") $ !cat json_data_format_index.json
bp["new charttime"]=bp["charttime"]-time_delta $ bp["new realtime"]=bp["realtime"]-time_delta $ bp.head()
groceries[[0,-1]]
plt.figure() $ qplt.contourf(temperature_cube, 25) $ plt.gca().coastlines() $ plt.show()
reviews.to_csv('./cleaned/reviews.csv', header=False, index=False)
sme = SMOTEENN() $ X_train, y_train = sme.fit_sample(X_train, y_train) $ print(X_train.shape) $ print(y_train.shape) $ unique(y_train, return_counts=True)
token_text = [token.orth_ for token in parsed_review] $ token_pos = [token.pos_ for token in parsed_review] $ pd.DataFrame(zip(token_text, token_pos), $              columns=['token_text', 'part_of_speech'])
Jarvis_ET_Combine
client.repository.list_models()
data.head()
preprocessor("</a>This :) is :( a test :-)!")
autos['price'].value_counts().sort_index(ascending = False).head(20)
train.shape
pre_strategy_google.head(1)
sumPre = dfPre['MeanFlow_cfs'].describe(percentiles=[0.1,0.25,0.75,0.9]) $ sumPost = dfPost['MeanFlow_cfs'].describe(percentiles=[0.1,0.25,0.75,0.9])
messy['Conditions (standardized)'] = conditions_clean $ messy.head()
use_parameters = w.data_handler.physical_chemical.filter_parameters['use_parameters'] $ print(use_parameters)
def get_integer3(s): $     return gearbox_list.index(s)
%%time $ tcga_target_gtex_expression_hugo_tpm = tcga_target_gtex_expression_hugo \ $     .apply(np.exp2).subtract(0.001).groupby(level=0).aggregate(np.sum).add(0.001).apply(np.log2)
cig_data_SeriesCO.shape
soup = bs(response.text, "html.parser")
import pandas as pd $ from bs4 import BeautifulSoup as bsfacts $ import requests
print('There are {:.0f} visa applications for jobs located in Mountain View in this dataset.'.format(df_h1b_mv.shape[0]))
cur.execute("use db")
ttarc_clean['dog_stage'] = np.where(ttarc_clean['puppo']==1, 1, 0) $ ttarc_clean['dog_stage'] = np.where(ttarc_clean['pupper']==2, 2, ttarc_clean['dog_stage']) $ ttarc_clean['dog_stage'] = np.where(ttarc_clean['doggo']==3, 3, ttarc_clean['dog_stage']) $ ttarc_clean['dog_stage'] = np.where(ttarc_clean['floofer']==4, 4, ttarc_clean['dog_stage'])
print(dictionary.token2id)
df_outcomes = np.array(pd.get_dummies(df_h1b_ft_US_Y.status).CERTIFIED)
ts
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
abc = pd.read_csv('DRG Categorical Payments by YEAR.csv',index_col=0)
df = pd.read_csv('anova_one_factor.csv')
final = pd.merge(merged, areas, on='state', how='left') $ final.head()
cols = ['chanel', 'logins'] $ users_visits = users_visits[cols] $ logins_per_user_per_chanel_path =  output_path + '\\logins_per_user_per_chanel.csv' $ users_visits.to_csv(logins_per_user_per_chanel_path)
to_be_predicted_Day1 = 30.84 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
rollrank_fxn = lambda x: x.rolling(200,min_periods=20).apply(lambda x: pd.Series(x).rank(pct=True)[0],raw=True) $ features['f11'] = prices.groupby(level='symbol').volume.apply(rollrank_fxn)
conn = pg8000.connect(user = 'dot_student', database='training', port=5432, host='training.c1erymiua9dx.us-east-1.rds.amazonaws.com', password='qgis')
bikes['hour_of_day'] = (bikes.start.dt.hour + (bikes.start.dt.minute/60).round(2)) $ hours = bikes.groupby('hour_of_day').agg('count') $ hours['hour'] = hours.index $ hours.start.plot() $ sns.lmplot(x='hour', y='start', data=hours, aspect=1.5, scatter_kws={'alpha':0.2})
def is_getlocated(tweet): $     if isinstance(tweet, dict) and tweet['place']: $         return True $     return False
per_tweet_archive_by_month.plot()
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True)
df.replace({0: 3, 1: 1}, 99)
(p_diffs > actualdiff).mean()
ben_fin['reverted_mode'].value_counts()
changes = [] $ for day in dict_17: $     changes.append(dict_17[day]['High']- dict_17[day]['Low']) $ largest_variation = max(changes) $ round(largest_variation,2)
r = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2018-03-27&end_date=2018-03-27&api_key='+API_KEY)
noise = noise.drop('Unnamed: 0',axis=1)
df_vow[['Open','High','Close','Low']].plot()
len(ind)
df_count_clean["tweet_id"] = df_count_clean["tweet_id"].astype(str)
'square of 5' in squares
archive_df.head()
df['duration'].sum()
dates_location = combined_location['DATE'] $ entries_location = combined_location['ENTRIES'] $ dates_station = combined_station['DATE'] $ entries_station = combined_station['ENTRIES']
df.loc['r2']['A']
p_new_0 = df_treatment.query('converted == 1').user_id.nunique() / df_treatment.user_id.nunique() $ p_old_0 = df_control.query('converted == 1').user_id.nunique() / df_control.user_id.nunique() $ obs_diff_0 = p_new_0 - p_old_0 $ np.mean(p_diffs > obs_diff_0)
tweets_df_clean.drop(['retweeted'], axis=1, inplace=True)
daily_returns=compute_daily_returns(df) $ plot_data(daily_returns,title="Daily Returns")
data_vi_week.ix[days].plot(lw=3, figsize=(6,4), kind='bar'); $ plt.ylim(179000);  # Set the bottom axis to 179000.
closing_values=pd.concat([df_nike.Close,df_apple.Close,df_disney.Close],axis=1) $ closing_values.columns=['Nike','Apple','Disney'] $ closing_values.head()
from pyspark.sql import Row
import pandas as pd $ data = pd.read_csv('users.csv', error_bad_lines=False, header=None) $
df2['user_id'].unique().size
ign = pd.read_csv ("ign.csv") $ ign = ign.drop('Unnamed: 0', axis=1) $ ign.head()
sns.countplot(y="lang", data=tweet_table, palette="Blues")
tweets_master_df.ix[774, 'expanded_urls']
from nltk.corpus import stopwords $ stop = set(stopwords.words('english')) $ clubs = pd.read_sql("SELECT DISTINCT subreddit FROM comments", con=engine)['subreddit'] $ clubs
n_old= df2['landing_page'].value_counts()["old_page"] $ n_old
preproc_reviews = pipeline.fit_transform(review_body) $ pipe_cv = pipeline.named_steps['cv']
full['Age'].unique().shape[0]
with open('tweets-23-04.json') as json_data1: $     jdata1 = json.load(json_data1) $     dfall = pd.DataFrame(jdata1)
df['CIK']=df['CIK'].map(lambda x:str(x).zfill(10))
from keras.models import Sequential $ from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout $ from keras.optimizers import SGD $ from keras.utils import np_utils
df.head()
dataset.hist()
import seaborn as sns $ planets = sns.load_dataset('planets') $ planets.shape
df_dd = pd.DataFrame() $ for date_col in df_datecols: $     dd_col = date_col + '_index_dd' $     df_dd[dd_col] = (df_EMR['index_date'] - df_datecols[date_col]).dt.days
random_walk = random_steps.cumsum()
df_merged['country'].value_counts()
stations = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).order_by(func.count(Measurement.date).desc()).all() $ stations
predict_acct_id_udf = F.udf(predict_acct_id,types.DoubleType()) $
grp_tweet["content"].count()
content_performance_bytime[content_performance_bytime['_merge']=='right_only']
temp = pd.Series(28 + 10*np.random.randn(10)) $ print(temp.describe())
def convert_GMT_to_timezone(twitter_datetime_object, timezone_desired): $     GMT_datetime_object = twitter_datetime_object.replace(tzinfo=timezone('GMT')) $     timezone_datetime_object = GMT_datetime_object.astimezone(timezone(timezone_desired)) $     return timezone_datetime_object
wavelengths = serc_refl['Metadata']['Spectral_Data']['Wavelength'] $ print('wavelengths:',wavelengths)
specsJson = getspec(specsUrl)
lab_choices = ['[High school level laboratory]','[First year undergraduate physics laboratory]','[First year undergraduate chemistry laboratory]','[Higher level physics labs]','[Higher level chemistry labs]'] $ p = [] $ p.append(lab_choices) $ p.append([len(pre_analyzeable[pre_analyzeable['[prior_lab] What lab courses are you presently taking or have taken in the past? Check all that apply. '+choice]==1]) for choice in lab_choices]) $ print tabulate(p) $
(p_diffs > p_diff_ab).mean()
tq1 = [] $ tq2 = [] $
features = pd.merge(ind, noise_graf, on='AFFGEOID')
pd.value_counts(doctors['AppointmentDuration'])
twitter_archive.rating_denominator.value_counts()
control_df = df2.query('group == "control"') $ control_pro = control_df.query('converted == 1').user_id.nunique() / control_df.user_id.nunique() $ control_pro
oil_pandas = oil_pandas.fillna(method='bfill') $ oil_pandas = oil_pandas.fillna(method='ffill') $ oil_pandas.head()
Y_mat = Y_df1.interest.as_matrix() $ Y_mat[:5]
loan_requests_indebtedness.to_csv('loan_requests_indebtedness_all_user_{}.csv'.format(tod),encoding='UTF-8')
min_x = openmc.XPlane(boundary_type='reflective', x0=-0.63) $ max_x = openmc.XPlane(boundary_type='reflective', x0=0.63) $ min_y = openmc.YPlane(boundary_type='reflective', y0=-0.63) $ max_y = openmc.YPlane(boundary_type='reflective', y0=0.63)
df_not_categorical = df_EMR.select_dtypes(include=['float64', 'datetime64', 'int64']) $ df_not_categorical.shape $ df_not_categorical.head()
sentiments_pd = pd.DataFrame.from_dict(sentiments).round(3) $ sentiments_pd
date_price.head(1)
BroncosBillsPct.to_csv('BB_Pct.csv')
df['PRCP'].plot(figsize=(15,8),  style=['-']);
df2['intercept'] = 1 $ df2_dummies = pd.get_dummies(df2['landing_page']) $ df2 = df2.join(df2_dummies) $ df2['ab_page'] = pd.get_dummies(df['group'])['treatment'] $
pd.Series(feature_names).sample(20)
p_value=(1-0.19/2) $ p_value
a= np.dot(tfidf_test,tfidf_train.T) $ print("cosine sim. values : " +str(a.A)) $ a_sort = np.argsort(a.A) $ print("index from closest to farthest away : "  + str(a_sort))
for df in (train,test): $     df.loc[df.CompetitionDaysOpen<0, "CompetitionDaysOpen"] = 0 $     df.loc[df.CompetitionOpenSinceYear<1990, "CompetitionDaysOpen"] = 0
dup_user = df2[df2['user_id'].duplicated()] $ dup_user
pd.value_counts(ac['If No CR, Why?'])
file = 'https://assets.datacamp.com/production/course_1975/datasets/hourly_wages.csv' $ wages = pd.read_csv(file) $ wages.head()
(np.array(p_diffs)>diff_ab_data).mean()
df_clean.head(5)
gMapAddrDat.testConnection()  ## uses default test record to just ensure connection is working
from IPython.display import Markdown, display $ def printmd(string): $     display(Markdown(string)) $ p_new = df2['converted'].mean() $ printmd("**P*new***: {:0.4}".format(p_new))
tweets_original['retweeted'].value_counts()
%matplotlib inline
project_size=pd.value_counts(ac['Project'].values, sort=True, ascending=False) $ project_size.head(20)
X_predict = recent.iloc[:, 1:8].as_matrix() $ y = regr.predict(X_predict) $ recent["Predictions"] = pd.DataFrame(y) $ ordered = recent.sort_values("Predictions", ascending = False)
full['AdmitDate'].hist(xrot=90,figsize=(12,4)) $ plt.title('Patient distribution over dates',size=15) $ plt.ylabel('count')
for cell in openmc_cells: $     for rxn_type in xs_library[cell.id]: $         xs_library[cell.id][rxn_type].load_from_statepoint(sp)
fig, ax = plt.subplots() $ ax.plot_date(trueDate, dff['GOOG'][trueDate], marker='o', linestyle='-') $ ax.plot_date(trueDate, df_true['GOOG'][trueDate], marker='o', linestyle='-') $
len(artist)
nb = MultinomialNB()
out_train.columns = ["high", "medium", "low"] $ out_train.head()
twitter_final['time_cat'] = twitter_final.time.apply(lambda x: firstpart(x)) $ twitter_final['time_cat'] = twitter_final.time_cat.apply(lambda x: strip_zero(x)) $ twitter_final['time_cat'] = twitter_final.time_cat.apply(lambda x: replace_zero(x)) $ twitter_final['time_cat'] = twitter_final.time_cat.apply(lambda x: time(int(x)))
contribs.committee_name.value_counts().reset_index()
noshow_df.info()
df.rename(columns={'Indicator':'Indicator_id'}).head(2)
print confusion_matrix(y_test, rf_yhat)
lm=sm.Logit(df3['converted'],df3[['Intercept','ab_page','UK','US']]) $ results=lm.fit() $ results.summary2()
plt
dfETHPrice.corr()
df_selparams.head()
visited.head()
fix_zip('UNKNOWN')
df = get_directory_info(r'sample_logs')
data["media"] = None
total_df.drop_duplicates(subset = 'Res_id', keep = 'first', inplace = True)
stories.shape
baseball[['player','sb','cs']].sort_values(ascending=[False,True], $                                            by=['sb', 'cs']).head(10)
df_2003 = pd.DataFrame(rows)
df['listed'] = [True if i=='y' else False for i in df['listed']]
excelref_df.head(1)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print("The value for the z_score is - {}".format(z_score)) $ print("The value for the p_value is - {}".format(p_value))
np.mean(bs_new_means), np.mean(bs_old_means), np.mean(p_diffs)
new_page_converted = np.random.binomial(1, p_new, n_new) $ new_page_converted[:5]
multi.handle.value_counts() / multi.shape[0]
drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7 # We should probably update these to match below? Or tune mult upwards
contract_history.isnull().sum()
full.tail()
ALL = BAL.append([CHI, CIN, HOU, NYG, PHI, PIT, SEA]) # Merges the eight data sets into one big sample $ ALL.head() # Just checking the column headers
stats_url = 'https://api.blockchain.info/stats'
type(my_town.sentiment.polarity)
crimes['month'] = crimes.DATE__OF_OCCURRENCE.map(lambda x: x.month) $
tobsDF = pd.DataFrame(popStationData[:-1], columns=["date", "tobs"]) $ tobsDateDF = tobsDF.set_index("date") $ tobsDateDF = tobsDateDF.sort_values(by=["date"], ascending=False) $ tobsDateDFclean = tobsDateDF.dropna(axis=0, how="any") $ tobsDateDFclean.head()
expenses_df.pivot(index = "Day", columns = "Buyer", values = "Amount")
faa_data_substantial_damage_pandas = faa_data_pandas[faa_data_pandas['DAMAGE'] == "S"] $ print(faa_data_substantial_damage_pandas.shape) $ faa_data_substantial_damage_pandas.head()
autos["price"].median()
all_tweets.shape
co_occurence_on_top50_slug
uniqueCreatedTimeByUser = firstWeekUserMerged[['userid','createdtm']].drop_duplicates() $ uniqueCreatedTimeByUser.head(5)
acs_df = acs_df[acs_df['year'] != 2014]
df1_clean = df1_clean.drop(['in_reply_to_status_id', 'in_reply_to_user_id'], axis=1)
print(list(cos.buckets.all()))
node_types_DF = pd.read_csv(node_models_file, sep = ' ') $ node_types_DF
breakdown[breakdown != 0].sort_values().plot( $     kind='bar', title='Russian Trolls Number of Links per Topic' $ );
stock_data.loc[stock_data['close'] > 80]
from geopy.geocoders import Nominatim $ geolocator = Nominatim(timeout = 199) $ location = geolocator.reverse("40.7145, -73.9425").address
writer = pd.ExcelWriter("test.xls")
btc_price_df['mean'] = btc_price_df.loc[:,["close", 'open', 'low', 'high']].mean(axis = 1)
df.tail(1)
df_data_1.dtypes
ms_columns = inspector.get_columns('measurements') $ for c in ms_columns: $     print(c['name'], c["type"]) $
df.head(2) # See the top 2 rows
from nltk.corpus import brown $ hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned'])) $ hobbies_learned.findall(r"<\w*> <and> <other> <\w*s>")
print 'total number of observations:', len(gas)
df.subtract(df['R'], axis=0)
sentiments_pd.count()
brand_counts.head(4)
Station = Base.classes.station $ stations = session.query(Station).count() $ print(f'There are {stations} stations in the database')
tmp_df = ratings.pivot(index='userId', columns='movieId', values='rating')
kick_projects_ip_scaled_ftrs[:3] $
print(os.getcwd())
(p_diffs > obs_diffs).mean()
autos.head(10)
df.mean()
df = pd.read_sql('SELECT actor_id, first_name, last_name FROM actor WHERE last_name ilike \'%GEN%\'', con=conn) $ df
bacteria_1000 = [name.endswith('bacteria') for name in data.phylum] and [val > 1000 for val in data.value] $ data[bacteria_1000]
site_valsdf = pd.DataFrame.from_records(site_vals)
cc.close.describe()
goog.head()
tweets_df = tweets_df[tweets_df['Text'].apply(lambda x: ner.is_tweet_about_country(x, 'FR'))] $ tweets_df
seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)
print(tweets)
aq = [w for w in cfd_index if re.search('^[aq]+$', w)] $ print({k:cfd_index[k] for k in aq if k in cfd_index})
df = pd.read_sql('SELECT actor_id, first_name, last_name FROM actor WHERE last_name ilike \'%LI%\' ORDER BY last_name, first_name', con=conn) $ df
df_city_reviews.count()
dbData.head(7)  # NaN's show up when the field has no data.  Need both masses, eccentricity, semimajor axis, $
giss_temp = giss_temp[[u'Jan', u'Feb', u'Mar', u'Apr', u'May', u'Jun', u'Jul', $                        u'Aug', u'Sep', u'Oct', u'Nov', u'Dec']] $ giss_temp
len(df) #Afters cleansing, df contains 9840 cases.
a.A.flatten().shape
df_new.info()
search = tmdb.Search() $ response = search.movie(query='Lost in translation') $ for s in search.results: $     print s $     print(s['title'], s['id'], s['release_date'], s['popularity'])
engine.execute("SELECT COLUMN_NAME FROM information_schema.COLUMNS WHERE TABLE_NAME =  'contractor';").fetchall()
fruits['bananas'] + 2
df_twitter_copy[df_twitter_copy.retweet_count.isnull()]
S.decision_obj.stomResist.value = 'Jarvis' $ S.decision_obj.stomResist.value
classification_data.loc[0]
train_small_data.info("deep")
x_min=np.around(np.amin(windfield_matched_array),decimals=-1)-10 $ x_max=np.around(np.amax(windfield_matched_array),decimals=-1)+10 $ x_num= np.around(np.amax(windfield_matched_array)-np.amin(windfield_matched_array))
act_diff = df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean() $ p_diffs = np.array(p_diffs) $ (act_diff < p_diffs).mean()
for i in c.find(): $     print i
rf_v2.hit_ratio_table(valid=True)
import pandas as pd $ from sqlalchemy import create_engine $ db_connection = create_engine('sqlite:///data/chinook.db') $ df = pd.read_sql_query('SELECT * FROM albums', db_connection) $ df.head(2)
projectTable = pd.read_csv('projectTable.csv', index_col = 0) $ projectTable
print('This is the list of folders in your directory for this HydroShare resource.') $ test = [each for each in os.listdir(homedir) if os.path.isdir(each)] $ print(test)
data=requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2018-07-01&end_date=2018-07-31&api_key=%s" %API_KEY)
from sklearn.model_selection import train_test_split
file = '../Data_sources/coinbase_historical_pricechange.csv' $ df = pd.read_csv(file) $ df.head()
df[df['Complaint Type'] == 'Noise - Residential']['Descriptor'].value_counts()
train_data_df ['created_year'] = train_data_df ['created'].apply(lambda ts:ts_interval_to_years(ts))
prcp_data_df  = pd.DataFrame(prcp_data,columns=["Precipitation Date", "Precipitation"])
b2b_df = pd.read_csv(data_fp, header=None, names=['brain_weight', 'body_weight']) $ b2b_df.head()
cvec_1 = CountVectorizer(ngram_range = (1,2), lowercase = True, stop_words = 'english') $ cvec_2 = CountVectorizer(ngram_range = (2,4), lowercase = True, stop_words = 'english') $ cvec_3 = CountVectorizer(ngram_range = (3,4), lowercase = True, stop_words = 'english', max_features = 5000) $ cvec_4 = CountVectorizer(ngram_range = (4,5), lowercase = True, stop_words = 'english', max_features = 5000) $
regr = linear_model.LinearRegression()
df_new.tail()
df = pd.concat([df1, df2[df2['State'] == 'PCS-etgl']])
urban_type_df = clean_combined_city_df.loc[clean_combined_city_df["type"] == "Urban", :] $ suburban_type_df = clean_combined_city_df.loc[clean_combined_city_df["type"] == "Suburban", :] $ rural_type_df = clean_combined_city_df.loc[clean_combined_city_df["type"] == "Rural", :] $ rural_type_df.head() $
vid = 'hEDK3tC43SQ'
Counter(df['tweet_ats']).most_common(11)
for col_name in ['project_id','category','country','location_type','usd_goal']: $     if col_name in df_fail_success.columns: $             print('removed column {}'.format(col_name)) $             df_fail_success=df_fail_success.drop([col_name], axis=1)
telemetry_feat.head()
help(h2o.estimators.glm.H2OGeneralizedLinearEstimator) $ help(h2o.estimators.gbm.H2OGradientBoostingEstimator)
fe.bs.smallsample_loss??
print('There are {} Sites'.format(len(df.index)))
corn_vege.agg({"price": ["max","min"], "sold": "size"}).rename( $             columns={"max": "most expensive", "min": "cheapest", "size":"number of sold" })
colnames = ['GEO.id2', 'HC01_EST_VC10'] $ wealthy = census[colnames] $ wealthy.rename(columns={'GEO.id2': 'zipcode', 'HC01_EST_VC10': 'betw150kand200k'}, inplace = True) $ wealthy = wealthy.loc[1:,]
query = get_asset(asset_id_list[0])
import statsmodels.api as sm $ from statsmodels.formula.api import logit $ sts_model = logit('label ~ total_bill + tip + size', data=tdf).fit() $ sts_model.summary()
reddit['title_len'] = reddit['title'].apply(lambda x: len(x.split()))
data.plot()
len(rng)
joined = joined.dropna(axis=0) $ print('Number of rows in joined = {}'.format(joined.CustomerID.count()))
print(y_hat)
df_EMR_dd_dummies_no_sparse = df_EMR_dd_dummies.drop(columns=ls_sparse_cols)
df = pd.DataFrame(df, columns = ['tweet_id', 'favorites', 'retweets']) $ df.to_csv('tweet_json.txt', encoding = 'utf-8')
adj_close.head()
df.set_index('Date',inplace=True) $ df.head()
releases['jurisdiction_cd'].value_counts()
skills = pickle.load(open('./data/skills.pickle.dat', 'rb')) $ sim = TagSimilarity(skills, df.iloc[:-100]) $ sim.train()
sub_mean = df.groupby('subreddit').agg({'num_comments': 'mean'}) $ top_com = sub_mean.sort_values('num_comments', ascending = False).head() $ top_com
p_old = df2.query('converted == "1"').count()[0]/df2.shape[0] $ old_page_convert = np.random.choice([0,1], n_old, p=(1-p_old, p_old))
columns = inspector.get_columns('stations') $ for c in columns: $     print(c['name'], c["type"]) $
view = 'top_five_genres' $ engine.execute(f'drop view if exists {view}') $ pd.read_sql_table(view, engine)
reddit2
injuries_hour['injuries']=injuries_hour['injuries'].fillna('0')
dfEtiquetas["city"] = dfEtiquetas["place"].apply(lambda p: p["location"]["city"] if "city" in p["location"].keys() else None)
dayofweek.count()
from sklearn.svm import SVR $ model = SVR(kernel='rbf', C=1e3, gamma=0.1) $ print ('SVR') $ reg_analysis(model,X_train_, X_test_, y_train_, y_test_)
ff3['Date'] = ff3['Date'].apply(lambda x: str(pd.to_datetime(str(x), format= '%Y%m'))[0:7])
print(ndarray_original[:2])
excel=pd.rea_excel("File Name")
IDX_train = df_train.iloc[0:,[0,2]].values $ IDX_train = IDX_train[0::sample_days] $ IDX_test = df_test.iloc[0:,[0,2]].values $ IDX_test = IDX_test[0::sample_days]
clinton_tweets = get_tweets_with_cache("HillaryClinton", "keys.json")
df_train = generator(df_train) # "Lake" of data for Training $ df_test = generator(df_test) # "Lake" of data for test to submit on Kaggle
df_andorra = df[df['Country'] =='Andorra'] $ df_andorra = df_andorra[['Indicator_id','Country','Year','WHO Region','Publication Status']] $ df_andorra = df_andorra.sort_values(['Indicator_id','Country','Year','WHO Region'],ascending=[False,False,True,False]).drop_duplicates(keep="first") $ df_andorra =df_andorra.reset_index(drop=True) $ df_andorra.head(3)
bonus_points.interpolate(axis=1)
plt.hist(p_diffs); $ plt.title('p_diffs simulated 10,000 times') $ plt.xlabel('p_new - p_old') $ plt.ylabel('Frequency')
df.head()
train['has_clicked'].describe()
routing_weights = tf.nn.softmax(raw_weights, dim=2, name="routing_weights")
s.groupby(['ID','group']).size().unstack().plot(kind='bar', stacked=True); $ plt.xlabel('Number of sms\'s') $ plt.ylabel('Number of people targeted') $ plt.title('Frequency of sms each person received');
%%time $ df.to_csv('losses.csv')
pk_mnth_cnsmr = cnsmr_lvl[cnsmr_lvl['LOA DATE'] == '2013-03-01'] $ pk_mnth_dstrb = dstrb_lvl[dstrb_lvl['LOA DATE'] == '2016-04-01'] $ pk_recall_months = pd.concat([pk_mnth_cnsmr, pk_mnth_dstrb]) $ pk_recall_months.groupby(['PRODUCT','PRIMARY VIOLATION','COUNTRY'])\ $                 .size().nlargest(len(pk_recall_months))
pd.options.mode.chained_assignment = None  # default='warn'
%matplotlib inline
people = pd.read_csv("../01-data/raw/people.csv", sep=';')
df2[(df2.group == 'treatment')].converted.mean()
neg_freq = {k: neg_tfidf.dfs.get(v) for v, k in neg_dic.id2token.items()} $ sorted(neg_freq.items(), key=lambda x: x[1], reverse=True)
morning_rush.iloc[:5]['longitude']
A or B
sentiment_df = pd.DataFrame(results_list) $ sentiment_df
df3['intercept'] = pd.Series(np.zeros(len(df3)), index=df3.index) $ df3['ab_page'] = pd.Series(np.zeros(len(df3)), index=df3.index)
df['water_year2'] = df['datetime'].apply(lambda x: x.year if x.month < 10 else x.year + 1)
import statsmodels.api as sm $ convert_old = df2[df2.group == 'control'].converted.sum() $ convert_new = df2[df2.group == 'treatment'].converted.sum() $ n_old = df2[df2.group == 'control'].converted.count() $ n_new = df2[df2.group == 'treatment'].converted.count()
Image(test_image) $ test_label = classify_image(test_image, model_128_id, display_image=True) $ test_label
with open('data/kochbar_01.json') as data_file:    $     kochbar01 = json.load(data_file) $ koch01df = preprocess(kochbar01) $ koch01df.info()
%matplotlib inline $ sns.violinplot(data=october, inner="box", orient = "h", bw=.03) $
qqqq = pd.merge(c,d , on =['msno','transaction_date']) $ qqqq
newdf['Date'] = newdf['Date'].apply(lambda x: pd.to_datetime(x, format= '%Y%m'))
print clean_text2[1:8]
def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides): $     conv_layer = tf.layers.conv2d(x_tensor, conv_num_outputs, kernel_size=conv_ksize, strides=conv_strides, activation=tf.nn.relu)   $     conv_layer = tf.layers.max_pooling2d(conv_layer, pool_size=pool_ksize, strides=pool_strides) $     return conv_layer $ tests.test_con_pool(conv2d_maxpool)
df2['intercept'] = 1 $ df2['ab_page'] = 0 $ df2.loc[(df2["group"] == "treatment"), "ab_page" ] = 1
tsla_tuna_neg = mapped.filter(lambda row: (row[3] < 0 and row[4] < 0)) $
df_c2[['CA', 'UK', 'US']] = pd.get_dummies(df_c2['country'])
train['dot_org'] = train.url.str.contains('.org', case=False, na=False, regex=False).astype(int) $ train.groupby('dot_org').popular.mean()
df.axes
top_features = [(key, value) for key, value in feature_set.items() if value >= 100]
result_df=pd.merge(df2,countries,on='user_id') $ result_df.head()
import numpy as np $ vectorized_body = np.load('body_vectors.npy') $ vectorized_title = np.load('title_vectors.npy')
pumaPop.head()
session.query(Measurement.station, func.count(Measurement.station)).\ $     group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()
sst.shape
lr = LinearRegression() $ lr.fit(train_data, train_labels)
%load "solutions/sol_2_44.py"
top_supporters = merged.groupby(["contributor_firstname", "contributor_lastname"]).amount.sum().reset_index().sort_values("amount", ascending=False).head(10)
aqmdata = pd.read_csv('data-new.csv') $ aqmdata.head()
type(ts.index)
fig, ax = plt.subplots(figsize=(14, 5)) $ ax.set(title='File Sizes', xlabel='File length in bytes', ylabel='Count') $ ax.hist(af.length, bins=range(0, af.length.max(), 100000)) $ ax.set_yscale('log')
T = 0 $ train = pd.read_csv('../../input/preprocessed_data/trainW-{0}.csv'.format(T)) $ df = pd.merge(df, tran_time_diff, on='msno', how='left').drop_duplicates(['msno','order_number']) $ df = df.dropna()
imgp
(autos["ad_created"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $ )
import matplotlib.pyplot as plt $ df[['Date','GasPrice']].set_index('Date').plot()
df.hist(column = 'founded_year',bins = 20)
import tweepy $ import hidden $ import sqlite3 $ from langdetect import detect
trgt_aux=[] $ for i in range(0,l2): $     if i not in seq: $         trgt_aux.append(target[i]) $ Y=np.array(trgt_aux)
cust_data.loc[:,'ID'].head(2)
frame2
print(corr)
data.info()
df_final['R'] = df_final['recency'].apply(LowValue, args=('recency',quantiles,)) $ df_final['F'] = df_final['frequency'].apply(HighValue, args=('frequency',quantiles,)) $ df_final['M'] = df_final['monetary'].apply(HighValue, args=('monetary',quantiles,)) $ df_final['P'] = df_final['total_profit'].apply(HighValue, args=('total_profit',quantiles,)) $ df_final['J'] = df_final['join_days'].apply(HighValue, args=('join_days',quantiles,))
df2.query('landing_page=="new_page"')['user_id'].count()
wqYear = dfWQ.groupby('Year')['TotalN'].mean() $ dfWQ_annual = pd.DataFrame(wqYear)
results = session.query(Measurement.date).order_by(Measurement.date.desc()).first() $ for row in results: $     print(f"The most recent date recorded is {row}.")
log_mod= sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ results= log_mod.fit()
from pandas import DataFrame, read_csv $ import matplotlib.pyplot as plt $ import pandas as pd $ import sys $ %matplotlib inline
customer_visitors_new.index = pd.MultiIndex.from_tuples([(x[0], list(calendar.day_name)[x[1]]) for x in customer_visitors_new.index]) $ customer_visitors_new
pd.date_range('2017-01', periods=16, freq='1H30T') 
distinct_user_ids = df2.nunique()['user_id'] $ print("Unique users are : {}".format(distinct_user_ids))
tweet_en.describe()
f = open("json_example.json","w") $ json.dump(obj, f) $ f.close()
american_test.iloc[2]['reviews_text']
df['pct_chg_opencls'] = ((df['dollar_chg_opencls']/df['open_price']))
output2 = top_hashtags(df_tweets,10) $ output2
import numpy as np $ extraRecordsForLag = pd.DataFrame({'ID': ['HeatPump', 'S Cmprsr', 'N Cmprsr', 'Chiller', 'Boiler'] * 720,'DATE':[np.nan, np.nan, np.nan, np.nan, np.nan] * 720, 'TEMP':[ np.nan, np.nan, np.nan, np.nan, np.nan] * 720,'X':[np.nan, np.nan, np.nan, np.nan, np.nan] * 720, 'Z':[ np.nan, np.nan, np.nan, np.nan, np.nan] * 720, 'Y':[ np.nan, np.nan, np.nan, np.nan, np.nan] * 720}) $ extraRecordsForLag = extraRecordsForLag[['ID','DATE','TEMP','X','Y','Z']] $ print extraRecordsForLag;
staff.describe()
Conversion_No=df2.loc[df2['landing_page']=="new_page",].shape[0] $ print("The no of observations regarding the new page equals",Conversion_No)
sns.set_color_codes("pastel") $ sns.barplot(x="p_pos", y="hashtag", data=analysis1, $             label="Total")
df.drop(['ORIGIN_STATE_ABR','ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID','DEST_STATE_ABR'],axis=1,inplace=True) $ df.drop(['Unnamed: 27','CARRIER_DELAY','WEATHER_DELAY','NAS_DELAY','SECURITY_DELAY','LATE_AIRCRAFT_DELAY'],axis=1,inplace=True)
from statsmodels.tsa.arima_model import ARIMA $ arima10 = ARIMA(dta_713,(1,0,0),freq='Q').fit() $ arima10.summary() $
print("Number of Relationships in Enterprise ATT&CK") $ print(len(all_enterprise['relationships'])) $ df = all_enterprise['relationships'] $ df = json_normalize(df) $ df.reindex(['id','relationship', 'source_object', 'target_object'], axis=1)[0:5]
raw.shape
beds = 1 $ for i in range(3): $     bed_df.set_value(i, 'bedrooms', beds) $     beds +=1 $ print(bed_df)
old_page_converted = np.random.choice(2,n_old,p=[0.8804,0.1196])
from h2o.estimators.gbm import H2OGradientBoostingEstimator $ gbm_model = H2OGradientBoostingEstimator(model_id="GBM", distribution = 'bernoulli') $ gbm_model.train(x = predictor_columns, y = target, training_frame = train, validation_frame = valid)
plt.hist(p_diffs); $ plt.ylabel('Frequency', fontsize=12); $ plt.xlabel('p_diffs', fontsize=12); $ plt.title('10,000 simulated p_diffs', fontsize=14);
autos["registration_year"].describe()
n_new=df2.query("landing_page =='new_page'").shape[0] $ n_new
np.isnan(StockData).sum().sum()
terror
words_sum = preproc_reviews.sum(axis=0) $ counts_per_word = list(zip(pipe_cv.get_feature_names(), words_sum.A1)) $ sorted(counts_per_word, key=lambda t: t[1], reverse=True)[:20]
import pandas as pd $ import codecs
image_predictions_df.tail(3)
s.index.freqstr
df2['intercept'] = 1 # add intercept $ df2[['control','treatment']] = pd.get_dummies(df2['group']) #create dummy variables from group $ df2['ab_page'] = df2['treatment'] # create ab_page column by duplicating treatment column $ df2 = df2.drop(['control','treatment'], axis=1) # drop original 'control' and 'treatment' dummy variables $ df2.sample(5)
hpdcom = pd.read_csv('../data/rawdata/Complaints20170201/Complaint20170131.csv', dtype = {'ComplaintID':str}, $                     parse_dates = ['ReceivedDate', 'StatusDate']) $ hpdcom.head(2)
new_page_converted = np.random.choice([0,1], size = n_new, p=[1-p_new, p_new]) $ new_page_converted.mean()
df1['Year'] = pd.to_datetime(pd.Series(df1['Year']).astype(int),format='%Y').dt.year  #converting the year to pandas date time $ df1.tail()
data.drop(['Colorado', 'Ohio'])
cityID = '01c060cf466c6ce3' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Long_Beach.append(tweet) 
old_p = df2['converted'].mean() $ old_p
complaints2016_filtered_cat_wlocations['geometry'] = ( $     complaints2016_filtered_cat_wlocations $     .apply(lambda z: Point(z["Longitude"], z["Latitude"]), axis = 1) $ )
tweet_archive_enhanced_clean = tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['retweeted_status_id'].isnull()] $ tweet_archive_enhanced_clean = tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['in_reply_to_status_id'].isnull()]
trios['855_trio1']
imageids = list(range(1, avg_preds.shape[0]+1)) $ print (type(imageids), len(imageids), imageids[0:5], imageids[-5:])
h3 = qb.History(spy.Symbol, datetime(2014,1,1), datetime.now(), Resolution.Daily)
d = df.query('group == "treatment" or landing_page == "new_page"')
gwt.ggSimp(1,0.0)
op_ed_articles.shape $
twitter_archive_full[ $     twitter_archive_full['stage'] != 'None'][ $     ['retweet_count','favorite_count'] $ ].describe()
data.shape
print(len(all_complaints)) $ all_complaints.head()
obj
complete_solar_df = pd.read_excel(weather_folder + '/compete_solar_df.xlsx') $ complete_wind_df = pd.read_excel(weather_folder + '/compete_wind_df.xlsx') $ solar_wind_df = pd.read_excel(weather_folder + '/complete_solar_wind.xlsx')
import numpy as np; np.random.seed(8) $ mean, cov = [4, 6], [(1.5, .7), (.7, 1)] $ x, y = np.random.multivariate_normal(mean, cov, 80).T $ ax = sb.regplot(x=x, y=y, color="g")
df[df['Descriptor'] == 'Pothole']['Unique Key'].groupby(df[df['Descriptor'] == 'Pothole'].index.hour).count().plot()
df_country.shape
news_df['positive_ratio'] = ((news_df['num_hahas'] + $                                          news_df['num_loves'] + $                                          news_df['num_thankfuls']) / (news_df['num_reactions'] - news_df['num_likes'])) $ news_df['negative_ratio'] = (news_df['num_sads'] + news_df['num_angries']) / (news_df['num_reactions'] - news_df['num_likes']) $ news_df['wow_ratio'] = news_df['num_wows'] / (news_df['num_reactions'] - news_df['num_likes'])
con_df = pd.read_csv('.\countries.csv') $ con_df.head()
Z = np.linspace(0,1,11,endpoint=False)[1:] $ print(Z)
GroupZip = df.groupby(by = ['Zip Code']).mean()
all_sf_zip_codes = ["94102", "94103", "94104", "94105", "94107", "94108", "94109", "94110", "94111", "94112", "94114", "94115", "94116", "94117", "94118", "94119", "94120", "94121", "94122", "94123", "94124", "94125", "94126", "94127", "94128", "94129", "94130", "94131", "94132", "94133", "94134", "94137", "94139", "94140", "94141", "94142", "94143", "94144", "94145", "94146", "94147", "94151", "94158", "94159", "94160", "94161", "94163", "94164", "94172", "94177", "94188"]
input_node_types_DF = pd.read_csv(input_models_file, sep = ' ') $ input_node_types_DF
station_df.head(10)
logit_mod2 = sm.Logit(df_ab_cntry['converted'], df_ab_page[['intercept','US_ind_ab_page','CA_ind_ab_page']]) $ results2 = logit_mod2.fit()
p_diffs
type(cig_data_SeriesCO)
plt.hist(p_diffs, bins=21) $ plt.axvline(p_convert_obs_diff, color='green') $ plt.axvline(np.percentile(p_diffs, 95), color='red');
df2_treatment = df2.query('group == "treatment"') $ df2_treatment.converted.mean()
tvec = TfidfVectorizer(stop_words='english') $ X_train_matrix = tvec.fit_transform(X_train['title']) $ X_test_matrix = tvec.transform(X_test['title'])
autos['registration_year'].describe()
df.head()
g_test = data_test[data_test['isvalid']>0].groupby('customer').apply(lambda x: x['isvalid'].sum()).reset_index() $ g_test.columns = ['customer','validOrders'] $ p_test = data_test[data_test['isvalid']>0].groupby('customer')['date'].agg({"first": lambda x: x.min(),"last": lambda x: x.max()}).reset_index()
tweets_kyoto.describe()
n_net2.score(x_test,y_test)
pd.read_sql('desc actor', engine)
reviews.groupby('taster_name').points.mean()
print('With KNN (K=) accuracy is: ',knn.score(x_test,y_test)) $
p_value_obs_lt = (p_diffs < obs_p_diff).mean() $ _, p_value_ztest_lt = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='smaller') $ p_value_obs_lt, p_value_ztest_lt
pd.eval?
tweet_archive_enhanced_clean['rating_numerator'].sum()
learn.save("dnn10")
from biopandas.mol2 import PandasMol2 $ import pandas as pd $ pd.set_option('display.width', 600) $ pd.set_option('display.max_columns', 8)
Combined_data = Combined_data.fillna('unknown') $ Combined_data = Combined_data.set_index('Name') $ Combined_data
fname_messages = 'messages.csv' $ messages = load_messages(fname_messages) # messages data $ messages.head()
df
solar_df
X["inactivePeriod"] = endDate - X['recentTime'] $ X["inactivePeriod"] = X.inactivePeriod.dt.days $ X["memberPeriod"] = startDate - X["createdtm"] $ X["memberPeriod"] = X.memberPeriod.dt.days $ X.head(5)
closes.plot(figsize=(8,6));
jobPostDF['month'] = jobPostDF['date'].apply(lambda x: x.month)
data.sort_index(inplace=True) $ data.head(5)
df_new[['CA', 'UK', 'US' ]] = pd.get_dummies(df_new['country']) $ model2 = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']]) $ model2.fit().summary() $
crimes['date']=crimes['Date'].dt.date $ crimes['date']=crimes['date'].astype(str) $ crimes.head()
dog_stage_stats = tweet_archive_master[['dog_stage', 'rating_numerator', 'favorite_count', 'retweet_count']].groupby('dog_stage').mean() $
log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ result = log_mod.fit()
res.summary2()
data = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key="+API_KEY)
!h5ls -r 'data/my_pytables_file.h5'
import pandas as pd $ %matplotlib inline $ import matplotlib.cm as cm $ import numpy as np $ dotslog_c, dots_c, line_c, *_ = cm.Paired.colors
len(RatingSampledf)
old_page_converted = np.random.choice([0,1],size=n_old,p= [1-p_old,p_old])
newdf.head()
elnet2 = ElasticNet(alpha=0.0001, l1_ratio=0.25) $ elnet2.fit(train_data, train_labels)
import random $ sample=movie1['ceiled_ratings'].tolist() $ x = np.linspace(1,5,100)
df_new[['UK', 'US']] = pd.get_dummies(df_new['country'])[['UK', 'US']] $ lm = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US']]) $ results = lm.fit() $ results.summary()
df_goog.index + timedelta(days=4380, hours=1, seconds=43)
df2[df2['user_id'] == 773192]
df2.reset_index(inplace=True);
new_page_converted = np.random.binomial(1, treatment_cnv, treatment_num) $ new_page_converted.mean()
def _weekday(date): $     return datetime.strptime(date, DATETIME_PARSE_PATTERN).weekday() $ weekday2 = udf(_weekday, IntegerType())
cluster_label_group.ix[:,0:21]
name_dataframe = pd.DataFrame(autos["name"].str.split("_", expand = True)) $ name_dataframe
fullDF = clean_tweets(fullDF)
number = df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0] $ number
url = 'https://twitter.com/marswxreport?lang=en' $ response = requests.get(url) $ soup = bs(response.text, 'lxml')
y.shape
DF.show()
corpus[corpus['Stem'] == 'cannibal']
import gensim 
import statsmodels.api as sm $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = len(df2.query("group == 'control'")) $ n_new = len(df2.query("group == 'treatment'")) $
archive_copy.loc[archive_copy['name'] == 'None']
access_token = '' $ client_secret = ''
df_likes = df_comment.drop('liked_by', axis=1)[['author_id']].join(s).dropna().reset_index()
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, (1 - p_new)]) $
df = pd.read_sql_query('SELECT * FROM data LIMIT 3', disk_engine) $ df.head()
investors.loc[0]
tweet_url = 'http://nodeassets.nbcnews.com/russian-twitter-trolls/tweets.csv' $ df_tweets = pd.read_csv(tweet_url, dtype={'id' : str, 'user_id' : str, 'tweet_id' : str}) $ df_tweets.drop_duplicates(subset=['tweet_id'], inplace=True) $ print("N = {}".format(len(df_tweets))) $ df_tweets.head(3)
df_reg2['US_ind_ab_page'] = df_reg2['country_US']*df_reg2['ab_page'] $ df_reg2['UK_ind_ab_page'] = df_reg2['country_UK']*df_reg2['ab_page'] $ logit_co_2 = sm.Logit(df_reg2['converted'], df_reg2[['intercept', 'ab_page', 'country_US', 'country_UK', 'US_ind_ab_page', 'UK_ind_ab_page']]) $ result_co_2 = logit_co_2.fit()
gpCreditCard.Trip_distance.describe()
nnew = df2[df2["landing_page"] == "new_page"].count() $ nnew = nnew[0] $ nnew
image_df_clean.sample(10)
pn_qty[pn]['storeinfo'].append(table_store.ix[0])
train_cats(train_data) $ apply_cats(test_data,train_data)
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)]) $ old_page_converted.mean()
df = df4[df4.day == days[0]]
train_history=model.fit(Xcnn, y_label_train_OneHot, $                         validation_split=0.2, $                         epochs=8, batch_size=20, verbose=1)       
df['created_date'] = pd.to_datetime(df['created_date']) $ df.head()
followers = api.GetFollowers() $ print([u.screen_name for u in followers])
perc_df.map(two_digits) $
df2.query('landing_page=="new_page"').count()/df2.shape[0]
df['id'] = df['id'].astype('category') $ df['sentiment'] = df['sentiment'].astype('category') $ df['created_at'] = pd.to_datetime(df['created_at'])
df_full['school_type'] = df_full['school_type'].map(DATA_L1_HDR_DICT)
sns.barplot(x='Total_Num_Comments', y='Subreddit', orient='h', data=subred_num_tot[:10])
df.loc[:,['A','C']]
eth_market_info.drop(['Date'],inplace=True,axis=1) $ scaler_eth = MinMaxScaler(feature_range=(0, 1)) $ scaled_eth = scaler_eth.fit_transform(eth_market_info) $
sum(pred.duplicated())
from pyspark import SparkContext $ from pyspark.sql import SQLContext $ from pyspark.sql.functions import * $ print (pyspark.__version__)
airbnb_df['room_type'] = airbnb_df['room_type'].astype('category')
dedup = dedup.drop_duplicates(subset='hash');
aml.leaderboard
discounts_table.Country.unique()
tfav_rf = pd.Series(data=data_rf['Likes'].values, index=data_rf['Date']) $ tret_rf = pd.Series(data=data_rf['RTs'].values, index=data_rf['Date']) $ tfav_rf.plot(figsize=(16,4), label="Likes", legend=True) $ tret_rf.plot(figsize=(16,4), label="Retweets", legend=True);
df['operator'].unique()
df98 = pd.read_csv('1998.csv')
block_populations = numpy.load('columncache/census2010_block2010/p001001.numpy') $ print 'block_populations has', sum(block_populations), 'total people'
df2['intercept'] = 1 $ df2['ab_page'] = df2['group'].replace(('treatment', 'control'), (1, 0))
groupedDf = joined_df.groupby(['id', 'churned', 'feature_name']) $ groupedDf.mean().head()
n_new = df2[df2.group=='treatment']['converted'].shape[0] $ n_new
autos['registration_year'].describe()
sf_subset = merged_data[merged_data['start_city'] == "San Francisco"]
y = X.sold $ del X['sold']
print(X_train.shape) $ print(y_train.shape) $ print(X_test.shape) $ print(y_test.shape)
import wget $ results=50 $ url = 'https://api.thingspeak.com/channels/489412/feeds.csv?results='+str(results) $ fl = wget.download(url) $ fl
new_cases.columns = ["Date", "Total_new_cases_guinea", "country"]
log_US = sm.Logit(new['converted'],new[['intercept','country_US']]) $ r = log_US.fit() $ r.summary()
df_twitter.info()
df['AQI Category'] = df['AQI Category'].astype('category') $ df['AQI Category'].cat.categories
slicer = df.iloc[:, 0:5] $ slicer
autos["odometer_km"].value_counts() $
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ act_diff
for row in reversed(station_activity):                                       # extracts station with most activity $     most_active_station=row.station $     most_active_name=str(row.name)
sess.run(embeddings)
r = yfs.session.get(url, params={'format': 'json'}) $ r.status_code
df['body_tokens'] = df['body_tokens'].apply(nltk.word_tokenize) $ print(df['body_tokens'])
clean_train_df = pd.DataFrame(parse_data(train_df['text'].tolist()))
len(botoresult)
import tensorflow as tf $ import apache_beam as beam $ import shutil $ print(tf.__version__)
active_df.head()
len(tract_block_indexes.keys())
wrd_clean['expanded_urls'].str.contains('https://gofundme.com/').sum()
n_old = df2.query(('landing_page == "old_page"')).count()[0] $ n_old
cityID = 'd5dbaf62e7106dc4' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Jacksonville.append(tweet) 
submission2[['proba']].median()
training.head()
df_gt[[u'Equipment Type', u'Equipment Sap Code']].head()
jobs['GPU'] = (~jobs.ReqGRES.isna()).apply(int)
import numpy as np $ len((secondDataFrame1['Rating_m'] == secondDataFrame1['Rating_m']) == True) $ len(secondDataFrame1['Rating_m']) $ for i in secondDataFrame1['Rating_m']: $     print(i)
pd.Series([1,2,9])
candidate_df[['name', 'office', 'state', 'party_full']]
contractor_clean[contractor_clean.city.isnull()] # The result is empty. $ contractor_clean.loc[contractor_clean['contractor_id'] == 139] $
change = [price_dict['Close'][i] - price_dict['Close'][i-1] for i in range(1,Len)] $ max_change = round(max(change), 2) $ print('The largest change between any two days (based on Closing Price) is: ' + str(max_change))
import  psycopg2 $ import psycopg2.extras $ import sys $ import pprint $ import pandas as pd
new_converted_simulation.mean() - old_converted_simulation.mean()
impressions = pd.read_sql_query("SELECT * from impressions"+RetSqlLimit("impressions",sqlLimit), conn) $ impressions_products = pd.read_sql_query("SELECT * from impressions_products"+RetSqlLimit("impressions",sqlLimit), conn) $ impressions['group'] = impressions['ab'].apply(lambda x: x[:1]) $ impressions['session'] = impressions['ab'].apply(lambda x: x[2:])
data['subreddit'].value_counts()
len(_dict_word_count)
s = pd.Series([1,3,5,np.nan,6,7,8], index=dates).shift(2) $ s
print('The maximum opening price was: ' + '${:,.2f}'.format(max([sublist[1] for sublist in inputlist]))) $ print('The minimum opening price was: ' + '${:,.2f}'.format(min([sublist[1] for sublist in inputlist])))
bwd.head(10)
fig = plt.figure(figsize=(12,8)) $ ax = fig.add_subplot(111) $ fig = qqplot(resid_713, line='q', ax=ax, fit=True)
stories.head()
transit_df['DELEXITS']= transit_df['EXITS'].diff() $
max(change)
def make_snakecase(text): $     remove_spaces = re.sub("\s+", "_", text) $     return re.sub('([a-z0-9])([A-Z])', r'\1_\2', remove_spaces).lower() $ raw.columns = map(make_snakecase, raw.columns) $ raw.columns
min_max_avg = session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).all() $ min_max_avg
temp = dfall $ temp.tweet_time = temp.tweet_time.dt.date
weather.Date = weather.Date.apply(lambda x:x.month)
number_unpaids[number_unpaids['paid_status'] > 1].size / n_customers
n_new = len(df2[df2['landing_page'] == 'new_page']) $ print ("n_new is: {}".format(n_new))
treatment_num = df2.query('group=="treatment"').shape[0] $ treatment_num
merged_311_data.groupby("Category")['Category'].count().sort_values(ascending=False)
trans_data = pd.read_excel("/home/x7/Desktop/projxdata/01_superstore_data__trans_cg6.xlsx", sheetname=0)
!wget -O cell_samples.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/cell_samples.csv
valid_start_dt = '2014-09-01 00:00:00' $ test_start_dt = '2014-11-01 00:00:00' $ T = 6 $ HORIZON = 3
omega = numpy.logspace(-3, -2) $ s = omega*1j
df_archive_clean.rating_denominator.value_counts()
final_topbikes.groupby(by=final_topbikes.index.weekday)['id'].count().plot(kind='bar', figsize=(6,3))
df_2009 = pd.DataFrame(rows)
X = df[['word_count','sentiment','subjectivity','domain_d','post_duration','title','text']] $ le=LabelEncoder() $ y_cm = le.fit_transform(df['com_label'])#Comments $ y_sc = le.fit_transform(df['score_label'])# Score $
rawautodf.head()
observed_mean_control = df2.query('group=="control"')['converted'].mean() $ print(observed_mean_control)
path_list
norm.ppf(1-(0.05/2))
key, secret = open('api_key.txt').read().splitlines()
autos = autos[autos["registration_year"].between(1900, 2016)]
df_new = df_new.join(pd.get_dummies(df_new['group']))
actual_diff = treatment[treatment['converted'] == 1].shape[0] / treatment.shape[0] - control[control['converted'] == 1].shape[0] / control.shape[0] $ null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size) $ plt.hist(null_vals); $ plt.axvline(x=actual_diff, color='r');
knn.fit(X_train, y_train)
precip_data_df1.plot(x="date",y="Precipitation",kind="line",ax=None,legend=True, $                      title="Hawaii - Date vs precipitation ") $ plt.savefig("Images\datevsprecip_vsk.png") $ plt.show()
taxi_hourly_df = taxi_hourly_df.reindex(dateindex)
df_mes = df_mes[df_mes['improvement_surcharge']>=0] $ df_mes.shape[0]
sb.regplot(y_train, y_pred, fit_reg=False) $ plt.xlabel('y_train') $ plt.ylabel('y_pred')
import pandas as pd $ df1 = pd.read_stata('https://github.com/QuantEcon/QuantEcon.lectures.code/raw/master/ols/maketable1.dta') $ df1.head()
femalebyphase = femalemoon.groupby(['Moon Phase']).sum().reset_index() $ femalebyphase
session.query(Measurement.station, func.count(Measurement.station)).\ $ group_by(Measurement.station).\ $ order_by(func.count(Measurement.station).desc()).all() $
df['intercept']=1 $ df[['control', 'treatment']] = pd.get_dummies(df['group']) $ df['ab_page'] = df['treatment'] $ df = df.drop(['control', 'treatment'], axis=1) $ df.head()
k_means.fit(X)
tsne = TSNE(n_components=2, learning_rate=150, verbose=2).fit_transform(one_hot_domains_questionable)
pd.concat([msftAV, aaplA], join='inner')
np.random.binomial(size_treatment, converted_prob, 10000).mean()
X_train.shape, X_test.shape
ffr2008 = ffr["2008"] $ print("ffr2008 is a", type(ffr2008)) $ df_info(ffr2008)
df.gender.value_counts().plot(kind='bar');
df_tweet_data.describe()
stops = stopwords.words('english') $ more = "united states whereas section country subsection shall act paragraph countries ii usc aa aaaa aaiiiiiaabb ag bb aab aai aaiii aaii aaiiiii ab iii iv b c d e".split() $ stops += more
own_star['rating'] = own_star[['starred','owned']].sum(axis=1) $ own_star.head(20)
wikipedia_meritocracy = 'https://en.wikipedia.org/wiki/Meritocracy' $ meritocracy_save = 'wikipedia_meritocracy.html'
import numpy as np $ uncjobs = jobs[np.isnan(jobs['cleaner_id'])][['id']] $ pd.merge(uncjobs, occurrences, left_on='id', right_on='job_id')
price_ceiling_to_ignore = 100 $ price_floor_to_ignore = 10 $ meal_ids_to_ignore_because_price_outlier = X[(X.ticket_price < price_floor_to_ignore) | (X.ticket_price > price_ceiling_to_ignore)].meal_id.unique() $ X = X[~X['meal_id'].isin(meal_ids_to_ignore_because_price_outlier)]
comments = df.groupby(['subreddit'])['body'].count()
x_train_cat = snowshoe_df[msk] $ snowshoe_prob = x_train_cat[['month','snowshoes']].groupby(['month']).mean() $ test_month_list = snowshoe_df.month[~msk].tolist() $ y_pred_baseline = snowshoe_prob.snowshoes[test_month_list]
import numpy as np $ import pandas as pd $ autos = pd.read_csv("autos.csv", encoding="Latin-1")
from sklearn import metrics
df2.user_id.duplicated().sum()
print(len(tweets)) $ tweets.head()
import pandas as pd
unique_domains.sort_values('total_payout', ascending=False).head()
y = np.array(df['label'])
sorted_budget_cheapest = df.sort_values(by=['budget_adj'], ascending = True).head(200)
missing_tweets.code.value_counts()
ks_categories = ks_projects.groupby(["main_category", "state"]).size().reset_index(name='counts') $ ks_cat_success = ks_categories.drop(ks_categories.index[ks_categories.state != 'successful']) $ ks_cat_success.set_index('main_category', inplace=True) $ ks_cat_success $
park['named_drug'] = park.abstract.apply(named_drug)
stacked_image_predictions.drop('is_dog', axis=1, inplace=True) $ stacked_image_predictions.head()
iris = sns.load_dataset("iris") $ iris.head()
scores = unpack_grid_scores(gs.grid_scores_) $ scores
from sklearn.feature_selection import SelectFromModel $ from sklearn import pipeline $
archive.rating_denominator.value_counts()
X = df_modeling_categorized.drop('final_status', axis=1) $ y = df_modeling_categorized['final_status']
cell = openmc.Cell(cell_id=1, name='cell') $ cell.region = +min_x & -max_x & +min_y & -max_y $ cell.fill = inf_medium
data = data.reset_index()
path =r'Data/ebola/liberia_data' $ liberia =read_data( path, 'Variable', 'liberia')
data = scale(df_select) $ noOfClusters = 4 $ model = KMeans(init='k-means++', n_clusters=noOfClusters, n_init=20).fit(data)
stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' '))) $ lbl = LabelEncoder() $ for i in range(10): $     stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else '')) $ stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ z_score, p_value
P.plot_1d_hru(0,'airtemp')
csvData = pd.read_csv('data_s1_ass4.csv')
quiet = open("quiet.json").read() $ print(type(quiet),"\n") $ print(quiet)
df.to_csv(file_name, index=False,header=False)
unhappy_teams = df[(df['wins'] > (df['loses'] + 20)) & (df['rank'] > 1)] $ unhappy_teams.sort_values(by=['wins'], ascending=False).head(10)
coinbase_btc_eur_min=coinbase_btc_eur.groupby('Timestamp', as_index=False).agg({'Coin_price_EUR':'mean', 'Coin_volume':'sum'})
df_archive_clean.head()
old_page_converted = np.random.choice([0,1], size = n_old, p = [1-p_old, p_old]) $ old_page_converted.mean() $ print(old_page_converted)
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == 1').shape[0] $ convert_new = df2.query('landing_page == "new_page" and converted == 1').shape[0] $ n_old = df2.query('landing_page == "old_page"').shape[0] $ n_new = df2.query('landing_page == "new_page"').shape[0]
!wget -O ChurnData.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/ChurnData.csv
calls_nocontact.ticket_status.value_counts()
data_libraries_df = pd.merge(left=libraries_df, right=tmp, on="asset_name", how="outer")
dfg.describe()
daily_averages.head()
lst_columns= list(df.columns) $ lst_columns.pop(lst_columns.index('Country')) $ lst_columns.insert(0,'Country') $ df[lst_columns].head(5)
from matplotlib.dates import MonthLocator, WeekdayLocator, DateFormatter $ %pylab inline $ pylab.rcParams['figure.figsize'] = (15, 9) $
fix_space = lambda x: pd.Series([i for i in reversed(x.split(' '))])
full_dataset.to_excel('rolling_dataset.xlsx')
games_to_bet.drop(labels=[x for x in games_to_bet.index if x%2], inplace=True)
df.to_html('table.html')
autos["price"] = autos["price"].str.replace(",","").str.replace("$","").astype(int) $ autos.rename({"price":"price_dollars"},axis=1,inplace=True) $ autos["odometer"]=autos["odometer"].str.replace(",","").str.replace("km","").astype(int) $ autos.rename({"odometer":"odometer_km"},axis=1,inplace=True)
births = births.query('(births > @mu - 5 * @sig) & (births < @mu + 5 * @sig)')
df["text"] = df["text"].apply(lambda x: normalise(x).replace("\n", " "))
ab_file2 = pd.read_csv('ab_edited.csv')
todaydate = datetime.now() $ oneyearback = todaydate.replace(year=todaydate.year-1).strftime("%Y-%m-%d") $ oneyearback
to_be_predicted_Day1 = 21.37 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
X_train, X_test, y_train, y_test = train_test_split(X, $                                                     y, $                                                     test_size=0.3, $                                                     random_state=42)
to_be_predicted_Day1 = 43.22 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
labels.shape
np.count_nonzero(np.any(nba_df.isnull(), axis = 1))
law.to_csv(folder + "\\" + "law-all.txt", sep="\t", index=False)
All_tweet_data_v2.name[(All_tweet_data_v2.name.str.len() < 3) & (All_tweet_data_v2.name.str.contains('^[(a-z)]'))]
%matplotlib notebook $ import matplotlib.pyplot as plt
rng.asi8[0]
learner.save('lm1')
sns.factorplot(x='Year', data=affordability, y='Ratio')
autos['kilometer'].unique().shape[0]
local.export_to_quilt(post_process_info["DatasetId"])
autos["date_crawled"].str[:10].value_counts(normalize=True, dropna=False).sort_index().plot(kind="bar", title="Date_crawled", colormap="Blues_r")
df['subject'].value_counts()
aux.shape
path = 'C:\\Users\\csell\\OneDrive for Business\\Projects\\Active\\{}\\info\\'.format(project_name) $ fig.savefig(path+'metrics.svg', transparent=False, dpi=80, bbox_inches='tight') $
final_topbike.head(0)
df.groupby('userid')['price'].sum()
df_train = pd.merge(train, store, left_on='Store', right_on='Store') $ df_test = pd.merge(test, store, left_on='Store', right_on='Store')
ts['2014-08-02']
from gensim.models import word2vec as word2vec $ models = gensim.models.Word2Vec(nltk_Tokenize, size=32, window=5, min_count=1, workers=4)
df_goog[['Open','Close','High','Low','Adj Close']].plot()
import pandas as pd $ df = pd.read_csv('/data/measurements/C47C8D65CB0F.csv', $                  names=['time', 'moisture', 'temperature', 'conductivity', 'light']) $ print(df.to_string())
nocol['day'] = ser $ nocol
df.head()
(df.groupby('episode_id')['number'].max() + 1).head()
df_tweet_clean=df_tweet.copy() $
df.nd_key_formatted.unique()
print("x es una {}".format(type(x)) , "y es un {}".format( type(y)))
github_data = pd.concat([pd.read_csv(filename) for filename in glob.glob('../data/raw/github_data/*')])
possible_features = [val for val in X.columns if val in X_unseen.columns] $ possible_features
f_counts_week_channel = spark.read.csv(os.path.join(mungepath, "f_counts_week_channel"), header=True) $ print('Found %d observations.' %f_counts_week_channel.count())
%%time $ M_NB_model = MultinomialNB()
focount.columns = ['name', 'weight']
archive_clean[(archive_clean['rating_numerator'] == 0) | (archive_clean['rating_numerator'] == 1)].tweet_id
df_more[df_more.Senior != 0]
driver = webdriver.Chrome(executable_path='./chromedriver') $ driver.get(URL) $ html = driver.page_source $ soup = BeautifulSoup(html, 'lxml') $ html[:2500]
tweet_clean.columns
Nnew = df2.query('group == "treatment"')['user_id'].nunique() $ print (Nnew)
to_be_predicted_Day4 = 21.37347759 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
n_old = (df2[df2['landing_page']=='old_page']).count()[0] $ n_old
pd.Timestamp('9/2/2016 8:10AM') + pd.Timedelta('12D 3H')
df = pd.read_csv('tweet_json.txt', encoding = 'utf-8') $ df.set_index('tweet_id', inplace = True) $ df.tail()
import sys $ print('\n'.join(sys.path))
tweet = "Wed Aug 27 13:08:45 +0000 2008" $ time_info = datetime.strptime(tweet,'%a %b %d %H:%M:%S +%f %Y') $ print('Day of week: ', time_info.strftime("%a")) $ print('Hour: ', time_info.strftime("%H")) $ print('Year: ', time_info.strftime("%Y"))
pd.set_option('display.max_colwidth', -1) $ sqlClient.get_jobs().head(100)
import pandas as pd $ import numpy as np $ np.random.seed(2018)
from pyspark.sql.functions import log $ df_input_clean = df_input_clean.withColumn("log_Resp_time", log(df_input_clean.Resp_time))
p_new = sum(df2.converted == 1) / 290584 $ p_new
from sagemaker.predictor import csv_serializer, json_deserializer $ linear_predictor.content_type = 'text/csv' $ linear_predictor.serializer = csv_serializer $ linear_predictor.deserializer = json_deserializer
cp311.head(2)
globalCityBytes = io.BytesIO(globalCityRequest.content) $ print(readPDF(globalCityBytes)[:550])
to_be_predicted_Day2 = 31.22544606 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
b == a
columns = ['bedrooms', '1 bathroom', '2 bathrooms', '3 bathrooms'] $ bed_df = pd.DataFrame( columns=columns) $ bed_df = bed_df.fillna(0) $ print(bed_df)
loans_df.shape
m3 = np.matmul(m2.T,m) $ print("m3: ", m3)
new_albums.head()
df2[df2['group']=='control']['converted'].mean() $
tokenizer = RegexpTokenizer(r'[a-zA-Z0-9_@-]+') $ lemmatizer = WordNetLemmatizer()
df.landing_page.unique()
t0 = t0.reshape((4,1))
surveys_df = pd.concat([surveys2001_df, surveys2002_df], axis=0, sort=False) $ surveys_df = surveys_df.reset_index(drop=True)
data = ['peter', 'Paul', None, 'MARY', 'gUIDO'] $ for s in data: $     print(s.capitalize())
train_df = pd.read_json("../Kaggle Files/train.json") $ test_df = pd.read_json("../Kaggle Files/test.json") $ train_df = train_df.set_index('listing_id') $ test_df = test_df.set_index('listing_id') $
plot = sb.regplot(x='Price', y='Volume', data=dfEPEXbase, fit_reg=True) $ plot.set_title('Relationship between Price and Volume')
import re
print("Number of Mitigations in Mobile ATT&CK") $ mitigations = lift.get_all_mobile_mitigations() $ print(len(mitigations)) $ df = json_normalize(mitigations) $ df.reindex(['matrix', 'mitigation', 'mitigation_description', 'url'], axis=1)[0:5]
p_diff = p_actual_new - p_actual_old $ p_diff
df = pd.read_csv('Pro3ExpFinal0602.csv',index_col ='Unnamed: 0' , engine='python')
plt.figure(figsize=(15,8)) $ ax = sns.boxplot(x = 'Type 1', y = 'Total', data = pokemon ) $ plt.show()
s1 = pd.Series(np.arange(1, 6, 1)) $ s2 = pd.Series(np.arange(6, 11, 1)) $ pd.DataFrame({'c1': s1, 'c2': s2})
err = pd.DataFrame() $ for subject in etsamples.subject.unique(): $     err = pd.concat([err,CALIBRATION.pl_accuracy(subject),CALIBRATION.el_accuracy(subject)],ignore_index=True) $ err.loc[:,'avg'] = err.avg.astype(float) $ err.loc[:,'msg_time'] = err.msg_time.astype(float) 
df2['intercept'] = 1 $ df2[['control','treatment']] = pd.get_dummies(df2['group'])
street_freq = train.groupby("Block")['Block'].count() $ train = train.join(street_freq, on="Block", rsuffix='_fre')
df_new[['CA','UK', 'US']] = pd.get_dummies(df_new['country'])
pokemon.drop(["id"],inplace=True,axis=1) $ pokemon.drop(["Type 2"],inplace=True,axis=1) $ pokemon.head()
popt_axial_brace_crown, pcov_axial_brace_crown = fit(d_axial_brace_crown)
df_uro_no_cat.tail()
df_site.head()
findNumber = r'\d' $ regexResults = re.search(findNumber, 'not a number, not a number, numbers 2134567890, not a number') $ regexResults
with ZipFile('{0}.zip'.format(datapath / zipfile), 'r') as myzip: $     myzip.extractall(datapath)
df_us[df_us['ab_page'] == 0]['converted'].mean()
logreg = LogisticRegression(penalty='l1', solver='liblinear') $ y_pred = cross_validation.cross_val_predict(logreg, X, y, cv=5) $ print(metrics.accuracy_score(y, y_pred))
merge_email.loc[(merge_email.language == 'english') & (merge_email.action == "email_clickthrough")].groupby('nweek')['user_id'].nunique().plot(label="english") $ merge_email.loc[(merge_email.language != 'english') & (merge_email.action == "email_clickthrough")].groupby('nweek')['user_id'].nunique().plot(label="non-english") $ plt.legend() $ plt.show() $
w = 'zionist' $ model.wv.most_similar (positive = w)
my_model_q3_proba = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='probability') $ my_model_q3_proba.fit(X_train, y_train) $ my_model_q3_proba.stackData.head()
autodf.info()
r.json()['dataset_data']['data']
tst_lat_lon_df.tail() ## final records in the input
data[data['Processing Time']<datetime.timedelta(0,0,0)]
def printer(date_time): $     return date_time.weekday()
baseball.player.describe()
pd.options.display.max_colwidth = 1000 $ it_df[["buyers", "buyer_name","bidDeadline", "finalPrice", "publicationDate", "price", "title"]]
y_train.value_counts()
data.drop(['ceil_15min'], axis=1,inplace=True)
y_error = y-y_predicted
jobs.loc[jobs.FAIRSHARE == 3].groupby('ReqCPUS').JobID.count().sort_values(ascending= False)
svm_classifier.estimator #estimator svm
df2 = df2.drop(index=[1899], axis = 0)
df[df.num_shares == max(trumpint.num_shares)]
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
autos = autos[autos["registration_year"].between(1910,2016)] $ autos["registration_year"].value_counts(normalize=True).head(30)
p_treat = df2.query('group=="treatment"').converted.mean() $ p_treat
temp_df.head()
stories = pd.concat([stories.drop(['submitter_user'], axis=1), $                      user_df], axis=1)
mean = np.mean(data['len']) $ m = "{0:.4g}".format(mean) $ print("{} has an average tweet length of {} characters\n".format(target_user,m))
print(arma_mod20.aic, arma_mod20.bic, arma_mod20.hqic) $ print(arma_mod30.aic, arma_mod30.bic, arma_mod30.hqic) $ print(arma_mod40.aic, arma_mod40.bic, arma_mod40.hqic) $ print(arma_mod50.aic, arma_mod50.bic, arma_mod50.hqic)
tweet_archive_clean.stage.unique()
real_diff = df2.query('group == "control"').converted.sum()/df2.query('group == "control"')['user_id'].count() - df2.query('group == "treatment"').converted.sum()/df2.query('group == "treatment"').user_id.nunique() $ real_diff
tweets1 = pd.concat([una_tweets, amanda_tweets], axis=0) $ tweets1.shape
df = df[['Country', 'Indicator_id', 'Publication Status', 'Year', 'WHO Region', 'World Bank income group', 'Sex', 'Display Value', 'Numeric','Low', 'High', 'Comments']] $ df.head()
train_cols = ["Bachelor's degree","High school degree","Master's degree","PhD" $               ,"General interest in the topic (personal growth and enrichment)","Grow skills for my current role" $               ,"Help move from academia to industry","Help prepare for an advanced degree" $               ,"Start a new career in this field"] $ logit = sm.Logit(data['submitted'],data[train_cols])
test_dum_clean.head()
tweet_data.sample(20)
df.info(memory_usage='deep')
results=session.query(Measurements.Date,Measurements.Prcp).\ $         filter(Measurements.Date>=start_date, Measurements.Date<=end_date).\ $         order_by(Measurements.Date.desc()).all() $ Precipitation_DF=pd.DataFrame(results) $ Precipitation_DF.set_index("Date",inplace=True)
df_users.last_session_creation_time.isnull().values.ravel().sum()
MATTHEWKW.head().text.values
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, 1-p_new]) $ new_page_converted
epoch3_df = pd.read_pickle('iteration1_files/epoch3/epoch3_df.pkl')
beta_dist[np.arange(mcmc_iters), betas_argmax] *= 10000 $ sum(beta_dist)/1000
logistic_model2=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA','UK']])
ls -l *.R
plt.figure(2) $ goog_plot = plt.subplot() $ goog_plot.plot(goog['Close'],color='blue') $ plt.legend(['Google Close Value'],loc="upper left") $ plt.title('Valores de Cierre Google')
goog.sort_values(by='Date', inplace=True)
split_pct=0.75 $ train_test_cut_period = int(len(data)*split_pct) #split point $ train_set = data[:train_test_cut_period].copy() $ test_set = data[train_test_cut_period:].copy()
df_h1b_nyc_ft.groupby(['lca_case_employer_name']).lca_case_employer_name
import test_package $ test_package.print_hello_function_container.print_hello_function()
csv_df['date'] = csv_df.index.date $ print(len(set(csv_df['date'])))
name = 'tm_2016' $ year = '2016' $ title = 'Transmission ' + year + ' [TWh], marathon'
ratio.describe()
df2 = df2.drop('treatment', axis = 1) $ df2.head()
X = pd.get_dummies(df[['month', 'day', 'gap_open_pct', $                        'dollar_change_open', 'offer_price', 'open_price', $                        'dollar_chg_opencls']], drop_first = True)
crimes_wea=pd.merge(crimes, weather, on='date', how='left') $ crimes_wea.tail()
googletrend.head()
forecasted_features = ['ds','yhat','yhat_lower','yhat_upper']
learning_rate = .01 $ embed_size = 300 $ batch_size = 64 $ steps = 1000000
volumes[['Bitcoin', 'Ethereum', 'Ripple']].plot(logy=True) $ plt.ylabel('Volume') $ plt.show()
tweet = "Wed Aug 27 13:08:45 +0000 2008" $ time_info = datetime.strptime(tweet,'%a %b %d %H:%M:%S +%f %Y')
X_today.head()
students.loc['Ryan']
from statsmodels.stats.proportion import proportions_ztest $ count = [convert_old, convert_new] $ nobs = [n_old, n_new] $ stat, pval = proportions_ztest(count, nobs, alternative='smaller') $ print (stat, pval)
df2[df2.duplicated(['user_id'], keep = False)]
ebay.index = pd.to_datetime(ebay.index) $ ebay.index
old = len(df2.query("landing_page == 'old_page'")) $ old
data.head(10)
validation_size = 5000 $ data_valid = data.iloc[-validation_size:] $ data_train = data.iloc[:-validation_size] $ print('Train size:', len(data_train), 'Validation size:', len(data_valid))
ndvi_change= ndvi_of_interest02-ndvi_of_interest $ ndvi_change.attrs['affine'] = affine
df_gene = df[df['action']=="click_genetic_rec"]
df_tte_all = df[df['LinkedAccountName'] == 'TTE_DEV']
pd.Series(['a','b','c','d','e'], index=[10,20,30,40,50])
autos["last_seen"].str[:10].value_counts(normalize=True, dropna=False).sort_values()
uber_14["day_of_week"] = uber_14["Date/Time"].apply(lambda x: getDayofWeek(x, 2014)) $ uber_14.head()
liquor2016_q1.Profit = (liquor2016_q1.StateBottleRetail - liquor2016_q1.StateBottleCost) * liquor2016_q1.BottlesSold $ liquor2016_q1_profit = liquor2016_q1.Profit.groupby(liquor2016_q1.StoreNumber).agg(['sum']) $ liquor2016_q1_profit.columns = ['Profit'] $ liquor2016_q1_profit.tail()
rpt_regex = re.compile(r"(.)\1{1,}", re.IGNORECASE); $ def rpt_repl(match): $     return match.group(1)+match.group(1)
apple = MovingAverage('aapl',aapl,40,100) $ print(apple.generate_signals())
data = pd.concat(data_list, axis=0).fillna(value = "unknown") $ data.index.name = 'Name'
ssc.awaitTermination()
nb_words
for k, v in co_occurence_on_top50.items(): $     ds_json = {k: v} $     filename = '../../datasets_reco/reco_json/{}.json'.format(k) $     with open(filename, 'w') as f: $         json.dump(ds_json, f)
df_2003['bank_name'] = df_2003.bank_name.str.split(",").str[0] $
params_opt = {'num_leaves': [x for x in range(20,31,1)], $             'max_depth': [x for x in range(15,21)], $             'feature_fraction': [x / 1000.0 for x in range(825,900,25)], $             'bagging_fraction': [x / 1000.0 for x in range(825,900,25)], $             'lambda_l2' : [x / 100.0 for x in range(0,110,10)],}
file_path = "/mnt/idms/temporalNodeRanking/data/filtered_timeline_data/tsv/usopen/usopen_mentions.csv" $ au.recode_and_export_mentions(file_path,mentions_df,user_names)
df_sched2 = df_sched.iloc[:,1:].apply(pd.to_datetime,format='%Y-%m-%d')
bucket.upload_dir('data/city-util/raw', 'city-util/raw', clear_dest_dir=True)
df.median()
stocks = df_piotroski_all[['id', 'name', 'market_type']]
df['Descriptor'].tail()
for item in all_simband_data.subject_id.unique(): $     if item not in proc_rxn_time.subject_id.unique(): $         print(item)
glm_binom_feat_3 = H2OGeneralizedLinearEstimator(family='binomial', model_id='glm_v6', lambda_search=True) $ glm_binom_feat_3.train(covtype_X, covtype_y, training_frame=train_bf, validation_frame=valid_bf)
html_table = df.to_html() $ html_table
autos["price"] = autos["price"].astype(int) $ autos["odometer"] = autos["odometer"].astype(int)
users_converted = df.converted.sum() $ prop_users_converted = users_converted / total_rows $ print(prop_users_converted)
for df in (joined, joined_test): $   df['CompetitionOpenSinceMonth']= df['CompetitionOpenSinceMonth'].fillna(1).astype(np.int32) $   df['CompetitionOpenSinceYear']= df['CompetitionOpenSinceYear'].fillna(1900).astype(np.int32) $   df['Promo2SinceWeek']=df['Promo2SinceWeek'].fillna(1).astype(np.int32) $   df['Promo2SinceYear']=df['Promo2SinceYear'].fillna(2009).astype(np.int32)
display(taskData.iloc[25000:25010])
tweet_archive_clean.tweet_id.dtypes
X = pivoted.fillna(0).T.values $ X.shape
e_pos = df_elect[df_elect['Polarity'] >= 0] $ e_pos.count()
auth = tweepy.OAuthHandler(credentials.api_key, credentials.api_secret) $ auth.set_access_token(credentials.token_key, credentials.token_secret) $ api = tweepy.API(auth)
df["Complaint Type"].value_counts().head(5).sort_values().plot(kind='barh')
df_joined.country.value_counts()
print(temp_nc) $ for v in temp_nc.variables: $     print(temp_nc.variables[v])
driver.close()
cm_dt = confusion_matrix(y_final, dt_predicted)
df_new['intercept'] = 1 $ logit2 = sm.Logit(df_new['converted'],df_new[['intercept','CA','US']]) $ r2 = logit2.fit() $ r2.summary()
talks_train.to_json('train2.json')
p_treatment = df2[df2['group'] == 'treatment']['converted'].mean() $ p_treatment
random_integers.as_matrix()
last_values_liberia = grouped_months_liberia.last() $ last_values_liberia=last_values_liberia.rename(columns = {'National':'last_v_T'}) $
(p_diffs > actual).mean()
joined_samp.head(2)
run txt2pdf.py -o "YALE-NEW HAVEN HOSPITAL  Sepsis.pdf"   "YALE-NEW HAVEN HOSPITAL  Sepsis.txt"
run txt2pdf.py -o"2018-06-19 2012 FLORIDA HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2012 FLORIDA HOSPITAL Sorted by discharges.txt"
cryptoprice = dfBTC.join(dfETH) $ cryptoprice.head()
x
df.groupby("cancelled")[["first_rental", "new_customer", "repeat_customer"]].mean()
x_climate, y_size = shuffle(x_climate, y_size, random_state = 123)
cityID = '013379ee5729a5e6' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Tucson.append(tweet) 
iris.loc[::30,"Species"]
pd.concat([s1,s2,s3],axis=1)
topUserItemDocs.to_pickle('datasets/topUserItemDocsCB_27Jun_vFULL300.pkl')
y = df['comments'] $ X = df[['subreddit', 'title']].copy(deep=True) 
dfTickets.to_csv('all_tickets.csv', index=False, index_label=False)
SCC_med_inc.reset_index(level=0,inplace=True) $ SCC_med_inc.columns = ['Year','Median_HH_Income'] $ SCC_med_inc
dropCols = (['CustomerID', 'Invest', 'Educ', 'MARITAL', 'TimeYears', 'lasttrans', 'current', 'Monetary_score']) $ joined.drop(dropCols, axis=1, inplace=True) $ joined.head()
from google.colab import files $ uploaded = files.upload()
import pandas as pd $ df = pd.DataFrame(records, columns=['date', 'lie', 'explanation', 'url'])
n_converted = df.query('converted == 1').user_id.nunique() $ p_converted = n_converted / n_users $ p_converted
df = pd.DataFrame(['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D'], $                   index=['excellent', 'excellent', 'excellent', 'good', 'good', 'good', 'ok', 'ok', 'ok', 'poor', 'poor']) $ df.rename(columns={0: 'Grades'}, inplace=True) $ df
df = pd.concat(pd.read_csv(fname, index_col=0) for fname in csv_paths)
turnstile_cluster.head()
df = fuel_mgxs.get_pandas_dataframe() $ df
print(np.isfinite(trainDataVecs).all())
dfBTCVol.corr()
session.query(Adultdb).filter_by(occupation="?").delete(synchronize_session='fetch') $ session.commit()
run_augmented_Dickey_Fuller_test(therapist_duration, num_diffs=2)
X_cm_n=X_d
pd.Series(['San Francisco', 'San Jose', 'Sacramento'])
male_journalists_retweet_summary_df[['retweet_count']].describe()
df_ml_59 = df.copy() $ df_ml_59.index.rename('date', inplace=True) $ df_ml_59_01=df_ml_59.copy()
p_diffs = np.array(p_diffs) $ p_diffs
tbl3 = pd.concat([ff,hp2], axis = 1, join='outer') $ tbl3.columns = ['Mkt_RF','SMB','HML','RF','HPr'] $ tbl3['HPr_RF'] = tbl3['HPr'] - tbl3['RF'] $ tbl3.head()
mismatch['match'] = np.where((df['group'] == 'treatment') & (df['landing_page'] == 'new_page'), 'match','mismatch')
bListenCount = sc.broadcast(trainData.map(lambda r: (r[1], r[2])).reduceByKey(lambda x,y: x+y).collectAsMap()) $ def predictMostListened(allData): $      return allData.map(lambda r: Rating(r[0], r[1], bListenCount.value.get( r[1] , 0.0))) $
from scipy import stats $ stats.describe(RandomPercentage)
test_df_01 = test[:, ['label', 'imagePath']].cbind(preds['predict']).as_data_frame() $ test_df_01.head()
random.seed(2018)
y_pred = reg.predict(X_)
cursor = connection.cursor() $ cursor.execute(query) $ results = cursor.fetchall() $ pd.DataFrame.from_records(results)
staff.T
kde_plot(df, variable = 'amount_tsh', bw = 1000, lower = 0) $ kde_plot(df, variable = 'construction_year', bw = 1, lower = 1000, upper = 2016) $ kde_plot(df, variable = 'gps_height', bw = 100)
pizza_ratings, pizza_counts = topic_ratings_all(pizza_dictionary, pizza_train_model_final, cat_pizza, 'reviews_without_rare_words', 10)
re.findall('theme__text--light">(.*?)<', html.replace(' [at] ','@').replace(' [@] ','@') )
dataset.head()
df2 = df.drop(df.query('(landing_page == "new_page" & group == "control") or (landing_page != "new_page" & group != "control")').index)
run txt2pdf.py -o"2018-06-14-1513 FLORIDA HOSPITAL - 2011 Percentiles.pdf"  "2018-06-14-1513 FLORIDA HOSPITAL - 2011 Percentiles.txt"
patterns_sex="m|f|NA" $ non_matches=df.sex.loc[df.sex.str.match(patterns_sex)!=True] $ print("\nThere are {} entries for sex which do not match the patterns {}:"\ $       .format(non_matches.shape[0],patterns_sex.split("|"))) $ non_matches.value_counts()
r1 = requests.get("https://API_KEY@www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2018-08-23&end_date=2018-08-23")
import numpy as np $ import matplotlib.pyplot as pp $ import pandas as pd $ import seaborn $ %matplotlib inline
titanic.dtypes
features_delta
bedrooms = train_df['bedrooms'].value_counts() $ x = bedrooms.index $ y = bedrooms.values $ sns.barplot(x, y )
recommendation_df = recommendation_df[recommendation_df['contest_id'] == 'c8ff662c97d345d2']
cbow_m1 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 5, size=100) $ cbow_m2 = gensim.models.Word2Vec(train_clean_token, min_count=5, workers=2, window = 5, size=100) $ cbow_m3 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 10, size=100) $ cbow_m4 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 5, size=300)
X = reddit_master['subreddit'] $ y = reddit_master['Class_comments'].apply(lambda x: 1 if x == 'High' else 0) $ X = pd.get_dummies(X, drop_first = True) $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05)))
plt.hist(p_diffs) $ plt.xlabel('Diff') $ plt.ylabel('X(frequency)')
df2.reset_index(inplace=True)
twitter_archive_clean[~twitter_archive_clean['retweeted_status_id'].isnull()]
us['country'].value_counts(dropna=False)
w,h = 16,16 $ I = np.random.randint(0,2,(h,w,3)).astype(np.ubyte) $ F = I[...,0]*(256*256) + I[...,1]*256 +I[...,2] $ n = len(np.unique(F)) $ print(n)
print(ozzy.age)
frames = [df1, df2] $ reddit = pd.concat(frames) $ reddit = reddit.reset_index(drop=True) $ reddit
df.head() # our raw twitter data from json file
dr_num_patients = doctors['id'].resample('W-MON', lambda x: x.nunique())
new_page_converted = np.random.choice([0,1], size = n_new, p = [1-p_under_null, p_under_null]) $ print(new_page_converted)
z_score, p_value
new = df[df['landing_page']=='new_page']['user_id'].unique().shape[0] $ tot = df['user_id'].unique().shape[0] $ print(new/tot)
image_predictions_clean = image_predictions_clean[['tweet_id', 'dog_breed']]
print("There are", twitter_data.doggo.value_counts()[1], "doggos") $ print("There are",twitter_data.doggo.value_counts()[1], "floofers") $ print("There are", twitter_data.pupper.value_counts()[1], "puppers") $ print("There are", twitter_data.puppo.value_counts()[1], "puppos") $ twitter_data.doggo.value_counts()[1]+twitter_data.doggo.value_counts()[1]+twitter_data.pupper.value_counts()[1]+twitter_data.puppo.value_counts()[1]
fig, ax = plt.subplots() $ ax.scatter(df.track_popularity, df.artist_followers) $ ax.set_title('Artist Followers vs. Track Popularity') $ ax.set_xlabel('Track Popularity') $ ax.set_ylabel('Artist Followers')
to_be_predicted_Day1 = 51.60 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
backup = clean_rates.copy()
df.groupby('brand').count().reset_index().sort('price',ascending=False)
automl_feat = AutoSklearnRegressor( $         time_left_for_this_task=480,  # 8 minutes $         per_run_time_limit=120)       # 2 minutes
hist
prcip_df.describe() $
def MA(df, n):  $     MA = pd.Series(pd.rolling_mean(df['Close'], n), name = 'SMA_' + str(n))  $     df = df.join(MA)  $     return df $ dataframe1=MA(dataframe,30) $
models = ['Top', 'TopDoc', 'TopOP', 'All'] $ sp_re_df = (pd.DataFrame(specrecf1)).T
lookforward_window = 1 $ lookback_window = 6
news_df = pd.read_json("Newschannel_tweets_df.json").sort_values('Source Acc.') $ news_df.head()
records = pd.read_csv('mock_student_data.csv') $ records.head()
import os $ import subprocess $ from subprocess import check_output, run $ string_to_do ="sleep 1; echo hello" $ out = run(string_to_do,shell=True,stdout=subprocess.PIPE)
df_clean['doggo'].value_counts(),df_clean['floofer'].value_counts(),df_clean['pupper'].value_counts(),df_clean['puppo'].value_counts()
vectorized_text_labeled = vectorizer.fit_transform(description_labeled)
df2.converted.sum() / len(df2)
tweet_archive_clean.rating_numerator.value_counts().sort_index().head(10)
grades + better_bonus_points
learner.save_encoder('adam1_enc')
os.chdir('object_detection')
df[df.rating_denominator > 10]
df_clean = df_clean.loc[df_clean['expanded_urls'].str.extract(pat=r'(photo)', expand=False).notnull(), :]
assert (ebola >= 0).all().all()
actual = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ actual
df2 = df.query('(landing_page=="old_page" & group=="control") or (landing_page=="new_page" & group=="treatment")') $ df2.shape
close_prices = df['Close'] $ close_prices.head()
import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline $ %config InlineBackend.figure_format = 'svg'
cols_filt=building_pa_prc.columns[filt_cols] $ cols_filt
totalOutputs = reduce((lambda x, y: x + y), outAny).sort_values() $ plt.plot(df['id'], totalOutputs, 'ro') $ plt.xlabel('Mechanism ID') $ plt.ylabel('totalOutputs') $ plt.show()
race_vars.columns = race_vars.columns.str.replace(' ', '_') $ race_vars.columns = race_vars.columns.str.replace('/', '_') $ race_vars.columns = race_vars.columns.str.lower()
df.rename(columns={'id_x':'Matching Parcel status', $                    'id_y':'Matching Shipping Method id', $                    'name':'Shipping Method name', $                    'message':'Parcel Status'}, $           inplace=True)
treatment_df=df2.query('group=="treatment"') $ treatment_df.query('converted==1').user_id.nunique()/treatment_df.converted.count()
labels['Time']  = constant_t $ labels
guineaDeaths1 = guineaFullDf.loc['New deaths registered'] $ guineaDeaths2 = guineaFullDf.loc['New deaths registered today'] $ guineaDeaths = pd.concat([guineaDeaths1, guineaDeaths2]) $ guineaDeaths.head()
df_train.min(axis=0)
from pyhive import hive
import statsmodels.api as sm # fail to import in local notebook, haven't figure out yet $ convert_old = len(df2[(df2['landing_page']=='old_page')&(df2['converted']==1)]) $ convert_new = len(df2[(df2['landing_page']=='new_page')&(df2['converted']==1)]) $ print("convert_old:", convert_old,", convert_new:",convert_new,"\nNold:", Nold,", Nnew:", Nnew)
bacteria_data['value']
roll_train_df = train[columns] $ roll_test_df = test[columns]
len(df.Genre.value_counts())
coeffs = pd.Series(model.coef_, index = X.columns)
%%time $ df.to_hdf('store.h5', key='losses')
plt.figure(figsize=(16,10)) $ plt.clf() $ sns.boxplot(x='Journal',y='PubDays',hue='Publisher',data=df) $ plt.xticks(rotation=90)
temp_nc = Dataset("../data/nc/air.mon.mean.nc")
sites_no_net = sites[sites['On Zayo Network Status'] == 'Not on Zayo Network'].groupby(['Account ID'])['Building ID'].count().reset_index().sort_values(by='Building ID', ascending=False)
snow.select ("select count (distinct pt_id) from st_alb_angina")
appointments['Provider'].unique(), len(appointments['Provider'].unique())
len(df2['user_id'].unique())
plt.hist(holdout_residuals)
tweet_full_df.head()
pd.to_datetime(['2009/07/31', 'asd'], errors='coerce')
stocksRdd = sc.textFile("stocks") $ stocksRdd.cache().count()
temp_df = pd.DataFrame(normals, columns=['tmin','tavg','tmax']) $ trip_date_str = [dt.datetime.strftime(d,'%Y-%m-%d') for d in date_range] $ temp_df['date'] = trip_date_str $ temp_df.set_index('date', inplace=True) $ temp_df
articles_train, articles_test, authors_train, authors_test = train_test_split( $     articles, authors, test_size=0.3, random_state=42, stratify=authors)
df_batch6_sec = df_batch6_sec[df_batch6_sec['Comments2'] != '[removed]'] $ df_batch6_sec = df_batch6_sec[df_batch6_sec['Comments2'] != '[deleted]']
model.fit(x, ynum, epochs=25, batch_size=32,verbose=2)
os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $ Show_files_generated_today()
def adder(ele1,ele2): $     return ele1+ele2 $ df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) $ df
filename = "data/temperatures/annual.land_ocean.90S.90N.df_1901-2000mean.dat" $ full_globe_temp = pd.read_table(filename) $ full_globe_temp
import numexpr $ mask_numexpr = numexpr.evaluate('(x > 0.5) & (y < 0.5)') $ np.allclose(mask, mask_numexpr)
borough_complaints = question_3_dataframe.\ $     groupby('borough')['created_date'].\ $     count() $ borough_complaints
bild.head(3)
df.groupby(['date']).price.mean()
import ggplot as gg
sum(tweet_archive_clean['expanded_urls'].isnull())
df_CLEAN1A['AGE_groups'] = df_CLEAN1A['AGE_groups'].astype('category')
Results_kNN500x.to_csv('soln_kNN500_extra.csv', index=False)
sns.boxplot(x=df.author, y=df.totalwords); $ plt.xticks(np.arange(10), ('collins', 'hclinton', 'hirono', 'hoeven', 'mccain', 'obama', 'ryan', 'sanders', 'schwarzenegger', 'trump'), rotation = 50);
df.sum(axis=1)
nn_X_train,nn_X_test,nn_y_train,nn_y_test=train_test_split(newnew,to_categorical(wenwen),test_size=0.1)
tropical = reviews.description.map(lambda r: 'tropical' in r).value_counts() $ fruity = reviews.description.map(lambda r : 'fruity' in r).value_counts() $ pd.Series([tropical[True],fruity[True]],index=['tropical','fruity'])
stocks.head()
values = [] $ for df in dfs: $     values.append(df['Outside Temperature'].max()) $ print (values)
datetime.strptime('20091031', '%Y%m%d')
autos.drop(['seller', 'offer_type'], axis = 1, inplace = True) $ autos.head()
pd.value_counts(merged1['AppointmentDuration'])
df.groupby("userid").sum().sort_values(by="postcount",ascending=False)[:10]
timeseries = dfs['Close']
run txt2pdf.py -o "CHRISTIANA CARE HEALTH SERVICES, INC.  Sepsis.pdf"   "CHRISTIANA CARE HEALTH SERVICES, INC.  Sepsis.txt"
control_group_sample = df2[df2.group == 'control'] $ num_of_converted_in_control = len(control_group_sample[control_group_sample.converted == 1]) $ p_control_converted = num_of_converted_in_control/len(control_group_sample) $ p_control_converted
k150_bets_under = [x[1] > .6 for x in pred_probas_under_k150]
X_mat = X_df1.as_matrix()
france_tops = lda.get_term_topics(d.token2id['france'], minimum_probability=0.001) $ france_tops, get_topic_desig(france_tops)
A + B
tweets['weekNumber'] = ((tweets['daysFromStart']/7) + 1).apply(np.floor) $ tweets['dayNumber'] = tweets['daysFromStart'] + 1 - (tweets['weekNumber'] -1)*7 $ tweets['hourNumber'] = tweets['hoursFromStart'] - (tweets['weekNumber'] -1)*168
from google.colab import files $ files.upload()
x3 = poly3.fit_transform(x)
trans_data.head()
autos = pd.read_csv("autos.csv", encoding = 'Latin-1') $
goodreads_users_df.isnull().sum()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ df = pd.read_table(path, sep ='\s+', nrows=80, skiprows=(1,2,5,10,60), usecols=(1,3,5)) $ print("df.head(5)\n",df.head(5)) $ print("Length of df = ",len(df))
sfs1.k_feature_names_
from pandas.plotting import autocorrelation_plot $ autocorrelation_plot(CH_electric['Total_Demand_KW'])
freq = all_acct_ids.stat.freqItems(['sales_acct_id'], 0.25) $ freq.collect()[0]
df['genre'].value_counts()
pd.DatetimeIndex(pivoted.columns)
url = form_url('handednesses', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
corr = np.corrcoef(opendf['repair_time'], opendf['2017_homeAverage']) $ corr
pipeline = Pipeline(stages=[OutcomeType_indexer, AnimalType_indexer, SexuponOutcome_indexer, Breed_indexer, Color_indexer, AgeuponOutcome_transformer, Bin_transformer]) $ model = pipeline.fit(shelter_df)
fig, ax = plt.subplots(1, figsize=(12,4)) $ plot_with_moving_average(ax, 'Seasonal AVG Doctors', doc_duration, window=52)
treat_convert = df2[df2['group']== 'treatment'].converted.mean() $ print(treat_convert)
predictions_train=predictions_train.withColumn('scores',explode('rec')) $ predictions_train=predictions_train.withColumn('repo_id',predictions_train.scores['item'])\ $                        .withColumn('score',predictions_train.scores['score']) $ predictions_train=predictions_train.select('user_id','repo_id','score') $
life_ship_top_vec = all_top_vecs[bks.life_ship[0]] $ twentyk_leagues_top_vec = all_top_vecs[bks.twentyk_leagues[0]] $ diff_vec2 = vec_lst_add_subtract(twentyk_leagues_top_vec, life_ship_top_vec, mode='subtract') $ find_most_similar(diff_vec2, all_top_vecs, title_lst, vec_in_corp='N')
counts = df2.user_id.value_counts() $ repeated_id = counts[counts > 1].index $ repeated_id[0]
import statsmodels.api as sm $ logit = sm.Logit(df2['converted'],df2[['intercept','ab_page']])
cats_df.head()
finals.loc[(finals["pts_l"] == 1) & (finals["ast_l"] == 1) & (finals["reb_l"] == 1), 'type'] = 'useless'
df = engine.get_data(**opts)
guardian_data = guardian.scrapeFacebookPageFeedStatus()
df['dog_type'].value_counts()
from calendar import day_name $ pd.DataFrame({'Day':[day_name[i] for i in range(7)], $             'Avg. Temp':noaa_data['AIR_TEMPERATURE'].groupby(noaa_data.index.dayofweek).mean()}, $              columns=['Day','Avg. Temp'] $ ).set_index('Day').T
activity = session.query(Measurement.station, Station.name, func.count(Measurement.tobs)).\ $ filter(Measurement.station == Station.station).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all() $ activity
sharpe_ratio(data,daily_rf=0.00037828653,portfolio=True)
N = 200
results_countries_2.summary()
raw = pd.read_excel( param.weather_folder + '/wind-forecast.xlsx') $ raw['TIMESTAMP_UTC'] = pd.to_datetime(raw['TIMESTAMP_UTC'])
menu_dishes_about_latent_features = pd.DataFrame(menu_dishes_about_latent_features_with_menu_ids, columns=menu_dishes_about_latent_features_column_names)
plot_contingency(sessions_summary, "country_destination", "action_similar_listings_v2")
df2.groupby('group')['converted'].describe()
grbreg.score(X_test, y_test)
twitter_archive.info()
for rect in rectangles: $     ax2.add_patch(rect)
data[['Sales']].resample('D').mean().head()
df_train.head()
knn = KNeighborsClassifier(n_neighbors = 3) $ knn.fit(X_train, Y_train) $ Y_pred = knn.predict(X_test) $ acc_knn = round(knn.score(X_test, Y_test) * 100, 2) $ acc_knn
folium.GeoJson(watershed).add_to(m);
list("1" "2" "abc" "Two words.")
df_weather.precipitation_inches.unique()
os.chdir('C:\\Users\\Ganesh\\Desktop')
model = gl.item_similarity_recommender.create(train_data, user_id = 'hacker_id', item_id= 'challenge_id', similarity_type='jaccard')
G = nx.Graph() #creates empty graph, initiliasize a graph object $ G.add_nodes_from(node_names) $ G.add_edges_from(edges)
train.to_pickle('../data/merged_data/train.pkl')
tags = pd.read_csv('data/tags.csv', index_col = 0) $ tags.head()
df2.keys()
(p_diffs > prob_convert_given_treatment- prob_convert_given_control).mean() $
crimes['PRIMARY_DESCRIPTION'].head()
scraped_batch6_top
member=bixi[bixi['is_member']==1] $ non_member=bixi[bixi['is_member']==0]
S.decision_obj.stomResist.value = 'BallBerry' $ S.decision_obj.stomResist.value
station_count2 = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all() $ station_count2
image_predictions_copy.info()
len(data_cus_id), len(data_dates)
df.head()
a=[0,1] $ new_page_converted = np.random.choice(a,145310,p=[0.8804,0.1196]) $ print(new_page_converted.mean())
old_page_converted = np.random.binomial(1, p_old,n_old)
pd.options.display.max_colwidth = 100 $ data_df[data_df.nwords == 1]['clean_desc'].head(15)
nba_df = nba_data_list[0] $ nba_df
df.plot(column = "Cluster_Labels_GaussianMixture", cmap = "Reds", legend = True) $ plt.title("Clusters: Gaussian Mixture")
Meter1.AutoRun(5, 'Sec')
df2[df2['landing_page'] == 'new_page']['landing_page'].count() / df2['landing_page'].count()
 1 - (election_data['pop_2012'].isnull().sum() / len(election_data['pop_2012'])) $
trump_tfidf.shape, trump_cleaned_tfidf.shape, trump_stemmed_tfidf.shape
!python extract_dl1.py -f refdata/Run17473_r1.tio -m refdata/Run17473.mon
question_2_dataframe_in_top_zips = question_2_dataframe_in_top_zips.groupby(['incident_zip', 'complaint_type'])
ET_Combine
from sklearn.feature_extraction.text import TfidfVectorizer $ tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=2, max_df=0.5, stop_words=portuguese_stop_words)
bug_count = df_bug[u'Service Location'].value_counts() $ mdid_lst = list(df_bthlst[df_bthlst["booth_id"].isin(bug_count.index.values)]["md_id"]) $ indx = ['mdb' + str(v) for v in mdid_lst] $ df_selparams = pd.DataFrame(index=indx, columns=['n_faults', 'n_amb', 'n_door', 'n_relay', 'n_mcu', 'n_temp'])
users.head()
scores = [] $ for tweet in tweets[1]: $     scores.append(sample_analyze_sentiment (tweet)) $ scores
f(137.5, True)
filepath = os.path.join('extracts', 'monthly_reports')
slice_metrics(valid_scores, 'tx_TacticId')
shows.isnull().sum()
cars.yearOfRegistration.unique() $
X_train_feature_counts
pd.Series(STATE_COLORS.values()).value_counts()
table_names = ['train'] $ str('{PATH}{fname}.csv')
appleInitialNegs.shape
predicted_live = model.predict(X_live) $ predicted_live
import matplotlib.pyplot as plt $ import seaborn as sns $ sns.set()
df_2012['bank_name'] = df_2012.bank_name.str.split(",").str[0] $
start_time = time_utc[0] $ end_time = time_utc[-1] $ print("Total time in mins is", (end_time - start_time)/ 60000)
page = requests.get(url, timeout=5)
active_df['start_date'] = pd.to_datetime(active_df['scns_created'].apply(lambda x:x[0])).dt.strftime('%Y-%m')
print(interests_tokens[1]) $ print(corpus[1]) $ print(tf_idf[corpus][1])
tweepy.__version__
len(graffiti2)
from scipy import stats $ stats.chisqprob = lambda chisq, df2: stats.chi2.sf(chisq, df2) # https://github.com/statsmodels/statsmodels/issues/3931 $ mod = sm.Logit(df2['ab_page'], df2[['converted','intercept']]) $ res = mod.fit() $ res.summary()
to_be_predicted_Day4 = 34.57130151 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
df3 = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df3.head()
autos["unrepaired_damage"].unique()
url = "https://mars.nasa.gov/news/"
train_features = spark.read.csv(os.path.join(mungepath,"model_data/20180504/rf_lr_lasso_inter2_noip/train_features/*"), header=True) $ print("Number of observations in train :", train_features.count())
pd.isnull(train_df).sum() $ pd.isnull(test_df).sum() 
df_out
datetime_cumulative = {turnstile: [(datetime.strptime(date + time,'%m/%d/%Y%X'),int(in_cumulative)) $                                    for _, _, date, time,_, in_cumulative, _ in rows] $                        for turnstile, rows in raw_readings.items()}    
df_t.sort_values(by='timestamp').head(2)
user.query("location not in ['DC', 'NYC']").head(3)
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets']))) $ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets']))) $ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
df_survival.sort_values(by=['Donor ID', 'Donation Received Date'], inplace=True) $ df_survival.loc[:,'Time between donations'] = df_survival.groupby('Donor ID')['Donation Received Date'].diff().fillna(0)
df_all = pd.concat(dfs[2:], keys=selected_reporting_dates[2:])
pd.read_csv("data.csv", index_col=[0, 1, 2, 3, 4, 5], skipinitialspace=True, parse_dates=['Date']).head(3)
grid_id_flat[:10]
total_control = (df2['group'] == 'control').sum() $ control_converted = len((df2[(df2['group'] == 'control') & (df2['converted'] == 1)])) $ print((control_converted / total_control))
cityID = 'e67427d9b4126602' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Madison.append(tweet) 
df = df [new_colorder] $ df.set_index('empID',inplace=True) $ df
tweets=api.GetUserTimeline('826036134', include_rts=False, count=200) $ print(tweets[0]) $ print(len(tweets))
import functools $ L = ['Testing', 'shows', 'the', 'presence', ',','not', 'the', 'absence', 'of', 'bugs']
startTime_user = train['user_date_created'].min() $ y = train['is_fake'].astype(int)
hits_df = hits_df.interpolate(method='time')
tesla.text = tesla.text.str.lstrip('b') $ tesla.created_at = pd.to_datetime(tesla.created_at) $ tesla['date'] = tesla.created_at.apply(lambda x: x.date())
import requests $ import collections $ import json $ from key import key
(pd.DataFrame.from_dict(data=importances, orient='index') $    .to_csv(base_filename + '_featimp.csv', header=False))
rfc = RandomForestClassifier(class_weight='balanced_subsample') $ s = cross_val_score(rfc,subrcvec, y_train, cv=cv, n_jobs=-1) $
archive_df_clean['timestamp'] = archive_df_clean.timestamp.str[:19] $ archive_df_clean['timestamp'] = pd.to_datetime(archive_df_clean['timestamp'], format = "%Y-%m-%d %H:%M:%S")
props.info()
geo_db.head()
autos["price"].value_counts().sort_index(ascending=False).head(20)
X_test = pd.get_dummies(columns=['term', 'home_ownership', 'verification_status', 'purpose'], data=X_test)
csv_data = pd.read_csv("Returns.csv") $ print(csv_data)
All_tweet_data_v2.rating_denominator.value_counts()
cutoff_times.loc[cutoff_times['days_to_next_churn'] == 0]
pd.get_dummies(grades_ord,drop_first= True)
format = lambda x: '%.2f' % x
temp_df = pd.concat([most_common_registered_addresses_for_no_psc_companies,temp_s],axis=1) $ temp_df.columns = ['number_no_psc','total_companies'] $ temp_df['proportion_no_psc'] = temp_df['number_no_psc'] / temp_df['total_companies'] $ temp_df[temp_df.total_companies > 100].sort_values(by='proportion_no_psc',ascending=False).head(10)
X_future = sandag_df.values
grouped.to_csv('tweetcount.csv')
covtype_df
from sklearn.model_selection import KFold $ cv = KFold(n_splits=100, random_state=None, shuffle=True) $ estimator = Ridge(alpha=8000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
print("Logistic Regression") $ param_grid = {'solver':['newton-cg'], 'n_jobs':[-1], 'max_iter':[100, 300, 400], 'C':[0.01, 0.1, 1.0, 10.0, 100, 1000]} $ train_give_me_a_value(LogisticRegression(solver='newton-cg', n_jobs=-1), param_grid) $
x = store_items.isnull() $ print(x)
(p_diffs > p_treatment - p_control).sum() / 10000
sale_average = sales.reset_index(drop=False) $ sale_average
df_mes = df_mes[(df_mes['average_speed'] != np.inf) & (df_mes['average_speed'] < 100)] $ df_mes.shape[0]
df.groupby(['character_id', 'raw_character_text'])['id'].nunique().reset_index().head(10)
red_4['age'] = red_4['time fetched'] - red_4['created_utc']
year= train_data.date_account_created[0][:4] $ print year $ month= train_data.date_account_created[0][5:7] $ print month
issues
df_loudparties = df[df['Descriptor'] == 'Loud Music/Party'] $ df_loudparties.groupby(df_loudparties.index.weekday).apply(lambda x: len(x)).plot()
df.shape
pmol.df['distances'] = distances $ pmol.df.head()
rent_db.columns
print 'Most VICE-like:', tweets_pp[tweets_pp.handle == 'VICE'].sort_values('VICE_Prob', ascending=False).text.values[0] $ print 'Least VICE-like:', tweets_pp[tweets_pp.handle == 'VICE'].sort_values('VICE_Prob', ascending=True).text.values[0]
locations = session.query(Measurement).group_by(Measurement.station).count() $ locations
twitter_archive_clean.info()
df_test, _, nas, mapper = proc_df(joined_test, 'Sales', do_scale=True, skip_flds=['Id'], $                                   mapper=mapper, na_dict=nas)
table_names = ['train', 'store', 'store_states', 'state_names', 'googletrend', 'weather', 'test']
new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just movie IDs $ new_user_unrated_movies_RDD = (complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0]))) $ new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD) $
print('Examples of tweets (with only text and hashtag column):') $ tw6[['text', 'hashtag']].head(3)
y_pred = m.predict(df)
zip_counts = bus.groupby("postal_code").count() $ zip_counts = zip_counts.iloc[:, 0:1] $ zip_counts.columns = ["count"] $ zip_counts
df['category_main'] = [json.loads(x)['urls']['web']['discover'][:].split('/')[5] for x in df['category']] $ df['category_main'] = df['category_main'].replace({'film%20&%20video': 'film_and_video'})
df = raw_df.copy() $
nb_classifier = MultinomialNB() $ nb_classifier.fit(count_train, y_train) $ pred = nb_classifier.predict(count_test) $ metrics.accuracy_score(y_test, pred)
df_group
dftemp = df1[(df1['Area'] == "Iceland")] $ dftemp.head(10) $
wb.search('cell.*%')
df.state.unique()
df_user.created_at = pd.to_datetime(df_user.created_at) $ df_user.activated_at = pd.to_datetime(df_user.activated_at) $ df_user["nweek_create"] = df_user.created_at.dt.week $ df_user["nweek_active"]   = df_user.activated_at.dt.week
import pandas as pd $ import matplotlib.pyplot as plt $ import seaborn as sb $ from datetime import datetime 
n_old = df2[df2.landing_page == 'old_page']['user_id'].count() $ n_old
print(tweets.loc[300074]["content"]) $ tweets.loc[300074]
def graph_identification(): $     result = ['PA','SW_L','SW_L','PA','SW_H'] $     return result $
cc['loghigh']=np.log(cc['high']) $ plt.hist(cc['loghigh']) $ plt.show()
len(train_data[train_data.offerType == 'Gesuch'])
knn3 = KNeighborsClassifier( n_neighbors=3,  $                            weights='uniform') $ scores = cross_val_score(knn3,  X_train, y_train,  cv=5) $ knn3.fit(X_train, y_train) $ np.mean(scores), np.std(scores)
flightv1_1.select('trip').distinct().show()
a = house_data[['bathrooms', 'sqft_living', 'grade', 'sqft_living15', 'sqft_lot15', 'zipcode', 'waterfront', 'zipcode', 'condition', 'bedrooms']] $ b = house_data[['price']]
out_train['display_address'] = out_train['display_address'].map(lambda x: x.replace('\r','')) $ out_train['street_address'] = out_train['street_address'].map(lambda x: x.replace('\r',''))
x.to_csv('data_cleaned')
plt.subplots(figsize=(6, 4)) $ sn.barplot(train_session_v2['isNDF'],train_session_v2['search'])
df['budget']=df['budget'].replace(0,df['budget'].mean()) $ df['revenue']=df['revenue'].replace(0,df['revenue'].mean()) $ df['budget_adj']=df['budget_adj'].replace(0,df['budget_adj'].mean()) $ df['revenue_adj']=df['revenue_adj'].replace(0,df['revenue_adj'].mean())
df.describe()
X_sample = X.loc[4232] $ y_sample = y.loc[4232] $ X_sample, y_sample
numberly = pd.read_csv('numberly.csv') $ ttest(numberly["Many"], numberly["A lot"])
tweet
pd.to_datetime('20170109',format='%Y%d%m')
test_residuals = test_preds - test_target $
autos['registration_year'].value_counts(normalize=True).sort_index()
df = pd.merge(ffr, vc, left_index=True, right_index=True, how="left") $ df_info(df)
p_diff = (new_page_converted/n_new) - (old_page_converted/n_old) $ p_diff
def timestamp2datetime(timestamp): $     try: $         return  pd.to_datetime(timestamp,unit='s') $     except AttributeError: $         return timestamp 
df.plot(x='playId', y='homeWinPercentage', figsize=(15,5))
news_source = ["FoxNews", "CNN", "BBCWorld", "CBSNews", "nytimes"] $
ax=contractor_merge.groupby(['month_year'])['contractor_number'].nunique().plot(kind='Line', title ="Bar Chart for Frequency of Contractor Updates by year", figsize=(15, 10), legend=True, fontsize=12,) $ ax.set_xlabel("Month_Year", fontsize=12) $ ax.set_ylabel("Count of Contractor Updates", fontsize=12) $ plt.show()
health_data
tweets_predictions_all.to_csv('twitter_archive_master.csv', index=False)
pp.pprint(emoji_lis(text)) $ line() $ print('Emojis: ', emoji_count(text))
grid_clf.best_estimator_.feature_importances_
np.vstack((a, a))
print(len(x_tokens))
autos.registration_year.describe()
tweets_df[~(tweets_df.place.isnull())]
print "The day size is:", len(df_day['tripduration']) $ print "The night size is:",len(df_night['tripduration'])
model_1_features = ['sqft_living', 'bedrooms', 'bathrooms', 'lat', 'long'] $ model_2_features = model_1_features + ['bed_bath_rooms'] $ model_3_features = model_2_features + ['bedrooms_squared', 'log_sqft_living', 'lat_plus_long']
ttDaily = (ttTimeEntry.groupby(mainkey+['DATE']) $            .ENTRIES.first().reset_index()) $ ttDaily.head()
MICROSACC.plot_densities(microsaccades)
cp311 = pd.concat([cp311,dfagency],axis=1,join='inner').copy()
print('Most negative tweets:') $ for t in trump.sort_values('polarity').head()['text']: $     print('\n  ', t)
from sklearn.feature_extraction.text import CountVectorizer
data = pd.read_csv(datafile,index_col='Unique Key')
from bs4 import BeautifulSoup $ example1 = BeautifulSoup(train['review'][0],'lxml') $ print(len(train['review'][0])) $ print(len(example1.get_text()))
calls_df.columns
del sensor_list $ del sensor1_nbart $ del sensor2_nbart $ del sensor3_nbart
df2[df2.duplicated(subset = ['user_id']) == True]
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
df.merge?
delays_by_origin = grouped_by_origin['DepDelay'].agg(['mean', 'count']) $ delays_by_origin.sort_values('mean', ascending=False).head(10)
import sqlite3 # To add sqlite3 support in our version of Pandas
m!wget --header="Host: nlp.stanford.edu" --header="User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.119 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-GB,en-US;q=0.9,en;q=0.8" --header="Cookie: _ga=GA1.2.959822175.1516051099; _gac_UA-139650-1=1.1518275788.Cj0KCQiAzfrTBRC_ARIsAJ5ps0swqKwO1ffWhAVp7AHjmug3ZFNz9AuOSsfrc9jm8SD85fUNB3PVNTkaAhkbEALw_wcB; _mkto_trk=id:194-OCQ-487&token:_mch-stanford.edu-1518275788980-17345; _gid=GA1.2.1093929497.1518850289; _gat=1" --header="Connection: keep-alive" "https://nlp.stanford.edu/data/glove.6B.zip" -O "glove.6B.zip" -c
jpl_logo_href = jpl_soup.find_all('div', class_='jpl_logo') $ print(jpl_logo_href)
total_cars = autos['registration_year'].shape[0] $ correct_range_cars = autos[autos['registration_year'].between(1886, 2016)].shape[0] $ correct_range_percentage = correct_range_cars / total_cars $ correct_range_percentage
vectorized = cv.fit_transform(shows['stemmed_keywords']).todense() $ vectorized = pd.DataFrame(vectorized, columns=cv.get_feature_names())
new_page_converted = np.random.choice([1, 0], size = n_new, p=[p_new, (1-p_new)]).mean() $ new_page_converted
rounds_df.hist(column = 'announced_year',bins = 27, figsize = (20,8))
raw = raw[raw.find("I--DOWN THE RABBIT-HOLE"):raw.rfind("End of the Project Gutenberg EBook")] $ raw.find("I--DOWN THE RABBIT-HOLE") #Trimmed book text
to_be_predicted_Day2 = 26.69068092 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
url = form_url(f'organizations/{org_id}/teamTypes', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
new_page_converted = np.random.choice(a=[0, 1], $                                       p=[1-p_new, p_new], $                                       size=n_new) $ new_page_converted
twitter_df.dtypes
apple.resample('M').mean().plot(grid=True)
print("Number of unique user_ids in df2: {}".format(df2.user_id.nunique()))
op_add_comms.head(10)
pca_df.plot(kind='scatter', x='PC1', y='PC2') $ plt.xlabel('PC1 - {}%'.format(per_var[0])) $ plt.ylabel('PC2 - {}%'.format(per_var[1])) $
from pixiedust.display import * $ display(result_df)
observations_node = opoints_node['observations'] $ summarize_node(observations_node)
sklearn.metrics.mean_absolute_error(y_true= y_test,y_pred=lin_pred)
n_new=df2[df2['group']=='treatment'].user_id.nunique() $ n_new
def historical_data(ticker, outsize = "full"): $     alphavantage_link = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={0}&apikey=NXY0VT9AHBRYGKKC&datatype=csv&outputsize={1}'.format(ticker, outsize) $     df = pd.read_csv(alphavantage_link) $     return df $
result = github_issue_search('repo:greenelab/deep-review type:issue state:closed') $ stats['closed_issues'] = result['total_count']
import requests $ import quandl $ from pandas.io.json import json_normalize $ quandl.ApiConfig.api_key = API_KEY 
X_new = pd.DataFrame({'TV': [data.TV.min(), data.TV.max()]}) $ X_new.head()
path = os.path.join('EIA country-wide gen fuel CO2.csv') $ eia_total = pd.read_csv(path, parse_dates=['datetime'], low_memory=False)
testing.head(10)
df.columns
bacteria_data[['value']]
obs_diff= treatment_converted -control_converted $ obs_diff
proc.token_count_pandas().head(20)
is_unpaid = data.groupby('customer_id').agg({'score': max, $                                    'paid_status': lambda x: 1 * (np.sum(x == 'UNPAID') > 0) + np.random.randn(1) * 0.05})
output= "SELECT date, sum(retweets) as srt from tweet_details group by date order by srt desc limit 10 " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['Date','Sum Of Retweets'])
data.index
df['converted'].sum() / df['converted'].count()
float(len(dd_df)) / float(len(df))
top50
ebola_0 = ebola.fillna(value=0) $ ebola_0.info()
temperature_sensors_df['state'][ $     temperature_sensors_df['entity_id'] == 'sensor.darksky_sensor_temperature'].hist( $     bins=50); $ plt.title("Outside temperature"); $ plt.xlabel("Temperature $^\circ$C");
print("Un ejemplo de un registro de Etiqueta: \n") $ print(json.dumps(l[0], indent=4))
df1 = pd.DataFrame({"A":["A1", "A2"], $                     "B":["B1","B2"]},index=[1,2]) $ df2 = pd.DataFrame({"C":["C1", "C2"], $                     "D":["D1","D2"]},index=[1,2]) $ pd.concat([df1,df2], axis="col")
train_data['vehicleType_int'] = train_data['vehicleType'].apply(get_integer2) $ test_data['vehicleType_int'] = test_data['vehicleType'].apply(get_integer2) $ del train_data['vehicleType'] $ del test_data['vehicleType']
actual_payments_combined[actual_payments_combined.residual_principal_amount_borrower.isnull()].head().T
routerID = 'default/' # $ base_url += routerID $ print(base_url)
sp500.at['MMM', 'Price']
link = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ r = requests.get(link, auth=('user', 'pass'))
warm.value_counts()
jan_2015_frame.head()
keep_types = [u'WWW', u'WND', u'WAS', u'SUN', 'DPV', u'NUC', u'NG', $        u'PEL', u'PC', u'OTH', u'COW', u'OOG', u'HPS', u'HYC', u'GEO'] $ keep_cols = ['generation (MWh)', 'total fuel (mmbtu)', 'elec fuel (mmbtu)', $              'all fuel CO2 (kg)', 'elec fuel CO2 (kg)'] $ eia_total_monthly = eia_total.loc[(eia_total['type'].isin(keep_types))].groupby(['type', 'year', 'month'])[keep_cols].sum()
adopted_cats = cats_merge.loc[cats_merge['Outcome Type']=='Adoption']
df4 = bg_df2[['Date', 'BG']].copy() $ df4 # whew, this way works fine $
url_df_full.head()
dir = "/Users/Collier/Dropbox/Skills/Python/Projects/Real_Estate/htx_crime_scraping/months/" $ dir_hcad = "/Users/Collier/Downloads/Real_acct_owner (3)/"
import seaborn as sns; $ titanic = sns.load_dataset('titanic') $ titanic.head()
data[['Sales']].resample('D').mean().rolling(window=15).mean().plot()
finalGoodTargetUserItemInt.to_pickle('datasets/goodNonColdTargetUserInt.pkl')
df_new['US_ab_page'] = df_new['US'] * df_new['ab_page'] $ df_new['UK_ab_page'] = df_new['UK'] * df_new['ab_page'] $ df_new.head()
converted_treatment = df2.query('group == "treatment"')['converted'].mean() $ print("Given that an individual was in the control group, the probability they converted is{0: .4} ".format(converted_treatment))
print('Number of class 1 :',sum([1 for y in df_2017['label'] if y == 1.0 ])) $ print('Number of class 0 :',sum([1 for y in df_2017['label'] if y == 0.0 ])) $
df.loc[:,"message"] = df.message.apply(lambda x : str.lower(x)) #Lower capital letters.
len(df_sched), len(schedId_in_proj), len(schedId_in_bud)
for field_name, dtype in df.dtypes[df.dtypes == 'object'].items(): $     print(field_name) $     df[field_name] = pd.Series(pd.Categorical(df[field_name]).codes) $
df.head()
P(V|C)/P(V)
print("Rows: {}".format(train.shape[0])) $ print("Columns: {}".format(train.shape[1]))
!h5ls -r 'data/my_pytables_file.h5'
df_mes = df_mes[df_mes['tip_amount']>=0] $ df_mes.shape[0]
df.query('converted == 1').user_id.size / df.user_id.size
source_counts = clinton_df['source'].value_counts() $ sns.set_style("whitegrid") $ sns.set_palette("deep") $ source_counts.plot.barh()
df.columns = [column.strip() for column in df.columns] $ df.columns
np.exp(results.params)
tweet_archive_enhanced_clean = tweet_archive_enhanced
data['2015']
df2['intercept']=1 $ df2[['control','treatment']] = pd.get_dummies(df2['group']) $ df2.head()
x =  store_items.isnull().sum().sum() $ print('Number of NaN values in our DataFrame:', x) $
len(youthUser2018)
backup = clean_rates.copy() $ clean_rates = clean_rates.loc[clean_rates.retweeted_status_id.isnull(),:] $ clean_rates = clean_rates.drop(['retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'], axis=1) $ clean_rates = clean_rates.merge(image_clean, how='inner', on='tweet_id')
extractor = twitter_setup() $ sname = "realDonaldTrump" $ tweets = extractor.user_timeline(screen_name=sname, count = 200) $ print("Tweets Extracted: {}.\n".format(len(tweets)))
from sklearn.preprocessing import MinMaxScaler $ scaler = MinMaxScaler() $ scaler.fit(X_train) $ scaled_X_train = scaler.transform(X_train) $ X_train = pd.DataFrame(scaled_X_train, columns=X_train.columns)
apple.dtypes
print('Shape : ', pd_train_filtered.shape) $ pd_train_filtered.sample(10)
titanic_dummy_df.head()
with open('df_3.out','w') as outFile: $     pickle.dump(fullDf,outFile)
treatment_group = df2.query('group == "treatment"') $ pconversion_treatment = treatment_group['converted'].mean() $ pconversion_treatment
import pandas as pd $ import numpy as np $ %matplotlib inline
tweet_archive_clean['tweet_id'].isin(tweet_image_clean['tweet_id']).value_counts() $
bd.reset_index()
url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=featured#submit' $ browser.visit(url) $ time.sleep(3)  #allow time for page to load
df_countries = pd.read_csv('countries.csv') $ df_countries.head(1) $ df_countries.country.unique() $ df_dummy = pd.get_dummies(data=df_countries, columns=['country']) $ df_countries.country.value_counts()
class_merged['month_year']=100*class_merged['date'].dt.year+class_merged['date'].dt.month $ monthyear_group=class_merged.groupby(['month_year'],as_index=False) $ month_year_products=pd.DataFrame(monthyear_group['no_items'].agg('sum')) $ month_year_products=month_year_products.sort_values(by='month_year') $ month_year_products.plot('month_year','no_items',kind='bar', color='r', figsize=(15,5))
print('The number of unique users is {}'.format(df.user_id.nunique()))
np.exp(results2.params)
lrf=learn.lr_find() $
archive_copy.loc[archive_copy['tweet_id'] == tweet_id_11, 'new_rating_denominator'] = 10 $ archive_copy.loc[archive_copy['tweet_id'] == tweet_id_13, 'new_rating_denominator'] = 10 $ archive_copy.loc[archive_copy['tweet_id'] == tweet_id_13a, 'new_rating_denominator'] = 10 $ print (archive_copy['new_rating_denominator'].value_counts()) $
fitA_Cli1 =  sma.Logit(endog=yA, exog=sma.add_constant(xA[['li1(-1)']])).fit() $ fitA_Cli2 =  sma.Logit(endog=yA, exog=sma.add_constant(xA[['li2(-1)']])).fit()
row_num = df.shape[0] $ row_num
b = pd.DataFrame(a) $ print b
prob = df2.groupby('group') $ prob_control = prob.mean()['converted']['control'] $ print("The probability of an individual converted, being in the 'control' group is - {}".format(prob_control))
autos["odometer_km"].unique().shape
images.info()
df2 = df2.drop_duplicates(['user_id'], keep = 'first') $ df2.info()
import pandas as pd $ import numpy as np
Base = automap_base()
run txt2pdf.py -o"2018-06-19 2015 CLEVELAND CLINIC Sorted by discharges.pdf"  "2018-06-19 2015 CLEVELAND CLINIC Sorted by discharges.txt"
%time df_columns['created_at'] = pd.to_datetime(df_columns['Created Date'], format="%m/%d/%Y %H:%M:%S %p") $
con_logit = sm.Logit(df2['ab_page'],df2[['intercept','converted','UK','US']]) $ result = con_logit.fit() $ result.summary()
print df_vow.index.weekday_name[:5] $
df2['user_id'].nunique() $
messy = pd.read_csv('data/weather_yvr_messy.csv') $ messy.head()
house_data['grade'].unique()
df.isnull()
np.exp(0.0506), np.exp(0.0408)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
gain_df.corr()
free_data.head() $ free_data.country.iloc[0]
w.cfg['indicators']['din_winter']=['DIN', 'SALT_CTD'] $ w.cfg['indicators'] $ if len(w.get_filtered_data(step = 1)[['DIN', 'SALT_CTD']].apply(pd.to_numeric).dropna(thresh= 2)) > 0: $     print(w.get_filtered_data(step = 1)[['DIN', 'SALT_CTD']].apply(pd.to_numeric).dropna(thresh=2)) $
rt_count.head(20).to_csv('retweets.csv')
model.most_similar("awful")
print(df_weather.shape,col_totals.shape,df_weather1.shape)
grpConfidence['MeanFlow_cfs'].count()
df1['Description'].value_counts().head(15)      
missing_nums = df.isnull().sum() $ print(missing_nums.iloc[missing_nums.nonzero()[0]])
motion_sensors_df = parsedDF[parsedDF['entity_id'].isin(motion_sensors_list)] $ motion_sensors_df['state'] = motion_sensors_df['state'].apply(lambda x: binary_state(x)) # Binarise $ motion_sensors_df= motion_sensors_df[motion_sensors_df['state']!=0]                      # Get only motion events $ motion_sensors_df.head()
ll
df['Complaint Type'].value_counts().head()
percentage = nypd_unspecified/nypd_complaints_total*100 $ print("%1.2f"%percentage)
df['Created Date'] = pd.to_datetime(df['Created Date'], errors='coerce') $ df.sort_values('Created Date', inplace = True) $ df['Week Number'] = df['Created Date'].dt.week $ df
users.columns
df.query ('group == "treatment" and landing_page != "new_page"')
Plot_Boxplot(rankings_USA['total_points'])
display('df1', 'df2', "pd.merge(df1, df2, on='employee')")
miss_grp2 = df.query("group =='control' and landing_page =='new_page'") $ print('The number of times a user from the control group lands on the new page is {}'.format(len(miss_grp2)))
five = [] $ unique_country = uni.groupby('country')['country'].count() $ for i in range(len(unique_country)): $     five.append(free_data[free_data['country'] == unique_country.index[i]])
import matplotlib.pyplot as plt $ plt.style.use('ggplot') $
train_embedding=pd.DataFrame(train_embedding)
life.polarity
tweets['splitText'] = tweets['text'].apply(lambda t : str(t).strip().split(' '))
my_image = tf.placeholder("uint8",[None,None,3]) $ slice = tf.slice(my_image,[10,0,0],[16,-1,-1])
len(active_psc_controls[active_psc_controls.nature_of_control.str.contains('trust')].company_number.unique())
desc_stats = desc_stats.transpose().unstack(level=0).transpose() $ desc_stats
grouped = data[['float_time','Agency']].groupby('Agency') $ grouped.mean().sort_values('float_time',ascending=False)
np.linspace(0, 100, 5)
(dummies_df.keys())
labels_ = np.array(is_duplicate, dtype=int) $ print('Shape of label tensor:', labels_.shape)
X = combine_X_data(train_projection, train_df.length, $                    train_df.paragraphs, train_df.num_words, train_df.spelling_errors, $                   train_df.average_sentence_length, train_df.sentence_length_variance) $ y = train_df.reward.values.astype(float) $ X,y
INT.head(1)
ls
pprint.pprint(parser.HHParser.format)
pass_students = grades[(grades.Mark >= 50)] $ fail_students = grades[(grades.Mark < 50)] $ print("PASS:", pass_students.ATAR.mean()) $ print("FAIL:", fail_students.ATAR.mean())
df_comments = df_comments.rename(columns={'_submission':'id', $                                          'body':'selftext'})
commits_per_day = git_timed.resample("D").count() $ commits_per_day.head()
autos.ad_created_year.value_counts(normalize=True)
price_bool1 = autos["price"].between(300, 100000, inclusive = True) $ autos = autos[price_bool1] $ print(autos.shape)
measurements_df.head()
import pandas as pd $ %pylab inline
df2.query('converted==1').user_id.count()/df2.user_id.count()
stocks.drop(['Week','Month'],axis=1,inplace=True)   # Dropping columns - new week column to be engineered
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
dfChile.shape
yhat = loansvm.predict(X_test) $ yhat [0:5]
merged=pd.merge(merged,oil,on='date', how='left') $ print("Rows and columns:",merged.shape)
p_value,z_score
all_data_wide = all_data_wide.reindex_axis(sorted(all_data_wide.columns), axis=1)
tweets = pd.read_csv("dataframe_terror.csv", parse_dates=[0]) $ terror = pd.read_csv('attacks.csv', parse_dates=[0]) $ terror["type"] = terror["type"].fillna("UnkownType")
[random_date(start="1/1/2012 1:30 PM", end="1/1/2019 4:50 AM") for _ in range(10)]
gdf_gnis.shape
twitter_ar['text'] = twitter_ar.text.apply(lambda row: removenotdogcolumns(row))
srf.rainfall.isel(time=50).plot()
pd.merge(df_a, df_b, on='mathdad_id', how='left')
access_logs_parsed = access_logs_raw.map(lambda x: parse_apache_log_line(x)).filter(lambda x: x is not None)
results.to_csv(path_or_buf=path + '/NFL_Fantasy_Search_2016_PreSeason.csv')
print ("Data Frame with Forward Fill limiting to 1:") $ df2.reindex_like(df1,method='ffill',limit=1)
dfjoined = dfrecent.merge(dfcounts, how = 'inner', on = ['created_date'])
from nltk.corpus import stopwords $ nltk_stops = stopwords.words()
df_goog.Close.resample('A').std().plot()
n_old = len(df2.query("landing_page == 'old_page'")) $ print('N_old is {}'.format(n_old))
max_value = rankings['cur_year_avg'].idxmax() $ rankings.iloc[max_value]
df1 = pd.DataFrame({"A":["A1", "A2"], $                     "B":["B1","B2"]},index=[1,2]) $ df2 = pd.DataFrame({"C":["C1", "C2"], $                     "D":["D1","D2"]},index=[1,2]) $ pd.concat([df1,df2], axis=1)
df2.query("group=='treatment' & converted==1").count()[0]/df2.query("group=='treatment'").count()[0]
df.head()
df_sb.head(2)
import datetime $ datetime.datetime(2015, 12, 31, 0, 0).strftime("%I:%M%p on %A %B %d, %Y")
ip_clean['p1'].str.replace('_', ' ').str.capitalize()
list(trump_tweets[0].__dict__.keys())
cont_prob = df2[df2['group'] == 'control']['converted'].mean() $ cont_prob
average_polarity=pd.concat([average_polarity_2012,average_polarity_2013,average_polarity_2014 $                            ,average_polarity_2015,average_polarity_2016], axis=1)
df_pr = pd.DataFrame([r for r in results if r is not None])
data['ATR5'] = (data['ATR'].rolling(min_periods=1, window=5).sum())/4 $ data.tail()
print('The difference between the two conversion rate:', p_new - p_old)
filename = processed_dir+'pulledTweetsProcessedAndClassified_df' $ gu.pickle_obj(filename,pulledTweets_df)
ks_name_success = ks_name_success.sort_values(by = ['counts'], ascending = False) $ ks_name_success.head(10)
autos = autos[autos['registration_year'].between(1900,2016)] $ autos['registration_year'].value_counts(normalize=True)
df_goog.index = df_goog.index.to_datetime() $ df_goog.index.dtype
import pandas as pd $ file_name = "data/survey_Brussels_Electricity_Water_projected_dynamic_resampled_bias_1000_{}.csv" $ sample_survey = pd.read_csv(file_name.format(2016), index_col=0) $ sample_survey.head()
csvData[csvData['street'].str.match('.*West.*')]['street']
is_aux=[] $ for i in range(0,l2): $     if i not in seq: $         is_aux.append(issue_types_num[i]) $ col.append(np.array(is_aux))
financial_crisis[[0,1]]
times=[] $ for df in dfs: $     times.append(df['Time'][df['Outside Temperature'] == df['Outside Temperature'].max()].values[0]) $ print (times)    
nb = MultinomialNB() $ nb.fit(X_train_dtm, y_train) $ y_pred_class = nb.predict(X_test_dtm)
offices = pd.read_csv('./data/Offices.csv')
!cat ../../data/msft_with_footer.csv # osx / Linux
len(index_p_p) $ l_names=l[l['index_P']==str(11313)]['lesks_name'] $ l_names
display(df_save.head(5)) 
f_counts_minute_ip = spark.read.csv(os.path.join(mungepath, "f_counts_minute_ip"), header=True) $ print('Found %d observations.' %f_counts_minute_ip.count())
origin=table.find(text='Flight origin').find_next('td').text $ origin
df2[df2['group']=='control']['converted'].mean()
m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), $                    0.04, 1, [1000,500], [0.001,0.01], y_range=y_range) $ lr = 1e-3
max_dif_Q5 = max(close.values()) $ min_dif_Q5 = min(close.values())
twitter_archive_full[twitter_archive_full.tweet_id.isin([778027034220126208, 786709082849828864, 680494726643068929])]['rating_numerator']
df_2010.dropna(inplace=True) $ df_2010
df_potholes = df[df['Descriptor'] == 'Pothole'] $ df_potholes.groupby(df_potholes.index.weekday).apply(lambda x: len(x)).plot()
import time $ import json
def load_weigths_into_target_network(agent, target_network): $
ab_df2.loc[ab_df2.user_id.duplicated(),:]
lr = LogisticRegression()
results = pd.read_csv('../data/result.csv', $                       low_memory=False      #This is required as it's a large file... $                      )
msft = pd.read_csv("msft.csv", $                     dtype = { 'Volume' : np.float64}) $ msft.dtypes
print ("The convert rate for  p_old  under the null is: {:.4f}".format(convert_p_old))
df_new['US_ab_page'] = df_new['ab_page'] * df_new['US'] $ df_new['UK_ab_page'] = df_new['ab_page'] * df_new['UK'] $ model3 = sm.Logit(df_new['converted'], df_new[['intercept', 'US_ab_page', 'UK_ab_page','ab_page', 'UK', 'US']]) $ results3 = model3.fit()
bnbA.shape
import statsmodels.api as sm $ convert_old = len(df2[(df2['landing_page']=='old_page') & (df2['converted']==1)]) $ convert_new = len(df2[(df2['landing_page']=='new_page') & (df2['converted']==1)]) $ n_old = n_old $ n_new = n_new
date_retweets_sample=date_retweets.sample(5000)
catalog_df.rename(columns={'title' : 'Course Name'})
generate_seq(model, proc, max_length, 'is')
n_old = df2.query('landing_page == "old_page"').shape[0] $ n_old
t.index = [101,102,103,104,105] $ t
duration = df['dauer'] $ duration.head()
vectorizer = CountVectorizer(analyzer=stemmed_words,min_df=3)
len(twitter_archive_df['retweeted_status_id'].unique())
print ("The Number of time the 'UK' Conversion is better than 'CA' : {} ".format(np.exp(0.0507))) $ print ("The Number of time the 'US' Conversion is better than 'CA' : {} ".format(np.exp(0.0408)))
grouped = retweets.groupby(['parentPostAuthor','parentPost']).size().reset_index() $ grouped.rename(columns={0:'counts'},inplace=True) $ grouped.sort_values('counts', ascending=False, inplace=True)
y, x = dmatrices('encoded_state ~ backers_count', kick_data_state, return_type="dataframe") $ x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training, 30% test $ x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5) # 15% test, 15% validation
%env lc_wrapper=4:4:4:4
df.to_csv('df_ancestry.csv') $ df_model = h2o.import_file(path='df_ancestry.csv')
plots.top_n_port_plots(10,traffic_type = 1) #Malicious traffic top 10 ports
df = pd.read_csv('ab_data.csv') $ df.head(5)
tweet_archive_clean.info()
unique_users
pd.concat([city_loc, city_pop], axis=1)
jobs.loc[(jobs.FAIRSHARE == 10) & (jobs.ReqCPUS == 1) & (jobs.GPU == 1)].groupby(['Group']).JobID.count().sort_values(ascending = False)
terror.loc[1387]
k_means = KMeans(init = "k-means++", n_clusters = 4, n_init = 12)
print(autos['price'].unique().shape) $ print(autos['price'].describe())
fig1 = (taxi_sample["tip_pct"] $          .to_pandas() $          .hist(figsize=(8, 4), range=[0, .4], density=True, cumulative=True) $ ) $ plt.show(fig1)
ds_train.Schema
yc.eval({y_true: uu})
tr_roll = roll_17.merge(roll_90, 'left', ['date', 'store_nbr']).merge(roll_360, 'left', ['date', 'store_nbr'])
unique_domains.sort_values('total_votes', ascending=False).head()
users = df.index.get_level_values('user').unique()
run txt2pdf.py -o "2018-06-12-1308 FLORIDA HOSPITAL ALL DRGs.pdf"  "2018-06-12-1308 FLORIDA HOSPITAL ALL DRGs.txt"
print('Proportion of converted users:',format(100*df.converted.mean(),'.3f'),'%')
vect_bow = CountVectorizer(ngram_range=(1, 1), stop_words = 'english') $ vect_cleaned_bow = CountVectorizer(ngram_range=(1, 1), stop_words = 'english') $ vect_stemmed_bow = CountVectorizer(ngram_range=(1, 1), stop_words = 'english')
df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})
import seaborn as sns $ import matplotlib.pyplot as plt $ %matplotlib inline
obs_diff = p_treat_conv - p_control_conv
conv_prob = df2['converted'].mean() * 100 $ output = round(conv_prob, 2) $ print("The probability of an individual converting regardless of the page they receive is: {}%".format(output))
conn = sqlite3.connect(os.path.join(outputs,'example.db')) $ c = conn.cursor()
pd.get_option("display.max_columns")
import datetime $ d2 = datetime.date.today() + datetime.timedelta(days=-30) $ datetime.date.today().weekday()
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=32000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
df.location_id.shift().head()
autos["odometer_km"].value_counts()
xml_in_sample.shape
clean_liverp['period'] = ' ' $ clean_liverp.head()
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','OriginationPhase']).closed_at.count() $ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'green', 'red', 'yellow'], grid=False) $
df_new.info()
tweets = pd.read_csv('Data/tweets2.csv', encoding='utf-8', parse_dates=True)
df_a=df.query("group=='control' and landing_page=='old_page'") $ df_b=df.query("group=='treatment' and landing_page=='new_page'") $ df_a.nunique()
y_train = np.array(test_df['Log_Price_Sqft'][mask]) $ y_test = np.array(test_df['Log_Price_Sqft'][~mask]) $ print len(y_train),len(y_test) $ print y_test
run txt2pdf.py -o"2018-06-19 2015 OROVILLE HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2015 OROVILLE HOSPITAL Sorted by discharges.txt"
ind = ['NY.GDP.PCAP.KD', 'IT.MOB.COV.ZS'] $ dat = wb.download(indicator=ind, country='all', start=2011, end=2011).dropna() $ dat.columns = ['gdp', 'cellphone'] $ print(dat.tail())
df2[df2.group == 'treatment'].converted.mean() $
print cust_data.drop_duplicates().head(3) $
model.doesnt_match("input is lunch he sentence cat".split())
litecoin_github_issues_df[['title', 'html_url', 'created_at','updated_at']].head()
df.to_csv('edited_ab_data.csv',index=False)
print(len(train_df['device'].unique()))
flight_delays.head()
features_to_use = ["latitude", "longitude", "price", $                    "num_photos", "num_features", "num_description_words" $                    ] $ features_to_use.append('manager_id')
f_close_clicks_app_train = spark.read.csv(os.path.join(mungepath, "f_close_clicks_app_train"), header=True) $ f_close_clicks_app_test = spark.read.csv(os.path.join(mungepath, "f_close_clicks_app_test"), header=True) $ print('Found %d observations in train.' %f_close_clicks_app_train.count()) $ print('Found %d observations in test.' %f_close_clicks_app_test.count())
InfinityWars = df[['text','lang','created_at','favorited']]
df = pd.DataFrame(file.generateRecords()) $ df.head()
user_logs = pd.read_csv('../input/user_logs.csv') # 392,106,544 rows --> nealy 400 millions rows
re_json = search_response.json()
non_zero = np.count_nonzero(test_set[1]) $ zero = len(test_set[1]) - non_zero $ print("validation set includes: {} non zero and {} items woth value zero".format(non_zero, zero))
df2_new_converted_rate =df2[df2.landing_page == 'new_page'].converted.sum()/len(df2[df2.landing_page == 'new_page']) $ df2_old_converted_rate =df2[df2.landing_page == 'old_page'].converted.sum()/len(df2[df2.landing_page == 'old_page']) $ (p_diffs > (df2_new_converted_rate - df2_old_converted_rate)).mean()
df2.info()
df = pd.DataFrame(A, columns=list('QRST')) $ df - df.iloc[0]
menu_dishes_about_latent_features_column_names = ['menu_id'] + labels_for_menu_dishes_about_latent_features
btc = ha.accounts.manual_current('bitcoin', path=os.path.join('data', 'manual_accounts'), $                                  currency='BTC') $ btc.add_transaction('20.04.2016', 'me', 'buy', 6.5382, t_type='buying rate: 391.3129 EUR') $ dep.add_account(btc)
pd.date_range('2017-12-30', periods=4, freq='h')  # 4 values using hourly frequency 
a,b = data.shape $ print('Number of merchants '+str(a)) $ print('Number of features '+str(b))
df_arch_clean['source'] = df_arch_clean['source'].astype('category')
first_result.find('a')
ab_data.user_id.nunique()
ctv = CountVectorizer() $ project_cats = train.project_subject_categories.apply(lambda x: scrub(x)) $ x = ctv.fit_transform(project_cats) $ train = train.reset_index
df
class_merged_hol=pd.merge(class_merged_hol,local_hol,on=['date','city'],how='left') $ print("Rows and columns:",class_merged_hol.shape) $ pd.DataFrame.head(class_merged_hol)
df.tail()
ax=gdp_df.plot(x='Year', y="Detroit GDP", title='Detroit GDP Recovery 2001-2016 \n', color='green') $ ax.set_xlabel("Year") $ ax.set_ylabel("Detroit GDP (in Millions USD)") $ ax.spines["right"].set_visible(False) $ ax.spines["top"].set_visible(False)
print(fdist.most_common(50))
ts.asfreq('B').fillna(0)
decoder_model_inference.save('decoder_model_inference.h5')
talks_train.shape
temp_df2.info()
dataA.columns = cols
requests.delete(BASE + 'networks/' + str(sample_network_suids[0]) + '/nodes')
dates_with_nulls=len(nulls['date'].unique()) $ all_dates=len(merged['date'].unique()) $ dates_with_nulls/all_dates
autos['price'].value_counts().head()
total = read_csv('mmenv01.csv') 
hsi["Date"] = pd.to_datetime(hsi.Date, format="%b %d, %Y", errors='ignore') $ hsi = hsi.set_index("Date")
data['Created Date']
df.to_csv('ab_edited.csv', index=False)
y_pred = ridgeregcv.predict(X_test) $ print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
df_495_table = pd.read_sql(sql_495_table,conn_laurel) $ df_495_table.sort_values(by='Values',ascending=False)
df_keys = tuple(appended_multiIndex) # keys gotta be a tuple, whatever that means $ ocean_df = pd.concat(appended_ocean_list, keys = df_keys, names=['ride_id']) $ motion_df = pd.concat(appended_motion_list, keys = df_keys, names = ['ride_id']) $ ocean_df.head()
import requests $ r = requests.get('https://en.wikipedia.org/wiki/Python_(programming_language)')
old_DUL_file = [f for f in os.listdir(old_folder) if re.match('.*Dul.*', f)][0] $ old_DUL_file = old_folder + "\\" + old_DUL_file $ old_DUL_file
iowa["Date"] = pd.to_datetime(iowa["Date"], errors='raise', format="%m/%d/%Y") $ iowa
df.shape[0] $
df1.head()
df[df.index.month.isin([12,1,2])].head()
data_vi['Weekday']
column_list = sheets['Erl'].columns ^ sheets['Tyler'].columns $ display(column_list)
class Insect(Bug): $     def __init__(self): $         super().__init__(name='insect') $ Insect()
print("{:5.3f}".format(123.4545454))
frame3
print len(word_freq_df) $ word_freq_df.head()
new_prob_mean = new_page_converted.mean() $ old_prob_mean = old_page_converted.mean() $ print(new_prob_mean) $ print(old_prob_mean) $ print(new_prob_mean - old_prob_mean)
usage_400hz_filter.head()
status = api.update_status(status='1, Updating using OAuth authentication via Tweepy!') $ type(status)
xgb_grid.fit(X_train, y_train) $
scr_churned_df.head()
print(sorted(df_mes['payment_type'].unique()))
matplotlib.style.use('ggplot')
df = pd.read_csv(chat_file, error_bad_lines=False)
Z = np.random.uniform(0,1,10) $ z = 0.5 $ m = Z.flat[np.abs(Z - z).argmin()] $ print(m)
cat_sandwich = reviews_without_rare_words(cat_sandwich,'reviews_token',rare_wrds)
yhat=neigh.predict(X_test) $ yhat[0:5]
now = datetime.now() $ print(now)
session.query(measurements.date)[-1]
bnb.groupby('language') $
wikiMarvelRequest = requests.get(wikipedia_marvel_comics) $ print(wikiMarvelRequest.text[:1000])
for element in y2: $     print element['id'] $     print(element['full_text']) $     print('--')
df_n_rows = df.shape[0] $ df_n_rows
df.select("column_name")
df.keys()
z_score, p_value = sm.stats.proportions_ztest([17489, 17264], [145274, 145310], alternative='smaller') $ (z_score, p_value)
pca = PCA(n_components=100)
day_access = log_with_day.groupBy('dayOfWeek').count().sort('dayOfWeek', ascending=True) $ day_access.show()
rng = pd.date_range(start = '7/1/2017', end = '7/21/2017', freq = 'B') $ rng
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin' $ S_distributedTopmodel.executable = excutable +'/summa.exe'
perf_train['Default'].value_counts()
from sklearn.feature_extraction import text $ a = frozenset(list(term_freq_df.sort_values(by='total', ascending=False).iloc[:10].index)) $ b = text.ENGLISH_STOP_WORDS $ set(a).issubset(set(b))
Results_ZeroFill = pd.DataFrame({'ID': Test.index, $                                 'Approved': prob1_ZeroFill}) $ Results_ZeroFill.head()
np.exp(0.0408)
inc = np.exp(0.0140) $ inc
n_old = df2.query('group =="control"').shape[0] $ n_old
tweet_df.text[4]
df2[df2.user_id == int(ruid)] 
rowsToSkip = list(range(28)) $ rowsToSkip.append(29)
control_new_page.set_index('timestamp') $ treatment_old_page.set_index('timestamp');
red_4.info()
len(gdf)
train.drop('second',1,inplace=True) $ train.drop('minute',1,inplace=True) $ train.drop('action',1,inplace=True) $
approved.loan_amnt.describe(), rejected.loan_amnt.describe()
import copy $ categorical_no_target = copy.deepcopy(categorical) $ categorical_no_target.remove("Attrition_numerical") $ categorical_no_target.remove("Attrition") $ categorical_no_target
df_links['link.domain'].value_counts().head(15)
417*0.061028*6.77
gh_hashlist.dropna(axis=0, how='all')
df4 # see df4 again, since it has date NOT as index $
import pandas as pd $ df = pd.read_csv("kc_house_data.csv") $ df.dropna(how='any',inplace=True)    #to drop if any value in the row has a nan $ df.head()
newdf.head(12)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new,n_old], alternative='larger') $ z_score, p_value
 print(deep_match_model.summary())
writer.save()
plt.subplots(figsize=(6, 4)) $ sn.barplot(train_session_v2['isNDF'],train_session_v2['show'])
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets']))) $ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets']))) $ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
df["tweet_source"].value_counts().head(10)
words_only_scrape = [term for term in words_scrape if not term.startswith('#') and not term.startswith('@')] $ print('The number of words only (no hashtags, no mentions): ', len(words_only_scrape))
df2.query('landing_page == "old_page"').converted.count()
x = soup2.find_all('div', class_='schedule-container')[0].select('.disabled') $ str(type(x[0].get('attr-mov'))) == "<class 'NoneType'>"
df['campaign_new']=df.apply(lambda x:process_campaign(x['campaign']), axis=1 )
plt.hist(p_diffs); $ plt.axvline(x = full_diff, color = 'red');
df_image_clean.loc[1024, :]
start = '2017-01-31' $ end = '2017-12-01'
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $
import pydata_simpsons $ df = pydata_simpsons.clean_simpsons_script_lines(df)
data_df = create_asset_id_dataframe(data_query) $ individuals_df = create_asset_id_dataframe(individuals_query) $ libraries_df = create_asset_id_dataframe(libraries_query)
from sklearn.decomposition import PCA $ pca = PCA(n_components=0.95) $ X_train_reduced = pca.fit_transform(X_train)
df_all_wells_basic.info()
results_df.to_csv(csv_file_name)
prob1_rf = pd.Series(x[1] for x in prob_rf) $ Results_rf10 = pd.DataFrame({'ID': Test.index, 'Approved': prob1_rf}) $ Results_rf10 = Results_rf10[['ID', 'Approved']] $ Results_rf10.head()
predictions_copy
sample = pd.Series(weather.TMAX.sample(frac=0.1, replace=False)) $ sample.mean()
results = [] $ for tweet in tweepy.Cursor(api.search, q='%23Australia').items(2500): $     results.append(tweet) $ print(len(results))
len(training), len(holdout)
data['popular'].shape
! mv data/yob2016.txt data/test_data.txt $ ! cat data/yob* > data/allnames.txt
suspects_with_25_2['day'] = suspects_with_25_2['timestamp'].dt.day
monthly_portfolio_var = round(np.dot(stock_weights.T,np.dot(monthly_cov_matrix, stock_weights)),2) $ monthly_portfolio_var
idxs = dataset.get_cv_idxs(n, val_pct=150000/n) $ joined_samp = joined.iloc[idxs].set_index('Date') $ samp_size = len(joined_samp); samp_size
meta = parse('metadata.json.gz')
bnb.columns $ type(bnb.gender)
lr_model_saga.fit(X_train, y_train)
train_dum_clean = train_dum[['ID'] + [col for col in train_dum.columns if 'Source_' in col or $                       'Customer_Existing_' in col]]
df2.head()
sum(data.tag.value_counts().unique())
df2.head()
r.loc['20170703', ['optimized_weight', 'weight']]
from textblob import TextBlob
cc['logclose']=np.log1p(cc['close']) $ plt.hist(cc['logclose']) $ plt.show()
random_integers.values
df2.query('group == "control"').converted.sum() / df2.query('group == "control"').shape[0]
np.random.seed(42) $ tf.set_random_seed(42)
df_TempIrregular.info()
df_mes[(df_mes['extra']!=0)&(df_mes['extra']!=0.5)&(df_mes['extra']!=1)&(df_mes['extra']!=1.5)].shape[0]
so.loc[so['favoritecount'].between(30, 40), 'title'::3].head()
html_table = df.to_html() $ html_table
pres_date_df.sort_values('start_time', ascending=True, inplace=True) $ pres_date_df.head(10)
df_train.loc[:, 'features'] = df_train.apply(lambda row: tweet_to_features(row['text']), axis=1) $ display(df_train.head())
1/np.exp(-0.0150) $
twitter_df_clean = twitter_df_clean.drop(['retweeted_status_id', 'retweeted_status_user_id','retweeted_status_timestamp'], axis=1)
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_mean, (1-p_mean)]) $ old_page_converted.mean()
old_p_c = np.random.choice([1, 0], size=n_old, p=[p_mean, (1-p_mean)]) $ old_p_c.mean()
sales.head()
manager.image_df.loc[1,:] #This image should be in both dataframes
scores[scores.IMDB == min_IMDB]
raw_data.columns
rating_and_retweet = twitter_archive_clean.copy() $ rating_and_retweet = rating_and_retweet.sort_values(by='score') $ rating_and_retweet = rating_and_retweet[:int(len(rating_and_retweet)*0.9)]
posts_groupby = posts_date_df.groupby([pd.TimeGrouper('1D', closed='left')])
user=tweet.author $ for param in dir(user): $     if not param.startswith("_"): $         print ("%s : %s" % (param, eval("user." + param)))
def analyst(x): $     if 'Analyst' in x: $         return 1 $     return 0 $ df_more['Analyst'] = df_more['Title'].apply(analyst)
closeSeriesR = closingPrices.resample('5D').ffill()
sns.distplot(df['score']);
merged
nba_df.index[0:5]
%run -i 'label_image/label_image.py' --graph='/tmp/output_graph.pb' --labels='/tmp/output_labels.txt' --input_layer='Mul' --output_layer='final_result' --input_mean=128 --input_std=128 --image='test/George-W-Bush.jpg'
overallYrSold = pd.get_dummies(dfFull.YrSold)
m.fit(lr, 3, metrics=[exp_rmspe])
df.count()
from sklearn.preprocessing import Binarizer, \ $     OneHotEncoder, PolynomialFeatures, StandardScaler, \ $     MinMaxScaler, RobustScaler
df_predictions_clean.head()
tweet_data = pd.read_json('tweet_json.txt', lines=True, encoding='utf-8')
df['label'] = 0
stores.columns
data['inday_icu_wkd'] = np.where(data.intime_icu.dt.weekday <= 4, $                                  'weekday','weekend') $ data['inday_icu_wkd'].value_counts()
conf_gain = x.loc[x['pred'] > 0.75] $ conf_gain = pd.DataFrame(conf_gain, columns=['pred','pred_std', 'Gain +1d']) $ len(conf_gain.loc[conf_gain['Gain +1d'] == True])/len(conf_gain)
mortraffic_byday.head()
import numpy as np $ np.random.seed(0) $ df = df.reindex(np.random.permutation(df.index))
tc2 = tc1[~tc1['ISBN RegEx'].isin(bad_tc_e_isbns)] $ tc2['ISBN RegEx'].size
plt.hist(p_diffs);
all_data_long['api'].value_counts(dropna=False)
from datetime import datetime
pmol.df[pmol.df['atom_type'] == 'O.2']
df = pd.read_sql('SELECT actor_id, first_name, last_name FROM actor WHERE first_name ilike \'Joe\'', con=conn) $ df
df.filter(items=["MES", "CODIGOCLIENTE"], axis=1)
old_page_converted = np.random.choice([0,1], n_old, p=[1-p_old, p_old]) $ old_page_converted
train_r_order = resturaunt_orders_count(train,38) $ test_r_order = resturaunt_orders_count(test,38)
titles = [] $ for item in soup.find_all('div',class_='text-section'): $     if item.div is not None: $         title = item.find_all('h3')[0].string $         titles.append(title)
today = pd.to_datetime("today") $ today
ts.shift(3, freq='M')
plt.hist(p_diffs) $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('Different between new_page_converted and old_page_converted');
from sklearn.cross_validation import cross_val_score $ scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10) $ print scores $ print scores.mean() $
All_tweet_data_v2=All_tweet_data_v2[All_tweet_data_v2.Stage!='None']
maxitems = 10 $ print("London tweets retrieve testing") $ print('----------------------------------') $ for tweet in tweepy.Cursor(api.search, q="place:%s" % place_id_L).items(maxitems): $     print(tweet.text)
parsed_sierra_df = pd.concat([sierra_df_new_cases, sierra_df_new_deaths_cum]) $ parsed_sierra_df.rename(columns={'date': DEFAULT_NAME_COLUMN_DATE, $                                  'variable': DEFAULT_NAME_COLUMN_DESCRIPTION, $                                  'National': DEFAULT_NAME_COLUMN_TOTAL}, inplace=True) $ parsed_sierra_df[DEFAULT_NAME_COLUMN_COUNTRY] = countries['sl']
sns.jointplot(x='figure_density', y='prosecution_period', data=figure_density_df[figure_density_df.figure_density < 5], kind="kde", color="green") $ plt.show()
print('looking for the duplicate entry') $ df2[df2.duplicated(['user_id'], keep=False)]['user_id']
df2['intercept'] = 1 $ df2.head()
from urllib.parse import urlparse
set(train_data['vehicleType'])
carmileage={} $ for brand in top20.index: $     mile=autos[autos["brand"]==brand]['odometer_km'].mean() $     carmileage[brand]=mile $ print(carmileage)   
df2['ab_page'] = df2['treatment'] $ df2=df2.drop(['control', 'treatment'],axis=1) $ df2.head()
for key in sorted(counter_clinton, key=counter_clinton.get, reverse=True): $     print(key,counter_clinton[key])
StockData.loc[StockData.Date < StartDate, 'Set'] = 'history' $ print("We now have {:,} rows marked as 'history'".format(len(StockData.loc[StockData.Set == 'history']))) $ print("We now have {:,} rows marked as 'train'".format(len(StockData.loc[StockData.Set == 'train'])))
df.to_csv('311_week_number.csv', encoding = 'utf-8', index = False)
sigma_est = sim_closes_kernel.iloc[-1].std() $ (call_kernel.iloc[-1].Prima-4.5*sigma_est*np.exp(-r*ndays)/np.sqrt(nscen),call_kernel.iloc[-1].Prima+4.5*sigma_est*np.exp(-r*ndays)/np.sqrt(nscen))
training.info()
df2.head(5)
for i in range(0,10): $     topics = model.show_topic(i, 10) $     print "%s: %s" %(i, (', '.join([str(word[0]) for word in topics])))
df_ml_690 = df.copy() $ df_ml_690.index.rename('date', inplace=True) $ df_ml_690_01=df_ml_690.copy()
dfHashtags.rename(columns={ 0:"date", 1:'word', 2:'latitude', 3:"longitude" }, inplace=True)
df = pd.read_csv("data_science_challenge_samp_18.csv", parse_dates=['order_date'])
tmp.full_text
a400hz = hc.table('asm_wspace.analoog_400hz_2017q4') \ $ .where("t between '{0}' and '{1}'".format(start_date, end_date)) \ $ .persist()
ranking_table = p_stats.groupby(['season', 'PLAYER_ID', "PLAYER_NAME"])[["MIN", "PTS", "AST", "BLK", "REB", "STL"]].mean().reset_index() $ games_played = p_stats.groupby(['season', 'PLAYER_ID', "PLAYER_NAME"])['GAME_ID'].count().reset_index()
x=DataFrameSummary(joined).summary() $ x.loc['missing'][x.loc['missing']>0] $ print('\n') $ joined.filter(like='Comp', axis=1).min() $ joined.filter(like='Promo', axis=1).min()
twitter_archive_clean['quick_url'] = twitter_archive_clean.text.str.extract('(https.+)', expand=True) $
import pandas as pd $ date = pd.to_datetime("4th of July, 2015") $ date
base_dir = "M:/Performance_Analytics/Training/" $ data_dir = base_dir + "Data/" $ irradiance_clear_file_path = data_dir + "White_Cross/WhiteCross_Irradiance_Clearsky_20180708.csv" $ irradiance_cloudy_file_path = data_dir + "White_Cross/WhiteCross_Irradiance_Cloudy_20180712.csv"
pickle_full = "sorted_stays.p"
s_n_s_df.columns
data[data['age'] < 30]['name']
print('Tokenization') $ en_df['tokens'] = en_df.combined.apply(lambda x: x.split(' ')) $ en_df['num_words'] = en_df.tokens.apply(lambda x: len(x)) $ en_df.head()
df4 = df.drop('Cabin', axis=1) \ $     .loc[lambda x: pd.notnull(x['Embarked'])] \ $     .fillna(30)
df2['intercept']=1 $ df2[['control','treatment']] = pd.get_dummies(df2['group']) $ df2.head()
intervention_test.dtypes
df2[df2["user_id"]== 773192] ### check if there is only one user with 773192
df.info()
joined['onpromotion']=joined['onpromotion'].astype(np.int8)
gDateEnergy_plot = gDateEnergy_content.count().unstack().plot.bar(stacked=True, figsize=(10, 10)) $ gDateEnergy_plot.set_title('Task Completion (Sorted by Energy Level)') $ gDateEnergy_plot.set_ylabel("Tasks Completed")
final_text_words = ['global issues', 'science', 'culture', 'new','humor','entertainment']
recentd =[] $ recentd =session.query(Measurement.prcp).\ $     filter(Measurement.date > '2016-08-24').\ $     order_by(Measurement.date).all()\ $
establecimientos_educativos = pd.read_csv('datasets/establecimientos-educativos.csv', sep=';', error_bad_lines=False, low_memory=False) $ establecimientos_educativos.info()
saved_model.meta.available_props()
df_control = df2[(df2['group'] == 'control')] # create new DF with only control group $ df_control['converted'].mean() #probablility of conversion for  is 0.1203863045004612
type(trump_tweets[0])
df
df.head(2)
for k in range(3): $     print(k,abc.columns[k])
from datetime import datetime $ import pandas as pd
df_t1 = df_closed[[u'Created Date', u'Close Date', u'Service Location']].reset_index(drop=True) $ df_t1.columns = ['Start', 'Finish', 'Task'] $ df_t1.head()
import numpy as np $ import pandas as pd
from sklearn.feature_selection import RFE $ rfe = RFE(lm,10)
new_page_sim = (np.random.choice([1, 0], size=n_new, p=[p_mean/100, (1-p_mean/100)])).mean() $ output1 = round(new_page_sim, 4) * 100 $ print(output1,'%')
client = WatsonMachineLearningAPIClient(wml_credentials)
pos_tweets2 = [ tweet for index, tweet in enumerate(data2['Tweets']) if data2['SA'][index] > 0] $ neu_tweets2 = [ tweet for index, tweet in enumerate(data2['Tweets']) if data2['SA'][index] == 0] $ neg_tweets2 = [ tweet for index, tweet in enumerate(data2['Tweets']) if data2['SA'][index] < 0]
url_weather = "https://twitter.com/marswxreport?lang=en" $ browser.visit(url_weather)
io2 = ioDF.copy()
train = pd.read_csv('data/train.csv') $ test = pd.read_csv('data/test.csv') $ store = pd.read_csv('data/store.csv') $
finnal_data = drop_duplicate(merge_data,20)
tweet_full_df['rating_denominator']=tweet_full_df['rating_denominator'].astype(float)
All_tweet_data_v2.rating_numerator[All_tweet_data_v2.rating_numerator>30].value_counts() $
speakers.to_json('speakers.json')
list(c.find())
intervention_train.dtypes
odometer_high = autos[autos['odometer_km'] >= 100000] $ odometer_mid = autos[(autos['odometer_km'] >= 50000) & (autos['odometer_km'] < 100000)] $ odometer_low = autos[autos['odometer_km'] < 50000]
advancedmodel = LogisticRegression() $ advancedmodel = advancedmodel.fit(advancedtrain, train["rise_nextday"])
grouped_dpt.aggregate(np.sum) # aggregate sum based on one group
print('First rows of dataset:') $ tw.head(2)
%matplotlib inline $ import h2o $ h2o.init()
churn_df.shape $
ghana.index = ghana['GMT']
df.head(2)
scores[scores.RottenTomatoes == scores.RottenTomatoes.max()]
subwaydf.columns = subwaydf.columns.str.strip()
qtrclosePrice = quarter['Close']
import pg8000 $ import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline $ plt.style.use('ggplot')
msft_monthly_cum_ret = msft_cum_ret.resample("M") $ msft_monthly_cum_ret[:5]
out_train = train[feature_for_geo] $ out_test = test[feature_for_geo]
to_be_predicted_Day3 = 21.37332097 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
sum(df2.converted == 1)/df2.shape[0]
smith = pd.read_csv('../Data/stephenasmith_tweets.csv') $ smith['label'] = 0 $ skip = pd.read_csv('../Data/RealSkipBayless_tweets.csv') $ skip['label'] = 1
url = form_url(f'actionTypes/{baseball_swing_action_type_id}/metricGroupTypes', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
tweet_table.head()
df3['intercept']=pd.Series(np.zeros(len(df3)),index=df3.index) $ df3['ab_page']=pd.Series(np.zeros(len(df3)),index=df3.index) $ df3.head()
frame.rank(axis='columns')
import pandas as pd $ import numpy as np $ import csv $ pokemon = pd.read_csv("pokemon.csv") $
labels_np
pd.DataFrame(mnnb.feature_log_prob_, columns = cvec.get_feature_names())
pats_chiefs_nov8_plays.head()
import pandas as pd, numpy as np $ import matplotlib.pyplot as plt $ periods=60 $ dates = pd.date_range('20130101', periods=periods) $ dates
dtObj = binary_sensors_df.iloc[0]['last_changed'] $ device_id = 'device_tracker.robins_iphone' $ print("At {} {} is {}".format(dtObj, device_id, get_device_state(parsedDF, device_id, dtObj)))
bird_data['timestamp'] = pd.Series(timestamps, index = bird_data.index) $ print(bird_data.timestamp.head())
df_unique_providers, df_DRGs = Create_Unique_Providers_and_df_DRGs( df_providers ) $ type(df_unique_providers)
from sklearn.model_selection import train_test_split $ train, val = train_test_split(sample, test_size=0.2, random_state=42)
new_page_converted = np.random.choice([0,1], size = (145310), p = [0.88, 0.12]) $ p_new = (new_page_converted == 1).mean() $
future_df.head()
primitives[primitives['type'] == 'transform'].head(10)
p_val = (null_vals > p_convert_obs_diff).mean() $ p_val
dfTrain = dfFull.iloc[:1460] $ dfTest = dfFull.iloc[1460:]
for v in cat_vars: joined[v] = joined[v].astype('category').cat.as_ordered()
sum(df['xseller'] == 1)/float(df.shape[0])
t.isnull().values.any()
autos["registration_year"].value_counts().sort_index().head(20)
df_movies[df_movies.movieId == "907474"]
openHouse = openstates[openstates["chamber"] == "lower"] $ legHouse = leg[leg["filerHoldOfficeCd"] == "STATEREP"]
list(data.items())
base1 = df1[['placeId', 'hashtags']].groupby('placeId').aggregate(lambda x: [i for l in x for i in l ])
url='https://www.twitch.tv/directory' $ html=requests.get(url).content $ soup=BeautifulSoup(html,'lxml')
plt.plot(x, y) $ X=np.linspace(1,len(x),len(x)) $ plt.plot(X, t0+t1*X, "r") $ plt.show()
from datetime import date $ goodTargetUserItemInt['weeknum']=[date.fromtimestamp(i).isocalendar()[1] for i in goodTargetUserItemInt['created_at']] $ print sorted(goodTargetUserItemInt['weeknum'].unique())
df2=df2.apply(qnt)
plt.hist(p_diffs); $ plt.axvline(obs_diff[0],color = 'red')
dfFull['GrLivAreaNorm'] = dfFull.GrLivArea/dfFull.GrLivArea.max()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative = 'larger') $ z_score, p_value
nba_df.sort_values(by = ["Tm.STL"], ascending = False)
flight_cancels.head()
autos.ad_created_month.value_counts(normalize=True) $
autos.columns
corn_vege.sum()
dfq115.groupby(['county','store_number',dfq115['date'].dt.month]).agg({'sale':['sum']})
sampled = cleaned_sampled_contirbutors_human_df_saved \ $   .withColumn( $     "Input_projects", $     cleaned_sampled_contirbutors_human_df_saved.Input_project_name)
df.ndim   # number of dimension
trump.tail()
df[df.handle == 'Donald J. Trump'].head()
typeof_city=pd.Series(total_ridepercity["index"]) $ typeby_city=pd.Series(total_ridepercity["type"]) $ colors =["lightcoral","lightskyblue","gold"] $ explode = (0.1, 0, 0)
import pandas as pd $ import numpy as np $ pd.options.mode.chained_assignment = None  # default='warn' $ baseball = pd.read_csv("Data/baseball.csv", index_col='id')
query = "SELECT cdb_cartodbfytable('new_york_new_york_points_int_ct10')" $
run txt2pdf.py -o"2018-06-14-1513 FLORIDA HOSPITAL - 2012 Percentiles.pdf"  "2018-06-14-1513 FLORIDA HOSPITAL - 2012 Percentiles.txt"
archive_clean.source.value_counts()
df2=pd.read_csv('C:\\Users\\Christopher\\Google Drive\\TailDemography\\csvFiles by year\\xCC2017x.csv') $ df2.loc[df2.Toes=='43085',]
df['open'].describe()
soup
import collections
[column.name for column in get_child_column_data(observations_node)]
pd.Series(clusters.value_counts()).plot.hist()
dta.describe()
datAll['zip'].value_counts(dropna=False)[0]/len(datAll)
treat_new_pageUU = df2.loc[(df2['group'] == 'treatment')] $ len(treat_new_pageUU[(treat_new_pageUU['converted']==1)] )/ treat_new_pageUU.shape[0]
best_svm_epoch = 13
git_blame['age'] = pd.Timestamp('now') - git_blame.timestamp $ git_blame.head()
stages_with_random_forest = stages + [rf]
for key, value in parse_dict.items(): $       parse_dict[key] = pd.read_json(df_us_[key].to_json(orient="records"))
train.question_score.corr(train.answer_score)
sl['mindate'] = sl.groupby('wpdx_id')["new_report_date"].transform('min') $ sl['maxdate'] = sl.groupby('wpdx_id')["new_report_date"].transform('max')
summary = records.describe(include='all') $ summary $
chk = joined.loc[joined['nat_hol']==1].head(); chk
import locale $ locale.setlocale(locale.LC_ALL, "en_GB.utf8")
Google_stock.corr()
top_songs[top_songs['Track Name'].isnull()]['Region'].unique() $
latest_df = latest_df.apply(id_annotation, axis=1)
rng = pd.date_range('1/1/2011', periods=72, freq='H') $ rng[:5]
from pyspark.sql.types import IntegerType $ df = df.withColumn("HOD", df["call_hour_of_day"].cast(IntegerType())).drop("call_hour_of_day")
df2.drop(labels=1899, axis=0, inplace=True) $ df2[df2['user_id']==773192]
pr_list = [(2017, x + 1) for x in range(60)]
actual_difference = df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean()
n_unique = len(df2.user_id.unique()) $ print("Number of unique IDs =",n_unique)
df[df['Complaint Type'] == 'Noise - Residential']['Descriptor'].value_counts().head()
hr["new charttime"]=hr["charttime"]-time_delta $ hr["new realtime"]=hr["realtime"]-time_delta $ hr.head()
tweet_archive_enhanced_clean['rating_denominator'] = tweet_archive_enhanced_clean['text'].str.extract('([0-9]+[.]*[0-9]*)\/(10)',expand = True)[1].astype('float') $
np.savetxt(outputs / 'data.dat', data)
jpl_url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars" $ browser.visit(jpl_url)
scaler = preprocessing.MinMaxScaler()
transactions.merge(users,how="outer",on="UserID")
import pandas as pd  # only import we'll need!
fig, ax = plt.subplots() $ df.groupby(['epoch', 'batch']).mean().plot(ax=ax) $ plt.show()
from mpl_toolkits.basemap import Basemap
ds[4].ix['2013-01-01']
siteCode = 'PR_SpringCreekStateRoad113' $ site = sites[sites['SamplingFeatureCode'] == siteCode]
classify_df = pd.get_dummies(adopted_cats, columns=['Intake Type', 'Intake Condition'])
df.affiliate_channel.value_counts()
n_old=count_pages[1]# this gives the count for old pages $ n_old
data['manufacturer'].value_counts().sort_index()
cbs_df = constructDF("@CBS") $ display(constructDF("@CBS").head())
inputs = tf.placeholder(tf.int32, [None]) $ targets = tf.placeholder(tf.int32, [None])
max_features = 1000 $ tfidf_vec = TfidfVectorizer(max_df=0.95, min_df=2, $                             max_features=max_features, stop_words="english") $ tfidf = tfidf_vec.fit_transform(comment_sentences) $ tfidf_feats = tfidf_vec.get_feature_names()
one_station.sort_values(by=['DATE', 'weekday'], inplace=True)
linear = linear_model.LinearRegression() $ linear.fit(X_20, y2) $ (linear.coef_, linear.intercept_)
p_diffs = np.asarray(p_diffs) # converting p_diffs to numpy array
print df2['# Of Positions'].unique() $
df_2015 = df[df['Date'].dt.year==2015] $ df_2016 = df[df['Date'].dt.year==2016]
forecast_column = 'Adj. Close' $ df.fillna(value=-99999, inplace=True) $ forecast_out = int(math.ceil(len(df)*0.01))
df_parsed = pd.read_json(df.to_json(orient="records"))
archive_clean.head()
weather.ice_pellets.value_counts()
transit_df = transit_df.drop_duplicates() $ transit_df = transit_df.dropna()
crimes.tail()
df["booking_application"].value_counts()
from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor
'this is {type} number {order}'.format( type="string", order="1")
print '1 hr ahead:', date_now + timedelta(hours=1) $ print '1 hr ahead:', date_now - timedelta(days=3) $ from dateutil.relativedelta import relativedelta $ print '1 hr ahead:', date_now + relativedelta(years=1) + timedelta(days=3, seconds=2)
del users['user_type']
sns.kdeplot(pm_final['obs_date'], shade=True).set_title("Distribution of cumulative date for readings")
a_set
finals['type'].value_counts()
a_tags = soup.find_all('a') $ for link in a_tags: $     print(link.get('href')) $
s_eastern + s_mountain
import pandas as pd # Pandas for Data Frame $ import numpy as np # NumPy for vetorial operations
print len(low_polarity) $ print len(mid_polarity) $ print len(high_polarity)
conn.columninfo(table=dict(name='iris_sql', caslib='casuser'))
date_df = pd.DataFrame({'date': date, 'count': count, 'duration': duration}) $ date_df.head()
df.loc[df.index[df.handle == 'Donald J. Trump'], 'label'] = 1
df_all_wells_wKNN_DEPTHtoDEPT = useDiffColNamesToFillInNA(df_all_wells_wKNN,colReplaceList)
plt.hist(house_data['price'])
train.head()
irr_df
bnbAx['country_code'] = bnbAx.country_destination.map({'other':0,'US':1,'FR':2,'IT':3,'GB':4,'ES':5,'CA':6,'DE':7,'NL':8,'AU':9,'PT':10})
print("modelType: " + saved_model.meta.prop("modelType")) $ print("runtime: " + saved_model.meta.prop("runtime")) $ print("creationTime: " + str(saved_model.meta.prop("creationTime"))) $ print("modelVersionHref: " + saved_model.meta.prop("modelVersionHref"))
import pandas as pd $ import numpy as np $ import matplotlib $ import matplotlib.pyplot as plt
finalDf['kmLabels'] = km_res.labels_ $ finalDf.head()
data_v = data[['VIOLATION DATE','VIOLATIONS']]
test = contest_data_sample.sample(False,0.002).select('end_customer_party_ssot_party_id_int_sav_party_id','sales_acct_id','decision_date_time').take(1) #Randomly picking an entry $ test[0]
mol = xyzfile_to_psi4mol('C3H10NO2+.xyz') $ e, wfn = psi4.optimize('pbe/sto-3g', molecule=mol, return_wfn='on')
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=9245)
control = df2[df2['group'] == 'control'] $ control_converted = control.converted.sum() $ prob_control_conversion = control_converted / control.shape[0] $ print(prob_control_conversion)
myseries['one']
commits_per_year = corrected_log.resample('AS',on='timestamp')['author'].agg('count') $ print(commits_per_year.head())
nba_df[nba_df.isnull().any(axis=1)].loc[:, ["Season", "Team", "G", "Opp", "Home.Attendance", "Referee3"]]
df['group'].value_counts()
levels = approved.purpose.unique() $ sns.countplot(y="purpose", data=approved) $ plt.show() $ levels
pst.template_files
print(result.summary())
active_with_return.iloc[:,0] = active_with_return.iloc[:,0].astype("int")
hdf5_file = h5py.File(refl_filename,'r') $ hdf5_file
logreg = LogisticRegression() $ logreg.fit(X_train,y_train) $ y_preds=logreg.predict(X_test) $ metrics.accuracy_score(y_test,y_preds)
yah.add_transaction('23.04.2007', agent='me', desc='buy', amount=145, $                     t_type='buying rate: 28.70 USD') $ yah.add_transaction('01.12.2008', agent='me', desc='sell (need cash)', amount=-45, $                     t_type='selling rate: 12.20 USD') $ yah.transactions
stopwords = get_stop_words('spanish') $ stopwords.append("rt") $ stopwords.append("https") $ stopwords.append("http") $ stopwords.append("co")
df[df['public']=='online'].count()[0]
count = wo_rt_df.groupby('username')['full_text'].count() $ more_100 = count[count > 100].index
s4.unique()
df2['tweet_id'][(df2['predict_1_breed'] == True)].count()/2075 $
from sklearn.model_selection import cross_val_score, StratifiedKFold $ from sklearn.tree import DecisionTreeClassifier $ from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier $ from sklearn.linear_model import LogisticRegression $ from sklearn.preprocessing import StandardScaler $
df.to_csv("C:/Users/cvalentino/Desktop/UB/Project/data/tweets_publics_ext.csv", sep=',')
vectorized_text_predict = vectorizer.transform(description_predict) $ vectorized_text_predict.toarray()
merged_df['team_id_x'].apply(lambda x: 1 if x.isnull() == False)
display(Markdown(q7c_answer))
to_be_predicted_Day1 = 51.40 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
data1 = pd.DataFrame(data=df) $ df = data1[['ml_id','topic','text']] $ import os
reg_mod_uk = sm.OLS(df_all['converted'], df_all[['UK_int', 'ab_page']]) $ analysis_uk = reg_mod_uk.fit() $ analysis_uk.summary()
week_end
def Plot_Boxplot(dataframe): $     return dataframe.plot(kind = "box")
data
frame3.values
df = pd.merge(df_demo, df_clinic, on='patient_id', how='left') $ df.shape
building_pa_prc_fix_issued=pd.read_csv('buildding_03.csv',parse_dates=['permit_creation_date','issued_date'])
P_new=df2['converted'].mean() $ print("convert rate for P_new under the null is {}" .format(P_new))
round((model_x.rsquared_adj), 3)
male_journalists_mention_summary_df[['mention_count']].describe()
import nltk $ hashtags_total = [hash_tag.lower() for hash_tag in hashtags_total] # list of total hashtags $ hashtags_dist = nltk.FreqDist(hashtags_total) # term frequencies of hashtags $ hashtags_dist.most_common(10) # the most common 10 hashtags
method_label = train['signup_method'].unique() $ print(method_label)
df = pd.read_csv('twitter-archive-enhanced.csv') $ df.head()
model_data = clean_users.drop(['creation_time','last_session_creation_time', 'max_login_per_7'],axis = 1) $ model_data['account_life'] = model_data['account_life'].dt.days $ model_data['invited_by_user_id'] = model_data['invited_by_user_id'] > 0 $ model_data
stock.columns
run txt2pdf.py -o "BARNES JEWISH HOSPITAL title_page.pdf"  "BARNES JEWISH HOSPITAL title_page.txt"
store_items.insert(4, 'shoes', [8,5,0]) $ store_items
with open('ogh_meta.json','r') as r: $     meta_file = json.load(r) $     r.close() $ sorted(meta_file.keys())
df.shape
vessels.info()
df_cont = pd.read_csv("contact.csv", index_col=None)
X_train.shape, y_train.shape
ind.columns
df2.query('group=="treatment"')['converted'].mean()
url1 = 'https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-05-23&end_date=2018-05-23&api_key='+API_KEY $ r1 = requests.get(url1)
len(joined)
from pandas.tseries.offsets import BDay $ pd.date_range('2015-07-01', periods=5, freq=BDay())
p_value
df_goog['Year']   =  df_goog.index.year $ df_goog['Month']  =  df_goog.index.month $ df_goog['Day']    =  df_goog.index.day
gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_
cityID = '0562e9e53cddf6ec' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Santa_Ana.append(tweet) 
autos["price"].sort_index(ascending=False) #sorts according to index aka row positioning, so if its descending it will show from bottom up. $
print('Number of rows in dataset :: ',df.shape[0]) $ print('dataset information :: ') $ df.info()
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"').converted.sum() $ convert_new = df2.query('landing_page == "new_page"').converted.sum() $ n_old = df2.query('landing_page == "old_page"').shape[0] $ n_new = df2.query('landing_page == "new_page"').shape[0]
cig_data_SeriesCO.dtype
print(autos["price"].describe()) $ print(autos.loc[autos["price"]<300,"price"].shape) $ print(autos.loc[autos["price"]<300,"price"].value_counts(ascending = True).sort_index())
df2 = df2.drop(rep_row) $
logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','US','UK']]) $ result = logit_mod.fit() $ result.summary()
loans_df.shape
d=df2.query('group=="treatment"').converted.mean()-df2.query('group=="control"').converted.mean()
finals = pd.concat([finals, pd.get_dummies(finals['type'])], axis=1) $ finals.head()
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'], $                    'data': range(6)}, columns=['key', 'data']) $ df
google['high'].apply(custome_roune).value_counts().sort_index()
engine.execute("select * from station limit 5").fetchall()
from nltk import pos_tag $ sentence = word_tokenize('I always lie down to tell a lie.') $ pos_tag(sentence)
trump.info()
km = KMeans(n_clusters=5) $ modeler = km.fit(X_train) $ predi = km.predict(X_test) $ print predi
grouped_grades = final_grades.groupby("hobby") $ grouped_grades
b1 = total_base_dict_by_place.sort_values(by='base_twitter_count')
repaid_loans_cash[(repaid_loans_cash.fk_loan==36) & (repaid_loans_cash.fk_user_investor==51)].to_clipboard()
apple.resample('M').mean()
avg_delta_nbs = total_delta_nbs / len(hits_df) $ avg_delta_nbs
client
djb_url = 'http://www.sosuave.net/forum/threads/most-commonly-asked-newbie-questions-answered-right-here.56520/' $ djb_content = requests.get(djb_url) $ djb_save = 'djb.html' $ with open(djb_save, mode='w', encoding='utf-8') as f: $     f.write(djb_content.text)
df2['intercept'] = 1 $ df2= df2.join(pd.get_dummies(df2['landing_page'])) $ df2['ab_page'] = pd.get_dummies(df['group']) ['treatment'] $ df2.head()
cust_clust = crosstab.copy() $ cust_clust['cluster'] = c_pred $
test.head()
Combined_df
typesub2017.head()
import copy $ right = weather_df $ left = copy.deepcopy(crime_df.iloc[[100, 200, 300, 400, 500]]) $ left['Date_ TimeCST'] = left['Date_Time'].apply(lambda x: nearestDate(weather_df['Date_ TimeCST'], x)) $ left
s4g.groupby(['Symbol', 'Year', 'Month'],as_index=False).agg(np.mean)[:5]
monte.str.split().str.get(-1)
train.head(3)
autos["brand"].unique()
X = np.array(pcaData[selectedComponents]) $ y = np.array(data2Scaled['Label'])
import pandas as pd $ import numpy as np
print(classification_report(yy_test, knnmodel_predictionsX))
data['intercept'] = 1.0
!wget 'http://chianti.ucsd.edu/~kono/ci/data/deep-cell/go-sparse_original.cyjs' -O ./data/go-original.cyjs
data.index
owns = pd.read_pickle('data/pickled/new_subset_owns.pkl') $ stars = pd.read_pickle('data/pickled/new_subset_starred.pkl')
df2.query('landing_page == "new_page"').landing_page.count()/df2.landing_page.count()
def get_unique_words(word_list, existing_list): $     return(set(word_list)-set(existing_list))
autos['registration_year'].describe()
releases.iloc[0]
stocks = pd.concat([AAPL, GOOGL]) $ stocks
reviews['n']=0 $ reviews.groupby([reviews.country,reviews.variety]).n.count().sort_values(ascending=False)
reddit_info.to_csv('reddit_data_2.csv', encoding = 'utf-8', index = False)
archive_version = None  # i.e. '2016-07-14'
tickers = ["VALE3.SA", "PETR4.SA", "ITUB4.SA", "BBAS3.SA", "LREN3.SA"] $ start = '2010-01-01' $ end = '2017-11-14' $ data = pdr.get_data_yahoo(tickers, start=start, end=end)['Adj Close'] $ data.plot(); $
start = datetime.datetime(2012, 1, 3) $ end = datetime.date.today() $ amzn = web.DataReader("AMZN", 'morningstar', start, end)
try: $     df.select(col('nonsenseColumnName')) $ except: $     print("Doesn't exist, friendo")
import matplotlib.pyplot as plt $ %matplotlib inline $ plt.hist(stops["date"].dt.weekday, bins=6)
count_bldg_opps.head()
engine = create_engine("sqlite:///Resources/hawaii.sqlite")
df.head()
n_old = df2.loc[(df2.landing_page == "old_page")].user_id.nunique() $ n_old
rnd_search_cv.best_estimator_
prop = props [props.prop_name == 'PROPOSITION 063- FIREARMS. AMMUNITION SALES. INTIATIVE STATUTE.'] 
pred5 = nba_pred_modelv1.predict(g5) $ prob5 = nba_pred_modelv1.predict_proba(g5) $ print(pred5) $ print(prob5)
(taxiData2.Tip_amount < 0).any() # This Returns True, meaning there are values that are negative
grid_df.tail()
abc = Grouping_Year_DRG_discharges_payments.groupby(['year','drg3']).\ $ agg({'discharges':[np.size,np.sum, np.mean,np.max]}) $ abc.head()
data['fico_diff'] = data['fico_range_high'] - data['fico_range_low'] $ data[~data['last_pymnt_d'].isnull()].groupby(['loan_status']).agg({'fico_diff':'mean', 'member_id':'count'})
oecd_site = 'http://www.oecd.org/about/membersandpartners/list-oecd-member-countries.htm' $ pd.read_html(oecd_site)
result = c.update_one({'name.last': 'Bowie'}, $                       {'$set': {'albums': []}})
weekly.sort_values(by=['weekday'], inplace=True)
MATTHEWKW['SOCIAL_NETWORKS'] = MATTHEWKW.text.apply(lambda text: pd.Series([x in text for x in SOCIAL_NETWORKS]).any()) $ MATTHEWKW['DECISION_MAKING']  = MATTHEWKW.text.apply(lambda text: pd.Series([x in text for x in DECISION_MAKING]).any()) $ MATTHEWKW['ADAPTIVE_CAPACITY']  = MATTHEWKW.text.apply(lambda text: pd.Series([x in text for x in ADAPTIVE_CAPACITY]).any())
cr_new_null = len(df2.query('converted == 1')) / len(df2['converted']) $ print('The convert rate for P_new under the null is {}'.format(cr_new_null))
KKK = df.groupby(["Source"]).mean()["Compounded Score"] $ overallDF = pd.DataFrame.from_dict(KKK) $ overallDF["Compounded Score"]
y_preds = xgb_grid.best_estimator_.predict(X_test) $ xgb_scores = show_model_metrics('XGBoost', xgb_grid, y_test, y_preds)
bikes['hour_of_day'] = (bikes.start.dt.hour + (bikes.start.dt.minute/60).round(2)) $ bikes.head()
nt.set_index("Date", inplace=True)
df[df['Descriptor'] == 'Loud Music/Party'].index.weekday.value_counts().sort_index().plot(kind='bar')
donations["Donation Received Date"] = donations["Donation Received Date"].astype("datetime64[ns]")
BDAY_PAIR_df.set_index('uid',inplace=True)
df2.drop(labels=2893, axis=0, inplace=True)
who_purchased = pd.get_dummies(questions['purchased'])
import tensorflow as tf
print xs
speeches_metadata = speeches_features.merge(speeches_similarity_df, on = 'index')
trip_index_25_1
np.__version__
complete_df = complete_df.fillna(0)
april_acj_data.head()
df_image_clean['img_num'].value_counts()
%%bash $ head md_traffic.json
tesla['Close'].mean() # getting the mean by just calling the mean function
station_count = session.query(Station.id).count() $ station_count
df['upper'] = df['body'].apply(lambda x: len([x for x in x.split() if x.isupper()])) $ df[['body','upper']].head()
from sklearn.ensemble import RandomForestClassifier $ from sklearn.grid_search import GridSearchCV $ clf = RandomForestClassifier(class_weight={0 : 1, 1 : int(len(y_train)/sum(y_train))}) $ parameters = {'n_estimators' : [2**n for n in range(0,8)]} $ gscv = GridSearchCV(clf, parameters, scoring='precision')
train_df = pd.read_csv("train.csv", skiprows=range(1,159903891), nrows=25000000, dtype=dtypes) $ train_df.head()
df.loc[:,['A','B']]
ab_file2.drop(labels=2862, inplace=True) $ ab_file2[ab_file2['user_id']==773192]
tweet_df_polarity = my_tweet_df.groupby(["tweet_source"]).mean()["tweet_vader_score"] $ pd.DataFrame(tweet_df_polarity)
from test_package.print_hello_class_container import Print_hello_class # Import the class $ print_hello_instance = Print_hello_class()                             # Instantiate the class $ print(print_hello_instance.name)                                       # Print a property of the instance $ print_hello_instance.print_hello_method_within_class()                 # Call a method of the instance 
most_retweeted_tweeps_sorted=most_retweeted_tweeps_df.sort_values(by='number_of_retweets') $ most_retweeted_tweeps_sorted.tail(5)
from sklearn.model_selection import train_test_split
client.experiments.get_status(experiment_run_uid)['state']
slope, intercept
p_old = df2.converted.mean() $ p_old $
try: $     recipes = pd.read_json('data/recipeitems-latest.json') $ except ValueError as e: $     print("ValueError:", e)
from pandas.tseries.holiday import AbstractHolidayCalendar, Holiday, nearest_workday $ class myBirthDay(AbstractHolidayCalendar): $     rules = [ Holiday('Noushad Khan Birthday', month=8, day=20) ] $ myc = CustomBusinessDay(calendar = myBirthDay()) $ myc
tlen = pd.Series(data=data['len'].values, index=data['Date']) $ tfav = pd.Series(data=data['Likes'].values, index=data['Date']) $ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
val.isnull().values.sum() $
twitter_archive_master['rating_denominator'][918] = 10 $ twitter_archive_master['rating_numerator'][918] = 10
df2.drop_duplicates(subset='user_id', inplace = True) $ if (df2.shape[0] == df2.user_id.nunique()): $     print( "No Duplicate(s) found!") $ else: $     print("Duplicate(s) not removed!")
X = X[X.columns[rfe.get_support()]]
valid_probs = pd.DataFrame(columns=Y_valid_df.columns, index=Y_valid_df.index)
all_tables_df.loc[0]
print(vip_reason.sum(axis=0).sum()) $ print() $ print(vip_reason.sum(axis=0))
dr_num_providers = doctors['Provider'].resample('W-MON', lambda x: x.nunique())
stockdf['Stock'].unique()
weekday_df = pd.get_dummies(df_total.index.weekday, prefix='weekday') $ weekday_df.index = df_total.index
G = nx.DiGraph() $ loadInNodes(G) $ loadInEdges(G)
from dateutil.parser import parse $ parse(segments.st_time.iloc[0])
lm_2 = sm.Logit(new_df2['converted'], new_df2[['intercept', 'Middle', 'End', 'ab_page']]) $ reg_lm2 = lm_2.fit() $ reg_lm2.summary()
burgers = a * np.array([0.5, 0.0, -0.5]) $ x_axis = np.array([ 1, 0,-1]) $ y_axis = np.array([ 1, 1, 1]) $ z_axis = np.array([ 1,-2, 1])
obj = pd.Series([7, -5, 7, 4, 2, 0, 4])
print('Using database file ' + csvFileName) $ with open(csvFileName, 'r') as ifile: $     print(ifile.readline(), '\n', ifile.readline() )  #Print a couple of lines and reset the pointer. $     ifile.seek(0); $     dbData = pd.read_csv(ifile)  # Read in the whole file to a Panda Dataframe, handles NaN's nicely, see below. $
austin= austin.set_index('RIDE_ID')
masterList = {'lexieheinle': {}, 'realDonaldTrump': {}} $ def get_np_counts(tweet, profile): $     analysis = TextBlob(tweet) $     return analysis.np_counts $ print(masterList)
cross_val_score()
plt.figure(figsize=(16,8)) $ dendrogram(features['abc-news-au']['linkage'], orientation='top', $           p=300, truncate_mode='lastp', no_labels=True, color_threshold=0) $ plt.axes().get_yaxis().set_visible(False) $ plt.show()
y_test_array = y_test.as_matrix()
for dataset in combine: $     dataset['IsAlone'] = 0 $     dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1 $ train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()
pysqldf("select * from genes where genomic_accession = 'CP020543.1' and start > 3253706 and end < 3303410")
df.loc[df['survey'] == '201702']
df_archive_clean["name"] = df_archive_clean["name"].replace("a","None").replace("the","None").replace("an","None")
df3 = pd.DataFrame({'key': np.repeat(1, users.shape[0]), 'UserID': users.UserID}) $ df4 = pd.DataFrame({'key': np.repeat(1, products.shape[0]), 'ProductID': products.ProductID}) $ pd.merge(df3, df4,on='key')[['UserID', 'ProductID']]
df_state_victory_margins = pd.read_csv('./data/state_victory_margins_data.csv') $ df_state_victory_margins
masterList['lexieheinle']
financial_crisis.shape
df.min()  # default axis=0, column-wise
df_customers.head()
ls_other_columns = df_onc_no_metac.loc[:, ls_both].columns $ for column in ls_other_columns: $     print(column) $     df_onc_no_metac[column].unique()
df.loc['Mon', ['A', 'B']]
price_outliers = autos.loc[expensive, :].index
import pandas as pd $ git_log = pd.read_csv('datasets/git_log.gz', sep='#', header=None, encoding='latin-1', names=['timestamp','author']) $ print(git_log.head(5))
correct = tf.equal(y, y_pred, name="correct") $ accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")
find_most_similar(pandp_top_vec, all_top_vecs, title_lst, \ $                   vec_in_corp='Y', n_results=11)
df2= df_img_algo_clean.copy() $ df2.info() $ df2[(df2['predict_1_breed'] == False) & (df2['predict_2_breed'] == False) & (df2['predict_3_breed'] == False)]
df2.query('group == "treatment" & converted == 1').shape[0] / df2.query('group == "treatment"').shape[0]
p_old = round(float(df2.query('converted == 1')['user_id'].nunique())/float(df2['user_id'].nunique()),4) $ p_old
vow['Date'] = pd.to_datetime(vow['Date'])
df_final.head()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)
own_star.fillna(0, inplace=True) $ own_star.head(20)
example.head(5)
regressor2 = sm.Logit(df_new['converted'], df_new[['country_US', 'country_UK', 'intercept']]) $ result2 = regressor2.fit()
Z = np.random.uniform(-10,+10,10) $ print (Z) $ print (np.abs(Z)) $ print (np.ceil(np.abs(Z))) $ print (np.copysign(np.ceil(np.abs(Z)), Z))
df[(df['landing_page'] == 'new_page') & (df['group'] == 'control')].landing_page.count()
dif = new_page_converted.mean() - old_page_converted.mean() $ dif
consumerKey = 'XXXX' $ consumerSecret = 'XXXX'
autos = autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].value_counts(normalize=True, ascending=False).head(20)
files = [local_orig1, local_orig2] $ for file in files: $     with rio.open(file, 'r') as src: $         profile = src.profile $         print(profile)
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == 1').user_id.nunique() $ convert_new = df2.query('landing_page == "new_page" and converted == 1').user_id.nunique() $ n_old = df2.query('landing_page == "old_page"').user_id.nunique() $ n_new = df2.query('landing_page == "new_page"').user_id.nunique()
master_df.loc[master_df.favorite_count > 0, ['favorite_count']].boxplot();
df = pd.read_csv("/Users/peterjost/Downloads/12-classwork/classwork-12-311/data/311_Service_Requests_from_2010_to_Present.csv", usecols=["Created Date", "Closed Date", "Agency", "Complaint Type", "Descriptor", "Borough"]) $ df.head()
df2['ltv_bin_median'] = df2['ltv'].map(lambda x: 1 if x > 29 else 0) $ df2['ltv_bin_mean'] = df2['ltv'].map(lambda x: 1 if x > 42 else 0)
yc_new2 = yc_new1[['Fare_Amt', 'tripDurationHours', 'Trip_Distance', $        'Tip_Amt', 'income_departure', 'income_dest']] $ yc_new2.head()
heatmap(ddf)
breed_predict_df = pd.read_csv('image_predictions.tsv', sep = '\t') $
run txt2pdf.py -o "METHODIST HOSPITAL  Sepsis.pdf"   "METHODIST HOSPITAL  Sepsis.txt"
merged2['AppointmentDate'] = pd.to_datetime(merged2.index,format='%Y-%m-%d')
MEDHIST.head()
titanic3 = pd.read_excel('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls') $ titanic3.head()
cm = metrics.confusion_matrix(y_val, res) $ print(cm) $ print(classification_report(y_pred=res,y_true=y_val)) $ print(np.round(f1_score(y_pred=res,y_true=y_val),3))
open('test_data//write_test.txt', mode='w')
results.summary()
old_page_converted = np.random.choice([1,0], size=n_old, p=[p_old, 1-p_old]) $ old_page_converted
df_schools.describe()
backup = aux.copy()
assert pd.notnull(ebola_0).all().all()
subwaydf['4HR_Entries'].nlargest(20)
sqlContext.sql("select sum(count) from pcs").show()
df_new["CanadaNew"]=df_new["ab_page"]*df_new["CA"] $ df_new["UKNew"]=df_new["ab_page"]*df_new["UK"] $ df_new["USNew"]=df_new["ab_page"]*df_new["US"]
autos=autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].value_counts(normalize=True).sort_index(ascending=True).head(10)
p_new = df2['converted'].mean() $ print("Convert rate for p_new :", p_new)
def PlotCountsAboveBars(): $     ax = plt.gca() $     for p in ax.patches: $         ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), $             fontsize=12, color='black', ha='center', va='bottom')
print('dsFDR:\n%s\n\nBH-FDR\n%s' % (dd, dd2))
countries= pd.read_csv('countries.csv')
new_df = flatten_plot_df.loc[flatten_plot_df['tag'].isin(flatten_plot_df['tag'].value_counts()[:10].index)]
def returnCategory(x): $     return x.split(' ~ Category: ')[1] $ def returnName(x): $     return x.split(' ')[0]
df1 = pd.read_csv("mindbody_twitter_scores_monday.csv") $ df2 = pd.read_csv("mindbody_twitter_scores_tuesday.csv")
dfagency = pd.get_dummies(cp311['agency'])
plt.hist(p_diffs);
raw_full_df['building_id_iszero'] = raw_full_df.building_id == '0'
results_jar_rootDistExp, output_jar_rootDistExp = S.execute(run_suffix="jar_rootDistExp", run_option = 'local')
df = dfRegMet $ del dfRegMet
x = [1, 2, 3] $ y = [4, 5, 6] $ z = [7, 8, 9] $ np.concatenate([x, y, z])
first_row = session.query(Measurement).first() $ first_row.__dict__
!mkdir -p out/0 $ df.to_pickle('out/0/donations.pkl') $ noloc_df.to_pickle('out/0/donations_noloc.pkl')
df2.converted.sum()/df2.user_id.nunique()
StockData.count()
len(nt.catfathername.unique())
print("The number of duplicated pairs (user, artist):", newUserArtistDF.groupBy('userID','artistID').count().where('count > 1').count())
date + pd.to_timedelta(np.arange(12), 'D')
temp_df['reorder_interval_group'].replace('', np.nan, inplace=True) $ temp_df.dropna(subset=['reorder_interval_group'], inplace=True)
data.info() # .info method will tell theh datatype and null values of each of the column in the dataset
os.getcwd()
ALLbyseasons = offseason07.append([season07, offseason08, season08, offseason09, season09, offseason10, season10, offseason11, $                                   season11, offseason12, season12, offseason13, season13, offseason14, season14, offseason15, $                                   season15, offseason16, season16, offseason17, season17, offseason18]) $ ALLbyseasons.head() # Checks the headers
new_page_p = ab_df2.landing_page.value_counts()[0]/ab_df2.shape[0] $ old_page_p = ab_df2.landing_page.value_counts()[1]/ab_df2.shape[0] $ print(('new page probability', new_page_p),('old page probability', old_page_p))
borough_group = data.groupby('Borough') $ borough_group.size().plot(kind='bar') $
display('df6', 'df7', "pd.merge(df6, df7, how='left')")
(0.190/2)
bob_shopping_cart = pd.DataFrame(items, columns=['Bob']) $ bob_shopping_cart
pets.to_pickle('petpickle.pkl')
print(df_van_ip.shape, $ df_tor_ip.shape, $ df_mtl_ip.shape)
df2.ix[df2.assessments_mandatory == 'YES', 'assessments_mandatory'] = 1 $ df2.ix[df2.assessments_mandatory == 'NO', 'assessments_mandatory'] = 0 $ df2.willingtoRelocate=df2.willingtoRelocate.str.replace('Yes','1') $ df2.willingtoRelocate=df2.willingtoRelocate.str.replace('No','0')
part1_flt[['campaign_id', 'bid', 'matched_targeting']].drop_duplicates()
counts_df.describe()
from sklearn.naive_bayes import MultinomialNB
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.chdir('/Users/Vigoda/Library/Mobile Documents/com~apple~CloudDocs/CMS Project') $ print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $
twitter_df_clean.date.sample(2)
import itertools
c=table.find(text='Crew').find_next('td').text $ crews=re.search(r'\d+', c).group() $ crews
twitter_archive_master['rating'] = twitter_archive_master['rating_numerator']/twitter_archive_master['rating_denominator']
df_main.to_csv('twitter_archive_master.csv')
first_result.find('a') 
splits
precipitation_df = pd.DataFrame(DatePrcp)
grp = data.groupby(['product','issue'])['text'].count() $ grp['Bank account or service'].plot(kind='barh',title = 'Complaints Per Issue',color='black',figsize=(6,6)) $ plt.xlabel('# Of Complaints') $ plt.ylabel('Issue') $ plt.show()
df_raw[df_raw.list_date.isnull()]
afx_x_oneday = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2014-12-01&end_date=2014-12-01&api_key=F57SPh3gyaXMHjLAp-pY') $
lr = LinearRegression() $ cv_score = cross_val_score(lr, features_regress_vect, overdue_duration, scoring='neg_median_absolute_error', cv=5) $ 'MAE score: {:.3f}, std: {:.3f}'.format(np.mean(cv_score), np.std(cv_score))
df2.landing_page.value_counts()['new_page'] / df2.shape[0]
df2=pd.read_csv('edited_ab_data.csv')
donnees_alarm["TimeOn"][3448] $ df[0].index[28]
db.summarize(db.arrays.KG_VARIANT_EXAMPLE)[:]
delte1 = [] $ for i in or_list1: $     if(i not in and_list1): $         delte1.append(i) $
not_missing_values_pd = shelter_pd.loc[(shelter_pd['SexuponOutcome'] != 'Unknown') & (shelter_pd['SexuponOutcome'] != 'NaN')] $ not_missing_values_pd.describe()
f_new[['CA','US']]=pd.get_dummies(f_new['country'])[['CA','US']]
data.head()
feat_imp = pd.Series(XGB_model.booster().get_fscore()).sort_values(ascending=False) $ feat_imp.plot(kind='bar', title='Feature Importances') $ plt.ylabel('Feature Importance Score')
len(community_index)
iris.iloc[(iris.iloc[:,0]>7.5).values,4] $
csvData[csvData['street'].str.match('.*Avenue.*')]['street']
outliers = abs(daily_deltas - daily_deltas.mean()) > 2.5*daily_deltas.std()
p_overall = df2.converted.mean() $ p_overall
nth = 1 $ t = ttt[0::nth] $
print cust_data[-cust_data.duplicated()].head(5) $
df.shape
my_data.collect()
type(room_temp).__dict__['temperature'].__get__(room_temp,Celsius)
df2.loc['2016-09-18', ['GrossIn', 'NetIn']]
taxiData2.loc[taxiData2.Fare_amount <=0, "Fare_amount"] = 1
p_diff_actual = p_new_actual - p_old_actual $ p_diff_actual $ (p_diffs > p_diff_actual).mean()
opening
X.shape, y.shape
data['Created Date'].head()
from test_package import *
import statsmodels.api as sm $ mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = mod.fit() #Fitting the model
model.fit(X_train, y_train)
%%time $ turnstile_timeseries_list = [] $ for key, turnstile in turnstiles: $     turnstile_timeseries_list.append(get_time_series(turnstile)) $ print('Aggregated data for {} turnstiles.'.format( len(turnstile_timeseries_list)))
absorption.tallies
vio.head(3)
df12 = pd.read_csv('2012.csv')
datafile = "mlab_dtp_data_mlabnetdb.csv" $ df.to_csv(datafile)
df2[(df2['landing_page']=='new_page')].landing_page.count()/df2.landing_page.count()
props.prop_name.value_counts()
len(tuning_job_name)
grouped_dpt.nth(2) # nth row
gold = IMDB_df.groupby("movie_name") $ review_count = gold.movie_name.count() $ star_avg = gold.stars.mean() $ positive = gold.positive.mean()
twitter_df_clean = twitter_df.copy() $ image_pred_df_clean = image_pred_df.copy() $ tweet_json_df_clean = tweet_json_df.copy()
df_sched.fillna(value= '1900-01-01',inplace=True)
mean_weekday['Embarcadero at Sansome']
df.info()
plt.figure(figsize=(15,5)) $ plt.title("Number of mentions per day") $ plt.plot(num_of_mentions_by_day.values) $ plt.xticks(range(len(num_of_mentions_by_day)),num_of_mentions_by_day.index,rotation='vertical') $ plt.show()
df2['DepDelay'].describe()
result = [num for num in range(11)] $ print(result)
len(MultData)
sel_shopping_cart = pd.DataFrame(items, index = ['pants', 'book']) $ sel_shopping_cart
dictionary = corpora.dictionary.Dictionary.load(os.path.join(outputs, 'reviews.dict'))
import math $ def roundup_15k(x): $     return float(math.ceil(x / 15000.0)) * 15000 $ df['sale_price'] = df['sale_price'].apply(lambda x: roundup_15k(x)) $ df['sale_price']
pd.merge(msftAR0_5, msftVR2_4, how='outer')
price_df.to_csv('price2.csv')
daily_revenue = daily_revenue.select(col("date").alias("date"), col("sum(trans_amt)").alias("daily_rev_sum")) $ daily_revenue.show()
df.head()
data_filename = 'Red_Light_Camera_Violations.csv' $ data = pd.read_csv(data_filename, parse_dates=True)
aapl = getStockPrice("AAPL",1982, 1, 1, 2018, 1, 23) $ aapl.head() $
amanda_tweets = pd.DataFrame(amanda) $ amanda_tweets
pd.Series(df_train.values.ravel()).unique()
print(data.last_trip_date.iloc[0], type(data.last_trip_date.iloc[0]))
s = pd.Series([1, 2, 3, 4, 5, 6]) $ s
%%time $ df = spark.read.csv("output/metadata/part-*", sep="\t", header=False, schema=schema)
clf = RandomForestClassifier() $ clf.fit(x_train, y_train)
df2.groupby('group')['converted'].value_counts()/df.groupby('group')['converted'].count()
df_new[['ca', 'uk', 'us']] = pd.get_dummies(df_new['country']) $ df_new.head()
df['datetime'] = pd.to_datetime(df['datetime'],format=('%Y-%m-%d')) $ df.dtypes
stock['target'] = stock.daily_change.shift(-1) $ stock.head()
terrorism = text_file.filter(lambda t: is_interesting(t,bds)).take(10)
tables = pd.read_html(url) $ tables
def convert_to_sgt(s): $     tz = pytz.timezone('Asia/Singapore') $     dt = datetime.strptime(s, '%Y-%m-%d %H:%M:%S') $     dt.replace(tzinfo=pytz.UTC) $     return dt.replace(tzinfo=tz)
all_data = pd.concat(datasets) $ all_data.head()
merge[merge.columns[16]].value_counts().sort $
zero_rev_acc_opps.sort_values(by='Total Buildings', ascending=False, inplace=True)
sum(df2.duplicated('user_id'))
recortados = [recortar_tweet(t) for t in tweets_data_all] $ tweets = pd.DataFrame(recortados)
%load_ext watermark $ %watermark -d -u -p pandas,biopandas
user_clients = toggl.request("https://www.toggl.com/api/v8/clients")
stocks.plot(secondary_y = ["AAPL", "MSFT"], grid = True)
print('size of control group is {}'.format(len(df2_control))) $ print('size of treatment group is {}'.format(len(df2_treatment)))
train.male.value_counts()
import numpy as np $ import pandas as pd $ import glob $ import matplotlib.pyplot as plt $ import seaborn as sns $
groups = intervention_history.groupby('INSTANCE_ID')
(autos['date_crawled'] $  .str[:10] $  .value_counts(normalize=True,dropna=False) $  .sort_index() $ )
mismatch_group1 = df.query("group == 'treatment' and landing_page == 'old_page'") $ mismatch_groupp2 = df.query("group == 'control' and landing_page == 'new_page'") $ len(mismatch_group1) + len(mismatch_groupp2)
image_predictions_df.info()
print(crime_data["BORO_NM"].value_counts()) $ print(crime_data["DATE_TIME"].dt.year.value_counts())
temp_df['reorder_interval_group'].isnull().any()
s = lv_workspace.get_subset_object('B').indicator_objects['dip_winter'] $ s.get_filtered_data(subset = 'B', step = 'step_2')
with open('2015--Pascuet-M-I--Al--LAMMPS--ipr1.json', 'w') as f: $
from google.datalab.ml import TensorBoard $ OUTDIR='gs://{0}/taxifare/feateng2m'.format(BUCKET) $ print OUTDIR $ TensorBoard().start(OUTDIR)
y = df_cond['lead_result_max_bucket'] $ y = y.replace('max_below_5', 0).replace('max_5_plus', 1).replace('max_15_plus', 1)
station_availability_df.to_csv('station_availability_df')
obs_diffs=((new_page_converted/nnew)-(old_page_converted/nold)) $ print(obs_diffs)
significant_features[:10]
Count_Row=df.shape[0] $ Count_Col=df.shape[1] $ print("There are %s rows in this file" % (Count_Row)) $ print("There are %s columns in this file" % (Count_Col))
pd.Series( 1, index = ['a','b','c','d'])
n_old = df2[df2['group']=='control']['user_id'].count() $ n_old
month = month.sort_values(['LOCATION', 'DATE/TIME']) $ month = month.reset_index() $ month = month.drop('index', axis = 1) $ month
prediction_url = "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv" $ with open(prediction_url.split("/")[-1],"wb") as file: $     file.write(requests.get(prediction_url).content)
red_inter_recr = reduce_data(interactions_recruiter)
df_weather.dtypes
ben_final['isReverted'].value_counts()
eia_total_monthly.head()
r = requests.get(url_api) $ print(r.status_code)
predictions
new_page_converted=np.random.binomial(1,Conversion_Rate,Conversion_No) $
tweet_full_df['img_num']=tweet_full_df['img_num'].astype(int)
df_new[['canada','uk','us']] = pd.get_dummies(df_new['country'])
quadratic = [[x ** 2, x, 1] for x in np.arange(0, len(y))] 
print(len(pats_chiefs_nov8_tweets), "Unique Tweets on Gameday") $ print(len(pats_chiefs_nov8_plays), "Unique Plays on Gameday")
beta_GLD , alpha_GLD = np.polyfit(daily_returns['SPY'],daily_returns['GLD'],1) $ print(beta_GLD , alpha_GLD) $ daily_returns.plot(kind='scatter',x='SPY',y='GLD') $ plt.plot(daily_returns['SPY'],daily_returns['SPY']*beta_GLD + alpha_GLD,'-',color='r') $ plt.show()
times = pd.DatetimeIndex(df_en['date']) $ grouped = df_en['text'].groupby([times.hour, times.minute]).agg(['count']) $ grouped.plot() $ price_df.plot()
max_followers = DataSet['userFollowerCt'].max() $ print(max_followers)
df4.tail() # last 5 rows
merged.committee_position.value_counts()
df2[pd.get_dummies(df2['dow']).columns]=pd.get_dummies(df2['dow'])
forcast_out=int(math.ceil(.01*len(df1))) $ forcast_out $
df.head()
from sklearn.model_selection import train_test_split $ Xtr, Xts, Ytr, Yts = train_test_split(X, Y, train_size=500)
ocsvm_tfidf.fit(trump_tfidf, y = y_true_tfidf) $ prediction_tfidf = ocsvm_tfidf.predict(test_tfidf) $ prediction_tfidf
! python keras-yolo3/convert.py keras-yolo3/yolov3.cfg yolov3.weights keras-yolo3/model_data/yolo.h5 $
accepted_answer_ids = list(set(posts_df['AcceptedAnswerId'])) $ posts_df['post_label'] = [get_post_label(post_id,parent_id,accepted_answer_ids) for post_id,parent_id in zip(posts_df['Id'],posts_df['ParentId'])] $ posts_df['post_label'].value_counts()
test_portfolio[['date', 'ticker', 'weight', 'price', 'share', 'net']].head()
df2[df2['user_id'].duplicated()].index.values.tolist()[0]
soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]['href']
merged[merged['population'].isnull()].head()
combined_df.to_csv('manual_comment_results.csv', index=False)
p_old = df2['converted'].mean() $ p_old
from h2o.automl import H2OAutoML $ automl = H2OAutoML(max_runtime_secs = 60, seed = 12345) $ automl.train(x = predictor_columns, y = target, training_frame = loan_stats)
mean_encoding_test(val, train,'Block',"any_spot" ) $ mean_encoding_test(val, train,'DOW',"Real.Spots" ) $ mean_encoding_test(val, train,'hour',"Real.Spots" ) 
gdp = web.DataReader("GDP","fred",datetime.date(2012,1,1),datetime.date(2014,1,27)) $ gdp
df['Descriptor'].value_counts()
df_melt = pd.melt(frame=df, id_vars='name', $        value_vars=['treatment a', 'treatment b'], $        var_name = 'treatment', $        value_name = 'result') $ df_melt
(p_diffs > p_observed).mean()
df_archive_clean["timestamp"] = pd.to_datetime(df_archive_clean["timestamp"],format='%Y-%m-%d %H:%M:%S ',utc = True)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(dfX.set_index('user_id'), how='inner') $ df_new.head()
newmod = X_dunno $ newmod['y_pred'] = yhat_summore $ topten = newmod.sort_values('y_pred', ascending=False).head(10) $ topten.loc[:, (topten != 0).any(axis=0)] $
k1.head()
dfRegMet2014.shape
pizza_poor_reviews = cat_pizza[cat_pizza['ratings'] <= 4]
average_of_averages = df['average'].mean()
bacteria_data.treatment
actual_value_second_measure = holdout_results[holdout_results.second_measurement==1].\ $ groupby('wpdx_id').status_binary.sum() $ actual_value_second_measure.head()
rf_words = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1) $ scores = cross_val_score(rf_words, X, y, cv=skf) $ print "Cross-validated RF scores based on words:", scores $ print "Mean score:", scores.mean()
p_stats = p_stats.drop_duplicates()
! ls ./data/
print 'See correlation with actual: ',test_case.select('address1').take(1) $ actual_acct_id.select('address1').distinct().show(10,False)
naimp = NaImputer(df_airbnb)
cityID = '00ae272d6d0d28fe' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Lexington_Fayette.append(tweet) 
if tres.code == 200: $     tw = pd.read_csv(tres) # A HTTPResponse is an IO object $     print(tw.shape)
import pandas as pd $ import numpy as np $ import random $ import matplotlib.pyplot as plt $ %matplotlib inline
ideas = pd.read_csv('final_ideas_df.csv') # Compiled in Notebook 1 $ stocks = pd.read_csv('final_stocks.csv')  # Compiled in Notebook 3
df.location_id.shift(-1).head()
help(np.random.randn)
print(final_new_columns_names)
final_topbikes.groupby(by=final_topbikes.index.hour)['id'].count().plot(kind='bar', figsize=(12,6))
journalists_mention_df = mention_df.join(user_summary_df['gender'], how='inner', on='mention_user_id', rsuffix='_mention') $ journalists_mention_df.rename(columns = {'gender_mention': 'mention_gender'}, inplace=True) $ journalists_mention_df.count()
df_usa = df1[df1['Area'].isin(['United States of America'])]    #getting data where area is USA $ df_usa  = df_usa.set_index(['Year'])                            #setting Year as index
coefs = pd.DataFrame(gs.best_estimator_.coef_[0], index=X_train_all.columns, columns=['coef']) $ coefs['coef'] = np.exp(coefs['coef']) $ coefs = coefs.sort_values(by='coef', ascending=False) $ coefs.head()
sp_rec
for row in c.execute('SELECT * FROM stocks ORDER BY price'): $         print(row)
data = pd.DataFrame(data=[tweet.text for tweet in public_tweets], columns=['Tweets']) $ display(data.head(10))
tweets_df_clean.columns = ['tweet_id', 'retweet_count', 'favorite_count']
datetime_index = pd.to_datetime(df['created_at']).copy() # copy, because reference is deleted next $ df = df.set_index(datetime_index).drop('created_at', axis=1) # set index and drop redundant variable $ df.index.names = ['date_time'] # rename index $ df.head()
ls_not_numeric = [not pd.api.types.is_numeric_dtype(dtype) for dtype in df_with_metac_with_onc.dtypes] $ prog = re.compile('DATE[0-9]*$') $ ls_not_date = [not bool(prog.search(colname)) for colname in df_with_metac_with_onc.columns] $ ls_both = [num and date for num, date in zip(ls_not_numeric, ls_not_date)] $ df_with_metac_with_onc.loc[:,ls_both].nunique()
df = df[df.Address.notnull()]
data.fillna({'year': 2013, 'treatment':2})
total_N = num_base_twitters
hour.apply(pd.Timestamp('2014-01-01 22:00'))
dfLikes = pd.DataFrame(l)[["id", "name", "created_time", "link", "description", "category", "fan_count"]]
bg3.columns # find out what the column headers are
movie_ratings['year'].head(3)
len(df.index) - df.count()
[tweet.lang for tweet in tweet_list]
df2 = df[df['Title'].str.contains(blacklist) == False] $ df2 = df2.drop_duplicates(subset = 'Title', keep = False) $ df2 = multi_manufacturer_designer(df2, 'Title') $
df11 = pd.read_csv('2011.csv')
FREEVIEW.plot_heatmap(raw_freeview_df,raw_fix_count_df)
df_new.country.value_counts()
Quandl_DF = ql.get("BOE/XUDLHDS", authtoken="AUpKE1-xMYxathtHrkNK", start_date="1989-01-01", end_date="2017-12-31").reset_index() $ Quandl_DF.columns = ['Date', 'GBP_to_HKD']
def get_integer4(s): $     return fuelType_list.index(s)
autos["odometer_km"].describe()
r=df.resample('3D')
games['FG_PCT_HOME'].hist(label='FG', alpha=0.5) $ games['FG3_PCT_HOME'].hist(label='FG3',alpha=0.5) $ games['FT_PCT_HOME'].hist(label='FT',alpha=0.5) $ plt.legend()
np.random.normal(size=(2,3))
model2 = linear_model.LinearRegression() $ model2.fit(x2, y) $ (model2.coef_, model2.intercept_)
INC['dateShort'] = pd.to_datetime(INC['date_incid']) $ print INC.ix[:, 'dateShort'].head() $
weather_data2 = pd.read_csv('201408_weather_data.csv'); weather_data2.head()
with tf.Session() as sess: $     saver.restore(sess, "./my_model_final.ckpt") # or better, use save_path $     X_new_scaled = mnist.test.images[:20] $     Z = logits.eval(feed_dict={X: X_new_scaled}) $     y_pred = np.argmax(Z, axis=1)
parks_score_PSA = parks_sc.groupby(['PSA','FQ'])[['Score']].mean() $ parks_score_quar = parks_sc.groupby(['FQ','PSA'])[['Score']].mean() $ parks_score_quar.plot(kind='hist', bins=30);
injury_df.sort_values('Date', ascending=False).head()
df_arch = pd.read_csv("twitter-archive-enhanced.csv", sep= ",") $ id_list = df_arch["tweet_id"] $
importances.sort_values(by='Gini-importance', ascending=False)[0:50]
from urllib import URLopener $ URLopener.version
df_new['intercept'] = 1 $ log_mod_con = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'UK']]) $ results = log_mod_con.fit() $ results.summary()
p3_table = profits_table.groupby(['Segment']).Profit.sum().reset_index() $ p3_result = p3_table.sort_values('Profit', ascending=False) $ p3_result.head()
df['logViewsPercentChange']=df['Views-PercentChange'].apply(np.log)
cm = metrics.confusion_matrix(y_val, res) $ print(cm) $ print(classification_report(y_pred=res,y_true=y_val)) $ print(np.round(f1_score(y_pred=res,y_true=y_val),3))
ttarc_clean[(ttarc_clean.doggo == 3) & (ttarc_clean.pupper == 2)].head() $
fitted_pdf = norm.pdf(x,loc = parameters[0],scale = parameters[1])
data
from nltk.stem import porter $ from nltk.stem import LancasterStemmer $ import re
cols = ['chanel', 'retention'] $ users_visits = users_visits[cols] $ users_retention_per_chanel_path =  output_path + '\\users_retention_per_chanel.csv' $ users_visits.to_csv(users_retention_per_chanel_path)
total_df[['cat1','cat2','cat3']] = total_df['Categories'].apply(lambda x : pd.Series(x))
Y = np.ones((num_names,1)) $ Y[df['Gender'] == 'F',0] = 0
data2010 = final.query("year == 2010 & ages == 'total'") $ data2010.head()
tweet_json.describe()
grouped_by_dow_df = full_df.groupby('weekday')['listing_id'].count().reset_index().copy() $ grouped_by_dow_df.columns = ['day_of_week','count_of_listings'] $ grouped_by_dow_df['day_of_week'] = grouped_by_dow_df['day_of_week']\ $         .map( {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Sautrday', 6: 'Sunday'}).astype(str) $ grouped_by_dow_df.head(7)
df.loc[df['lead_mgr'].str.contains('Stanl'), 'lead_mgr'] = 'Morgan Stanley' $
tub['Qty']=tub['Qty']*tub['Proportion'] $ tub
test['question_dt'].describe()
df['device_class'] = df.device_type.apply(lambda d: classify_device_type(d)) $ df.drop(['device_family', 'device_id', 'device_model', 'device_type'], axis=1, inplace=True)
image_predictions.sample(5)
re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', cfd_index['pr'][13])
d
concat = pd.concat([ti_mar[ti_mar['store'] != 'Suning'],local_mar]).drop_duplicates(subset = ['review_id'],keep = 'first') $ concat.shape
demographics = my_df_free1.iloc[:,0:3] $ scores = my_df_free1.iloc[:,5:]
focount = final[['screenName', 'followersCount']]
temps_df.iloc[1]
fuelType_list = list(set(train_data.fuelType))
df.info()
dfs_morning.loc[dfs_morning['ENTRIES_MORNING']<=0, 'ENTRIES_MORNING'] = dfs_morning.ENTRIES_MORNING.quantile(.5)
sp_df_test = spark.createDataFrame(aggDF) $ sp_df_test.show(2, truncate=False)
train['weekend'] = (train.day >= 5).astype(int) $ train.groupby('weekend').popular.mean()
print('Click the + symbol above to add new cells, and the up and down arrows to re-order cells.')
import pymysql $ import pymysql.cursors $ import mysql.connector
np.datetime64('2015-07-04 12:59:59.50', 'ns')
len(adjectives)  # number of adjective tokens
Test.CorectionFactorSetup(MF1Gas='xenon', MF2Gas='Krypton', MF3Gas='Argon')
austin['miles']= austin['distance_travelled']*.000621371
dl = df.groupby('converted').count() $ dl $
class_merged_hol=pd.merge(class_merged,national_hol,on=['date'],how='left') $ print("Rows and columns:",class_merged_hol.shape) $ pd.DataFrame.head(class_merged_hol)
df_vow['Volume'].unique()
close_price = [float('nan') if item[4] is None else float(item[4]) for item in afx['dataset_data']['data']] $ diff_close = [(close_price[k+1] - close_price[k]) for k in range(len(close_price)-1)]
autos["price"].value_counts().sort_index(ascending=False).head(20)
plot_distplot(df_modified["modified_time"], (0, 200))
dfstationmeanlunch[['STATION','totals']].nlargest(10, ['totals'])
image_file.info()
shuffled = data.sample(frac=1) $
plt.rcParams['figure.figsize'] = [16,4] $ plt.plot(pd.to_datetime(mydf2.datetime),mydf2.fuelVoltage, 'g.'); $ plt.xlim(datetime.datetime(2018,2,8),datetime.datetime(2018,3,25))
df.info()
p_new_page = df2.query('landing_page == "new_page"').shape[0] / df2.shape[0] $ print('Probability of an individual received the new page: {}'.format(p_new_page))
(df.shape[0] - df2.shape[0]) == 3893
df = remove_data_outside_window(df)
img_url_cerberus = soup.find('div', 'downloads').a['href']
full = full.set_index('AdmitDate')
plot_ARIMA_model_save_fig(data=dr_num_new_patients, order=(2,1,2), start=start_date,\ $                 end=end_pred, title='Dr number new patients AR2/MA2', xlabel='Time', ylabel='Number of New Patients',\ $                               figname='./images/dr_number_new_patients_AR[2]IMA[2].png')
smpl.select('end_customer_party_ssot_party_id_int_sav_party_id').distinct().count()
autos['unrepaired_damage'].value_counts()
xml_in_sample.shape
df2[df2.duplicated('user_id',keep=False)==True]
users.gender.value_counts()
data.info()
type(g.get_group('106.152.115.161'))
result_df.head()
tablename='news' $ pd.read_csv(read_inserted_table(dumpfile, tablename), $             delimiter=",", $             error_bad_lines=False).head(10)
ks_ppb=pd.DataFrame(kick_projects.groupby(['category','launched_year','goal_cat_perc'])['pledge_per_backer','goal_reached'].mean()) $ ks_ppb.reset_index(inplace=True) $ ks_ppb.columns= ['category','launched_year','goal_cat_perc','avg_ppb','avg_success_rate'] $ ks_ppb[:2]
tobs_stn_281 = session.query(Measurement.date, Measurement.station, Measurement.tobs).\ $                 filter(Measurement.station == 'USC00519281').\ $                 filter(Measurement.date > '2016-08-23').\ $                 filter(Measurement.date <= '2017-08-23').all() $
session.query(func.count(station.id)).scalar()
new_page_converted = np.random.choice([0,1], n_new, p=[conversion_rate_all_pages, 1-conversion_rate_all_pages])
d = {'prediction' : pd.Series(predictions), 'SalePrice' : pd.Series(y_test)} $ pd.DataFrame(d)[:500].plot(y=['prediction', 'SalePrice'], figsize=(16,4), alpha=0.5)
ab_file.isnull().sum()
f_app_hour_clicks = spark.read.csv(os.path.join(mungepath, "f_app_hour_clicks"), header=True) $ print('Found %d observations.' %f_app_hour_clicks.count())
df_city[['Name', 'census_area', 'avg_elec_usage']].head(10)
van15_fin['stiki_percent'].value_counts()
bin_data
!wget -O drug200.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/drug200.csv
(data_archie['cur_sell_price'] == 0.0).sum()
ideas.drop(['Unnamed: 0', 'Week','Month'],axis=1,inplace=True)  # Dropping columns - new week column to be engineered
corr = data.corr()[["target"]].abs().sort_values(by="target", ascending=False).reset_index() $ corr.drop(0, inplace=True) $ fig = plt.figure(figsize=(20,5)); $ plt.xticks(rotation=90) $ sns.barplot(x="index", y="target", data=corr, ax=plt.subplot(111))
df_never_moved['Lat'].value_counts().head(10)
import pandas as pd $ import numpy as np $ from datetime import datetime
ppdb = rdb.conn(driver=rdb.drivers()[8], $                 server='Freshdb03', $                 database='permission_pass_db', $                 uid='sa')
df2.query('converted == 1')['user_id'].nunique() / total_users
tt1 = pd.read_csv('WebExtract.txt', names=['CAMIS', 'DBA', 'BORO', 'BUILDING', 'STREET', 'ZIPCODE', $                                           'PHONE', 'CUISINECODE', 'INSPDATE', 'ACTION', 'VIOLCODE', $                                           'SCORE', 'CURRENTGRADE', 'GRADEDATE', 'RECORDDATE'], $                  skiprows=1)
accepted_types = ['angel_group','corporate_venture_capital','family_investment_office','individual','micro_vc', $                   'venture_capital']
regime_orig = apple.ix[-1, "Regime"] $ apple.ix[-1, "Regime"] = 0 $ apple["Signal"] = np.sign(apple["Regime"] - apple["Regime"].shift(1)) $ apple.ix[-1, "Regime"] = regime_orig $ apple.tail()
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print(z_score, p_value)
store_items = store_items.rename(index = {'store 3': 'last store'}) $ store_items
log_mod2 = sm.Logit(df_joined['converted'], df_joined[['intercept', 'ab_page', 'CA', 'UK']]) $ results2 = log_mod2.fit() $ results2.summary()
((null_values<obs_diff).mean())
data = fat.add_sma_columns(data, 'Close', [6,12,200])
round(max(multi.handle.value_counts(normalize=True)),4)
new = pd.get_dummies(data = new , columns=['country'])
nposts = nposts[(nposts.index >= startDate) & (nposts.index <= endDate)] $ nposts = nposts.resample(sampling, how='mean') $ nposts.head()
carrierDF = trainDF[['UNIQUE_CARRIER', 'CARRIER_CODE']].drop_duplicates() # Only get unique examples
plt.figure(figsize=(15, 7)) $ df.followers.hist(log=True, bins=80);
tUnderweight = len(df[df['bmi']< 18.5]) $ tUnderweight
import mysql.connector $ from sqlalchemy import create_engine $ engine = create_engine('mysql+mysqlconnector://admin:geotwitter@geotwitter.uncg.edu:3306/geotwitter', echo=False) $ tweetsIRMA.to_sql(name='tweetIrmaTimes', con=engine, if_exists = 'append', index=False)
df.describe()
!h5ls 'data/my_pytables_file.h5'
knn_reg = KNeighborsRegressor(n_neighbors = 5) $ knn_reg.fit(x_train,y_train)
df['dateCreated'] = pd.to_datetime(df['dateCreated']) $ display(df.head(), df.dtypes)
precip_df.head()
cur_a.callproc('updateContactEmail', $                ('contact@sydney.hyatt.com', 'Hyatt Regency Sydney'))
num_df = pd.get_dummies(fraud_df[columns_convert],prefix = columns_convert) $ columns_num = ['delay_time','purchase_value','age','device_id_count','ip_address_count','signup_hour','signup_week','purchase_hour','purchase_week'] $ data_df = fraud_df[columns_num].join(num_df) $ feature_columns = columns_num + num_df.columns.tolist() $ target_column = 'class'
d1 = ['2 June 2013', 'Aug 29, 2014', '2015-06-26', '7/12/16'] $ ts3 = pd.DataFrame(np.random.randint(10, 100, (4,2)), index=d1, columns=list('ab')) $ ts3
logit_country = sm.Logit(merged['converted'],merged[['intercept','UK', 'US']]) $ result_merged = logit_country.fit()
df_comment['time']= df_comment.datetime.dt.hour $ df_comment['datetime_moscow']= df_comment.datetime +  pd.DateOffset(hours=15) $ df_comment['time_moscow']= df_comment.datetime_moscow.dt.hour
grid_search.best_score_  
frame.apply(f)
join_e.count()
free_data.groupby("country").mean() $
df_events.iloc[:,7].value_counts()
entities = df['ENTITY'] 
(df.converted==1).mean()
import pandas as pd $ import re $ from SE.sql import _get_DF
states.plot(kind='bar') $ plt.title('Counts for States of All Kickstarter Campaigns');
cityID = 'b463d3bd6064861b' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Detroit.append(tweet) 
plt.hist(p_diffs) $ plt.axvline(x=actual_diff, color='r', linewidth=2);
df
lm=sm.Logit(lr['converted'],lr[['intercept','ab_page']])
(obs_diff-np.mean(p_diffs))/np.std(p_diffs)
pickup_orders = data[['ceil_15min','pickup_cluster']] $ dropoff_orders = data[['ceil_15min','dropoff_cluster']] $ ride_orders = data[['ceil_15min','ride_cluster']]
cryptos[cryptos.percent_change_7d > 25]
%%HTML $ <blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> keeps disappearing as a trend and it&#39;s not because trump has gotten better as negotiating. <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/GOPShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#GOPShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/GOPShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#GOPShutdown</a> <a href="https://twitter.com/hashtag/GOPShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#GOPShutdown</a></p>&mdash; mach229 (@mach229) <a href="https://twitter.com/mach229/status/954503720934535168?ref_src=twsrc%5Etfw">January 19, 2018</a></blockquote> $ <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
clf_RGF_tfidf = RGFClassifier(max_leaf=240, $                               algorithm="RGF_Sib", $                               test_interval=100, $                               verbose=False,).fit(X_traincv_tfidf, y_traincv_tfidf)
print("Number of rows : " + str(df.shape[0]))
df.iloc[:,1:3] = df.iloc[:,1:3].apply(pd.to_datetime, errors='coerce') $ df['Trip_duration']=df['Lpep_dropoff_datetime']-df['lpep_pickup_datetime']
import nltk $ from gensim import models
df = pd.read_sql_query(sql_query,con) $
total_rows = len(train_results_df.index) $ predicted_accuracy = 1.0 - float(float(num_misses) / float(total_rows)) $ log.info("Number of Incorrect Predictions: {}/{}".format(num_misses, total_rows)) $ log.info("Predicted Accuracy was: {}%".format(predicted_accuracy))
dsi_me_1_df.to_csv('./dsi_me_1.csv', index_label='id')
df2 = df2[df2['timestamp'] != "2017-01-14 02:55:59.590927"] $ sum(df2['user_id'].duplicated())
regression_line = [(m*x)+b for x in xs] $ r_squared = coefficient_of_determination(ys,regression_line) $ print(r_squared) $
df.index
response = urllib.request.urlopen("https://api.iextrading.com/1.0/stock/nati/quote") $ str_response = response.read().decode('utf-8') $ obj = json.loads(str_response) $ obj
perc = lambda x: x/x.max() * 100 $ perc_df = cryptos.set_index('name')[['24h_volume_usd', 'market_cap_usd', 'price_btc']].apply(perc) $ perc_df.head()
plt.figure(figsize=(10,3)) $ plt.plot(dates,counts)
ts
comp_leader = pd.merge(competitions, leaderboard, on='ref') $ df = pd.merge(comp_leader, metrics, on='ref') $ df.head()
modern_train
pop = pd.Series([5.7, 82.7, 17.0], name='Population'); pop  #The descriptive name is optional.
datatest = pd.concat([train, test, rest])
ax2.plot(anomalies.index, anomalies.score, 'ko') $ fig
cvec = CountVectorizer(stop_words='english', ngram_range=(1,2)) $ hvec = HashingVectorizer(stop_words='english') $ tvec = TfidfVectorizer(stop_words='english', ngram_range=(1,2)) $ lr = LogisticRegression() $
df = pd.read_sql('SELECT AVG(p.amount) FROM payment p;', con=conn) $ df
np.random.rand(3,4)
dfRegMet.to_pickle(data+"dfRegMet.p")
hours = df4['Date'].dt.hour $ hours $
jh_data = pd.DataFrame(jh_data2.groupby('candidate_id').count()['employer']) $ jh_data.head(5)
min_date=min(merged['date']) $ max_date=max(merged['date']) $ print ("Min Date:",min_date) $ print ("Max Date:",max_date)
df_new['intercept'] = 1 $ df_new[['CA','UK','US']]= pd.get_dummies(df_new['country']) $ df_new = df_new.drop('CA',axis = 1) $ df_new.head(5)
df_location = pd.DataFrame.from_dict(df_us_.location.to_dict()).transpose() $ df_location.head(5) $
(station_max , count_max) = station_cnts_desc[0] $ print(station_max,count_max)
urls_pattern = re.compile(r'(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:\'".,<>?\xab\xbb\u201c\u201d\u2018\u2019]))')
titanic.groupby(['sex', 'class'])['survived'].mean()
combined_df3['vo_propdescrip'].str.upper();
param_test4 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]} $ gsearch4 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=80,min_samples_leaf=30,max_depth=11,min_samples_split=241, max_features='sqrt'),\ $                         param_grid = param_test4, scoring='log_loss',n_jobs=4,iid=False, cv=5) $ gsearch4.fit(drace_df[feats_used],y)
len(df.index)  # number of tweets with treatment
a = [0,1] $ ps_new = [p_new, 1-p_new] $ new_page_converted = np.random.choice(a, size=n_new, p=ps_new)
rs.best_params_
print('{} / {} '.format(data['TIDF Compliance'].notna().sum(), len(data['TIDF Compliance']))) $ print('{:.3f} % '.format(data['TIDF Compliance'].notna().sum() * 100 / len(data['TIDF Compliance']))) $ data[data['TIDF Compliance'].notna()]['TIDF Compliance']
df_plot
session = Session(engine)
reddit_comments_data = spark_hive.read.csv('/user/sohom/rc_2018_02.csv', header=True, mode="DROPMALFORMED", inferSchema = True)
dump.head()
df_all_columns.head()
for i, (label,col) in enumerate(agency_borough.iteritems()): $     print(i,label,col)
predicted = model.predict(test_data) $ score = metrics.accuracy_score(test_labels, predicted) $ print("Accuracy = {:.2f}".format(score))
client = MongoClient() $ db = client.test_database #acessa ou cria o banco $
df_predictions = pd.read_csv('image-predictions.tsv', sep='\t')
findNumbers = r'\d+' $ regexResults = re.search(findNumbers, 'not a number, not a number, numbers 2134567890, not a number') $ print(regexResults.group(0))
del df['Announced At']  # There is no difference between announced and created at so we can remove one of them
contribs.head()
rfecv.support_
access_token='751119440-JgiH9foYL9Dp4Eef5FZNsTcWYmlqZmM6wc0PxOW0' $ access_token_secret='dgLzy3Ht0kbLrxG8daPoZbBTEcEszY3UJIjFgMXJlkvle'
from sklearn.ensemble import GradientBoostingClassifier $ gbdt_params = { $ } $ gbdt_pipe = make_pipeline(IncidentPreprocessor(), GradientBoostingClassifier()) $ gbdt_grid = GridSearchCV(gbdt_pipe, param_grid=gbdt_params, cv=3)
df_ad_airings_4.to_pickle('./TV_AD_AIRINGS_FILTER_DATASET_3.pkl')
transfer_holidays=holidays_events[(holidays_events.type=='Transfer') & (holidays_events.transferred==False)] $ print("Rows and columns:",transfer_holidays.shape) $ pd.DataFrame.head(transfer_holidays)
users = df2['user_id'].nunique() $ users
pd.read_csv('../data/ebola/sl_data/2014-08-12-v77.csv').head()
df2['user_id'].count()
df = df.groupby(('C/A', 'UNIT', 'SCP', 'STATION', 'DATE', 'TIME')).sum().reset_index() $ df
lt = time.localtime() $ print('{}:{}:{}'.format(lt.tm_hour, lt.tm_min, lt.tm_sec)) $ print(time.asctime())
col_to_drop = ['text', 'extended_tweet','ideo_score', 'friends', 'followers', 'listed', 'screen_name', 'id', 'name'] $ clean_madrid = df_madrid.drop(col_to_drop, axis=1)
yhat_summore = modell.predict(X_dunno) $
depthdf = pd.DataFrame(depth_scores) $ depthdf.columns = ['max_depth', 'time_taken', 'num_cols', 'score']
confusion_matrix(y_test, lr_predicted)
Final_question_df = pd.read_csv('/Users/Gian/GitHub/Giancarlo-Programming-for-Analytics/Assignments/financial_table.csv')
ypred = model.predict(np.random.random((300, 95,1)))
try: $     donors_c['Donor Zip'] = donors_c['Donor Zip'].astype(int).astype(str) $ except: $     print("ValueError: invalid literal for int() with base 10: 'nan'")
big_df_count.to_csv('polarity_results_LexiconBased/season/polarity_count_season.csv',index=None) $ big_df_avg.to_csv('polarity_results_LexiconBased/season/polarity_avg_season.csv',index=None)
pets.to_csv('pets.csv')
rows_in_table = ins.count()[0] $ unique_ins_ids = len(ins["business_id"].unique()) $
weather_url = "https://twitter.com/marswxreport?lang=en" $ browser.visit(weather_url)
eligible_by_week = {} $ for key in set([x['first.comment.week.diff'] for x in eligible_comments]): $     eligible_by_week[key] = [x['author'] for x in eligible_comments if $                              x['first.comment.week.diff'] == key]
X_train.shape[1]
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ print("z score = {}".format(z_score)) $ print("p value = {}".format(p_value))
for c in ccc[:2]: $     spp[c] /= spp[c].max()
nba_df.values[0:3]
autos["price"].describe()
pd.DataFrame(dict(VMM=VMMmeans))
image_predictions.status_code
results_df.describe(percentiles=None, include=None, exclude=None)
aggreg.to_excel('aggreg.xlsx', index=False)
X.shape
print(autos["price"].describe()) #See the statistics of the price
df.columns
mlp_df.index
num_rows = df.shape[0] $ print(df.shape) $ print("There are {} many rows ".format(num_rows))
df.head()
from sklearn.linear_model import LogisticRegression
df2.query('group == "control"').converted.sum() $
columns_convert = ['source','browser','sex','high_risk_country','signup_date','purchase_date'] $ fraud_df[columns_convert].head(5)
len(image_predictions_df[(image_predictions_df.p1_dog == False) & $                          (image_predictions_df.p2_dog == False) & $                          (image_predictions_df.p3_dog == False)])
df_series
deployment_details = client.deployments.create(model_uid, 'Retail_Churn_with_XGBoost')
nfl.shape
temp_series_freq_2H = temp_series.resample("2H").min() $ temp_series_freq_2H
from sklearn.ensemble import RandomForestClassifier $ clf = RandomForestClassifier(max_depth=2, random_state=0) $ clf.fit(X_tr[0:len(X_train)-40-1], y_ls) $ print(clf.feature_importances_)
datacamp.head()
import subprocess $ out = subprocess.check_output(["./zhunt3", '24', '16', '16', 'SimianVirus40.txt'])
data['funny'].shape
twitter_df_clean.groupby('stage_name')['stage_name'].count()
table_names = [ $     'train', 'store', 'store_states', 'state_names', $     'googletrend', 'weather', 'test' $ ]
df_merged = pd.merge(df_clean, tweet_df_clean, on='tweet_id', how='inner') $ df_merged.info()
autos['registration_month'].hist()
params = {'figure.figsize': [4,3],'axes.grid.axis': 'both', 'axes.grid': True, 'axes.labelsize': 'Medium', 'font.size': 12.0, \ $ 'lines.linewidth': 2} $ plot_partial_autocorrelation(series=dr_new_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='PACF {}'.format('first difference of dr hours new patients')) $ plot_partial_autocorrelation(series=dr_existing_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='PACF {}'.format('first difference of dr hours existing patients'))
week34 = week33.rename(columns={238:'238'}) $ stocks = stocks.rename(columns={'Week 33':'Week 34','231':'238'}) $ week34 = pd.merge(stocks,week34,on=['238','Tickers']) $ week34.drop_duplicates(subset='Link',inplace=True)
session.query(func.min(measurement.tobs),func.max(measurement.tobs),func.avg(measurement.tobs)).\ $     filter(measurement.station == 'USC00519281').all()
four_d = dt.timedelta(days = 4) $ creations["creator_autoconfirmed"] = ( $     (creations["user_edit_count"] >= 10) & $     (creations["creation_timestamp"] >= (creations["user_registration"] + four_d)) $     )
print(dir(public_tweets[0]))
live_kd915_filt_df = pd.concat([live_df, kd915_filtered], axis = 0)
reliableData['ID'].unique()
age.loc['Alice']
tweet_json_filename = 'tweet_json.txt'
monthly_residuals = test_preds_monthly - test_target_monthly $ monthly_percent_off = ((test_preds_monthly - test_target_monthly) / test_target_monthly) * 100 $ annual_percent_off = ((test_preds_monthly.sum() - test_target_monthly.sum()) / test_target_monthly.sum()) * 100 $ print(monthly_percent_off) $ print(annual_percent_off)
train = train.merge(weather, on = train.date)
graph01 = df['complaint'].value_counts().head(3).sort_values(ascending=True).plot(kind='barh') $ print("Top 3 popular type of complaint\n") $ print(df['complaint'].value_counts().head(3).sort_values(ascending=False)) $ graph01
df_cat = pd.get_dummies(data[catFeatures])
files_lists = [f for f in os.listdir(master_folder + lists +  "unsubscribed/")]
year8 = driver.find_elements_by_class_name('yr-button')[7] $ year8.click()
data = pd.concat([approved, rejected], ignore_index=True)
merge.shape
monte.str.lower()
API_KEY = '8F4u-pRdtQsDCn-fuBZh'
print(f"Number of stations is {sq.station_count()}")
lin_pred = lin.predict(x_test) $ lin_pred[:5]
Jarvis_ET_Combine_Graph = Jarvis_ET_Combine.plot(color=['blue', 'green', 'orange']) $ Jarvis_ET_Combine_Graph.invert_yaxis() $ Jarvis_ET_Combine_Graph.scatter(xvals, df_gp_hr['Observation (aspen)'], color='black') $ Jarvis_ET_Combine_Graph.set(xlabel='Time of day (hr)', ylabel='Total evapotranspiration (mm h-1) ') $ Jarvis_ET_Combine_Graph.legend()
print(XGBClassifier.feature_importances_)
sim_diff = new_page_converted.mean() - old_page_converted.mean() $ print('The difference in the simulated conversion rates is {}.'.format(round(sim_diff,4))) $
twitter_df = twitter_df.groupby('Date').mean() $ twitter_df.head()
cc['date'] =  pd.to_datetime(cc['date'], format='%Y-%m-%d') $ cc.date.describe()
cityID = '0570f015c264cbd9' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         St_Louis.append(tweet) 
ensemble_1 = api.create_ensemble(train_dataset, { \ $     "balance_objective" : True}) $ api.ok(ensemble_1)
train_size = 12000 $ val_size = 1200 $ x_tr, y_tr = gen_dataset(train_size, N_CLASSES, invoices, targets, tokenizer) $ x_val, y_val = gen_dataset(val_size, N_CLASSES, invoices, targets, tokenizer)
dfWeek.set_index('Date').to_csv('PipelineInventorySales_Weekly.csv')
i1 ^ i2  # Difference
test_data
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], 0, 'smaller') $ z_score, p_value
m
df['lead_mgr'] = df['managers'].astype('str').map(lambda x:x.split('/')[0])
X.shape
print( "The best accuracy was with", mean_acc.max(), "with k=", mean_acc.argmax()+1) 
data['KEY2'] = data['C/A']+","+data['UNIT']+","+data['STATION']
max_features = 1000 $ tfidf_vec = TfidfVectorizer(max_df=0.95, min_df=2, $                             max_features=max_features, stop_words="english") $ tfidf = tfidf_vec.fit_transform(comment_column) $ tfidf_feats = tfidf_vec.get_feature_names()
tweets.retweet_count
all_df = train_df.append(test_df) $
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline $ df = pd.read_csv("../../Orb/EquipmentTask_LD118.csv")
lat = temp_nc.variables['lat'][:] $ lon = temp_nc.variables['lon'][:] $ time = temp_nc.variables['time'][:] $ temp = temp_nc.variables['air'][1,:,:] $ np.shape(temp)
last_day = '2017-08-23' $ one_year_ago = '2016-08-23'
import test_package.print_hello_class_container $ my_instance = test_package.print_hello_class_container.Print_hello_class() $ my_instance
recipes.iloc[0]
con.activity_type = con.activity_type.astype(str) $ con.activity_type = con['activity_type'].apply(str.lower) $ con.activity_type.unique()
temp_hist_data = pd.DataFrame(session.query(Measurement.tobs).filter(Measurement.station == "USC00519281").filter(Measurement.date > '2016-08-23').all()) $ temp_hist_data.head()
m = np.append(a,b).reshape(5,4) $ print("m: ", m)
tweet_data_v2.tweet_id.unique().shape
url_votes = grouped['net_votes'].agg({'total_votes': 'sum', 'avg_votes': 'mean'})    $
unique = df2.user_id.nunique() $ print("{} unique 'user_id' could be found in the dataset 'df2'.".format(unique))
print ( "The row information is given below:") $ duplicate
train = pd.merge(train, air_reserve, how='left', on=['air_store_id','visit_date']) $ test = pd.merge(test, air_reserve, how='left', on=['air_store_id','visit_date'])
train = data.groupby(pd.Grouper(freq='W'))['text'].apply(lambda x: "%s" % ' '.join(x)) $ train.tail()
combined_df2 = pd.merge(combined_df, balance_df,  how = 'left', left_on = 'acct_id', right_on = 'account_id', suffixes =['','_2']) $ combined_df2.shape
from ipywidgets import interact
data.sort_values('commits', ascending=False)
X[col_with_na].describe()
df_clean.source.head()
results.summary()
pop = {'Nevada': {2001:2.4, 2002: 2.9}, $       'Ohio': {2000: 1.5, 2001: 1.7, 2002: 3.6}}
df2.tail(2)
autos.columns
TEXT.numericalize([md.trn_ds[0].text[:12]])
a.upper()
df_arch_clean = df_arch_clean.drop('in_reply_to_user_id', axis = 1) $ df_arch_clean = df_arch_clean.drop('in_reply_to_status_id', axis = 1) $ df_arch_clean = df_arch_clean.drop('retweeted_status_id', axis = 1) $ df_arch_clean = df_arch_clean.drop('retweeted_status_user_id', axis = 1) $ df_arch_clean = df_arch_clean.drop('retweeted_status_timestamp', axis = 1)
df_master = df2.merge(df_select, on='id', how='outer')
o_page_converted = np.random.binomial(1, p_old, n_old) $ print('The old_page convert rate: {}.'.format(o_page_converted.mean())) $ print('The old_page convert rate: {}.'.format(round(o_page_converted.mean(), 4)))
lr.fit(X_train_dtm, y_train)
df = pd.read_csv('https://raw.githubusercontent.com/jackiekazil/data-wrangling/master/data/chp3/data-text.csv ') $ df.head(2)
apple["Regime"].value_counts()
from sklearn.grid_search import GridSearchCV $ param_test1 = {'n_estimators':range(20,81,10)} $ gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8),\ $                         param_grid = param_test1, scoring='log_loss',n_jobs=4,iid=False, cv=5) $ gsearch1.fit(drace_df[feats_used],y)
data.drop(['ceil_10min'], axis=1,inplace=True)
users= pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/users.csv' ) $ sessions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/sessions.csv' ) $ products =pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/products.csv' ) $ transactions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/transactions.csv') $
def calc_temps(start_date, end_date): $     return session.query(func.min(Measurements.tobs), func.avg(Measurements.tobs), func.max(Measurements.tobs)).\ $         filter(Measurements.date >= start_date).filter(Measurements.date <= end_date).all() $ temp_range = (calc_temps('2012-02-28', '2012-03-05')) $ print(temp_range) $
df_training = df_centered[df_centered.seq < df_centered.numOfReviews] $ df_test = df_centered[df_centered.seq == df_centered.numOfReviews]
import dllib.lr_sched as lr_sched
print community.modularity(politiciansPartyDict, Gun, weight='weight') $ print community.modularity(partitions, Gun, weight='weight') $ print len(set(partitions.values()))
r.json()
All_tweet_data_v2.rating_denominator.value_counts()
import matplotlib.pyplot as plt $ %matplotlib inline $ import seaborn as sns; sns.set() $ bikedataframe['predicted'] = y_predit $ bikedataframe[['count', 'predicted']].plot(figsize=(24,16),alpha=0.5)
for i in [203106, 203490, 201985, 201228, 200839, 101236]: $     print p_stats[p_stats.PLAYER_ID==i].PLAYER_NAME.unique()
df.info() #checking the dataset
(p_diffs<p_diffs_actual).mean()+(p_diffs>p_diffs+(n-p_diffs_actual)).mean()
train_df.shape
missing_conditions.sum()
submission_full = pd.concat([submission2, submission1], axis=0) $ submission_full.to_csv('lgbm.csv', index=False)
price_data = heading.append(price_data) $ price_data.columns = price_data.iloc[0]
print("Hello, World!")
print(train.age.describe()) $ print(len(train[train["age"]>90])) #jc: 1.2% of age above 90, drop these? team decision $ train[train["age"]>90] $
df
merged_portfolio_sp['Equiv SP Shares'] = merged_portfolio_sp['Cost Basis'] / merged_portfolio_sp['SP 500 Initial Close'] $ merged_portfolio_sp.head()
fin_pivot_table_tr = pd.pivot_table(fin_df, values = 'totalRevenue', index = ['symbol'], aggfunc = np.mean) $ fin_pivot_table_tr = fin_pivot_table_tr.rename(index=str, columns={"totalRevenue": "Avg Total Revenue"}) $ fin_pivot_table_tr.sort_values(by = ['Avg Total Revenue'], ascending = False)
unique_desc_loc=class_merged_hol['description_local_hol'].unique() $ print (unique_desc_loc)
autos['brand'].value_counts(normalize=True)
y.value_counts().values[0]/float(len(y))
df_ad_airings_5.to_pickle('./TV_AD_AIRINGS_STATE_METRO_AREA_5.pkl')
df2.user_id.nunique() 
git_blame['knowing'] = git_blame.timestamp >= six_months_ago $ git_blame.head()
df.to_csv('ab_data_mod.csv')
plt.figure(figsize=(10,3)) $ plt.plot(df_daily2['DATE'],df_daily2['DAILY_ENTRIES'])
%matplotlib inline $ AAPL.plot()
index_missin_hr0to6_before2016.shape
train.head(3)
merged[['CA', 'UK', 'US']] = pd.get_dummies(merged['country']) $ merged.head()
titanic[titanic.survived & (titanic.sex == 'female') & (titanic.age > 50)].head()
dta.info()
imgp_clean = imgp_clean.drop(imgp_clean[(imgp_clean.p1_dog == False) & (imgp_clean.p2_dog == False) & (imgp_clean.p3_dog == False)].index) $
fig = data["author"].value_counts()[:].plot.pie(labels = None) $ plt.show()
visualize_tree(observations_node, show_leaf_nodes=True)
print('Molecule ID: %s' % pmol.code) $ print('\nRaw MOL2 file contents:\n\n%s\n...' % pmol.mol2_text[:500])
xirr_actual[('rating_switch', 'rating_base')].head()
import requests $ import json
onlyfiles = [f for f in os.listdir(datapath2) if os.path.isfile(datapath2 / f) and f.endswith('.pdf')] $ onlyfiles.sort() $ print('Files in the folder:') $ for i, w in enumerate(onlyfiles): $     print(i+1, '--' ,w)
data[['Sales']].resample('M').apply(['median', 'mean']).head()
data.isnull().sum() # column name and number of nulls 
df_birth.describe()
voters.LastVoted.value_counts(dropna=False) $
n_old = df2[df2.group == "control"].count()[0] $ print("The population of user under treatment group: %d" %n_old)
df_weather.head(5)
df2['intercept'] = 1 $ df2[['new_page','old_page']] = pd.get_dummies(df2['landing_page']) $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2.head()    
tweets_df.info()
Results_kNN500x.to_csv('soln_kNN500_extra.csv', index=False)
lines_to_read=100 $ with open(dumpfile) as myfile: $     firstlines=myfile.readlines()[0:lines_to_read] #put here the interval you want $     for x in firstlines: $         print(x.strip())
temps_diffs.mean()
(new_page_converted/n_new) - (old_page_converted/n_old)
for link in shows['link'][0:5]: $     print(link)
import os $ import json $ import tweepy $ from smappdragon import JsonCollection
from tclab import runexperiment, labtime
tip_sample.describe()
bigram_chunker = BigramChunker(train_trees) $ print(bigram_chunker.evaluate(valid_trees))
%%bash $ module load R $ Rscript --vanilla _pheno.R
df3 = pd.merge(campaign_cost, df2, left_on=['id_partner', 'campaign'], right_on=['id_partner', 'name']).sort_values('Costs')
All_tweet_data=pd.merge(All_tweet_data, Imagenes_data_v2, on='tweet_id', how='left') $
fire_size = pd.read_csv('../data/model_data/1979-2016_fire_size.csv', index_col=0) $ fire_size.dropna(inplace=True) $
dropped =guinea[['Date', 'Totals','country']] $ dropped =dropped.fillna(0) $ dropped.head()
values = df[df['converted'] == 1] $ values_counts = values["converted"].count() $ values_total = df['converted'].count() $ proportion = ((values_counts / values_total) * 100).round() $ print('The proportion of users converted is {}%.'.format(proportion))
df_countries.set_index('user_id', inplace=True)
apple = tweets[tweets['apple'] == True] $ apple.head()
isinstance(df.date[1],datetime)
df.head()
train = energy.copy()[energy.index < valid_start_dt][['load', 'temp']]
images.info()
twitter_archive_clean[twitter_archive_clean.rating_denominator != 10]
print 'number of possible observable objects:', int((-hrs_before_midnight+hrs_after_midnight)/integratione_time*60+1.)
ndf.shape
rank_meters['afternoon_12_17'].most_common(11)[1:]
ebola_melt = pd.melt(ebola, $     id_vars=['Date', 'Day'], $     var_name='type_country', $     value_name='counts') $ ebola_melt.head()
from datetime import timedelta
trip_data = pd.read_csv('babs_y1_y2_summary.csv') $ display(trip_data.head())
svm_classifier = GridSearchCV(estimator=estimator, cv=kfold, param_grid=svm_parameters) $ svm_classifier.fit(X_train, Y_train)
X['is_cat'] = X['title'].map(lambda x: 1 if 'cat' in x else 0) $ X['is_funny'] = X['title'].map(lambda x: 1 if 'funny' in x else 0)
sessions.shape[0]
extract_deduped_with_elms.loc[(~extract_deduped_with_elms.ACCOUNT_ID.isnull()) $                              &(extract_deduped_with_elms.LOAN_AMOUNT.isnull()) $                              &(extract_deduped_with_elms.application_month=='2018-05')].groupby('app_branch_state').size()
df2.drop(1404, inplace=True) $ df2.shape
s = pd.Series([1,2,3,4,5],index=['a','b','c','d','e']) $ s
actual_diff = treatment_conv - control_conv $ actual_diff
import pandas as pd
sns.countplot(x="fluctuation", data=eth, palette="Greens_d")
df2 = df[(((df.group == 'treatment') & (df.landing_page == 'new_page')) | ((df.group == 'control') & (df.landing_page == 'old_page')))]
full_nan.isnull().sum() / full_nan.shape[0]
tree_features_df[tree_features_df['p_hash'] == manager.image_df.loc[1,'p_hash']] #Yes, it's in both, with the same name
model.add(Conv1D(filters=30,kernel_size=30, $                  input_shape=(527,1), $                  activation='relu', $                  padding='same'))
c2b = c2.galactic $ print(c2b) $ print(c2b.l, c2b.b)
control_df2 = df2.query('group == "control"') $ control_conversion = control_df2['converted'].mean() $ control_conversion
token.date = token.date.map(lambda x : datetime.datetime.strptime(str(x)[:10],"%Y-%m-%d"))
rfr = RandomForestRegressor(n_estimators=20, min_samples_split=3, random_state=0)
%load sol_2_22.py
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
tce2.drop(columns=['ElectronicCollection'],inplace=True) $ tce2
train.ORIGINE_INCIDENT.unique()
cust_demo.head(10)
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2016-06-07&end_date=2016-06-09&api_key=yUSy9ms6sx2Ubk55dXdN") $
df.head()
allqueryDF['domain'].unique()
from scipy.stats import norm $ norm.ppf(1-(0.05))
df_merged.to_csv('twitter_archive_master.csv', index=False)
roc_auc_score(y_test, NB.predict_proba(X_test)[:,1])
!du -h test.csv
len(climate_data) $
year12 = driver.find_elements_by_class_name('yr-button')[11] $ year12.click()
items2 = [{'bikes': 20, 'pants': 30, 'watches': 35, 'shirts': 15, 'shoes':8, 'suits':45}, $ {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5, 'shirts': 2, 'shoes':5, 'suits':7}, $ {'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4, 'shoes':10}] $ store_items = pd.DataFrame(items2, index = ['store 1', 'store 2', 'store 3']) $ store_items
!dir
df_titanic.head()
regexp_tok = RegexpTokenizer(r'\w+')
parks_don_quar = parks_don.groupby(['fq'])[['amount']].sum() $ parks_don_quar.plot(kind='bar');
popCon[popCon.content == 'link'].sort_values(by='counts', ascending=False).head(10).plot(kind='bar')
1- one_tail
from tensorflow.examples.tutorials.mnist import input_data $ mnist = input_data.read_data_sets("/tmp/data/")
df = pd.read_pickle(pretrain_data_dir+'/pretrain_data_03.pkl') $ df = df.sort_values(by=['seq_id','work_day'],axis=0,ascending=[True, False])
CARDINALITY_SINGLE_VALUED = 1 $ CARDINALITY_MULTI_VALUED = 2 $ metric_series_type_ids = [metric['id'] for metric in response.json()['items'] $                           if metric['cardinality']['id'] == CARDINALITY_MULTI_VALUED]
from google.datalab.ml import TensorBoard $ TensorBoard().start('./taxi_trained') $ TensorBoard().list()
X.isnull().sum()
tmp = test_df.join(tt) $
df.resample('M').mean()
def parse_devices_domain(df): $
potholes = df[df['Descriptor'] == 'Pothole'] $ potholes.groupby(potholes.index.hour)['Created Date'].count().plot(y='Created Date')
plt.savefig('tweets_scatterplot.png', bbox_inches = 'tight')                 # saves plot to file
states_df.columns = ['State','Abrv','State-hood','Capital','CapitalSince','Area','PopMunic','PopMetro','Rank','Notes'] $ states_df.drop([0,1], axis=0, inplace=True) $ states_df
tweets_clean['user'] = user_info['id'] $ tweets_clean.head()
preds_test = pd.DataFrame(y_test) $ preds_test['knn'] = knn_pred $ preds_test['rf'] = rf_pred $ preds_test['lr'] = lr_pred
new_page_converted = np.random.choice([0,1],size=n_new,p= [1-p_new,p_new])
d.groupby('label')['tweet_id'].count()
nypd_complaints_total = data[data['Agency']=='NYPD']['Borough'].count() $ nypd_complaints_total
td_norm = td ** (10/11) $ td_norm = td_norm.round(1)
reviewsDFslice.tail(50)
import geopandas as gpd
train.head()
bacteria + bacteria2
dayofweek = df.groupby('Day_of_week') $ print(dayofweek)
students.tail()
archive_clean.info()
df['state_cost'].replace('\$','',regex=True,inplace=True) $ df['state_retail'].replace('\$','',regex=True,inplace=True) $ df['sale'].replace('\$','',regex=True,inplace=True)
p_new = df2['converted'].mean() $ print(" Actual values of Probability of conversion for new page (p_new):", p_new)
merged.isnull().any()
conn, cur = connect()
pd.Series({2:'a', 1:'b', 3:'c'})
stat_range = lambda x: x.max() - x.min() $ stats.apply(stat_range)
[sample_pivot_table,test] = split_data(order,70,100) $ similarity_weight = cosine_similarity(sample_pivot_table) $ prefiltering_sw = prefiltering_of_neighbors(similarity_weight, 0.1) $ predicted_table =  cs_classification_predicted_score_user_based(sample_pivot_table,prefiltering_sw,[0,1])
p_old=df2.query('converted==1').user_id.count()/df.shape[0] $ p_old
pMean = np.mean([pnew, pold]) $ NewPage = np.random.choice([1, 0], size=new,p=[p, (1-p)]) $ new_avg = NewPage.mean() $ print(new_avg)
dfbreakfast = df[(df['TIME'] == '11:00:00') | (df['TIME'] == '12:00:00')] $ dfbreakfast.head(2)
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt
wrd.query('name == "a"')['text']
invoice_sat.columns[~invoice_sat.columns.isin(v_invoice_sat.columns)]
df.loc['Monday'] # select by row label 'Monday'
print("Number of Relationships in Mobile ATT&CK") $ relationships = lift.get_all_mobile_relationships() $ print(len(relationships)) $ df = json_normalize(relationships) $ df.reindex(['id','relationship', 'relationship_description', 'source_object', 'target_object'], axis=1)[0:5]
cust_data1.dtypes $ cust_data1['No_of_30_59_DPD'] = cust_data1['No_of_30_59_DPD'].astype('str')
autos["odometer_km"].value_counts().sort_index(ascending=True)
df.join(d).head(10)
price_counts = autos['price_dollars'].value_counts() $ print(price_counts.sort_index(ascending=True))
eval_LR_tfidf_tts = clf_LR_tfidf.score(X_testcv_tfidf, y_testcv_tfidf) $ print(eval_LR_tfidf_tts)
facts_url = 'https://space-facts.com/mars'
data = pd.DataFrame(data=[tweet.text for tweet in results], $                     columns=['Tweets'])
points
df['Market'].head(1) $ df['Market'].head(2) $ df['Market'].head(3) $ "Only I will print."
ts.shift(1,DateOffset(minutes=0.5))
df3.info()
print('Highest opening price - {} \nLowest opening price - {}'.format(max(opening_price), min(opening_price)))
prop['bathroomcnt'].value_counts().sort_values()
ts.shift(1)
y_predict = clf.predict(X_test) $ print(accuracy_score(y_test, y_predict)*100)
cercanasA1_11_14Entre50Y75mts = cercanasA1_11_14.loc[(cercanasA1_11_14['surface_total_in_m2'] >= 50) & (cercanasA1_11_14['surface_total_in_m2'] < 75)] $ cercanasA1_11_14Entre50Y75mts.loc[:, 'Distancia a 1-11-14'] = cercanasA1_11_14Entre50Y75mts.apply(descripcionDistancia, axis = 1) $ cercanasA1_11_14Entre50Y75mts.loc[:, ['price', 'Distancia a 1-11-14']].groupby('Distancia a 1-11-14').agg(np.mean)
df_2016 = pd.DataFrame(rows)
df_b
df.isnull().sum()
minMaxDate = minMaxDate[minMaxDate['diff'] > timedelta(days=7)]
rain_period = sf_small_grouped[sf_small_grouped['Rain'] == 1].start_date
condo_6.shape
sortHighThenVol = AAPL.sort_values(by=["high", "volume"], ascending=False) $ sortHighThenVol.head()
Grouping_Year_DRG_discharges_payments.xs(2, level='drg3')
total = ALL.groupby("Team") # This groups our final sample by team.
df = spark.read.parquet('/datasets/twitter_hashtags')
messages_with_dates_ext = messages.apply(find_dates_in_row,axis=1)
import statsmodels.api as sm $ df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $
t_rows=len(t) $ print(t_rows)
joined = join_df(joined, transactions_df, ['date', 'store_nbr'])
autos["price"].sort_values(ascending=True)[2000] # 150 $ autos.loc[autos["price"] < 150,:]
import psycopg2 $ import pandas as pd $ import numpy as np $ from scipy import stats $ from math import sqrt
for column in ideas: $     ideas[column] = ideas[column].dt.week
61 / 656, sum(test['badge_#']) / 326625
df_freq_users = df[df["user_id"].duplicated()]
english_df = df.filter(df.lang == 'en').select( $     df.timestamp_ms, $     df.lang, $     lowercase(df.hashtag).alias('hashtag'), $     functions.from_unixtime(df.timestamp_ms / 1000, 'yyyy-MM-dd HH:mm:ss').alias('date'))
old_page_converted = np.random.binomial(1,p_old,n_old)
pd.DataFrame({'Minimum': [min_lat, min_lon], 'Maximum': [max_lat, max_lon]}, index=['Latitude','Longitude'])
df_t.sort_values(by='timestamp',ascending=False).head(2)
df.info()
holdout_results.groupby(['wpdx_id', 'second_measurement'])['status_binary','cv_predictions'].sum().unstack().head()
new_page_converted = np.random.choice([1, 0], p=[p_new, 1-p_new], size=n_new) $ new_page_converted.mean()
evaluator = RegressionEvaluator().setLabelCol("ratings_centered") \ $                             .setPredictionCol("prediction") \ $                             .setMetricName("rmse") $ train_rmse = evaluator.evaluate(predictions) $ print(train_rmse)
plot(df_train['a'])
LOC_outstanding_user=calc_LOC(merkmale)
impossible_year = ( $     autos.loc[(autos["registration_year"] > 2016) == True, :] $     .index $ )
features = ['recency', 'monetary', 'total_profit', 'frequency', 'join_days']
(etsamples_engbert,etmsgs_engbert,etevents_engbert) = be_load.load_data(algorithm='') $ raw_freeview_df, raw_fix_count_df  = condition_df.get_condition_df(data=(etsamples_engbert,etmsgs_engbert,etevents_engbert),condition='FREEVIEW')
joined_test[dep] = 0 $ joined_test = joined_test[cat_vars+contin_vars+[dep, 'Date', 'Id']].copy()
df2['intercept'] = 1 $ df2[['control', 'ab_page']] = pd.get_dummies(df2['group']) $ df2.drop(['control'], axis = 1, inplace = True) $ df2.head(5)
popt_ipb_chord_crown, pcov_ipb_chord_crown = fit(d_ipb_chord_crown)
mailing_list_posts_mbox_df_saved.printSchema()
S_distributedTopmodel = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure09/summa_fileManager_distributedTopmodel.txt')
pd.period_range('11-Sep-2017', '17-Sep-2017', freq='M')
weather_max = weather_all.groupby('Station Name').max() $ weather_max.sort_values('Wind Spd (km/h)', ascending=False)
dog_ratings = dog_ratings.dropna(subset=['jpg_url']) $ dog_ratings.info()
conditions = ((autos.registration_year >2016) $               |(autos.registration_year <1920)) $ print(autos.loc[conditions,'registration_year'].value_counts() $                                                .sort_index())
def calculate_dividend_weights(ex_dividend): $     return None $ project_tests.test_calculate_dividend_weights(calculate_dividend_weights)
autos['registration_year'].value_counts().sort_index()
words_sk, corups_tweets_streamed_keywords = count_freq(tweets_l_stream_keywords, corpus_tweets_streamed_keywords) $ wordfreq = FreqDist(words_sk) $ print('The 100 most frequent terms, including special terms: ', wordfreq.most_common(100))
train['dot_edu'] = train.url.str.contains('.edu', case=False, na=False, regex=False).astype(int) $ train.groupby('dot_edu').popular.mean()
stop_words = set(nltk.corpus.stopwords.words('english')) $ for w in ['say', 'said', 'also', 'would', 'wouldve', 'would\'ve', 'mr', 'ms', 'told', 'get']: $     stop_words.add(w) $ %timeit articles['tokens'] = articles['tokens'].map(lambda s: [w.lower() for w in s if w.lower() not in stop_words])
df.info()
train.MODELE_CODE.value_counts()
contour_sakhalin.shape
dup_user = df2.groupby('user_id') $ dup_user.filter(lambda x: len(x) > 1) $
facts_metrics[facts_metrics['dimensions_item_id']==3063339]
converted_old = df2.query("group == 'control' and converted == '1'").count()[0]/df2.query("group == 'control'").count()[0] $ converted_old
country = ['Argentina', 'Kenya', 'Nigeria', 'South Africa', 'France', 'Germany', $            'Scotland', 'Spain', 'Ireland', 'Italy', 'New Zealand', 'Pakistan'] $ for i in country: $     notus.loc[notus['country'] == i, 'country'] = i $     notus.loc[notus['cityOrState'] == i, 'country'] = i
grid_search.best_estimator_
df_imputed_mean_NOTCLEAN1A = df_NOTCLEAN1A.fillna(df_NOTCLEAN1A.mean())
pd.merge(df1, def2, ...) $ pd.concat([df1, df2, df3, ...])
data1.head()
status = extractor.get_status(942535103099502592) $ print(status)
df_goog.Close
print(data.shape) $ {x:len(data[x].unique()) for x in data.columns}
td = pd.read_excel('input/data/TransmissionSampleData.xlsx', $                     parse_cols='B:F', $                     skiprows=[16], $                     header=11, $                     index_col=0)
from pyspark.sql.functions import to_timestamp $ df_city_reviews = df_city_reviews.withColumn('date', to_timestamp(df_city_reviews['date'], 'yyyy-mm-dd'))
ttarc.expanded_urls.str.extract('(photo)').count()
commits_per_hour = git_log.timestamp.dt.hour.value_counts(sort=False) $ commits_per_hour.head()
prop = donors.loc[donors['Donor Zip'].isnull(), 'Donor Is Teacher'].value_counts(normalize=True) $ prop
gsmodel.best_params_
hk = pd.read_csv(path+"datas/28hse_HongKong.csv")
dictionary.save('/home/david/Desktop/guardian.dict')
clean_prices.head()
proj_df.head()
df_cs['cleaned_text'] = df_cs['text'].apply(lambda x : text_cleaners(x))
df2[df2.duplicated(subset='user_id', keep=False)]
df[df['Descriptor'] == 'Pothole'].groupby(by=df[df['Descriptor'] == 'Pothole'].index.hour).count().plot(y='Agency') $
df.to_csv('ab_updated_data.csv', index=False)
geocoded_df.groupby('Plaintiff.Name')['Plaintiff.Name'].count().sort_values(ascending=False).head(50)
prop_ca = df_full.query('country == "CA"')['converted'].mean() $ prop_uk = df_full.query('country == "UK"')['converted'].mean() $ prop_us = df_full.query('country == "US"')['converted'].mean() $ print("Proportion of conversions\nCA: {:.5f}\nUK: {:.5f}\nUS: {:.5f}"\ $       .format(prop_ca, prop_uk, prop_us))
pres_df.shape
obj
df_all.to_csv('./data/df_all_no_sess.csv', index=False) $
(autos["date_crawled"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $ )
data = pd.read_csv("train.csv", parse_dates=['DateTime']) $ print "The shelter data has %d examples and has %d attributes" %(data.shape[0],data.shape[1]) $ data.head()
sns.distplot(utility_patents_subset_df['number-of-figures'], color="red") $ plt.show()
train['month_of_first_active'] = train['timestamp_first_active_to_date'].apply(lambda x: x.strftime('%m'))
gs.score(X_test, y_test)
verify_response.json()['screen_name']
fb.tail()
contractor_clean['last_updated'] = pd.to_datetime(contractor_clean.last_updated) $ contractor_clean['updated_date'] =contractor_clean['last_updated'].dt.strftime('%m/%d/%Y') $ contractor_merge['month_year'] =contractor_merge['last_updated'].dt.to_period('M')
top_active = session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all() $ most_active_id = session.query(Measurement.station).group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).first() $ top_active $
nb_pipe_2.fit(X_train, y_train) $ nb_pipe_2.score(X_test, y_test)
distinct_authors_with_gh = authors_to_github_username_saved.withColumn( $     "new_unique_id", $     F.when(F.col("github_username") != "", $          F.col("github_username")).otherwise( $         F.col("email"))) $
session_top_subset = session[session['action'].isin(top_actions)] $ session_top_subset.head()
nodes[(nodes['timestamp'].dt.year == 2017) | $       (nodes['timestamp'].dt.year == 2016)]['user'].value_counts().head(10)
autos["date_crawled"].str[:10].value_counts(dropna = False, normalize = True).sort_index()
for fld in tqdm(holidays): $     add_elapsed(fld, 'before_')
pivoted_data.cumsum().plot(figsize=(10,10))
len(df_enhanced.query('retweeted_status_id != "NaN"'))
np.set_printoptions(suppress=True)
np.save('myfile.npy', a)  #Save `a` as a binary .npy file.
df_users_6_mvp['created_year']=df_users_6_mvp['created'].apply(lambda x:x.year) $ df_users_6_mvp['created_month']=df_users_6_mvp['created'].apply(lambda x:x.month) $ df_users_6_after_mvp['created_year']=df_users_6_after_mvp['created'].apply(lambda x:x.year) $ df_users_6_after_mvp['created_month']=df_users_6_after_mvp['created'].apply(lambda x:x.month)
uk = ['London', 'United Kingdom', 'England'] $ notus.loc[notus['country'].isin(uk), 'country'] = 'UK' $ notus.loc[notus['cityOrState'].isin(uk), 'country'] = 'UK' $ notus.loc[notus['country'] == 'UK', 'cityOrState'].value_counts(dropna=False)
list1 = [] $ for i in range(len(data['text'])): $     list1.append(data['text'].str.split(' https')[i][0])
data.nlargest(10, 'Z-Score') # This could be later put into a button for easy access. 
autos = autos.rename(columns={'odometer':'odometer_km'})
kick_projects['launched_date'] = pd.to_datetime(kick_projects['launched'], format='%Y-%m-%d %H:%M:%S') $ kick_projects['deadline_date'] = pd.to_datetime(kick_projects['deadline'], format='%Y-%m-%d %H:%M:%S')
pystore.list_stores()
fm_confident_over = y_test_over[fm_bet_over]
df2[df2.user_id == 773192]
print("The maximum value of userID:") $ userArtistDF.agg(max("userID")).show()
ml
rt_count_1 = df_rt[['keyword']].groupby(['keyword']).size().reset_index() $ rt_count_1.columns = ['keyword', 'count'] $ rt_count_1.sort_values(by = ['count'], ascending = False, inplace = True)
time2close_2012 = merged_tickets_2012["time2close"]
 from imblearn.over_sampling import SMOTE $ sm = SMOTE(random_state=42) $ X_res, y_res = sm.fit_sample(X,y)
ctc /= 1000
df.groupby(df.index.hour).count().plot(y='Complaint Type')
converted = (df['converted'] == 1).sum() $ prop_converted = converted/row_count $ prop_converted $
df2['converted'].sum()/df2['converted'].count()
train = pd.read_csv("../input/web-traffic-time-series-forecasting/train_1.csv") $ keys = pd.read_csv("../input/web-traffic-time-series-forecasting/key_1.csv") $ ss = pd.read_csv("../input/web-traffic-time-series-forecasting/sample_submission_1.csv")
X.head()
mars_news_url = "https://mars.nasa.gov/news/"
p_new = df2[df2['landing_page']=='new_page']['converted'].mean() $ p_old = df2[df2['landing_page']=='old_page']['converted'].mean() $ convert_rate_new = np.mean([p_new, p_old]) $ convert_rate_new
r2_score(y_test, best_model['model'].predict(X_test))
import codecs $ import spacy $ nlp = spacy.load('en')
a == 0
x = np.array([1,2,3,4,5], dtype=np.float64)
p_old = df2['converted'].mean() $ p_old
grouped = df[["year", "week", "multi"]].groupby(by=["year", "week"]).agg(["count", "sum"]).reset_index() $ grouped["p"] = grouped["multi"]["sum"]/grouped["multi"]["count"] $ grouped.tail(grouped[grouped["year"] == 2018]["week"].max())
import pandas as pd $ import numpy as np
df_twitter_archive.info()
commiters.sort_values(by='total_commits') $ pprint (commiters[['author', 'total_commits']])
FREEVIEW.plot_number_of_fixations(raw_fix_count_df, option='eyetracker')
query=rawdata.replace("EVERY_N", "100000") $ trips = bq.Query(query).execute().result().to_dataframe() $ print "Our sample dataset consists of {} taxi rides (representing {}% of total)".format(len(trips), 100.0/100000.0) $ trips[:5]
df.index
atloc_4x_tabledata = atloc_4x_count_prop_byloc.reset_index() $ create_study_table(atloc_4x_tabledata, 'locationType', 'remappedResponses', $                    location_remapping, atloc_response_list)
data.head()
data = get_fremont_data() $ pivoted = data.pivot_table('Total', index=data.index.time, columns=data.index.date)
df_protest.loc[df_protest.towncity_name=='Johannesburg', 'start_date'].min()
value = 152000  # Deletion threshold  (52: 152000) $ index_drop = []  # ['MNO(WT%)', 'ND(PPM)', 'RB(PPM)']  # Deleted elements $ index_keep = []  # ['TA(PPM)', 'HF(PPM)', 'SC(PPM)', 'TH(PPM)', 'CE(PPM)', 'ZR(PPM)', 'NB(PPM)', 'Y(PPM)']  # Preserved elements $ index_ = (a > value) & ~ a.index.isin(index_keep) | a.index.isin(index_drop)
tweet_scores.favorite_count
import numpy as np $ import pandas as pd $ from collections import Counter
soup.title.contents
g_geo = g_geo.to_crs({'init':'epsg:4269'})
df_crea = parse_dict['creator'].drop(['avatar','id','slug','urls'],axis=1).rename(columns={'name':'creator_name', $                                                                                  'is_registered':'creator_registered'})
import pandas as pd $ import numpy as np $ import datetime as dt
week49 = week48.rename(columns={343:'343'}) $ stocks = stocks.rename(columns={'Week 48':'Week 49','336':'343'}) $ week49 = pd.merge(stocks,week49,on=['343','Tickers']) $ week49.drop_duplicates(subset='Link',inplace=True)
lims_query = "SELECT ephys_roi_results.id, specimens.id, specimens.name \ $ FROM ephys_roi_results JOIN specimens ON specimens.ephys_roi_result_id = ephys_roi_results.id" $ lims_df = get_lims_dataframe(lims_query) $ lims_df.tail()
df.groupby('Single Name').sum()[df.groupby('Single Name').count()['Name']>6]
entities.value_counts()[:10]
feature_matrix_duplicated
rf.fit(hX, hy) $ cm = confusion_matrix(hy, rf.predict(hX)) $ sns.heatmap(cm, annot=True)
with pd.option_context('float_format', '{:.2f}'.format): print(df_mes.loc[:,['travel_time','average_speed']].describe()) #10.294.628 cases
df.dtypes
filtered.shape
pickle.dump(cv_fitted, open('iteration1_files/epoch3/cv_fitted.pkl', 'wb'))
len(df), len(date), len(count)
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']]) $ results = logit_mod.fit() $ results.summary2()
test_df = pd.concat([test_df,  get_flat_features(test_df)], axis=1)
trump_pivoted = pivot_count(trump_df, 'source', 'tweet_type') $ trump_pivoted
df_archive_clean.sample(5)
df = pd.read_csv('msa.csv')
print(mbti_text_collection_filler.drop('text', axis=1)[mbti_text_collection_filler.text_count<50])
li1[4]
import pandas as pd $ file_name = "data/survey_Sorsogon_Electricity_Water_wbias_projected_dynamic_resampled_1000_{}.csv" $ sample_survey = pd.read_csv(file_name.format(2010), index_col=0) $ sample_survey.head()
sa=38 $ len(final.chips.values), len((times+utcoffset).value[:-sa])
df2[df2['landing_page']=='new_page'].user_id.count()/df2['landing_page'].count()
iowa_county_2015 = iowa_2015.groupby('County')[['Volume Sold (Gallons)', 'Profit']].sum().reset_index() $ iowa_county_2015['Profit Per Gallon'] = iowa_county_2015['Profit'] / iowa_county_2015['Volume Sold (Gallons)'] $ iowa_county_2015.sort_values(by='Profit Per Gallon', ascending=False).head(10)
with open("Twitter_Keys.json", "r") as file: $     keys = json.load(file) $ API_KEY = keys["API_KEY"] $ API_SECRET = keys["API_SECRET"]
df_archive_clean[[ "timestamp", "retweeted_status_timestamp"]].sample(5)
scaled = scaled.dropna() $ scaled.head().T
train_ind, test_ind, train_dep, test_dep = train_test_split(kick_projects_ip_scaled_ftrs, kick_projects_ip[response], test_size=0.3, random_state=0)
print(g1800s.describe())
pred_adp=model_ADP.predict()
poly3 = PolynomialFeatures(degree=12)
count_by_Confidence = grpConfidence['MeanFlow_cfs'].count() $ count_by_Confidence.plot(kind='pie');
categories = list(train_users.columns.values) $ categories = categories[4:len(categories)-1] $ categories.remove('age') $ categories.remove('gender') $ categories
plot_series_save_fig(series=doc_duration, figsize=(12,6), xlabel='Date', ylabel='Appointment Time (hours)',\ $                      plot_name='Doctors', figname='./images/doctors_weekly_time_series.png')
!head -20 sample_data.json
ac['Dispute Resolution End'].groupby([ac['Dispute Resolution End'].dt.year]).agg('count')
ct_df = pd.read_csv('./countries.csv') $ ct_df.head()
exploration_titanic.findcorr() # no highly numerical correlated columns 
import statsmodels.api as sm $ convert_old = control_converted $ convert_new = treatment_converted $ n_old = control.shape[0] $ n_new = treatment.shape[0]
from matplotlib.pyplot import figure $ figure(num=None, figsize=(30, 4), dpi=80, facecolor='w', edgecolor='k') $ sources = list(tweet_source_hist.head(15).index) $ frequencies = list(tweet_source_hist.head(15).source_freq) $ plt.bar(sources, frequencies)
b = tweets_df.retweet_count.hist() $ b.set(yscale="log") $ plt.annotate("max", xy=(1000, 20000), xytext= (3000, 1000), arrowprops=dict(facecolor='black', shrink=0.05))
pos_sent = open("positive_words.txt").read() $ neg_sent = open("negative_words.txt").read() $ print(pos_sent[:101])
from functions.ARIMA_functions import get_ARIMA_model, plot_ARIMA_model, plot_ARIMAX_model_save_fig, plot_ARIMA_resids,\ $ get_ARIMA_forecast, plot_ARIMA_forecast_and_CI, plot_data_plus_ARIMA_predictions, \ $ test_rolling_ARIMA_forecast,get_predictions_df_and_plot_rolling_ARIMA_forecast, get_ARIMAX_predictions
autos['odometer'] = autos['odometer'].str.replace("km","").str.replace(",","").astype(int) $ autos['odometer'].head(10)
p_new_real = df2.query('landing_page == "new_page"')['converted'].mean() $ p_new = df2.query('converted == 1').count()[0]/df2.count()[0] $ p_new
len(news_df)
snowshoe_df = pd.read_pickle('..\data\interim\df_for_snowshoe_classification') $ snowshoe_df.drop(columns = ['datetime'],inplace=True) $ snowshoe_df.head()
shown = data.tasker_id.value_counts() $ hired = data.groupby('tasker_id').hired.sum()
lm = smf.ols(formula='sales ~ TV + radio + newspaper + TV*radio', data=data).fit() $ lm.params
new_page_converted = np.random.binomial(n_new,p_new) $ print('new_page_converted :: ',new_page_converted)
df.shape
nba_df.head(3)
X_train.shape
print(df.sex.str.len().unique())# returns unique lengths of sex $ df.sex=df.sex.str.strip() $ print(df.sex.str.len().unique())
idx = df_sites[ (df_sites['disc_times_pay'] < 1000000)].index.tolist() $ len(idx) $ df_sites.loc[idx,:].head()
pd.DataFrame(X.toarray()).head()
print(my_list[0:4])     # select whole range $ print(my_list[3])       # as can be seen, the last element of the list corresponds to index value of '3' in single-value indexing (instead of the '4' as was the case in slicing) $ print(my_list[1:4])     # slice (leave first element out)
df['converted'].value_counts()[1]/len(df)
most_informative_features_top_and_bottom(vectorizer=vectorizer, classifier=lasso, binary=False, n=15)
crimes_by_yr_month.head()
wrd.sample(3)
P.plot_1d_layer('mLayerLiqFluxSoil')
closingPrices.head()
sns.distplot(posts[(posts['ViewCount'].notnull() & $                     (posts['ViewCount'] < 4000))]['ViewCount'], kde = False, bins = 30) $ plt.xlim((0,4000))
%%bash $ python3 app.py
load2017.isnull().sum()
vals = df_all.values $ X = vals[:piv_train] $ le = LabelEncoder() $ y = le.fit_transform(labels) $ X_test = vals[piv_train:] $
from scipy.stats import norm $ norm.cdf(z_score) $ norm.ppf(1-(0.05/2)) $
session.query(Measurement.station, func.count(Measurement.id)).group_by(Measurement.station).\ $                        order_by(func.count(Measurement.id).desc()).all()
def generate_dollar_volume_weights(close, volume): $     assert close.index.equals(volume.index) $     assert close.columns.equals(volume.columns) $     return None $ project_tests.test_generate_dollar_volume_weights(generate_dollar_volume_weights)
config = configparser.ConfigParser() $ config.read('./data/mongo.ini')
train['lon_plus_lat'] = train['longitude'] + train['latitude'] $ test['lon_plus_lat'] = test['longitude'] + test['latitude']
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], value=0, alternative='smaller') $ print(z_score) $ print(p_value)
gs_pca_under.score(X_train_pca, y_train_under)
df_test = df_clean.loc[df_clean['rating_numerator'] > 10] $ df_test[['text','rating_numerator']].head()
fix_wages(df_h1b_nyc_ft,6000,35000,500)
session = Session() $ query = session.query(articles_raw).limit(100) $ df = pd.read_sql(query.statement, query.session.bind) $ session.close()
print(surveys_df.columns) $ print(species_df.columns)
CSV_FILE_PATH = os.path.join('pims_cloudpbx_subset_201806051550_1million.csv') $ GEOLITE_ASN_PATH = os.path.join('GeoLite2-ASN.mmdb') $ GEOLITE_CITY_PATH = os.path.join('GeoLite2-City.mmdb')
max_calc = df.loc[df["station"] == "USC00519397"].groupby('station').max() $ min_calc = df.loc[df["station"] == "USC00519397"].groupby('station').min() $ mean_calc = df.loc[df["station"] == "USC00519397"].groupby('station').mean() $
s = df_sms.groupby(['group','ShopperID'])['ID'].count().reset_index() $ s.groupby(['ID','group']).size();
fwd = df[['Store'] + columns]\ $     .sort_index(ascending=False)\ $     .groupby("Store")\ $     .rolling(7, min_periods=1).sum()
userMovies = moviesWithGenres_df[moviesWithGenres_df['movieId'].isin(inputMovies['movieId'].tolist())] $ userMovies
df[df['Descriptor'] == 'Loud Music/Party'].groupby(by=df[df['Descriptor'] == 'Loud Music/Party'].index.dayofweek).count().plot(kind='bar', y='Agency')
optimizer_col= optim.Adam(model.parameters(),lr=0.0005) $ lr_scheduler_col = lr_sched.myCosineAnnealingLR(optimizer_col,400,cycle_mult=1.5) $ train_col.train_model(num_epochs=3,optimizer=optimizer_col,scheduler=lr_scheduler_col)
list(data.columns.values)
CountVectorizer?
from pandas import Series, DataFrame
import pandas as pd $ from random import randint, sample
energytolerance = 1e-8 $ forcetolerance = uc.set_in_units(0.0, 'eV/angstrom') $ maxiterations = 10000 $ maxevaluations = 100000 $ maxatommotion = uc.set_in_units(0.01, 'angstrom')
type(df2.created_time.loc[0])
endometrium_data = pd.read_csv('data/small_Endometrium_Uterus.csv', sep=",")  # load data $ endometrium_data.head(n=5)  # adjust n to view more data
trump.describe()
wildfires_df['FIRE_YEAR'] = wildfires_df['FIRE_YEAR'].astype('str') #use .astype function to convert to a string $ Info_Dataframe(wildfires_df)
df.groupby('episode_id')['id'].nunique().max()
user = user.set_index("name") $ user.head(3)
for tweet in tweets: $     print(tweet.text)
pd.Series(PDSQ.min(axis=1)<0).value_counts()  # no
n_new = len(df2.query("landing_page == 'new_page'"))           $ print('n_new :: ', n_new)
Which_DRGs_in_each_year
tweet1.favorite_count
print("123".isdigit()) $ print("1X3".isdigit()) $ print("NOOOOooo".isupper())
archive_clean.sample(5)
number_of_rows = df['user_id'].count() $ print('number of rows in the dataset is {} '.format(number_of_rows))
jobs.loc[(jobs.FAIRSHARE == 3) & (jobs.ReqCPUS == 8) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
amazon_review = pd.read_csv('amazon_cells_labelled.tsv',sep='\t') $ amazon_review
for c in ccc: $     for i in ved[ved.columns[ved.columns.str.contains(c)==True]].columns: $         ved[i] /= ved[i].max()
preds = xgb_learner.best_model.predict(dtest)
s3.value_counts?
titanic[~filter.fare]
print("Probability an individual recieved new page:", $       df2['landing_page'].value_counts()[0]/len(df2))
df.index[-5:]
y_newpage = df2["user_id"].count() $ prob_newpage = x_newpage/y_newpage $ prob_newpage $
stock.iloc[915:].shape
autos.drop(["num_pictures","seller","offer_type"],axis = 1,inplace = True)
located_data['subregion'][located_data['subregion'] != 'Nowhere'].describe()
params = {"objective": "reg:linear", "booster":"gblinear"} $ gbm = xgboost.train(dtrain=T_train_xgb, params=params)
to_be_predicted_Day3 = 48.64887236 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
pwd
res3 = df_test3_promotions.groupby('PromotionID') \ $     .agg({'PromotionRuleCount':'sum', 'AwardAmount':'sum', 'ShopperID':'nunique'}) $ res3['AwardAmount'] = res3['AwardAmount'] / 100 $ res3
labels = train.trips $ train = train.drop(['trips', 'date'], 1)
staff.index
corn.first()
import plotly.graph_objs as go $ from plotly.offline import download_plotlyjs, init_notebook_mode, iplot $ init_notebook_mode(connected=True)
header = "{ 'content' : " $ c = ' , ' $ footer = "}" $ jsonx2 = header + s1 + c + footer
predictions_final=predictions_train.filter('score>0')
df_new = df_countries.set_index('user_id').join(df.set_index('user_id'), how='inner') $ df_new.head()
git_log.info()
sklearn.metrics.roc_auc_score(Y_test.reshape(-1), preditions.reshape(-1))
new_data = pd.read_csv("JY_Final_Spire.csv")
wikiContentRequest = requests.get(wikipedia_content_analysis) $ print(wikiContentRequest.text[:1000])
df_treatment_group = df2.query('group == "treatment"') $ conversion_rate_treatment = df_treatment_group.query('converted == 1').shape[0] / df_treatment_group.shape[0] $ print('The probability of an individual from the treatment group converting is {}'.format(conversion_rate_treatment))
multiIndex=pd.MultiIndex.from_arrays(myarray,names=['Number','Colour']) $ print(multiIndex)
import textract
suffixes = ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']
tweet_archive_clean = tweet_archive_clean.dropna(subset=['expanded_urls']) $ tweet_archive_clean = tweet_archive_clean.dropna(subset=['jpg_url'])
wrd_full['year'] = wrd_full['timestamp'].apply(lambda x: datetime.strptime(x, "%Y-%m-%d %H:%M:%S +0000").year)
X = digits.drop('5', axis = 1).values $ X
tweet_archive_df.rating_denominator.value_counts().sort_index()
full_orig['PrimaryDx_Dx2'] = full_orig['PrimaryDx']+full_orig['Dx2'] $ full_orig['PrimaryDx_Dx3'] = full_orig['PrimaryDx']+full_orig['Dx3'] $ full_orig['Dx2_Dx3'] = full_orig['Dx2']+full_orig['Dx3']
example_time = tweet_archive_clean.timestamp[0] $ datetime.strptime(example_time,'%Y-%m-%d %H:%M:%S %z')
noaa_data.set_index('LST_DATE_LST_TIME',inplace=True)
import pandas as pd
phoneNumbers = uniqueTagValues(chosenFile, "phone") $ phoneNumbers[:10]
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 1) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'facilitator'
old_page_converted = np.random.choice([1, 0], p=[p_old, 1-p_old], size=n_old) $ old_page_converted.mean()
os.listdir()
last_year = dt.date(2017,8,23)-dt.timedelta(days=365) $ last_12_months = session.query(Measurement.date,Measurement.tobs).\ $     filter(Measurement.date > last_year, Measurement.station =='USC00519281').all() $ tobs_df = pd.DataFrame.from_dict(last_12_months)  $ tobs_df.plot.hist()
d3 = pd.DataFrame({'place': ['Boston', 'New York', 'Seattle'], 'area': [48.42, 468.48, 142.5]}) $ pd.merge(d1, d3, left_on='city', right_on='place')
noise['AFFGEOID'] = noise['AFFGEOID'].astype(str)
training_active_listing=Binarizer(10000) $ training_active_listing.fit(X_train['Active Listing Count '].values.reshape(-1,1)) $ training_active_listing_dummy=training_active_listing.transform( $     X_train['Active Listing Count '].values.reshape(-1,1))
import pandas as pd $ import numpy as np $ from sklearn.linear_model import LinearRegression $ from sklearn.model_selection import GridSearchCV $ %matplotlib inline
df_transactions['amount_per_day'] = df_transactions['actual_amount_paid']/df_transactions['payment_plan_days']
result_df = sqlClient.run_sql(sql) $ if isinstance(result_df, str): $     print(result_df)
from scipy.stats import norm $ print(norm.ppf(1-0.05))
r.json()['dataset_data']['column_names']
pred1 = model(*test1)
tst_lat_lon_df = pd.read_csv("testset_unique_lat_and_lon_vals.csv", index_col=0)
df["stamp"] = to_datetime(df["created_at"],format='%a %b %d %H:%M:%S +0000 %Y') $ df.head()
%timeit pd.eval('df1 + df2 + df3 + df4')
import pickle $ output = open('mentioned_bills.pkl', 'wb') $ pickle.dump(mentioned_bills_all, output) $ output.close()
week29 = week28.rename(columns={203:'203'}) $ stocks = stocks.rename(columns={'Week 28':'Week 29','196':'203'}) $ week29 = pd.merge(stocks,week29,on=['203','Tickers']) $ week29.drop_duplicates(subset='Link',inplace=True)
train_downsampled.cache() $ testing.cache() $
from collections import Counter $ from imblearn.over_sampling import RandomOverSampler $ ros = RandomOverSampler(random_state=0) $ X_resampled, y_resampled = ros.fit_sample(X, y) $ print(sorted(Counter(y_resampled).items()))
import re $ soup.find_all(href = re.compile("/item/main.nhn?"))
df2['user_id'].nunique() $ df2[df2['user_id'].duplicated() == True]
tweet_archive_enhanced_clean = tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['rating_denominator'].notnull()]
print "Mean time for closing a ticket in 2015: %f hours" % (time2close_2015.mean()/3600.0) $ print "Median time for closing a ticket in 2015: %f hours" % (time2close_2015.median()/3600.0) $ print "Quantiles: " $ print time2close_2015.quantile([0.25, 0.5, 0.75])
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=8000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
store_items = store_items.rename(columns = {'bikes': 'hats'}) $ store_items
print("Number of Malware objects in Enterprise ATT&CK") $ print(len(all_enterprise['malware'])) $ df = all_enterprise['malware'] $ df = json_normalize(df) $ df.reindex(['matrix', 'software', 'software_labels', 'software_id', 'software_description'], axis=1)[0:5]
len(tokendata)
print("Number of Groups in ATT&CK") $ groups = lift.get_all_groups() $ print(len(groups)) $ df = json_normalize(groups) $ df.reindex(['matrix', 'group', 'group_aliases', 'group_id', 'group_description'], axis=1)[0:5]
engine.execute('SELECT * FROM station LIMIT 5').fetchall()
folderint = folderData + '\\interactions_day\\'
import math $ import numpy as np $ train_df['Log_price']=np.log(train_df['price'].astype(float))
num_both = df.query('landing_page == "new_page" and group == "treatment"').shape[0] $ num_newpage = df.query('landing_page == "new_page"').shape[0] $ num_treatment = df.query('group == "treatment"').shape[0] $ mismatch = (num_newpage - num_both) + (num_treatment - num_both) $ mismatch
grid_clf = GridSearchCV(rdf_clf, param_grid, cv=10) $ grid_clf.fit(X_final[columns], y_final)
!pip install sqlalchemy
weather_df["weather_main"] = weather_df.weather_main.str.lower() $ weather_df["weather_description"] = weather_df.weather_description.str.lower()
p_converted_control_user2 = df2.query('converted==1 and group=="control"').user_id.nunique()/df2.query('group=="control"').user_id.nunique() $ p_converted_control_user2
minimum = date_price[date_price==date_price.min()].index[0] $ maximum = date_price[date_price==date_price.max()].index[0] $ print(f'The lowest average price was on {minimum}') $ print(f'The highest average price was on {maximum}')
p_new = df2.converted.mean() $ p_new
trains_fe2_y= trains_fe1[['reordered']] $ trains_fe2_y.head()
dtm_pos = dtm_df[pos_columns] $ dtm_pos
pd.to_datetime(data.last_trip_date).hist()
pd.Timestamp('2017-02-13') + pd.Timedelta('2D 3H')
scaled.describe()
print "Product ID      Count   Relative Frequency" $ print "-------------------------------------------\n" $ !hdfs dfs -cat /user/koza/hw3/3.2.1/productFrequencies/* | head -n 50
r = requests.get('https://{}/api/v3/user/orgs'.format(baseurl), headers=header) $ orgs = json.loads(r.content) $ for org in orgs: $     print('{}: id = {}'.format(org['login'], org['id']))
sns_plot = sns.barplot(x=stages,y=favorites,color='lightblue',order = ['puppo','doggo','floofer','pupper'],ci=None) $ plt.ylabel('Median Favorite Count') $ plt.xlabel('Dog Stages') $ sns_plot.figure.savefig('stage_vs_favorite.jpg')
import re $ def getHours(x): $   return re.match('([0-9]+(?=h))', x) $ temp = flight.select("duration").rdd.map(lambda x:getHours(x[0])).toDF() $ temp.select("duration").show()
f = sql.udf.register("fadd", lambda x: (np.array(x[3]) * 3.1).tolist(), ArrayType(FloatType())) $ fagg = sql.udf.register("fagg", lambda x,y: (np.array(x[3]) + np.array(y[3])).tolist(), ArrayType(FloatType()))
drace_df.columns
df_twitter.text.loc[822]
twitter_ar = pd.read_csv('twitter-archive-enhanced.csv')
extract_all.loc[(extract_all.APP_FIRST_NAME.isin(['GARY','Gary','gary'])) $                &(extract_all.app_branch_state=='VA'), $                 ['APP_APPLICATION_ID','APPLICATION_DATE_short','APP_PRODUCT_TYPE','APP_LOGIN_ID', $                 'APP_FIRST_NAME','APP_MIDDLE_NAME','APP_LAST_NAME','APP_SSN', $                 'APP_DOB','APP_CELL_PHONE_NUMBER','DEC_LOAN_AMOUNT1']]
autos["registration_year"].value_counts(normalize = True).sort_index()
df[df['lead_mgr'].str.contains('Stanl')]['lead_mgr'].unique()
from sklearn.cross_validation import KFold, train_test_split $ train, test= train_test_split(table3, test_size= 0.3, random_state= 42)
average_math_score = df_students['math_score'].mean() $ average_math_score
set_names_trans.tail()
leadDollarsClosedPerMonth.tail()
cursor = con.execute('SELECT * FROM samples') $ rows = cursor.fetchall() $ rows
np.std(pipe.tracking_error)
x = np.arange(-10, 11)
mgxs_lib.build_library()
df_archive_clean.info()
df_ad_airings_5['subjects'].isnull().sum()
df.loc[df.toes=='nan','toes'] = np.nan
autos.registration_year.describe() $
df_vow["Date"].dtype
df_train_time = df_train_21[['id', 'fulldate2', 'month']] $ df_test_time = df_test_21[['id', 'fulldate2', 'month']]
lsi_out = np.hstack(([np.array(lsimodel[tfidf[dictionary.doc2bow(trigram[bigram[doc]])]])[:, 1][:, np.newaxis] for doc in corpus])).T $ lsi_out = pd.DataFrame(lsi_out, columns=np.arange(lsi_out.shape[1]), index=np.arange(lsi_out.shape[0])) $ lsi_out['key'] = df['key']
pp_new = new_page_converted.mean() $ pp_old = old_page_converted.mean() $ p_new_old = pp_new - pp_old $ p_new_old
from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating
airbnb_df.loc[0:5, ['date_listed', 'listed_year_month']]
i = (np.arange(0, df3[0].iloc[-1]-df3[0].iloc[0], 43.259))/1000
for i in cpi_all.Region.cat.categories.tolist(): $     print i
sampling = 'M'
df_new['ab_page_CA']=df_new['ab_page']*df_new['CA'] $ df_new['ab_page_UK']=df_new['ab_page']*df_new['UK'] $ log_mod_new2=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','ab_page_CA','ab_page_UK']]) $ results_new2=log_mod_new2.fit() $ results_new2.summary2() $
df_daily2.sort_values(by=['DATE'], inplace=True)
from ipywidgets import IntSlider
z_score, p_value = sm.stats.proportions_ztest(count=[convert_old, convert_new], nobs=[n_old, n_new], alternative="smaller") $
for img in soup("img"): $     print img.attrs
API_KEY = 'Pk6ZuqMP8suwvbQsXshi'
pd.read_csv('data/eth-price.csv', parse_dates=[0]).head()
n_new = len(df2.query("group == 'treatment'")) $ print(n_new)
x, y = get_concated_data(raw_data, 'Close', 5) $ x[:10], y[:10]
print(rows) $ df.isnull().any().any(), df.shape
df2_with_country[['CA', 'US','UK']] = pd.get_dummies(df2_with_country['country'])[['CA','US','UK']] $ df2_with_country[['old_page','new_page']] = pd.get_dummies(df2_with_country['landing_page'])[['old_page','new_page']] $ df2_with_country.head() $
df_zeroConvs = pd.DataFrame(df_conv.iloc[zeroConvs]) $ df_zeroConvs
device.get_sensors('gas')
df = df.replace('712-2', '51529') $ df['Zip Code'] = df['Zip Code'].astype(int) $ df.dtypes
active_users.shape[0] / users.shape[0] # Only a third of users signed to the site were active
mi_index=pd.MultiIndex.from_arrays(arraylist) $ print(mi_index)
df2['DepTimeStr'].count() == df2['DepTime'].count()
a = 'this is the first half ' $ b = 'and this is the second half' $ a + b
multi_table([df_metric[df_metric['count']>30].head(n=3),df_metric[df_metric['count']>30].tail(n=3)])
mean = np.mean(data['len']) $ print("the mean length of the tweets is: {}".format(mean))
pd.to_datetime([1349720105100, 1349720105200, 1349720105300, 1349720105400, 1349720105500 ], unit='ms')
shopping_carts = pd.DataFrame(items) $ shopping_carts
df3.head()
df.isnull().values.any()
temps1 / temps2
dfHaw_Discharge['flow_MGD'] = dfHaw_Discharge['meanflow_cfs'] * 0.64631688969744
tree_c_features = c_features.apply(class_le.fit_transform)
infy_df = pd.read_csv( "INFY.csv" ) $ glaxo_df = pd.read_csv( "GLAXO.csv" ) $ beml_df = pd.read_csv( "BEML.csv" ) $ unitech_df = pd.read_csv( "UNITECH.csv" )
df_arch_clean['dog_stage'] = df_arch_clean['text'].str.extract('(doggo|floofer|pupper|puppo)', expand=True) $
df['comments'] = df['comments'].str.replace('\\', '')
df2[df2.duplicated(subset = ['user_id']) == True] 
autos.hist(column = 'odometer_km')
xirrs_all[splits[0]]
plt.plot(time_local, key_press, 'ro') $ plt.show()
lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ result = lm.fit() $ result.summary()
import re
logit_mod = sm.Logit(df_joined['converted'],df_joined[['intercept','ab_page','UK','CA']]) $ results_3 = logit_mod.fit() $ results_3.summary()
dedup = raw.copy()
wrd_clean['floofer'].value_counts()[:10]
daily_ret_b_mean = daily_ret_b.mean() $ daily_ret_b_mean
lda.print_topics(num_topics=3, num_words=20)
bt.summary()
TensorBoard.stop(327)
deal.head()
health_data['Guido','HR']
top_views = doctype_by_day.loc[:,doctype_by_day.min().sort_values(ascending=False)[:10]] $ ax = top_views.plot() $ ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
archive_df[archive_df.name == 'None'].sample(20, random_state=1210)
nold = len(df2[(df2.landing_page=='old_page')]) $ print(nold)
tweets_clean = pd.read_csv('tweets_clean.csv')
sampled_authors_saved = non_blocking_df_save_or_load( $     sampled_authors, "{0}/sampled_authors_6".format(fs_prefix)).alias("sampled_authors")
gender.value_counts()
combined_df =  pd.DataFrame(dict_df)
type(pres_df['subject_count_tmp'][0])
print(data.columns.values) $ data.drop(['WPts', 'LPts'], axis=1, inplace=True) $ print('-'*40) $ print(data.columns.values)
membership_loyalty[membership_loyalty.early_expiration_days == -14]
walk['2014-08-01 00:00'].mean()
plt.scatter(pca_df['PC1'],pca_df['PC2'], c = day_of_week, cmap='rainbow'); $ plt.colorbar();
df.year.unique()
bad_comments[0]
retweet_pairs[["FromType","FromName","Edge","ToType","ToName","Weight"]][retweet_pairs["Weight"]>1].to_csv("For_Graph_Commons.csv",index=False)
to_be_predicted_Day1 = 38.60 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
df.show(1)
week33 = week32.rename(columns={231:'231'}) $ stocks = stocks.rename(columns={'Week 32':'Week 33','224':'231'}) $ week33 = pd.merge(stocks,week33,on=['231','Tickers']) $ week33.drop_duplicates(subset='Link',inplace=True)
df['in_reply_to_status_id'].value_counts()
team_names.drop([6,24],inplace=True)  # Drop Knicks, Nets
randomdata1[(randomdata1 >= 1).any(axis=1)]
year_of_precipitation.describe()
regr = LinearRegression() $ regr.fit(X, y) $ fl = ["${:,.2f}".format(x) for x in list(regr.coef_)] $ for feature, coef in zip(X.columns, fl): $     print(feature, coef)
my_file = open('test_data//open_close_test.txt') $ my_file.close()
model.set_parameter(par=demand.append(new_demand), name='demand')
crime_month_window__, stops_month_window__ = stops_vs_crime(e_p_b_one,s_n_s_epb_one,'MS','MS') $ crime_two_week_window__, stops_month_window__ = stops_vs_crime(e_p_b_one,s_n_s_epb_one,'14D','MS') $ crime_week_window__, stops_month_window__ = stops_vs_crime(e_p_b_one,s_n_s_epb_one,'7D','MS') $ crime_day_window__,stops_month_window__ = stops_vs_crime(e_p_b_one,s_n_s_epb_one,'D','MS')
print("Number of Relationships in PRE-ATT&CK") $ print(len(all_pre['relationships'])) $ df = all_pre['relationships'] $ df = json_normalize(df) $ df.reindex(['id','relationship', 'source_object', 'target_object'], axis=1)[0:5]
closemeans.sort_values(ascending = False)
merged = pd.merge(prop, contribs, on= 'calaccess_committee_id')
austin.drop(3572, inplace=True)
pd.Period('2016-01-07')
run txt2pdf.py -o '2013 Snapshot.pdf' '2013 Snapshot.txt' $
vader_df.dtypes
len(live)
n_old = df2[df2.landing_page == 'old_page'].user_id.count() $ print(n_old)
newshows = shows[shows['first_year'] >= 1980]
or_list = df.query("group=='treatment'| landing_page=='new_page'").index
df_test['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True) $ df_test.head()
url = "https://api.vk.com/method/users.get?user_ids=6045249&access_token="+str(token)
archive_copy = archive_copy.merge(predictions_copy, on='tweet_id', how='inner')
print('No missing values \nIsnull: {}'.format(df.isnull().values.any())) $
print('Column array using a variable') $ WHO_Region=df['WHO Region'] $ np.array(WHO_Region)
print positiveInt.shape $ positiveIntMore=positiveInt.join(itemDataWithInt.reset_index(drop=True).reset_index().set_index('id')['index'],on='item_id') $ positiveIntMore.columns=['item_id','user_id','interaction_type','jobroles','title','tags','index_corpus'] $ print positiveIntMore.shape $ positiveIntMore.head()
countries_df = pd.read_csv('./countries.csv') $ countries_df.head() $
ttTimeEntry['DT'] = pd.to_datetime(ttTimeEntry['DT'])
raw_distinct_authors_latest_commit = authors_by_project_and_commit_df.groupBy( $     "project_name", "email").agg( $     F.last("Author").alias("Author"), F.max("CommitDate").alias("latest_commit")) $ raw_distinct_authors_latest_commit.persist()
df2 = pd.DataFrame(np.random.randn(2,3),columns=["Col1","Col2","Col3"]) $ df2
url = "https://www.cgv.id/en/movies/info/17009800" $ request = urllib.request.Request(url) $ page = urllib.request.urlopen(request) $ soup = BeautifulSoup(page,"lxml") $ soup.find('div', class_='movie-info-title').string
from sklearn.preprocessing import OneHotEncoder  $ encoder = OneHotEncoder(sparse=False)  $ y_onehot = encoder.fit_transform(y)  $ y[0], y_onehot[0,:] $
autos = autos.drop(["nr_of_pictures"], axis=1)
s = pd.Series([7, 'AmarAkbar', 3.14, -1789710578, 'Sholay']) $ s
conn_b.commit()
p_old = df2['converted'].mean() $ p_old
fe.stat(lossprob2)
plt.plot(data['Year'], data['Body_Count'], 'rx')
%debug
tweet_text = mars_weather['text'] $ date = mars_weather['created_at'] $ tweet_date = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(date,'%a %b %d %H:%M:%S +0000 %Y')) $
image_predictions_df.info()
train['Title'] = train.apply(lambda row: row[0].split(" ")[0], axis = 1) $ train['Gender'] = train.apply(lambda row: row[0].split(" ")[1].split("G")[0], axis = 1) $ test['Title'] = test.apply(lambda row: row[0].split(" ")[0], axis = 1) $ test['Gender'] = test.apply(lambda row: row[0].split(" ")[1].split("G")[0], axis = 1) $ train.head(3)
executable_path = {'executable_path': 'chromedriver.exe'} $ browser = Browser('chrome', **executable_path, headless=False)
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'], $                   'data': range(6)}, columns=['key', 'data']) $ df
df1=df1[(df1['min_price']<=df1['max_price']) & (df1['modal_price'] <= df1['max_price'])] $ df1=df1[(df1['max_price']>0) & (df1['modal_price']>0) & (df1['max_price']<=1000000)]
new_page_converted/n_new - old_page_converted/n_old
df_2002.dropna(inplace=True) $ df_2002
run txt2pdf.py -o"2018-06-14 2148 UNIVERSITY HOSPITALS OF CLEVELAND Sorted by Payments.pdf"  "2018-06-14 2148 UNIVERSITY HOSPITALS OF CLEVELAND Sorted by Payments.txt"
git_log['timestamp']=pd.to_datetime(git_log['timestamp'], unit='s') $ print(git_log['timestamp'].describe())
url_HOU = "https://texans.seasonticketrights.com/Images/Teams/HoustonTexans/SalesData/Houston-Texans-Sales-Data.xls"
matrix_det = tf.matrix_determinant(matrix2)
m[0].bs = 1 $ m.eval() $ m.reset() $ res, *_ = m(t) $ m[0].bs=bs
df['lemma'] = df['lemma'].apply(lambda x: ' '.join(x))
df2[df2['user_id']==773192] # there are two entries with the same user_id - 773192
data_adv = data_adv.round(4)
reddit_comments_data.count()
df2.index $
raw_windows_path = "/home/ahmet/notebooks/data/G9_data/Raw/snippets/" $ processed_file_path = "/home/ahmet/notebooks/data/G9_data/processed.csv"
(keys.shape, keys_0611.shape)
s.str.get_dummies()
raw_text = [raw_text]
num_of_converted = df2[df2.converted == 1] $ p_new = len(num_of_converted)/len(df2) $ p_new
summed.fillna(0)
df2[df2['dupe']==True]
hr_total = hr2006 + hr2007 $ hr_total
rng = pd.date_range(start = '6/1/2017', end = '6/30/2017', freq = 'B') $ rng
print("Number of Relationships in PRE-ATT&CK") $ relationships = lift.get_all_pre_relationships() $ print(len(relationships)) $ df = json_normalize(relationships) $ df.reindex(['id','relationship', 'relationship_description', 'source_object', 'target_object'], axis=1)[0:5]
t = pd.datetime.today() - pd.Timestamp('1/1/1970')
app_ver_map['1.0.4']
prcp_analysis_df = prcp_analysis_df.loc[prcp_analysis_df["date"]>=pa_min_date]
import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
x_train.head()
type(zc['zipcode'][0])
pd_data.paidTime = pd_data.paidTime.map(lambda x: pd.to_datetime( $     x, unit='ms', utc=True).astimezone('Asia/Shanghai').strftime('%Y-%m-%d %H:%M:%S') if x else '')
datAll.shape
df.to_excel("../../data/stocks2.xlsx")
import numpy as np $ import pandas as pd
mean_per_group = df.groupby('group').converted.mean() $ mean_per_group
def print_tweet(tweet): $     print ("@%s - %s (%s)" % (tweet.user.screen_name, tweet.user.name, tweet.created_at)) $     print (tweet.text) $ tweet=results[0] $ print_tweet(tweet)
df_country=pd.read_csv('countries.csv') $ df_country.country.unique()
for element in x: $     print element['id'] $     print(element['full_text']) $     print('--')
es_client = Elasticsearch('http://localhost:9200')
(np.cumsum(df.Category.value_counts())/len(df)).head(20)
f_lr_hash_modeling2 = spark.read.csv(os.path.join(mungepath, "logisitc_regression/lasso/f_lr_hash_inter2_2p18_noip_modeling2_split"), header=True) $ f_lr_hash_test = spark.read.csv(os.path.join(mungepath, "logisitc_regression/lasso/f_lr_hash_inter2_2p18_noip_test"), header=True) $ print('Found %d observations in f_lr_hash_modeling2.' %f_lr_hash_modeling2.count()) $ print('Found %d observations in f_lr_hash_test.' %f_lr_hash_test.count())
sox.loc[sox.hour >= 18,'late'] = 1
new_page_converted = np.random.binomial(n_new, p_new)
df = pd.merge(tweets, users[['screenName', 'lang']], how='left', on='screenName')
rounds_df.shape[0] * 0.15
print('potential.pair_info() ->') $ print(potential.pair_info()) $ print() $ print("potential.pair_info(['Al', 'Al', 'Al']) ->") $ print(potential.pair_info(['Al', 'Al', 'Al']))
result_3 = pd.concat([df1, df3], ignore_index = True) # same as option 1 but with reset index $ result_3
last_seen =  max(df.lastSeen) $ last_seen
max([len(h.tweets) for h in heap])
pd.read_excel('PPB_gang_records_UPDATED_100516.xlsx',sheetname=1).head()
data_url = "https://data.wprdc.org/datastore/dump/b7cb30c8-b179-43ff-8655-f24880b0f578" $ data = pd.read_csv(data_url, index_col="date", parse_dates=True) $ data = data.drop(columns="_id") $ data.head()
plt.plot(pipe.tracking_error)
tmp1 = air_reserve.groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'}) $ tmp2 = air_reserve.groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'}) $ air_reserve = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])
test_df.head(2)
cercanasA1_11_14Entre150Y200mts = cercanasA1_11_14.loc[(cercanasA1_11_14['surface_total_in_m2'] >= 150) & (cercanasA1_11_14['surface_total_in_m2'] < 200)] $ cercanasA1_11_14Entre150Y200mts.loc[:, 'Distancia a 1-11-14'] = cercanasA1_11_14Entre150Y200mts.apply(descripcionDistancia, axis = 1) $ cercanasA1_11_14Entre150Y200mts.loc[:, ['price', 'Distancia a 1-11-14']].groupby('Distancia a 1-11-14').agg(np.mean)
df_total.to_csv("en-wikipedia_traffic_200801-201709.csv", sep='\t', index=False)
tweets2.text[0]
tweet_image_clean['tweet_id'].isin(tweet_archive_clean['tweet_id']).value_counts()
driver.close()
with open('../data/rockville_unclean.json') as f: $     data = json.load(f) $
clf = svm.SVC(kernel='linear', C = 1.0) $ clf.fit(X, y)
new_page_converted = np.random.choice([1,0], size = n_new, p = [p_new,(1-p_new)])
clean_madrid['clean_text'].replace('', np.nan, inplace=True) $ print(clean_madrid['clean_text'].isna().sum()) $ df_madrid.iloc[clean_madrid[clean_madrid.isna().any(axis=1)].index,:].head()
tfidf_vectorizer = TfidfVectorizer(min_df=0) $ tfidf_vectorizer.fit(dataset.data)#TODO $ tfidf_matrix = tfidf_vectorizer.transform(dataset.data)#TODO $ tfidf_matrix $
df.head(5)
from dateutil import relativedelta
test.head(1)
converted = (df.query('converted == 1').count()[0]) $ total = df.shape[0] $ print ("The number of user converted is {}".format(float(converted) /  float(total)))
avg_day_of_month14 = avg_day_of_month14.rename(columns={"day_of_year":"rides"}) $ avg_day_of_month14.head()
rng = pd.date_range('3/6/2012 00:00:00', periods=10,freq="D",tz="US/Mountain") $ rng.tz, rng[0].tz
journeys.fillna(method = 'ffill', inplace = True) $ journeys.fillna(method = 'bfill', inplace = True) $
a=np.random.normal(0,50,size=(5,20)) $ plt.plot(a) $ plt.show()
df.info()
soup.find('div', class_="poster-section left").find('img')['src']
precipitation_2yearsago = session.query(Measurement.prcp, Measurement.date).filter(Measurement.date > query_date_2yearsago).order_by(Measurement.date.desc()).all() $ print(precipitation_2yearsago)
airlines_day_unstacked = airlines_day_unstacked[(airlines_day_unstacked != 0).all(1)]
df["numerized_tokens"] = numerized_tokens
plt.hist(yes.chol, bins=12, range=(100,350)) $ plt.hist(no.chol, bins=12, range=(100,350), color='g') $ plt.title('Cholesterol distribution');
sns.set() $ sns.distplot(df_input_pd.Resp_time.dropna(), kde=True, color='b')
df_h1b_mv_ft.pw_1.describe()
print("dfMonth = ",dfMonth['Contract Value (Monthly)'].sum(), "dfMonth Project Count = ", dfMonth['Project Name'].nunique()) $ print("dfDay = ",dfDay['Contract Value (Daily)'].sum(), "dfDay Project Count = ", dfDay['Project Name'].nunique())
dep = 'Sales' $ joined = joined[cat_vars+contin_vars+[dep, 'Date']].copy()
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ from pandas.tools.plotting import scatter_matrix $ from matplotlib import rcParams
talks.head()
automl = AutoSklearnRegressor( $         time_left_for_this_task=360, $         per_run_time_limit=90)
autos["registration_year"].describe()
products_with_nulls=len(nulls['item_nbr'].unique()) $ all_products=len(items['item_nbr'].unique()) $ products_with_nulls/all_products
np.mean(df['converted'])
tweet_image_clean.dtypes
Copy_a_file_to_put_in_a_directory(str(today),'txt2pdf.py') $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.chdir('../') $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $
scores['test_accuracy'].mean() $
tweets.head()
h2020_proj.shape, h2020_part.shape, h2020.shape
lr.fit(features_class_norm, overdue_transf) $ print_feature_importance(vectorizer.feature_names_, lr.coef_)
df['age'] = 2017 - df['birth year'] $ df['age'].dropna(inplace= True)
beforeUsersAfter=fullDf[(fullDf.user.isin(beforeUsers.values))&(fullDf.index>pd.datetime(2015,4,25))]
clf = GridSearchCV(svc, parameters, verbose=10, n_jobs=4)
web.DataReader("A576RC1A027NBEA","fred",datetime.date(1929,1,1),datetime.date(2013,1,1))
%matplotlib inline $ california_house_dataframe.hist('median_income')
df['converted'].mean()
def sort_files(f): $     return int(f.split('_')[4][4:])
df.loc[df.toes.str.match(pattern1)==True]
import requests $ import json $ url = "https://www.coindesk.com/category/markets-news/page/3/" $ data = '' $ resp = requests.post(url, data=data, headers=headers)
print(autos['price'].unique().shape) $ print(autos['price'].describe()) $ print(autos['price'].value_counts().head(15))
merged.groupby('committee_name_x')['amount'].sum().reset_index().sort_values('amount', ascending = False)
bucket.upload_dir('data/heat-pump/raw/', 'heat-pump/raw', clear_dest_dir=True)
(ggplot(raw_large_grid_df.groupby(['eyetracker','posx','posy'],as_index=False).mean(),aes(x="posx",y="posy",size="rms"))+geom_point()+facet_wrap("~eyetracker"))+coord_fixed()
save_picle(embeddings_matrix, embedding_save_path)
new_reps.Kasich.astype("float64").describe()
geocoded_df['Judgment.In.Favor.Of.Plaintiff'] = for_plaintiff
plots.proto_distribution(traffic_type = 1,scale = "log") #Malicious traffic log scale
from statsmodels.graphics.tsaplots import plot_acf $ plot_acf(dataBPL['Total_Demand_KW'], lags=5209) $
model_preds['predicted_class'] = predictions $ model_preds['actual_class'] = labels $ model_preds['predic_prob'] = predict_prob $ model_preds['prob_off'] = abs(df['predicted_class'] - df['predic_prob'])
prob_converted = df.converted.mean() $ print('The probability of conversion is: ' + str(prob_converted))
df = web.DataReader('TSLA', 'google', datetime(2016, 1, 1)) $ df
train.head()
df_ml_51 = df.copy() $ df_ml_51.index.rename('date', inplace=True) $ df_ml_51_01=df_ml_51.copy()
labeled.drop(['name_feat'], axis='columns', inplace=True)
speeches_df4.shape
import pandas as pd $ ts
prob_convert = df2.converted.mean() $ print("Probability of individual converting is :", prob_convert)
portfolio_df = pd.read_excel('Sample stocks acquisition dates_costs.xlsx', sheetname='Sample') $ portfolio_df.head(10)
df_daily5=df_daily.groupby(["STATION", "DATE"]).DAILY_ENTRIES.sum().reset_index() $ df_daily5.head(5) $
ks_projects['deadline'] = ks_projects['deadline'].apply(lambda x: x.split(" ")[0]) $ ks_projects['launched'] = ks_projects['launched'].apply(lambda x: x.split(" ")[0]) $ ks_projects['deadline'] = ks_projects['deadline'].apply(lambda x: datetime.strptime(x, "%Y-%m-%d")) $ ks_projects['launched'] = ks_projects['launched'].apply(lambda x: datetime.strptime(x, "%Y-%m-%d"))
autos.loc[autos['price'].between(0, 1e5), 'price'].describe()
test.dropna().shape
assert (len(compiled_data)==len(data)), "Input data and output data has a different count"
X_train, X_test, y_train, y_test = train_test_split(trains_fe2_x, trains_fe2_y, test_size=0.2) $ print(X_train.shape) $ print(y_train.shape) $ print(X_test.shape) $ print(y_test.shape)
plt.hist(p_diffs)
df3 = df2.join(df_countries.set_index('user_id'), on='user_id')
plt.figure(figsize=(10,4)) $ plt.plot(Year2_df.Month, Year2_df.Sales_in_CAD, 'bo', Year1_df.Month, Year1_df.Sales_in_CAD, 'ko', $          Year3_df.Month, Year3_df.Sales_in_CAD, 'ro') $
df = train_data
to_be_predicted_Day2 = 31.33310125 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
meta = pd.read_excel("./Data/microbiome/metadata.xls", index_col="BARCODE").fillna(value="Na")
print(list(cos.buckets.all()))
people = pd.DataFrame(np.random.randn(5, 5), columns=['a', 'b', 'c', 'd', 'e'], index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis']) $ people
logit_country = sm.Logit(df3['converted'], df3[['intercept','country_US', 'country_UK']]) $ results2 = logit_country.fit()
out_total=final[['chips','ra','dec','redshift','priority']] $
goo2 = pd.read_csv('data/goog.csv',index_col='Date', parse_dates=['Date'])
access_logs_df.createOrReplaceTempView('AccessLog') $
polarity_avg.reset_index(inplace=True) $ polarity_count.reset_index(inplace=True)
import statsmodels.api as sm $ logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'treatment']])
sess.get_data('ibm us equity','px last')
df_json_tweets.info()
status.isnull().any()
%%time $ df.head()
def join_df(dat1, dat2): $     return dat1.join(dat2, how='left')
daily_averages['2014-10-2':'2014-10-7'].head()
query_date = dt.date(2017, 8, 23) - dt.timedelta(days=7) $ print("Query Date: ", query_date)
autos['date_crawled'].str[:10].describe()
print('WITHOUT resetting the random seed:') $ print('%d different bacteria between the two function calls' % len(set(dd.feature_metadata.index)^set(dd2.feature_metadata.index)))
data_compare['SA_mix'] = np.array([ analize_sentiment_multilingual(tweet) for tweet in data_compare['tweets_original'] ])
pop_df = pop.unstack() $ pop_df
theft.sort_values('DATE_OF_OCCURRENCE', inplace=True, ascending=True) $ theft.head()
abc = ((prediction['lgb'] + prediction['xg'] + prediction['cat'] + prediction['rf'] + prediction['lgb_s'])/5) $ abc1 = (( prediction['rf'] + prediction['lgb_s'])/2) $ abc2 = (np.sqrt( prediction['rf'] * prediction['lgb_s'])) $
cog_simband_times = cog_simband_times.drop(labels=['ST-OTS BHC0066-1'])
df1.describe()
xpdraft1 = pd.merge(xpdraft, orgName, left_on='orgId', right_on='orgId', how='left') $ xpdraft1.head()
df_ad_state_metro_1[df_ad_state_metro_1['candidates'] =='Donald Trump, Hillary Clinton'].head(5)
titanic.deck.unique()
is_attributed[:5]
giss_temp.dropna(how="any").tail()
deaths.shape
print(test_str)
import pandas as pd $ url = 'https://raw.githubusercontent.com/chrisalbon/simulated_datasets/master/data.xlsx' $ df = pd.read_excel(url, sheet_name=0, header=1) $ df.head(2)
stations = session.query(Precip.date, Precip.prcp, Precip.station) $ station_df = pd.DataFrame(stations[:], columns = ['Date', 'Precip', 'Station ID'])
print(autos["registration_year"].value_counts(normalize=True) $                           .sort_index())
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
Results_kNN2500 = Results_kNN2500[['ID', 'Approved']] $ Results_kNN2500.head()
to_be_predicted_Day3 = 36.47754126 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
results
print("Number of rows in the dataset are: {}".format(df.shape[0]))
MergeWeek['Sales Target'] = 242308
soup = BeautifulSoup(html, "lxml")
real_old = len(df2.query('group == "control" & converted == 1'))/len(df2.query('group == "control"')) $ print(real_old)
%time df = pd.read_csv("/data/311_Service_Requests_from_2010_to_Present.csv", error_bad_lines=False) $
(autos["date_crawled"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         )
print str(loadedModelArtifact.name)
w = train['label'].value_counts()/train['label'].value_counts().sum() $ w = w.max()/w
ts.shift(-2)
day_zero = weather['date'].min()
feature_imp_RF=pd.DataFrame(list(zip(features,trained_model_RF.feature_importances_))) $ column_names_RF= ['features','RF_imp'] $ feature_imp_RF.columns= column_names_RF
df[((df['group']=='treatment') &(df['landing_page']!='new_page'))|((df['group']!='treatment') &(df['landing_page']=='new_page'))].group.count()
df.columns=df.columns.str.strip() #strip off trailing and leading whitespace
print('Autocorrelation 1: ', BPL_electric['Total_Demand_KW'].autocorr(1)) $ print('Autocorrelation 7: ', BPL_electric['Total_Demand_KW'].autocorr(24)) $ print('Autocorrelation 168: ', BPL_electric['Total_Demand_KW'].autocorr(168))
csvtonumpy.csvtonumpy(inputFile, outputFile, True)
corn_vege.size()
users = df2.nunique()["user_id"] $ print("Number of unique users - {}".format(users))
week35 = week34.rename(columns={245:'245'}) $ stocks = stocks.rename(columns={'Week 34':'Week 35','238':'245'}) $ week35 = pd.merge(stocks,week35,on=['245','Tickers']) $ week35.drop_duplicates(subset='Link',inplace=True)
autos["ad_created"].str[:10].value_counts(dropna = False, normalize = True).sort_index()
dfg = dfg.sort_values(['discharges'], ascending=[False]) $ dfg = dfg.reset_index(['drg3']) $ dfg.head()
df3 = df.copy() $ df3 = df3.drop('ENTRIES',1) $ df3 = df3.drop('EXITS',1) $
c['location'].value_counts(dropna=False)
len(filenames.unique())#apparently there are duplicates somehow.
pscore = pscore.reshape(-1, 1) $ btc_price = btc_price.reshape(-1, 1) $ print(pscore[:5]) $ print(btc_price[:5])
ts.resample('5Min').sum()
df_tot.info()
at_mentions_pattern = re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-\.]))@([A-Za-z0-9_]+)')
archive_df_clean['date'] = archive_df_clean['timestamp'].apply(lambda time: time.strftime('%m-%d-%Y')) $ archive_df_clean['time'] = archive_df_clean['timestamp'].apply(lambda time: time.strftime('%H:%M'))
GA_profit.head()
bobby_ols = ols('opening_gross ~ star_avg',dftouse_four).fit() $ bobby_ols.summary()
df1['forcast'].plot() $ plt.xlabel('Date') $ plt.ylabel('Price')
merge.head(2)
with open('myfile.csv', 'r') as f: $     print(f.read())
df_providers[['id_num','name','year',\ $               'drg3','discharges','discharge_rank', 'disc_times_pay','payment_rank']].head()
twosample_pol = scipy.stats.ttest_ind(locationing.polarity, tweetering.polarity) $ twosample_pol
no_of_unique_users_in_df2 = df2.user_id.nunique() $ no_of_unique_users_in_df2
newdf.loc[newdf['score'] == 0.187218571]
unemp = unemp.dropna().reset_index(drop=True) $ unemp.rate = unemp.rate.str.replace('%', '').astype(float)
images_predictions.p1.value_counts()
dates = pd.date_range('1950-01', '2013-03', freq='M') $ ts =pd.DataFrame(np.random.randn(758, 4), columns=list('ABCD'), index=dates) $ ts['year'] = ts.index.year $ ts.drop('year', axis=1).cumsum().plot(figsize=(10, 6))
df1_clean.rating_denominator.value_counts()
cur.close() $ con.close()
finals[finals.season==2016].head()
talks_train['onespeaker']=onespeaker
tweets_df.in_reply_to_screen_name.describe()
join_e.show(5)
train_view=graf_train[['DETAILS', 0, 1, 2, 3 ,4, 5, 6, 7]] $ test_view=graf_test[['DETAILS', 0, 1, 2, 3, 4, 5, 6, 7]]
nb.class_count_
df[df.index.month.isin([8,9,10])]['Complaint Type'].value_counts().head(10).plot(kind='bar', color='yellow', figsize=(18,5)) $ df[df.index.month.isin([12,1,2])]['Complaint Type'].value_counts().head(10).plot(kind='bar', figsize=(18,5), alpha=0.5)
df2.info()  
data
price2017 = price2017[['Date', 'Time', 'Germany/Austria/Luxembourg[Euro/MWh]']] $ price2017.columns = ['Date', 'Time', 'DE-AT-LUX'] $ price2017.head()
model.doesnt_match('man woman child kitchen'.split())
for column in ['Announced At','Created At','Shipped At','Updated At']: $     df[column] = pd.to_datetime(df[column])
new_page_converted = np.random.choice([0, 1], size= new, p=[(1-x), x]) $ new_page_converted.mean() $
leadPerMonth.mean().plot.bar(figsize={12,6}) $ plt.title('Average Leads per month');
morning_rush.iloc[:1000][['latitude', 'longitude']].get_values()
df['day'] = df.day.map({1: 'mon', 2: 'tues', 3: 'weds', 4:'thurs', 5:'fri'   })
ip['p2_conf'].describe()
from pyspark.sql.functions import col $ file2=file.where((col('trans_start_month')==7) | (col('trans_start_month')==8)) $ file2.show(3)
kk = pd.read_csv('data/kktv/data-001.csv') $ kk.head(30)
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=31000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
rows=df.shape[0] $ print("The total number of rows are : {}".format(rows))
details['Released'] = pd.to_datetime(details['Released'])
df_columns['Time value'] = pd.to_datetime(df_columns['Created Date'], format="%m/%d/%Y %H:%M:%S %p") $ df_columns['Day of the week'] = df_columns['Time value'].dt.weekday_name $ df_columns.head() $
msft = pd.read_csv('msft.csv', index_col=0) $ msft.head()
daily_df = pd.concat([predictions_daily, stock_daily], axis=1, join='inner') $ daily_df.head()
ind = pd.read_sql_table('industry_type', engine)
from yahoo_fantasy_sports import YahooFantasySports as yfs_test
X_d = pd.get_dummies(X, columns=['domain_d'], drop_first=True)
s519397_df["prcp"].min()
def find_dot(x): $     if '.' in str(x): $         return True $     else: $         return False
assortativity = nx.attribute_assortativity_coefficient(multiG, 'politicalSpectrum') $ print assortativity
daily_cases = daily_cases[daily_cases<200]
rng.day
top_songs['Streams'].isnull().sum()
madrid_coords = {'lat': 40.4, 'lon': -3.7} $ r = requests.get('http://api.open-notify.org/iss-pass.json', params=madrid_coords) $ json.loads(r.content)
Image("/Users/jamespearce/repos/dl/data/dogscats/train/dog.9126.jpg")
Precipitation_DF.head(10)
mod = sm.Logit(df3['ab_page'], df3[['converted','country_code_CA','country_code_UK','country_code_US','intercept']]) $ res = mod.fit() $ res.summary()
total = flattened_pandas_df[['title','body','tag']].reset_index(drop=True)
rf_yhat = knn.predict(X_test)
r.json()
Type_date = Exchange_df[['New_or_Returning', 'Sales_in_CAD']] $ Type_date.head()
transConcat = trans["Improvements"] $ data.update(transConcat) $ data[data["SiteName"] == "Tokyo Joypolis"]["Improvements"].iloc[:20] $
autos = autos[autos['price'] < 10000000]
smo = SMOTE(random_state=0) $ X_resampled, y_resampled = smo.fit_sample(X_train, y_train)
len(df.user_id.unique())
df[df.index.month.isin([11, 12, 1, 2])]['Complaint Type'].value_counts().head()
pt_all=pd.DataFrame.pivot_table(df_users_6,index=['cohort'],values=['uid'],aggfunc='count',fill_value=0)
predictions = dtModel.transform(testData)
vader_df.head()
a = ['10','22','111']
byYear = df.resample('y').count()['id'] $ byYear
wrd_clean = wrd.copy() $ wrd_clean['name'] = wrd_clean['name'].apply(lambda x: 'NaN' if x == "a" else x) $ wrd_clean['name'] = wrd_clean['name'].apply(lambda x: 'NaN' if x == "an" else x)
order_data.shape
questions['zipcode'].unique() $
station_mean = session.query(weather.date, func.avg(weather.prcp)).\ $     filter(weather.date.between('2015-01-01','2015-12-31')).\ $     group_by(weather.date).all() $ station_mean
%sql \ $ SELECT twitter.user_id, twitter.tweet_text, twitter.heats FROM twitter \ $   WHERE twitter.tweet_text REGEXP 'Roger Federer|Tennis' \ $     AND heats>5 \ $   ORDER BY heats DESC ;
pd.concat([per_tweet_archive_by_month, ave_ratings_over_time], axis=1).plot(subplots=True)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print(z_score, p_value)
difference_p = p_new - p_old $ print("The difference in probability of conversion for the new page and the old page is: " + str(difference_p))
type.__call__('Example', (), {})
df_test, _, nas, mapper = proc_df(joined_test, 'Sales', do_scale=True, skip_flds=['Id'], $                                   mapper=mapper, na_dict=nas)
app_pivot = app_pivot.pivot(index='ab_test_group', columns='is_application', values='email') $ app_pivot
archive_version = '2017-11-10' # i.e. '2017-11-10'
terror.loc[1390]
tokaise["stamp"] = to_datetime(tokaise["created_at"],format='%a %b %d %H:%M:%S +0000 %Y') $ tokaise.head()
for post in test_collection.find(): $     pprint.pprint(post)
tweets_pp = pd.concat([multi.reset_index(), pp.reset_index()], axis=1) $ tweets_pp.head(2)
df_A3=pd.read_csv("classA.csv",index_col=0) $ df_A3.index $ df_A3
newdf['Date'] = newdf['Date'].apply(lambda x: x.split('-')[0] + x.split('-')[1])
df_ad_airings_5['location'][0].split(",")[1]
coo_matrix((df2['D'], (df2['D'], df2['F']))).toarray()
X = pd.DataFrame(data.data, columns=data.feature_names) $ X
df.loc[6]={'age':20,'gender':'F','name':'qoo', 'employee':True} $ df
df_user_info.head(3)
repos.describe()
np.where(df_train.columns.values != 'date_first_booking')
agent.epsilon=0 # Don't forget to reset epsilon back to previous value if you want to go on training
df.head()
df.text.isnull().sum() $ df.text.fillna('', inplace=True)
tmpdf_md
unsorted_df.sort_values(by='col1',ascending=False)
today = datetime.now() $ dates = list(pd.date_range(join_date, today)) $ print("Days Since Joining: " + str(len(dates))) # days since joining
py.iplot(data_predict.groupby(freq).sum()[['sales', 'predict']].iplot(asFigure=True, $                                kind='bar',xTitle='Dates',yTitle='Sales',title='Actual vs. Predicted'))
learner = md.get_model( $     opt_fn, em_sz, nh, nl, dropouti=0.05, dropout=0.05, $     wdrop=0.1, dropoute=0.02, dropouth=0.05) $ learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)
liberiaCases = liberiaCases.rename(columns = {'National':'New cases'}) $ liberiaDeaths = liberiaDeaths.rename(columns = {'National':'New deaths'})
All_tweet_data_v2.name[All_tweet_data_v2.name.str.len() < 2]='None'
df[['text', 'retweet_count', 'date']][df.retweet_count == np.min(df.retweet_count)]
raw['parse_date'] = pd.to_datetime(raw['parse_date'])
python tutorial_part_1.py | nc -lk 9999
non_grad_age_mean = records3[records3['Graduated'] == 'No']['Age'].mean() $ non_grad_age_mean
test_df = train_df.reset_index() $ print test_df.created[0], type(test_df.created[0])
apple.index.is_unique
print(16 in squares.values)
autos.price.describe()
train.created.describe()
pres_df['metro_area'].value_counts()
resampled_groups = Exchange_df.groupby(['Field', 'Sales_in_CAD']).sum() $ resampled_groups.head()
year_ago = datetime.date(2017, 8, 23) - datetime.timedelta(days = 365) $ print(year_ago)
sum(data.days_repayment.isnull())
fig = plt.figure(figsize=(10,4)) $ ax = fig.add_subplot(111) $ ax = resid_6203.plot(ax=ax);
autos["brand"].value_counts(normalize=True)
a = 5 $ isinstance(a, int)
sum(df2['user_id'].duplicated())
train_words.shape
train.created.max(), train.created.min()
cur.fetchall()
new_page_converted = np.random.binomial(n_new,p_new) $
def normalize_user_properties_column(df): $     df_user_properties = json_normalize(data=df.user_properties) $     df_user_properties = df_user_properties.set_index(df.index) $     standardize_column_names(df_user_properties) $     return df.merge(df_user_properties, left_index=True, right_index=True) $
os.listdir(B.paths['subset_directory'])
get_day(suspects_with_25_1)
df.groupby('Year')[['Points','Rank']].agg(np.sum)
so_score_10_or_more = so[criteria] $ so_score_10_or_more.head()
today = datetime.datetime.today() $
%pylab inline $ import re $ import datetime as dt
data.head()
len(" ".join(tm['text']).split(" "))
joined['QorE'] = joined['Headline'].str.contains(r'\!|\?').astype(int) $ joined['Q&A'] = joined['Headline'].str.contains(r'Q\. and A\.').astype(int)
len(df[df.location_id != df.prev_location_id])
londonDFSubsetWithCounts['centroid'] = londonDFSubsetWithCounts['geometry'].centroid $ londonDFSubsetWithCounts['lng'] = londonDFSubsetWithCounts['centroid'].map(lambda x: x.xy[0][0]) $ londonDFSubsetWithCounts['lat'] = londonDFSubsetWithCounts['centroid'].map(lambda x: x.xy[1][0])
aggregated_data
df_t = df_new $ df_t['timestamp'] = pd.to_datetime(df_t['timestamp']) $ df_t.head(3)
baseball_newind.index.is_unique
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ z_score, p_value $
sessions = pd.read_csv('./airbnb firstdestinations/sessions.csv')
df_clean2.sample(5)
df3['country'].value_counts()
model = gensim.models.Word2Vec(sentences, min_count=1)
autos = autos[autos['registration_year'].between(1900, 2016)] $ autos['registration_year'].value_counts(normalize = True).head(10)
len(baby_scn_postgen)
match[match.iloc[:,66 :77].notnull().any(axis=1)].iloc[:5,66 :77] # get rows with non-nulls in columns 66 :77
store_preds_2016 = x_2016 $ store_preds_2016['PredictedSales'] = preds_2016
tweets_df[tweets_df['id'].duplicated()]
df_imputed_median_NOTCLEAN1A = df_NOTCLEAN1A.fillna(df_NOTCLEAN1A.median())
DataAPI.write.update_factors(factors=factors,trading_days=trading_days , override=False, log=False)
movies=pd.read_csv('..\\Data\\ml-20m\\ml-20m\\movies.csv')
print(f'total_open_users: {len(open_users)}') $ print(f'total_pass: {len(open_users_test[open_users_test["TEST"]])}') $ print(f'total_fail: {len(open_users_test[~open_users_test["TEST"]])}') $ print(f'failed_names: {open_users_test[~open_users_test["TEST"]]["USERNAME"].values}')
colmns=['category', 'launched_year', 'launched_quarter', 'goal_cat_perc', 'participants'] $ ks_particpants.columns=colmns
df_pagecounts_mobile.head()
df
samp_size = n $ joined_samp = joined.set_index("timePeriodStart")
print("Probability of control group:", $       df2[df2['group']=='treatment']['converted'].mean())
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=15000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
with open('data/chefkoch_01.json') as data_file:    $     chef01 = json.load(data_file) $ clean_new(chef01) $ chef01df = convert(chef01) $ chef01df.info()
bruins['season'] = (pd.DatetimeIndex(bruins.date) - np.timedelta64(9,'M')).year
full_globe_temp = pd.read_table(filename, sep="\s+", names=["year", "mean temp"], $                                 index_col=0) $ full_globe_temp
stadium_arr.groupby('away_team')['arrests'].sum().sort_values(ascending=False)[:10].plot(kind='bar', title='Overall Number of Arrests when Away (top 10)') $ plt.savefig('overallArrestsAtAway.png', bbox_inches='tight')
cityID = '7068dd9474ab6973' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Toledo.append(tweet) 
tweet_th[tweet_th['text'].apply(lambda x: "I'm at" in x)]['text']
model.compile(loss='binary_crossentropy', $               optimizer='adam', $               metrics=['accuracy'])
transactions[~transactions.UserID.isin(users.UserID)]
articles.head()
df3[['CA', 'US']] = pd.get_dummies(df3['country'])[['CA', 'US']]
print(df.head()) $ print('\n Types Data: ') $ print(df.dtypes)
a + 1.2
offseason13["InorOff"] = "Offseason"
pd.merge(transactions,transactions,how='outer',on='UserID')
from bs4 import BeautifulSoup $ import requests $ from selenium import webdriver $ import time $ import pandas as pd
backup=education_data['Category']
repeated_user_id = df2.user_id.value_counts().reset_index().query('user_id > 1')['index'][0] $ repeated_user_id
liquor2016_q1_days = pd.DataFrame.from_dict(liquor2016_q1.Date.groupby(liquor2016_q1.StoreNumber).unique().apply(lambda x: len(x))) $ liquor2016_q1_days.columns = ['Days'] $ liquor2016_q1_days.tail()
np.mean(df.query('group == "control"')['converted'])
people = df.In.unique() $ people.tolist() $
epoch3_df.head(5)
token_sendreceiveCnt = token_sendreceiveCnt[["ID","month","sendReceiveCnt"]]
r = requests.get(data_request_url, params=params, auth=(USERNAME, TOKEN)) $ data = r.json()
testing_array.shape
raw_data = raw_data.loc[raw_data['date'] >= pd.to_datetime(start_date)] $ raw_data = raw_data.loc[raw_data['date'] <= pd.to_datetime(end_date)]
df2 = df2.drop(df2.index[2893])
df_combined = df2.merge(df_countries, on='user_id') $ df_combined.head()
ss_sparse = (~df_EMR_dd_dummies.isnull()).sum() < 3 $ ls_sparse_cols = ss_sparse[ss_sparse].index.tolist()
constituents_data, err = ek.get_data(instruments=['0#ALLCOCO='], fields=['OFFCL_CODE']) $ constituents_data
to_be_predicted_Day3 = 21.39121221 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
from pymongo import MongoClient $ client = MongoClient() $ db = client.twitterDb $ posts = db.modiPosts
import datetime $ import matplotlib.pyplot as plt $ from matplotlib import style
trip_data = pd.read_csv("green_tripdata_2015-09.csv", header=0, delimiter=",") $ trip_data.head(5)
round_count = pd.DataFrame(df.groupby('funding_rounds').size()).reset_index() $ round_count.columns = ['funding_rounds','count']
df_goog['Year'] = df_goog.index.year $ df_goog['Month'] = df_goog.index.month $ df_goog['Day'] = df_goog.index.day
class_le = LabelEncoder()
liveonly_live_woc = liveonly_live_woc[['name','category_name','blurb','blurb_count','goal_USD','backers_count',\ $                                'launched_at','state_changed_at','days_to_change','state']]
lr=0.2
df_new['interaction_ca_ab_page'] = df_new.CA * df_new.ab_page $ df_new['interaction_uk_ab_page'] = df_new.UK * df_new.ab_page $ df_new.head() $
!ls ..
df.head()
import quandl $ quandl.ApiConfig.api_key = "U5cJSsnv4Ad7UUnHNGu8"
print ("Filtered records for query 4 :  ", len(final_location_ll))
tablename='gateways' $ pd.read_csv(read_inserted_table_entirein1line(dumpfile, tablename),delimiter=",",error_bad_lines=False).head(10)
conn.execute(sql)
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True) $ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
repos[repos.id == 6]
list(legos.keys())
rshelp.query("SELECT parking_spot_id, ev_charging FROM postgres_public.parking_spot ORDER BY RANDOM() LIMIT 100;")
req = requests.get("https://simple.wikipedia.org/wiki/List_of_U.S._state_capitals") $ page = req.text $ page_soup = BeautifulSoup(page, 'html.parser') $
df_features2.loc[df_features2["CustID"].isin([customer])]
reduced_new_list_split_amenities = list(set(new_list_split_amenities)) $ len(reduced_new_list_split_amenities)
pd.unique(tag_df.values.ravel())
ozzy.setBuddy(filou)
import seaborn as sns
tlen.plot(figsize=(16,4), color='r')
history_with_target = intervention_history.join(intervention_train['target'], how='left')
p_old = df2.query('converted == 1').user_id.count()/df2.user_id.count() $ print('Conversion of old Page is: ', p_old)
tweet_info = df.filter(['id','favorite_count','retweet_count'], axis=1) $ tweet_info.head()
lda_tfidf = LatentDirichletAllocation(n_topics=10, random_state=0) $ lda_tfidf.fit(X)
df4.head()
nnew = df2.query('group == "treatment"').user_id.nunique() $ nnew
print("Probability of treatment group converting:", df2[df2['group']=='treatment']['converted'].mean()) $ print("Probability an individual recieved new page:", $       df2['landing_page'].value_counts()[0]/len(df2))
fulldata_copy.dropna(inplace = True)
check_measurements.__dict__
df2 = pd.read_csv('df_providers1.csv' )
learner.sched.plot_loss()
pipeline = Pipeline([ $        OneHotVectorizer(columns=columns[catefeatures_index].values.tolist()), $        LightGbmBinaryClassifier(feature=columns[features_index].values.tolist(), num_boost_round=200) $ ]) $ pipeline.fit(ds_train, 'Label')
pivoted_table = data.pivot_table('Total', index = data.index.time, columns = data.index.date) $ pivoted_table.head()
ca_de.shape
park.groupby(by = 'named_drug').count().abstract
def count_comments(df): $
df2[df2['group'] == 'control'].converted.mean()
x = df2[df2.group == 'control'] $ x["converted"].mean() * 100
twitter_archive_enhanced.info()
for col in [col for col in full.columns if full[col].dtype == 'object' and 'Patient' not in col]: $     dummies = pd.get_dummies(full[col],prefix=col) $     full.drop(col,axis=1,inplace=True) $     full = pd.concat([full,dummies],axis=1)    
df.rename(columns={'Created Date': 'created_at'})
df.index.name=None $ df.reset_index(inplace=True)
autos['odometer'].head()
df_new.head()
active_with_return.info()
print(pd.isnull(titanic).sum()) $
y_pred = etr.predict(X_test)
import pandas as pd
pd.Period('2006Q1', 'Q-MAR')
start_date = "2016-03-24" $ end_date = "2016-04-09" $ Pre_start_date = stripdate(start_date) $ Pre_end_date = stripdate(end_date)
log_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results = log_mod.fit() $ results.summary()
from tempfile import mkstemp $ fs, temp_path = mkstemp("gensim_temp")  $ model.save(temp_path)  
test_words.shape
time_length = pd.Series(data = twitter_data['length'].values, index = twitter_data['Date']) $ time_fav = pd.Series(data = twitter_data['Likes'].values, index = twitter_data['Date']) $ time_rt = pd.Series(data = twitter_data['RTs'].values, index = twitter_data['Date'])
df_business = df_average.join(df_categories_svd) $ print df_business.shape $ df_business.head()
os.getcwd()
df.info()
X = test_df.drop(['Log_Price_Sqft','ZIP CODE'], axis=1)
df
!tar zxvf {weather_dir}/{zip_file} -C {weather_dir}
params = pd.Series(model.coef_, index=X.columns) $ params
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == 1')['user_id'].count() $ convert_new = df2.query('landing_page == "new_page" and converted == 1')['user_id'].count() $ n_old = df2.query('landing_page == "old_page"')['user_id'].count() $ n_new = df2.query('landing_page == "new_page"')['user_id'].count()
counts = Counter(l_hashtags) $ df = pd.DataFrame(counts.most_common(20), columns=['Hashtag', 'Count']) $ df.to_csv('hashtag_counts.csv')
approved['approved'] = True $ approved.head()
convert_prob = convert/tc $ convert_prob
ip_orig = pd.read_csv('image-predictions.tsv', sep = '\t') $ ip_orig.head()
data = [go.Histogram(x=twitter_final['length'])] $ iplot(data, filename='basic histogram')
d.info()
train_geo = pd.read_csv('./data/geo_train.csv')
ts = [t.assign(category = c, course = i) for t, c, i in zip(record_tables, category, course)] $ raw = pd.concat(ts).reset_index(drop=True) $ raw.sample(5)
import pandas as pd $ import numpy as np $ import helper $ import project_tests
X = [re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', string) for string in sample]
pca.fit(scaled_data)
y.is_leap_year # Is this time period a leap year?
dfDay = dfDay.drop(dfDay[(dfDay['Sales Stage'] == 'Work without contract') & (dfDay['Likelihood of Win'] == 'Certain (100%)')].index) $ dfDay = dfDay[dfDay['Sales Stage'] != 'Potential']
avg_stops_window_crimes__ =  int(np.average(stops_month_window__['sum_window_crimes'])) $ avg_monthly_crimes__ = int(np.average(crime_month_window__['sum_crimes'])) $ avg_bi_weekly_crimes__ = int(np.average(crime_two_week_window__['sum_crimes'])) $ avg_weekly_crimes__ = int(np.average(crime_week_window__['sum_crimes'])) $ avg_daily_crimes__ = int(np.average(crime_day_window__['sum_crimes']))
city_ride_data = clean_combined_city_df.groupby(["type"]).sum()["fare"] $ city_ride_data.head() $
no_answer = so['ans_name'].isnull() $ no_answer.head(6)
base_dict_by_place['dict'] = base_dict_by_place['hashtags'].apply(lambda x: count_hash_tags(pd.Series([x])))
df_customers['number of customers'].mean()
def getHighest(scoreList): $     if len(scoreList) == 0: $         return scoreList $     else: $         return scoreList[0][0]
df_new[['CA','UK','US']] = pd.get_dummies(df_new['country']) $ df_new.head()
da = DataFrame(res.A, columns=vectorizer.get_feature_names())
df = pj.convert_to_df(scaling=True, filtered=True) $ df.info()
data = pods.datasets.movie_body_count()['Y'] $ data.head()
pd.to_datetime([1349720105100, 1349720105200, 1349720105300, 1349720105400, 1349720105500 ], unit='ms')
tweets_master_df.groupby(tweets_master_df["timestamp"].apply(lambda x: x.year))['timestamp'].count()
top10_categories
df_subset2.plot(kind='scatter', x='Initial Cost', y='Total Est. Fee', rot=70) $ plt.show()
formatted_sample.show()
n_new = df2.query('landing_page == "new_page"').count()[0] $ n_new
df.user_id.nunique()
df
pop_result = pop_rec.recommend()
autos.info()
data = np.tile(np.arange(10)[:, np.newaxis], (1, 2)) $ df = pd.DataFrame(data) $ df
scaled['Gain +1d'] = scaled['Close -1d'].shift(-1) < 1.0 $ scaled.head()
payments_all_yrs.loc[100:120,:] $ payments_all_yrs =payments_all_yrs.sort $ payments_all_yrs = payments_all_yrs.sort_values.loc[:,:].sort_values(['disc_times_pay'], ascending=[False]) $ payments_all_yrs = payments_all_yrs.reset_index(drop=True) $ payments_all_yrs.head()
loans_act_arrears_latest_paid_xirr=cashflows_act_arrears_latest_paid_investor.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf)) $ loans_act_origpd_latest_paid_xirr=cashflows_act_origpd_latest_paid_investor.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
list(xp.columns.values)
d = feedparser.parse('http://rss.nytimes.com/services/xml/rss/nyt/InternationalHome.xml')
genes = pysqldf("select genomic_accession, symbol, name, start, end, strand, attributes from genes where feature != 'CDS'")
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
%load "solutions/sol_2_42.py"
plot_df['DAY_OF_WEEK'] = plot_df['DATE'].apply(lambda x: datetime.weekday(x))
!ls ../data/imsa-cbf/ | grep _b.cbf | wc -l
loans_xirr[loans_xirr.bucket_pd.notnull()].head()
df_ind_site.groupby(['id_num', 'year'])['drg3'].agg([np.sum, np.mean, np.max]) $
deltadf.to_csv('exports/trend_deltas_chefkoch.csv')
df_group_by2[categorical_features] = df_group_by2[categorical_features].apply(preprocessing.LabelEncoder().fit_transform)
import datasetdatabase as dsdb $ mngr = dsdb.ConnectionManager(user="jacksonb") $ mngr.add_connections(dsdb.LOCAL) $ mngr
s.asfreq('3B', method='bfill').head(15)
print sum(x) for query_df['previous_payouts'][i]['amount'] in query
df2.query('user_id == 773192')
print("# of files in unsubscribed :", filecount(os.fspath(master_folder + lists + "unsubscribed/"))) $ print("# of files in members :", filecount(os.fspath(master_folder + lists + "members/"))) $ print("# of files in cleaned :", filecount(os.fspath(master_folder + lists + "cleaned/")))
prop_converted = len(df.query('converted == 1')) / df.shape[0] $ print('The proportion of users converted is {}'.format(prop_converted))
old_position=get_current_position() $ new_position=lots.ix[-1]
sl_pf_v2.shape
pt = to_plot_df['vote_count'].plot(kind='bar', figsize=(20,10), rot=0, fontsize=20) $ plt.xlabel(pt.get_xlabel(), fontsize=22) $ plt.ylabel('Average of vote_count', fontsize=22) $ plt.title('Average of vote_count by original_language', fontsize=30)
n_old = len(df2.query("landing_page == 'old_page'"))           $ print('n_old :: ', n_old)
learn.fit(lrs, 1, wds=wd, cycle_len=20, use_clr=(32,10)) $
rGraphData.tail()
first_result.find('a').text[1:-1]
airlines = pd.read_csv('flights/airlines.csv') $ airlines.head(3)
dtm_df = pandas.DataFrame(countvec.fit_transform(df.body).toarray(), columns=countvec.get_feature_names(), index = df.index) $ dtm_df
brand_count = autos["brand"].value_counts(normalize=True) $ sel_brand = brand_count[brand_count >= 0.05].index
converted = df[df['converted']==1].count() $ proportion_converted = (((converted/number_of_rows)*100) ) $ print('The proportion of users converted is {}%'.format(round(proportion_converted['converted'],3)))
oneDayData[:15]
h1ba = h1bdata[h1bdata['CASE_STATUS'] == 'CERTIFIED'] $ sd = h1ba[(h1ba['WORKSITE_STATE'] == 'CA') & (h1ba['WORKSITE_CITY'] == 'SAN DIEGO')] $ len(sd)
df = build_dataframe('../data/raw/') $ df.info()
df.iloc[3:6, 2:3]
daily_constituent_count = QTU_pipeline.groupby(level=0).sum() $ QTU_pipeline.groupby(level=0).median().describe()
train.columns
Y_lin_reg = lin_reg.predict(X) $ from sklearn import metrics $ print('RMSE:', np.sqrt(metrics.mean_squared_error(Y, Y_lin_reg))) $ print('Variance explained: ', metrics.explained_variance_score(Y, Y_lin_reg))
counts_by_campaign_date.loc['Sport']
frtcor = pearsonr(df[df['RT']==False]['fcount'],df[df['RT']==False]['rtcount']) $ print('Correlation between favourite and retweet counts is',round(frtcor[0],2),get_sigstar(frtcor[1]))
df2.query('group=="control"').query('converted==1')['user_id'].count()/df2.query('group=="control"').shape[0]
for trigram_sentence in it.islice(trigram_sentences, 350, 370): $     print u' '.join(trigram_sentence) $     print u''
data.loc[data.density > 10, ['pop', 'density']]
user_total = df.nunique()['user_id'] $ print("Number of unique users is : {}".format(user_total))
grouped_newsorgs = news_sentiment_df.groupby(["News Organization"], as_index = False)
stemmer = nltk.stem.porter.PorterStemmer() $ %timeit articles['tokens'] = articles['tokens'].map(lambda s: [stemmer.stem(w) for w in s])
bb.plot(y=['high','low'])
ExponentStories
EmployeeRecords = [{'EmployeeID':451621, 'EmployeeName':'Preeti Jain', 'DOJ':'30-Aug-2008'}, $ {'EmployeeID':123621, 'EmployeeName':'Ashok Kumar', 'DOJ':'25-Sep-2016'}, $ {'EmployeeID':451589, 'EmployeeName':'Johnty Rhodes', 'DOJ':'04-Nov-2016'}]
p_new_std = (df2['converted']==1).std() $ p_new_std
t2['p1'] = t2['p1'].str.title() $ t2['p2'] = t2['p2'].str.title() $ t2['p3'] = t2['p3'].str.title()
sample_survey.loc[:, ['i_Education', 'wf']].groupby('i_Education').sum().plot.bar();
deadline = pd.to_datetime('2016-08-01')
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib notebook
(autos['last_seen'] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index() $ )
b_vs_bshift = b.shift(1) != b $ fund_nr_coinswitches = b_vs_bshift.T.sum() $ fund_nr_coinswitches = fund_nr_coinswitches[1:] # delete first row
from sklearn.feature_extraction.text import TfidfVectorizer $ from sklearn.neighbors import LSHForest
aux = df2[df2.group == 'control'] $ aux[aux.converted == 1].shape[0] / aux.shape[0]
print('The probability of receiving new page:', (df2['landing_page'] == 'new_page').mean())
df_t = standardize(df_t,x='Updated Shipped diff',m =(u'Updated Shipped diff', u'mean'),std=(u'Updated Shipped diff', u'std'))
df.rating_numerator.value_counts()
result1 = (df1 < df2) & (df2 <= df3) * (df3 != df4) $ result2 = pd.eval('df1 <df2 <= df3 != df4') $ np.allclose(result1, result2)
portfolio_df.set_index(['Ticker'], inplace=True) $ portfolio_df.head()
adopted_cats.loc[adopted_cats['Color']=='Black/Black','Color'] = 'Black' $ adopted_cats.loc[adopted_cats['Color']=='Tortie/Black','Color'] = 'Tortie Black' $ adopted_cats.loc[adopted_cats['Color']=='Calico/Black','Color'] = 'Calico Black' $ adopted_cats.loc[adopted_cats['Color']=='Tortie/Black Smoke','Color'] = 'Tortie Black Smoke'
df_twitter_archive_master.sort_values(by=['timestamp'],ascending=False).head(5)
data3=data2.sort_values(by=["date"]) $ data3.head(10)
dfUsers.head() $ for userId in dfUsers['userFromId']: $     temp = dfUsers['userFromName'][dfUsers['userFromId']==userId] $     G.node[userId]['userName'] = temp.values[0]
pd.DataFrame(tweet_df.groupby('username')['lang'].value_counts()).add_suffix('_Count').reset_index()
grid_svc.best_params_
df2['converted'].value_counts()[1]/df2['converted'].count() 
model.add(Flatten()) $ model.add(Dropout(rate=0.25))
y_train[:3]
connections.to_sql(con=engine, name='systemconnections', if_exists='replace', flavor='mysql')
missing_hours = taxi_hourly_df[(taxi_hourly_df.missing_dt == True)].index.hour
nums = [12, 8, 21, 3, 16]
query = \ $ data = pd.read_sql_query(query,con)
crf.fit(X_train, Y_train) $ train_score = crf.score(X_train, Y_train) $ print('crf training score is: %f' %(train_score)) $ test_score = crf.score(X_test, Y_test) $ print('crf test score is: %f' %(test_score))
figure, axes = plt.subplots() $ num_fli_by_month.plot(ax=axes, kind='bar', color='blue') $ axes.set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sept','Oct','Nov','Dec']) $ axes.set_xlabel("Month")
fig = acc.plot_history()
pd_data.head()
choice = 3112 $ radius = 450
baseball_newind.loc['gonzalu01ARI2006', ['h','X2b', 'X3b', 'hr']]
train_df, test_df = ordered_train_test_split(final_data, 'created_at') $ train_df.drop('created_at', axis=1, inplace=True) $ test_df.drop('created_at', axis=1, inplace=True)
df[['favorites', 'retweets']].plot(style = '.', alpha = 0.4) $ plt.title('Favorites and Retweets with Time') $ plt.xlabel('Date') $ plt.ylabel('Count');
a.zipcode = a.zipcode.astype("str")
print "Training set:\n{}".format(df_train.priority.value_counts())
model.resid.plot()
meat = load_meat $ births = load_births
data.drop_duplicates().shape
model.compile(loss='binary_crossentropy', $               optimizer='adam', $               metrics=['accuracy'])
p_diff_simulated = new_converted.mean() - old_converted.mean() $ print(p_diff_simulated)
import sklearn $ from sklearn import decomposition $ from sklearn.decomposition import PCA $ from sklearn import datasets
test = df["converted"].mean() $ test
df['four'] = df['one'] + df['two'] $ df
x = df.query('landing_page == "new_page" and group == "control"').count()[0] $ y = df.query('landing_page != "new_page" and group == "treatment"').count()[0] $ x + y
apple.set_index('Date', inplace=True) $ apple.head()
mars_weather = current_weather_info[0].text $ mars_weather
france_tops = lda.get_term_topics(d.token2id['france'], minimum_probability=0.001) $ france_tops, get_topic_desig(france_tops)
random_numbers['2015-01-01':'2015-04-30'].idxmax() # for first 4 months
techmeme.original_title = techmeme.original_title.apply(lambda x: x.replace('\n', '')) $ techmeme.extra_titles = techmeme.extra_titles.apply(lambda x: x.lstrip('[').rstrip(']')) $ techmeme['titles'] = techmeme.original_title + ' ' + techmeme.extra_titles
grpConfidence = df.groupby(['Confidence'])
station_count_stats = session.query(Measurement.station, func.min(Measurement.tobs),func.max(Measurement.tobs),\ $ func.avg(Measurement.tobs)).group_by(Measurement.station).filter(Measurement.station == 'USC00519281').all() $ station_count_stats
stock.columns
tweets_df.loc[tweets_df.language == 'und', :] $
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
df10 = pd.read_csv('/project/stack_overflow/Data/Query/QueryResults (10).csv') $ df9 = pd.read_csv('/project/stack_overflow/Data/Query/QueryResults (9).csv') $ df8 = pd.read_csv('/project/stack_overflow/Data/Query/QueryResults (8).csv') $ df = pd.concat([df10,df9,df8],axis=0)
print('Under the null hypothesis p_old is the same conversion rate regardless of the page: {}.'.format(conversion_rate_all_pages))
idx = df_providers[ (df_providers['year']==2011) & \ $                   (df_providers['drg3']==39)  ].index.tolist() $
sum(df['converted'].values)/row_num
group_count_lower_bound = groups_by_metro.count().nlargest(5, 'id')['id'].min() $ sample_groups = groups_by_metro.filter(lambda x: x['id'].count() >= group_count_lower_bound)
archive_clean.head(3)
plt.rcParams['axes.unicode_minus'] = False $ dta_6208.plot(figsize=(15,5)) $ plt.show()
for row in session.query(stations, stations.station).all(): $     print(row)
len(tweets_df.tweet_source.unique())
def remove_punctuation(text): $     exclude = set(string.punctuation) $     return "".join(ch for ch in text if ch not in exclude)
donors['Donor City'].value_counts().head(20).plot(kind='barh') $ plt.title('Top 20 Cities with most Donors') $ plt.xlabel('Number of Donors') $ plt.gca().invert_yaxis()
vals[5] = 0 $ vals $ data
from IPython.core.display import display, HTML $ display(HTML("<style>.container { width:100% !important; }</style>"))
grouped_by_bedrooms_df.to_csv('grouped_by_num_of_bedrooms_df.csv',index=False)
fig, ax = plt.subplots(1, 2, figsize=(14, 6)) $ pivoted.T[labels == 0].T.plot(legend=False, alpha=0.1, ax=ax[0]); $ pivoted.T[labels == 1].T.plot(legend=False, alpha=0.1, ax=ax[1]); $ ax[0].set_title('Purple Cluster') $ ax[1].set_title('Red Cluster')
import seaborn as sns $ sns.set() $ sns.set_style('dark')
wheels.merge(onehot, left_index=True, right_index=True).drop('drive_wheels', axis=1)
df2['intercept'] = 1 $ df2[['control','treatment']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('control', axis = 1) $ df2.rename(columns = {'treatment':'ab_page'}, inplace = True)
import pandas as pd $ git_log = pd.read_csv('datasets/git_log.gz', sep='#', encoding='latin-1', $                       header=None, names=['timestamp','author']) $ print(git_log.head())
df[df['Complaint Type'].str.contains('firework', case=False)]['Complaint Type'].unique()
train['has_opened'].describe()
print('date, previous_wednesday') $ for test_date_offset in range(16): $     dt = datetime.date(2017,7,18) + datetime.timedelta(days=test_date_offset) $     offset = (dt.isoweekday() + 4) % 7 $     print(dt, dt - datetime.timedelta(days=offset))
top_10_zipcodes_by_population_columns = ['incident_zip', 'population'] $ top_10_zipcodes_by_population = question_2_dataframe[top_10_zipcodes_by_population_columns].\ $     drop_duplicates().\ $     nlargest(10, 'population')['incident_zip'].values $ top_10_zipcodes_by_population
wrd_api.sample(3)
odds.head(3)
m, b = np.polyfit(ages, weights,1) $ print(m,b)
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == 1').shape[0] $ convert_new = df2.query('landing_page == "new_page" and converted == 1').shape[0] $ n_old = n_old $ n_new = n_new
classifier_df.head()
empInfo["senderlevel"] = empInfo.level $ empInfo["receiverlevel"] = empInfo.level
stat, p, med, tbl = scipy.stats.median_test(df2["tripduration"], df4["tripduration"]) $ print p $ print tbl
not_numbers = data_read.genre_ids.astype(str).apply(lambda x: x.isnumeric()) == False $ data_read["genre_ids"][not_numbers.values].sample(10)
run txt2pdf.py -o"2018-06-18  2015 460 disc_times_pay.pdf"  "2018-06-18  2015 460 disc_times_pay.txt"
import seaborn as sns $ goog[['Open','High','Low','Close','Adj Close']].plot()
tweets_df_clean[tweets_df_clean['id'].duplicated()]
tag_pairs = hashtags[["id","screen_name","hashtag"]].groupby(["screen_name","hashtag"]).agg({"id":"count"}).rename(columns={"id":"Weight"}) $ tag_pairs.reset_index(inplace=True) $ tag_pairs.head()
one_day=86400 #in seconds $ next_unix=last_unix+one_day  #next day
(grades == 10).any(axis = 1)
X_df = classify_df.drop(['Y'],axis=1)
df.plot(x='s1', y='s2', kind='scatter')
birth_dates.head()
df2.groupby(df2['landing_page']=='new_page').size().reset_index()[0].iloc[1]/df2['landing_page'].count()
import pandas as pd $ from datetime import timedelta $ %matplotlib inline $ df = pd.read_csv('datasets/vow.csv')
df
y_test[:10]
local_mar.groupby('store').size()
input_data.describe(include='all')
z_score, p_value = sm.stats.proportions_ztest([convert_new[0], convert_old[0]], [n_old, n_new], alternative='larger') $ z_score , p_value
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_page', 'US', 'UK']]) $ results = logit_mod.fit() $ results.summary()
ts / ts.shift(1)
df2.drop_duplicates(inplace=True)
twitter_df_clean.name.value_counts()[0:5]
utils.plot_user_steps(pax_raw, None, 2, 15)
resp = json.loads(r.content)['response'] $ pd.DataFrame(resp)
expensive = (autos["price"] > price_UF) == True
for i, image in enumerate([img1, img2]): $     plt.subplot(2, 2, i + 1) $     plt.axis('off') $     plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
train.readingScore[train.male==1].mean()
print(1) $ print(2)
old_page = df2.query('landing_page == "old_page"') $ old_page.landing_page.count()
oppose.amount.sum()
financial_crisis.loc['Wall Street Crash / Great Depression'] = '1929 - 1932' $ print(financial_crisis)
rdg2 = Ridge(alpha=5) $ rdg2.fit(train_data, train_labels)
from datetime import datetime $ date = nc.num2date(time, 'hours since 1800-01-01 00:00:0.0') $ ts = pd.Series(date, index = date) $ print(ts.head(), ts.tail())
score['is_false_positive'] = np.where((score['pred_lr']==1) & (score['is_shift']==0), 1, 0) $ score['is_false_negative'] = np.where((score['pred_lr']==0) & (score['is_shift']==1), 1, 0) $ score['is_true_positive'] = np.where((score['pred_lr']==1) & (score['is_shift']==1), 1, 0) $ score['is_true_negative'] = np.where((score['pred_lr']==0) & (score['is_shift']==0), 1, 0)
len(data.groupby(['longitude', 'latitude']))
weekday = austin['weekday'].value_counts() $ weekday= weekday.reindex(['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']) $ weekday
tweets = pd.read_json('tweet_json.txt')
fig,ax = plt.subplots() $ geo_df.plot(ax = ax)
unique_users = df.user_id.nunique() $ unique_users
plt.hist(review_df.fake_review) $ plt.show()
video_id = df['video_id'].tolist() $ video_id[:2]
new_reps.drop(new_reps.index[0], inplace = True) $ new_reps.reset_index() $ new_reps.head()
shows.head(10)
twitter_archive.sample(10)
plt.hist(p_diffs)
zero_rev_acc = accounts[accounts[' Total BRR '] == 0] $ zero_rev_acc_opps = pd.merge(zero_rev_acc, opportunities_not_lost,\ $                                     on=['Account ID'],\ $                                     how='inner').sort_values(by='Building ID', ascending=False)
dfBTCPrice.corr()
numdays, cols = 252, 10; freq = 5; lookback = 20; hist_window = 252*5 $ end_date_str = tgt_date_str = '1-3-2018' $ start_date = datetime.strptime('1-3-2018', date_fmt) $ start_date = start_date - timedelta(hist_window) $ dwld_key = 'XLK'
twitter.name.value_counts() $
for field_name, dtype in df.select_dtypes(include=categories).items(): $     print(field_name) $     df[field_name] = pd.Series(pd.Categorical(df[field_name]).codes)
age1_mean = records4[(records4['Graduated'] == 'Yes') & (records4['Gender'] == 'Male')]['Age'].mean() $ age2_mean = records4[(records4['Graduated'] == 'Yes') & (records4['Gender'] == 'Female')]['Age'].mean() $ age3_mean = records4[(records4['Graduated'] == 'No') & (records4['Gender'] == 'Male')]['Age'].mean() $ age4_mean = records4[(records4['Graduated'] == 'No') & (records4['Gender'] == 'Female')]['Age'].mean() $ age1_mean, age2_mean, age3_mean, age4_mean
esp['google_translated'] = esp.explain.apply(google_trans,)
modCrimeData = crimeData $ modCrimeData.set_index("Year", inplace=True) $ modCrimeData.head()
p_new-p_old
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation
data_for_model.corr()
import pandas as pd $ import datetime as dt $ import matplotlib.pyplot as plt $ import seaborn as sns $ import numpy as np
clf_vot_tfidf = VotingClassifier(estimators=[('rf', clf_RF_tfidf), $                                              ('lr', clf_LR_tfidf), $                                              ('rgf', clf_RGF_tfidf)], voting='soft').fit(X_traincv_tfidf, y_traincv_tfidf)
import pandas as pd $ from datetime import timedelta $ %pylab inline $ df_vow = pd.read_csv('assets/datasets/vow.csv')
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']]
bmp_series = pd.Series(brand_mean_prices) $ mile_series = pd.Series(mean_mileage) $ brand_df = pd.DataFrame(bmp_series, columns = ['mean_prices']) $ brand_df['mean_mileage'] = mile_series $ brand_df
xtrain,xtest,ytrain,ytest=cross_validation.train_test_split(X,Y,test_size=0.2)
df_DRGs = df_DRGs.sort_values(['year','DRG_name'], ascending=[True,True]) $
_ = ok.grade('q08c') $ _ = ok.backup()
target_users = ("@BBC", "@CBS", "@CNN", "@FoxNews", "@nytimes") $ sentiments = [] $ for target_term in target_users: $     sentiments = analyse_tweeter(target_term) $ sentiments_pd = pd.DataFrame.from_dict(sentiments) $
record = session.query(Measurement.station, func.count(Measurement.date)).group_by(Measurement.station).\ $             order_by(func.count(Measurement.date).desc()).all() $ record
pd.Series(baseball_newind.index).value_counts()
modeling1.show(5)
df['Timestamp'] = pd.to_datetime(df['Timestamp'])
house_data.info()
DataSet_sorted = DataSet.sort_values('prediction', ascending=False)
trunc_df.company_name = trunc_df.company_name.str.lower() $ trunc_df[trunc_df['homepage_url'].str.contains("airbnb.com",na=False) & trunc_df['company_name'].str.contains("airbnb",na=False)].index[0]
transactions_items.info()
df_states = pd.get_dummies(df_h1b_ft_US_Y.lca_case_workloc1_state)
(2 * x) == (x ** 2)
df['closed_at'] = pd.to_datetime(df['Closed Date'], format= '%m/%d/%Y %I:%M:%S %p')
beirut.dtypes
questions = pd.concat([questions.drop('attend_with', axis=1), attend_with], axis=1)
trump.index=pd.DatetimeIndex(trump['created_at']) $ trump.drop('created_at', axis = 1, inplace =True) $ trump.shape
df_res
v = portfolio_metrics(pdf) $ p_template.format(v[0], v[1], v[2])
print('The largest change between any two days is ' + str(max(max((stock - mini).abs()),max((maxi - stock).abs()))))
p_new=df2['converted'].mean() $ print(p_new)
df_new['country'].value_counts()
geo=Geolocator.Geolocator() $ geo.init(worldPickleFileName='../geo-world-spark-nepal.pkl',dataFileName='../geo-data/ungp-geo.txt.gz')
filenames
full_df = pd.read_csv('sf_airbnb2017.csv', low_memory=False) $ full_df.head(10)
len(df)*0.5/(60*60*24) #Convert to days
df_everything_about_DRGs[(df_everything_about_DRGs['drg3_str'] == '001') ].index.tolist()
testheadlines = test["text"] $ advancedtest = advancedvectorizer.transform(testheadlines) $ advpredictions = advancedmodel.predict(advancedtest)
csvData[csvData['street'].str.match('.*Court.*')]['street']
collect = JsonCollection(f, compression='bz2', throw_error=0, verbose=1)
df_copy['stage'].value_counts()
add_quarter(epa, year='YEAR', month='MONTH') $ epa.head()
step_threshold = 300 $ pax_raw[pax_raw.paxstep<step_threshold].paxstep.hist(bins=30)
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.choice([0,1], size=n_new, p=[1-p_new, p_new]) $     old_page_converted = np.random.choice([0,1], size=n_old, p=[1-p_old, p_old]) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean())
np.save('../models/crosstab_40937.pkl', crosstab)
vectors = sess.run(embeddings) $ vectors = pd.DataFrame(vectors, index=vocab.index)
diff = prob_convert_t - prob_convert_c $ plt.hist(p_diffs); $ plt.axvline(x= diff, color = 'red'); $ plt.title("Histogram of P_diffs");
emails_dataframe['address'].str.split("@")
fuel_mgxs.print_xs()
model = Sequential() $ model.add()
a=list(df) $ feature_names = a[41:82] $ X=pd.DataFrame(X_all) $ X.columns=feature_names
rng + pd.tseries.offsets.DateOffset(months=2)
df = pd.read_csv('C:\\Users\\infom\\Documents\\GitHub\\AB_testing\\ab_data.csv') #loading data $ df.head()
cat_outcomes = cat_outcomes.loc[(cat_outcomes['outcome_type'] == 'Adoption') | $                                  (cat_outcomes['outcome_type'] == 'Transfer')]
from patsy import dmatrices $ from statsmodels.stats.outliers_influence import variance_inflation_factor $ y, X = dmatrices('converted ~ intercept + ab_page + CA + UK', df2, return_type='dataframe')
df_master.p1.value_counts() $ df_master.p2.value_counts() $ df_master.p3.value_counts()
trading_exemption_records.apply(lambda x: x['exemptions.psc_exempt_as_shares_admitted_on_market.exemption_type'] if not pd.isnull(x['exemptions.psc_exempt_as_shares_admitted_on_market.exemption_type']) else x['exemptions.psc_exempt_as_trading_on_regulated_market.exemption_type'],axis=1 ).value_counts()
islab("john_labuilding")
tf_vec = CountVectorizer(max_df=0.95, min_df=2, $                          max_features=max_features, stop_words="english") $ tf = tf_vec.fit_transform(comment_column) $ tf_feats = tf_vec.get_feature_names()
tweets.shape
tuned_parameters = [{'alpha': [0.001,0.01,0.1,1,10] , 'eta0': [10 ** -4,10 ** -5,10 ** -6,10 ** -7,10 ** -8,10 ** -9,10 ** -10]}] $ gridSearchModel = GridSearchCV(SGDRegressor(random_state=42,loss='squared_loss',penalty='l2'),tuned_parameters,scoring = my_scorer, cv=5) $ gridSearchModel.fit(df_train, tsne_train_output) $ results = gridSearchModel.cv_results_
counterlist
tfv = TfidfVectorizer(ngram_range=(1, 3), max_features=2500) $ X = tfv.fit_transform(clean_text)
inv_hist=pd.DataFrame({'bin':inv_bin[:-1],'count':inv_count})
tag_pairs[["FromType","FromName","Edge","ToType","ToName","Weight"]].to_csv("For_Graph_Commons3.csv",encoding="utf-8",index=False)
column_list1 = ['DewPoint'] $ df[column_list1].plot() $ plt.show()
import re $ regex_1 = re.compile('\w[A-Z]+')
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM 
authors_to_github_username.persist() $ authors_to_github_username_saved = non_blocking_df_save_or_load( $     authors_to_github_username, $     "{0}/authors_to_github-10".format(fs_prefix))
data.iloc[[1,10,3], [2, 3]]
len([doc['id'] for doc in resp.json()])
p_new = df2.converted.sum()/df2.converted.count() $ new_page_converted = np.random.choice([0,1],new_page.landing_page.count(), p=(p_new,1-p_new))
conn_helloDB.execute('drop database HelloDB3;')
'this is {0} number {1}'.format( "string", "1")
closed_issue_age = Issues(github_index).is_closed()\ $                                        .fetch_results_from_source('time_to_close_days', 'id_in_repo', dataframe=True) $ print(closed_issue_age.head())
new_page = new_page[:145274] $
df_release = pd.read_csv( pwd+'/releases.052317.csv', encoding='utf-8') #earning release?
df_new[['UK', 'US', 'CA']] = pd.get_dummies(df_new['country'])[['UK', 'US', 'CA']] $ df_new.head()
df.shape #Returns the number of rows and columns
df_en['polarity_blob'] = df_en['text'].apply(lambda tweet: TextBlob(tweet).sentiment.polarity)
df_user = pd.read_csv("Data/yammer_users.csv") $ df_event = pd.read_csv("Data/yammer_events.csv") $ df_email = pd.read_csv("Data/yammer_emails.csv") $ df_period = pd.read_csv("Data/dimension_rollup_periods.csv") $
b_cal.shape
All_tweet_data_v2=All_tweet_data_v2.drop('Remove', 1)
df.sample(5)
X_test.info()
autos['last_seen'].str[:10].describe()
df_mes = df_mes[df_mes['mta_tax']>=0] $ df_mes.shape[0]
dummies = pd.get_dummies(df_merge['country']) $ df_merge[['CA', 'UK', 'US']] = dummies $ df_merge.drop(['CA'], axis = 1, inplace = True) $ df_merge.head()
train_comment2
train = train_cleaner(train)
for c, v in zip(logreg_words.fit(X_words, y).coef_[0].round(3)[-4:], ['Curse','Fun','Happy','Vice']): $     print v, c
my_prediction = my_tree_one.predict(test_features)
df['Views-PercentChange'].apply(np.log).hist()
bnbAx.country_destination.value_counts() $
appleinbounds.shape
df_users_3.shape
segmentData.lead_source.value_counts()
speeches_metadata_evidence.columns
S_1dRichards.forcing_list.filename
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline $
movies['year'] = pd.to_numeric(movies['year'], errors='coerce')
pd.merge(cust_data, cust_demo, how='left', left_on='ID', right_on='ID').head(3)
df=df[df["ORIGIN"].apply(lambda x: x in ["BNA","DAL","HOU","STL"])] $ df=df[df["DEST"].apply(lambda x: x in ["BNA","DAL","HOU","STL"])]
tlen.plot(figsize=(16,4), color='r')
ab_diffs = df2[df2['group'] == 'treatment']['converted'].mean() -  df2[df2['group'] == 'control']['converted'].mean() $ print(ab_diffs)
tweets_clean[tweets_clean.doggo == 'doggo']
df.groupby(['character_id', 'raw_character_text']).size().reset_index().head(10)
df_date = df.copy()
tweets_list = [] $ for i,row in df_clean.iterrows(): $     tweet = api.get_status(row['tweet_id'],tweet_mode='extended') $     tweets_list.append(tweet._json)
x2 = poly2.fit_transform(x)
df_columns.groupby(["Hour of day","AM|PM"]).size().reset_index().sort_values([0], ascending=[False]) $
data['density'] = data['pop'] / data['area'] $ data
X =Quandl_DF.where(Quandl_DF['Date'] >= '20100101').groupby(['Year','Month'])['GBP_to_HKD'].mean().plot()
res[0]
do_ifcformant?
titanic.age.fillna(titanic.age.mean(), inplace=True)
app.test_client().get( $     "/", $     headers={"Referer": "http://localhost:8000/"}, $ )
cm = confusion_matrix(y_test,gb_ypred) $ plot_confusion_matrix(cm)
infered_relevant_info.show()
d = {'one' : pd.Series([1., 2., 3.], index=['a', 'b', 'c']), 'two' : pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])} $ df = pd.DataFrame(d) # create a DataFrame from dictionary d $ df $
poly20 = PolynomialFeatures(degree=20) $ X_20 = poly20.fit_transform(X)
p_convert_treatment = df2.query('group == "treatment"').converted.mean() $ p_convert_treatment
browser.visit(url) $ browser.click_link_by_partial_text('Cerberus Hemisphere Enhanced') $ html = browser.html $ soup = BeautifulSoup(html, 'lxml') $
print(dfUD_unique.shape) $ print(df_all.shape) $ dfOld = pd.merge(df_all,dfUD_unique,how='right',left_index=True,right_index=True,sort=False) $ print(dfOld.shape) $ dfOld.head()
p_old = df2[df2['converted'] == 1]['user_id'].count() / df2['user_id'].count() $ p_old
consumerKey= 'XXXXXXXXXXXXXXXXXXXXXXXXXXXX' $ consumerSecret='XXXXXXXXXXXXXXXXXXXXXXXXXXXX' $ auth = tweepy.OAuthHandler(consumer_key=consumerKey, $                            consumer_secret=consumerSecret) $ api=tweepy.API(auth)
plt.scatter(x=df['tweet_id'], y=df['favorite_count']);
print(calc_temps('2017-07-22', '2017-07-29'))
file = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', sep=',').load('treas_parking_payments_2017_datasd_parsed.csv') $ file.show(3)
df2.drop([1899],inplace=True)
prices.head()
date_df = pd.DataFrame({'date': date, 'weekday': weekdays, 'count': count, 'duration(min)': duration}) $ date_df.head()
prob_control = df2[df2.group == 'control']['converted'].mean() $ prob_control
tweets_streamedDF.full_text[0]
1/np.exp(-0.0150)
pre_analyzeable.columns
train['month_account_created'] = train['date_account_created'].apply(lambda x: x.strftime('%m'))
base_dict_by_place = pd.DataFrame(c.apply(lambda x: [i for l in x for i in l], axis=1), columns=['hashtags'])
test_post = re.sub('[^a-zA-Z]',' ',test_post) $ print(test_post) # remove special character
pd.crosstab(df.launched_year, df.State)
new_cases_liberia_concat.columns = ["Date", "New Case/s (Suspected)","country","Date1","New Case/s (Probable)","country","Date2","New case/s (confirmed)","country","Total_new_cases_liberia"] $ Total_new_cases_liberia =new_cases_liberia_concat[['Date','Total_new_cases_liberia']] $ Total_new_cases_liberia.head() $
X_train_dtm = pd.DataFrame(cvec.fit_transform(X_train.title).todense(), columns=cvec.get_feature_names())
nitrodata['Year'] = pd.DatetimeIndex(nitrodata['ActivityStartDate']).year $ nitrodata['Month'] = pd.DatetimeIndex(nitrodata['ActivityStartDate']).month
vi['ENDDATE'] = pd.to_datetime(vi['ENDDATE'], format='%Y-%m-%d %H:%M:%S')
new_page_converted = np.random.choice([0, 1], size = n_new, p = [p_new, 1 - p_new])
def vader_all_score(text): $     return vaderAnalyzer.polarity_scores(text)['compound']
all_sets.cards["XLN"].head()
eth_getTransactionCount('0x267be1C1D684F78cb4F6a176C4911b741E4Ffdc0')
result.shape
training_set.info()
for index, row in im_clean.iterrows(): $     assert("-" not in row["p1"]) $     assert("-" not in row["p2"]) $     assert("-" not in row["p3"])
autos[autos["brand"].isin(common_brands)].groupby(autos["brand"])["price"].mean().round(2).plot(kind='bar') 
session.query(measurement.station,func.count(measurement.station)).group_by(measurement.station).order_by(func.count(measurement.station).desc()).all()
tweet_text = mars_weather['text'] $ date = mars_weather['created_at'] $ tweet_date = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(date,'%a %b %d %H:%M:%S +0000 %Y')) $ print(f"On {tweet_date} the weather on mars was {tweet_text}")
html = requests.get(url)
query = session.query(Measurement) $ rain = query.order_by(Measurement.date.desc()).all() $ print(rain[0].date)
df['car_age'] =  (last_seen.year - df.yearOfRegistration).apply(lambda x: int(x))
print hpdvio[hpdvio['BuildingID']==1]['NOVDescription'].tolist()
ad_source.to_csv('../data/ad_source.csv')
df
num_stations
test_df.columns
y = train.rating
pca = PCA(n_components=6858) $ pca.fit(crosstab)
df.head()
visualize_tree(columns)
tot_cstmers = c_df.groupby('CUSTOMER_ID_CAT').size().shape[0] $ tot_cstmers
df_daily2 = df_daily.groupby(["C/A", "UNIT", "SCP", "STATION", "DATE"]).DAILY_ENTRIES.sum().reset_index() $ df_daily2.head(5)
df_archive["rating_numerator"].value_counts()
facts_url = 'http://space-facts.com/mars/'
register.settings().all()
pca.explained_variance_ratio_
window_length = 50
mi_diccionario = {"Nombre" : "Luis" , "Apellido":"Cueva " , "Edad":25 }
d - pd.tseries.offsets.Week(normalize=True)
words_df[words_df['count'] > 100] $
date -= timedelta(1) # adjust date by one day $ while(date.month == 10): $     dfEPEX = dfEPEX.drop([date.strftime('%a, %d/%m')], axis=1) $     date -= timedelta(1)
1 / np.exp(-0.0150)
nitrogen = results[mediaMask & hydroMask & charMask & sampFracMask] $ nitrogen.shape
def simple_function(v): $     return int(v * 10) $ print simple_function(3)
donors[donors['Donor Zip'] == 606 ]['Donor State'].value_counts()
Data.groupby('to_account')['deposited', 'withdrawn'].sum().head()
titanic_df = pd.read_excel(DATA_FOLDER+'/titanic.xls') $ titanic_df.head()
df = df[df['duration'].astype('timedelta64[h]') < 100]
bm_model = DecisionTreeClassifier(**bg, random_state=101) $ bm_model.fit(X_train, y_train)
psy_prepro = psy $ psy_prepro.to_csv("psy_prepro.csv")
archive_df.drop(columns=['in_reply_to_status_id', 'in_reply_to_user_id', $                          'source', 'retweeted_status_user_id', $                          'retweeted_status_timestamp'], inplace=True, errors='ignore')
LOC.shape
pivoted.T[labels==1].T.plot(legend=False, alpha = 0.1);
df2 = df.drop(df.columns[non_null_counts < 1000], axis=1) #by default df.drop will take out rows, axis = 0
twitter_Archive.drop(['retweeted_status_user_id', $                'retweeted_status_id', $                'retweeted_status_timestamp'], axis=1,inplace=True)
logs.name.value_counts()
control_cr
df.head()
page_dummies = pd.get_dummies(df_new.landing_page) $ df_new = df_new.join(page_dummies) $ df_new['CA_new_page'] = df_new.CA * df_new.new_page $ df_new['US_new_page'] = df_new.US * df_new.new_page $ df_new.head()
rmse=math.sqrt((lmscore.resid**2).mean()) $ print rmse
def eggs(x): $     return x
df2.query('landing_page == "new_page"').converted.count()
%writefile /tmp/test.json $ {"age":"29.0","activity":3.0}
agg_stats = get_cleaned_aggregate_player_stats(player_stats)
df2.drop(repeated_index, inplace=True) #drop the mismatch
print(predictor.predict(test_X.values).decode('utf-8'))
df = pd.read_excel('data/analysis2/Dow.xlsx') $ df.head(3) $ df.shape
Celsius.__dict__['temperature'].__delete__(Celsius)
temp = pd.read_csv('phillydata/Parking_Violations.csv') $ temp.head().T
session = Session(engine)
next_day_pf.add_beta() $ next_day_pf.add_factor_loadings() $ next_day_pf.add_sector() $ next_day_pf.add_price() $ next_day_pf.add_daily_return()
model.most_similar('feeding')
engine
s = pd.Series(['a_b_c', 'c_d_e', np.nan, 'f_g_h'])
Xtrain.describe(percentiles=[.5]).round(3).transpose()
df_new.head()
set(top5_best_fan.keys())
records[0:3]
df2_new[['CA','UK']]=pd.get_dummies(df2_new['country'])[['CA','UK']]
date = np.array('2017-10-21', dtype=np.datetime64) $ date = date + np.arange(189)
weather = weather_soup.find('div', class_='js-tweet-text-container') $ mars_weather= weather.p.text.lstrip() $ print(mars_weather)
IP = '137.110.137.158'
import json $ with open('config.json') as config: $     config_file = json.load(config) $ spks = np.loadtxt('output/spikes.csv') $ print(spks[:10, :])
x = np.float32(x) $ x = x.reshape((len(x),1)) $ Y = neigh.predict(x)
autos["brand"].value_counts(normalize=True) $
raw_df.sample(5)
department_df
poly_features = PolynomialFeatures(2, include_bias=False) $ poly_features.fit(X_train['Pending Ratio'].values.reshape(-1, 1)) $ training_pending_ratio = poly_features.transform( $     X_train['Pending Ratio'].values.reshape(-1, 1)) $ print(training_pending_ratio[0:5, :]) $
import statsmodels.api as sm $ log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])
baseball = pd.read_csv("Data/baseball.csv", index_col='id') $ baseball.head()
s2 = pd.Series(data = [3, 0, 1, 4], index = ['dogs', 'cats', 'fish', 'birds']) $ s2
first_line = blame.loc[2].copy() $ first_line.line = 1 $ first_line
from datetime import datetime $ df3['DAY'] = df3['DATE'].apply(lambda x: x.weekday()) $ df3['DATE'] = df3['DATE'].apply(pd.datetools.normalize_date)
print (ser.index) $ print (ser.size) $ print (ser.dtype) $ print (ser.values)
top_supporters["contributor_fullname"] = top_supporters.contributor_firstname + " " + top_supporters.contributor_lastname
%%timeit $ total=0 $ for i in range(1000): $     for j in range(1000): $         total+=i*(-1)**j
legLastOnly = leg[leg["first_name"].isnull()]
duration(end-start)
LabelsReviewedByDate = wrangled_issues_df.groupby(['created_at','DetectionPhase']).created_at.count() $ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue','yellow', 'purple', 'red', 'green'], grid=False)
dfb_train.describe()
donors_c[donors_c['Donor Zip'] == 606 ]['Donor State'].value_counts()
park[park.named_drug.notnull()][:5]
df = pd.read_csv("btime.csv", index_col = 0) $ df
df_test4_promotions['AwardType'].fillna('Discount', inplace=True);
from shapely.geometry import Polygon, Point $ closed_sakhalin_contour = np.vstack([contour_sakhalin, contour_sakhalin[-1]]) # Polygon should be closed to check inclusions $ sakhalin_poly = Polygon(closed_sakhalin_contour)
baby_scn_postgen.to_csv('Paired_Activated_postgen_61218.csv')
samples_query.execute_call('Person', 'ByName', 'A') $ samples_query.display_records(5)
plt.rcParams['axes.unicode_minus'] = False $ dta_713.plot(figsize=(15,5)) $ plt.show()
sum(tw.duplicated(subset=['expanded_urls']))
gender[196]=1;gender[820]=1;gender[897]=1;gender[1300]=1
%time df_csv['Total Profit'].sum().compute()
indices
url = "http://api.turfgame.com/v4/zones/all" # get request returns .json $ r = requests.get(url) $ df = pd.read_json(r.content) # create a df containing all zone info $ current_date = datetime.datetime.now() # system time at time of import $ print(current_date)
readFromFieldsDB() $ c.close() $ connect.close()
df = pd.DataFrame() $ df['timestamp'] = timestamp $ df['sn'] = sn $ df['text'] = text
np.exp(-0.0335 + 0.0877)
bacteria_data.treatment = 1 $ bacteria_data
df3[['CA', 'US']] = pd.get_dummies(df3['country'])[['CA','US']] $ df3['country'].astype(str).value_counts()
df3 = df3.drop('CA', axis=1) $ df3.head()
import pandas as pd $ import numpy as np
header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken} $ response_get = requests.get(endpoint_published_models, headers=header) $ print response_get $ print response_get.text
df1 =pd.read_csv('https://raw.githubusercontent.com/kjam/data-wrangling-pycon/master/data/berlin_weather_oldest.csv') $ df1.head(2)
df_predictions.describe()
from sklearn.linear_model import LogisticRegression
h2020_part = 0 $ h2020_proj = 0
closingPrices.nlargest(5)
popt_opb_chord_saddle, pcov_opb_chord_saddle = fit(d_opb_chord_saddle)
df2 = pd.read_csv("msft.csv", $                   usecols=['Date', 'Close'], $                   index_col=['Date']) $ df2.head()
topten.loc[:, (topten != 0).any(axis=0)].columns
df_final.dropna(inplace=True) $ df_final.reset_index(drop=True, inplace=True) $ df_final.head()
my_tweet_df["tweet_date"] = pd.to_datetime(my_tweet_df["tweet_date"]) $ my_tweet_df.sort_values("tweet_date", inplace=True) $ my_tweet_df.reset_index(drop=True, inplace=True) $ my_tweet_df.head()
pd.read_html(html_string, header=0)[0]
old_folder = folder + "\\last batch" $ old_TC_file = [f for f in os.listdir(old_folder) if re.match('.*TC.*', f)][0] $ old_TC_file = old_folder + "\\" + old_TC_file $ old_TC_file
df_germany $
resilience[:10]
af = pd.DataFrame(allfiles())
import numpy as np $ np.sqrt(fruits)
path = os.path.join("../Datasets/Beat_Data/") $ path
hp.find_device('FL03001562')
print(area.index) $ print(population.index) $ print(states.index)
print "About: Incidents table date field 1 {}".format(INC.ix[:, 'date_incid'].head()) $ print "About: Incidents table date field Original {}".format(INC.ix[:, 'incident_d'].head()) $ print "About: Weather table date field {}".format(weather.ix[:, 'EST'].head()) $
for f in feature_layer.properties.fields: $     print(f['name'])
dfFull.BsmtFullBath = dfFull.BsmtFullBath.fillna(dfFull.BsmtFullBath.mean().round())
obs_diff = user_treat_conv - user_control_conv $ obs_diff
df['converted'].mean()
new_page_converted.mean() - old_page_converted.mean()
!ls ../data/imsa-cbf/ | tail
nameHasMaxScoreList = scoreListOfList.apply(getHighest)
random_locs = random.choices(list(non_empty_crash_data_df['Location']), k=sample_size)
df.head(2)
df.word_count.sum()
run txt2pdf.py -o '2018-06-22 2012 FLORIDA HOSPITAL Sorted by discharges.pdf'  '2018-06-22 2012 FLORIDA HOSPITAL Sorted by discharges.txt' $
clean_weather = pd.read_csv('weather_clean.csv') $ clean_weather.head()
df_station.drop('zipcheck', axis=1, inplace=True )
gb_model.feature_importances_
df_filtered = df_api[['id','retweet_count','favorite_count','created_at','retweeted_status']] $ df_filtered.columns $
for i in xrange(len(station_splits)): $     station = station_splits.ix[i,'station'] $     for pct in station_splits.columns[1:]: $         gatecount_station_line.ix[gatecount_station_line.name == station,pct.replace('pct','entries')] = gatecount_station_line.ix[gatecount_station_line.name == station,'entries']*station_splits.ix[i,pct]
def get_lims_dataframe(query): $
print('Getting a single greeting by row key.') $ key = 'greeting0'.encode('utf-8') $ row = table.row(key) $ print('\t{}: {}'.format(key, row[column_name.encode('utf-8')])) $
details = pd.read_csv('../../data/raw/AllMoviesDetailsCleaned.csv', sep=';', engine = 'python')
data_dir = "./data/" $ raw_data_dir = data_dir + 'raw_data' $ merged_data_dir = data_dir + 'merged_data' $ pretrain_data_dir = data_dir + 'pretrain_data' $ train_data_dir = data_dir + 'train_data'
s.str. startswith ('T')
trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt) $ val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt) $ md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)
pd.Series(np.r_[1,2,9])
not_in_dsi = dsi_me_1_df.merge(data_science_immersive_df, how='left', on='name')
df_all.describe()
from scipy.stats import norm $ z_critical_value = norm.ppf(1-0.05) $ print('critcal value of z-score at 5% alpha is: {}'.format(z_critical_value))
grouped_by_bedrooms_df = full_df.groupby('bedrooms')['listing_id'].count().reset_index().copy() $ grouped_by_bedrooms_df.columns = ['bedrooms','count_of_listings'] $ grouped_by_bedrooms_df
df
archive_df.info()
bacteria_data['year'] = 2013 $ bacteria_data
len(archive_clean[archive_clean.name.isnull()]) == len(faulty_names)
labels = investments.copy() $ labels['invested'] = 1
caps2_predicted
import requests $ from bs4 import BeautifulSoup as bs $ import pandas as pd
archive_clean.drop(archive_clean[archive_clean['retweeted_status_id'].notnull() == True].index, inplace = True)
ab_dataframe.isnull().any().any()
writer = pd.ExcelWriter("output/mediaflux_output_{}.xlsx".format(date))
df2 = df.copy() $ df2 = df2.drop('ENTRIES',1) $ df2 = df2.drop('EXITS',1) $
%%sql mysql://admin:admin@172.20.101.81 $ drop user if exists 'user1'@'%';
db_user_scenario_load =pd.read_sql_query( "SELECT * FROM user_scenario_load",conn) $ db_user_scenario_update =pd.read_sql_query( "SELECT * FROM user_scenario_update",conn) $ db_user_scenario_action =pd.read_sql_query( "SELECT * FROM user_scenario_action",conn)
tall = height > 180 $ tall
spark.createDataFrame(moviesCleanRdd, schema).show(10, False)
conv_treat_prob = len((df2[(df2['group'] == 'treatment') & (df2['converted'] == 1)])) / (df2['group'] == 'treatment').sum() $ print(conv_treat_prob)
from sklearn.preprocessing import StandardScaler $ scaler = StandardScaler() $ x_train1 = scaler.fit_transform(x_train1) $ x_test1 = scaler.transform(x_test1)
p_old =df2['converted'].mean() $ p_old
fb.message = fb.message.fillna(fb.story)
msftAC.tshift(1, freq="D")[:5]
loans_plan_origpd_xirr=cashflows_plan_origpd_all.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
kl_true = kl_divergence_gaussians(q_mu, q_sigma, p_mu, p_sigma) $ kl_true
week6 = week5.rename(columns={42:'42'}) $ stocks = stocks.rename(columns={'Week 5':'Week 6','35':'42'}) $ week6 = pd.merge(stocks,week6,on=['42','Tickers']) $ week6.drop_duplicates(subset='Link',inplace=True)
p_control_conv = df2.query('group == "control"')['converted'].mean() $ p_control_conv
df['Avg_speed']= (df['Trip_distance'] / df['Trip_duration']) #miles/second $ df['Avg_speed']=df['Avg_speed']*3600  #miles/hour $ df['Avg_speed'].describe()
watch_table = watch_table.rename(columns={'created_at':'date_starred', 'login':'user_login'}) $ watch_table['date_starred'] = watch_table['date_starred'].apply(pd.to_datetime)
nf = pd.read_csv('https://bit.ly/2mFxANJ') $ nf.head(2)
if search_results is not []: $     print(json.dumps(search_results[0]._json, indent=2))
print(type(blurb_SVD)) $ print(np.sum(tSVD.explained_variance_ratio_))
print(autos[~autos["registration_year"].between(1900,2016)]["registration_year"].value_counts())
dtc = DecisionTreeClassifier() $ param_grid = {'max_leaf_nodes': np.arange(2,50000, 1000)} $ CV_dtc = GridSearchCV(dtc, param_grid) $ CV_dtc.fit(X_train, y_train) $ CV_dtc.best_params_
user1 = user[['uuid', 'is_enabled']]
x["A"] $ x.get("A") $ x.A # works only for column names that are valid Python variable names
df_twitter_copy.info()
(autos['last_seen'] $         .str[:10] $         .value_counts(normalize = True, dropna = False) $         .sort_index() $         )
df_en[['text', 'polarity_blob', 'polarity_vader']][0:100] $
autos["last_seen"].str[:10].\ $ value_counts(normalize=True,dropna=False).\ $ sort_index(ascending=True)
logit_new = sm.Logit(df_mod['converted'], df_mod[['intercept','US', 'UK','ab_page','UK_ab_page']]) $ model_new = logit_new.fit() $ model_new.summary()
make_date_features(train, "train", "hour") $ make_date_features(test, "test", "hour")
df.index.name = 'week_ending'
r = requests.get('http://www.contextures.com/SampleData.zip') $ ZipFile(io.BytesIO(r.content)).extractall()
f = h5py.File('../../data/NEON_D02_SERC_DP3_368000_4306000_reflectance.h5','r') 
fraud_data_updated.info()
scores_mean = np.mean(raw_scores) $ scores_std = np.std(raw_scores) $ print('The mean is {:.5} and the standard deviation is {:.5}.'.format(scores_mean, scores_std))
re.findall('mailto:neal.caren@gmail.com so', text)
words_df=pd.DataFrame(convert_tuple_to_dict(most_common_words))
top_supporters["contributor_fullname"] = top_supporters.contributor_firstname + " " + top_supporters.contributor_lastname
weather_df = weather_df.reindex(taxi_hourly_df.index)
cutoff_times = pd.read_csv('s3://customer-churn-spark/p1/MS-30_labels.csv') $ cutoff_times.head() $ cutoff_times.tail()
print(len(train_df['app'].unique()))
bigdf.shape
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs)
support.sort_values('amount', ascending= False)
sorted(modern_combos.map(lambda c: (c[0], 1)).countByKey().iteritems(), key=lambda x: x[1], reverse=True)
data = data[data.price_aprox_usd.notnull()]
df.sort_values('Year',ascending=True).head()
us_cities = pd.read_csv("us_cities_states_counties.csv", sep="|") $ us_cities.head() $
breed_ratings = breed_ratings.sort_values(ascending = False)
pred_dict = {'Single Model': regular_df, 'Weak Learner': learner_df}
countries_df = pd.read_csv(r'C:\Users\Hank2\Desktop\Code_Nano\Python\ABtest\AnalyzeABTestResults 2./countries.csv') $
demographics = bookings[['hash_id', 'gender_desc', 'race_desc']]
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets']))) $ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets']))) $ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
autos = autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].value_counts(normalize=True).head(10)
weather_yvr['Relative Humidity (fraction)'] = weather_yvr['Relative Humidity (%)'] / 100
ioDF.columns.tolist()
qualification.qual_conversion.value_counts()
df.sort_index(axis=1, ascending=False)  # Sort by column
day.apply(pd.Timestamp('2014-01-01 09:00'))
adj_close_pivot_merged = pd.merge(adj_close_pivot, adj_close $                                              , on=['Ticker', 'Adj Close']) $ adj_close_pivot_merged.head()
df2 = df.drop(df.query('landing_page=="new_page"').query('group=="control"').index) 
automl.refit(X_test.copy(), y_test.copy()) $ print(automl.show_models())
autos = autos.drop(["num_photos", "seller", "offer_type"], axis=1)
filename = 'data/pulled_tweets/PT_all_airlines_df' $ pulledTweets_df = gu.read_pickle_obj(filename) $ pulledTweets_df.drop_duplicates(subset='text', inplace=True) $ pulledTweets_df.head(3)
autos['price'].value_counts().sort_index(ascending = False).head(20)
injury_df['team_id'] = injury_df['Team'].apply(lambda x: team_slugs_dict[process.extract(x, team_slugs_dict.keys(), limit=1)[0][0]])
questions.head()
sorted(data.date.tolist())[-1], sorted(data.date.tolist())[-1][0:4]
df.groupby('key').aggregate({'data1': 'min', $                             'data2': 'max'})
df_prep0 = df_prep(df0) $ df_prep0_ = pd.DataFrame({'date':df_prep0.index, 'values':df_prep0.values}, index=pd.to_datetime(df_prep0.index))
week20 = week19.rename(columns={140:'140'}) $ stocks = stocks.rename(columns={'Week 19':'Week 20','133':'140'}) $ week20 = pd.merge(stocks,week20,on=['140','Tickers']) $ week20.drop_duplicates(subset='Link',inplace=True)
es = es.entity_from_dataframe(entity_id = 'clients', dataframe = clients, $                               index = 'client_id', time_index = 'joined')
sub.to_csv('./submission/submission_2017-04-21_r1.csv', index=False)
for col in ["campaign_spend", "campaign_budget", "bid", "charged"]: $     print "{0: <20}{1}".format(col, raw_data[col].map(lambda x: x < 0).any())
ea = df_clean.copy()
run_augmented_Dickey_Fuller_test(RN_PA_duration, num_diffs=2)
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ res = logit_mod.fit()
%%writefile /tmp/test.json $ {"dayofweek": "Sun", "hourofday": 17, "pickuplon": -73.885262, "pickuplat": 40.773008, "dropofflon": -73.987232, "dropofflat": 40.732403, "passengers": 2}
talks['title'] = talks.name.apply(gettitle)
well_data['time'] = days $ print(well_data.head())
df['delivery_method'].unique()
len([x for x in comment_list if x[1]>1])
P_diff_actual = df2.query('group=="treatment"').converted.mean() - \ $     df2.query('group=="control"').converted.mean() $ (np.array(p_diffs) > P_diff_actual).mean()
plt.figure(1) $ tsla_plot = plt.subplot() $ tsla_plot.plot(tsla['Close'],color='green') $ plt.legend(['Tesla Close Value'],loc="upper left") $ plt.title('Valores de Cierre Tesla')
brazil.plot()
a = df2['landing_page'] == 'new_page' $ new_page_received = len(df2[a])/len(df2) $ print('probability that an individual received the new page:{}'.format(new_page_received))
results=logistic_model2.fit()
week1 = opening.rename(columns={7:'7'}) $ stocks = stocks.rename(columns={'Opening Price':'Week 1','Time':'7'}) $ week1 = pd.merge(stocks,week1,on=['7','Tickers']) $ week1.drop_duplicates(subset='Link',inplace=True)
predicted_talks_vector = classifier.predict( vectorized_text_predict )
slCases = slCases.rename(columns = {'National':'New cases'}) $ slDeaths = slDeaths.rename(columns = {'National':'New deaths'})
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == 1').count()[0] $ convert_new = df2.query('landing_page == "new_page" and converted == 1').count()[0] $ n_old = df2.query('group == "control"').count()[0] $ n_new = df2.query('group == "treatment"').count()[0]
df2['user_id'].value_counts().head()
train.head()
data = data.loc[data.duplicado == False]
ratings.show(5)
drop_table(cur_a, 'booking_contacts') $ drop_table(cur_b, 'booking_amounts')
df4 = df2.set_index("user_id").join(df3.set_index("user_id"))
df=pd.get_dummies(df,columns=['signup_app','affiliate_channel','affiliate_provider','first_affiliate_tracked','first_browser','first_device_type','signup_method','language'],drop_first=True)
california['bins'] = pd.qcut(california['FIRE_SIZE'],4) $ california['bins'].value_counts()
print('Shape : ', pd_train_filtered.shape) $ pd_train_filtered.sample(10)
popCon[popCon.content == 'photo'].sort_values(by='counts', ascending=False).head(10).plot(kind='bar')
df["Date"] = pd.to_datetime(df.Date)
first_result.find('strong').text 
lv_workspace.get_available_indicators(subset = 'B', step = 'step_2')
data['Closed Date'] = data['Closed Date'].apply(lambda x: datetime.datetime. $                                                strptime(x,'%m/%d/%Y %H:%M')) $ data.info()
df_Country = df_new.groupby('country').mean() $ df_Country['converted']
start = pd.datetime(2010, 1, 1) $ end = pd.datetime.today() $ p = web.DataReader("^GSPC", 'yahoo', start, end)  #S&P500 $ p.tail()
data1_new.head()
new_reps.head()
df_schema.query('table_name=="actor"')
hits_df.tail(3)
evaluator.plot_confusion_matrix(type='word_level', normalize = True)
with open('filename.pickle', 'wb') as handle: $     pickle.dump(Maindf, handle, protocol=pickle.HIGHEST_PROTOCOL)
PRE_LM_PATH = WT_PATH / 'fwd_wt103.h5'
perf_train.dtypes.value_counts()
med = out_df.copy() $ med['low']=0 $ med['medium']=1 $ med['high']=0 $ med.to_csv('all_med.csv',index=False)
stores_tran_nulls=pd.DataFrame(class_merged[pd.isnull(class_merged.transactions)]) $ stores_with_nulls=len(stores_tran_nulls['store_nbr'].unique()) $ all_stores=len(stores['store_nbr'].unique()) $ stores_with_nulls/all_stores
 df.raw_character_text.nunique(), df.character_id.nunique()
autos["price"].value_counts().sort_index(ascending = True).head(20)
dfz=df_clean.dropna(axis=0, how='any') $
prediction['p1'].value_counts()[:10]
df.converted.sum()/df_len
trump['est_time'] = ( $     trump['time'].dt.tz_localize("UTC") # Set initial timezone to UTC $                  .dt.tz_convert("EST") # Convert to Eastern Time $ ) $ trump.head()
treatment_counts = Counter() $ for elem in df["treatments"]: $     treatment_counts += Counter(elem)
z_score, p_value = sm.stats.proportions_ztest([old_conversions, new_conversions], [n_old, n_new], alternative = 'smaller') $ print(z_score) $ print(p_value) $
repeated = df2[df2.user_id.duplicated()] $ print("The repeated 'user_id' in df2 is:\n {}".format(repeated))
date_str = loc_date_str.split(',')[1].strip() + ', ' + loc_date_str.split(',')[2][:5].strip() $ date_str
tall.all()
df_goog['good'] = df_goog.Close > df_goog.Open $ df_goog
df_date_vs_count = reddit_comments_data.groupBy('created_date').count().orderBy('created_date').toPandas()
pt_weekly = pt_weekly.reindex(weeks, method='bfill') $ pt_weekly.head(10)
sorted(problem_combos.map(lambda c: (c, 1)).countByKey().iteritems(), key=lambda x: x[1], reverse=True)[:20]
from sqlalchemy import create_engine $ engine = create_engine('postgresql://postgres:123@localhost:5433/postgres')
soup = bs(facts_res.text, 'html.parser')
session_data = results_dict['session_summaries'][0] $ print("Data is saved as a {} object.\nSession Completed On: {}".format(type(session_data), $                                                                        session_data.session_datetime))
sns.heatmap(temp_fine)
data = stock.df() $ data.columns = [_.replace(" ", "_") for _ in data.columns] $ data = data.reset_index(drop=True).reset_index(drop=False) $ data.columns = ["t"] + list(data.columns[1:]) $ data.head()
lossprob2 = fe.bs.smallsample_loss(2560, poparr2, yearly=256, repeat=500, level=0.90, inprice=1.0) $
autos['odometer'] = autos.odometer.str.replace(',','').str.replace('km','').astype(float)
events.toPandas().head()
pca.explained_variance_ratio_
price_bb = pd.Series(price_by_brand) $ mileage_bb = pd.Series(mileage_by_brand) $ means_bb = pd.DataFrame(price_bb, columns = ['mean_price']) $ means_bb.loc[:,'mean_mileage'] = mileage_bb
n1 = len(df.query('landing_page=="new_page"').query('group=="control"')) $ n1
autos.rename({"odometer" : "odometer_km"},axis=1,inplace = True) $ autos["odometer_km"] = autos["odometer_km"].str.replace(",","").str.replace("km","").astype(float)
df.loc["Equifax",:]
df.describe(exclude = np.number)
ax = sns.boxplot(x="empl", y="y", data = psy_native) $
df[~treatment]['converted'].mean()
from bs4 import BeautifulSoup $ soup=BeautifulSoup(myhtml,"html.parser") $ _td=soup.find_all("td")
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2.head()
df_merged.info()
access_logs_df = 
response = requests.get(CSV_URL) $ with open(os.path.join("./", $                       CSV_URL.split("/")[-1]), mode='wb') as file: $     file.write(response.content)
P_Control_Converted = (df2.query('converted == 1')['group'] == 'control').mean() $ P_Control = (df2.group=='control').mean() $ P_Converted_Control = (P_Control_Converted * P_Converted) / P_Control $ P_Converted_Control
unique_users = len(df2.user_id.unique()) $ print('There are {} unique user ids in the dataframe df2.'.format(unique_users))
timezones = DataSet['userTimezone'].value_counts()[:10] $ print(timezones)
val
!rm -rf evalme
df.text[df.text.str.len() == df.text.str.len().min()]
df_clean3.loc[1202, 'text']
df = pd.read_csv('data/distances.csv') $ df.head()
np.mean(df2.days_repayment)
demographics.head()
LS_loans.loc[0,'APP_APPLICATION_ID']
all_data_long = pd.concat([all_data_as_dfs['pagecounts_desktop-site'], all_data_as_dfs['pagecounts_mobile-site'], $                            all_data_as_dfs['pageviews_desktop'], pageviews_mobile_df]) $ len(all_data_long)
agreement = tf.matmul(caps2_predicted, caps2_output_round_1_tiled, $                       transpose_a=True, name="agreement")
cleansed_search_df['SearchTerm'] = np.where(cleansed_search_df['SearchCategory'] == "Plant", cleansed_search_df['SearchTerm'].str.rpartition('-')[2].str.strip() , cleansed_search_df['SearchTerm']) $ cleansed_search_df.loc[cleansed_search_df['SearchCategory'] == "Plant"]
df_concensus_uaa.sort_values('latest_consensus_created_date')
own_star.sort_values(by=['user_id', 'repo_id']).head(10)
dataframe = dataframe.sort_index()
metrics = client.experiments.get_latest_metrics(experiment_run_uid)
joined=join_df(joined,trend_de,["Year","Week"],suffix='_DE') $ joined_test=join_df(joined_test,trend_de,["Year","Week"],suffix='_DE') $ sum(joined['trend_DE'].isnull()),sum(joined_test['trend_DE'].isnull())
weather.head()
public_tweets = api.user_timeline(target_user)
_ = ok.grade('q12') $ _ = ok.backup()
unique_urls.sort_values('total_votes', ascending=False)[0:20][['url', 'total_votes']]
experimentp = df2[df2['group'] == 'treatment']['converted'].mean() $ experimentp
pd.options.display.float_format = '{:,.2f}'.format $ feature_imp = pd.DataFrame({'name': X.columns, 'imp': gbm.feature_importances_}).sort_values(by = 'imp', ascending = False) $ feature_imp['mult_gbm'] = feature_imp.imp.max() / feature_imp['imp'] $ feature_imp['mult_rfc'] = rfc.feature_importances_.max()/rfc.feature_importances_ $ feature_imp $
output= "CREATE TEMPORARY TABLE ABC AS select user_id, tweet_content, retweets from tweet as t inner join tweet_details as td where t.tweet_id=td.tweet_id order by td.retweets desc" $ cursor.execute(output) $
churned_unordered = unordered_df.loc[churned_unord]
(np.random.binomial(1, p_new, n_new).mean()) - (np.random.binomial(1, p_old, n_old).mean())
df.corr()   # checking the correlation of the features
import re $ df.loc[:,"message"] = df.message.apply(lambda x : " ".join(re.findall('[\w]+',x))) $
top_brands_mean_mileages
plt.figure(figsize=(10, 7)) $ plt.plot(range(0,n_comp), list(tfidf_svd_v2.singular_values_)) $ plt.xlabel('Reduced Feature Space') $ plt.ylabel('SVD Singular Values') $ plt.title('Elbow Plot for TFIDF-SVD(' + str(n_comp) + ' reduced features)');
vueling = df[df["text_3"].str.contains("vueling", case = False)] $ vueling.text_3.str.split(expand=True).stack().value_counts().head(10).reset_index()
sns.factorplot(x='call_type',y='length_in_sec',col='call_day',data=calls_df,kind='bar')
sns.jointplot(train_data["pp_neg_words"], target_data['Prediction'], $               kind='reg', ylim=5000, size=6, space=0, color='b')
mydata = mydata.fillna(method="bfill")
dul1 = dul[~dul['ISBN RegEx'].isin(bad_dul_isbns)] $ dul1['ISBN RegEx'].size
details = client.repository.get_experiment_details(experiment_uid)
df.sort_values('prob', ascending=False).head(10)
import datetime as dt $ NOW = dt.datetime(2016,3,27)
df_tweet_json_clean['created_at'].head() $
df2['intercept'] = 1 $ df2['ab_page'] = df2.apply(lambda row: 1 if row.group == 'treatment' else 0, axis=1) $ df2.head()
taxi_hourly_df.loc[index_missin_hr0to6_before2016, "num_pickups"] = 0 $ taxi_hourly_df.loc[index_missin_hr0to6_before2016, "num_passengers"] = 0 $ taxi_hourly_df.loc[index_missin_hr0to6_before2016, "missing_dt"] = False
my_data.take(10)
num_encoder_tokens, body_pp = load_text_processor('body_pp.dpkl') $ num_decoder_tokens, title_pp = load_text_processor('title_pp.dpkl')
model = 0 $ model = Sequential()
nullrate
cutoff = datetime.datetime(2015, 3, 30, 11, 38, 5, 291165)
train.duplicated().sum()
dfHashtags.head()
number_pos = data_df[data_df['is_high_val'] == 1].shape[0] $ number_all = data_df.shape[0] $ print(f'Target labels of class \'1\': {number_pos} or {(number_pos/number_all)*100:.2f}% over all.')
n_old = df2.query("landing_page=='old_page'").shape[0] $ n_old
df.dist_km.mean()
sp500['XYL':'YUM']
dti = pd.to_datetime(['Aug 1, 2014', '2014-08-02', $ '2014.8.3', None]) $ dti #Notice that None is converted into a not-a-time value, NaT, which represents that the source data could not be converted into datetime.
df.head(1)
tweets_clean['dog_class'] = None
change_int_datatype_cols(df_test ) $ change_float_datatype_cols(df_test ) $ change_int_datatype_cols(df_train) $ change_float_datatype_cols(df_train)
null_name = raw_data[(raw_data['name'].isna())] $ null_name.T
element = driver.find_element_by_xpath('//*[@id="comic"]/img') $ element.get_attribute("title")
import sqlite3
df.loc[monthMask, 'water_year'] = df['year'] + 1
temp = df2.converted.value_counts() $ prop = temp/temp.sum() $ print "Probability of converting regardless of the page they receive:",prop[1]
from datetime import datetime, date, timedelta
print "Mean Time for closing a ticket: %f hours" % (time2close.mean()/3600.0) $ print "Median Time for closing a ticket: %f hours" % (time2close.median()/3600.0) $ print "Quantiles: " $ print time2close.quantile([0.25, 0.5, 0.75])
archive_df.sample(20)
ws = Workspace.create(name = workspace_name, $                       subscription_id = subscription_id, $                       resource_group = resource_group, $                       location = workspace_region) $ ws.get_details()
fig = plt.figure(figsize=(12,8)) $ ax = fig.add_subplot(111) $ fig = qqplot(resid_701, line='q', ax=ax, fit=True)
import numpy as np $ import matplotlib.pyplot as plt $ import mlai
validation_data = data[data['loan_status']=='Current'].copy() $ validation_data['month_since_launch'] = (datetime.datetime(2016,1,1) - validation_data['issue_d']).\ $ apply(int) / (86400 * 1e9) $ validation_data['time_to_fully_paid'] = 365 * 3.0 - validation_data['month_since_launch'] 
for tweet in trumpTweets: $     cleaner_tweets.append({'id': tweet.id, 'text': tweet.text, 'created_at': tweet.created_at, 'profile': tweet.user.screen_name}) $
tweets.head(3)
train_pred = rf.predict(X_train)
SVPOL(data/'realdata'/'MV8.dat').to_dataframe().head()
df.info()
post_url = 'https://api.instagram.com/oembed?url=http://instagr.am/p/fA9uwTtkSN/' $ response = requests.get(post_url)
plotdf.loc[plotdf.index[-1], 'forecast'] = hourlyRates.loc[hourlyRates['hourNumber'] == thisWeekHourly['hourNumber'].max(),'meanRemainingTweets'].iloc[0] + plotdf['currentCount'].max() $ plotdf.loc[plotdf.index[-1], 'forecastPlus'] = plotdf.loc[plotdf.index[-1], 'forecast'] + hourlyRates.loc[hourlyRates['hourNumber'] == thisWeekHourly['hourNumber'].max(),'stdRemainingTweets'].iloc[0] $ plotdf.loc[plotdf.index[-1], 'forecastMinus'] = plotdf.loc[plotdf.index[-1], 'forecast'] - hourlyRates.loc[hourlyRates['hourNumber'] == thisWeekHourly['hourNumber'].max(),'stdRemainingTweets'].iloc[0]
priority_num[:10]
q = '(immigration OR immigrants) AND (families OR family) AND (separate OR separation) AND trump)'
who_counts[:10]
! ls -l ~/.dw/cache/data-society/the-simpsons-by-the-data/latest/data
!python -m spacy download en
%%time $ df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', usecols=['Unique Key','Created Date', 'Closed Date', 'Agency', 'Complaint Type', 'Descriptor', 'City']) $
monte.str.startswith('T')
first_cluster = LDACluster(num_topics=4)
dmap = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'} $ DayOfWeek = list(map(lambda var: dmap[var],df['Day of Week'])) $ df['Day of Week'] = DayOfWeek $ df.head()
dr = pd.date_range('1/1/2010', periods=3, freq=3 * pd.datetools.bday)
df["precipitation"].describe()
import pandas as pd
rtc = pd.read_excel('input/data/ExogenousTransmissionCapacity.xlsx', $                     pars_cols='B:R', $                     header=3)
print "NYC:", dfNYC["Created Date"][1] $ print "SF:", dfSF["Opened"][1]
len(df2_treatment)/len(df2)
dfRegMet2015 = dfRegMet[dfRegMet.index.year == 2015]
df_arch_clean[df_arch_clean['retweeted_status_timestamp'].notnull()]
df2=df2.set_index('user_id').join(df_country.set_index('user_id'))
def get_long_short(close, lookback_high, lookback_low): $     return ((close < lookback_low).astype(int) * -1) + (close > lookback_high).astype(int) $ project_tests.test_get_long_short(get_long_short)
tl_2020 /= 1000 $ tl_2020_norm = tl_2020 ** (10/11) $ tl_2020_norm = tl_2020_norm.round(1) $ tl_2020_alpha = tl_2020 ** (1/3) $ tl_2020_alpha = tl_2020_alpha / tl_2020_alpha.max().max()
mydata = quandl.get("FSE/AFX_X", start_date="2017-01-01", end_date="2017-12-31")
len(joined_train_df)
time_step=list() $ for i in range(0,len(values)): $         time_step.append(i);
Rationales
df = pd.read_table("msft.csv", sep=",") $ df.head()
wine_reviews.shape # To check the number of rows and columns in our datset
closed_pr = PullRequests(github_index).is_closed().get_cardinality("id_in_repo").by_period() $ print("Trend for month: ", get_trend(get_timeseries(closed_pr)))
Test.SetFlowValues(6.5, 9.5, 0.45)
r.aggregate({'A' : np.sum,'B' : np.mean}) #Apply Different Functions to Different Columns of a Dataframe
trading.df.tail()
dmh.CASDataMsgHandler.__subclasses__()
autos[['date_crawled','ad_created','last_seen']].head(5)
df
raw.shape
try: $     import test_package.print_hello_function_container.print_hello_function $ except: $     print("A function cannot be imported directly with dot notation")
celtics = pd.read_csv('../../../data/celtics/home.csv')
joined=load_df('joined_promo_bef_af2.pkl')
pax_raw.loc[pax_raw.paxstep > step_threshold, 'paxstep'] = np.nan $ pax_raw['paxstep'] = pax_raw.groupby('seqn').paxstep.transform(pd.DataFrame.interpolate, method='linear')
scores.head()
fuel_therm_abs_rate = sp.get_tally(name='fuel therm. abs. rate') $ therm_util = fuel_therm_abs_rate / therm_abs_rate $ therm_util.get_pandas_dataframe()
hr["icustay_id"].value_counts()
sorted(json_dict, key = lambda x: x[0]) $ (result,date1,date2) = max([(abs(u[4]-v[4]),u[0],v[0]) for u,v in zip(json_dict,json_dict[1:])]) $ print('The largest change between any two dates {} and {} is {:.2f}'.format(date1,date2,result)) $
df4.head() # first 5 rows by default
countries_df = pd.read_csv('countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
fullProduct = dataSet1 @ dataSet2.T
lr = 1e-3 $ lrs = lr
testObjDocs.outDF.tail()
p_diffs=[] $ new_convert=np.random.binomial(n_new, p_new, 10000)/n_new $ old_convert=np.random.binomial(n_old, p_old, 10000)/n_old $ p_diffs=new_convert-old_convert $
df = pd.read_csv('data/clean_data.csv') $ df = df.drop(['Unnamed: 0'], axis=1)   # I don't care for the first column that was imported
df_A.groupby('Gender').get_group('F')
url_img = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars' $ browser.visit(url_img)
dframe_team = dframe_team[~((dframe_team['Start'] >= dframe_team['start_cut']) & (dframe_team['End'] < dframe_team['end_cut']))] $ dframe_team = dframe_team[~((dframe_team['Start'] <= dframe_team['start_cut']) & (dframe_team['End'] <= dframe_team['start_cut']))] $ dframe_team
df_sig_features = pd.read_csv('./data/significant_features.csv')
X = fires.loc[:, 'glon':'rhum_perc_lag12'] $ X = X.drop(['date'], axis=1) $ y = fires['fire']
test_cleaned = test.replace(re.compile(r"http.?://[^\s]+[\s]?")) $ test_cleaned = test_cleaned.replace(re.compile(r"@[^\s]+[\s]?")) $ test_cleaned = test_cleaned.replace(re.compile(r"\s?[0-9]+\.?[0-9]*"))
missing_info = list(Users_first_tran.columns[Users_first_tran.isnull().any()]) $ missing_info
scores = scores[['date','home team', 'home pts', 'away team', 'away pts', $                  'winner', 'home team 41 game win%', 'home team 8 game win%', $                  'away team 41 game win%', 'away team 8 game win%',]] $ scores.head(3)
y = df.groupby('Journal')['Title'].count() $ df_less = df[df['Journal'].apply(lambda x: y[x]>50)]
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import glob $ %matplotlib inline
arma_mod30 = sm.tsa.ARMA(dta_713, (3,0)).fit(disp=False) $ print(arma_mod30.params)
neighborhoods = pd.read_csv('../final_project/neighborhoods.csv') $ neighborhoods.head()
print(p)
mean_absolute_percentage_error(y_test, y_pred)
fmt = "%Y-%m-%d %H:%M:%S" $ data_sample['startDate'] = pd.Series([datetime.datetime.strptime(i, fmt) for i in data_sample['startDate']])
df_categories.category_id.unique().shape
my_df.to_csv("my_df.csv") $ my_df.to_html("my_df.html") $ my_df.to_json("my_df.json")
import statsmodels.api as sm $ lm = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ results = lm.fit() $
reddit.shape
use_data, comments, comment_list, x_tokens = make_data(data, only_fans=True, tail_win_diffs=True)
payload = { $     'location': [40.746054, -111.847987], $     'snappingOn': True, $     'dataSource': 'nhd'} $ json_dat = json.dumps(payload)
df3 =df2.merge(countries, on='user_id') $ df3.head(3) $
all_within_3A = pmol.df[pmol.df['distances'] <= 3.0] $ all_within_3A.tail()
df_complete_temp_CTD_1988
users['key'] = 0 $ products['key'] = 0 $ df_user_product = users.merge(products, how='outer') $ cols = ['UserID','ProductID'] $ df_user_product[cols]
autos = autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].value_counts(normalize=True).head(10)
to_be_predicted_Day2 = 21.3690037 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
df_ratings.drop('video_watched_type', axis=1, inplace=True) $ df_ratings.drop('rating_date_creation', axis=1, inplace=True) $ df_ratings
learn.load('clas_2')
data.drop(columns=['deadline', 'launched_at'], axis=1, inplace=True)
df = df[['text', 'sentiment']] $ display.display(df['text'].describe()) $ display.display(df['sentiment'].describe())
taxa_labels=list(set(merged_data.taxa)) $ plot_labels=list(set(merged_data.plot_id)) $ print(taxa_labels) $ print(plot_labels)
test_data.tail()
sunspots.index = sunspots['year_month_day'] $ sunspots.index.name = 'date'  # change the col name to date $ sunspots.info()
conv_giv_ctrl = prob_conv_and_ctrl/prob_ctrl $ conv_giv_ctrl
alameda = geojson.load(open("data/alameda-2010.geojson")) $ myMap = folium.Map(location=(37.8044, -122.2711), zoom_start=11.4) $ map_data(myMap, alameda, image_data).save("map1.html") $ IFrame('map1.html', width=700, height=400)
df.loc[7,:] = ('Moore', 'Andrew') $ df
df.dateCrawled = pd.to_datetime(df.dateCrawled) $ df.dateCreated = pd.to_datetime(df.dateCreated) $ df.lastSeen = pd.to_datetime(df.lastSeen)
dum = pd.get_dummies(TestData, sparse=True, drop_first=True, dummy_na=True, $                     columns=['Gender', 'City_Category', 'Employer_Category1', 'Primary_Bank_Type', 'Contacted', $                             'Source_Category', 'Employer_Category2', 'Var1']) $ dum.head()
tweet_image.info()
(details.Popularity == 0).value_counts()
sp500[(sp500.Price < 10) & (sp500.Price > 0)][['Price']]
old_page_converted = np.random.choice(np.arange(2), size=n_old, p=[(1-p_old), p_old]) $
y = df['custcat'].values $ y[0:5]
df_archive["pupper"].value_counts()
import sklearn $ sklearn.__version__
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response = requests.get(url) $ response
row_count= df.shape[0] $ df_length = len(df)         $ print(df_length) $ print("Number of rows is: {}".format(row_count))
print(df,'\n') $ print(df.apply(lambda x: x.max()-x.min()))
for ix, file in enumerate(s3_key_origs): $     s3_download.meta.client.download_file(s3_bucket, file, local_orig_keys[ix])
%matplotlib nbagg $ df4.plot(x='Date', y='BG') $ plt.title('Blood Glucose over 100 Values on Dec. 22, 2017', $           color='Blue') $ plt.show() $
pitcherFirst = input("Please enter a pitcher's first name: ") $ pitcherLast = input("Please enter a pitcher's last name: ") $ pid = pyb.playerid_lookup(pitcherLast,pitcherFirst).iloc[0]['key_mlbam'] $ pid $
vocabulary_expression['component_2'].sort_values(ascending=False).head(7) $
donors_c.loc[donors_c['Donor Zip'].notnull(), 'Donor Zip'].value_counts().tail(20)
pivoted.T[labels==0].T.plot(legend=False, alpha = 0.1);
kick_data_state.columns
tollrides = trips[trips['tolls_amount'] > 0] $ tollrides[tollrides['pickup_datetime'] == '2014-05-20 23:09:00'][5:]
now = datetime.datetime.utcnow() $ today = now.strftime("%Y-%m-%d") $ print('This notebook was last rendered on {}.'.format(today))
naive_bayes_classifier = nltk.NaiveBayesClassifier.train(train_set)
net.add_nodes(pop_name='excitatory', $               ei='e', $               model_type='population', $               model_template='dipde:Internal', $               dynamics_params='excitatory_pop.json')
sorted_agg_churned_plans_counts
DataSet = DataSet[DataSet.userTimezone.notnull()] $ len(DataSet) $ len(DataSet)/5000*100 $
df = pd.merge(data, tags, left_on=['tag'], right_on=['Tags'], how='outer')
data.head()
words = dicts['words2idx'] $ labels = dicts['labels2idx'] $ tables = dicts['tables2idx']
sub_df.head()
folderData = 'data\\'
db = sqlite3.connect('data.db') $ df_tweets = pd.read_sql_query("SELECT * FROM clean", db) $ df_mentions = pd.read_sql_query("SELECT * FROM mentions", db) $ df_hashtags = pd.read_sql_query("SELECT * FROM hashtags", db) $ df_tweets
df['comments'] = df['comments'].str.replace(',', '')
plt.scatter(sing_fam.sqft.values, sing_fam.rp1lndval.values);
df_birth.Continent.value_counts(dropna=False)
import pandas as pd $ df = pd.read_csv('purchases.csv') $ df
to_be_predicted_Day5 = 22.24313565 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
df['source_new']=df.apply(lambda x:process_source(x['source']), axis=1 )
random_features = bag_of_words_vectorizer.transform(random_corpus)
Model = XGBClassifier() $ Model.fit(num_round)
trees = model_outer.get_params()['n_estimators'] $ lr = model_outer.get_params()['learning_rate'] $ depth = model_outer.get_params()['max_depth'] $ clf = sk.ensemble.GradientBoostingClassifier(learning_rate=lr, n_estimators=trees, max_depth=depth).fit(X_train, y_train)
!gsutil cp %Zs3_key2% %Zgs_key%
!rm world_bank.json.gz -f $ !wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz
ng_m1 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 5, size=100, sg=1) $ ng_m2 = gensim.models.Word2Vec(train_clean_token, min_count=5, workers=2, window = 5, size=100, sg=1) $ ng_m3 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 10, size=100, sg=1) $ ng_m4 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 5, size=300, sg=1) $ ng_m5 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 30, size=300, sg=1)
from __future__ import print_function $ import pybacktest  # obviously, you should install pybacktest before importing it $ import pandas as pd
df['ts_dayofweek']=df['timestamp'].apply(lambda x:x.dayofweek)
df_h1b_nyc = df_h1b_nyc.drop([60049,396316,275446])
print('Null Values for Donations.csv: ', donations.isnull().sum()) $ print('Null Values for Donors.csv: ', donors.isnull().sum()) $
ldamodel = models.ldamodel.LdaModel( $     terms_matrix, num_topics=10, id2word=dictionary, passes=5)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?' $                  + 'start_date=2017-01-01&end_date=2017-12-31' $                  + '&api_key=' + API_KEY) $
pop[:, 2010]
df_test['dayofweek'] = df_test['effective_date'].dt.dayofweek $ df_test['weekend']= df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0) $ df_test.head()
parse_dict['profile'].head(5) $
comments = pd.read_csv('data/comments.csv', index_col = 0, parse_dates = [2]) $ comments.head()
str(user) $
learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn) $ learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1) $ learn.clip=25. $ learn.metrics = [accuracy]
df.head()
print(match_model.summary())
Train.describe()
total= pd.concat([df1,df2]) $ total.index=range(total.shape[0]) $ total
df['Temperature']   = df['Temperature'].astype(float) $ df['Precipitation'] = df['Precipitation'].astype(float)
print('Most LaziestCanine:', tweets_pp[tweets_pp.handle == 'Lazy dog'].sort_values( $     'laziestcanine_pp', ascending=False).text.values[0]) $ print('Least LaziestCanine:', tweets_pp[tweets_pp.handle == 'Lazy dog'].sort_values( $     'laziestcanine_pp', ascending=True).text.values[0])
df2['DepTimeStr'] = series
df_new.head(3)
labels.columns = ['id','label']
df['AQI Category'].cat.categories = ['Good', 'Moderate', 'Unhealthy', 'Somewhat Unhealthy'] $ df['AQI Category'] = df['AQI Category'].cat.add_categories(['Very Unhealthy']) $ df['AQI Category'] = df['AQI Category'].cat.reorder_categories(['Good', 'Moderate', 'Somewhat Unhealthy', 'Unhealthy', 'Very Unhealthy'], ordered=True)
urban_summary_table = pd.DataFrame({"Average Fare": urban_avg_fare, $                                    "Total Rides": urban_ride_total}) $ urban_summary_table.head()
pred = xgboost.predict(cvecogXfinaltemp) $ print classification_report(pred, ogy) $ print confusion_matrix(ogy, pred) $ print "cross val score accuracy :"+str(sum(cross_val_score(xgboost, cvecogXfinaltemp, ogy))/3) $ print 'roc_auc score :'+str(sum(cross_val_score(xgboost, cvecogXfinaltemp, ogy, scoring='roc_auc'))/3) $
call_model_price = black_scholes_call_value(S, K, r, t, vol) $ print 'Call implied volatility if market and model were equal (should be close to 0.25) %0.6f' % call_implied_volatility(S, K, r, t, call_model_price) $ put_model_price = black_scholes_put_value(S, K, r, t, vol) $ print 'Put implied volatility if market and model were equal (should be close to 0.25) %0.6f' % put_implied_volatility(S, K, r, t, put_model_price)
print("Number of unique users : " + str(df.user_id.nunique()))
le_data = le_data_all.reset_index().pivot(index="country", $                                           columns="year") $ le_data.ix[:,0:3] $
tweets = mydata[['tweet.id','tweet.created_at','tweet.text','user.id','tweet.coordinates','location.name','tweet.media','tweet.lang', 'tweet.hashtags']] $ tweets
expiry = datetime.date(2015,1,17) $ aapl_calls = aapl.get_call_data(expiry=expiry) $ aapl_calls.iloc[0:5,0:4]
pivoted.T[labels == 1].T.plot(legend=False, alpha=0.1)
from dask.diagnostics import Profiler, ResourceProfiler $ with Profiler() as prof, ResourceProfiler(dt=0.05) as rprof: $      out = d.compute()
df['Memo'].apply(returnCategory).value_counts()
import warnings $ warnings.filterwarnings('ignore')
p_old = df2[df2['converted'] == 1].user_id.nunique()/df2.user_id.count() $ p_old
image_predictions.info()
station_count.sort_values(['Count'],ascending=False, inplace=False, kind='quicksort', na_position='last')
subwaydf.iloc[154900:154915] #this low number seems to be because entries and exits resets
archive_copy['dog_description'].unique()
apple['2017-07']
(p_diffs>p_diff_obs).mean()
train_data.groupby(['device.browser']).agg({'visitNumber': 'count'}).reset_index().set_index("device.browser",drop=True).plot.bar()
for n in range(20): $     ypred = model.predict(np.random.random((1, 95,1))) $     int_to_char = {v:k for k,v in char_to_int.items()} $     results = int_to_char[np.argmax(ypred)] $     print(np.argmax(ypred),results)
eia_extra.loc[idx['DPV',:,:]]
ts.head()
learn.load("clas_0")
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_opb_brace_saddle[0][0])) $ perr = np.sqrt(np.diag(pcov_opb_brace_saddle[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
print(soup.get_text()[:500])
from jyquickhelper import add_notebook_menu $ add_notebook_menu()
train.head()
print('Python version ' + sys.version) $ print('Pandas version ' + pd.__version__) $ print('Matplotlib version ' + matplotlib.__version__) $ print('Numpy version ' + np.__version__)
obj = pd.Series([4, 7, 3, -2])
n_old = df2[df2.landing_page =='old_page'].count()['user_id'] $ n_old
autos['price'] = autos['price'].str.replace(',','').str.replace('$','').astype(int) $ autos['odometer'] = autos['odometer'].str.replace(',','').str.replace('km','').astype(int) $ autos.rename({"odometer": "odometer_km"}, axis=1, inplace=True) $ autos.head() $
[t for t in spacy_tok(review[0])][:10]
sub_df['sub_len'] = [len(x.split(' ')) for x in sub_df['text']]
from sklearn.preprocessing import LabelEncoder $ le = LabelEncoder() $ y_train = le.fit_transform(train[target])
import pandas as pd
df_train["num_photos"] = df_train["photos"].apply(len) $ df_train["num_features"] = df_train["features"].apply(len) $ df_train["num_description_words"] = df_train["description"].apply(lambda x: len(x.split(" ")))
beirut.index = beirut['EET']
query_result1 = feature_layer.query(where='POP2010>1000000', $                                     out_fields='WHITE,BLACK,MULT_RACE,HISPANIC') $ len(query_result1.features)
import urllib $ iris_url="http://aima.cs.berkeley.edu/data/iris.csv" $ urlRequest=urllib.request.Request(iris_url) $ iris_file=urllib.request.urlopen(urlRequest) $ iris_fromUrl=pd.read_csv(iris_file,sep=',',header=None,decimal='.',names=['sepal_length','sepal_width','petal_length','petal_width','target'])
import os $ import numpy as np $ import pandas as pd $ import tables as tb
tweets_master_df = pd.merge(twitter_archive_df_clean, tweets_df_clean, $                             on='tweet_id', how='inner') $ tweets_master_df = pd.merge(tweets_master_df, image_predictions_df, $                             on='tweet_id', how='inner')
df.apply(lambda x: x.max() - x.min())
crime 
santos_winpct.loc[santos_winpct['date'] == '2017-09-07']['playtext'].unique()
print(df.apply(np.cumsum))
promo_df.sort_values(['item_nbr','store_nbr', 'date'], inplace=True)
unidentified[0]
caps2_output
change_index = df3[df3['group'] == 'treatment'].index $ df3.set_value(index=change_index, col='ab_page', value=1) $ df3.set_value(index=df3.index, col='intercept', value=1) $ df3[['intercept', 'ab_page']] == df3[['intercept', 'ab_page']].astype(int) $ df3 = df3[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
import statsmodels.api as sm $ convert_old = conv_giv_ctrl $ convert_new = conv_giv_trt $ n_old = n_old $ n_new = n_old
len(df_json.query('retweeted == True'))
df2_clean.jpg_url.duplicated().value_counts()
coin_hour = get_historical_price_hour(COIN) $ print(coin_hour.head())
autos.describe(include='all')
Convert_Prob=df2.query("converted==1").shape[0]/df2.shape[0] $ print("The probability of any individual converting is ", Convert_Prob)
users.toPandas().head()
def get_http_json_response_contents(url): $     return urllib.request.urlopen(url).read()
a.sum(0)
params = {'figure.figsize': [4,3],'axes.grid.axis': 'both', 'axes.grid': True, 'axes.labelsize': 'Medium', 'font.size': 12.0, \ $ 'lines.linewidth': 2} $ plot_partial_autocorrelation(series=RNPA_new_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='PACF {}'.format('first difference of RNPA hours new patients')) $ plot_partial_autocorrelation(series=RNPA_existing_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='PACF {}'.format('first difference of RNPA hours existing patients'))
a.find('t')
original_months = months
tmi['weekEnergy'].max()
p_sort = prod_sort.merge(sale_prod_sort,how='left', left_on='Country', right_on='Country') $ p_sort['SalePrice'] = p_sort['SalePrice'].fillna(0) $ p_sort
store_items.fillna(method = 'backfill', axis = 1)
print(datetime.now() - timedelta(hours=1)) $ print(datetime.now() - timedelta(days=3)) $ print(datetime.now() + timedelta(days=368, seconds=2))
df['payout_type'].unique()
words_sum = preproc_reviews.sum(axis=0) $ counts_per_word = list(zip(pipe_cv.get_feature_names(), words_sum.A1)) $ sorted(counts_per_word, key=lambda t: t[1], reverse=True)[:10]
girls_by_name.loc['HARJIT':'HARKIRAN']
df2[df2['user_id'] == 773192] 
rng = pd.date_range('1/1/2018', periods=120, freq='S') $ len(rng)
page = re=json.load(urllib2.urlopen(url)) $ page = pd.DataFrame.from_dict(page)
print('dataframe shape: {}'.format(country.shape)) $ country.head()
data3.head()
pd.scatter_matrix(df, diagonal='kde', color='k', alpha=0.5, figsize=(12, 6)) $ tight_layout()
len(df2.query('group=="control"').query('converted==1'))/len(df2.query('group=="control"'))
from sklearn.model_selection import train_test_split, GridSearchCV $ from sklearn.tree import DecisionTreeClassifier $ from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier $
sns.jointplot(x = "negative_ratio", y = "wow_ratio", data = news_df)
news_sentiments = pd.DataFrame.from_dict(sentiments) $ news_sentiments.head()
df_MC_most_Convs = pd.concat([year_month.transpose(), df_MC_mostConvs], axis=1) $ print 'DataFrame df_MC_most_Convs: ', df_MC_most_Convs.shape $ df_MC_most_Convs
planevisits_df=planevisits.toPandas() $ planevisits_df
X = pd.merge(X, meals[['id','venue_id']], left_on='meal_id', right_on='id', how='inner') $ del X['id'] $ X = pd.merge(X, venues[['id', 'is_venue_style_pop-up-space', 'is_venue_style_apartment', 'is_venue_style_restaurant', 'is_venue_style_farm', 'is_venue_style_house', 'is_venue_style_brown-stone']], left_on='venue_id', right_on='id', how='inner') $ del X['id'] $ del X['venue_id'] $
list(meals.columns)
nba_df.drop([30,31], axis=0, inplace= True) $ nba_df.loc[28:34]
venues_df[venues_df['rating_count']>0][['venue_id','venue_type','normalised_rating']].groupby('venue_type')\ $             .agg({'venue_id':'count','normalised_rating':['mean','median',percentile(10),percentile(90)]})
sns.distplot(utility_patents_subset_df.prosecution_period, color="orange") $ plt.show()
df_new[['UK', 'US', 'CA']] = pd.get_dummies(df_new['country']) $ df_new=df_new.drop('UK',axis=1) $
r.loc['20170228':'20170307', ['share']]
df.select('b', b_threeish).show(5)
train = full['2015'] $ test = full['2016']
all_gen2_verse=[] $ for verse in gen2: $     gen2verse = sent_tokenize(verse) $     all_gen2_verse.extend(gen2verse) $ print(all_gen2_verse)
print(multi.shape, pp.shape)
f_model = sm.Logit(df_new["converted"],df_new[["intercept","uk","us","treatment"]]) $ f_result = f_model.fit() $ f_result.summary2() $
1 - p_value/2
n=len(df) $ n
scoring_url = client.deployments.get_scoring_url(deployment_details) $ print(scoring_url)
weather.date = pd.to_datetime(weather.date, format='%m/%d/%Y')
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ z_score,p_value
loan_stats['loan_status'].isna().sum()
reddit.describe()
help(pd.read_excel)
print(len(start_lat)) $ print(len(start_lon))
fx_injured = pd.merge(df1,df2,how='left',on=['Name','Season']) $ fx_healthy = pd.read_csv('/Users/isaacgammal/Downloads/healthy.csv') $ fx = pd.concat([fx_injured,fx_healthy],axis=0)
business_days
stocks = pd.concat([AAPL, GOOGL, FB], keys=['Apple', 'Google', 'Facebook'])
obs_diff_treatment = df2.query('group == "treatment"').query('converted == 1')['user_id'].count() / df2.query('group == "treatment"')['user_id'].count() $ obs_diff_treatment
df.head()
new_page = df2[df2["landing_page"] == 'new_page'] $ new_page_prob = new_page.shape[0]/df2.shape[0] $ new_page_prob
get_descendant_frame(observations_ext_node, data)[observations_ext_node['number'][DATA].notnull()].head(14)
b_cal_q1.dtypes
Counter(weather_df["weather_main"].values)
df.groupby("cancelled")["pickup_dow"].mean()
df2['intercept'] = 1 $ df2['ab-page'] = df2['group'].replace(('treatment', 'control'), (1, 0)) $ df2.head()
!protoc object_detection/protos/*.proto --python_out=.
html = browser.html $ soup = bs(html, 'html.parser') $ current_img_link = soup.find_all('article', class_="carousel_item") $ current_img_link
movies.fillna(value=5)
national_hol=national_holidays[['date','description','locale']] $ print("Rows and columns:",national_hol.shape) $ pd.DataFrame.head(national_hol)
print(type(df.groupby("grade").count())) # as data frame ('id' column and 'raw_grade' column both contained) $ df.groupby("grade").count()
returns['MSFT'].corr(returns['IBM'])
null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size)
tweet_data = pd.DataFrame(columns = ['id','retweet_count','favorite_count']) $ for line in lines: $     x = json.loads(line) $     tweet_data = tweet_data.append({'id':x['id'], 'retweet_count':x['retweet_count'],'favorite_count':x['favorite_count']}, ignore_index = True)
df2 = df[['sentiment', 'state']] $ df3 = df2.pivot_table(index='state', aggfunc='mean') $ df3 = df3.drop(['', 'Burger King', 'IHOP', 'United States', 'Yogurtland']) $ pd.set_option("display.max_rows", 500) $ df3
with open('../../vectors/GloVe_scratch_files/vectors.txt', 'r') as file: $     lines = file.readlines() $
miner = TweetMiner(twitter_keys, api, result_limit=2)
scoring_url = json.loads(response_online.text).get('entity').get('scoring_url') $ print scoring_url
df3=df3.merge(df2,how="inner") #Merging by Common User Id $ df3.head(3) $ df3[["UK","US"]]=pd.get_dummies(df3['country'],drop_first=True)
df_A2=pd.read_csv("classA.csv") $ df_A2.index
gene_to_seq_dict = {'Rv1295': 'MTVPPTATHQPWPGVIAAYRDRLPVGDDWTPVTLLEGGTPLIAATNLSKQTGCTIHLKVEGLNPTGSFKDRGMTMAVTDALAHGQRAVLCASTGNTSASAAAYAARAGITCAVLIPQGKIAMGKLAQAVMHGAKIIQIDGNFDDCLELARKMAADFPTISLVNSVNPVRIEGQKTAAFEIVDVLGTAPDVHALPVGNAGNITAYWKGYTEYHQLGLIDKLPRMLGTQAAGAAPLVLGEPVSHPETIATAIRIGSPASWTSAVEAQQQSKGRFLAASDEEILAAYHLVARVEGVFVEPASAASIAGLLKAIDDGWVARGSTVVCTVTGNGLKDPDTALKDMPSVSPVPVDPVAVVEKLGLA', $                     'Rv2233': 'VSSPRERRPASQAPRLSRRPPAHQTSRSSPDTTAPTGSGLSNRFVNDNGIVTDTTASGTNCPPPPRAAARRASSPGESPQLVIFDLDGTLTDSARGIVSSFRHALNHIGAPVPEGDLATHIVGPPMHETLRAMGLGESAEEAIVAYRADYSARGWAMNSLFDGIGPLLADLRTAGVRLAVATSKAEPTARRILRHFGIEQHFEVIAGASTDGSRGSKVDVLAHALAQLRPLPERLVMVGDRSHDVDGAAAHGIDTVVVGWGYGRADFIDKTSTTVVTHAATIDELREALGV'} $ my_gempro.manual_seq_mapping(gene_to_seq_dict)
feat_imp[:66]
bds = words_to_match()
autos["price"].describe()
print(lowest_open)
df2 = df2[df2.duplicated(subset='user_id', keep='first') == False] $ df2.info()
df.loc[[101,103,105]]
n_old = df2[df2['landing_page'] == "old_page"]['converted'].count() $ n_old
automl.leader.predict(test_data=test)
sns.violinplot(x="status", y="borough",data = samp311);
ptgeom = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])] $ sites = gpd.GeoDataFrame(df, geometry=ptgeom, crs={'init': 'epsg:4326'}) $ sites.head(5)
data = [{'alex': 1, 'joe': 2},{ 'ema': 5, 'dota': 6, 'alice': 11}]
doctors = duration_train_df[duration_train_df['Specialty'] == 'doctor'] $ therapists = duration_train_df[duration_train_df['Specialty'] == 'therapist'] $ RN_PA = duration_train_df[duration_train_df['Specialty'] == 'RN/PA']
data['inday_icu'].value_counts()
autos.columns
cd ..
df.sum()
with open("valence_df_dict.json", "r") as fp: $     valence_df_dict = json.load(fp) $ valence_df = pd.DataFrame(valence_df_dict)
train_df[['comments', 'favs', 'views', 'views_lognorm', 'comments_lognorm', 'favs_lognorm']].corr()
conn.columninfo(table=dict(name='banklist', $                            caslib='casuser'))
print(df_A.index) $ print(df_A)
pd.date_range(pd.datetime(2016, 1, 1), pd.datetime(2016, 7, 1), freq="W")
testObjDocs.reindex_OutDF() $ testObjDocs.outDF[984:991]
control = df2[df2['group'] == 'control'] $ control[control['converted'] == 1].shape[0] / control.shape[0]
df2_treatment = df2.query("group == 'treatment'") $ df_treatment_and_converted = df2_treatment.query("converted == 1") $ p_covert_given_treatment = df_treatment_and_converted.shape[0]/df2_treatment.shape[0] $ p_covert_given_treatment = 100*round(p_covert_given_treatment,5) $ p_covert_given_treatment
days_to_30 = (spencer_bday + thirty_years - today).days $ print("Spencer will be 30 in {} days".format(days_to_30))
df_nd101 = df_pivot[df_pivot['nd_key_formatted'] == 'nd101']
filtered_store_stuff = joined_store_stuff.filter("store_level > 2").sort( joined_store_stuff['count'].desc() ) $ filtered_store_stuff.show()
MultData = Data[(Data['from_account'] != Data['to_account'])]
def get_list_of_coms_hashtags(the_posts): $     list_of_coms_hashtags = [] $     for i in list_Media_ID: $         list_of_coms_hashtags.append(the_posts[i]["comm_tags"]) $     return list_of_coms_hashtags   
df_providers.to_csv('df_providers1.csv')
train.head()
df['nmf_clusters'] = np.argmax(nmf_factors, axis=1) $ feature_hist(df, 'nmf_clusters', title='NMF (n_components=22)', xlabel='Cluster') $ pp_counts(df.nmf_clusters.value_counts(normalize=True), caption='NMF Clusters:', rows=3)
department_df
df.head()
'dcf' in payment_plans_combined.columns
Pipeline = dfDay[['Date', 'Contract Value (Daily)', 'Weighted Value']] $ Pipeline = Pipeline.groupby(['Date'])[['Contract Value (Daily)', 'Weighted Value']].sum().reset_index()
data = pd.DataFrame( $     {'time_step': time_step, $      'valuea': values, $     })
actions.columns = ['BUY=0_SELL=1_HOLD=2'] $ result.join(actions).plot(subplots=True, figsize=(15,10));
df = cs.loc[cs['SUMLEV'] == 50].groupby('STNAME').agg({'CTYNAME' : "count"}) $ df.head()
stock_change.plot(grid = True).axhline(y = 0, color = "black", lw = 2)
treatment_converted = df2[((df2['group'] == 'treatment') &(df2['converted'] == 1))].count()[0] $ treatment_all = df2[((df2['group'] == 'treatment') )].count()[0] $ print('Given that an individual was in the treatment group, the probability they converted is {}'.format(round((treatment_converted/control_all),4))) $ print('\nConverted in treatment is {} '.format(treatment_converted)) $ print('Total number of treatment group {} '.format(treatment_all))
pd.options.display.float_format = '{:.3f}'.format $ np.set_printoptions.float_format = '{:.3f}'.format
np.all(x < 10)
nbr_data =1000
ifcs = []                           # Initialize the list of DataFrames $ for row in cachedf.itertuples():    # Loop over the filenames $     ifcs.append(ifc2df(row))        #   Append a new DataFrame to the list $ ifcdf = pd.concat(ifcs)             # Combine the list of DataFrames into a single DataFrame $ ifcdf
wrd.shape[0]
sample_df['first_genre'].unique()
access_logs_parsed = access_logs_raw.map(parse_apache_log_line).filter(lambda x: x is not None) $
print([0.25,0.5,0.75,1])
'Similarly, militant groups in the Niger Delta attacked Nigeria oil facilities to the point that production fell to roughly half of its former levels at times.' $ 'But Nigeria still expects that its exemption will continue for at least six months, according to oil minister Ibe Kachikwu.' $ 'Libya also has plans to raise output to 1.32 million bpd by the end of the year, up from an earlier target of 1.1 million bpd.')
df.dropna(inplace=True)
testObj.rtn_null = True           ## change error handling: bad records will not simply get blank Address values $ testObj.set_statusMsgGrouping(10) ## get status message about every 10 records (this will be a small test) $ testObj.set_timeDelay(0)          ## remove time delay (this increases risk of errors) $
deaths_Sl_concat['Total_deaths_Sl'] = deaths_Sl_concat['Total_deaths_Sl'].astype(int) $ deaths_Sl_concat.columns = ["date", "death_suspected","country","date1","death_probable","country","date2","death_confirmed","country","Total_deaths_Sl"] $ Total_deaths_Sl =deaths_Sl_concat[['date','Total_deaths_Sl']] $ Total_deaths_Sl.head()
summer[['Mean TemperatureC', 'Precipitationmm']].plot(grid=True, figsize=(10,5))
print(type(full_globe_temp)) $ print(full_globe_temp.dtype) $ print(full_globe_temp.shape) $ print(full_globe_temp.nbytes)
hdf['Age'].groupby(level=0).apply(lambda x: x.max() - x.min()).reset_index(name='diff')
filteredTweets = tweetsIRMA.loc[tweetsIRMA.Distance <= 240]
lst_date_to_set
uber_14["month"].value_counts()
!ls
print ("Discarded records:\n") $ for rec in quarantineArticles: $     print(rec['PMID'], rec['numWords'], rec['diseaseCategory'], rec['FV'])
mydict = r.json()['dataset'] $ dict(list(mydict.items())[0:10])
set(article_word_list)
new_page_converted = np.random.choice([1,0], size=df_newlen, p=[pnew,(1-pnew)]) $
X_train = train.drop('overdue', axis=1) $ y_train = train['overdue']
tw_sample_df.created_at
import pickle
pprint.pprint(treaties.find_one({"reinsurer": "AIG"}))
year_prcp_df = year_prcp_df.sort_values("date") $ year_prcp_df.head(10)
svc = SVC() $ scores = cross_validate(svc, X_tfidf, y_tfidf, cv=10, n_jobs=-1, return_train_score=True) $ print ('The mean of test_accuracy of the model with the default hyperparameter of SVC model:') $ scores['test_score'].mean()
cpi_sdmx.lookup_code('housing','Index')
season12["InorOff"] = "In-Season"
df2[df2['user_id']==773192]
df_new.country.unique()
x_normalized.head()
df[df['converted'] == 1].groupby('user_id').nunique().shape[0] / df.nunique()['user_id']
for dim in d.dimensions: $     print('%s:\t %s'% (dim, d.dimensions[dim]))
df_merge.info()
print("{:.2f} GB".format(df.fileSizeMB.sum() / 1024))
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ r = req.get(url, allow_redirects=True) $ open('image-predictions.tsv', 'wb').write(r.content)
perf_train.head(10)
df_pr['sentiment'] = df_pr['body'].apply(compute_sentiment)
cbg = cbg.to_crs({'init':'epsg:4269'})
ser5.loc["a"]
autos["registration_year"].describe()
conn.commit()
All_tweet_data=pd.merge(twitter_data_v2, tweet_data_v2, on='tweet_id', how='left') $ All_tweet_data.shape
pd.period_range('2005', '2012', freq='A')
sentences = MySentencesFromFile(lee_train_file)
tweet_json.head()
preds, model = runXGB_sklearn(train_X, train_y, test_X,num_rounds=4000) $ out_df = pd.DataFrame(preds) $ out_df.columns = ["low", "medium", "high"] $ out_df["listing_id"] = test_df.listing_id.values $
x_axis1 = [] $ y_axis1 = [] $ for item in output1: $     x_axis1.append(item[0]) $     y_axis1.append(item[1])
ts=df15.groupby('day_of_week').agg({'sale':['sum']}) $ ts
df_NOTCLEAN1A['AGE'].head(20)
df.set_index('id', inplace=True)
prob_group = df2.query("group == 'control'")["converted"].mean() $ print("In the 'control' group the probability they converted is {}.".format(prob_group))
store_items.size - store_items.count().sum()
df_user_engagement = pd.read_csv('takehome_user_engagement.csv')
cycling_data = [10.7, 0, None, 2.4, 15.3, 10.9, 0, None] $ zipped = zip(step_data,cycling_data) $ joined_data = list(zipped) $ activity_df = pd.DataFrame(joined_data) $ print(activity_df)
tweet_hour.loc[1340,'tweet_text'].split()
feeds = json.loads(response) $ print(type(feeds)) $ print(feeds)
print('Sample:') $ print('Polarity:', sampleDF.Polarity.mean()) $ print('Subjectivity:', sampleDF.Subjectivity.mean())
df=pd.read_csv('d:\djohnson\Desktop\ea_trading_rev.csv')
import warnings $ warnings.filterwarnings('ignore') $ import pandas as pd $ import numpy as np
pd.merge(staff_df, student_df, how='left', left_index=True, right_index=True)
df_t1["time_hrs"] = df_delta.apply(lambda td: td.total_seconds() / 3600)
pumashp = gpd.GeoDataFrame.from_file(os.getenv('PUIDATA')+'/'+ $                                      [file for file in os.listdir(os.getenv('PUIDATA')+'/') if 'geo_export' in file and '.shp' in file][0])
df_not_treat_new = df[((df['group'] == 'treatment') & (df['landing_page'] == 'new_page')) == True] $ df_not_cntrl_old = df[((df['group'] == 'control') & (df['landing_page'] == 'old_page')) == True] $ df2 = pd.concat([df_not_treat_new, df_not_cntrl_old]) $
import statsmodels.formula.api as sm $ result = sm.ols(formula="pr_rf ~ mr_rf", data=tbl).fit() $ result.summary()
lsi_tfidf.print_topics(10)
myplot = plot_ensemble_score(means) $ myplot.savefig("AUC.png", dpi=300)
def combine_names(row): $     if row.contributor_fullname.startswith('SEAN PARKER'): $         return 'SEAN PARKER' $     return row.contributor_fullname
actual_diff = df.query("group == 'treatment'").converted.mean() - df.query("group == 'control'").converted.mean() $ p_diffs = np.array(p_diffs) $ (actual_diff < p_diffs).mean()
holiday_model = Prophet(holidays=holidays_df,yearly_seasonality =True,weekly_seasonality= True,daily_seasonality = True) $ holiday_model.fit(df3_holidays); $ holiday_future = holiday_model.make_future_dataframe(periods= 6, freq = "m") $ holiday_forecast = holiday_model.predict(holiday_future) $
to_be_predicted_Day4 = 56.18021778 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
len(labeled['name_feat'].unique()) / len(labeled['name_feat'])
set(stories.columns) - set(stories.dropna(thresh=9, axis=1).columns) $
poly2 = PolynomialFeatures(degree=10)
events = pd.read_table('events.gz', sep=',', engine='c')
appleinbounds.head(10)
learner.fit(3e-3, 1, wds=1e-6, cycle_len=20, cycle_save_name='adam3_20')
rf_v1.varimp(use_pandas=True)
f_lr_hash_modeling2 = f_lr_hash_modeling2.withColumn('id', col('id').cast('long')) $ f_lr_hash_test = f_lr_hash_test.withColumn('id', col('id').cast('long'))
store_items.dropna(axis=0)
weekday= weekday.groupby([weekday.index.hour]).mean() $ weekend= weekend.groupby([weekend.index.hour]).mean()
unique_domains.query('mentions >= 10').sort_values('avg_payout', ascending=False).head()
pd.to_datetime(['04-01-2012 10:00'])
top_tracks
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df.set_index('user_id'), how='inner') $ df_new.head()
cohort_churned_df.to_csv('churned_cohorts_518.csv')
order_item = pd.read_csv('../data_clean/order_item_merge.csv') $ order_item.head()
session = Session(engine)
transactions.info()
RNPA_trend = fit_linear_trend(RN_PA_duration) $ RNPA_detrended = RN_PA_duration - RNPA_trend
df['Week']=pd.DatetimeIndex(df['Lpep_dropoff_datetime']).week $ df['Week']=df.Week.astype('category') $ df['Week'].describe()
word_list = [] $ for post in post_list: $     for word in post: $         word_list.append(word)
df[df['Descriptor'] == 'Pothole'].index.weekday.value_counts()
s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])
rodelar_pages = pagegenerators.CategorizedPageGenerator(category)
op_after = op_after.loc[at_snapshot].reset_index() $ op_a2 = op_after.loc[op_after.groupby(['id_container', 'time_in'])['time_move'].idxmin()] $ op_a2.head()
train_holiday_oil_store_transaction_item_test_004 = train_holiday_oil_store_transaction_item_test_004.join(stores_df, 'store_nbr', 'left_outer') $
1965+1928
pm_data.isnull().sum(axis=0)
get_data(DATA_DIR, aru_url, aru_file)
sns.distplot(dfz.retweet_count, color = 'blue', label = 'Retweets')
dates = pd.date_range('2018-03-22',periods=ndays).astype('str') $ sim_ret = pd.DataFrame(sigma*np.random.randn(ndays,nscen)+mu,index=dates) $ sim_closes = (closes_aapl.iloc[-1].AAPL)*np.exp(sim_ret.cumsum())
model.get_config()
def number_ocurrences(x): $     airlines = ["aireuropa", "iberia", "jetblue", "norwegian", "ryanair", "spanair", "vueling"] $     for airline in airlines: $         print(airline + ": "+ str((len(df[df["text_3"].str.contains(airline, case = False)])))) $ number_ocurrences(df)
rng[5]
obs_diff = cr_treatment - cr_control $ plt.hist(p_diffs); $ plt.axvline(x=obs_diff, color='red');
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
r.json() 
df['state'] = df['state'].str.capitalize() $ df.groupby('state')['ID'].count()
df_all['US'] = df_all['country'].replace(('US', 'UK', 'CA'), (1, 0, 0)) $ df_all['UK'] = df_all['country'].replace(('US', 'UK', 'CA'), (0, 1, 0)) $ df_all['CA'] = df_all['country'].replace(('US', 'UK', 'CA'), (0, 0, 1))
df_lm.filter(regex='tch_attempt|last_month').boxplot(by='last_month', figsize=(10,10),showfliers=False)
ia = imdb.IMDb() $ response_imdb = ia.search_movie('Lost in translation') $ response_imdb[0] $ for s in response_imdb: $     print (s['title'], s.movieID)
df.sort_values(by="grade")
dates = list(map(parser.parse, dates))
test = single_patient[single_patient.field_stream == 'gsr'] $ test.reset_index(drop=True, inplace=True) $ test.set_index('parsed_ts', inplace=True) $ test = test.cumsum()
df_arch_clean.loc[df_arch_clean.tweet_id== '681340665377193984', 'text'] $
e_p_b_two.TimeCreate = e_p_b_two.TimeCreate.apply(lambda x:x.date())
i = 1125 $ 'Real: {:.3f}, Predicted: {:.3f}'.format(overdue_duration[i], rfr.predict(features_regress_vect[i])[0])
display(flight2.show(5)) $
finals[finals.PLAYER_NAME.str.contains("Durant")]
data.head()
type(k)
hemispheres_url = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars" $ browser.visit(hemispheres_url)
ldamodel = LdaModel(corpus=corpus, num_topics=10, id2word=dictionary)
y_pred = lr.predict(X_test)
tweet_scores.head()
stocks_ts = pd.read_sql("select distinct(ticker) from daily_price where instrument_type='stock'", engine) $
git_log.timestamp.dt.hour.head()
autos[(autos.registration_year > 2016)|(autos.registration_year < 1900)].shape
df_clean.source = df_clean.source.str.extract(pat = pattern)
len(df2.query('converted == 1')) / len(df2)
shift_entries.head(10)[['SE_ID', 'SE_LOCATION', 'SE_START', 'SE_TIMESTAMP','TimeElapsed', 'SE_SHIFTACTIVITY']]
df1=pd.read_csv('countries.csv', header=0) $ df1.head() $ df3=df2.set_index('user_id').join(df1.set_index('user_id')) $ df3.head()
print("Data Types: \n%s" % (excel_data.dtypes))
print autodf.kilometer.count() $ print max(autodf.kilometer) $ print min(autodf.kilometer) $ sns.boxplot(autodf.kilometer)
del train_data $ del val_data $ gc.collect()
car_brands = autos['brand'].value_counts(normalize=True).index
dbname = 'CHURN' $ username = 'deniz'
my_gempro.set_representative_structure() $ my_gempro.df_representative_structures.head()
temp_df = temp_df.reset_index()[['titles', 'timestamp']]
inputNetwork.save_edges(edges_file_name='edges.h5', edge_types_file_name='edge_types.csv', output_dir=directory_name)
age[1]
X_train,X_test,y_train,y_test = train_test_split(X, y, test_size = .33, random_state= 7779311)
mig_l12.agg(np.mean)
df1
localized.asi8[0]
X_df = pd.DataFrame(X, columns=['sepal len', 'petal len', $                                   'sepal width', 'petal width']) $ X_df.head()
vlc[pd.notnull(vlc.bio.str.strip())].id.size
fish.index
ebola_melt['country'] = ebola_melt.str_split.str.get(1) $ ebola_melt.head()
training_X_scaled = scaler_M7.transform(training_X) $ holdout_X_scaled = scaler_M7.transform(holdout_X) $ len(training_X_scaled),len(holdout_X_scaled)
dict2017 = {} $ for data in data2017: $     date = data[0] $     dict2017[date] = data[1:] 
full_data.head()
BOM_movie_list = movie_df.title.values.tolist()
new_page_converted = np.random.binomial(1, p_new, n_new)
import pandas as pd $ df = pd.read_csv('/u/username/data/iris.csv') $ df.head()
PTrueNegative + PFalseNegative
causes = data['Cause'].value_counts() $ causes = pd.DataFrame(data['Cause'].value_counts()) $ causes.plot(kind='bar') $ plt.show()
z1=match_id(net_loans_exclude_US_outstanding,net_loans_exclude_US_outstanding.Beschreibung==u'R\xfcckgabe vom Inkassounternehmen/von einer internen Inkassostelle an den Gl\xe4ubiger') $ z1
n_customers = data['customer_id'].unique().size
df = df.loc[(df['retweeted_status'].isnull()) & (df['lang'] == "en")]
df.drop(['created_at', 'updated_at'], axis=1).describe(exclude = np.number)
for df in (joined, joined_test): $   df['Promo2Since']=pd.to_datetime(df.apply(lambda x: Week(x['Promo2SinceYear'],x['Promo2SinceWeek']).monday(),axis=1).astype(pd.datetime)) $   df['Promo2Days']=df['Date'].subtract(df['Promo2Since']).dt.days
train_users
May = df['2015-05'] $ May['Complaint Type'].value_counts().head(5)
all_enterprise = lift.get_all_enterprise()
for_plaintiff = like_plaintiff > like_defendant $ for_plaintiff.sum(), for_plaintiff.count()
train_df.head(1)
df[df['price'] < 10].head(1)
data = pd.DataFrame(data_dict) $ data.head()
urls["domain"].value_counts()
df_new['country'].value_counts() $ df_new[["CA","US"]] = pd.get_dummies(df_new['country'])[["CA","US"]] $ df_new.head()
print(classification_report(y_test, y_pred))
device_browser = train_data.groupby(['device.browser']).agg({'totals.transactionRevenue': 'sum'}).reset_index() $ device_browser = device_browser.assign(pct = (device_browser["totals.transactionRevenue"]/device_browser["totals.transactionRevenue"].sum())) $ device_browser.set_index("device.browser",drop=True)["pct"].plot.bar()
np.exp(0.0506) #UK $
pd.date_range('2015-07-03', '2015-07-10')
sumToRG = toRG.sum()
df.head()
conditions
model.train(x=mtcars_filtered.col_names, training_frame=mtcars_filtered)
ABT_tip = pd.merge(reviewsDFslice, tips_sentiment, how = 'left', on = ('business_id'))
autos['price'].value_counts().sort_index(ascending=True).head(10)
print(90*'_') $ print("\nCount of players in each cluster") $ print(90*'_') $ pd.value_counts(model.labels_, sort=False)
order
vc.plot(kind='bar', x='date', y='count')
shortcodes = [] $ for i in range(35): $     shortcodes.append(df[0:40]['link_to_post'][i][-11:]) $ shortcodes[0] $ len(shortcodes)
df_enhanced.info()
data.shape, test.shape
tweet_df_raw = pd.read_json('tweet_json.txt', lines = 'True') $ tweet_df = tweet_df_raw[['id', 'retweet_count', 'favorite_count']]
df1_clean.sample(1)
rodelar.registration().strftime("%Y-%m-%d")
test_clean["Month"] = test_clean["DateTime"].apply(month_slicer) $ list_of_test_features.append(("Month", test_clean["Month"]))
from urllib import FancyURLopener $ class MyOpener(FancyURLopener): $     version = 'My new User-Agent' $ MyOpener.version
segments.dtypes
releases.info()
df.resample('D').mean()
archive.info()
auto.Price.mean()
final_feat = telemetry_feat.merge(error_count, on=['datetime', 'machineID'], how='left') $ final_feat = final_feat.merge(comp_rep, on=['datetime', 'machineID'], how='left') $ final_feat = final_feat.merge(machines, on=['machineID'], how='left') $ print(final_feat.head()) $ final_feat.describe()
E_grid = np.zeros((100, 100)) $ for i in range(100): $     for j in range(100): $         E_grid[i, j] = ((y - m_grid[i, j]*x - c_grid[i, j])**2).sum()
converted_controlusers2 = float(df2.query('converted == 1 and group == "treatment"')['user_id'].nunique()) $ treat_users2 =float(df2.query('group == "treatment"')['user_id'].nunique()) $ tp2 = converted_controlusers2 /treat_users2 $ print(" Given that an individual was in the treatment group, the probability they converted is {0:.2%}".format(tp2))
tweets_df.in_reply_to_screen_name.value_counts().plot()  # alseyaseyah has people reply more than other newspapers but retweets less than other people $ plt.title("reply")                                                       # the cause may be that the people is replying with somthing they didn't like about the tweet 
df_episodes.id.nunique(), df.episode_id.nunique(), len(df_episodes[~df_episodes.id.isin(df.episode_id)])
df = df.sort_values(by=['seq_id','work_day'],axis=0,ascending=[True, True]) $ df.to_pickle(pretrain_data_dir+'/pretrain_data_03.pkl')
con=psycopg2.connect(dbname= 'youcaring', $                      host='youcaring.XXX.us-west-1.redshift.amazonaws.com', $                      port= '5439', user= 'youcaring', password= 'YYY') $ con.set_client_encoding('UTF8')
ids = df2['user_id'] $ df2[ids.isin(ids[ids.duplicated()])]
clean_rates.source.value_counts()
endpoint_published_models = json.loads(response_get_instance.text).get('entity').get('published_models').get('url') $ print(endpoint_published_models) $
doc_duration.shape, RN_PA_duration.shape, therapist_duration.shape
people.loc["charles"]
red_4.drop(['created_utc', 'time fetched'], axis = 1, inplace = True)
reddit.head() #Yes! It worked.
theft = crimes[theft_bool] $ theft.head()
wk_output
df.head()
sorted(ltm_0to3000.keys())
df_trn.text.str.len().mean(), df_test.text.str.len().mean()
digits.data
print("Number of tweets: %i" % num_tweets) $ print("Number of retweets: %i" % num_retweets) $ print("Number of mentions extracted from tweets: %i" % len(mentions_df))
Fraud_Data.groupby('country')['class'].mean().sort_values(ascending=False)[:10]
np.vstack((a, a))[0:6:2,:] # by steps of 2.
df.loc['2018-05-21',['Open','Volume']]
MergeMonth['Date'] = MergeMonth['Date'].dt.to_period("M")
y_pred = classifier.predict( ... ) $ report = sklearn.metrics.classification_report( ... , ... ) $ print(report)
def split2(elem): $     elem = elem.replace('POINT (', '') $     elem = elem.replace(')', '') $     return elem.split(' ')[1]
for post in got_data.head(10)['id']: $     print("https://facebook.com/"+post)
(~autos["price"].between(200,155000)).sum() / autos.shape[0] #In this code ~ invert operator selects $
log_mod = sm.Logit(df3.converted,df3[['intercept','country_UK','country_US']]) $ result_2 = log_mod.fit() $ result_2.summary()
n_old = df2[df2['landing_page']=='old_page']['user_id'].nunique() $ n_old 
df.dtypes
rdf_clf = RandomForestClassifier(n_estimators = 20,criterion = 'gini', max_features='log2')
s.values?
spark_df = hc.createDataFrame(table) $ spark_df.write.saveAsTable('tweet_table')
compound_sub1 = compound_wdate_df0.append(compound_wdate_df1)
cpi_sdmx.names
df.dtypes
transactions.merge(users, how='left', on=['UserID'])
pol_users.head()
vis = points.copy() $ vis.set_index(["Team"], inplace = True) $ odds = vis["Wins"].plot(kind = "bar", figsize = (25, 5), title = "Number of wins in the world cup", rot = 90, legend = True) $ odds.set_ylabel("Number of Wins", fontsize = 15) $ plt.show()
plt.show()
conn_helloDB = create_engine("mysql+pymysql://{user}:{pw}@localhost".format(user="few", pw="123456")) $
from sklearn.ensemble import VotingClassifier
import statsmodels.api as sm $ convert_old = ((df2.query('landing_page == "old_page"')).query('converted == 1')).count()[0] $ convert_new = ((df2.query('landing_page == "new_page"')).query('converted == 1')).count()[0] $ n_old = df2.query('landing_page == "old_page"').count()[0] $ n_new = df2.query('landing_page == "new_page"').count()[0]
users = users.query("created_at > '1984-01-01 00:00:00'") #removing those 2 outliers
print(pd.value_counts(train_df['os'])[:20])
Merge.head()
chinadata.dropna(inplace = True)
!cat p32_input.txt | python mapper3.2.A.py
clean_colnames = [clean_string(colname) for colname in df_oncstage_dummies.columns] $ df_oncstage_dummies.columns = clean_colnames
plt.hist(p_diffs, color='g') $ plt.title('\n Simulated differences for conversion rates \n', fontsize=16) $ plt.axvline(x = obs_diff, color='red');
training_df = features[~features.gameId.isin(production_df.gameId)]
percipitation_measurement_df.describe()
with tb.open_file(filename='data/my_pytables_file.h5', mode='a') as f: $     current_mark = f.get_current_mark() $ current_mark
water_body = wb $ temp_df = w.get_filtered_data(step = 2, subset = subset_uuid, indicator = 'din_winter', type_area = type_area)[['SDATE','MONTH', 'WATER_BODY_NAME', 'VISS_EU_CD', 'WATER_TYPE_AREA', 'DIN','SALT_CTD', 'SALT_BTL']].dropna(thresh=7) $ print('Waterbodys left: {}'.format(temp_df['WATER_BODY_NAME'].unique())) $
train.groupby("hour")["any_spot"].mean()
df2.groupby(df2['group']=='control')['converted'].mean()[1]
train.head()
y_test_over[fm_bet_over].mean()
aggregate = bow1.union(bow2,generalterms) $ score1 = genscorematrix(bow1,1) $ score2 = genscorematrix(bow2,1) $ scoreagg = genscorematrix(aggregate,0.66)
likes = likes.drop(['timestamp', 'actor', 'title'],axis=1) $ likes.head(10)
pipeline.fit(X_train, y_train)
print(nc_file) $ nc_file.close(); print('Dataset is closed!')
twitter_archive=pd.read_csv('twitter-archive-enhanced.csv') $ tweet_id_all=twitter_archive['tweet_id']
plt.plot(aapl)  # plots all col at once $ plt.show()
import statsmodels.api as sm $ convert_old = df2.query('group=="control"')['converted'].sum() $ convert_new = df2.query('group=="treatment"')['converted'].sum() $ n_old = df2.query('group=="control"').shape[0] $ n_new = df2.query('group=="treatment"').shape[0]
authors_per_file[authors_per_file == 1].count()
count_np_not_treat = df.query('landing_page == "new_page" & group != "treatment"') $ count_op_not_ctrl = df.query('landing_page == "old_page" & group != "control"') $ count_np_not_treat.shape[0] + count_op_not_ctrl.shape[0]
plot_data(df.query('amount > 5000'))
new_page_converted = np.random.binomial(1,p_new,n_new) $ new_page_converted.mean()
result.acknowledged
df_new = df_new.drop('CA', axis=1)
train_data.loc[(train_data.gearbox == 'automatik') & (train_data.vehicleType.isnull()), 'vehicleType'] = 'limousine'
multiple_party_votes_all.drop(columns = ['index', 'votes_hour', 'time', 'district', 'date'], inplace = True)
np.unique(train_small_sample.click_timeDay), np.unique(train_small_sample.click_timeHour)
files8= files8.drop('EndDate',axis=1) $ files8= files8.drop('StartDate',axis=1) $ files8.head() $
IMDB_df = train_df.append(test_df)
conv_prob_treatment = df2[df2.group == 'treatment'].converted.mean() $ print('Given that an individual was in the treatment group, the probability they converted is {}.'.format(conv_prob_treatment))
nfea = Xtr_scale.shape[1]
n_estimator = 100 $ rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator, n_jobs=3, verbose=2) $ rf.fit(X_train, y_train) $
pumashplc.head()
s4.unique()
d + BMonthEnd()
d + pd.tseries.offsets.Week(normalize=True)
texas_city.head()
msft = pdr.get_data_yahoo('MSFT', $                           start=datetime.datetime(2014, 1, 1), $                           end=datetime.datetime(2017, 3, 1))
constructor.sort_values(by='mileage',ascending=False)
assembler = VectorAssembler(inputCols = feature_col, outputCol = "features") $ assembled = assembler.transform(ibm_train)
print 'First observation period :', min(cpi_all['Time'].cat.categories.tolist()) $ print 'Last observation period:', max(cpi_all['Time'].cat.categories.tolist())
df_ab_raw.user_id.nunique()
combined_df5 = combined_df4.copy() $ threshold=20 $ counts = combined_df5['llpg_usage'].value_counts() $ repl = counts[counts <= threshold].index $ combined_df5['llpg_usage']=combined_df5['llpg_usage'].replace(repl, 'Other') $
mod_model.scen2xls(version=None)
consumer_key = 'CONSUMER KEY' $ consumer_secret = 'CONSUMER SECRET' $ access_token = 'ACCESS TOKEN' $ access_secret = 'ACCESS SECRET'
sakhalin_shp.bbox
tfms = tfms_from_model(arch, sz, aug_tfms=transforms_side_on, max_zoom=1.1)
print(df['Confidence'].nunique())
obs_diff = np.mean(df.query('group == "treatment"')['converted']) - np.mean(df.query('group == "control"')['converted']) $ obs_diff
ebay.index
%run -i common/load_data.py $ %run -i common/mape.py $ %run -i common/TimeSeriesTensor.py $ %run -i common/create_evaluation_df.py
print(r.json()['dataset_data']['column_names']) $ print(r.json()['dataset_data']['data'][0]) $ print(r.json()['dataset_data']['data'][-1])
no_test_df = df[df["dataset"]=="train"] #.drop_duplicates(subset="text") actually can't do this w out changing vocab $ trn_df, val_df = sklearn.model_selection.train_test_split(no_test_df, test_size=0.1) $ len(no_test_df), len(df), len(trn_df), len(val_df)
print('Training data shape: ', train.shape) $ print('Testing data shape: ', test.shape)
dream = str(dreamSoup.body).split('span id="dreamContent">')[1].split('</span>')[0] $ user = re.search("phone - (.*)'s", dreamSoup.title.text).group(1) $ daysPassed = re.search(r'(\d+) days ago',str(dreamSoup.body)).group(1) $
from sklearn import preprocessing $ min_max_scaler = preprocessing.MinMaxScaler()
places = api.geo_search(query="Edinburgh", granularity="city") $ place_id_E = places[0].id $ print('Edinburgh id is: ',place_id_E)
df2_c = df2.query('group == "control"') $ df2_c.query('converted == 1').user_id.count()/df2_c.user_id.count()
df.loc['2018-05-30'::2,['Open','Volume']]   # What is that we are trying to do here?
print(summarize(gen2str, word_count=200))
views_data_clean.to_csv('data/views_data_clean.csv',index=False)
data = pd.Series([1, np.nan, 2, None, 3], index=list('abcde')) $ data
df \ $     .filter(df.name == 'Samwise')\ $     .take(1)
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs),func.avg(Measurement.tobs)).filter(Measurement.station=="USC00519281").all()
bonus_points
df.set_index("ZIPCODE", inplace=True)
df.groupby('A').sum()
hc.sql('drop table if exists asm_wspace.final_400hz_2017q4')
result[1][1]['coordinates']
hmeq = cassession.CASTable('hmeq') $ hmeq.head() $
flight_delays["ARR_DATETIME"] = pd.DatetimeIndex(flight_delays.ARR_DATETIME) $ flight_delays.set_index("ARR_DATETIME", inplace=True) $ flight_delays = flight_delays.drop([pd.Timestamp('2014-03-09 02:00:00'), pd.Timestamp('2014-11-02 01:00:00'), pd.Timestamp('2015-03-08 02:00:00'), pd.Timestamp('2015-11-01 01:00:00'), pd.Timestamp('2016-03-13 02:00:00'), pd.Timestamp('2016-11-06 01:00:00'), pd.Timestamp('2017-03-12 02:00:00')]) $ flight_delays.index = flight_delays.index.tz_localize('America/New_York')
sq_index = df.query('full_sq<life_sq').index
df2[df2.user_id == dup_id[0]]
%%time $ df['created_at']= pd.to_datetime(df['Created Date'], format="%m/%d/%Y %X %p")
train_embedding=graf_train['DETAILS3'].apply(lambda words: np.mean([model[w] for w in words if w in model], axis=0))
test.isnull().sum()
comments = pd.read_csv('data/comments.csv', index_col = 0, parse_dates = [2]) $ comments.columns
all_texts = [ $     normalize_links(msg['text']) $     for msg in ods.messages $     if msg['type'] == 'message' and msg.get('subtype') is None $ ]
logs_df.show(truncate=True) $ print(total_log_entries) $ logs_df.cache()
complaints2016_geodf = gp.GeoDataFrame(complaints2016_filtered_cat_wlocations)
cityID = 'b046074b1030a44d' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Jersey_City.append(tweet) 
plt.hist(reddit['Comments'], range=(0,150)) $ plt.xlabel('Number of Comments',fontsize='large') $ plt.ylabel('Number of Reddit Posts',fontsize='large') $ plt.title('About half are above or below the number of comments!', fontsize='large') $ plt.show()
print(d.headers)
tweets.describe(include='all')
glimpse(selected_leases)
len(non_rle_pscs[non_rle_pscs.secret_base == True])
sns.distplot(answers_scores[:int(len(answers_scores)*0.95)])
df_merged.boxplot(column='rating_numerator');
ds_complete_temp_CTD_1988 = xr.Dataset.from_dataframe(df_complete_temp_CTD_1988)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ (z_score, p_value)
moviesRdd = sc.textFile("movielens/movies.csv") $ moviesRdd.take(10)
sentiment = stock['tesla_tweet'].apply(sia.polarity_scores) $ sent = pd.DataFrame(list(sentiment)) $ sent.index = stock.index $ sent.columns = ['telsa_compound', 'tesla_neg', 'tesla_neu', 'tesla_pos'] $ stock = pd.merge(stock, sent, how='left', left_index=True, right_index=True)
lmdict = pd.read_excel('https://www3.nd.edu/~mcdonald/Word_Lists_files/LoughranMcDonald_MasterDictionary_2014.xlsx')
people.dtypes
lr = LinearRegression()
%%time $ gc_conn = psycopg2.connect(user='postgres', password='Xypherium-0', $                         dbname='jpstat', $                         host='35.224.240.50') $ print(gc_conn.closed) #0 when open
df['edition_type']=df['edition'].map(lambda x: 'National' if x=='National' else 'Media' if x=='PunditFact' else 'Global' if x=='Global News Service' else 'State')
d
res = df_exp.join(dfpf, dfpf.rf == df_exp.rf)
green_line = data[:training_split_cut] $ purple_line = [] $ for iter_x in np.arange(training_split_cut): $     purple_line.append(np.nan) $ purple_line = purple_line + list(X_test_predict_i[:,0])    
data=customer.join(customer_churn,customer['ID']==customer_churn['ID']).select(customer['*'],customer_churn['CHURN'])
df_weather.head()
shown = pd.DataFrame(data.tasker_id.value_counts()) $ shown.loc[shown['tasker_id']==1]
print(ser5[['a','c','d','e']])
number_of_rows = df.shape[0] $ number_of_rows
df_2012.dropna(inplace=True) $ df_2012
yt.get_subscriptions(channel_id, key)
df2[df2.duplicated(subset='user_id')==True]
datafile = "2013_ERCOT_Hourly_Load_Data.xls"
print (Ralston.TMAX.mean(), Ralston.TMAXc.mean())
soup.ul.contents
scaler = StandardScaler() $ X_train_scaled = scaler.fit_transform(X_train.astype(np.float32)) $ X_test_scaled = scaler.transform(X_test.astype(np.float32))
alphas = gb.sum().values                 # number of successes $ betas = (gb.count() - gb.sum()).values   # failures == total - successes $ beta_dist = np.empty(shape=(mcmc_iters, n_bandits))   # matrix to contain simulated Beta distributions $ for n in range(n_bandits): $     beta_dist[:, n] = np.random.beta(np.asscalar(alphas[n]), np.asscalar(betas[n]), size=(mcmc_iters))
df = pd.DataFrame() $ df = df.append(pd.read_csv('rorypul_tweets.csv')) $ df
metrics.recall_score(testy,predict_y)
df_master=pd.read_csv('cleaned data/final_master.csv')
my_person.firstname
tweets_rt = pd.DataFrame(ndarray_rt)
plt.xlabel('Votes') $ plt.ylabel('Number of Movies') $ plt.title('Distribution of Votes') $ exp_budget_vote.hist(histtype = 'stepfilled', label = 'Rates of the Most Expensive Movies')
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
my_columns = list(data.columns) $ my_columns
columns = ['SchoolHoliday', 'StateHoliday', 'Promo']
tweet_image_clean.head()
df2.head()
df.shape $
df_batch6_top = df_batch6_top[df_batch6_top['Comments'] != '[removed]'] $ df_batch6_top = df_batch6_top[df_batch6_top['Comments'] != '[deleted]']
yc_new3 = yc_new2[yc_new2.tipPC < 100]
saved_model = ml_repository_client.models.save(model_artifact)
dates = most['created_date'].tolist()
!pip install -U -q kaggle $ !mkdir -p ~/.kaggle
df["updated_at"].dt.date.value_counts().head(10)
sox = sox[['game_id','season','date','day_of_week','time','hour','minute','late','opponent','playoff']]
import urllib.request $ import json $
results = log_mod.fit() $ results.summary()
image_clean = image_preview.copy() $ image_clean[['p1']] = image_clean[['p1', 'p1_conf', 'p1_dog']].astype(str).apply(lambda x: '|'.join(x), axis=1) $ image_clean[['p2']] = image_clean[['p2', 'p2_conf', 'p2_dog']].astype(str).apply(lambda x: '|'.join(x), axis=1) $ image_clean[['p3']] = image_clean[['p3', 'p3_conf', 'p3_dog']].astype(str).apply(lambda x: '|'.join(x), axis=1) $ image_clean = image_clean.drop(['p1_conf', 'p1_dog','p2_conf', 'p2_dog', 'p3_conf', 'p3_dog' ],axis=1)
visual_df.index.name = 'team'
cityID = '73d1c1c11b675932' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chesapeake.append(tweet) 
group_key
max(change_high_low_list)
results.summary()
vectorized_text_predict = vectorizer.transform( ... ) $ vectorized_text_predict.toarray()
MostActiveStationId = "USC00519281" $ Active_TempObs = session.query(Measurement.date, Measurement.tobs).order_by(Measurement.id.desc()). \ $     filter(Measurement.station == MostActiveStationId).limit(365).all() $ Active_TempObs
donors[donors['Donor Zip'].isnull()].count()
check_int('2017-01-17')[check_int('2017-01-17').number_repeat == 4999]
ce = CategoricalEncoder(encoding='onehot-dense',handle_unknown='ignore')
def splitcol(s): $     s = ''.join([c for c in s if c not in exclude]) # Removes punctuation $     s = [x for x in s.split(" ") if x != ""] # Accounts for double whitespace $     return s
gbm_perf = gbm_model.model_performance(valid = True) $ gbm_perf.plot()
print(questions.shape) $ print(questions.dtypes) $ print(questions.head(5))
sampleDF.info()
print(df.index, '\n') $ print(df.columns, '\n') $ print(df.values,'\n')  #as numpy array $ print(df.values.shape)
for id,row in tweets.loc[tweets.snspostid.isin(least_retweeted.parentPost.astype(np.str).values)].iterrows(): $     print(row.text) $     print('')
%matplotlib inline
r.loc[0, 'test-result-id']
run txt2pdf.py -o"2018-06-14 2148 FLORIDA HOSPITAL Sorted by Discharges.pdf"  "2018-06-14 2148 FLORIDA HOSPITAL Sorted by Discharges.txt"
raw_full_df = raw_full_df[~bad_bathrooms]
game_info.head(3)
trn_y = train_small_sample.is_attributed $ val_y = val_small_sample.is_attributed
data = data.withColumnRenamed("Est Income", "EstIncome").withColumnRenamed("Car Owner","CarOwner") $ data.toPandas().head()
amts = amts.rename(columns={'LOAN_AMOUNT':'LOAN_AMOUNT_'}) $ extract_deduped_with_elms_v2 = extract_deduped_with_elms.merge(amts, how='left', on='ACCOUNT_ID') $ extract_deduped_with_elms_v2['LOAN_AMOUNT'] = [lamt if str(lamt) not in ['nan','(null)',''] else lamt_ $                                                for (lamt, lamt_) in zip(extract_deduped_with_elms_v2.LOAN_AMOUNT, $                                                                         extract_deduped_with_elms_v2.LOAN_AMOUNT_)]
data_ps['SA'] = np.array([ analize_sentiment(tweet) for tweet in data_ps['Tweets'] ]) $ display(data_ps.head)
prec_group.index = pd.DatetimeIndex(prec_group.index) $ prec_group = prec_group.drop([pd.Timestamp('2014-11-02 01:00:00'), pd.Timestamp('2015-11-01 01:00:00'), pd.Timestamp('2016-11-06 01:00:00')]) $ prec_group.index = prec_group.index.tz_localize('America/New_York')
fulldf.info()
for filename in ("my_df.csv", "my_df.html", "my_df.json"): $     print("#", filename) $     with open(filename, "rt") as f: $         print(f.read()) $         print() $
df['Agency Name'].value_counts()
print("Startzeit:", df['Timestamp'].head(1)[0], $      "Endzeit:",str(df['Timestamp'].tail(1))[:26][7:])
df2.groupby(['group'],as_index=False).mean()
train.StateHoliday = train.StateHoliday!='0' $ test.StateHoliday = test.StateHoliday!='0'
old_page_converted = np.random.binomial(1, p_old, n_old) $ old_page_converted
df2_control = df2.query('group == "control"') $ control_prob = df2_control.converted.mean() $ print(control_prob)
classes = {0:'negative', 1:'neutral', 2:'positive'}
spark_df = context.createDataFrame(pandas_df)
plt.figure() $ plt.hist(word_count, bins = 100)
uvws = [[ 1, 1, 0], $         [-1, 1, 0], $         [ 0, 0, 1]] $ system = system.rotate(uvws) $ print(system)
new_df = keep_good_tweets(raw_df, search_category)
tweets['rating_denominator'].nunique() $
df[df['yearOfRegistration'] > 2016].head(1) $
clean_rates.shape
lr_scheduler_col = lr_sched.myCosineAnnealingLR(optimizer_col,879,cycle_mult=1.0) $ train_col.train_model(num_epochs=3,scheduler=lr_scheduler_col)
df[3] = np.nan $ df
304-20
new_page_converted = np.random.choice([0, 1], size=n_new, p=[(1-p_new), p_new]) $ (new_page_converted == 1).sum()
cvec.fit(reddit['title']) $ title_tokens = pd.DataFrame(cvec.transform(reddit['title']).todense(), $              columns=cvec.get_feature_names()) $ title_tokens.shape
tweet_df.describe()
Quantile_95_disc_times_pay = df_ind_site.groupby(['id_num','year']).quantile([0.1,0.9]) $ Quantile_95_disc_times_pay.head(10)
stack_score.mean()
attend_with = questions['attend_with'].str.get_dummies(sep="'")
reddit['Titles'].nunique() #how many unique titles do we have? 
autos['registration_year'].value_counts().sort_index(ascending=True).head(20)
new_texas_city["Measurement_date"] = pd.to_datetime(new_texas_city["Measurement_date"])
dftouse_four = flattened_df[flattened_df['star_avg'] <= 4] $ dftouse_seven = flattened_df[flattened_df['star_avg'] >= 7]
df.to_pickle("dfSentences.p")
bikedataframe = bikedataframe.loc[bikedataframe['count'] > 0] $ bikedataframe.shape
from scipy.stats import f_oneway
minute_range = bars.high - bars.low $ minute_range.describe()
train['total_docks'] = total_docks
len(test5result.columns)
lasso.score(X_test,Y_test)
bacteria[bacteria>1000]
prec_group.head()
nypd_df['create_date_time'] = pd.to_datetime(nypd_df['Created Date']) $ nypd_df['closed_date_time'] = pd.to_datetime(nypd_df['Closed Date'])
df[df.is_oc_company == True]
crime_tweek_tweek_window_, stops_two_week_window_ = stops_vs_crime(zip_2_df,zip_2_sns,'14D','14D') $ crime_week_tweek_window_, stops_two_week_window_ = stops_vs_crime(zip_2_df,zip_2_sns,'7D','14D') $ crime_day_tweek_window_, stops_two_week_window_ = stops_vs_crime(zip_2_df,zip_2_sns,'D','14D')
df_change_count.sort_values(by='Count', ascending=False).head(10)
for columns in DummyDataframe: $     basic_plot_generator(columns, "Graphing Dummy Data" ,DummyDataframe.index, DummyDataframe)
np.round(df['converted'].mean()*100)
databreach_2017.head()
df = df.drop_duplicates(subset="tweet") $ df
open_yearly = open_prices.groupby(open_prices.index.year).mean() $ open_yearly
pdiff = new_page_converted.mean() - old_page_converted.mean() $ pdiff
try: $     pd.concat([x, y], verify_integrity=True) $ except ValueError as e: $     print('ValueError:', e)
[(s.id, s.user.name, s.user.screen_name) for s in retweets]
get_data(DATA_DIR, cama_url, cama_file)
articles.to_csv('super.json')
test_df['loan_status']= test_df['loan_status'].apply(lambda x: 0 if (x == "PAIDOFF")  else 1) $ test_y = test_df['loan_status'].values $ test_y[0:5]
uu = np.array([[1,0,1],[0,0,1]], dtype='float32')
first_result.find('strong').text[0:-1]
images_copy = images_copy[pd.notnull(images_copy['jpg_url'])] #without pics $ images_copy.info() $
yc_new1.rename(columns={'incomePC':'income_dest'}, inplace=True) $ yc_new1.columns
merged2["tick"] = 9.16 $ est_tick = (merged2.opening // merged2.tick).astype("int64") $ merged2.insert(loc=3, column='est_tick', value=est_tick) $ merged2 = merged2.rename(columns={"opening":"op_gross"})
destLangToLangMap = defaultdict(str) $ destLangtoLangMap = { 'eng':'en', 'deu':'de', 'spa':'es', 'fra':'fr', 'ita':'it', 'por':'pt', 'nld':'nl' }
support = merged[merged.committee_position == "SUPPORT"]
trans_data.head()
df_total.head()
props
to_be_predicted_Day4 = 38.494209 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
fb = pd.read_json(json.dumps(res_json['posts']['data']))
ml_20m_url = 'http://files.grouplens.org/datasets/movielens/ml-20m.zip' $ ml_1m_url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'
len(df_enhanced.query('retweeted_status_id != "NaN"'))
sp_companies = pd.read_html("http://en.wikipedia.org/wiki/List_of_S%26P_500_companies", header = 0)
(p_diffs >actual_diff).mean()
pd.value_counts(appointments['Specialty'])
logit_mod_joined = sm.Logit(df_joined_dummy.converted, \ $                            df_joined_dummy[['intercept', 'ab_page_new_page',\ $                                             'country_UK', 'country_US']])
parsed_times = raw.time.str.extract("(?P<m>\d{1,2})?:?(?P<s>\d{2})\.(?P<ms>\d{2})", expand = True) $ time_seconds = parsed_times.m.astype(float).fillna(0) * 60 + parsed_times.s.astype(float) + parsed_times.ms.astype(float) / 100 $ raw.assign(time_seconds = time_seconds)[["event", "time_seconds"]].sample(10)
df_concat.rename(columns={"likes.summary.total_count" : "likes_total", $                           "comments.summary.total_count" : "comments_total" }, inplace = True)
pd.set_option('display.max_columns', None) $ customers.head()
cluster["ZIPCODE"] = cluster.index
autos['ad_created'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
df2.iloc[146678] $
ds.tail()
filter_df.shape
%%time $ pool = ThreadPool(16) $ doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED) $ threaded_reps = pool.map(infer_one_doc, doc_contents) $
pd.DataFrame({'count':user_summary_df.verified.value_counts(), 'percentage':user_summary_df.verified.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})
old_page_converted = np.random.choice([1,0],size = len(old),p = [convert,1-convert])
no_of_rows = df.shape[0] $ no_of_rows
q_agent_new.scores += run(q_agent_new, env, num_episodes=50000)  # accumulate scores $ rolling_mean_new = plot_scores(q_agent_new.scores)
df_vow.head()
mvrs = ratings.groupby('movieId').size().sort_values(ascending=False) $ tmp_ratings = ratings.ix[mvrs[mvrs > rating_count].index].dropna()
autos['price'] = autos['price'].str.replace("$","") $ autos['price'] = autos['price'].str.replace(",","") $ autos['price'] = autos['price'].astype(float)
week36 = week35.rename(columns={252:'252'}) $ stocks = stocks.rename(columns={'Week 35':'Week 36','245':'252'}) $ week36 = pd.merge(stocks,week36,on=['252','Tickers']) $ week36.drop_duplicates(subset='Link',inplace=True)
xgb_learner.best_params
result1 = (df1 < 0.5) & (df2 < 0.5) | (df3 < df4) $ result2 = pd.eval('(df1 < 0.5) & (df2 < 0.5) | (df3 < df4)') $ np.allclose(result1, result2)
df["RNG_INGRESO_BRUTO_COD"] = df["RNG_INGRESO_BRUTO"].map(ingreso_rng_dict)
ser6.sum()
jobPostDFSample = jobPostDF.sample(100)
from datetime import datetime $
sox = pd.read_csv('../../../data/sox_master.csv')
user_retention = cohorts_unstacked_nonblank.divide(cohort_group_size, axis=1) $ user_retention.head(10)
df_train[0:5].T
cols = ['date_crawled', 'name', 'seller', 'offer_type', 'price', 'abtest','vehicle_type', 'registration_year', 'gear_box', 'power_ps', 'model','odometer', 'registration_month', 'fuel_type', 'brand','unrepaired_damage', 'ad_created', 'nr_of_pictures', 'postal_code','last_seen'] $ autos.columns = cols $ autos.info()
tweetsIn22Mar.index = pd.to_datetime(tweetsIn22Mar['created_at'], utc=True) $ tweetsIn1Apr.index = pd.to_datetime(tweetsIn1Apr['created_at'], utc=True) $ tweetsIn2Apr.index = pd.to_datetime(tweetsIn2Apr['created_at'], utc=True)
finals[(finals["pts_l"] == 1) & (finals["ast_l"] == 1) & (finals["blk_l"] == 1) & $        (finals["reb_l"] == 1) & (finals["stl_l"] == 1)].PLAYER_NAME.value_counts()
fin_p.columns = ['SP500', 'OMXS30', 'EURSTOXX', 'Gold']
train, test = data.randomSplit([0.8,0.2], seed=6) $ train.cache() $ test.cache()
users.schema
print(soup.prettify()[28700:30500])
news.date.dt.date.value_counts().plot(linewidth=0.5, figsize=(16, 5), title='Number of news per day');
NoOfUniqueUser = df['user_id'].nunique() $ NoOfUniqueUser
sns_plot = sns.lmplot(x='score',y='favorite_count',data=rating_and_retweet,fit_reg=False,scatter_kws={'alpha':0.05}) $ sns_plot.savefig("score_vs_favorite.jpg")
fraud_data.info()
df = pd.read_gbq(query, project_id='np-hacks',verbose=True, private_key='auth_key.json', dialect='standard')
import pycountry $ country_mapping = {country.alpha_2: country.alpha_3 for country in pycountry.countries} $ geo_code = [] $ for code in geo_countries: $     geo_code.append(country_mapping[code])
close = panel_data.ix['Close'] $ all_weekdays = pd.date_range(start=start_date, end=end_date, freq='B') $ close = close.reindex(all_weekdays) $ close.tail(10)
data.iloc[2, [3, 0, 1]]
def remove_stopwords(text): $     token_text = tokenize(text) $     text = ' '.join(i for i in token_text if (i.strip() not in stop_words)) $     return(text)
import matplotlib.pyplot as plt $ import pandas as pd $ import pylab as pl $ import numpy as np $ %matplotlib inline
visitors.head(5)
from scipy.stats import norm $ norm.cdf(z_score), norm.ppf(1-(0.05))
tweets_rt.head()
df_geo_unique.head()
df_DRGs.tail()
df_ml_57_01.tail(5)
result_copy.index.is_unique
hp
for row in cursor.execute("SELECT * FROM person;"): $     print(row)
df_big = df_big.drop(columns=['doggo', 'floofer', 'pupper', 'puppo'])
df = pd.read_excel("mails taggen.xlsx")
S.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
df_raw.sort_values(by=['year','list_crawl_num', 'row_crawl_num']) # look at lists in original order
props.prop_name.value_counts()
output= "SELECT user_id, followers from user where followers>150000 order by followers desc " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['user_id','followers'])
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old]) $ print("z-score:", z_score, $      "\np-value:", p_value)
from scipy.stats.mstats import winsorize $ train_df['price'] = winsorize(train_df['price'], limits=[0.01, 0.01]) $ sns.distplot(train_df.price.values, bins=100, kde=True)
df4 = gbq.read_gbq(querry, project_id = project_id, dialect = "standard") $ df4.head()
twitter_df_clean.drop(twitter_df_clean[twitter_df_clean.in_reply_to_status_id.notnull()].index, inplace=True)
days_traded = len(r) $ n_years = days_traded / avg_annual_days_traded $ cagr = (end_value / start_value) ** (1 / n_years) $ print("Compound annual growth rate (CAGR) = {:5.4}%".format((cagr - 1) * 100))
import pandas as pd $ df = pd.read_csv('Consumer_Complaints.csv', encoding = "ISO-8859-1") $ df.head()[['Product', 'Consumer complaint narrative']]
url = '{}&start_date={}&end_date={}'.format(base_url, '2017-01-01', '2017-12-31') $ r = requests.get(url) $ if r.status_code != requests.codes.ok: $     print('API ERROR: ({}) -{} '.format(r.status_code, r.text))
df_clean.drop(df_clean[df_clean['retweeted_status_id'].notnull()].index,inplace=True)
about.find('a')
sorted_precip.describe(percentiles=None, include=None, exclude=None)
df_customers['price'] = vector_p[1:]
df_filtered_andora = df[df['Country'] =='Andorra'] $ df_filtered_andora = df_filtered_andora[['Indicator_Id','Country','Year','WHO Region','Publication Status']] $ df_filtered_andora = df_filtered_andora.sort_values(['Indicator_Id','Country','Year','WHO Region'],ascending=[False,False,True,False]).drop_duplicates(keep="first") $ df_filtered_andora =df_filtered_andora.reset_index(drop=True) $ df_filtered_andora.head(3)
google_stock.head() $ google_stock.head(10)
highmeans.sort_values(ascending = False)
user_summary_df[user_summary_df.gender == 'M'][['followers_count', 'following_count', 'tweet_count', 'original', 'quote', 'reply', 'retweet', 'tweets_in_dataset']].describe()
(grades > 5).all()
s.str.len()
df.head()
df.query_string.value_counts()
em_sz,nh,nl = 400,1150,3
train = train.sort('date') $ train.reset_index(drop=True, inplace=True)
plt.style.use('ggplot') $ fig = plt.figure() $
import matplotlib.pyplot as plt $ %matplotlib inline
n_old = control.shape[0] $ n_old
sns.barplot(x="class", y="survived", hue="sex", data=titanic)
r6s.head()
clim_train, clim_test, size_train, size_test = train_test_split(x_climate, y_size, $                                                                 test_size=0.25, $                                                                 random_state = 321)
df.loc[df['lead_mgr'].str.contains('ViewTrade'), 'lead_mgr'] = 'viewtrade securities'
twitter_df_merged
bigdf['score'] = pd.to_numeric(bigdf['score']) $ bigdf['comment_is_root'] = pd.to_numeric(bigdf['comment_is_root']) $ bigdf['comment_parent'] = bigdf.comment_parent.map(str) $ bigdf = bigdf.convert_objects()
country_size=pd.value_counts(ac['Country'].values, sort=True, ascending=False) $ country_size.head()
import requests $ import json $ PORT_NUMBER = 1234 # This is the default port number of CyREST
df_worst_metric.tail(n=3)
van_final['isReverted'].value_counts()
n_new = df2[df2['landing_page'] == "new_page"]['converted'].count() $ n_new
pd.date_range(start, periods=10, freq='1D10U')
Lags = MSUStats.groupby(level=1).shift(periods=1)  $ Lags.columns = [c+'Lag1' for c in Lags.columns] $ Lags.head()
df.columns
df3 = df.iloc[df.query('group == "control" and landing_page == "old_page"').index.values]
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation $ p_diffs = np.array(diff1) $
date_ny = day_ny.groupby(['day','month','year'])['count'].apply(shannon).reset_index()
df2 = df2.drop([2893])
file = 'https://assets.datacamp.com/production/course_1975/datasets/mnist.csv' $ digits = pd.read_csv(file) $ print(digits.shape) $ digits.head()
n_new = (df2.query('group == "treatment"')['converted'] >= 0).sum() $ n_new
potential_accounts_buildings_info_tbrr.head()
sns.set(color_codes=True) $ for column in ['net_profit']: $     plt.figure(figsize=(4,4)) $     sns.regplot(x=column, y='sale_dollars', data=df_2015)
q4_results = session.query(Measurements.tobs).filter(Stations.station == Measurements.station,)\ $             .filter(Measurements.date >= '2017-01-01').filter(Measurements.date<='2018-01-01').all() $
model.compile(loss='categorical_crossentropy', $               optimizer='adam', metrics=['accuracy'])
add_datepart(weather,"Date",drop=False)
c.execute('SELECT city FROM weather where warm_month = "July"') $ print(c.fetchall())
visits = dta.drop_duplicates(["address", "dba_name", "inspection_date"])
print('Before: ', kickstarter.shape) $ kickstarter = kickstarter.dropna(axis=0, how='any') $ print('After: ', kickstarter.shape) $ kickstarter.isnull().sum()
row_0['PassengerId']
df2["user_id"].value_counts()
users = pd.read_csv('data/users.csv', index_col = 0, parse_dates = [1,2]) $ users.columns
iris_dataframe.head()
techmeme['nlp_text'] = techmeme.titles.apply(lambda x: tokenizer.tokenize(x.lower())) $ techmeme.nlp_text = techmeme.nlp_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x]) $ techmeme.nlp_text = techmeme.nlp_text.apply(lambda x: ' '.join(x))
from  scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05))) $
df= df['data']
active_org_id = clean_users[clean_users['active']==1][['org_id','active']].groupby('org_id').count()
import pandas as pd $ import numpy as np
p_old = df2['converted'].mean(); $ p_old
print("\nClassification Report:\n",classification_report(y_test, y_pred)) $
tweets.columns
sel=[Measurement.date, $      Measurement.tobs] $ day_prcp=session.query(*sel).filter((Measurement.date>year_ago)).all() $
twitter_goodreads_users_df.info()
preg = pd.read_csv('2002FemPreg.csv', index_col = 0)
frame.sort_index()
train_fs2.shape
xmlData['sold_date'] = pd.to_datetime(xmlData['sold_date'], format = "%Y-%m-%d", errors = 'raise')
print(x_tr.shape) $ print(x_tr[1][1]) $ print() $ print(y_tr.shape) $ print(y_tr[1])
[reader]*2
q_all_count.results()["count"].sum()
df.filter(df.column_name>10)
pd_aux2=pd.DataFrame(building_pa_prc_fix_issued[['permit_creation_date','issued_date']])
df = pd.read_csv('reddit_comments.csv')
directory_name = os.path.join('network', 'recurrent_network')
def fund_vs_coins(date, coins, logy=True): $     iprice_fund = 100*np.exp(fundret.loc[date:].cumsum()).shift(1).fillna(1) # tmp fix. todo make it nicer $     iprice_coins = indexed_price(price_mat.loc[date:, coins]) $     fund_and_coins = pd.concat([iprice_fund, iprice_coins], axis=1) $     return fund_and_coins.plot(logy=logy)
df2.query('landing_page == "new_page"').user_id.nunique()
demand.loc[:,'value'] = demand.loc[:,'value'].copy() *1.5 $ demand.head()
logit_mod=sm.Logit(df['converted'],df[['intercept','treatment']])
%writefile ./test.json $ {"pickuplon": -73.885262,"pickuplat": 40.773008,"dropofflon": -73.987232,"dropofflat": 41.732403,"passengers": 2}
df1 =pd.read_csv('https://raw.githubusercontent.com/kjam/data-wrangling-pycon/master/data/berlin_weather_oldest.csv') $ df1.head(2)
intake.info()
theft.iloc[0:10]
np.unique(raw_train_y.values, return_counts=True)
df_new[['CA','UK']] = pd.get_dummies(df_new['country'])[['CA','UK']] #choosing to show CA and UK in terms of US
prcp_12monthsDf = pd.DataFrame(prcp_12months)
wrd_clean['rating_denominator'].describe()
check_max_loan(pipeline, X_valid[0:30], y_valid[0:30])
client = WatsonMachineLearningAPIClient(wml_credentials)
cur.execute('SELECT count(Comments_Ratings) FROM surveytabl WHERE Comments_Ratings is not null;') $ cur.fetchall()
df1 + df2
import StringIO $ from StringIO import StringIO  # got moved to io in python3.
baseball.shape
weekly_cases = daily_cases.unstack().T.resample('W').sum() $ weekly_cases
ufos_df.toDF().registerTempTable("ufo_withyear")
import json $ df.to_json('dataframe.json',date_format='iso') #specify date format so that "read_json" command can retrive datatime
sample_sizes.groupby().agg(F.sum("sample_size_1")).show()
bacteria_data.drop('treatment', axis=1)
autos['registration_year'].value_counts()
data = data[data.state != 'undefined'] $ data.info()
old_page_converted = np.random.binomial(1, p_old, n_old) $ plt.hist(old_page_converted) $ plt.axvline(x=p_old, color="red");
p_new = df2['converted'].sum() / df2.shape[0] $ p_new
merge = pd.merge(df1, df2, left_index=True, $                 right_index=True, how="outer", $                 suffixes=("", "_y")) $ merge.to_csv("{}MN_17_3616_merge.csv".format(out_dir), $             float_format='%g')
!conda install scikit-learn
df = sean['Event Type Name'].value_counts() $ df_sean = pd.Series.to_frame(df) $ df_sean.columns = ['Sean H'] $ df_sean
tree_features_df.loc[:,'p_hash'] = tree_features_df['p_hash'].apply(imagehash.hex_to_hash) $ tree_features_df.loc[0,'p_hash']
thalamus.build() $ thalamus.save_nodes(output_dir='network') $ thalamus.save_edges(output_dir='network')
print(df2.query('user_id == 773192'))
on_off(0.3, 1)
stock = stock[(stock["Close"] != "null")]
subred_counts.subreddit.values
df_imgs
df_DOT = df[df['Agency Name'].isin(['Department of Transportation', 'DOT'])]
df[df['C'].isin(['can', 'fish'])]
browser.visit(url) $ html = browser.html
%matplotlib inline $ modifications_per_authors_over_time.plot(kind='area', legend=None, figsize=(12,4))
with open(os.path.join(folder_name, url.split('/')[-1]), mode = 'wb')as file: $       file.write(r.content)
rf_gridcv.fit(x_train_scaled, y_train)
dfn.head(20)
for i in cpi_all['Frequency'].cat.categories.tolist(): $     print i    
df_movies.shape
scores = cross_val_score(model, X_train, y_train, cv=5)  # cross Validation $ np.mean(scores), np.std(scores) #  scoring the performance of training data
df['2014-05-02']
ts.resample("Min").sum() # 1 minute totals 
weekend_grid['predict'] = adj_glm_int.predict(weekend_grid[['inday_icu_wkd','admission_type']]) $ weekend_grid['log_odds'] = prob2logodds(weekend_grid['predict']) $ weekend_grid.set_index(['inday_icu_wkd','admission_type'], inplace=True) $ weekend_grid
x_scaled = min_max_scaler.fit_transform(x_normalized)
student_info = [(100, 62, 'F'), (120, 66, 'M'), (140, 68, 'M'), (110, 62, 'F'), (160, 70, 'M'), (140, 63, 'F'), (140, 66, 'F'), (110, 63, 'F'), (180, 72, 'M'), (190, 72, 'M'), (200, 73, 'M')] $ names = ['Mary', 'Mike', 'Joe', 'Janet', 'Steve', 'Alissa', 'Alison', 'Maya', 'Ryan', 'Paul', 'Michael'] $ students = pd.DataFrame(student_info, columns = ['weight', 'height', 'gender'], index = names) $ students
print('Best score for data:',np.mean(forest_clf.feature_importances_))
active = session.query(Measurement.station,Measurement.tobs) $ df2=pd.DataFrame(active[:],columns=["Stations","TOBS"]) $ activestations=df2.groupby("Stations").count().sort_values('TOBS',ascending=False) $ activestations
full_dataset.loc[full_dataset.team_A_name == 'NRG']
indexed_price(price_mat.loc['2017-03':, 'Bitcoin'])
import pandas as pd $ import numpy as np $ df=pd.read_csv('talks.csv') $ df.head()
Grouping_Year_DRG_discharges_payments.xs(2011, level='year').head()
tweet_json = pd.read_json('tweet_json.txt',lines= True)
df_drug_counts = pd.DataFrame(drug_counts_dict).transpose()
X_train.fillna(X_train.mean(), inplace=True)
df_mes['PULocationID'].unique().shape
cnxn = pyodbc.connect(conn_str) $ cursor = cnxn.cursor() $ for table_info in cursor.tables(tableType='TABLE'): $     print(table_info.table_name)
countries_df = pd.read_csv('./countries.csv') $ countries_df.head() $
df_TempIrrs = pd.DataFrame(Irregularities_data,columns=['timeStamp','pubTimeStamp','speed','level','lineString']) $
contractor_final
news_p = soup.find('div', class_='image_and_description_container').text.strip() $ news_p
data = allocate_equities(allocs=[0.25,0.25,0.25,0.25]) $ data.plot() $ plt.show()
print(result2.summary())
train_df.head().T
feature_names = [col for col in training_df if col.startswith('f_')] $ le = LabelEncoder() $ train_y = le.fit_transform(training_df.result) $ train_x = training_df[feature_names]
recipes['ingredients'].str.len().describe()
auth_uri
master_list[master_list['Count'] >= 5]['Count'].describe()
domain = 'steemit.com'
english_dict = enchant.Dict("en_US")
lr = 5e-4 $ learn.fit(lr, 20, cycle_len=1, use_clr=(10,10))
df.dropna(inplace=True) $ df.shape
T = 0 $ folder = 'trainW-'+ str(T) $ train = pd.read_csv('../../input/preprocessed_data/trainW-{0}.csv'.format(T))[['msno']] $ user_logs = utils.read_multiple_csv('../../feature/{}/user_lgos_listening_freq'.format(folder),parse_dates = ['date']) $
X_tfidf_df.head(10)
pre_strategy_google = pre_strategy_users[pre_strategy_users['channel'] == "Google"].sort_values(by='date')
print(stock_data.shape) $ print("The number of rows in the dataframe is: ", stock_data.shape[0]) $ print("The number of columns in the dataframe is: ", stock_data.shape[1]) $
df_train.head()
%%time $ pd.to_datetime(df['Closed Date'].head(10000), format='%m/%d/%Y %X %p')
df.drop(['gid_today', 'game_total_score'], axis=1, inplace=True)
places = api.geo_search(query="London", granularity="city") $ place_id_L = places[0].id $ print('London id is: ',place_id_L)
cohort_retention_df
agency_borough = data.groupby(['Agency', 'Borough']).size().unstack()
clf = RandomForestClassifier(n_estimators = 100, max_depth = 30) $ clf.fit(X_train, y_train)
data_activ = data_activ.groupby('new_date').count().reset_index()[['new_date','id']]
model.doesnt_match("man woman child kitchen".split())
df.head(3)
data=get_data(64) $
train.pivot_table(values = 'Fare', index = 'Class', aggfunc=np.mean)
autos.describe(include='all')
df_new.groupby(['country','landing_page']).count()
df.query('group == "treatment" and landing_page == "old_page"').count()['user_id'] + df.query('group == "control" and landing_page == "new_page"').count()['user_id']
series3
list_of_features = [] $ list_of_labels = []
import numpy as np $ import pandas as pd $
url_NOTCLEAN1A = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/NOTCLEAN1A.csv" $ df_NOTCLEAN1A = pd.read_csv(url_NOTCLEAN1A,sep=',')
df2['intercept'] = 1 $ df2[['control', 'ab_page']] = pd.get_dummies(df2['group']) $ df2.drop(columns=['control'], inplace=True) $ df2.head()
df=pd.read_csv('ab_data.csv') $ df.head()
persort = percent.abs().sort_values(ascending=False)
autos["price_dollars"].describe()
df = ek.get_timeseries(["MSFT.O"], $                        start_date="2016-01-01",  $                        end_date="2016-01-10") $ df
%matplotlib notebook
(df['margin_val'] < 0).sum()
click_percentage = (train['is_click'].value_counts()[1] / train.shape[0]) * 100 $ print('Percentage of total emails that get clicked: {0:.2f}%'.format(click_percentage))
pd.merge(left=city_loc, right=city_pop, on="city", how="right")
own_star.rename(columns={'created_at_star': 'created_at'}, inplace=True) $ own_star.head(10)
unique_animals.add("Tiger")
wrQualified.map(lambda p: p["gfx"]["features"]["compositor"]).countByValue()
df = pd.read_csv('../data/Pitchfork Reviews.csv') $ df['Date'] = pd.to_datetime(df.Time).dt.date $ df = df.drop(['Time', 'Unnamed: 0'], axis=1).set_index(['Date']) $ df.head()
weather_df_byday = weather_df.loc[weather_df.index.weekday == weekday] $ weather_df_byday.info() $ weather_df_byday.head()
np.sum(van_true)
sentiment_scores = [] $ for text in data_sentiment["text"]: $     sentiment_score = sid.polarity_scores(text) $     sentiment_scores.append(sentiment_score['compound']) $ data_sentiment["sentiment_score"] = sentiment_scores
X.shape
n_old
print(props.head())
autos['price'].value_counts().head(20)
baseball_newind.loc[:'myersmi01NYA2006', 'hr']
pd.read_html(driver.page_source)[0]
git_blame['component'] = git_blame.path.str.split("/", n=2).str[:2].str.join(":") $ git_blame.head()
df_archive['name'].value_counts()
df_geo_unique = df_geo.drop_duplicates() $ df_geo_unique.shape
n_old = df2.query('landing_page == "old_page"').converted.count() $ n_old
temp = date_df[['weekday', 'duration(min)']] $ temp.head()
actual_difference = df2_treatment['converted'].mean()- df2_control['converted'].mean() $ plt.hist(p_diffs); $ plt.axvline(x=actual_difference, color ='orange'); $ p_diffs = np.array(p_diffs) $ (p_diffs > actual_difference).mean()
openmc.run()
p_diff_real = p_new_real-p_old_real $ p_diff_real
train_users = pd.read_csv("../data/airbnb/train_users_2.csv")
dftestdum = trainsparsify(pd.get_dummies(dftest[testcols].astype(str))) $ print(dftestdum.shape)
x1 = [x[0] for x in scores_pairs_by_business["score_pair"]] $ y1 = [x[1] for x in scores_pairs_by_business["score_pair"]] $ plt.scatter(x = x1, y = y1) $ plt.plot([60, 100], [60, 100], color='k', linestyle='-', linewidth=2) $
mention_df['mention_user_id'].unique().size
tweet_archive_enhanced_clean.loc[2338,'text']
trump['source'].unique()
median = reviews.price.median() $ reviews.price.map(lambda v: v-median)
pipeline.fit(fb_train.message, fb_train.popular)
df = df.selectExpr("_c0 as text") $ df.show()
pd.Series(42)
countries_df=pd.read_csv('countries.csv') $ countries_df.head()
plt.scatter(cdf.CYLINDERS, cdf.CO2EMISSIONS, color='blue') $ plt.xlabel("Cylinders") $ plt.ylabel("Emission") $ plt.show() $
weather.head()
df = pd.read_sql('SELECT * FROM customer_male', con=conn_a) $ df
tweets['text'] = tweets['fixed'] $ tweets.drop('fixed', 1, inplace=True)
nas
import seaborn as sns $ import pandas as pd $ import numpy as np $ %pylab inline
version="version_7"
deaths_liberia =dropped_liberia.loc['Total death/s in confirmed, probable, suspected cases'] $
startDate = dateutil.parser.parse("2012-01-01") $ startDateStock = startDate - dateutil.relativedelta.relativedelta(months=2) $ endDate = dateutil.parser.parse("2017-12-31")
repeated_user = df2[df2['user_id'].duplicated()] $ repeated_user['user_id']
%run process_twitter2tokens.py -i ../data/Training_promotion_people.csv -ot ../data/Training_promotion_people.txt -oc ../data/Training_promotion_people_tokenized.csv -co text
unprocessed = [32,562,954,1383,1475]
baseUrl = "https://gc2.datadistillery.org/api/v1/sql/bcb?q=" $ queryUrl = baseUrl + searchCriteria $ recordsToSearch = requests.get(url=queryUrl) $ npsGc2Data = geojson.loads(recordsToSearch.text) $ gdfNps = gpd.GeoDataFrame.from_features(npsGc2Data['features'], 4326)
plt.bar(0, data, width=1, yerr=[[data - error[0]], [error[1] - data]]) $ plt.title("Trip Average Temperature") $ plt.ylabel("Temperature (F)") $ plt.xticks([]) $ plt.show()
mask = (youthUser4["creationDate"] > '2017-11-01') & (youthUser4["creationDate"]<= '2017-11-30') $ youthUserNov2017 = (youthUser4.loc[mask]) $ youthUserNov2017.head()
a = df2['landing_page'] == 'new_page' $ n_new = df2[a] $ n_new = n_new.converted.count() $ print('Number of rows in which landing page is aligned with new page is:{}'.format(n_new))
sns.lmplot('FLOW','REVENUE',df3) $ plt.show()
df2[df2.user_id == 773192]
pd.datetime(2016,6,2) - pd.datetime(2016,6,22)
query = pgh_311_codes['Issue'] == 'Building Maintenance' $ pgh_311_codes[query]
def exclude_retweets(tweet): $     if tweet['retweeted'] == True: $         return False $     return True
df1['day_of_week'].value_counts() $
x.dropna(axis=1)
df.groupby('episode_id')
temperature_sensors_df = sensors_df[sensors_df['entity_id'].isin(temperature_sensors_list)] $ temperature_sensors_df.head()
twitter_ar.rating_num.value_counts()
births['decade'] = 10 * (births['year'] // 10) $ births.pivot_table('births', index='decade', columns='gender', aggfunc='sum')
TotalNewPage = df2.query('landing_page == "new_page"')['user_id'].count() $ print("Probability of New Page: ",(TotalNewPage/NewTotalUser))
most_active = list(np.where(indexed_activity["station_count"]==max_activity)[0]) $ station_max = indexed_activity.iloc[most_active] $ station_max =str(station_max.index[0]) $ print("Station with the most observations recorded: "+station_max+ $       ", with a count of {:,} observations".format(max_activity))
experiment_df.drop(['email_opened'], axis=1, inplace=True) $ experiment_df.drop(['date'], axis=1, inplace=True) $ experiment_df.drop(['UUID_hash2'], axis=1, inplace=True)
model = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'afternoon', 'evening']]) $ results = model.fit()
result3.summary2()
pd.set_option('display.max_rows', 20) $ df
stock_data.shape
recipes.description.str.contains('[Bb]reakfast').sum()
ratings.printSchema()
com_grp['Age'].sum()
Image(filename='D:\kagglelearn\pics\wine_reviews.png')
df2 = pd.io.json.json_normalize(data=data) $ df2.head() $ df2.columns
%timeit functools.reduce( (lambda x,y:x+' '+y), L)
data.view_count_by_category[0]
popularity_clean.info()
import pandas as pd $ import datetime $ import matplotlib.pyplot as plt $ import numpy as np $ %matplotlib inline
new_page_user = len(df2.query("group == 'treatment'")) $ total_users = df2.shape[0] $ print("The probability that an individual received the new page: " + str(new_page_user/total_users)) $ print("As a percentage: " + str((new_page_user/total_users)*100) + "%")
df1['user_id'].nunique()
n_old = len(df2.query('landing_page == "old_page"')) $ n_old
df[df['Complaint Type']=='Street Condition']['Descriptor'].value_counts()
flight_delays = pd.read_csv("../clean_data/delays_per_hour.csv")
print_tweet_text(tweet_archive_df[(tweet_archive_df.rating_denominator % 10).astype(bool)])
df.drop('gender',axis=1,inplace=True)
assert b.T.sum().median() == c $
autos["brand"]
df = df[(df.yearOfRegistration > 1900) & (df.yearOfRegistration <= 2016) ]
p_new-p_old
joined = joined[joined.Sales!=0]
first_result.find('a')
bp[bp["icustay_id"]==14882].plot(x="new charttime", $                                  y=["systolic", "diastolic"]) $ bp[bp["icustay_id"]!=14882].plot(x="new charttime", $                                  y=["systolic", "diastolic"])
from IPython.display import Image
vectorizer.stop_words_
df2.nunique()['user_id']
logit_countries = sm.Logit(newset['converted'], $                            newset[['country_UK', 'country_US', 'intercept']]) $ result_new = logit_countries.fit()
full.info()
df2.drop([2862],inplace=True)
result[3]
display(Markdown(q3a_answer)) $ bus.loc[:,"postal_code"].apply(type)
counts = df.groupby( $     pd.Grouper(key='timestamp', freq='ms'), $ ).count() $ counts.tail()
nba_df["Points_Range"] = team_groups["Tm.Pts"].transform('max') - team_groups["Tm.Pts"].transform('min') $ nba_df.iloc[1:5, ]
ssc.start()
df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True) $ df.head()
from pyspark.sql import SparkSession, SQLContext, Row $ spark = SparkSession \ $     .builder \ $     .appName("Spark SQL") \ $     .getOrCreate()
associations = pd.read_csv('Data/associations.csv') $ users = pd.read_csv('Data/users3.csv')
for iter_x in np.arange(lookback_window)+1: $     df['{0}-{1}'.format(base_col,str(int(iter_x)))] = df[base_col].shift(iter_x)
df_imgs.info()
new.shape
fig, ax = plt.subplots(figsize=(10,8)) $ fig = arma_res.plot_predict(start=8000, end=16000, ax=ax) $ legend = ax.legend(loc='upper left') $ plt.title("Time Series Forecasting using the ARIMA model")
fc_clean
random.seed(1234) $ na_index = random.sample(range(X_mice.shape[0]), int(X_mice.shape[0]*0.3))
p_equal_old_new = (p_new+p_old)/2 $ p_equal_old_new
train.isnull().sum()
df2.groupby('group').describe()
df_data['VITIMAFATAL_BOOL'] = df_data.VITIMAFATAL.apply(yes_or_no_to_bool) $ df_data.head()
data.index[0] = 15
open('test_data//write_test.txt', mode='w').close()
train_file = r"D:\Git\Sellthrough2\STR_Pipeline\temp_data\data20180716\feature20180716\mdata_noopen\Data_Train_FeatureTactic_train_feature20180716_data_with_headermask_0.1.txt" $ valid_file = r"D:\Git\Sellthrough2\STR_Pipeline\temp_data\data20180716\feature20180716\mdata_noopen\Data_Train_FeatureTactic_test_feature20180716_data_with_headermask_0.1.txt" $ eval_file = r"D:\Git\Sellthrough2\STR_Pipeline\temp_data\data20180716\feature20180716\mdata_noopen\Data_Train_FeatureTactic_emk_feature20180716_data_with_headermask_0.txt"
file_attrs_string = str(list(hdf5_file.items())) $ file_attrs_string
tweet_lang_hist.sum()['count']
df_country.nunique()
data['moving_avg'] = pd.rolling_mean(data['sentiment'], 500)
us[us['cityOrState'].isin(states) == False]['cityOrState'].value_counts(dropna=False)
print(type(site))
most_active = stations[0][0] $ record = [func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)] $ session.query(*record).filter(Measurement.station == most_active).first()
shows.dtypes
train_topics_df=pd.DataFrame([dict(ldamodel[item]) for item in train_corpus], index=graf_train.index) $ test_topics_df=pd.DataFrame([dict(ldamodel[item]) for item in test_corpus], index=graf_test.index) $
scr_end_date = pd.Series(pd.to_datetime(scr_end_date).strftime('%Y-%m'),index=scr_churned_ix)
af.length.describe()
ac['If No Eligibility, Why?'].describe()
df = train[columns].append(test[columns])
warnings.simplefilter('ignore')
groups[0]
with open(json_filename) as json_file:  $     data = json.load(json_file) $ tweet_json = pd.DataFrame.from_records(data)
df.rename(columns={'Indicator':'Indicator_Id'}).head(2)
twitter_archive_clean.info()
eligible_posts = [x for x in all_posts if x['eligible']] $ post_ids = set([x['id'] for x in eligible_posts]) $ eligible_comments = [x for x in all_comments if x['link_id'].replace("t3_", "") in post_ids] $ days_in_dataset = (all_posts[-1]['created'] - eligible_posts[0]['created']).total_seconds() / 60. / 60. / 24.
jan_2015_frame = add_pickup_bins(frame_with_durations_outliers_removed,1,2015) $ jan_2015_groupby = jan_2015_frame[['pickup_cluster','pickup_bins','trip_distance']].groupby(['pickup_cluster','pickup_bins']).count()
with tb.open_file(filename='data/my_pytables_file.h5', mode='a') as f: $     dangling_link = f.create_soft_link(where='/', $                                        name='dangling_link', $                                        target='/gr1/gr2/gr3/some_not_yet_existing_array') $     print(dangling_link.target)
df2['user_id'].nunique()
X_train = train_df.drop("Survived", axis=1) $ Y_train = train_df["Survived"] $ X_test  = test_df.drop("PassengerId", axis=1).copy() $ X_train.shape, Y_train.shape, X_test.shape
print(df_train.shape) $ df_train.head(10)
field_stream_occurances_df = pd.concat(field_occurances_list)
plot_ngram(sorted_bigram,'Bigram')
prod_sort = averages[averages.Product == 'Amarilla'] $ prod_sort
diffs = [] $ for _ in range(10000): $     new_page = df2.converted.sample(n_new, replace = True) $     old_page = df2.converted.sample(n_old, replace = True) $     diffs.append(new_page.mean() - old_page.mean()) $
weather_mean.iloc[[4, 2], 3:7]
x,y=X[0:train],Y[0:train] $ print (x.shape,y.shape) $ model.fit(x,y,epochs=150,batch_size=10,shuffle=True)
rs = session.query(func.max(Measurement.date)) $ Last_date = rs[0][0] $ print(Last_date) $
control_conv_prob = df2.loc[(df2["group"] == "control"), "converted"].mean() $ control_conv_prob
hillary_data = data[data.cand_nm == 'Clinton, Hillary Rodham'] $ sorted_by_date = hillary_data.sort_values("contb_receipt_dt", ascending=True)
articles['id'].unique()
df[df.isnull()].count()
X_test_df = pd.DataFrame(X_test_matrix.todense(), $                         columns=tvec.get_feature_names(), $                         index=X_test.index)
building_pa_prc_zip_loc.sample(20)
df_best_metric.head(n=3)
import seaborn as sns $ sns.factorplot('sex', data=titanic3, kind='count')
token = token[~ token.receiver.isin([i for i in token.receiver.unique() if bool(re.search("[^0-9]", str(i)))])] $ token = token[~ token.sender.isin([i for i in token.sender.unique() if bool(re.search("[^0-9]", str(i)))])] $ token = token.dropna() $ token.index = range(len(token)) $
df = pd.DataFrame(results[:], columns=['Date', 'Precipitation']) $ df.set_index('Date', inplace=True, ) $ df.head()
uber.short_description
weather['events'] = weather['events'].apply(lambda x: 'Rain' if x == 'rain' else x)
def TimesTen(v): $     return int(v * 10) $ print TimesTen(3)
ts.asfreq(pd.tseries.offsets.BDay(), method='pad')
sim_closes = closes_aapl.iloc[-1].AAPL*np.exp(sim_ret.cumsum()) $ sim_closes
len(feature_names)
t = 4 $ m['predicted_purchases'] = bgf.conditional_expected_number_of_purchases_up_to_time(t, m['frequency'], m['recency'], m['T']) $ m.sort_values(by='predicted_purchases').tail(15)
top_songs['Date'].max()
msft = pd.read_csv("../../data/msft.csv") $ msft.head()
knn_model = KNeighborsClassifier(n_neighbors=7) $ knn_model.fit(X_train, y_train)
X_train.info()
g = sns.JointGrid(x='retweet_count',y='favorite_count',data=df) $ g = g.plot(sns.regplot, sns.distplot)
d = datetime(2014,8,29) $ do = pd.DateOffset(days = 1) $ d + do
sentimentos = [1, 0] $ print(metrics.classification_report(classes, classificacoes_treino, sentimentos))
print(contribs.head())
cashflows_act_investor_EOM[(cashflows_act_investor_EOM.id_loan==34) & (cashflows_act_investor_EOM.fk_user_investor==38)].to_clipboard()
twitter_archive[twitter_archive.text.str.contains(r'(\d+(\.\d+))\/(\d+)')]
print 'RF: %s' % rf.score(X_test, y_test) $ print 'KNN: %s' % knn.score(X_test, y_test)
import scipy as sp $ import numpy as np $ import math $ import random
list(archive_df_clean)
import glob, os
num_rows = 1000000  # 1 million $ matrix = np.random.random((num_rows, num_columns)).astype('float32')
np.exp(result_c.params)
autos.head(3)
map_values = {'N0(i-)': 'N0(i_minus)', 'N0(i+)': 'N0(i_plus)'} $ df_categorical['latest_n'] = df_categorical['latest_n'].replace(map_values) $ df_categorical['latest_n'].unique()
s = select([employee]) $ result = conn.execute(s) $ row = result.fetchall() $ print(row)
calls_nocontact.street_address.value_counts()
df.columns=['name','gender','age'] $ df
bins = [5, 25, 50, 100, 200, 300, 400, 500, 1000, 5000] $ df3_freq = df3['Donation Amount'].value_counts(bins=bins).sort_index() $ df3_freq.plot(x='Donation Amount', y='Donation Amount count', kind= 'line') $ plt.show() $
temp_frame = get_filtered_op_frame(OP_TYPE) $ temp_frame = temp_frame.set_index(temp_frame.timestamp) $ temp_frame = temp_frame[['timestamp','value']]
retention_10
twitter_archive_master.info()
df_archive_clean = df_archive_clean.sort_values('tweet_id', ascending = True)
Tue_index  = pd.DatetimeIndex(pivoted.T[labels==1].index).strftime('%a')=='Tue'
header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken} $ response_get = requests.get(endpoint_published_models, headers=header) $ print(response_get) $ print(response_get.text)
import statsmodels.api as sm $ convert_old = len(df2[(df2['group']=='control') & (df2['converted']==1)]) $ convert_new = len(df2[(df2['group']=='treatment') & (df2['converted']==1)]) $ convert_old, convert_new, n_old, n_new $
archive.rating_numerator.value_counts()
import spacy $ nlp = spacy.load('en_core_web_sm') $
!wget http://ekpwww.ekp.kit.edu/~tkeck/realdonaldtrump.csv
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country']) $ df_new = df_new.drop('US', axis = 1) $ log_mod2 = sm.Logit(df_new['converted'],df_new[['intercept','CA','UK']]) $ results2 = log_mod2.fit() $ results2.summary()
from carto.datasets import DatasetManager $ dataset_manager = DatasetManager(auth_client) $ datasets = dataset_manager.all()
df_gen
confusion_matrix(target_vs_created['target'], target_vs_created['target_test'])
df.isnull().any().any() $
first_commit_timestamp = git_log.loc[git_log["author"] == "Linus Torvalds", "timestamp"].min() $ last_commit_timestamp = pd.to_datetime("today") $ corrected_log = git_log.loc[(git_log["timestamp"]>=first_commit_timestamp) & (git_log["timestamp"]<=last_commit_timestamp)].copy() $ corrected_log["timestamp"].describe()
bobby_ols = ols('opening_gross ~ star_avg',dftouse).fit() $ bobby_ols.summary()
nnew = df2.query('landing_page == "new_page"').shape[0] $ nnew
nuscatter = xs_library[moderator_cell.id]['nu-scatter'] $ df = nuscatter.get_pandas_dataframe(xs_type='micro') $ df.head(10)
model.fit(X_tr, y_tr) $
plt.style.use('fivethirtyeight')
pd.DataFrame(file_lines).dmac.value_counts()
df_imputed_mean_NOTCLEAN1A.head(5)
df3['DATE'] = df3['DATE'].apply(lambda x: dateutil.parser.parse(x))
bets = games_to_bet[games_to_bet['bet_either'] == 1].copy()
%timeit pd.eval('df1 + df2 + df3 + df4')
temp_df2 = temp_df.drop_duplicates()
index_list = [] $ for x,y in zip(df_2.index, df_2.title): $     if y not in index_list: $
rfc_feat_sel.fit(blurbs_to_vect, y_encode)
train_data['price'].describe(percentiles = [0.01,0.99])
run txt2pdf.py -o '2018-06-22  2012 872 discharges.pdf'  '2018-06-22  2012 872 discharges.txt'
import matplotlib.pyplot as plt $ %pylab inline $ from datetime import datetime $ df.plot(y='lux',color='r', marker='.', linestyle="")
pd.concat([test, train]).plot(y=['PJME', 'Linear_Regression_Prediction'], figsize=(15,5))
apple_truncated = google_stocks('AAPL', enddate = (1, 1, 2006)) $ print(apple_truncated)
log_with_day = access_logs_df.withColumn('dayOfWeek', weekday(col('dateTime')))
reviews.country.value_counts()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ mydata = pd.read_table(path, sep ='\s+', na_values=['.'], names=['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'class']) $ mydata.head(5) $
for df in (joined,joined_test): $     df["CompetitionOpenSince"] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, $                                                      month=df.CompetitionOpenSinceMonth, day=15)) $     df["CompetitionDaysOpen"] = df.Date.subtract(df.CompetitionOpenSince).dt.days
station_df['station'].count()
np.count_nonzero(x < 6)
client.create_user('telegraf', 'telegraf', admin=True)
df_final.head(3)
col = ['Team', 'Executive', 'Tenure'] $ dframe_team[col] = dframe_team[col].ffill() $ dframe_team = dframe_team[((dframe_team['Draft_year'] >= 1976) & (dframe_team['Draft_year'] <= 2016))] $ dframe_team.head(10)
f_counts_hour_ip.show(1)
from sklearn.feature_extraction.text import TfidfVectorizer
treatment_group = df2.query('group == "treatment"') $ converted_treatment_group = converted_group.query('group == "treatment"') $ print(converted_treatment_group.user_id.nunique() / treatment_group.user_id.nunique())
model2.compile(loss='categorical_crossentropy', $           optimizer= "adam", $           metrics=[f1])
res4 = df_test4_promotions.groupby(['AwardType']).agg({'PromotionRuleCount':'sum', 'AwardAmount':'sum', 'ShopperID':'nunique'}) $ res4['AwardAmount'] = res4['AwardAmount'] / 100 $ res4 $
w=analize_sentiment(test_tweet) $ print(w)
gamma.E_gsf_surface_plot(length_unit=length_unit, energyperarea_unit=energyperarea_unit) $ plt.show()
area_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297, $              'Florida': 170312, 'Illinois': 149995} $ area = pd.Series(area_dict) $ area
import csv $ import pandas as pd $ import numpy as np $ from apiclient.discovery import build $ from oauth2client.service_account import ServiceAccountCredentials
recs = sess.get_data(['ibm us equity','aa us equity'],['best analyst recs bulk','px last','chg pct ytd']) $ recs
len(daily_trading_volume) # median is between position 127 and 128
p.to_timestamp('M', 'e')
df_test[['id','visitors']].copy().to_csv('lgbmtscv_out_tscv_s2.csv',index=False)
flight.printSchema() $ flightv1_1.printSchema()
def bound_prediction(predict): $     predict[predict < 0] = 0 $     return predict
irl.drop(['bill', 'congress'], axis=1, inplace=True) $ irl.head()
load2017.isnull().sum() 
dfX.head()
def relu(input): $
class_merged=pd.merge(class_merged,oil_interpolation,on=['date'],how='left') $ print("Rows and columns:",class_merged.shape) $ pd.DataFrame.head(class_merged)
df3=df2.copy() $ df3[['ab_page','old_page']] = pd.get_dummies(df3['landing_page']) $ df3['intercept']=1 $ df3=df3.drop(['old_page'], axis=1) $ df3.head(5) $
WorldBankdf.toPandas()
Daily_WRFraw_1950_2010 = ogh.getDailyWRF_salathe2014(homedir, mappingfile)
from sklearn.cluster import KMeans $ num_clusters = 50 $ km = KMeans(n_clusters=num_clusters) $ km.fit(model.docvecs) $ clusters = km.labels_.tolist()
estimator = LinearDiscriminantAnalysis() $ rfe = RFECV(estimator,cv = kfold) $ fit = rfe.fit(X,y) $ print("Num of feature: %d") % fit.n_features_ $ print("Feature Ranking: %s") % fit.ranking_
timelog.info()
temp_df2['timestamp'] = pd.to_datetime(temp_df2['timestamp'],infer_datetime_format=True)
add_datepart(train_data,"date") $ add_datepart(train_data,"visitStartTime") $ add_datepart(test_data,"date") $ add_datepart(test_data,"visitStartTime")
p_conv_treatment = df2.query('group == "treatment"')['converted'].mean() $ p_conv_treatment
df.tail(2)
df['field1']  #nos obtiene lacomunda llamada 'field1' $ df.loc[0,'field1']   #obtenemos el valor de la fila '0' y la  comunda 'field1' $ l=df['field1']>59  #se aplica la condicional para cada valor de la columna $ l $ df.loc[df['field1']>79,'field1'] #hacemos un filtro a la columna df.loc[boleano_o_int, 'name_columna'] $
reviews.loc[(reviews.points/reviews.price).idxmax()].title
iex_coll_reference.count()
url = 'http://www.plosone.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0026752.s001' $ labmt = pd.read_csv(url, skiprows=2, sep='\t', index_col=0)
Labels = (CurrentA1.iloc[:,3:14] + CurrentA2.iloc[:,3:14] + CurrentA3.iloc[:,3:14])/3 $ Class_frame = pd.concat([CurrentA1.iloc[:,0:2],Labels],axis=1) $ Class_frame.to_csv("average_voting_elisa.csv") $
df_pivot.hist(figsize=(15, 10), bins=50, alpha=0.7)
latest_tweet
goodTargetUserItemInt=goodTargetUserItemInt.join(itemData.set_index(['id'])[['title','discipline_id','industry_id','career_level','country','region','tags']],on='item_id',how='left') $ print goodTargetUserItemInt.shape
x_test = x_test.reshape((len(x_test),1)) $ print(x_test.shape, y_test.shape)
train_dum.drop(['Source_S130', 'Source_S135', 'Source_S140', 'Source_S154', 'Source_S160', $                'Customer_Existing_Primary_Bank_Code_B056'], axis=1, inplace=True) $ test_dum.drop(['Source_S126', 'Source_S131', 'Source_S132', 'Source_S142'], axis=1, inplace=True)
weekday_agg = weekday_agg[weekday_agg['end_station_id']<=15 ] # for sake of better visualization filtering for $
df.dist_km.describe() #1.87 million trips in Sept 2017, with average trip distance of 1.88km.
df3 = df.ix[0:3, ['X', 'Z']] $ df3.columns = ['P', 'Q'] $ df4 = df.ix[4:6, ['W']] $ df4.columns = ['R'] $ print df3, "\n\n", df4
import pods $ from ipywidgets import IntSlider
k1 = data[['cust_id','lane_number','total_spend']].groupby(['cust_id','lane_number']).agg('mean').reset_index() $ k1 = k1.pivot(index='cust_id', columns='lane_number', values='total_spend').reset_index().replace(np.nan,0) $ train = train.merge(k1,on=['cust_id'],how='left') $ test = test.merge(k1,on=['cust_id'],how='left')
from pandas.io.json import json_normalize $ user_info = json_normalize(tweets_clean['user']) $ user_info.columns
stock.head()
df["ZIPCODE"] = df.index
station_data = session.query(Stations).first() $ station_data.__dict__
feature_layer.query(where='POP2010>1000000', return_count_only=True)
re.findall('mailto:(.*?) so', text)
def save_combined_df( df_to_save, file_name ): $     df_to_save.to_csv( f'./data/{file_name}.csv' )
obs = df_gp_hr.groupby('level_0').mean() $ observation_data = obs['Observation (aspen)']
SANDAG_age_df.loc[(2012, '1')]
import statsmodels.api as sm $ logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']])
for i, incorrect in enumerate(incorrect[0:9]): $     plt.subplot(3,3,i+1) $     plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none') $     plt.title("Predicted {}, Class {}".format(predicted_classes[incorrect], y_true[incorrect])) $     plt.tight_layout()
import matplotlib.pyplot as plt $ import numpy as np $ import pandas as pd $ import seaborn as sns $
!head ../../data/msft2.csv  # Linux
df2 = df2.drop_duplicates(['user_id'], keep='first') $ df2.info()
cur.fetchall()
fig = plt.figure(figsize=(10,4)) $ ax = fig.add_subplot(111) $ ax = resid_701.plot(ax=ax);
squares = pd.Series([1, 4, 9, 16, 25]) $ print(squares.name) $ squares
df[~df.amount_initial.str.startswith('-$') & ~df.amount_initial.str.startswith('$')]
df_apps[df_apps.package_name.str.contains('com.facebook.katana')].count()
import matplotlib.pyplot as plt $ %matplotlib inline $ plt.hist(results["units"]["uqs"]) $ plt.xlabel("Sentence Quality Score") $ plt.ylabel("Sentences")
df.boxplot(column='rating', by='dog_type');
df = df.withColumn('label', F.when(col('label_tmp') > 0, 1.0).otherwise(0.0)) $ df.createOrReplaceTempView("df_view") $ df.orderBy('deviceid', 'date').select('deviceid', 'date', 'problemreported', 'label_tmp', 'label').show(20) $
dataframe = pd.DataFrame(tweets, columns=['User', 'Time','TweetText','Followers','RetweetCount']) $ dataframe.head()
df.to_csv(dataurl+'train200th.csv', sep=',', encoding='utf-8', index=False)
np.exp(0.0507), np.exp(0.0408)
n = len(df2[(df2.landing_page=='new_page')]) $ print(n) $ ntot = len(df2.index) $ prob_3 = n/ntot $ print(prob_3)
model.fit(train_words, fb_train.popular)
df_methods = [({'id':x['id'],'carrier':x['carrier'],'name':x['name']}) for x in json.loads(response.content)['shipping_methods']] $ df_methods = pd.DataFrame(df_methods)
pred_y_df = predict(classifier, df_x_test) $ pred_y_df[:10]
csvData.head()
image_predictions_df[image_predictions_df.tweet_id == 668623201287675904]
ts_kfold(3,3,regr_M7)
train_data.head()
all_dfs = [df, batting_df, pitching_df, fielding_df, salary_df] $ total_cells = sum([x.shape[0]*x.shape[1] for x in all_dfs]) $ print("total number of cells is = ", total_cells)
from scipy.stats import norm $ sig_level = norm.ppf(1-(0.05/2)) $ sig_level
STD_reorder_stats.shape
ltdate = pd.to_datetime(voters.LTDate.map(lambda x: x.replace(' 0:00', ''))) $ print(ltdate.describe()) $ ltdate.value_counts(dropna=False).head(5)
c1 = df.count() $ df = df.where(length('latitude') > 0).where(length('longitude') > 0) $ df = df.where(df.latitude > 0) $ c2 = df.count() $ print("Deleted {} rows with corrupted coordinates in latitude and longitude".format(c1-c2))
S.decision_obj.stomResist.value = 'Jarvis' $ S.decision_obj.stomResist.value
df_mes = df_mes[df_mes['extra']>=0] $ df_mes.shape[0]
x_train = x_train.reshape((len(x_train),1)) $ print(x_train.shape, y_train.shape)
plt.scatter(cdf.CYLINDERS, cdf.CO2EMISSIONS,  color='blue') $ plt.xlabel("Cylinder") $ plt.ylabel("Emission") $ plt.show()
def tvd(dist1, dist2): $     return 0.5*(np.sum(np.abs(dist1 - dist2)))
acc.find(vdate='20161103')
logit_mod2 = sm.Logit(df2['converted'], df2[['intercept','new_page','country_uk','country_us']]) $ results2 = logit_mod2.fit() $ results2.summary()
tmp.head()
weather_mean.iloc[4:8, 5:]
!head -n 40 example_data/clinvar_donor_acceptor_chr22.vcf
import numpy as np $ change = [] $ for entry in d["dataset_data"]["data"]: $     change.append(entry[4]) $ print("The largest change in a given day was $"+str(max(np.diff(change))))
q_all_pathdep = c.retrieve_query('https://v3.pto.mami-project.eu/query/8da2b65bd4f7cd8d56d90ddfcd85297e8aac54fcd0e04f0a0fa51b2937b3dc62') $ q_all_pathdep.metadata()
df_merged['text'].iloc[228]
ac['Issues'].describe()
df = pd.DataFrame([2, 3, 1, 4, 3, 5, 2, 6, 3]) $ df.quantile(q=[0.25, 0.75])
profits.to_csv('profits.csv')
mars_tweets = [] $ for status in tweepy.Cursor(api.user_timeline, id="@MarsWxReport").items(10): $     mars_tweets.append(status) $ mars_weather = mars_tweets[0]._json $ mars_weather
topics = 15
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs);
most = dfrecent.head(50)
df_clean.drop(df_clean[(df_clean.rating_numerator > 20)].index,inplace=True);
airbnb_df = pd.read_csv('data/airbnb2.csv') $ airbnb_df.head()
autos = autos.drop(["num_photos", "seller", "offer_type"], axis=1)
df_combined.dtypes $
df_clean.source.sample()
total_stations = session.query(Station.station).count() $ total_stations
grp1 = ncfile.createGroup('model_run1') $ grp2 = ncfile.createGroup('model_run2') $ for grp in ncfile.groups.items(): $     print(grp)
type(tw)
max_price = autos["price"] == autos["price"].max()
results = query_by_name(artist_url, query_type["simple"], "Nirvana") $ pretty_print(results['artists'])
df2[(df2.landing_page == 'new_page')].shape[0] / df2.shape[0]
week13 = week12.rename(columns={91:'91'}) $ stocks = stocks.rename(columns={'Week 12':'Week 13','84':'91'}) $ week13 = pd.merge(stocks,week13,on=['91','Tickers']) $ week13.drop_duplicates(subset='Link',inplace=True)
%matplotlib notebook $ from matplotlib import pylab
import pandas as pd $ import numpy as np $ from io import StringIO $ import re $ from IPython.display import display,clear_output
results = api.search(q=searchQuery, count=tweetsPerQry)
data.describe()
with open(datapath / 'simple.yaml', 'r') as stream: $     try: $         print(yaml.load(stream)) $     except yaml.YAMLError as exc: $         print(exc)
import requests
owns.info() $ owns.head(10)
df_tweets.reset_index().to_pickle("../tweets_extended.pkl")
from sklearn import tree $ clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=3, min_samples_split=2, max_features=4) $ clf = clf.fit(X, predict) $ tree.export_graphviz(clf, out_file='dt.dot',feature_names=features)
mb.Stool / mb.Tissue
train_df = pd.read_csv('Data/Titanic/train.csv') $ test_df = pd.read_csv('Data/Titanic/test.csv') $ combine = [train_df, test_df]
finals[finals.pts_l==1].sort_values(['season', "PTS"], ascending=[False, False]).head()
time_interval = suspect_1_data.groupby('day')['timestamp'].max() - suspect_1_data.groupby('day')['timestamp'].min()
run txt2pdf.py -o"2018-06-19 2013 FLORIDA HOSPITAL Sorted by payments.pdf"  "2018-06-19 2013 FLORIDA HOSPITAL Sorted by payments.txt"
from gensim import corpora, models $ dictionary = corpora.Dictionary(sws_removed_all_tweets)
df_h1b_nyc_ft.pw_1.describe()
bwd.head(20)
dfAnnualMGD = dfHaw_Discharge.groupby('Year')['flow_MGD'].agg(['sum','count']) $ dfAnnualMGD = dfAnnualMGD[dfAnnualMGD['count'] > 350] $ dfAnnualMGD.columns = ['AnnualFlow_MGD','Count']
X_train, y_train, X_test, y_test = utils.get_train_test_fm(feature_matrix,.75) $ y_train = np.log(y_train + 1) $ y_test = np.log(y_test + 1)
new_page_converted = np.random.choice(2, size = n_new, p=[0.8805, 0.1195]) $
output= "SELECT * from user where user_id='@Pratik'" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['User_id','User Name','Followers','Following','TweetCount'])
z_score, p_val = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller') $ ( 'p-value:', p_val, 'z-score:', z_score)
active_distinct_authors_latest_commit.schema
sp_re_df.columns = models
importances=model_rf_14_b.feature_importances_ $ features=pd.DataFrame(data=importances, columns=["importance"], index=x.columns) $ features
df_precep_dates_12mo = df_precep_dates_12mo.fillna(0) $ df_precep_dates_12mo.describe(include = [np.number]) $
today = datetime.date.today() $ margin = datetime.timedelta(days = 7) $ search_date = today - margin $ print('search date:', search_date) $
noaa_data['PRECIPITATION'].replace(range(-9999,0),np.nan,inplace=True)
merged_index_year = merged2.index.year
from pyspark import SparkContext $ sc = SparkContext()
new_page_converted = np.random.binomial(1, cr_under_null, size=n_new) $ print(len(new_page_converted)) $ new_page_converted
with open('test_data//open_close_test.txt') as my_file: $     print (my_file.read()[0:100])
number_of_commits =git_log['timestamp'].size $ number_of_authors = git_log.dropna(how='any')['author'].unique().size $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
df_grouped.drop(['passing_reading', 'passing_math'], axis = 1, inplace = True) $ df_grouped.columns
home_dest = df_titanic['home.dest'] $ print(home_dest.describe()) $
B2.paths['directory_paths']['indicator_settings'] $
duration_data = [doc_duration, RN_PA_duration, therapist_duration]
evaluator = text_classifier.evaluate(df_test)          $ evaluator.plot_confusion_matrix()
reg_mod_us = sm.OLS(df_all['converted'], df_all[['intercept', 'US']]) $ analysis_us = reg_mod_us.fit() $ analysis_us.summary()
animals = ['cat', 'dog', 'elephant', 'duck', 'hamster', 'sheep', 'parrot'] $ print(animals[0]) $ print(animals[2:5]) $ print(animals[::2])
tweet_df.shape
len(train_data[train_data.fuelType == 'andere'])
convert_old,convert_new,n_old,n_new
itemTable = pd.read_csv('itemTable.csv', index_col = 0) $ itemTable.head(10)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
df_ = df_merge[df_merge['Geography'].isin(['WY', 'AZ', 'MA', 'NY'])] $ df_ = pd.crosstab(df_['Geography'], df_['Medium'])
df2['intercept'] = 1 $ df2[['ab_page', 'drop_me']] = pd.get_dummies(df2['landing_page']) $ df2.head()
v_train = nltk.classify.apply_features(extract_features,sentiment_train_tweets) $ v_train_full = nltk.classify.apply_features(extract_features,sentiment_train_tweets_full) $ v_validation = nltk.classify.apply_features(extract_features,sentiment_validation_tweets) $ v_test  = nltk.classify.apply_features(extract_features,sentiment_test_tweets)
df_archive.sample(10)
bills_votes['congress'].value_counts()
infoExtractionRequest = requests.get('http://rfkhumanrights.org/media/filer_public/af/a1/afa1ea83-1d15-47d7-aea9-37658772f554/nelson_mandela.pdf', stream=True) $ infoExtractionBytes = io.BytesIO(infoExtractionRequest.content)
from sklearn.cross_validation import train_test_split
cutoff_times = generate_labels('5fPXqLcScoC93rH/gCPK+5Soj+XdNMXX9S3LhV5dJjM=', trans, $                                label_type = 'MS', churn_period = 30) $ cutoff_times.head()
data.iloc[1:3]
df = df[['Adj. Close', 'HL_PCT', 'PCT_change', 'Adj. Volume']] $ print(df.head())
learn.fit(1e-2, 2, cycle_len=1)
X_test.head()
len(kochdf.loc[kochdf['date'] == max_date])
iplot(data.groupby('user.login').size().sort_values(ascending=False)[:20].iplot(asFigure=True, dimensions=(750, 500), kind='bar'))
for col in autos["odometer_km"], autos["price"]: $     print(col.unique().shape) $     print(col.describe()) $     print(col.value_counts().head(10))
data[['phylum','value','patient']]
pd.set_option('display.float_format', lambda x: '%.0f' % x)
df
bad_indices = [] $ for i in range(len(temp['c'])): $     if not isinstance(temp['c'][i], list): $         bad_indices.append(i)
result.head(10)
client.experiments.list_runs()
store_items.fillna(method='ffill', axis=0)
df2.shape
print('\n Row x Columns of imported data:') $ print(dfa.shape) $ print(type(dfa))
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key='+API_KEY\ $ +'&start_date=2017-01-01&end_date=2017-12-31' $ r = requests.get(url)
fld = 'SchoolHoliday' $ df = df.sort_values(['Store', 'Date']) $ get_elapsed(fld, 'After') $ df = df.sort_values(['Store', 'Date'], ascending=[True, False]) $ get_elapsed(fld, 'Before')
p_diffs = [] $ for _ in range(10000): $     old_page_converted = np.random.binomial(1, control_cnv, control_num) $     new_page_converted = np.random.binomial(1, treatment_cnv, treatment_num) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean()) $
round(autos["odometer_km"].describe())
for var in filtered_df[numeric_cols]: $     filtered_df.replace({var: {'\$': ''}}, regex=True, inplace=True) $     filtered_df.replace({var: {'\,': ''}}, regex=True, inplace=True) $     filtered_df[var] = filtered_df[var].astype('float64', copy=False) $     filtered_df[var] = filtered_df[var].fillna(0)
emb_szs
stores.head()
df3.head(3)
batch = next(iter(md.trn_dl))
parse_dict['category'].head(5) $
df_new_page = df.query("landing_page == 'new_page'") $ df_new_page.count()['user_id'] / df['landing_page'].count()
inactive_org_id = clean_users[clean_users['active']==0][['org_id','active']].groupby('org_id').count() $ inactive_org_id.columns = ['inactive']
ifrom, iuntil = parse_date(sfrom), parse_date(suntil, end=True) $ print(f"Period of interest: {human_readable(ifrom)} - {human_readable(iuntil)}") $ print(f"Total duration = {(iuntil-ifrom)//(24*3600)} days")
from __future__ import division, print_function, unicode_literals
ALL.shape # The final sample we are working with contains 14,395 transactions across eight teams.
df_merge = pd.read_csv('Final_Documents/final_master.csv') $ df_merge.head()
novel_tweets(tw.get('tweets'))
df2 = df.query("(landing_page == 'old_page' and group == 'control') or (landing_page == 'new_page' and group == 'treatment')")
91//12
df_new.isnull().values.any()
print(nltk.sent_tokenize(example))
mean_weekday.iloc[[1,3]]
festivals
df_new=pd.merge(df_new, df_dummy, how='left',on='idx_id')
xd = pd.get_dummies(x)
data.keys()
treatment_mismatch = df.query("group == 'treatment' and landing_page == 'old_page'") $ control_mismatch = df.query("group == 'control' and landing_page == 'new_page'") $ print('The number of times the new_page and treatment dont line up is {}'.format(len(treatment_mismatch))) $ print('The number of times the old_page and control dont line up is {}'.format(len(control_mismatch))) $ print('Total dont line up is {}'.format(len(control_mismatch)+ len(treatment_mismatch)))
df.converted.sum()/df_length
compute_sentiment(text)
data = read_root('./preprocessed_files.root') $ data.head()
df1 = tweets_gametitle.groupby('Game Title Date')['date','source'].count()
new.head()
y_predict=[round(ii[0]) for ii in model.predict(x)] $ deviate=[0.5 for aa,bb in zip(y,y_predict) if aa==bb] $ plt.figure() $ plt.plot(y,marker='s',linestyle='') $ plt.plot(deviate,marker='h',markersize=1,linestyle='',color='k') $
jobs.loc[jobs.GPU == 1].groupby('FAIRSHARE').JobID.count().sort_values(ascending = False)
df.sample(10)
cont_total = df2.query('group == "control"').nunique() $ pct_conv_cont = df2.query('group == "control" and converted == 1').user_id.nunique() $ print(pct_conv_cont/cont_total['user_id'])
ts[[0, 2, 6]].index
df2.head() # checking dataframe for ab_page column
frame - series
df_daily = df_daily.reset_index() $ df_daily.drop('SCP', axis=1, inplace=True) $ df_daily.head()
merged_portfolio_sp_latest_YTD_sp['Cum Invst'] = merged_portfolio_sp_latest_YTD_sp['Cost Basis'].cumsum() $ merged_portfolio_sp_latest_YTD_sp['Cum Ticker Returns'] = merged_portfolio_sp_latest_YTD_sp['Ticker Share Value'].cumsum() $ merged_portfolio_sp_latest_YTD_sp['Cum SP Returns'] = merged_portfolio_sp_latest_YTD_sp['SP 500 Value'].cumsum() $ merged_portfolio_sp_latest_YTD_sp['Cum Ticker ROI Mult'] = merged_portfolio_sp_latest_YTD_sp['Cum Ticker Returns'] / merged_portfolio_sp_latest_YTD_sp['Cum Invst'] $ merged_portfolio_sp_latest_YTD_sp.head() $
X_test_matrix = cvec.transform(X_test) $ forest.predict(X_test_matrix) $ forest.score(X_test_matrix, y_test)
df[['VOL','FLOW','IC','SF','ACQ','REVENUE']].describe()
prices = DataFrame({'ESU2': near, 'ESZ2': far}, index=rng)
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05/2)))
new_cols_cust = parseAndComputeTopics(df_one_hot_campaign_cust) $ new_cols_prosp = parseAndComputeTopics(df_one_hot_campaign_prosp)
train[target].value_counts()
stanford_word_list = pd.read_csv('dictionary_stanford.txt', header=None) $ stanford_word_list.columns=['Word']
index = similarities.MatrixSimilarity(doc_vecs, $                                       num_features=topics) $ sims = sorted(enumerate(index[doc_vecs[6]]), key=lambda item: -item[1])
df2.drop_duplicates('user_id',inplace=True);
!wget https://storage.googleapis.com/kdd-seq2seq-2018/kdd_seq2seq_weights.h5
infinity.to_csv('InfinityWars_Predictions_2.csv', encoding='utf-8')
trump.groupby("source").describe()
df.info()
for (name, sex) in b_list: $     get_all_tweets(name, sex) $     df = df.append(pd.read_csv('%s_tweets.csv' % name))
model.add(Conv1D(filters=10, kernel_size=10, $                  activation='relu', padding='same'))
score = cross_val_score(lr, X_test_dtm, y_test, cv = cv, verbose = 1)
prob_temp = len(df2[df2["landing_page"] == "new_page"]) / len(df2) $ print("Probability that an individual received the new page: {}%".format(round(prob_temp, 2)))
X_test.isnull().sum()
df1['forcast']=np.nan
df_group_by.columns = ['msno','payment_method_id','payment_plan_days','not_auto_renew','msno_count','is_cancel','is_discount','amount_per_day','membership_duration','membership_expire_date']
def parse_full_date(local_date): $     return local_date.strftime('%m-%d-%Y')
tweet_df.head()
p_new = prob_conv $ p_new
cityID = '28ace6b8d6dbc3af' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chula_Vista.append(tweet) 
df.query('converted == 1').user_id.nunique() / df['user_id'].nunique()
file4=file4.dropna() $ file4.count()
image_copy.sample(5)
pgh_311_data = pd.read_csv("https://data.wprdc.org/datastore/dump/40776043-ad00-40f5-9dc8-1fde865ff571", $                            index_col="CREATED_ON", $                            parse_dates=True) $ pgh_311_data.head()
bc.info()
print (festivals.loc[2]['latitude']) $ print (festivals.loc[2]['longitude']) $ print(festivals.index) $ festivals.head(3) $
tm_2030 = pd.read_csv('input/data/trans_2030_m.csv', encoding='utf8', index_col=0)
pd.Series(index = feats_used, data = gbm1.feature_importances_).sort_values().plot(kind = 'bar')
import string $ def text_process(text): $     nopunc = [char for char in text if char not in string.punctuation] $     nopunc = ''.join(nopunc) $     return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]
model.fit(x=training_data, y=training_labels, $           validation_data=(validation_data, validation_labels), $           nb_epoch=1, verbose=1)
hs.to_sql("stations", conn, index=False, if_exists='replace')
posts.head()
df_new.tail()
import torchtext $ from torchtext import vocab, data $ from torchtext.datasets import language_modeling
autos['ad_created'].str[:10].describe()
df_B=pd.DataFrame({"Student_height":heights_B,"Student_weight":weights_B})
model.accuracy('./datasets/questions-words.txt')
data.groupby(data.index.time).mean().plot();
n_new = (df2['landing_page'] == 'new_page').sum(); $ n_new
print('Merchant plan offer distribution: ') $ print(data['Merchant_Plan_Offer'].value_counts())
"We have {0} tweets.".format(len(D1))
USER_PLANS_df.head()
data_l2_begin = tmpdf.index[tmpdf[tmpdf.isin(DATA_L2_HDR_KEYS)].notnull().any(axis=1)].tolist() $ data_l2_begin
print('The largest change in a day is ' + str(max((data['Open'] - data['Close']).abs())))
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt
parsed_email_data['institution'] = parsed_email_data['address'].str.split("@").str.get(1) $ parsed_email_data.head(10)
import json $ from urllib2 import urlopen
df_clean = pd.melt(df_clean, id_vars = ['tweet_id', 'timestamp', 'text', 'expanded_urls', $                       'rating_numerator', 'rating_denominator', 'name' ]) $ df_clean.info() $
cur.execute("DROP VIEW empvw_20")
blurb_SVD = tSVD.fit_transform(blurbs_to_vect)
has_missing_values = df.isnull().values.any() $ print("Yes, It has" if has_missing_values else "No, It has not")
AAPL['close'].ewm(alpha=.9, adjust=False).mean().plot()
d_housing=detroit_census2.drop(detroit_census2.index[:24]) $ d_housing=d_housing.drop(d_housing.index[5:]) $ d_housing
autos["price_euro"].value_counts(normalize = True).sort_index(ascending = True).head(20)
conn.columninfo(table=dict(name='iris', caslib='casuser'))
test_embedding=graf_test['DETAILS3'].apply(lambda words: np.mean([model[w] for w in words if w in model], axis=0)) $
calls_df.shape
conn.columninfo(table=dict(name='myclass', caslib='casuser'))
l = ['bmw','gas','std','two','sedan','rwd','six','182','5400','16','22','41315'] $ sum = 0 $ for i in l: $     sum+= len(i) $ print (sum)
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs) $ plt.title("Histogram") $ plt.show()
second_comb = df[(df['group']!='treatment') & (df['landing_page']=='new_page')]
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?" + \ $       "&start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY $ req = requests.get(url)
for i in forecast_set: $     next_date = datetime.datetime.fromtimestamp(next_unix) $     next_unix += 86400 $     df.loc[next_date] = [np.nan for _ in range(len(df.columns) - 1)] + [i]
companies = pd.read_csv('companies.csv') $ companies.set_index('Ticker', inplace=True) $ companies['tweet_ticker']=companies.index.map(lambda x: '$'+x) $ companies
stocks_happiness
advcoeffdf.tail(10)
seq2seq_inf_rec.demo_model_predictions(n=1, issue_df=testdf, threshold=1)
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True) $ toma.iloc[::20].plot(ax=ax, logy=True, ms=10, style=['.', '.', '.', '.', '.', '.']) $ ax.set_ylabel('Relative error') $
obj.rank()
best_dt = pickle.load(open('xgb_model.pkl', 'rb'))
df = pd.read_csv("FuelConsumption.csv") $ df.head()
access_logs_df.
reddit = pd.read_csv(filename) $ reddit.drop('Unnamed: 0', axis = 1, inplace = True) $ reddit.head()
df_uro_no_cat = df_uro_no_metac.drop(columns = ls_other_columns)
new_page_converted = np.random.choice([0,1], size=145310, p=[1-0.1196, 0.1196]); new_page_converted
texts = [[token for token in text.split() if frequency[token] > 1 and token not in nltk_stops] $           for text in documents] $ texts
summaries = "".join(df.titles) $ ngrams_summaries = cvec.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(20)
len(cpq_status['Building ID'].unique())
df = pd.read_csv('end-part2_df.csv').set_index('date') $ df.describe().T
fig = plt.figure(figsize=(12,8)) $ ax = fig.add_subplot(111) $ fig = qqplot(resid_6203, line='q', ax=ax, fit=True)
df_final.info()
NYPD_df['Complaint Type'].value_counts().head()
df.set_index(['operator', 'part'], inplace=True)
train.groupby("DOW")["any_spot"].mean()
if 'ROI' in globals(): $     del ROI
wash_park_matrix = count_vectorizer.fit_transform(important_tweets.text)
comments = comments[comments.parent_post_id.notnull()]
discasto = pb.User(pb.Site('es', 'wikipedia'), 'Discasto') $ discasto.isBlocked()
S_distributedTopmodel.decision_obj.bcLowrSoiH.options, S_distributedTopmodel.decision_obj.bcLowrSoiH.value
avg = (preds2_1month + preds4_2month + preds6_3month) / 3 $ submission2['proba'] = avg
old_page_converted = np.random.choice([1,0],size=n_old,p=[p_old,(1-p_old)]) $ old_page_converted.mean()
country_dummy_list = np.unique(df_new['country'].values) $ df_new[["control","treatment"]] = pd.get_dummies(df_new["group"]) # for group $ df_new[country_dummy_list] = pd.get_dummies(df_new["country"]) # for country $ df_new["intercept"] = 1 $ df_new.head()
endog = data_df.Count $ exog = data_df[['TempC','WindSpeed','Precip']] $
df = create_df('data/epl_data.csv')
prob_ZeroFill = LogisticModel_ZeroFill.predict_proba(Test) $ prob_ZeroFill
sqldmh = dmh.SQLTable('iris', eng)
old_converted = np.random.choice([1, 0], size=len(df_old), p=[P_mean, (1-P_mean)]) $ old_converted.mean()
conn.get_tables()
n_new=df2.query('landing_page == "new_page"').user_id.nunique() $ n_new
df3.groupby('country').mean()
pax_raw.groupby('seqn').paxcal.mean().value_counts()
tweet_table = df_cleaned[['created_at','id','source_cleaned_2','user_screen_name','user_followers_count','user_location','text','entities_hashtags_text','lang','in_reply_to_screen_name','retweet_count','retweeted','favorite_count','favorited']]
nypd_count_df = nypd_df.groupby(['create_date'], as_index=False)['Agency'].count() $ nypd_count_df.columns = ['create_date', 'sidewalk_parking_count']
import splinter
acc.get_last(10)
df_users.last_session_creation_time.fillna(method='ffill',inplace=True)
data_oneyear=dt.date(2018, 8, 6) - dt.timedelta(days=365) $ print(data_oneyear) $ precip_12mo = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date > data_oneyear).order_by(Measurement.date).all() $ print(precip_12mo)
runs = most_yards[most_yards['PlayType'] == 'Run']
df_predictions_clean['p1'] = df_predictions_clean['p1'].str.title() $ df_predictions_clean['p2'] = df_predictions_clean['p2'].str.title() $ df_predictions_clean['p3'] = df_predictions_clean['p3'].str.title() $
temp = temp.pivot(index = "time",columns = "owning_department",values = "count")
df.sort_index(axis=0, ascending=False)  # Sort by index
sys.path.insert(0, '/Users/pedrohserrano/neuroscience/utils') #internal packages $ import buildms as ms $ import statsms as stms $ color_ms = '#386cb0' #blue, This is the color chosen for patients with Multiple Sclerosis $ color_hc = 'red'#This is the color chosen for health control participants
transit_df['EXITS'] = pd.to_numeric(transit_df['EXITS'],errors='coerce') $ transit_df = transit_df.set_index('DATETIME') $ transit_df.info()
users = users.sort_values(by=['avg_score'], ascending=False) $ top_25_users = users.index[1:50].tolist()
all_tweets.columns
precision, recall, f_score, _ = prf(y_true = y_true, y_pred = y_pred, average = 'micro') $ accuracy = acc(y_true = y_true, y_pred = y_pred) $ print('precision: ', precision, 'recall: ', recall, 'f-score: ', f_score, 'accuracy: ', accuracy)
countries_df.country.unique()
import numpy as np $ import pandas as pd $ import seaborn as sns $ import matplotlib.pyplot as plt $ %config InlineBackend.figure_format = 'svg'
pd.crosstab(df_concat_2["message_likes_dummy"],df_concat_2["page"],margins=True)
import requests $ import json $
doctors = merged2[merged2['Specialty'] == 'doctor'] $ RNPA = merged2[merged2['Specialty'] == 'RN/PA'] $ therapists = merged2[merged2['Specialty'] == 'therapist']
sample_mean = new_page_converted.mean() - old_page_converted.mean() $ sample_mean
allocs=[0.4,0.4,0.1,0.1] $ allocs=np.array(allocs) $ alloced=(normed*allocs) $ alloced.head()
train = pd.read_json('./data/train.json')
coefs = pd.DataFrame(log_reg_over.coef_, columns=X_train.columns) $ coefs.T.sort_values(0).head().T
SANDAG_age_df = pd.read_csv(open(SANDAG_age_filepath), dtype={'TRACT': str}) $ SANDAG_age_df.columns = ['TRACT', 'YEAR', 'SEX', 'AGE_RANGE', 'POPULATION'] $ SANDAG_age_df.set_index(['YEAR', 'TRACT'], inplace=True) $ SANDAG_age_df.sort_index(inplace=True) $ SANDAG_age_df.head()
giss_temp.dtypes
df2.query("group=='treatment'").count()
evaluator = MulticlassClassificationEvaluator( $     labelCol="label", predictionCol="prediction", metricName="weightedRecall") $ recall = evaluator.evaluate(predictions) $ print("Recall = %g " % (recall))
tweets['created'] = pd.to_datetime(tweets['created'])
M_data = session.query(Measurement).first() $ M_data.__dict__
df2.drop(labels=['control'], axis=1,inplace=True) $ df2.head()
merged2 = pd.merge(dataset, doi_pid, how="left", left_on="fixed_doi", right_on="DOI") $ merged2.head(3).T
app_counts = df.groupby(['ab_test_group', 'is_application']).count().reset_index() $
sns.set_style("whitegrid") $ sns.barplot(x="sex", y="survived", data=titanic)
plt.rcParams['figure.figsize'] = [16,4] $ plt.plot(pd.to_datetime(mydf1.datetime),mydf1.fuelVoltage,'g.', markersize = 2); $
internet.head()
data['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data['Tweets'] ])
start = timer() $ partition_to_labels(1) $ end = timer() $ print(f'{round(end - start)} seconds elapsed.')
plt.hist(final_data.rating) $ plt.title('Starred, Owned, and Both') $ plt.ylabel("Count") $ plt.savefig('/home/ubuntu/PROJECT/github-collaborator/matplots/star_own_distribution.png');
IKEA_data = pd.read_csv(path + "IKEA_September_Data.csv", index_col=None, sep = ',')
s.loc['c'] $
weather_mean.iloc[1, 4]
predicted_talks_vector
df = df.drop(['DESC', 'EXITS'], axis = 1, errors = "ignore") $ df.head(10)
model = XGBRegressor()
head = pd.Timestamp('20150617') $ tail = pd.Timestamp('20150628') $ df=site.get_data(sensortype='electricity', head=head,tail=tail, diff=True, unit='kW') $ charts.plot(df, stock=True, show='inline')
ValidEvents.head(3)
type(dfs[2])
with open('data.csv', 'r') as fh: $     print fh.readline() # headers $     print fh.readline() # first row
date_retweets.created_at.value_counts().plot() $ plt.xticks(rotation=60) $
df1 = tier1_df.reset_index() $ df1 = df1.rename(columns={'Date':'ds', 'Incidents':'y'}) $ df_orig = df1['y'].to_frame() $ df_orig.index = df1['ds'] $ n = np.int(df_orig.count())
make(0) $ make(1) $ make(2) $ make(-1) $
target_pf.iloc[:2]
print('input dim = {}'.format(input_image.ndim)) $ print('input shape = {}'.format(input_image.shape))
allNames = list(df['sn'].unique())
pickup_orders = data[['ceil_10min','pickup_cluster']] $ dropoff_orders = data[['ceil_10min','dropoff_cluster']] $ ride_orders = data[['ceil_10min','ride_cluster']]
pd.Series([2, 4, 6])
clean_appt_df['Weekday'] = clean_appt_df['AppointmentDay'].dt.weekday $ clean_appt_df.groupby('Weekday')['No-show']\ $     .value_counts(normalize=True)\ $     .loc[:,'Yes']\ $     .plot.bar()
data[data['authorName'] == 'Lunulls A. Lima Silva']#['link_weight']#.loc[3]
q2_results = session.query(Stations.name,func.count(Measurements.date)).filter(Stations.station == Measurements.station)\ $             .group_by(Measurements.station).all() $
from scipy import stats $ instance.initialize(parameters)
guineaCases = guineaFullDf.loc['Total new cases registered so far'] $ guineaCases.head()
food.drop("created_datetime", axis=1, inplace=True)
trained_model_RF= random_forest_classifier(train_ind[features], train_dep[response])
import pandas as pd $ url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies' $ df = pd.read_html(url, index_col=None,skiprows=1)[0] $ df.columns=['Ticker symbol','Security','SEC filings','GICS Sector','GICS Sub Industry','Address of Headquarters','Date first added','CIK'] $ df.head()
log_reg_under = LogisticRegressionCV(Cs=[0.1, 0.6], scoring='neg_log_loss') $ log_reg_under.fit(X_train, y_train_under)
NameEvents.head(1)
titanic.survived.head()
df.sort_values(by=['Company','Product'],inplace=True)
from sklearn.naive_bayes import MultinomialNB
weather_mean[low_rh]['Pressure (kPa)'] = 0 $ weather_mean
df_concat.drop(["likes.data", 'likes.summary.can_like','likes.summary.has_liked'], 1, inplace = True)
%matplotlib inline $ sns.violinplot(data=december, inner="box", orient = "h", bw=.03)
dfss.price.max()
df_new['UK'] = pd.get_dummies(df_new['country'])['UK']
for key, value in close.items(): $     if value == max_dif_Q5: $         print key, value
result3 = result3.reset_index() $ result3.head(20)
np.percentile(p_diffs,2.5), np.percentile(p_diffs,97.5)
opt_fn = partial(optim.Adam, betas=(0.7, 0.99)) # changing down to 70
nb = MultinomialNB() $ nb.fit(X_train_total, y_train) $ nb.score(X_test_total_checked, y_test)
stock.head()
import statsmodels.api as sm $ old_page_converted = df2.query("landing_page == 'old_page' and converted == 1").shape[0] $ new_page_converted = df2.query("landing_page == 'new_page' and converted == 1").shape[0] $ n_old = nold $ n_new = nnew
mask = y_test.index $ t_flag = y_test == 0 $ p_flag = pred == 1
train_cols=df2.columns[5:7] $ print(train_cols) $ logit = sm.Logit(df2['converted'],df2[train_cols]) $ result = logit.fit()
len(data_issues_csv)
auto_new.CarYear = auto_new.CarYear.astype(str)
print(len(df5['andrew_id_hash'].value_counts().where(df5['andrew_id_hash'].value_counts() > 0).dropna())) $ print(len(df5['andrew_id_hash'].value_counts().where(df5['andrew_id_hash'].value_counts() > 1).dropna())) $ print(len(df5['andrew_id_hash'].value_counts().where(df5['andrew_id_hash'].value_counts() > 20).dropna()))
house_data = house_data.sort_values(by = ['listing_id'])
tweet = result[0] #Get the first tweet in the result $ for param in dir(tweet): $     if not param.startswith("_"): $         print "%s : %s\n" % (param, eval('tweet.'+param)) $
aux.loc[~ (aux.cuteness_y == 'nan'),:].head()
pre_strategy_users = people_person[pre_strategy] $ pre_strategy_users.head()
df = df.reset_index()
for post in posts.find({"reinsurer": "AIG"}): $     pprint.pprint(post)
org_id_table = active_org_id.join(inactive_org_id) $ org_id_table['active_proportion'] = org_id_table['active'] / (org_id_table['active']+org_id_table['inactive'])
baseball.rank(ascending=False).head()
df_main.p1 = clean_strings(df_main.p1) $ df_main.p2 = clean_strings(df_main.p2) $ df_main.p3 = clean_strings(df_main.p3)
from quilt.data.dsdb import processing_test $ processing_test
tweet = result[0] $ tweet
titanic[titanic.survived == 1].head()
%%time $ df = pd.read_pickle("newRev_VegCols_US.pkl")
from sklearn import cross_validation
autos['ad_created'] = autos['ad_created'].str[:10] $ ad_created_count_norm = autos['ad_created'].value_counts(normalize=True, dropna=False) $ ad_created_count_norm.sort_index() $
import pandas as pd $ import numpy as np
print ("Jumlah Data Tweets: ", len(tweets22Mar) + len(tweets1Apr) + len(tweets2Apr)) $ print(f"\nJumlah Data:\n22 Maret: {len(tweets22Mar)}\n1 April: {len(tweets1Apr)}\n2 April: {len(tweets2Apr)}\n")
austin= austin[austin['distance_travelled']>0] $ austin.shape
del df_ser_dict["Four"] $ df_ser_dict
df_twitter_copy = df_twitter_copy.drop(['in_reply_to_status_id', 'in_reply_to_user_id'], axis = 1)
import pandas as pd $ import numpy as np $ import datetime
from scipy.stats import norm $ z_crit = norm.ppf(0.95) $ print(z_crit)
predictions = pd.read_csv('image-predictions.tsv', sep='\t') $ predictions
preg.shape
ab_file.info()    # Displaying information about the dataset
validation.analysis(observation_data, Jarvis_resistance_simulation_1)
df.rolling(window=3).mean()
